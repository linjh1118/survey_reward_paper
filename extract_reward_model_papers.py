#!/usr/bin/env python3
"""
æå–ç»¼è¿°è®ºæ–‡ç›¸å…³çš„è®ºæ–‡æ•°æ®
Extract papers related to the survey paper chapters
"""

import json
import os
import glob
from collections import defaultdict
import re

# å®šä¹‰ä¸‰ä¸ªç« èŠ‚çš„å…³é”®è¯
CHAPTER_KEYWORDS = {
    "test_time_scaling": {
        "name": "Test-Time Scaling",
        "keywords": [
            # Test-time ç›¸å…³
            "test-time", "test time", "inference-time", "inference time",
            # Scaling ç›¸å…³
            "scaling", "scale", "scaling law", "compute scaling",
            # Test-time training/adaptation
            "test-time training", "test-time adaptation", "test-time fine-tuning",
            "test time training", "test time adaptation", "test time fine-tuning",
            # Test-time compute
            "test-time compute", "inference compute", "compute at inference",
            # æ¨ç†æ—¶ä¼˜åŒ–
            "inference optimization", "runtime optimization",
            # æ€ç»´é“¾ç›¸å…³
            "chain of thought", "reasoning at test time", "multi-step reasoning",
            # æœç´¢å’Œé‡‡æ ·
            "search at inference", "beam search", "sampling strategies",
            "monte carlo tree search", "MCTS",
            # o1ç›¸å…³
            "o1", "strawberry", "reasoning model",
            # éªŒè¯å’Œè‡ªæˆ‘æ”¹è¿›
            "self-verification", "self-correction", "iterative refinement"
        ]
    },
    "reward_model_rl": {
        "name": "Reward Model for RL", 
        "keywords": [
            # Reward model æ ¸å¿ƒ
            "reward model", "reward modeling", "reward function",
            # RLHFç›¸å…³
            "RLHF", "reinforcement learning from human feedback",
            "human feedback", "preference learning",
            # PPOå’ŒRLç®—æ³•
            "PPO", "proximal policy optimization", "policy gradient",
            "reinforcement learning", "policy optimization",
            # åå¥½å’Œæ¯”è¾ƒ
            "preference", "comparison", "ranking", "pairwise",
            # Constitutional AI
            "constitutional AI", "RLAIF", "AI feedback",
            # ä»·å€¼å¯¹é½
            "alignment", "value alignment", "human alignment",
            # å¥–åŠ±é»‘å®¢
            "reward hacking", "goodhart's law", "optimization pressure",
            # å…·ä½“æ–¹æ³•
            "DPO", "direct preference optimization", "SLiC", "LIMA",
            "InstructGPT", "ChatGPT training"
        ]
    },
    "reward_model_benchmark": {
        "name": "Reward Model Benchmark",
        "keywords": [
            # Benchmarkæ ¸å¿ƒ
            "benchmark", "evaluation", "dataset", "testbed",
            # Reward modelè¯„ä¼°
            "reward model evaluation", "preference dataset", 
            "human preference", "annotation",
            # å…·ä½“benchmark
            "helpfulness", "harmlessness", "honesty",
            "truthfulness", "factuality", "safety",
            # è¯„ä¼°æŒ‡æ ‡
            "correlation", "agreement", "consistency", "reliability",
            "inter-annotator agreement", "kappa", "accuracy",
            # æ•°æ®é›†
            "Anthropic HH", "OpenAI summarization", "WebGPT",
            "StackLLaMA", "Alpaca", "Vicuna",
            # è¯„ä¼°ä»»åŠ¡
            "summarization", "dialogue", "question answering",
            "code generation", "mathematical reasoning",
            # å¤šç»´åº¦è¯„ä¼°
            "multi-dimensional", "aspect-based", "fine-grained",
            "rubric", "criteria", "dimension"
        ]
    }
}

def load_jsonl_file(file_path):
    """åŠ è½½JSONLæ–‡ä»¶"""
    papers = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    papers.append(json.loads(line))
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
    return papers

def extract_keywords_from_text(text, keywords):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯åŒ¹é…"""
    text_lower = text.lower()
    matched_keywords = []
    
    for keyword in keywords:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œè¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
        pattern = r'\b' + re.escape(keyword.lower()) + r'\b'
        if re.search(pattern, text_lower):
            matched_keywords.append(keyword)
    
    return matched_keywords

def categorize_paper(paper, chapter_keywords):
    """æ ¹æ®å…³é”®è¯å°†è®ºæ–‡åˆ†ç±»åˆ°ç« èŠ‚"""
    title = paper.get('title', '')
    summary = paper.get('summary', '')
    
    # ç»„åˆæ ‡é¢˜å’Œæ‘˜è¦è¿›è¡ŒåŒ¹é…
    full_text = f"{title} {summary}"
    
    paper_categories = {}
    
    for chapter_id, chapter_info in chapter_keywords.items():
        matched_keywords = extract_keywords_from_text(full_text, chapter_info['keywords'])
        if matched_keywords:
            paper_categories[chapter_id] = {
                'chapter_name': chapter_info['name'],
                'matched_keywords': matched_keywords,
                'score': len(matched_keywords)  # åŒ¹é…å…³é”®è¯æ•°é‡ä½œä¸ºç›¸å…³æ€§åˆ†æ•°
            }
    
    return paper_categories

def extract_papers_by_chapters(data_dir='data'):
    """ä»æ‰€æœ‰æ•°æ®æ–‡ä»¶ä¸­æå–ç« èŠ‚ç›¸å…³è®ºæ–‡"""
    all_papers = []
    chapter_papers = defaultdict(list)
    
    # è·å–æ‰€æœ‰JSONLæ–‡ä»¶
    jsonl_files = glob.glob(os.path.join(data_dir, '*.jsonl'))
    # æ’é™¤AIå¢å¼ºçš„æ–‡ä»¶ï¼Œåªä½¿ç”¨åŸå§‹æ•°æ®
    jsonl_files = [f for f in jsonl_files if '_AI_enhanced_' not in f]
    
    print(f"æ‰¾åˆ° {len(jsonl_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file_path in sorted(jsonl_files):
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        papers = load_jsonl_file(file_path)
        
        for paper in papers:
            # åˆ†ç±»è®ºæ–‡
            categories = categorize_paper(paper, CHAPTER_KEYWORDS)
            
            if categories:
                # ä¸ºè®ºæ–‡æ·»åŠ åˆ†ç±»ä¿¡æ¯
                paper['survey_categories'] = categories
                paper['source_file'] = os.path.basename(file_path)
                
                # æŒ‰ç« èŠ‚å­˜å‚¨
                for chapter_id in categories.keys():
                    chapter_papers[chapter_id].append(paper)
                
                all_papers.append(paper)
    
    return all_papers, dict(chapter_papers)

def save_extracted_data(all_papers, chapter_papers, output_dir='survey_data'):
    """ä¿å­˜æå–çš„æ•°æ®"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # ä¿å­˜æ‰€æœ‰ç›¸å…³è®ºæ–‡
    with open(os.path.join(output_dir, 'all_survey_papers.jsonl'), 'w', encoding='utf-8') as f:
        for paper in all_papers:
            f.write(json.dumps(paper, ensure_ascii=False) + '\n')
    
    # æŒ‰ç« èŠ‚ä¿å­˜
    for chapter_id, papers in chapter_papers.items():
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        papers.sort(key=lambda x: max(x['survey_categories'][chapter_id]['score'] 
                                    for cat_id in x['survey_categories'] 
                                    if cat_id == chapter_id), reverse=True)
        
        filename = f'{chapter_id}_papers.jsonl'
        with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as f:
            for paper in papers:
                f.write(json.dumps(paper, ensure_ascii=False) + '\n')
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        'total_papers': len(all_papers),
        'chapter_stats': {}
    }
    
    for chapter_id, papers in chapter_papers.items():
        chapter_name = CHAPTER_KEYWORDS[chapter_id]['name']
        stats['chapter_stats'][chapter_id] = {
            'name': chapter_name,
            'count': len(papers),
            'top_keywords': get_top_keywords(papers, chapter_id)
        }
    
    with open(os.path.join(output_dir, 'extraction_stats.json'), 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    return stats

def get_top_keywords(papers, chapter_id):
    """è·å–æœ€å¸¸åŒ¹é…çš„å…³é”®è¯"""
    keyword_counts = defaultdict(int)
    
    for paper in papers:
        if chapter_id in paper.get('survey_categories', {}):
            for keyword in paper['survey_categories'][chapter_id]['matched_keywords']:
                keyword_counts[keyword] += 1
    
    # è¿”å›å‰10ä¸ªæœ€å¸¸è§çš„å…³é”®è¯
    return sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]

def print_statistics(stats):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("è®ºæ–‡æå–ç»Ÿè®¡ / Paper Extraction Statistics")
    print("="*60)
    print(f"æ€»è®ºæ–‡æ•°é‡: {stats['total_papers']}")
    print("\nç« èŠ‚ç»Ÿè®¡:")
    
    for chapter_id, chapter_stats in stats['chapter_stats'].items():
        print(f"\nğŸ“š {chapter_stats['name']}")
        print(f"   è®ºæ–‡æ•°é‡: {chapter_stats['count']}")
        print(f"   çƒ­é—¨å…³é”®è¯:")
        for keyword, count in chapter_stats['top_keywords'][:5]:
            print(f"     - {keyword}: {count}")

def main():
    print("å¼€å§‹æå–ç»¼è¿°è®ºæ–‡ç›¸å…³æ•°æ®...")
    
    # æå–è®ºæ–‡
    all_papers, chapter_papers = extract_papers_by_chapters()
    
    if not all_papers:
        print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚è¯·æ£€æŸ¥å…³é”®è¯è®¾ç½®ã€‚")
        return
    
    # ä¿å­˜æ•°æ®
    stats = save_extracted_data(all_papers, chapter_papers)
    
    # æ‰“å°ç»Ÿè®¡
    print_statistics(stats)
    
    print(f"\nâœ… æå–å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ° survey_data/ ç›®å½•")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - all_survey_papers.jsonl: æ‰€æœ‰ç›¸å…³è®ºæ–‡")
    print("  - test_time_scaling_papers.jsonl: ç¬¬ä¸€ç« ç›¸å…³è®ºæ–‡")
    print("  - reward_model_rl_papers.jsonl: ç¬¬äºŒç« ç›¸å…³è®ºæ–‡") 
    print("  - reward_model_benchmark_papers.jsonl: ç¬¬ä¸‰ç« ç›¸å…³è®ºæ–‡")
    print("  - extraction_stats.json: æå–ç»Ÿè®¡ä¿¡æ¯")

if __name__ == "__main__":
    main()
