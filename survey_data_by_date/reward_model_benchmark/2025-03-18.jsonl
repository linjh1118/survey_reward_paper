{"id": "2503.12215", "pdf": "https://arxiv.org/pdf/2503.12215", "abs": "https://arxiv.org/abs/2503.12215", "authors": ["Amulya Reddy Maligireddy", "Manohar Reddy Uppula", "Nidhi Rastogi", "Yaswanth Reddy Parla"], "title": "Gun Detection Using Combined Human Pose and Weapon Appearance", "categories": ["cs.CV"], "comment": null, "summary": "The increasing frequency of firearm-related incidents has necessitated\nadvancements in security and surveillance systems, particularly in firearm\ndetection within public spaces. Traditional gun detection methods rely on\nmanual inspections and continuous human monitoring of CCTV footage, which are\nlabor-intensive and prone to high false positive and negative rates. To address\nthese limitations, we propose a novel approach that integrates human pose\nestimation with weapon appearance recognition using deep learning techniques.\nUnlike prior studies that focus on either body pose estimation or firearm\ndetection in isolation, our method jointly analyzes posture and weapon presence\nto enhance detection accuracy in real-world, dynamic environments. To train our\nmodel, we curated a diverse dataset comprising images from open-source\nrepositories such as IMFDB and Monash Guns, supplemented with AI-generated and\nmanually collected images from web sources. This dataset ensures robust\ngeneralization and realistic performance evaluation under various surveillance\nconditions. Our research aims to improve the precision and reliability of\nfirearm detection systems, contributing to enhanced public safety and threat\nmitigation in high-risk areas.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "correlation", "agreement"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12215", "pdf": "https://arxiv.org/pdf/2503.12215", "abs": "https://arxiv.org/abs/2503.12215", "authors": ["Amulya Reddy Maligireddy", "Manohar Reddy Uppula", "Nidhi Rastogi", "Yaswanth Reddy Parla"], "title": "Gun Detection Using Combined Human Pose and Weapon Appearance", "categories": ["cs.CV"], "comment": null, "summary": "The increasing frequency of firearm-related incidents has necessitated\nadvancements in security and surveillance systems, particularly in firearm\ndetection within public spaces. Traditional gun detection methods rely on\nmanual inspections and continuous human monitoring of CCTV footage, which are\nlabor-intensive and prone to high false positive and negative rates. To address\nthese limitations, we propose a novel approach that integrates human pose\nestimation with weapon appearance recognition using deep learning techniques.\nUnlike prior studies that focus on either body pose estimation or firearm\ndetection in isolation, our method jointly analyzes posture and weapon presence\nto enhance detection accuracy in real-world, dynamic environments. To train our\nmodel, we curated a diverse dataset comprising images from open-source\nrepositories such as IMFDB and Monash Guns, supplemented with AI-generated and\nmanually collected images from web sources. This dataset ensures robust\ngeneralization and realistic performance evaluation under various surveillance\nconditions. Our research aims to improve the precision and reliability of\nfirearm detection systems, contributing to enhanced public safety and threat\nmitigation in high-risk areas.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "correlation", "agreement"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12303", "pdf": "https://arxiv.org/pdf/2503.12303", "abs": "https://arxiv.org/abs/2503.12303", "authors": ["Xiaoying Zhang", "Da Peng", "Yipeng Zhang", "Zonghao Guo", "Chengyue Wu", "Chi Chen", "Wei Ke", "Helen Meng", "Maosong Sun"], "title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs", "categories": ["cs.CV"], "comment": "38 pages", "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent pre-training approaches focus on enhancing perception by training on\nhigh-quality image captions due to the extremely high cost of collecting\nchain-of-thought (CoT) reasoning data for improving reasoning. While leveraging\nadvanced MLLMs for caption generation enhances scalability, the outputs often\nlack comprehensiveness and accuracy. In this paper, we introduce Self-Improving\nCognition (SIcog), a self-learning framework designed to construct\nnext-generation foundation MLLMs by enhancing their systematic cognitive\ncapabilities through multimodal pre-training with self-generated data.\nSpecifically, we propose chain-of-description, an approach that improves an\nMLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used to\nrefine the MLLM during multimodal pre-training, facilitating next-generation\nfoundation MLLM construction. Extensive experiments on both low- and\nhigh-resolution MLLMs across diverse benchmarks demonstrate that, with merely\n213K self-generated pre-training samples, SIcog produces next-generation\nfoundation MLLMs with significantly improved cognition, achieving\nbenchmark-leading performance compared to prevalent pre-training approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12303", "pdf": "https://arxiv.org/pdf/2503.12303", "abs": "https://arxiv.org/abs/2503.12303", "authors": ["Xiaoying Zhang", "Da Peng", "Yipeng Zhang", "Zonghao Guo", "Chengyue Wu", "Chi Chen", "Wei Ke", "Helen Meng", "Maosong Sun"], "title": "Towards Self-Improving Systematic Cognition for Next-Generation Foundation MLLMs", "categories": ["cs.CV"], "comment": "38 pages", "summary": "Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent pre-training approaches focus on enhancing perception by training on\nhigh-quality image captions due to the extremely high cost of collecting\nchain-of-thought (CoT) reasoning data for improving reasoning. While leveraging\nadvanced MLLMs for caption generation enhances scalability, the outputs often\nlack comprehensiveness and accuracy. In this paper, we introduce Self-Improving\nCognition (SIcog), a self-learning framework designed to construct\nnext-generation foundation MLLMs by enhancing their systematic cognitive\ncapabilities through multimodal pre-training with self-generated data.\nSpecifically, we propose chain-of-description, an approach that improves an\nMLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used to\nrefine the MLLM during multimodal pre-training, facilitating next-generation\nfoundation MLLM construction. Extensive experiments on both low- and\nhigh-resolution MLLMs across diverse benchmarks demonstrate that, with merely\n213K self-generated pre-training samples, SIcog produces next-generation\nfoundation MLLMs with significantly improved cognition, achieving\nbenchmark-leading performance compared to prevalent pre-training approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12797", "pdf": "https://arxiv.org/pdf/2503.12797", "abs": "https://arxiv.org/abs/2503.12797", "authors": ["Xinyu Ma", "Ziyang Ding", "Zhicong Luo", "Chi Chen", "Zonghao Guo", "Derek F. Wong", "Xiaoyi Feng", "Maosong Sun"], "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12799", "pdf": "https://arxiv.org/pdf/2503.12799", "abs": "https://arxiv.org/abs/2503.12799", "authors": ["Qiong Wu", "Xiangcong Yang", "Yiyi Zhou", "Chenxin Fang", "Baiyang Song", "Xiaoshuai Sun", "Rongrong Ji"], "title": "Grounded Chain-of-Thought for Multimodal Large Language Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12800", "pdf": "https://arxiv.org/pdf/2503.12800", "abs": "https://arxiv.org/abs/2503.12800", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Chunping Qiu", "Luoxi Jing", "Mengzhu Wang"], "title": "Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "With fully leveraging the value of unlabeled data, semi-supervised medical\nimage segmentation algorithms significantly reduces the limitation of limited\nlabeled data, achieving a significant improvement in accuracy. However, the\ndistributional shift between labeled and unlabeled data weakens the utilization\nof information from the labeled data. To alleviate the problem, we propose a\ngraph network feature alignment method based on pairwise similarity\nregularization (PaSR) for semi-supervised medical image segmentation. PaSR\naligns the graph structure of images in different domains by maintaining\nconsistency in the pairwise structural similarity of feature graphs between the\ntarget domain and the source domain, reducing distribution shift issues in\nmedical images. Meanwhile, further improving the accuracy of pseudo-labels in\nthe teacher network by aligning graph clustering information to enhance the\nsemi-supervised efficiency of the model. The experimental part was verified on\nthree medical image segmentation benchmark datasets, with results showing\nimprovements over advanced methods in various metrics. On the ACDC dataset, it\nachieved an average improvement of more than 10.66%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12852", "pdf": "https://arxiv.org/pdf/2503.12852", "abs": "https://arxiv.org/abs/2503.12852", "authors": ["Aditi Tiwari", "Klara Nahrstedt"], "title": "ACT360: An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing", "categories": ["cs.CV", "cs.MM"], "comment": "9 pages, 8 figures", "summary": "Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy", "summarization"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12974", "pdf": "https://arxiv.org/pdf/2503.12974", "abs": "https://arxiv.org/abs/2503.12974", "authors": ["Xueying Jiang", "Wenhao Li", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "Exploring 3D Activity Reasoning and Planning: From Implicit Human Intentions to Route-Aware Planning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12698", "pdf": "https://arxiv.org/pdf/2503.12698", "abs": "https://arxiv.org/abs/2503.12698", "authors": ["Dazhou Guo", "Zhanghexuan Ji", "Yanzhou Su", "Dandan Zheng", "Heng Guo", "Puyang Wang", "Ke Yan", "Yirui Wang", "Qinji Yu", "Zi Li", "Minfeng Xu", "Jianfeng Zhang", "Haoshen Li", "Jia Ge", "Tsung-Ying Ho", "Bing-Shen Huang", "Tashan Ai", "Kuaile Zhao", "Na Shen", "Qifeng Wang", "Yun Bian", "Tingyu Wu", "Peng Du", "Hua Zhang", "Feng-Ming Kong", "Alan L. Yuille", "Cher Heng Tan", "Chunyan Miao", "Perry J. Pickhardt", "Senxiang Yan", "Ronald M. Summers", "Le Lu", "Dakai Jin", "Xianghua Ye"], "title": "A Continual Learning-driven Model for Accurate and Generalizable Segmentation of Clinically Comprehensive and Fine-grained Whole-body Anatomies in CT", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Precision medicine in the quantitative management of chronic diseases and\noncology would be greatly improved if the Computed Tomography (CT) scan of any\npatient could be segmented, parsed and analyzed in a precise and detailed way.\nHowever, there is no such fully annotated CT dataset with all anatomies\ndelineated for training because of the exceptionally high manual cost, the need\nfor specialized clinical expertise, and the time required to finish the task.\nTo this end, we proposed a novel continual learning-driven CT model that can\nsegment complete anatomies presented using dozens of previously partially\nlabeled datasets, dynamically expanding its capacity to segment new ones\nwithout compromising previously learned organ knowledge. Existing multi-dataset\napproaches are not able to dynamically segment new anatomies without\ncatastrophic forgetting and would encounter optimization difficulty or\ninfeasibility when segmenting hundreds of anatomies across the whole range of\nbody regions. Our single unified CT segmentation model, CL-Net, can highly\naccurately segment a clinically comprehensive set of 235 fine-grained\nwhole-body anatomies. Composed of a universal encoder, multiple optimized and\npruned decoders, CL-Net is developed using 13,952 CT scans from 20 public and\n16 private high-quality partially labeled CT datasets of various vendors,\ndifferent contrast phases, and pathologies. Extensive evaluation demonstrates\nthat CL-Net consistently outperforms the upper limit of an ensemble of 36\nspecialist nnUNets trained per dataset with the complexity of 5% model size and\nsignificantly surpasses the segmentation accuracy of recent leading Segment\nAnything-style medical image foundation models by large margins. Our continual\nlearning-driven CL-Net model would lay a solid foundation to facilitate many\ndownstream tasks of oncology and chronic diseases using the most widely adopted\nCT imaging.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Incà", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11794", "pdf": "https://arxiv.org/pdf/2503.11794", "abs": "https://arxiv.org/abs/2503.11794", "authors": ["Bangzheng Li", "Fei Wang", "Wenxuan Zhou", "Nan Xu", "Ben Zhou", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11919", "pdf": "https://arxiv.org/pdf/2503.11919", "abs": "https://arxiv.org/abs/2503.11919", "authors": ["Jeonghwan Park", "Kang Li", "Huiyu Zhou"], "title": "k-fold Subsampling based Sequential Backward Feature Elimination", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12035", "pdf": "https://arxiv.org/pdf/2503.12035", "abs": "https://arxiv.org/abs/2503.12035", "authors": ["Zhengyuan Peng", "Jinpeng Ma", "Zhimin Sun", "Ran Yi", "Haichuan Song", "Xin Tan", "Lizhuang Ma"], "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is a classification task that aims to\nclassify both base and novel classes in unlabeled images, using knowledge from\na labeled dataset. In GCD, previous research overlooks scene information or\ntreats it as noise, reducing its impact during model training. However, in this\npaper, we argue that scene information should be viewed as a strong prior for\ninferring novel classes. We attribute the misinterpretation of scene\ninformation to a key factor: the Ambiguity Challenge inherent in GCD.\nSpecifically, novel objects in base scenes might be wrongly classified into\nbase categories, while base objects in novel scenes might be mistakenly\nrecognized as novel categories. Once the ambiguity challenge is addressed,\nscene information can reach its full potential, significantly enhancing the\nperformance of GCD models. To more effectively leverage scene information, we\npropose the Modeling Object-Scene Associations (MOS) framework, which utilizes\na simple MLP-based scene-awareness module to enhance GCD performance. It\nachieves an exceptional average accuracy improvement of 4% on the challenging\nfine-grained datasets compared to state-of-the-art methods, emphasizing its\nsuperior performance in fine-grained GCD. The code is publicly available at\nhttps://github.com/JethroPeng/MOS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12087", "pdf": "https://arxiv.org/pdf/2503.12087", "abs": "https://arxiv.org/abs/2503.12087", "authors": ["Gino E. Jansen", "Mark J. Schuuring", "Berto J. Bouma", "Ivana Išgum"], "title": "Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel approach to achieving temporally consistent mitral\nannulus landmark localization in echocardiography videos using sparse\nannotations. Our method introduces a self-supervised loss term that enforces\ntemporal consistency between neighboring frames, which smooths the position of\nlandmarks and enhances measurement accuracy over time. Additionally, we\nincorporate realistic field-of-view augmentations to improve the recognition of\nmissing anatomical landmarks. We evaluate our approach on both a public and\nprivate dataset, and demonstrate significant improvements in Mitral Annular\nPlane Systolic Excursion (MAPSE) calculations and overall landmark tracking\nstability. The method achieves a mean absolute MAPSE error of 1.81 $\\pm$ 0.14\nmm, an annulus size error of 2.46 $\\pm$ 0.31 mm, and a landmark localization\nerror of 2.48 $\\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for\nrecognition of missing landmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12191", "pdf": "https://arxiv.org/pdf/2503.12191", "abs": "https://arxiv.org/abs/2503.12191", "authors": ["Ying Zang", "Yuncan Gao", "Jiangi Zhang", "Yuangi Hu", "Runlong Cao", "Lanyun Zhu", "Qi Zhu", "Deyi Ji", "Renjun Xu", "Tianrun Chen"], "title": "Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches", "categories": ["cs.CV"], "comment": null, "summary": "This work advances zero-shot interactive segmentation for remote sensing\nimagery through three key contributions. First, we propose a novel sketch-based\nprompting method, enabling users to intuitively outline objects, surpassing\ntraditional point or box prompts. Second, we introduce LTL-Sensing, the first\ndataset pairing human sketches with remote sensing imagery, setting a benchmark\nfor future research. Third, we present LTL-Net, a model featuring a multi-input\nprompting transport module tailored for freehand sketches. Extensive\nexperiments show our approach significantly improves segmentation accuracy and\nrobustness over state-of-the-art methods like SAM, fostering more intuitive\nhuman-AI collaboration in remote sensing analysis and enhancing its\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12267", "pdf": "https://arxiv.org/pdf/2503.12267", "abs": "https://arxiv.org/abs/2503.12267", "authors": ["Aziz Amari", "Mariem Makni", "Wissal Fnaich", "Akram Lahmar", "Fedi Koubaa", "Oumayma Charrad", "Mohamed Ali Zormati", "Rabaa Youssef Douss"], "title": "An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation", "categories": ["cs.CV"], "comment": null, "summary": "In large organizations, the number of financial transactions can grow\nrapidly, driving the need for fast and accurate multi-criteria invoice\nvalidation. Manual processing remains error-prone and time-consuming, while\ncurrent automated solutions are limited by their inability to support a variety\nof constraints, such as documents that are partially handwritten or\nphotographed with a mobile phone. In this paper, we propose to automate the\nvalidation of machine written invoices using document layout analysis and\nobject detection techniques based on recent deep learning (DL) models. We\nintroduce a novel dataset consisting of manually annotated real-world invoices\nand a multi-criteria validation process. We fine-tune and benchmark the most\nrelevant DL models on our dataset. Experimental results show the effectiveness\nof the proposed pipeline and selected DL models in terms of achieving fast and\naccurate validation of invoices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "criteria"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12418", "pdf": "https://arxiv.org/pdf/2503.12418", "abs": "https://arxiv.org/abs/2503.12418", "authors": ["Shuo Gao", "Jingyang Zhang", "Jun Xue", "Meng Yang", "Yang Chen", "Guangquan Zhou"], "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, conference", "summary": "Carotid atherosclerosis represents a significant health risk, with its early\ndiagnosis primarily dependent on ultrasound-based assessments of carotid\nintima-media thickening. However, during carotid ultrasound screening,\nsignificant view variations cause style shifts, impairing content cues related\nto thickening, such as lumen anatomy, which introduces spurious correlations\nthat hinder assessment. Therefore, we propose a novel causal-inspired method\nfor assessing carotid intima-media thickening in frame-wise ultrasound videos,\nwhich focuses on two aspects: eliminating spurious correlations caused by style\nand enhancing causal content correlations. Specifically, we introduce a novel\nSpurious Correlation Elimination (SCE) module to remove non-causal style\neffects by enforcing prediction invariance with style perturbations.\nSimultaneously, we propose a Causal Equivalence Consolidation (CEC) module to\nstrengthen causal content correlation through adversarial optimization during\ncontent randomization. Simultaneously, we design a Causal Transition\nAugmentation (CTA) module to ensure smooth causal flow by integrating an\nauxiliary pathway with text prompts and connecting it through contrastive\nlearning. The experimental results on our in-house carotid ultrasound video\ndataset achieved an accuracy of 86.93\\%, demonstrating the superior performance\nof the proposed method. Code is available at\n\\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12470", "pdf": "https://arxiv.org/pdf/2503.12470", "abs": "https://arxiv.org/abs/2503.12470", "authors": ["Han Mei", "Kunqian Li", "Shuaixin Liu", "Chengzhi Ma", "Qianli Jiang"], "title": "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https://github.com/OUCVisionGroup/DPF-Net.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12531", "pdf": "https://arxiv.org/pdf/2503.12531", "abs": "https://arxiv.org/abs/2503.12531", "authors": ["Mehmet Kerem Turkcan", "Mattia Ballo", "Filippo Filicori", "Zoran Kostic"], "title": "Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks", "categories": ["cs.CV"], "comment": null, "summary": "We introduce specialized diffusion-based generative models that capture the\nspatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions\nthrough supervised learning on annotated laparoscopic surgery footage. The\nproposed models form a foundation for data-driven world models capable of\nsimulating the biomechanical interactions and procedural dynamics of surgical\nsuturing with high temporal fidelity. Annotating a dataset of $\\sim2K$ clips\nextracted from simulation videos, we categorize surgical actions into\nfine-grained sub-stitch classes including ideal and non-ideal executions of\nneedle positioning, targeting, driving, and withdrawal. We fine-tune two\nstate-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to\ngenerate high-fidelity surgical action sequences at $\\ge$768x512 resolution and\n$\\ge$49 frames. For training our models, we explore both Low-Rank Adaptation\n(LoRA) and full-model fine-tuning approaches. Our experimental results\ndemonstrate that these world models can effectively capture the dynamics of\nsuturing, potentially enabling improved training simulators, surgical skill\nassessment tools, and autonomous surgical systems. The models also display the\ncapability to differentiate between ideal and non-ideal technique execution,\nproviding a foundation for building surgical training and evaluation systems.\nWe release our models for testing and as a foundation for future research.\nProject Page: https://mkturkcan.github.io/suturingmodels/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12545", "pdf": "https://arxiv.org/pdf/2503.12545", "abs": "https://arxiv.org/abs/2503.12545", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Xiaojiang Peng", "Kai Wang", "Yang You", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11742", "pdf": "https://arxiv.org/pdf/2503.11742", "abs": "https://arxiv.org/abs/2503.11742", "authors": ["Moreno D'Incà", "Elia Peruzzo", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe", "Massimiliano Mancini"], "title": "Safe Vision-Language Models via Unsafe Weights Manipulation", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Vision-language models (VLMs) often inherit the biases and unsafe\nassociations present within their large-scale training dataset. While recent\napproaches mitigate unsafe behaviors, their evaluation focuses on how safe the\nmodel is on unsafe inputs, ignoring potential shortcomings on safe ones. In\nthis paper, we first revise safety evaluation by introducing SafeGround, a new\nset of metrics that evaluate safety at different levels of granularity. With\nthis metric, we uncover a surprising issue of training-based methods: they make\nthe model less safe on safe inputs. From this finding, we take a different\ndirection and explore whether it is possible to make a model safer without\ntraining, introducing Unsafe Weights Manipulation (UWM). UWM uses a calibration\nset of safe and unsafe instances to compare activations between safe and unsafe\ncontent, identifying the most important parameters for processing the latter.\nTheir values are then manipulated via negation. Experiments show that UWM\nachieves the best tradeoff between safety and knowledge preservation,\nconsistently improving VLMs on unsafe queries while outperforming even\ntraining-based state-of-the-art methods on safe ones.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11794", "pdf": "https://arxiv.org/pdf/2503.11794", "abs": "https://arxiv.org/abs/2503.11794", "authors": ["Bangzheng Li", "Fei Wang", "Wenxuan Zhou", "Nan Xu", "Ben Zhou", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "Semantic-Clipping: Efficient Vision-Language Modeling with Semantic-Guidedd Visual Selection", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) leverage aligned visual encoders to transform\nimages into visual tokens, allowing them to be processed similarly to text by\nthe backbone large language model (LLM). This unified input paradigm enables\nVLMs to excel in vision-language tasks such as visual question answering (VQA).\nTo improve fine-grained visual reasoning, recent advancements in\nvision-language modeling introduce image cropping techniques that feed all\nencoded sub-images into the model. However, this approach significantly\nincreases the number of visual tokens, leading to inefficiency and potential\ndistractions for the LLM. To address the generalization challenges of image\nrepresentation in VLMs, we propose a lightweight, universal framework that\nseamlessly integrates with existing VLMs to enhance their ability to process\nfinegrained details. Our method leverages textual semantics to identify key\nvisual areas, improving VQA performance without requiring any retraining of the\nVLM. Additionally, it incorporates textual signals into the visual encoding\nprocess, enhancing both efficiency and effectiveness. The proposed method,\nSEMCLIP, strengthens the visual understanding of a 7B VLM, LLaVA-1.5 by 3.3% on\naverage across 7 benchmarks, and particularly by 5.3% on the challenging\ndetailed understanding benchmark V*.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11849", "pdf": "https://arxiv.org/pdf/2503.11849", "abs": "https://arxiv.org/abs/2503.11849", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "categories": ["cs.CV"], "comment": "31 pages, 32 figures", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11919", "pdf": "https://arxiv.org/pdf/2503.11919", "abs": "https://arxiv.org/abs/2503.11919", "authors": ["Jeonghwan Park", "Kang Li", "Huiyu Zhou"], "title": "k-fold Subsampling based Sequential Backward Feature Elimination", "categories": ["cs.CV"], "comment": "8 pages", "summary": "We present a new wrapper feature selection algorithm for human detection.\nThis algorithm is a hybrid feature selection approach combining the benefits of\nfilter and wrapper methods. It allows the selection of an optimal feature\nvector that well represents the shapes of the subjects in the images. In\ndetail, the proposed feature selection algorithm adopts the k-fold subsampling\nand sequential backward elimination approach, while the standard linear support\nvector machine (SVM) is used as the classifier for human detection. We apply\nthe proposed algorithm to the publicly accessible INRIA and ETH pedestrian full\nimage datasets with the PASCAL VOC evaluation criteria. Compared to other state\nof the arts algorithms, our feature selection based approach can improve the\ndetection speed of the SVM classifier by over 50% with up to 2% better\ndetection accuracy. Our algorithm also outperforms the equivalent systems\nintroduced in the deformable part model approach with around 9% improvement in\nthe detection accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12035", "pdf": "https://arxiv.org/pdf/2503.12035", "abs": "https://arxiv.org/abs/2503.12035", "authors": ["Zhengyuan Peng", "Jinpeng Ma", "Zhimin Sun", "Ran Yi", "Haichuan Song", "Xin Tan", "Lizhuang Ma"], "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) is a classification task that aims to\nclassify both base and novel classes in unlabeled images, using knowledge from\na labeled dataset. In GCD, previous research overlooks scene information or\ntreats it as noise, reducing its impact during model training. However, in this\npaper, we argue that scene information should be viewed as a strong prior for\ninferring novel classes. We attribute the misinterpretation of scene\ninformation to a key factor: the Ambiguity Challenge inherent in GCD.\nSpecifically, novel objects in base scenes might be wrongly classified into\nbase categories, while base objects in novel scenes might be mistakenly\nrecognized as novel categories. Once the ambiguity challenge is addressed,\nscene information can reach its full potential, significantly enhancing the\nperformance of GCD models. To more effectively leverage scene information, we\npropose the Modeling Object-Scene Associations (MOS) framework, which utilizes\na simple MLP-based scene-awareness module to enhance GCD performance. It\nachieves an exceptional average accuracy improvement of 4% on the challenging\nfine-grained datasets compared to state-of-the-art methods, emphasizing its\nsuperior performance in fine-grained GCD. The code is publicly available at\nhttps://github.com/JethroPeng/MOS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12087", "pdf": "https://arxiv.org/pdf/2503.12087", "abs": "https://arxiv.org/abs/2503.12087", "authors": ["Gino E. Jansen", "Mark J. Schuuring", "Berto J. Bouma", "Ivana Išgum"], "title": "Temporally Consistent Mitral Annulus Measurements from Sparse Annotations in Echocardiographic Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a novel approach to achieving temporally consistent mitral\nannulus landmark localization in echocardiography videos using sparse\nannotations. Our method introduces a self-supervised loss term that enforces\ntemporal consistency between neighboring frames, which smooths the position of\nlandmarks and enhances measurement accuracy over time. Additionally, we\nincorporate realistic field-of-view augmentations to improve the recognition of\nmissing anatomical landmarks. We evaluate our approach on both a public and\nprivate dataset, and demonstrate significant improvements in Mitral Annular\nPlane Systolic Excursion (MAPSE) calculations and overall landmark tracking\nstability. The method achieves a mean absolute MAPSE error of 1.81 $\\pm$ 0.14\nmm, an annulus size error of 2.46 $\\pm$ 0.31 mm, and a landmark localization\nerror of 2.48 $\\pm$ 0.07 mm. Finally, it achieves a 0.99 ROC-AUC for\nrecognition of missing landmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12096", "pdf": "https://arxiv.org/pdf/2503.12096", "abs": "https://arxiv.org/abs/2503.12096", "authors": ["Ashshak Sharifdeen", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Salman Khan", "Muhammad Haris Khan"], "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Test-time prompt tuning for vision-language models (VLMs) is getting\nattention because of their ability to learn with unlabeled data without\nfine-tuning. Although test-time prompt tuning methods for VLMs can boost\naccuracy, the resulting models tend to demonstrate poor calibration, which\ncasts doubts on the reliability and trustworthiness of these models. Notably,\nmore attention needs to be devoted to calibrating the test-time prompt tuning\nin vision-language models. To this end, we propose a new approach, called O-TPT\nthat introduces orthogonality constraints on the textual features corresponding\nto the learnable prompts for calibrating test-time prompt tuning in VLMs.\nTowards introducing orthogonality constraints, we make the following\ncontributions. First, we uncover new insights behind the suboptimal calibration\nperformance of existing methods relying on textual feature dispersion. Second,\nwe show that imposing a simple orthogonalization of textual features is a more\neffective approach towards obtaining textual dispersion. We conduct extensive\nexperiments on various datasets with different backbones and baselines. The\nresults indicate that our method consistently outperforms the prior state of\nthe art in significantly reducing the overall average calibration error. Also,\nour method surpasses the zero-shot calibration performance on fine-grained\nclassification tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12191", "pdf": "https://arxiv.org/pdf/2503.12191", "abs": "https://arxiv.org/abs/2503.12191", "authors": ["Ying Zang", "Yuncan Gao", "Jiangi Zhang", "Yuangi Hu", "Runlong Cao", "Lanyun Zhu", "Qi Zhu", "Deyi Ji", "Renjun Xu", "Tianrun Chen"], "title": "Breaking the Box: Enhancing Remote Sensing Image Segmentation with Freehand Sketches", "categories": ["cs.CV"], "comment": null, "summary": "This work advances zero-shot interactive segmentation for remote sensing\nimagery through three key contributions. First, we propose a novel sketch-based\nprompting method, enabling users to intuitively outline objects, surpassing\ntraditional point or box prompts. Second, we introduce LTL-Sensing, the first\ndataset pairing human sketches with remote sensing imagery, setting a benchmark\nfor future research. Third, we present LTL-Net, a model featuring a multi-input\nprompting transport module tailored for freehand sketches. Extensive\nexperiments show our approach significantly improves segmentation accuracy and\nrobustness over state-of-the-art methods like SAM, fostering more intuitive\nhuman-AI collaboration in remote sensing analysis and enhancing its\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12267", "pdf": "https://arxiv.org/pdf/2503.12267", "abs": "https://arxiv.org/abs/2503.12267", "authors": ["Aziz Amari", "Mariem Makni", "Wissal Fnaich", "Akram Lahmar", "Fedi Koubaa", "Oumayma Charrad", "Mohamed Ali Zormati", "Rabaa Youssef Douss"], "title": "An Efficient Deep Learning-Based Approach to Automating Invoice Document Validation", "categories": ["cs.CV"], "comment": null, "summary": "In large organizations, the number of financial transactions can grow\nrapidly, driving the need for fast and accurate multi-criteria invoice\nvalidation. Manual processing remains error-prone and time-consuming, while\ncurrent automated solutions are limited by their inability to support a variety\nof constraints, such as documents that are partially handwritten or\nphotographed with a mobile phone. In this paper, we propose to automate the\nvalidation of machine written invoices using document layout analysis and\nobject detection techniques based on recent deep learning (DL) models. We\nintroduce a novel dataset consisting of manually annotated real-world invoices\nand a multi-criteria validation process. We fine-tune and benchmark the most\nrelevant DL models on our dataset. Experimental results show the effectiveness\nof the proposed pipeline and selected DL models in terms of achieving fast and\naccurate validation of invoices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "criteria"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12385", "pdf": "https://arxiv.org/pdf/2503.12385", "abs": "https://arxiv.org/abs/2503.12385", "authors": ["Yutao Hu", "Sen Li", "Jincheng Yan", "Wenqi Shao", "Xiaoyan Luo"], "title": "Car-1000: A New Large Scale Fine-Grained Visual Categorization Dataset", "categories": ["cs.CV"], "comment": "accepted to The Eleventh Workshop on Fine-Grained Visual\n  Categorization in CVPR 2024", "summary": "Fine-grained visual categorization (FGVC) is a challenging but significant\ntask in computer vision, which aims to recognize different sub-categories of\nbirds, cars, airplanes, etc. Among them, recognizing models of different cars\nhas significant application value in autonomous driving, traffic surveillance\nand scene understanding, which has received considerable attention in the past\nfew years. However, Stanford-Car, the most widely used fine-grained dataset for\ncar recognition, only has 196 different categories and only includes vehicle\nmodels produced earlier than 2013. Due to the rapid advancements in the\nautomotive industry during recent years, the appearances of various car models\nhave become increasingly intricate and sophisticated. Consequently, the\nprevious Stanford-Car dataset fails to capture this evolving landscape and\ncannot satisfy the requirements of automotive industry. To address these\nchallenges, in our paper, we introduce Car-1000, a large-scale dataset designed\nspecifically for fine-grained visual categorization of diverse car models.\nCar-1000 encompasses vehicles from 165 different automakers, spanning a wide\nrange of 1000 distinct car models. Additionally, we have reproduced several\nstate-of-the-art FGVC methods on the Car-1000 dataset, establishing a new\nbenchmark for research in this field. We hope that our work will offer a fresh\nperspective for future FGVC researchers. Our dataset is available at\nhttps://github.com/toggle1995/Car-1000.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12418", "pdf": "https://arxiv.org/pdf/2503.12418", "abs": "https://arxiv.org/abs/2503.12418", "authors": ["Shuo Gao", "Jingyang Zhang", "Jun Xue", "Meng Yang", "Yang Chen", "Guangquan Zhou"], "title": "A Causality-Inspired Model for Intima-Media Thickening Assessment in Ultrasound Videos", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, conference", "summary": "Carotid atherosclerosis represents a significant health risk, with its early\ndiagnosis primarily dependent on ultrasound-based assessments of carotid\nintima-media thickening. However, during carotid ultrasound screening,\nsignificant view variations cause style shifts, impairing content cues related\nto thickening, such as lumen anatomy, which introduces spurious correlations\nthat hinder assessment. Therefore, we propose a novel causal-inspired method\nfor assessing carotid intima-media thickening in frame-wise ultrasound videos,\nwhich focuses on two aspects: eliminating spurious correlations caused by style\nand enhancing causal content correlations. Specifically, we introduce a novel\nSpurious Correlation Elimination (SCE) module to remove non-causal style\neffects by enforcing prediction invariance with style perturbations.\nSimultaneously, we propose a Causal Equivalence Consolidation (CEC) module to\nstrengthen causal content correlation through adversarial optimization during\ncontent randomization. Simultaneously, we design a Causal Transition\nAugmentation (CTA) module to ensure smooth causal flow by integrating an\nauxiliary pathway with text prompts and connecting it through contrastive\nlearning. The experimental results on our in-house carotid ultrasound video\ndataset achieved an accuracy of 86.93\\%, demonstrating the superior performance\nof the proposed method. Code is available at\n\\href{https://github.com/xielaobanyy/causal-imt}{https://github.com/xielaobanyy/causal-imt}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12470", "pdf": "https://arxiv.org/pdf/2503.12470", "abs": "https://arxiv.org/abs/2503.12470", "authors": ["Han Mei", "Kunqian Li", "Shuaixin Liu", "Chengzhi Ma", "Qianli Jiang"], "title": "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https://github.com/OUCVisionGroup/DPF-Net.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12531", "pdf": "https://arxiv.org/pdf/2503.12531", "abs": "https://arxiv.org/abs/2503.12531", "authors": ["Mehmet Kerem Turkcan", "Mattia Ballo", "Filippo Filicori", "Zoran Kostic"], "title": "Towards Suturing World Models: Learning Predictive Models for Robotic Surgical Tasks", "categories": ["cs.CV"], "comment": null, "summary": "We introduce specialized diffusion-based generative models that capture the\nspatiotemporal dynamics of fine-grained robotic surgical sub-stitch actions\nthrough supervised learning on annotated laparoscopic surgery footage. The\nproposed models form a foundation for data-driven world models capable of\nsimulating the biomechanical interactions and procedural dynamics of surgical\nsuturing with high temporal fidelity. Annotating a dataset of $\\sim2K$ clips\nextracted from simulation videos, we categorize surgical actions into\nfine-grained sub-stitch classes including ideal and non-ideal executions of\nneedle positioning, targeting, driving, and withdrawal. We fine-tune two\nstate-of-the-art video diffusion models, LTX-Video and HunyuanVideo, to\ngenerate high-fidelity surgical action sequences at $\\ge$768x512 resolution and\n$\\ge$49 frames. For training our models, we explore both Low-Rank Adaptation\n(LoRA) and full-model fine-tuning approaches. Our experimental results\ndemonstrate that these world models can effectively capture the dynamics of\nsuturing, potentially enabling improved training simulators, surgical skill\nassessment tools, and autonomous surgical systems. The models also display the\ncapability to differentiate between ideal and non-ideal technique execution,\nproviding a foundation for building surgical training and evaluation systems.\nWe release our models for testing and as a foundation for future research.\nProject Page: https://mkturkcan.github.io/suturingmodels/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12545", "pdf": "https://arxiv.org/pdf/2503.12545", "abs": "https://arxiv.org/abs/2503.12545", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Weidong Tang", "Jiaxin Ai", "Wangbo Zhao", "Xiaojiang Peng", "Kai Wang", "Yang You", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "PEBench: A Fictitious Dataset to Benchmark Machine Unlearning for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable advancements in tasks such as visual question answering, visual\nunderstanding, and reasoning. However, this impressive progress relies on vast\namounts of data collected from the internet, raising significant concerns about\nprivacy and security. To address these issues, machine unlearning (MU) has\nemerged as a promising solution, enabling the removal of specific knowledge\nfrom an already trained model without requiring retraining from scratch.\nAlthough MU for MLLMs has gained attention, current evaluations of its efficacy\nremain incomplete, and the underlying problem is often poorly defined, which\nhinders the development of strategies for creating more secure and trustworthy\nsystems. To bridge this gap, we introduce a benchmark, named PEBench, which\nincludes a dataset of personal entities and corresponding general event scenes,\ndesigned to comprehensively assess the performance of MU for MLLMs. Through\nPEBench, we aim to provide a standardized and robust framework to advance\nresearch in secure and privacy-preserving multimodal models. We benchmarked 6\nMU methods, revealing their strengths and limitations, and shedding light on\nkey challenges and opportunities for MU in MLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12706", "pdf": "https://arxiv.org/pdf/2503.12706", "abs": "https://arxiv.org/abs/2503.12706", "authors": ["Rahul Deshmukh", "Avinash Kak"], "title": "SatDepth: A Novel Dataset for Satellite Image Matching", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep-learning based methods for image matching have\ndemonstrated their superiority over traditional algorithms, enabling\ncorrespondence estimation in challenging scenes with significant differences in\nviewing angles, illumination and weather conditions. However, the existing\ndatasets, learning frameworks, and evaluation metrics for the deep-learning\nbased methods are limited to ground-based images recorded with pinhole cameras\nand have not been explored for satellite images. In this paper, we present\n``SatDepth'', a novel dataset that provides dense ground-truth correspondences\nfor training image matching frameworks meant specifically for satellite images.\nSatellites capture images from various viewing angles and tracks through\nmultiple revisits over a region. To manage this variability, we propose a\ndataset balancing strategy through a novel image rotation augmentation\nprocedure. This procedure allows for the discovery of corresponding pixels even\nin the presence of large rotational differences between the images. We\nbenchmark four existing image matching frameworks using our dataset and carry\nout an ablation study that confirms that the models trained with our dataset\nwith rotation augmentation outperform (up to 40% increase in precision) the\nmodels trained with other datasets, especially when there exist large\nrotational differences between the images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12732", "pdf": "https://arxiv.org/pdf/2503.12732", "abs": "https://arxiv.org/abs/2503.12732", "authors": ["Zibin Liu", "Banglei Guan", "Yang Shang", "Yifei Bian", "Pengju Sun", "Qifeng Yu"], "title": "Stereo Event-based, 6-DOF Pose Tracking for Uncooperative Spacecraft", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Geoscience and Remote Sensing", "summary": "Pose tracking of uncooperative spacecraft is an essential technology for\nspace exploration and on-orbit servicing, which remains an open problem. Event\ncameras possess numerous advantages, such as high dynamic range, high temporal\nresolution, and low power consumption. These attributes hold the promise of\novercoming challenges encountered by conventional cameras, including motion\nblur and extreme illumination, among others. To address the standard on-orbit\nobservation missions, we propose a line-based pose tracking method for\nuncooperative spacecraft utilizing a stereo event camera. To begin with, we\nestimate the wireframe model of uncooperative spacecraft, leveraging the\nspatio-temporal consistency of stereo event streams for line-based\nreconstruction. Then, we develop an effective strategy to establish\ncorrespondences between events and projected lines of uncooperative spacecraft.\nUsing these correspondences, we formulate the pose tracking as a continuous\noptimization process over 6-DOF motion parameters, achieved by minimizing\nevent-line distances. Moreover, we construct a stereo event-based uncooperative\nspacecraft motion dataset, encompassing both simulated and real events. The\nproposed method is quantitatively evaluated through experiments conducted on\nour self-collected dataset, demonstrating an improvement in terms of\neffectiveness and accuracy over competing methods. The code will be\nopen-sourced at https://github.com/Zibin6/SE6PT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12772", "pdf": "https://arxiv.org/pdf/2503.12772", "abs": "https://arxiv.org/abs/2503.12772", "authors": ["Sung-Yeon Park", "Can Cui", "Yunsheng Ma", "Ahmadreza Moradipari", "Rohit Gupta", "Kyungtae Han", "Ziran Wang"], "title": "NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have\ndemonstrated strong performance across various domains; however, their ability\nto comprehend driving scenes remains less proven. The complexity of driving\nscenarios, which includes multi-view information, poses significant challenges\nfor existing MLLMs. In this paper, we introduce NuPlanQA-Eval, a multi-view,\nmulti-modal evaluation benchmark for driving scene understanding. To further\nsupport generalization to multi-view driving scenarios, we also propose\nNuPlanQA-1M, a large-scale dataset comprising 1M real-world visual\nquestion-answering (VQA) pairs. For context-aware analysis of traffic scenes,\nwe categorize our dataset into nine subtasks across three core skills: Road\nEnvironment Perception, Spatial Relations Recognition, and Ego-Centric\nReasoning. Furthermore, we present BEV-LLM, integrating Bird's-Eye-View (BEV)\nfeatures from multi-view images into MLLMs. Our evaluation results reveal key\nchallenges that existing MLLMs face in driving scene-specific perception and\nspatial reasoning from ego-centric perspectives. In contrast, BEV-LLM\ndemonstrates remarkable adaptability to this domain, outperforming other models\nin six of the nine subtasks. These findings highlight how BEV integration\nenhances multi-view MLLMs while also identifying key areas that require further\nrefinement for effective adaptation to driving scenes. To facilitate further\nresearch, we publicly release NuPlanQA at\nhttps://github.com/sungyeonparkk/NuPlanQA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12844", "pdf": "https://arxiv.org/pdf/2503.12844", "abs": "https://arxiv.org/abs/2503.12844", "authors": ["Junhyeok Kim", "Jaewoo Park", "Junhee Park", "Sangeyl Lee", "Jiwan Chung", "Jisung Kim", "Ji Hoon Joung", "Youngjae Yu"], "title": "GuideDog: A Real-World Egocentric Multimodal Dataset for Blind and Low-Vision Accessibility-Aware Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Mobility remains a significant challenge for the 2.2 billion people worldwide\naffected by blindness and low vision (BLV), with 7% of visually impaired\nindividuals experiencing falls at least once a month. While recent advances in\nMultimodal Large Language Models (MLLMs) offer promising opportunities for BLV\nassistance, their development has been hindered by limited datasets. This\nlimitation stems from the fact that BLV-aware annotation requires specialized\ndomain knowledge and intensive labor. To address this gap, we introduce\nGuideDog, a novel accessibility-aware guide dataset containing 22K\nimage-description pairs (including 2K human-annotated pairs) that capture\ndiverse real-world scenes from a pedestrian's viewpoint. Our approach shifts\nthe annotation burden from generation to verification through a collaborative\nhuman-AI framework grounded in established accessibility standards,\nsignificantly improving efficiency while maintaining high-quality annotations.\nWe also develop GuideDogQA, a subset of 818 samples featuring multiple-choice\nquestions designed to evaluate fine-grained visual perception capabilities,\nspecifically object recognition and relative depth perception. Our experimental\nresults highlight the importance of accurate spatial understanding for\neffective BLV guidance. GuideDog and GuideDogQA will advance research in\nMLLM-based assistive technologies for BLV individuals while contributing to\nbroader applications in understanding egocentric scenes for robotics and\naugmented reality. The code and dataset will be publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "fine-grained"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12912", "pdf": "https://arxiv.org/pdf/2503.12912", "abs": "https://arxiv.org/abs/2503.12912", "authors": ["Bin Tang", "Keqi Pan", "Miao Zheng", "Ning Zhou", "Jialu Sui", "Dandan Zhu", "Cheng-Long Deng", "Shu-Guang Kuai"], "title": "Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 6 figures, AAAI 2025 Oral", "summary": "In recent years, predicting Big Five personality traits from multimodal data\nhas received significant attention in artificial intelligence (AI). However,\nexisting computational models often fail to achieve satisfactory performance.\nPsychological research has shown a strong correlation between pose and\npersonality traits, yet previous research has largely ignored pose data in\ncomputational models. To address this gap, we develop a novel multimodal\ndataset that incorporates full-body pose data. The dataset includes video\nrecordings of 287 participants completing a virtual interview with 36\nquestions, along with self-reported Big Five personality scores as labels. To\neffectively utilize this multimodal data, we introduce the Psychology-Inspired\nNetwork (PINet), which consists of three key modules: Multimodal Feature\nAwareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed\nModality Correlation Loss (PIMC Loss). The MFA module leverages the Vision\nMamba Block to capture comprehensive visual features related to personality,\nwhile the MFI module efficiently fuses the multimodal features. The PIMC Loss,\ngrounded in psychological theory, guides the model to emphasize different\nmodalities for different personality dimensions. Experimental results show that\nthe PINet outperforms several state-of-the-art baseline models. Furthermore,\nthe three modules of PINet contribute almost equally to the model's overall\nperformance. Incorporating pose data significantly enhances the model's\nperformance, with the pose modality ranking mid-level in importance among the\nfive modalities. These findings address the existing gap in personality-related\ndatasets that lack full-body pose data and provide a new approach for improving\nthe accuracy of personality prediction models, highlighting the importance of\nintegrating psychological insights into AI frameworks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12955", "pdf": "https://arxiv.org/pdf/2503.12955", "abs": "https://arxiv.org/abs/2503.12955", "authors": ["Jiahe Zhao", "Ruibing Hou", "Zejie Tian", "Hong Chang", "Shiguang Shan"], "title": "HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new task to benchmark human-in-scene understanding for embodied\nagents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within\na 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,\nreason about its surrounding environment, and answer human-related questions\nwithin the scene. To support this new task, we present HIS-Bench, a multimodal\nbenchmark that systematically evaluates HIS understanding across a broad\nspectrum, from basic perception to commonsense reasoning and planning. Our\nevaluation of various vision-language models on HIS-Bench reveals significant\nlimitations in their ability to handle HIS-QA tasks. To this end, we propose\nHIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates\n3D scene context and human motion dynamics into large language models while\nincorporating specialized mechanisms to capture human-scene interactions.\nExtensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on\nHIS-QA tasks. We hope this work inspires future research on human behavior\nanalysis in 3D scenes, advancing embodied AI and world models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13053", "pdf": "https://arxiv.org/pdf/2503.13053", "abs": "https://arxiv.org/abs/2503.13053", "authors": ["Nassim Ali Ousalah", "Anis Kacem", "Enjie Ghorbel", "Emmanuel Koumandakis", "Djamila Aouada"], "title": "Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13111", "pdf": "https://arxiv.org/pdf/2503.13111", "abs": "https://arxiv.org/abs/2503.13111", "authors": ["Erik Daxberger", "Nina Wenzel", "David Griffiths", "Haiming Gang", "Justin Lazarow", "Gefen Kohavi", "Kai Kang", "Marcin Eichner", "Yinfei Yang", "Afshin Dehghan", "Peter Grasch"], "title": "MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13125", "pdf": "https://arxiv.org/pdf/2503.13125", "abs": "https://arxiv.org/abs/2503.13125", "authors": ["Pan Liu"], "title": "Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser Chips Based On Consistent Texture Entropy Recursive Optimization Semi-Supervised Network", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Laser chips, the core components of semiconductor lasers, are extensively\nutilized in various industries, showing great potential for future application.\nSmoothness emitting surfaces are crucial in chip production, as even\nimperceptible scratches can significantly degrade performance and lifespan,\nthus impeding production efficiency and yield. Therefore, non-destructively\ndetecting these imperceptible scratches on the emitting surfaces is essential\nfor enhancing yield and reducing costs. These sub-micron level scratches,\nbarely visible against the background, are extremely difficult to detect with\nconventional methods, compounded by a lack of labeled datasets. To address this\nchallenge, this paper introduces TexRecNet, a consistent texture entropy\nrecursive optimization semi-supervised network. The network, based on a\nrecursive optimization architecture, iteratively improves the detection\naccuracy of imperceptible scratch edges, using outputs from previous cycles to\ninform subsequent inputs and guide the network's positional encoding. It also\nintroduces image texture entropy, utilizing a substantial amount of unlabeled\ndata to expand the training set while maintaining training signal reliability.\nUltimately, by analyzing the inconsistency of the network output sequences\nobtained during the recursive process, a semi-supervised training strategy with\nrecursive consistency constraints is proposed, using outputs from the recursive\nprocess for non-destructive signal augmentation and consistently optimizes the\nloss function for efficient end-to-end training. Experimental results show that\nthis method, utilizing a substantial amount of unsupervised data, achieves\n75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5%\nand 33.6% improvement over conventional Unet, enhancing quality control in\nlaser chips.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12505", "pdf": "https://arxiv.org/pdf/2503.12505", "abs": "https://arxiv.org/abs/2503.12505", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Jiaxin Ai", "Wangbo Zhao", "Kai Wang", "Xiaojiang Peng", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12623", "pdf": "https://arxiv.org/pdf/2503.12623", "abs": "https://arxiv.org/abs/2503.12623", "authors": ["Vrushank Ahire", "Kunal Shah", "Mudasir Nazir Khan", "Nikhil Pakhale", "Lownish Rai Sookha", "M. A. Ganaie", "Abhinav Dhall"], "title": "MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal\nEmotion Network), a novel architecture for dynamic emotion recognition through\ndimensional modeling of affect. The model uniquely integrates visual, audio,\nand textual modalities via a bi-directional cross-modal attention mechanism\nwith six distinct attention pathways, enabling comprehensive interactions\nbetween all modality pairs. Our proposed approach employs modality-specific\nencoders to extract rich feature representations from synchronized video\nframes, audio segments, and transcripts. The architecture's novelty lies in its\ncross-modal enhancement strategy, where each modality representation is refined\nthrough weighted attention from other modalities, followed by self-attention\nrefinement through modality-specific encoders. Rather than directly predicting\nvalence-arousal values, MAVEN predicts emotions in a polar coordinate form,\naligning with psychological models of the emotion circumplex. Experimental\nevaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our\napproach, with performance measured using Concordance Correlation Coefficient\n(CCC). The multi-stage architecture demonstrates superior ability to capture\nthe complex, nuanced nature of emotional expressions in conversational videos,\nadvancing the state-of-the-art (SOTA) in continuous emotion recognition\nin-the-wild. Code can be found at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12990", "pdf": "https://arxiv.org/pdf/2503.12990", "abs": "https://arxiv.org/abs/2503.12990", "authors": ["Roba Al Majzoub", "Hashmat Malik", "Muzammal Naseer", "Zaigham Zaheer", "Tariq Mahmood", "Salman Khan", "Fahad Khan"], "title": "How Good is my Histopathology Vision-Language Foundation Model? A Holistic Benchmark", "categories": ["eess.IV", "cs.CV", "I.4.0; J.3"], "comment": null, "summary": "Recently, histopathology vision-language foundation models (VLMs) have gained\npopularity due to their enhanced performance and generalizability across\ndifferent downstream tasks. However, most existing histopathology benchmarks\nare either unimodal or limited in terms of diversity of clinical tasks, organs,\nand acquisition instruments, as well as their partial availability to the\npublic due to patient data privacy. As a consequence, there is a lack of\ncomprehensive evaluation of existing histopathology VLMs on a unified benchmark\nsetting that better reflects a wide range of clinical scenarios. To address\nthis gap, we introduce HistoVL, a fully open-source comprehensive benchmark\ncomprising images acquired using up to 11 various acquisition tools that are\npaired with specifically crafted captions by incorporating class names and\ndiverse pathology descriptions. Our Histo-VL includes 26 organs, 31 cancer\ntypes, and a wide variety of tissue obtained from 14 heterogeneous patient\ncohorts, totaling more than 5 million patches obtained from over 41K WSIs\nviewed under various magnification levels. We systematically evaluate existing\nhistopathology VLMs on Histo-VL to simulate diverse tasks performed by experts\nin real-world clinical scenarios. Our analysis reveals interesting findings,\nincluding large sensitivity of most existing histopathology VLMs to textual\nchanges with a drop in balanced accuracy of up to 25% in tasks such as\nMetastasis detection, low robustness to adversarial attacks, as well as\nimproper calibration of models evident through high ECE values and low model\nprediction confidence, all of which can affect their clinical implementation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13205", "pdf": "https://arxiv.org/pdf/2503.13205", "abs": "https://arxiv.org/abs/2503.13205", "authors": ["Zhen Chen", "Zhihao Peng", "Xusheng Liang", "Cheng Wang", "Peigan Liang", "Linsheng Zeng", "Minjie Ju", "Yixuan Yuan"], "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MA"], "comment": null, "summary": "Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13400", "pdf": "https://arxiv.org/pdf/2503.13400", "abs": "https://arxiv.org/abs/2503.13400", "authors": ["Qi Zhang", "Xiuyuan Chen", "Ziyi He", "Kun Wang", "Lianming Wu", "Hongxing Shen", "Jianqi Sun"], "title": "U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for Detecting T2 Hyperintensity in MRI Spinal Cord", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "T2 hyperintensities in spinal cord MR images are crucial biomarkers for\nconditions such as degenerative cervical myelopathy. However, current clinical\ndiagnoses primarily rely on manual evaluation. Deep learning methods have shown\npromise in lesion detection, but most supervised approaches are heavily\ndependent on large, annotated datasets. Unsupervised anomaly detection (UAD)\noffers a compelling alternative by eliminating the need for abnormal data\nannotations. However, existing UAD methods rely on curated normal datasets and\ntheir performance frequently deteriorates when applied to clinical datasets due\nto domain shifts. We propose an Uncertainty-based Unsupervised Anomaly\nDetection framework, termed U2AD, to address these limitations. Unlike\ntraditional methods, U2AD is designed to be trained and tested within the same\nclinical dataset, following a \"mask-and-reconstruction\" paradigm built on a\nVision Transformer-based architecture. We introduce an uncertainty-guided\nmasking strategy to resolve task conflicts between normal reconstruction and\nanomaly detection to achieve an optimal balance. Specifically, we employ a\nMonte-Carlo sampling technique to estimate reconstruction uncertainty mappings\nduring training. By iteratively optimizing reconstruction training under the\nguidance of both epistemic and aleatoric uncertainty, U2AD reduces overall\nreconstruction variance while emphasizing regions. Experimental results\ndemonstrate that U2AD outperforms existing supervised and unsupervised methods\nin patient-level identification and segment-level localization tasks. This\nframework establishes a new benchmark for incorporating uncertainty guidance\ninto UAD, highlighting its clinical utility in addressing domain shifts and\ntask conflicts in medical image anomaly detection. Our code is available:\nhttps://github.com/zhibaishouheilab/U2AD", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11731", "pdf": "https://arxiv.org/pdf/2503.11731", "abs": "https://arxiv.org/abs/2503.11731", "authors": ["Xianming Zeng", "Sicong Du", "Qifeng Chen", "Lizhe Liu", "Haoyu Shu", "Jiaxuan Gao", "Jiarun Liu", "Jiulong Xu", "Jianyun Xu", "Mingxia Chen", "Yiru Zhao", "Peng Chen", "Yapeng Xue", "Chunming Zhao", "Sheng Yang", "Qiang Li"], "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor simulation is pivotal for scalable validation of autonomous driving\nsystems, yet existing Neural Radiance Fields (NeRF) based methods face\napplicability and efficiency challenges in industrial workflows. This paper\nintroduces a Gaussian Splatting (GS) based system to address these challenges:\nWe first break down sensor simulator components and analyze the possible\nadvantages of GS over NeRF. Then in practice, we refactor three crucial\ncomponents through GS, to leverage its explicit scene representation and\nreal-time rendering: (1) choosing the 2D neural Gaussian representation for\nphysics-compliant scene and sensor modeling, (2) proposing a scene editing\npipeline to leverage Gaussian primitives library for data augmentation, and (3)\ncoupling a controllable diffusion model for scene expansion and harmonization.\nWe implement this framework on a proprietary autonomous driving dataset\nsupporting cameras and LiDAR sensors. We demonstrate through ablation studies\nthat our approach reduces frame-wise simulation latency, achieves better\ngeometric and photometric consistency, and enables interpretable explicit scene\nediting and expansion. Furthermore, we showcase how integrating such a GS-based\nsensor simulator with traffic and dynamic simulators enables full-stack testing\nof end-to-end autonomy algorithms. Our work provides both algorithmic insights\nand practical validation, establishing GS as a cornerstone for industrial-grade\nsensor simulation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11893", "pdf": "https://arxiv.org/pdf/2503.11893", "abs": "https://arxiv.org/abs/2503.11893", "authors": ["Md Abu Bakr Siddique", "Junliang Liu", "Piyush Singh", "Md Jahidul Islam"], "title": "UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The concept of waterbody style transfer remains largely unexplored in the\nunderwater imaging and vision literature. Traditional image style transfer\n(STx) methods primarily focus on artistic and photorealistic blending, often\nfailing to preserve object and scene geometry in images captured in\nhigh-scattering mediums such as underwater. The wavelength-dependent nonlinear\nattenuation and depth-dependent backscattering artifacts further complicate\nlearning underwater image STx from unpaired data. This paper introduces UStyle,\nthe first data-driven learning framework for transferring waterbody styles\nacross underwater images without requiring prior reference images or scene\ninformation. We propose a novel depth-aware whitening and coloring transform\n(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure\nperceptually consistent stylization while preserving scene structure. To\nenhance style transfer quality, we incorporate carefully designed loss\nfunctions that guide UStyle to maintain colorfulness, lightness, structural\nintegrity, and frequency-domain characteristics, as well as high-level content\nin VGG and CLIP (contrastive language-image pretraining) feature spaces. By\naddressing domain-specific challenges, UStyle provides a robust framework for\nno-reference underwater image STx, surpassing state-of-the-art (SOTA) methods\nthat rely solely on end-to-end reconstruction loss. Furthermore, we introduce\nthe UF7D dataset, a curated collection of high-resolution underwater images\nspanning seven distinct waterbody styles, establishing a benchmark to support\nfuture research in underwater image STx. The UStyle inference pipeline and UF7D\ndataset are released at: https://github.com/uf-robopi/UStyle.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11932", "pdf": "https://arxiv.org/pdf/2503.11932", "abs": "https://arxiv.org/abs/2503.11932", "authors": ["Dhruv Kudale", "Badri Vishal Kasuba", "Venkatapathy Subramanian", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "title": "SPRINT: Script-agnostic Structure Recognition in Tables", "categories": ["cs.CV"], "comment": "Accepted at ICDAR 2024", "summary": "Table Structure Recognition (TSR) is vital for various downstream tasks like\ninformation retrieval, table reconstruction, and document understanding. While\nmost state-of-the-art (SOTA) research predominantly focuses on TSR in English\ndocuments, the need for similar capabilities in other languages is evident,\nconsidering the global diversity of data. Moreover, creating substantial\nlabeled data in non-English languages and training these SOTA models from\nscratch is costly and time-consuming. We propose TSR as a language-agnostic\ncell arrangement prediction and introduce SPRINT, Script-agnostic Structure\nRecognition in Tables. SPRINT uses recently introduced Optimized Table\nStructure Language (OTSL) sequences to predict table structures. We show that\nwhen coupled with a pre-trained table grid estimator, SPRINT can improve the\noverall tree edit distance-based similarity structure scores of tables even for\nnon-English documents. We experimentally evaluate our performance across\nbenchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our\nfindings reveal that SPRINT not only matches SOTA models in performance on\nstandard datasets but also demonstrates lower latency. Additionally, SPRINT\nexcels in accurately identifying table structures in non-English documents,\nsurpassing current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predictions into\na widely used HTML-based table representation. To encourage further research,\nwe release our code and Multilingual Scanned and Scene Table Structure\nRecognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in\nthirteen languages encompassing several scripts at\nhttps://github.com/IITB-LEAP-OCR/SPRINT", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12034", "pdf": "https://arxiv.org/pdf/2503.12034", "abs": "https://arxiv.org/abs/2503.12034", "authors": ["Enes Erdogan", "Eren Erdal Aksoy", "Sanem Sariel"], "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures, 7 tables", "summary": "Recognition of human manipulation actions in real-time is essential for safe\nand effective human-robot interaction and collaboration. The challenge lies in\ndeveloping a model that is both lightweight enough for real-time execution and\ncapable of generalization. While some existing methods in the literature can\nrun in real-time, they struggle with temporal scalability, i.e., they fail to\nadapt to long-duration manipulations effectively. To address this, leveraging\nthe generalizable scene graph representations, we propose a new Factorized\nGraph Sequence Encoder network that not only runs in real-time but also scales\neffectively in the temporal dimension, thanks to its factorized encoder\narchitecture. Additionally, we introduce Hand Pooling operation, a simple\npooling operation for more focused extraction of the graph-level embeddings.\nOur model outperforms the previous state-of-the-art real-time approach,\nachieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual\nAction (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.\nMoreover, we conduct an extensive ablation study to validate our network design\nchoices. Finally, we compare our model with its architecturally similar\nRGB-based model on the Bimacs dataset and show the limitations of this model in\ncontrast to ours on such an object-centric manipulation dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dimension"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12047", "pdf": "https://arxiv.org/pdf/2503.12047", "abs": "https://arxiv.org/abs/2503.12047", "authors": ["Hangrui Xu", "Chuanrui Zhang", "Zhengxian Wu", "Peng Jiao", "Haoqian Wang"], "title": "PSGait: Multimodal Gait Recognition using Parsing Skeleton", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the parsing skeleton\nrepresentation, we propose a novel parsing skeleton-based gait recognition\nframework, named PSGait, which takes parsing skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods. Furthermore,\nas a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in\nRank-1 accuracy across various gait recognition models. These results\ndemonstrate the effectiveness and versatility of parsing skeletons for gait\nrecognition in the wild, establishing PSGait as a new state-of-the-art approach\nfor multimodal gait recognition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12094", "pdf": "https://arxiv.org/pdf/2503.12094", "abs": "https://arxiv.org/abs/2503.12094", "authors": ["Weiming Zhang", "Dingwen Xiao", "Lei Chen", "Lin Wang"], "title": "E-SAM: Training-Free Segment Every Entity Model", "categories": ["cs.CV"], "comment": "Under review", "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities\nwithin an image without the need for predefined class labels. This\ncharacteristic makes ES well-suited to open-world applications with adaptation\nto diverse and dynamically changing environments, where new and previously\nunseen entities may appear frequently. Existing ES methods either require large\nannotated datasets or high training costs, limiting their scalability and\nadaptability. Recently, the Segment Anything Model (SAM), especially in its\nAutomatic Mask Generation (AMG) mode, has shown potential for holistic image\nsegmentation. However, it struggles with over-segmentation and\nunder-segmentation, making it less effective for ES. In this paper, we\nintroduce E-SAM, a novel training-free framework that exhibits exceptional ES\ncapability. Specifically, we first propose Multi-level Mask Generation (MMG)\nthat hierarchically processes SAM's AMG outputs to generate reliable\nobject-level masks while preserving fine details at other levels. Entity-level\nMask Refinement (EMR) then refines these object-level masks into accurate\nentity-level masks. That is, it separates overlapping masks to address the\nredundancy issues inherent in SAM's outputs and merges similar masks by\nevaluating entity-level consistency. Lastly, Under-Segmentation Refinement\n(USR) addresses under-segmentation by generating additional high-confidence\nmasks fused with EMR outputs to produce the final ES map. These three modules\nare seamlessly optimized to achieve the best ES without additional training\noverhead. Extensive experiments demonstrate that E-SAM achieves\nstate-of-the-art performance compared to prior ES methods, demonstrating a\nsignificant improvement by +30.1 on benchmark metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12095", "pdf": "https://arxiv.org/pdf/2503.12095", "abs": "https://arxiv.org/abs/2503.12095", "authors": ["Walter Zimmer", "Ross Greer", "Daniel Lehmberg", "Marc Pavel", "Holger Caesar", "Xingcheng Zhou", "Ahmed Ghita", "Mohan Trivedi", "Rui Song", "Hu Cao", "Akshay Gopalkrishnan", "Alois C. Knoll"], "title": "Towards Vision Zero: The Accid3nD Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Even though a significant amount of work has been done to increase the safety\nof transportation networks, accidents still occur regularly. They must be\nunderstood as unavoidable and sporadic outcomes of traffic networks. No public\ndataset contains 3D annotations of real-world accidents recorded from roadside\nsensors. We present the Accid3nD dataset, a collection of real-world highway\naccidents in different weather and lighting conditions. It contains vehicle\ncrashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,\ninstance masks, and 3D bounding boxes with track IDs. In total, the dataset\ncontains 111,945 labeled frames recorded from four roadside cameras and LiDARs\nat 25 Hz. The dataset contains six object classes and is provided in the\nOpenLABEL format. We propose an accident detection model that combines a\nrule-based approach with a learning-based one. Experiments and ablation studies\non our dataset show the robustness of our proposed method. The dataset, model,\nand code are available on our website: https://accident-dataset.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12206", "pdf": "https://arxiv.org/pdf/2503.12206", "abs": "https://arxiv.org/abs/2503.12206", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Muhammad Haris Khan", "Mohsen Ali"], "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. We evaluated our\nmodels on 11 base-to-novel datasets and they achieved superior accuracy on 9 of\nthese, including benchmarks like ImageNet, SUN397 and Caltech101, while\nmaintaining a strictly training-free paradigm. Our overall accuracy of 83.44%\nsurpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.\nOur method achieved 83.6% average accuracy across 13 datasets, a 9.7%\nimprovement over the previous 73.9% state-of-the-art for training-free\napproaches. Our method improves domain generalization, with a 3.6% gain on\nImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12213", "pdf": "https://arxiv.org/pdf/2503.12213", "abs": "https://arxiv.org/abs/2503.12213", "authors": ["Ruyu Wang", "Xuefeng Hou", "Sabrina Schmedding", "Marco F. Huber"], "title": "STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted by WACV2025", "summary": "In layout-to-image (L2I) synthesis, controlled complex scenes are generated\nfrom coarse information like bounding boxes. Such a task is exciting to many\ndownstream applications because the input layouts offer strong guidance to the\ngeneration process while remaining easily reconfigurable by humans. In this\npaper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based\nmodel that produces photo-realistic images and provides fine-grained control of\nstylized objects in scenes. Our approach learns a global condition for each\nlayout, and a self-supervised semantic map for weight modulation using a novel\nEdge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)\nis also introduced to cross-condition the global condition and image feature\nfor capturing the objects' relationships. These measures provide consistent\nguidance through the model, enabling more accurate and controllable image\ngeneration. Extensive benchmarking demonstrates that our STAY Diffusion\npresents high-quality images while surpassing previous state-of-the-art methods\nin generation diversity, accuracy, and controllability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12230", "pdf": "https://arxiv.org/pdf/2503.12230", "abs": "https://arxiv.org/abs/2503.12230", "authors": ["Yihao Wang", "Raphael Memmesheimer", "Sven Behnke"], "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12350", "pdf": "https://arxiv.org/pdf/2503.12350", "abs": "https://arxiv.org/abs/2503.12350", "authors": ["Wenqing Kuang", "Xiongwei Zhao", "Yehui Shen", "Congcong Wen", "Huimin Lu", "Zongtan Zhou", "Xieyuanli Chen"], "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12451", "pdf": "https://arxiv.org/pdf/2503.12451", "abs": "https://arxiv.org/abs/2503.12451", "authors": ["Hossein Ranjbar", "Alireza Taheri"], "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12495", "pdf": "https://arxiv.org/pdf/2503.12495", "abs": "https://arxiv.org/abs/2503.12495", "authors": ["Xuan Ma", "Zewen Lv", "Chengcai Ma", "Tao Zhang", "Yuelan Xin", "Kun Zhan"], "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau", "categories": ["cs.CV"], "comment": "Journal of Applied Remote Sensing, 2025", "summary": "Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a\nsignificant environmental challenge due to overgrazing, climate change, and\nrodent activity, which degrade vegetation cover and soil quality. These\nextremely degraded grassland on QTP, commonly referred to as black-soil area,\nrequire accurate assessment to guide effective restoration efforts. In this\npaper, we present a newly created QTP black-soil dataset, annotated under\nexpert guidance. We introduce a novel neural network model, BS-Mamba,\nspecifically designed for the black-soil area detection using UAV remote\nsensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying\nblack-soil area across two independent test datasets than the state-of-the-art\nmodels. This research contributes to grassland restoration by providing an\nefficient method for assessing the extent of black-soil area on the QTP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12515", "pdf": "https://arxiv.org/pdf/2503.12515", "abs": "https://arxiv.org/abs/2503.12515", "authors": ["Pan Du", "Delin An", "Chaoli Wang", "Jian-Xun Wang"], "title": "AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "comment": "42 pages, 8 figures", "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12526", "pdf": "https://arxiv.org/pdf/2503.12526", "abs": "https://arxiv.org/abs/2503.12526", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditID, a training-free approach based on the DiT architecture,\nwhich achieves highly editable customized IDs for text to image generation.\nExisting text-to-image models for customized IDs typically focus more on ID\nconsistency while neglecting editability. It is challenging to alter facial\norientation, character attributes, and other features through prompts. EditID\naddresses this by deconstructing the text-to-image model for customized IDs\ninto an image generation branch and a character feature branch. The character\nfeature branch is further decoupled into three modules: feature extraction,\nfeature fusion, and feature integration. By introducing a combination of\nmapping features and shift features, along with controlling the intensity of ID\nfeature integration, EditID achieves semantic compression of local features\nacross network depths, forming an editable feature space. This enables the\nsuccessful generation of high-quality images with editable IDs while\nmaintaining ID consistency, achieving excellent results in the IBench\nevaluation, which is an editability evaluation framework for the field of\ncustomized ID text-to-image generation that quantitatively demonstrates the\nsuperior performance of EditID. EditID is the first text-to-image solution to\npropose customizable ID editability on the DiT architecture, meeting the\ndemands of long prompts and high quality image generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12539", "pdf": "https://arxiv.org/pdf/2503.12539", "abs": "https://arxiv.org/abs/2503.12539", "authors": ["Weiguang Zhao", "Rui Zhang", "Qiufeng Wang", "Guangliang Cheng", "Kaizhu Huang"], "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps://github.com/weiguangzhao/BFANet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindström", "Peng Su", "Matthias Nießner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12567", "pdf": "https://arxiv.org/pdf/2503.12567", "abs": "https://arxiv.org/abs/2503.12567", "authors": ["Abyad Enan", "Mashrur Chowdhury"], "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) for possible publication", "summary": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12595", "pdf": "https://arxiv.org/pdf/2503.12595", "abs": "https://arxiv.org/abs/2503.12595", "authors": ["Dan Halperin", "Niklas Eisl"], "title": "Point Cloud Based Scene Segmentation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11731", "pdf": "https://arxiv.org/pdf/2503.11731", "abs": "https://arxiv.org/abs/2503.11731", "authors": ["Xianming Zeng", "Sicong Du", "Qifeng Chen", "Lizhe Liu", "Haoyu Shu", "Jiaxuan Gao", "Jiarun Liu", "Jiulong Xu", "Jianyun Xu", "Mingxia Chen", "Yiru Zhao", "Peng Chen", "Yapeng Xue", "Chunming Zhao", "Sheng Yang", "Qiang Li"], "title": "Industrial-Grade Sensor Simulation via Gaussian Splatting: A Modular Framework for Scalable Editing and Full-Stack Validation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor simulation is pivotal for scalable validation of autonomous driving\nsystems, yet existing Neural Radiance Fields (NeRF) based methods face\napplicability and efficiency challenges in industrial workflows. This paper\nintroduces a Gaussian Splatting (GS) based system to address these challenges:\nWe first break down sensor simulator components and analyze the possible\nadvantages of GS over NeRF. Then in practice, we refactor three crucial\ncomponents through GS, to leverage its explicit scene representation and\nreal-time rendering: (1) choosing the 2D neural Gaussian representation for\nphysics-compliant scene and sensor modeling, (2) proposing a scene editing\npipeline to leverage Gaussian primitives library for data augmentation, and (3)\ncoupling a controllable diffusion model for scene expansion and harmonization.\nWe implement this framework on a proprietary autonomous driving dataset\nsupporting cameras and LiDAR sensors. We demonstrate through ablation studies\nthat our approach reduces frame-wise simulation latency, achieves better\ngeometric and photometric consistency, and enables interpretable explicit scene\nediting and expansion. Furthermore, we showcase how integrating such a GS-based\nsensor simulator with traffic and dynamic simulators enables full-stack testing\nof end-to-end autonomy algorithms. Our work provides both algorithmic insights\nand practical validation, establishing GS as a cornerstone for industrial-grade\nsensor simulation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11893", "pdf": "https://arxiv.org/pdf/2503.11893", "abs": "https://arxiv.org/abs/2503.11893", "authors": ["Md Abu Bakr Siddique", "Junliang Liu", "Piyush Singh", "Md Jahidul Islam"], "title": "UStyle: Waterbody Style Transfer of Underwater Scenes by Depth-Guided Feature Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The concept of waterbody style transfer remains largely unexplored in the\nunderwater imaging and vision literature. Traditional image style transfer\n(STx) methods primarily focus on artistic and photorealistic blending, often\nfailing to preserve object and scene geometry in images captured in\nhigh-scattering mediums such as underwater. The wavelength-dependent nonlinear\nattenuation and depth-dependent backscattering artifacts further complicate\nlearning underwater image STx from unpaired data. This paper introduces UStyle,\nthe first data-driven learning framework for transferring waterbody styles\nacross underwater images without requiring prior reference images or scene\ninformation. We propose a novel depth-aware whitening and coloring transform\n(DA-WCT) mechanism that integrates physics-based waterbody synthesis to ensure\nperceptually consistent stylization while preserving scene structure. To\nenhance style transfer quality, we incorporate carefully designed loss\nfunctions that guide UStyle to maintain colorfulness, lightness, structural\nintegrity, and frequency-domain characteristics, as well as high-level content\nin VGG and CLIP (contrastive language-image pretraining) feature spaces. By\naddressing domain-specific challenges, UStyle provides a robust framework for\nno-reference underwater image STx, surpassing state-of-the-art (SOTA) methods\nthat rely solely on end-to-end reconstruction loss. Furthermore, we introduce\nthe UF7D dataset, a curated collection of high-resolution underwater images\nspanning seven distinct waterbody styles, establishing a benchmark to support\nfuture research in underwater image STx. The UStyle inference pipeline and UF7D\ndataset are released at: https://github.com/uf-robopi/UStyle.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11932", "pdf": "https://arxiv.org/pdf/2503.11932", "abs": "https://arxiv.org/abs/2503.11932", "authors": ["Dhruv Kudale", "Badri Vishal Kasuba", "Venkatapathy Subramanian", "Parag Chaudhuri", "Ganesh Ramakrishnan"], "title": "SPRINT: Script-agnostic Structure Recognition in Tables", "categories": ["cs.CV"], "comment": "Accepted at ICDAR 2024", "summary": "Table Structure Recognition (TSR) is vital for various downstream tasks like\ninformation retrieval, table reconstruction, and document understanding. While\nmost state-of-the-art (SOTA) research predominantly focuses on TSR in English\ndocuments, the need for similar capabilities in other languages is evident,\nconsidering the global diversity of data. Moreover, creating substantial\nlabeled data in non-English languages and training these SOTA models from\nscratch is costly and time-consuming. We propose TSR as a language-agnostic\ncell arrangement prediction and introduce SPRINT, Script-agnostic Structure\nRecognition in Tables. SPRINT uses recently introduced Optimized Table\nStructure Language (OTSL) sequences to predict table structures. We show that\nwhen coupled with a pre-trained table grid estimator, SPRINT can improve the\noverall tree edit distance-based similarity structure scores of tables even for\nnon-English documents. We experimentally evaluate our performance across\nbenchmark TSR datasets including PubTabNet, FinTabNet, and PubTables-1M. Our\nfindings reveal that SPRINT not only matches SOTA models in performance on\nstandard datasets but also demonstrates lower latency. Additionally, SPRINT\nexcels in accurately identifying table structures in non-English documents,\nsurpassing current leading models by showing an absolute average increase of\n11.12%. We also present an algorithm for converting valid OTSL predictions into\na widely used HTML-based table representation. To encourage further research,\nwe release our code and Multilingual Scanned and Scene Table Structure\nRecognition Dataset, MUSTARD labeled with OTSL sequences for 1428 tables in\nthirteen languages encompassing several scripts at\nhttps://github.com/IITB-LEAP-OCR/SPRINT", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12034", "pdf": "https://arxiv.org/pdf/2503.12034", "abs": "https://arxiv.org/abs/2503.12034", "authors": ["Enes Erdogan", "Eren Erdal Aksoy", "Sanem Sariel"], "title": "Real-Time Manipulation Action Recognition with a Factorized Graph Sequence Encoder", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures, 7 tables", "summary": "Recognition of human manipulation actions in real-time is essential for safe\nand effective human-robot interaction and collaboration. The challenge lies in\ndeveloping a model that is both lightweight enough for real-time execution and\ncapable of generalization. While some existing methods in the literature can\nrun in real-time, they struggle with temporal scalability, i.e., they fail to\nadapt to long-duration manipulations effectively. To address this, leveraging\nthe generalizable scene graph representations, we propose a new Factorized\nGraph Sequence Encoder network that not only runs in real-time but also scales\neffectively in the temporal dimension, thanks to its factorized encoder\narchitecture. Additionally, we introduce Hand Pooling operation, a simple\npooling operation for more focused extraction of the graph-level embeddings.\nOur model outperforms the previous state-of-the-art real-time approach,\nachieving a 14.3\\% and 5.6\\% improvement in F1-macro score on the KIT Bimanual\nAction (Bimacs) Dataset and Collaborative Action (CoAx) Dataset, respectively.\nMoreover, we conduct an extensive ablation study to validate our network design\nchoices. Finally, we compare our model with its architecturally similar\nRGB-based model on the Bimacs dataset and show the limitations of this model in\ncontrast to ours on such an object-centric manipulation dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dimension"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12047", "pdf": "https://arxiv.org/pdf/2503.12047", "abs": "https://arxiv.org/abs/2503.12047", "authors": ["Hangrui Xu", "Chuanrui Zhang", "Zhengxian Wu", "Peng Jiao", "Haoqian Wang"], "title": "PSGait: Multimodal Gait Recognition using Parsing Skeleton", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition has emerged as a robust biometric modality due to its\nnon-intrusive nature and resilience to occlusion. Conventional gait recognition\nmethods typically rely on silhouettes or skeletons. Despite their success in\ngait recognition for controlled laboratory environments, they usually fail in\nreal-world scenarios due to their limited information entropy for gait\nrepresentations. To achieve accurate gait recognition in the wild, we propose a\nnovel gait representation, named Parsing Skeleton. This representation\ninnovatively introduces the skeleton-guided human parsing method to capture\nfine-grained body dynamics, so they have much higher information entropy to\nencode the shapes and dynamics of fine-grained human parts during walking.\nMoreover, to effectively explore the capability of the parsing skeleton\nrepresentation, we propose a novel parsing skeleton-based gait recognition\nframework, named PSGait, which takes parsing skeletons and silhouettes as\ninput. By fusing these two modalities, the resulting image sequences are fed\ninto gait recognition models for enhanced individual differentiation. We\nconduct comprehensive benchmarks on various datasets to evaluate our model.\nPSGait outperforms existing state-of-the-art multimodal methods. Furthermore,\nas a plug-and-play method, PSGait leads to a maximum improvement of 10.9% in\nRank-1 accuracy across various gait recognition models. These results\ndemonstrate the effectiveness and versatility of parsing skeletons for gait\nrecognition in the wild, establishing PSGait as a new state-of-the-art approach\nfor multimodal gait recognition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12049", "pdf": "https://arxiv.org/pdf/2503.12049", "abs": "https://arxiv.org/abs/2503.12049", "authors": ["Ruijie Lu", "Yixin Chen", "Yu Liu", "Jiaxiang Tang", "Junfeng Ni", "Diwen Wan", "Gang Zeng", "Siyuan Huang"], "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion", "categories": ["cs.CV"], "comment": "Project page: https://jason-aplp.github.io/TACO", "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12069", "pdf": "https://arxiv.org/pdf/2503.12069", "abs": "https://arxiv.org/abs/2503.12069", "authors": ["Wei Lai", "Tianyu Ding", "ren dongdong", "Lei Wang", "Jing Huo", "Yang Gao", "Wenbin Li"], "title": "Robust Dataset Distillation by Matching Adversarial Trajectories", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation synthesizes compact datasets that enable models to\nachieve performance comparable to training on the original large-scale\ndatasets. However, existing distillation methods overlook the robustness of the\nmodel, resulting in models that are vulnerable to adversarial attacks when\ntrained on distilled data. To address this limitation, we introduce the task of\n``robust dataset distillation\", a novel paradigm that embeds adversarial\nrobustness into the synthetic datasets during the distillation process. We\npropose Matching Adversarial Trajectories (MAT), a method that integrates\nadversarial training into trajectory-based dataset distillation. MAT\nincorporates adversarial samples during trajectory generation to obtain robust\ntraining trajectories, which are then used to guide the distillation process.\nAs experimentally demonstrated, even through natural training on our distilled\ndataset, models can achieve enhanced adversarial robustness while maintaining\ncompetitive accuracy compared to existing distillation methods. Our work\nhighlights robust dataset distillation as a new and important research\ndirection and provides a strong baseline for future research to bridge the gap\nbetween efficient training and adversarial robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12094", "pdf": "https://arxiv.org/pdf/2503.12094", "abs": "https://arxiv.org/abs/2503.12094", "authors": ["Weiming Zhang", "Dingwen Xiao", "Lei Chen", "Lin Wang"], "title": "E-SAM: Training-Free Segment Every Entity Model", "categories": ["cs.CV"], "comment": "Under review", "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities\nwithin an image without the need for predefined class labels. This\ncharacteristic makes ES well-suited to open-world applications with adaptation\nto diverse and dynamically changing environments, where new and previously\nunseen entities may appear frequently. Existing ES methods either require large\nannotated datasets or high training costs, limiting their scalability and\nadaptability. Recently, the Segment Anything Model (SAM), especially in its\nAutomatic Mask Generation (AMG) mode, has shown potential for holistic image\nsegmentation. However, it struggles with over-segmentation and\nunder-segmentation, making it less effective for ES. In this paper, we\nintroduce E-SAM, a novel training-free framework that exhibits exceptional ES\ncapability. Specifically, we first propose Multi-level Mask Generation (MMG)\nthat hierarchically processes SAM's AMG outputs to generate reliable\nobject-level masks while preserving fine details at other levels. Entity-level\nMask Refinement (EMR) then refines these object-level masks into accurate\nentity-level masks. That is, it separates overlapping masks to address the\nredundancy issues inherent in SAM's outputs and merges similar masks by\nevaluating entity-level consistency. Lastly, Under-Segmentation Refinement\n(USR) addresses under-segmentation by generating additional high-confidence\nmasks fused with EMR outputs to produce the final ES map. These three modules\nare seamlessly optimized to achieve the best ES without additional training\noverhead. Extensive experiments demonstrate that E-SAM achieves\nstate-of-the-art performance compared to prior ES methods, demonstrating a\nsignificant improvement by +30.1 on benchmark metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12095", "pdf": "https://arxiv.org/pdf/2503.12095", "abs": "https://arxiv.org/abs/2503.12095", "authors": ["Walter Zimmer", "Ross Greer", "Daniel Lehmberg", "Marc Pavel", "Holger Caesar", "Xingcheng Zhou", "Ahmed Ghita", "Mohan Trivedi", "Rui Song", "Hu Cao", "Akshay Gopalkrishnan", "Alois C. Knoll"], "title": "Towards Vision Zero: The Accid3nD Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Even though a significant amount of work has been done to increase the safety\nof transportation networks, accidents still occur regularly. They must be\nunderstood as unavoidable and sporadic outcomes of traffic networks. No public\ndataset contains 3D annotations of real-world accidents recorded from roadside\nsensors. We present the Accid3nD dataset, a collection of real-world highway\naccidents in different weather and lighting conditions. It contains vehicle\ncrashes at high-speed driving with 2,634,233 labeled 2D bounding boxes,\ninstance masks, and 3D bounding boxes with track IDs. In total, the dataset\ncontains 111,945 labeled frames recorded from four roadside cameras and LiDARs\nat 25 Hz. The dataset contains six object classes and is provided in the\nOpenLABEL format. We propose an accident detection model that combines a\nrule-based approach with a learning-based one. Experiments and ablation studies\non our dataset show the robustness of our proposed method. The dataset, model,\nand code are available on our website: https://accident-dataset.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12206", "pdf": "https://arxiv.org/pdf/2503.12206", "abs": "https://arxiv.org/abs/2503.12206", "authors": ["Ans Munir", "Faisal Z. Qureshi", "Muhammad Haris Khan", "Mohsen Ali"], "title": "TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. We evaluated our\nmodels on 11 base-to-novel datasets and they achieved superior accuracy on 9 of\nthese, including benchmarks like ImageNet, SUN397 and Caltech101, while\nmaintaining a strictly training-free paradigm. Our overall accuracy of 83.44%\nsurpasses the previous state-of-the-art few-shot methods by a margin of 6.75%.\nOur method achieved 83.6% average accuracy across 13 datasets, a 9.7%\nimprovement over the previous 73.9% state-of-the-art for training-free\napproaches. Our method improves domain generalization, with a 3.6% gain on\nImageNetV2, 16.96% on ImageNet-S, and 12.59% on ImageNet-R, over prior few-shot\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12213", "pdf": "https://arxiv.org/pdf/2503.12213", "abs": "https://arxiv.org/abs/2503.12213", "authors": ["Ruyu Wang", "Xuefeng Hou", "Sabrina Schmedding", "Marco F. Huber"], "title": "STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted by WACV2025", "summary": "In layout-to-image (L2I) synthesis, controlled complex scenes are generated\nfrom coarse information like bounding boxes. Such a task is exciting to many\ndownstream applications because the input layouts offer strong guidance to the\ngeneration process while remaining easily reconfigurable by humans. In this\npaper, we proposed STyled LAYout Diffusion (STAY Diffusion), a diffusion-based\nmodel that produces photo-realistic images and provides fine-grained control of\nstylized objects in scenes. Our approach learns a global condition for each\nlayout, and a self-supervised semantic map for weight modulation using a novel\nEdge-Aware Normalization (EA Norm). A new Styled-Mask Attention (SM Attention)\nis also introduced to cross-condition the global condition and image feature\nfor capturing the objects' relationships. These measures provide consistent\nguidance through the model, enabling more accurate and controllable image\ngeneration. Extensive benchmarking demonstrates that our STAY Diffusion\npresents high-quality images while surpassing previous state-of-the-art methods\nin generation diversity, accuracy, and controllability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12230", "pdf": "https://arxiv.org/pdf/2503.12230", "abs": "https://arxiv.org/abs/2503.12230", "authors": ["Yihao Wang", "Raphael Memmesheimer", "Sven Behnke"], "title": "LIAM: Multimodal Transformer for Language Instructions, Images, Actions and Semantic Maps", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The availability of large language models and open-vocabulary object\nperception methods enables more flexibility for domestic service robots. The\nlarge variability of domestic tasks can be addressed without implementing each\ntask individually by providing the robot with a task description along with\nappropriate environment information. In this work, we propose LIAM - an\nend-to-end model that predicts action transcripts based on language, image,\naction, and map inputs. Language and image inputs are encoded with a CLIP\nbackbone, for which we designed two pre-training tasks to fine-tune its weights\nand pre-align the latent spaces. We evaluate our method on the ALFRED dataset,\na simulator-generated benchmark for domestic tasks. Our results demonstrate the\nimportance of pre-aligning embedding spaces from different modalities and the\nefficacy of incorporating semantic maps.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12350", "pdf": "https://arxiv.org/pdf/2503.12350", "abs": "https://arxiv.org/abs/2503.12350", "authors": ["Wenqing Kuang", "Xiongwei Zhao", "Yehui Shen", "Congcong Wen", "Huimin Lu", "Zongtan Zhou", "Xieyuanli Chen"], "title": "ResLPR: A LiDAR Data Restoration Network and Benchmark for Robust Place Recognition Against Weather Corruptions", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based place recognition (LPR) is a key component for autonomous\ndriving, and its resilience to environmental corruption is critical for safety\nin high-stakes applications. While state-of-the-art (SOTA) LPR methods perform\nwell in clean weather, they still struggle with weather-induced corruption\ncommonly encountered in driving scenarios. To tackle this, we propose\nResLPRNet, a novel LiDAR data restoration network that largely enhances LPR\nperformance under adverse weather by restoring corrupted LiDAR scans using a\nwavelet transform-based network. ResLPRNet is efficient, lightweight and can be\nintegrated plug-and-play with pretrained LPR models without substantial\nadditional computational cost. Given the lack of LPR datasets under adverse\nweather, we introduce ResLPR, a novel benchmark that examines SOTA LPR methods\nunder a wide range of LiDAR distortions induced by severe snow, fog, and rain\nconditions. Experiments on our proposed WeatherKITTI and WeatherNCLT datasets\ndemonstrate the resilience and notable gains achieved by using our restoration\nmethod with multiple LPR approaches in challenging weather scenarios. Our code\nand benchmark are publicly available here:\nhttps://github.com/nubot-nudt/ResLPR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12419", "pdf": "https://arxiv.org/pdf/2503.12419", "abs": "https://arxiv.org/abs/2503.12419", "authors": ["Luming Wang", "Hao Shi", "Xiaoting Yin", "Kailun Yang", "Kaiwei Wang"], "title": "EgoEvGesture: Gesture Recognition Based on Egocentric Event Camera", "categories": ["cs.CV", "cs.RO", "eess.IV", "physics.optics"], "comment": "The dataset and models are made publicly available at\n  https://github.com/3190105222/EgoEv_Gesture", "summary": "Egocentric gesture recognition is a pivotal technology for enhancing natural\nhuman-computer interaction, yet traditional RGB-based solutions suffer from\nmotion blur and illumination variations in dynamic scenarios. While event\ncameras show distinct advantages in handling high dynamic range with ultra-low\npower consumption, existing RGB-based architectures face inherent limitations\nin processing asynchronous event streams due to their synchronous frame-based\nnature. Moreover, from an egocentric perspective, event cameras record data\nthat include events generated by both head movements and hand gestures, thereby\nincreasing the complexity of gesture recognition. To address this, we propose a\nnovel network architecture specifically designed for event data processing,\nincorporating (1) a lightweight CNN with asymmetric depthwise convolutions to\nreduce parameters while preserving spatiotemporal features, (2) a plug-and-play\nstate-space model as context block that decouples head movement noise from\ngesture dynamics, and (3) a parameter-free Bins-Temporal Shift Module (BSTM)\nthat shifts features along bins and temporal dimensions to fuse sparse events\nefficiently. We further build the EgoEvGesture dataset, the first large-scale\ndataset for egocentric gesture recognition using event cameras. Experimental\nresults demonstrate that our method achieves 62.7% accuracy in heterogeneous\ntesting with only 7M parameters, 3.1% higher than state-of-the-art approaches.\nNotable misclassifications in freestyle motions stem from high inter-personal\nvariability and unseen test patterns differing from training data. Moreover,\nour approach achieved a remarkable accuracy of 96.97% on DVS128 Gesture,\ndemonstrating strong cross-dataset generalization capability. The dataset and\nmodels are made publicly available at\nhttps://github.com/3190105222/EgoEv_Gesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12451", "pdf": "https://arxiv.org/pdf/2503.12451", "abs": "https://arxiv.org/abs/2503.12451", "authors": ["Hossein Ranjbar", "Alireza Taheri"], "title": "ISLR101: an Iranian Word-Level Sign Language Recognition Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Sign language recognition involves modeling complex multichannel information,\nsuch as hand shapes and movements while relying on sufficient sign\nlanguage-specific data. However, sign languages are often under-resourced,\nposing a significant challenge for research and development in this field. To\naddress this gap, we introduce ISLR101, the first publicly available Iranian\nSign Language dataset for isolated sign language recognition. This\ncomprehensive dataset includes 4,614 videos covering 101 distinct signs,\nrecorded by 10 different signers (3 deaf individuals, 2 sign language\ninterpreters, and 5 L2 learners) against varied backgrounds, with a resolution\nof 800x600 pixels and a frame rate of 25 frames per second. It also includes\nskeleton pose information extracted using OpenPose. We establish both a visual\nappearance-based and a skeleton-based framework as baseline models, thoroughly\ntraining and evaluating them on ISLR101. These models achieve 97.01% and 94.02%\naccuracy on the test set, respectively. Additionally, we publish the train,\nvalidation, and test splits to facilitate fair comparisons.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12460", "pdf": "https://arxiv.org/pdf/2503.12460", "abs": "https://arxiv.org/abs/2503.12460", "authors": ["Zhicheng Wang", "Zhiyu Pan", "Zhan Peng", "Jian Cheng", "Liwen Xiao", "Wei Jiang", "Zhiguo Cao"], "title": "Exploring Contextual Attribute Density in Referring Expression Counting", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Referring expression counting (REC) algorithms are for more flexible and\ninteractive counting ability across varied fine-grained text expressions.\nHowever, the requirement for fine-grained attribute understanding poses\nchallenges for prior arts, as they struggle to accurately align attribute\ninformation with correct visual patterns. Given the proven importance of\n''visual density'', it is presumed that the limitations of current REC\napproaches stem from an under-exploration of ''contextual attribute density''\n(CAD). In the scope of REC, we define CAD as the measure of the information\nintensity of one certain fine-grained attribute in visual regions. To model the\nCAD, we propose a U-shape CAD estimator in which referring expression and\nmulti-scale visual features from GroundingDINO can interact with each other.\nWith additional density supervision, we can effectively encode CAD, which is\nsubsequently decoded via a novel attention procedure with CAD-refined queries.\nIntegrating all these contributions, our framework significantly outperforms\nstate-of-the-art REC methods, achieves $30\\%$ error reduction in counting\nmetrics and a $10\\%$ improvement in localization accuracy. The surprising\nresults shed light on the significance of contextual attribute density for REC.\nCode will be at github.com/Xu3XiWang/CAD-GD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12472", "pdf": "https://arxiv.org/pdf/2503.12472", "abs": "https://arxiv.org/abs/2503.12472", "authors": ["Wenbo Dai", "Lijing Lu", "Zhihang Li"], "title": "Diffusion-based Synthetic Data Generation for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "The performance of models is intricately linked to the abundance of training\ndata. In Visible-Infrared person Re-IDentification (VI-ReID) tasks, collecting\nand annotating large-scale images of each individual under various cameras and\nmodalities is tedious, time-expensive, costly and must comply with data\nprotection laws, posing a severe challenge in meeting dataset requirements.\nCurrent research investigates the generation of synthetic data as an efficient\nand privacy-ensuring alternative to collecting real data in the field. However,\na specific data synthesis technique tailored for VI-ReID models has yet to be\nexplored. In this paper, we present a novel data generation framework, dubbed\nDiffusion-based VI-ReID data Expansion (DiVE), that automatically obtain\nmassive RGB-IR paired images with identity preserving by decoupling identity\nand modality to improve the performance of VI-ReID models. Specifically,\nidentity representation is acquired from a set of samples sharing the same ID,\nwhereas the modality of images is learned by fine-tuning the Stable Diffusion\n(SD) on modality-specific data. DiVE extend the text-driven image synthesis to\nidentity-preserving RGB-IR multimodal image synthesis. This approach\nsignificantly reduces data collection and annotation costs by directly\nincorporating synthetic data into ReID model training. Experiments have\ndemonstrated that VI-ReID models trained on synthetic data produced by DiVE\nconsistently exhibit notable enhancements. In particular, the state-of-the-art\nmethod, CAJ, trained with synthetic images, achieves an improvement of about\n$9\\%$ in mAP over the baseline on the LLCM dataset. Code:\nhttps://github.com/BorgDiven/DiVE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12495", "pdf": "https://arxiv.org/pdf/2503.12495", "abs": "https://arxiv.org/abs/2503.12495", "authors": ["Xuan Ma", "Zewen Lv", "Chengcai Ma", "Tao Zhang", "Yuelan Xin", "Kun Zhan"], "title": "BS-Mamba for Black-Soil Area Detection On the Qinghai-Tibetan Plateau", "categories": ["cs.CV"], "comment": "Journal of Applied Remote Sensing, 2025", "summary": "Extremely degraded grassland on the Qinghai-Tibetan Plateau (QTP) presents a\nsignificant environmental challenge due to overgrazing, climate change, and\nrodent activity, which degrade vegetation cover and soil quality. These\nextremely degraded grassland on QTP, commonly referred to as black-soil area,\nrequire accurate assessment to guide effective restoration efforts. In this\npaper, we present a newly created QTP black-soil dataset, annotated under\nexpert guidance. We introduce a novel neural network model, BS-Mamba,\nspecifically designed for the black-soil area detection using UAV remote\nsensing imagery. The BS-Mamba model demonstrates higher accuracy in identifying\nblack-soil area across two independent test datasets than the state-of-the-art\nmodels. This research contributes to grassland restoration by providing an\nefficient method for assessing the extent of black-soil area on the QTP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12515", "pdf": "https://arxiv.org/pdf/2503.12515", "abs": "https://arxiv.org/abs/2503.12515", "authors": ["Pan Du", "Delin An", "Chaoli Wang", "Jian-Xun Wang"], "title": "AI-Powered Automated Model Construction for Patient-Specific CFD Simulations of Aortic Flows", "categories": ["cs.CV", "cs.LG", "physics.med-ph"], "comment": "42 pages, 8 figures", "summary": "Image-based modeling is essential for understanding cardiovascular\nhemodynamics and advancing the diagnosis and treatment of cardiovascular\ndiseases. Constructing patient-specific vascular models remains\nlabor-intensive, error-prone, and time-consuming, limiting their clinical\napplications. This study introduces a deep-learning framework that automates\nthe creation of simulation-ready vascular models from medical images. The\nframework integrates a segmentation module for accurate voxel-based vessel\ndelineation with a surface deformation module that performs anatomically\nconsistent and unsupervised surface refinements guided by medical image data.\nBy unifying voxel segmentation and surface deformation into a single cohesive\npipeline, the framework addresses key limitations of existing methods,\nenhancing geometric accuracy and computational efficiency. Evaluated on\npublicly available datasets, the proposed approach demonstrates\nstate-of-the-art performance in segmentation and mesh quality while\nsignificantly reducing manual effort and processing time. This work advances\nthe scalability and reliability of image-based computational modeling,\nfacilitating broader applications in clinical and research settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12526", "pdf": "https://arxiv.org/pdf/2503.12526", "abs": "https://arxiv.org/abs/2503.12526", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditID: Training-Free Editable ID Customization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose EditID, a training-free approach based on the DiT architecture,\nwhich achieves highly editable customized IDs for text to image generation.\nExisting text-to-image models for customized IDs typically focus more on ID\nconsistency while neglecting editability. It is challenging to alter facial\norientation, character attributes, and other features through prompts. EditID\naddresses this by deconstructing the text-to-image model for customized IDs\ninto an image generation branch and a character feature branch. The character\nfeature branch is further decoupled into three modules: feature extraction,\nfeature fusion, and feature integration. By introducing a combination of\nmapping features and shift features, along with controlling the intensity of ID\nfeature integration, EditID achieves semantic compression of local features\nacross network depths, forming an editable feature space. This enables the\nsuccessful generation of high-quality images with editable IDs while\nmaintaining ID consistency, achieving excellent results in the IBench\nevaluation, which is an editability evaluation framework for the field of\ncustomized ID text-to-image generation that quantitatively demonstrates the\nsuperior performance of EditID. EditID is the first text-to-image solution to\npropose customizable ID editability on the DiT architecture, meeting the\ndemands of long prompts and high quality image generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12539", "pdf": "https://arxiv.org/pdf/2503.12539", "abs": "https://arxiv.org/abs/2503.12539", "authors": ["Weiguang Zhao", "Rui Zhang", "Qiufeng Wang", "Guangliang Cheng", "Kaizhu Huang"], "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic segmentation plays a fundamental and crucial role to understand\n3D scenes. While contemporary state-of-the-art techniques predominantly\nconcentrate on elevating the overall performance of 3D semantic segmentation\nbased on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave\nthe exploration of challenging regions for segmentation mostly neglected. In\nthis paper, we revisit 3D semantic segmentation through a more granular lens,\nshedding light on subtle complexities that are typically overshadowed by\nbroader performance metrics. Concretely, we have delineated 3D semantic\nsegmentation errors into four comprehensive categories as well as corresponding\nevaluation metrics tailored to each. Building upon this categorical framework,\nwe introduce an innovative 3D semantic segmentation network called BFANet that\nincorporates detailed analysis of semantic boundary features. First, we design\nthe boundary-semantic module to decouple point cloud features into semantic and\nboundary features, and fuse their query queue to enhance semantic features with\nattention. Second, we introduce a more concise and accelerated boundary\npseudo-label calculation algorithm, which is 3.9 times faster than the\nstate-of-the-art, offering compatibility with data augmentation and enabling\nefficient computation in training. Extensive experiments on benchmark data\nindicate the superiority of our BFANet model, confirming the significance of\nemphasizing the four uniquely designed metrics. Code is available at\nhttps://github.com/weiguangzhao/BFANet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12552", "pdf": "https://arxiv.org/pdf/2503.12552", "abs": "https://arxiv.org/abs/2503.12552", "authors": ["Tianyu Li", "Yihang Qiu", "Zhenhua Wu", "Carl Lindström", "Peng Su", "Matthias Nießner", "Hongyang Li"], "title": "MTGS: Multi-Traversal Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12567", "pdf": "https://arxiv.org/pdf/2503.12567", "abs": "https://arxiv.org/abs/2503.12567", "authors": ["Abyad Enan", "Mashrur Chowdhury"], "title": "GAN-Based Single-Stage Defense for Traffic Sign Classification Under Adversarial Patch Attack", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems (T-ITS) for possible publication", "summary": "Computer Vision plays a critical role in ensuring the safe navigation of\nautonomous vehicles (AVs). An AV perception module is responsible for capturing\nand interpreting the surrounding environment to facilitate safe navigation.\nThis module enables AVs to recognize traffic signs, traffic lights, and various\nroad users. However, the perception module is vulnerable to adversarial\nattacks, which can compromise their accuracy and reliability. One such attack\nis the adversarial patch attack (APA), a physical attack in which an adversary\nstrategically places a specially crafted sticker on an object to deceive object\nclassifiers. In APA, an adversarial patch is positioned on a target object,\nleading the classifier to misidentify it. Such an APA can cause AVs to\nmisclassify traffic signs, leading to catastrophic incidents. To enhance the\nsecurity of an AV perception system against APAs, this study develops a\nGenerative Adversarial Network (GAN)-based single-stage defense strategy for\ntraffic sign classification. This approach is tailored to defend against APAs\non different classes of traffic signs without prior knowledge of a patch's\ndesign. This study found this approach to be effective against patches of\nvarying sizes. Our experimental analysis demonstrates that the defense strategy\npresented in this paper improves the classifier's accuracy under APA conditions\nby up to 80.8% and enhances overall classification accuracy for all the traffic\nsigns considered in this study by 58%, compared to a classifier without any\ndefense mechanism. Our defense strategy is model-agnostic, making it applicable\nto any traffic sign classifier, regardless of the underlying classification\nmodel.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12595", "pdf": "https://arxiv.org/pdf/2503.12595", "abs": "https://arxiv.org/abs/2503.12595", "authors": ["Dan Halperin", "Niklas Eisl"], "title": "Point Cloud Based Scene Segmentation: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autonomous driving is a safety-critical application, and it is therefore a\ntop priority that the accompanying assistance systems are able to provide\nprecise information about the surrounding environment of the vehicle. Tasks\nsuch as 3D Object Detection deliver an insufficiently detailed understanding of\nthe surrounding scene because they only predict a bounding box for foreground\nobjects. In contrast, 3D Semantic Segmentation provides richer and denser\ninformation about the environment by assigning a label to each individual\npoint, which is of paramount importance for autonomous driving tasks, such as\nnavigation or lane changes. To inspire future research, in this review paper,\nwe provide a comprehensive overview of the current state-of-the-art methods in\nthe field of Point Cloud Semantic Segmentation for autonomous driving. We\ncategorize the approaches into projection-based, 3D-based and hybrid methods.\nMoreover, we discuss the most important and commonly used datasets for this\ntask and also emphasize the importance of synthetic data to support research\nwhen real-world data is limited. We further present the results of the\ndifferent methods and compare them with respect to their segmentation accuracy\nand efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12663", "pdf": "https://arxiv.org/pdf/2503.12663", "abs": "https://arxiv.org/abs/2503.12663", "authors": ["Imran Kabir", "Md Alimoor Reza", "Syed Billah"], "title": "Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial Knowledge for Road Scene Understanding", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "Large multimodal models (LMMs) are increasingly integrated into autonomous\ndriving systems for user interaction. However, their limitations in\nfine-grained spatial reasoning pose challenges for system interpretability and\nuser trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation\n(RAG) framework that improves LMMs' spatial understanding in driving scenarios.\nLogic-RAG constructs a dynamic knowledge base (KB) about object-object\nrelationships in first-order logic (FOL) using a perception module, a\nquery-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG\non visual-spatial queries using both synthetic and real-world driving videos.\nWhen using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous\ndriving system, these models achieved only 55% accuracy on synthetic driving\nscenes and under 75% on real-world driving scenes. Augmenting them with\nLogic-RAG increased their accuracies to over 80% and 90%, respectively. An\nablation study showed that even without logical inference, the fact-based\ncontext constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is\nextensible: it allows seamless replacement of individual components with\nimproved versions and enables domain experts to compose new knowledge in both\nFOL and natural language. In sum, Logic-RAG addresses critical spatial\nreasoning deficiencies in LMMs for autonomous driving applications. Code and\ndata are available at https://github.com/Imran2205/LogicRAG.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12720", "pdf": "https://arxiv.org/pdf/2503.12720", "abs": "https://arxiv.org/abs/2503.12720", "authors": ["Feng Qiao", "Zhexiao Xiong", "Eric Xing", "Nathan Jacobs"], "title": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching", "categories": ["cs.CV"], "comment": "Project page is available at https://qjizhi.github.io/genstereo", "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12745", "pdf": "https://arxiv.org/pdf/2503.12745", "abs": "https://arxiv.org/abs/2503.12745", "authors": ["Patrick Rim", "Hyoungseob Park", "S. Gangopadhyay", "Ziyao Zeng", "Younjoon Chung", "Alex Wong"], "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We present ProtoDepth, a novel prototype-based approach for continual\nlearning of unsupervised depth completion, the multimodal 3D reconstruction\ntask of predicting dense depth maps from RGB images and sparse point clouds.\nThe unsupervised learning paradigm is well-suited for continual learning, as\nground truth is not needed. However, when training on new non-stationary\ndistributions, depth completion models will catastrophically forget previously\nlearned information. We address forgetting by learning prototype sets that\nadapt the latent features of a frozen pretrained model to new domains. Since\nthe original weights are not modified, ProtoDepth does not forget when\ntest-time domain identity is known. To extend ProtoDepth to the challenging\nsetting where the test-time domain identity is withheld, we propose to learn\ndomain descriptors that enable the model to select the appropriate prototype\nset for inference. We evaluate ProtoDepth on benchmark dataset sequences, where\nwe reduce forgetting compared to baselines by 52.2% for indoor and 53.2% for\noutdoor to achieve the state of the art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12769", "pdf": "https://arxiv.org/pdf/2503.12769", "abs": "https://arxiv.org/abs/2503.12769", "authors": ["Shenghao Fu", "Qize Yang", "Yuan-Ming Li", "Yi-Xing Peng", "Kun-Yu Lin", "Xihan Wei", "Jian-Fang Hu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "ViSpeak: Visual Instruction Feedback in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Large Multi-modal Models (LMMs) are primarily focused on\noffline video understanding. Instead, streaming video understanding poses great\nchallenges to recent models due to its time-sensitive, omni-modal and\ninteractive characteristics. In this work, we aim to extend the streaming video\nunderstanding from a new perspective and propose a novel task named Visual\nInstruction Feedback in which models should be aware of visual contents and\nlearn to extract instructions from them. For example, when users wave their\nhands to agents, agents should recognize the gesture and start conversations\nwith welcome information. Thus, following instructions in visual modality\ngreatly enhances user-agent interactions. To facilitate research, we define\nseven key subtasks highly relevant to visual modality and collect the\nViSpeak-Instruct dataset for training and the ViSpeak-Bench for evaluation.\nFurther, we propose the ViSpeak model, which is a SOTA streaming video\nunderstanding LMM with GPT-4o-level performance on various streaming video\nunderstanding benchmarks. After finetuning on our ViSpeak-Instruct dataset,\nViSpeak is equipped with basic visual instruction feedback ability, serving as\na solid baseline for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12778", "pdf": "https://arxiv.org/pdf/2503.12778", "abs": "https://arxiv.org/abs/2503.12778", "authors": ["Gul Sheeraz", "Qun Chen", "Liu Feiyu", "Zhou Fengjin MD"], "title": "Adaptive Deep Learning for Multiclass Breast Cancer Classification via Misprediction Risk Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer remains one of the leading causes of cancer-related deaths\nworldwide. Early detection is crucial for improving patient outcomes, yet the\ndiagnostic process is often complex and prone to inconsistencies among\npathologists. Computer-aided diagnostic approaches have significantly enhanced\nbreast cancer detection, particularly in binary classification (benign vs.\nmalignant). However, these methods face challenges in multiclass\nclassification, leading to frequent mispredictions. In this work, we propose a\nnovel adaptive learning approach for multiclass breast cancer classification\nusing H&E-stained histopathology images. First, we introduce a misprediction\nrisk analysis framework that quantifies and ranks the likelihood of an image\nbeing mislabeled by a classifier. This framework leverages an interpretable\nrisk model that requires only a small number of labeled samples for training.\nNext, we present an adaptive learning strategy that fine-tunes classifiers\nbased on the specific characteristics of a given dataset. This approach\nminimizes misprediction risk, allowing the classifier to adapt effectively to\nthe target workload. We evaluate our proposed solutions on real benchmark\ndatasets, demonstrating that our risk analysis framework more accurately\nidentifies mispredictions compared to existing methods. Furthermore, our\nadaptive learning approach significantly improves the performance of\nstate-of-the-art deep neural network classifiers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12874", "pdf": "https://arxiv.org/pdf/2503.12874", "abs": "https://arxiv.org/abs/2503.12874", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Ke Ma", "Xinfeng Li", "Yihao Huang", "Wei Dong", "Yang Liu", "Xiaochun Cao"], "title": "Evolution-based Region Adversarial Prompt Learning for Robustness Enhancement in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained vision-language models (VLMs), such as CLIP, demonstrate\nimpressive generalization but remain highly vulnerable to adversarial examples\n(AEs). Previous work has explored robust text prompts through adversarial\ntraining, achieving some improvement in both robustness and generalization.\nHowever, they primarily rely on singlegradient direction perturbations (e.g.,\nPGD) to generate AEs, which lack diversity, resulting in limited improvement in\nadversarial robustness. To address these limitations, we propose an\nevolution-based region adversarial prompt tuning method called ER-APT, which\ncombines gradient methods with genetic evolution to generate more diverse and\nchallenging AEs. In each training iteration, we first generate AEs using\ntraditional gradient-based methods. Subsequently, a genetic evolution mechanism\nincorporating selection, mutation, and crossover is applied to optimize the\nAEs, ensuring a broader and more aggressive perturbation distribution.The final\nevolved AEs are used for prompt tuning, achieving region-based adversarial\noptimization instead of conventional single-point adversarial prompt tuning. We\nalso propose a dynamic loss weighting method to adjust prompt learning\nefficiency for accuracy and robustness. Experimental evaluations on various\nbenchmark datasets demonstrate the superiority of our proposed method,\noutperforming stateof-the-art APT methods. The code is released at\nhttps://github.com/jiaxiaojunQAQ/ER-APT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12905", "pdf": "https://arxiv.org/pdf/2503.12905", "abs": "https://arxiv.org/abs/2503.12905", "authors": ["Yuanbin Qian", "Shuhan Ye", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian", "Jiafei Wu"], "title": "UCF-Crime-DVS: A Novel Event-Based Dataset for Video Anomaly Detection with Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by AAAI 2025", "summary": "Video anomaly detection plays a significant role in intelligent surveillance\nsystems. To enhance model's anomaly recognition ability, previous works have\ntypically involved RGB, optical flow, and text features. Recently, dynamic\nvision sensors (DVS) have emerged as a promising technology, which capture\nvisual information as discrete events with a very high dynamic range and\ntemporal resolution. It reduces data redundancy and enhances the capture\ncapacity of moving objects compared to conventional camera. To introduce this\nrich dynamic information into the surveillance field, we created the first DVS\nvideo anomaly detection benchmark, namely UCF-Crime-DVS. To fully utilize this\nnew data modality, a multi-scale spiking fusion network (MSF) is designed based\non spiking neural networks (SNNs). This work explores the potential application\nof dynamic information from event data in video anomaly detection. Our\nexperiments demonstrate the effectiveness of our framework on UCF-Crime-DVS and\nits superior performance compared to other models, establishing a new baseline\nfor SNN-based weakly supervised video anomaly detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12914", "pdf": "https://arxiv.org/pdf/2503.12914", "abs": "https://arxiv.org/abs/2503.12914", "authors": ["Zhuoqun Su", "Huimin Lu", "Shuaifeng Jiao", "Junhao Xiao", "Yaonan Wang", "Xieyuanli Chen"], "title": "Efficient Multimodal 3D Object Detector via Instance-Level Contrastive Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal 3D object detectors leverage the strengths of both geometry-aware\nLiDAR point clouds and semantically rich RGB images to enhance detection\nperformance. However, the inherent heterogeneity between these modalities,\nincluding unbalanced convergence and modal misalignment, poses significant\nchallenges. Meanwhile, the large size of the detection-oriented feature also\nconstrains existing fusion strategies to capture long-range dependencies for\nthe 3D detection tasks. In this work, we introduce a fast yet effective\nmultimodal 3D object detector, incorporating our proposed Instance-level\nContrastive Distillation (ICD) framework and Cross Linear Attention Fusion\nModule (CLFM). ICD aligns instance-level image features with LiDAR\nrepresentations through object-aware contrastive distillation, ensuring\nfine-grained cross-modal consistency. Meanwhile, CLFM presents an efficient and\nscalable fusion strategy that enhances cross-modal global interactions within\nsizable multimodal BEV features. Extensive experiments on the KITTI and\nnuScenes 3D object detection benchmarks demonstrate the effectiveness of our\nmethods. Notably, our 3D object detector outperforms state-of-the-art (SOTA)\nmethods while achieving superior efficiency. The implementation of our method\nhas been released as open-source at: https://github.com/nubot-nudt/ICD-Fusion.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12927", "pdf": "https://arxiv.org/pdf/2503.12927", "abs": "https://arxiv.org/abs/2503.12927", "authors": ["Huangwei Chen", "Zhu Zhu", "Zhenyu Yan", "Yifei Chen", "Mingyang Ding", "Chenlei Li", "Feiwei Qin"], "title": "MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification Assisted with Textual Description Generation", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 7 figures", "summary": "Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12968", "pdf": "https://arxiv.org/pdf/2503.12968", "abs": "https://arxiv.org/abs/2503.12968", "authors": ["Guanhua Ding", "Yuxuan Xia", "Runwei Guan", "Qinchen Wu", "Tao Huang", "Weiping Ding", "Jinping Sun", "Guoqiang Mao"], "title": "OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as\nit enables robust perception, navigation, and planning in complex environments.\nWhile deep learning-based solutions have demonstrated impressive 3D MOT\nperformance, model-based approaches remain appealing for their simplicity,\ninterpretability, and data efficiency. Conventional model-based trackers\ntypically rely on random vector-based Bayesian filters within the\ntracking-by-detection (TBD) framework but face limitations due to heuristic\ndata association and track management schemes. In contrast, random finite set\n(RFS)-based Bayesian filtering handles object birth, survival, and death in a\ntheoretically sound manner, facilitating interpretability and parameter tuning.\nIn this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs\nan optimized Poisson multi-Bernoulli (PMB) filter while incorporating several\nkey innovative designs within the TBD framework. Specifically, we propose a\nmeasurement-driven hybrid adaptive birth model for improved track\ninitialization, employ adaptive detection probability parameters to effectively\nmaintain tracks for occluded objects, and optimize density pruning and track\nextraction modules to further enhance overall tracking performance. Extensive\nevaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior\ntracking accuracy compared with state-of-the-art methods, thereby establishing\na new benchmark for model-based 3D MOT and offering valuable insights for\nfuture research on RFS-based trackers in autonomous driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13025", "pdf": "https://arxiv.org/pdf/2503.13025", "abs": "https://arxiv.org/abs/2503.13025", "authors": ["ChangHee Yang", "Hyeonseop Song", "Seokhun Choi", "Seungwoo Lee", "Jaechul Kim", "Hoseok Do"], "title": "PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data", "categories": ["cs.CV", "cs.AI"], "comment": "The first three authors contributed equally to this work", "summary": "Despite considerable efforts to enhance the generalization of 3D pose\nestimators without costly 3D annotations, existing data augmentation methods\nstruggle in real world scenarios with diverse human appearances and complex\nposes. We propose PoseSyn, a novel data synthesis framework that transforms\nabundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn\ncomprises two key components: Error Extraction Module (EEM), which identifies\nchallenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),\nwhich synthesizes motion sequences around the challenging poses. Then, by\ngenerating realistic 3D training data via a human animation model aligned with\nchallenging poses and appearances PoseSyn boosts the accuracy of various 3D\npose estimators by up to 14% across real world benchmarks including various\nbackgrounds and occlusions, challenging poses, and multi view scenarios.\nExtensive experiments further confirm that PoseSyn is a scalable and effective\napproach for improving generalization without relying on expensive 3D\nannotations, regardless of the pose estimator's model size or design.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13073", "pdf": "https://arxiv.org/pdf/2503.13073", "abs": "https://arxiv.org/abs/2503.13073", "authors": ["Zhicheng Zhao", "Jinquan Yan", "Chenglong Li", "Xiao Wang", "Jin Tang"], "title": "DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with Adaptive State Space Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Optical remote sensing image dehazing presents significant challenges due to\nits extensive spatial scale and highly non-uniform haze distribution, which\ntraditional single-image dehazing methods struggle to address effectively.\nWhile Synthetic Aperture Radar (SAR) imagery offers inherently haze-free\nreference information for large-scale scenes, existing SAR-guided dehazing\napproaches face two critical limitations: the integration of SAR information\noften diminishes the quality of haze-free regions, and the instability of\nfeature quality further exacerbates cross-modal domain shift. To overcome these\nchallenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built\non a progressive haze decoupling fusion strategy. Our approach incorporates two\nkey innovations: a Haze Perception and Decoupling Module (HPDM) that\ndynamically identifies haze-affected regions through optical-SAR difference\nanalysis, and a Progressive Fusion Module (PFM) that mitigates domain shift\nthrough a two-stage fusion process based on feature quality assessment. To\nfacilitate research in this domain, we present MRSHaze, a large-scale benchmark\ndataset comprising 8,000 pairs of temporally synchronized, precisely\ngeo-registered SAR-optical images with high resolution and diverse haze\nconditions. Extensive experiments demonstrate that DehazeMamba significantly\noutperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR\nand substantial enhancements in downstream tasks such as semantic segmentation.\nThe dataset is available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13074", "pdf": "https://arxiv.org/pdf/2503.13074", "abs": "https://arxiv.org/abs/2503.13074", "authors": ["Shaolin Su", "Josep M. Rocafort", "Danna Xue", "David Serrano-Lozano", "Lei Sun", "Javier Vazquez-Corral"], "title": "Rethinking Image Evaluation in Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100\\%\ntrustworthy for model evaluations? How does GT quality affect this evaluation?\nAnd how to make fair evaluations if there exist imperfect GTs? To answer these\nquestions, this paper presents two main contributions. First, by systematically\nanalyzing seven state-of-the-art SR models across three real-world SR datasets,\nwe show that SR performances can be consistently affected across models by\nlow-quality GTs, and models can perform quite differently when GT quality is\ncontrolled. Second, we propose a novel perceptual quality metric, Relative\nQuality Index (RQI), that measures the relative quality discrepancy of image\npairs, thus issuing the biased evaluations caused by unreliable GTs. Our\nproposed model achieves significantly better consistency with human opinions.\nWe expect our work to provide insights for the SR community on how future\ndatasets, models, and metrics should be developed.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13134", "pdf": "https://arxiv.org/pdf/2503.13134", "abs": "https://arxiv.org/abs/2503.13134", "authors": ["Prakhar Bhardwaj", "Sheethal Bhat", "Andreas Maier"], "title": "Enhancing zero-shot learning in medical imaging: integrating clip with advanced techniques for improved chest x-ray analysis", "categories": ["cs.CV"], "comment": null, "summary": "Due to the large volume of medical imaging data, advanced AI methodologies\nare needed to assist radiologists in diagnosing thoracic diseases from chest\nX-rays (CXRs). Existing deep learning models often require large, labeled\ndatasets, which are scarce in medical imaging due to the time-consuming and\nexpert-driven annotation process. In this paper, we extend the existing\napproach to enhance zero-shot learning in medical imaging by integrating\nContrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),\nresulting in our proposed model, MoCoCLIP. Our method addresses challenges\nposed by class-imbalanced and unlabeled datasets, enabling improved detection\nof pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset\ndemonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,\nachieving relative improvement of approximately 6.5%. Furthermore, on the\nCheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,\nachieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,\nhighlighting its enhanced generalization capabilities on unseen data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13156", "pdf": "https://arxiv.org/pdf/2503.13156", "abs": "https://arxiv.org/abs/2503.13156", "authors": ["Zakariae Zrimek", "Youssef Mourchid", "Mohammed El Hassouni"], "title": "DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph Knowledge Distillation for Gait Disorders Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13184", "pdf": "https://arxiv.org/pdf/2503.13184", "abs": "https://arxiv.org/abs/2503.13184", "authors": ["Yuanze Li", "Shihao Yuan", "Haolin Wang", "Qizhang Li", "Ming Liu", "Chen Xu", "Guangming Shi", "Wangmeng Zuo"], "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process", "categories": ["cs.CV"], "comment": null, "summary": "Although recent methods have tried to introduce large multimodal models\n(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD\nfield is far inferior to that for general purposes. We summarize the main\nreasons for this gap into two aspects. On one hand, general-purpose LMMs lack\ncognition of defects in the visual modality, thereby failing to sufficiently\nfocus on defect areas. Therefore, we propose to modify the AnyRes structure of\nthe LLaVA model, providing the potential anomalous areas identified by existing\nIAD models to the LMMs. On the other hand, existing methods mainly focus on\nidentifying defects by learning defect patterns or comparing with normal\nsamples, yet they fall short of understanding the causes of these defects.\nConsidering that the generation of defects is closely related to the\nmanufacturing process, we propose a manufacturing-driven IAD paradigm. An\ninstruction-tuning dataset for IAD (InstructIAD) and a data organization\napproach for Chain-of-Thought with manufacturing (CoT-M) are designed to\nleverage the manufacturing process for IAD. Based on the above two\nmodifications, we present Triad, a novel LMM-based method incorporating an\nexpert-guided region-of-interest tokenizer and manufacturing process for\nindustrial anomaly detection. Extensive experiments show that our Triad not\nonly demonstrates competitive performance against current LMMs but also\nachieves further improved accuracy when equipped with manufacturing processes.\nSource code, training data, and pre-trained models will be publicly available\nat https://github.com/tzjtatata/Triad.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13272", "pdf": "https://arxiv.org/pdf/2503.13272", "abs": "https://arxiv.org/abs/2503.13272", "authors": ["Katja Schwarz", "Norman Mueller", "Peter Kontschieder"], "title": "Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing consistent and photorealistic 3D scenes is an open problem in\ncomputer vision. Video diffusion models generate impressive videos but cannot\ndirectly synthesize 3D representations, i.e., lack 3D consistency in the\ngenerated sequences. In addition, directly training generative 3D models is\nchallenging due to a lack of 3D training data at scale. In this work, we\npresent Generative Gaussian Splatting (GGS) -- a novel approach that integrates\na 3D representation with a pre-trained latent video diffusion model.\nSpecifically, our model synthesizes a feature field parameterized via 3D\nGaussian primitives. The feature field is then either rendered to feature maps\nand decoded into multi-view images, or directly upsampled into a 3D radiance\nfield. We evaluate our approach on two common benchmark datasets for scene\nsynthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model\nsignificantly improves both the 3D consistency of the generated multi-view\nimages, and the quality of the generated 3D scenes over all relevant baselines.\nCompared to a similar model without 3D representation, GGS improves FID on the\ngenerated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:\nhttps://katjaschwarz.github.io/ggs/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13360", "pdf": "https://arxiv.org/pdf/2503.13360", "abs": "https://arxiv.org/abs/2503.13360", "authors": ["Hai-Long Sun", "Zhun Sun", "Houwen Peng", "Han-Jia Ye"], "title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The project page is available at\n  https://sun-hailong.github.io/projects/TVC", "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13399", "pdf": "https://arxiv.org/pdf/2503.13399", "abs": "https://arxiv.org/abs/2503.13399", "authors": ["James Burgess", "Jeffrey J Nirschl", "Laura Bravo-Sánchez", "Alejandro Lozano", "Sanket Rajan Gupte", "Jesus G. Galaz-Montoya", "Yuhui Zhang", "Yuchang Su", "Disha Bhowmik", "Zachary Coman", "Sarina M. Hasan", "Alexandra Johannesson", "William D. Leineweber", "Malvika G Nair", "Ridhi Yarlagadda", "Connor Zuraski", "Wah Chiu", "Sarah Cohen", "Jan N. Hansen", "Manuel D Leonetti", "Chad Liu", "Emma Lundberg", "Serena Yeung-Levy"], "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "q-bio.CB"], "comment": "CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa", "summary": "Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13435", "pdf": "https://arxiv.org/pdf/2503.13435", "abs": "https://arxiv.org/abs/2503.13435", "authors": ["Ling Yang", "Kaixin Zhu", "Juanxi Tian", "Bohan Zeng", "Mingbao Lin", "Hongjuan Pei", "Wentao Zhang", "Shuicheng Yan"], "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/WideRange4D", "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13444", "pdf": "https://arxiv.org/pdf/2503.13444", "abs": "https://arxiv.org/abs/2503.13444", "authors": ["Ye Liu", "Kevin Qinghong Lin", "Chang Wen Chen", "Mike Zheng Shou"], "title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://videomind.github.io/", "summary": "Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12180", "pdf": "https://arxiv.org/pdf/2503.12180", "abs": "https://arxiv.org/abs/2503.12180", "authors": ["Yuhang Peng", "Sidong Wang", "Jihaoyu Yang", "Shilong Li", "Han Wang", "Jiangtao Gong"], "title": "Bench2FreeAD: A Benchmark for Vision-based End-to-end Navigation in Unstructured Robotic Environments", "categories": ["cs.RO", "cs.CV", "68T45"], "comment": "7 pages, 9 figures", "summary": "Most current end-to-end (E2E) autonomous driving algorithms are built on\nstandard vehicles in structured transportation scenarios, lacking exploration\nof robot navigation for unstructured scenarios such as auxiliary roads, campus\nroads, and indoor settings. This paper investigates E2E robot navigation in\nunstructured road environments. First, we introduce two data collection\npipelines - one for real-world robot data and another for synthetic data\ngenerated using the Isaac Sim simulator, which together produce an unstructured\nrobotics navigation dataset -- FreeWorld Dataset. Second, we fine-tuned an\nefficient E2E autonomous driving model -- VAD -- using our datasets to validate\nthe performance and adaptability of E2E autonomous driving models in these\nenvironments. Results demonstrate that fine-tuning through our datasets\nsignificantly enhances the navigation potential of E2E autonomous driving\nmodels in unstructured robotic environments. Thus, this paper presents the\nfirst dataset targeting E2E robot navigation tasks in unstructured scenarios,\nand provides a benchmark based on vision-based E2E autonomous driving\nalgorithms to facilitate the development of E2E navigation technology for\nlogistics and service robots. The project is available on Github.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12466", "pdf": "https://arxiv.org/pdf/2503.12466", "abs": "https://arxiv.org/abs/2503.12466", "authors": ["Jiahang Cao", "Qiang Zhang", "Hanzhong Guo", "Jiaxu Wang", "Hao Cheng", "Renjing Xu"], "title": "Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to ICLR 2025 Generative Models for Robot Learning Workshop", "summary": "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps://github.com/AndyCao1125/MCDP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12553", "pdf": "https://arxiv.org/pdf/2503.12553", "abs": "https://arxiv.org/abs/2503.12553", "authors": ["Xianzu Wu", "Zhenxin Ai", "Harry Yang", "Ser-Nam Lim", "Jun Liu", "Huan Wang"], "title": "Niagara: Normal-Integrated Geometric Affine Field for Scene Reconstruction from a Single View", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in single-view 3D scene reconstruction have highlighted the\nchallenges in capturing fine geometric details and ensuring structural\nconsistency, particularly in high-fidelity outdoor scene modeling. This paper\npresents Niagara, a new single-view 3D scene reconstruction framework that can\nfaithfully reconstruct challenging outdoor scenes from a single input image for\nthe first time.\n  Our approach integrates monocular depth and normal estimation as input, which\nsubstantially improves its ability to capture fine details, mitigating common\nissues like geometric detail loss and deformation.\n  Additionally, we introduce a geometric affine field (GAF) and 3D\nself-attention as geometry-constraint, which combines the structural properties\nof explicit geometry with the adaptability of implicit feature fields, striking\na balance between efficient rendering and high-fidelity reconstruction.\n  Our framework finally proposes a specialized encoder-decoder architecture,\nwhere a depth-based 3D Gaussian decoder is proposed to predict 3D Gaussian\nparameters, which can be used for novel view synthesis. Extensive results and\nanalyses suggest that our Niagara surpasses prior SoTA approaches such as\nFlash3D in both single-view and dual-view settings, significantly enhancing the\ngeometric accuracy and visual fidelity, especially in outdoor scenes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12937", "pdf": "https://arxiv.org/pdf/2503.12937", "abs": "https://arxiv.org/abs/2503.12937", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Huanjin Yao", "Shunyu Liu", "Xikun Zhang", "Shijian Lu", "Dacheng Tao"], "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13021", "pdf": "https://arxiv.org/pdf/2503.13021", "abs": "https://arxiv.org/abs/2503.13021", "authors": ["Omri Suissa", "Muhiim Ali", "Ariana Azarbal", "Hui Shen", "Shekhar Pradhan"], "title": "Dynamic Relation Inference via Verb Embeddings", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13057", "pdf": "https://arxiv.org/pdf/2503.13057", "abs": "https://arxiv.org/abs/2503.13057", "authors": ["Robin Zbinden", "Nina van Tiel", "Gencer Sumbul", "Chiara Vanalli", "Benjamin Kellenberger", "Devis Tuia"], "title": "MaskSDM with Shapley values to improve flexibility, robustness, and explainability in species distribution modeling", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13236", "pdf": "https://arxiv.org/pdf/2503.13236", "abs": "https://arxiv.org/abs/2503.13236", "authors": ["Ihab Asaad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Gradient Extrapolation for Debiased Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine learning classification models trained with empirical risk\nminimization (ERM) often inadvertently rely on spurious correlations. When\nabsent in the test data, these unintended associations between non-target\nattributes and target labels lead to poor generalization. This paper addresses\nthis problem from a model optimization perspective and proposes a novel method,\nGradient Extrapolation for Debiased Representation Learning (GERNE), designed\nto learn debiased representations in both known and unknown attribute training\ncases. GERNE uses two distinct batches with different amounts of spurious\ncorrelations to define the target gradient as the linear extrapolation of two\ngradients computed from each batch's loss. It is demonstrated that the\nextrapolated gradient, if directed toward the gradient of the batch with fewer\namount of spurious correlation, can guide the training process toward learning\na debiased model. GERNE can serve as a general framework for debiasing with\nmethods, such as ERM, reweighting, and resampling, being shown as special\ncases. The theoretical upper and lower bounds of the extrapolation factor are\nderived to ensure convergence. By adjusting this factor, GERNE can be adapted\nto maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The\nproposed approach is validated on five vision and one NLP benchmarks,\ndemonstrating competitive and often superior performance compared to\nstate-of-the-art baseline methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13277", "pdf": "https://arxiv.org/pdf/2503.13277", "abs": "https://arxiv.org/abs/2503.13277", "authors": ["Alfred Simbun", "Suresh Kumar"], "title": "Artificial Intelligence-Driven Prognostic Classification of COVID-19 Using Chest X-rays: A Deep Learning Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "27 pages, 6 figures, 10 tables", "summary": "Background: The COVID-19 pandemic has overwhelmed healthcare systems,\nemphasizing the need for AI-driven tools to assist in rapid and accurate\npatient prognosis. Chest X-ray imaging is a widely available diagnostic tool,\nbut existing methods for prognosis classification lack scalability and\nefficiency. Objective: This study presents a high-accuracy deep learning model\nfor classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest\nX-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a\ndataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained\nand validated a deep learning model leveraging Convolutional Neural Networks\n(CNNs). The model was evaluated on an unseen dataset to measure accuracy,\nprecision, and recall. Results: Our model achieved an average accuracy of 97%,\nwith specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When\nclassifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),\n95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's\npotential for real-world clinical applications, aiding in faster\ndecision-making and improved resource allocation. Conclusion: AI-driven\nprognosis classification using deep learning can significantly enhance COVID-19\npatient management, enabling early intervention and efficient triaging. Our\nstudy provides a scalable, high-accuracy AI framework for integrating deep\nlearning into routine clinical workflows. Future work should focus on expanding\ndatasets, external validation, and regulatory compliance to facilitate clinical\nadoption.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13369", "pdf": "https://arxiv.org/pdf/2503.13369", "abs": "https://arxiv.org/abs/2503.13369", "authors": ["Wan Ju Kang", "Eunki Kim", "Na Min An", "Sangryul Kim", "Haemin Choi", "Ki Hoon Kwak", "James Thorne"], "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "37 pages, 10 figures, 21 tables", "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11750", "pdf": "https://arxiv.org/pdf/2503.11750", "abs": "https://arxiv.org/abs/2503.11750", "authors": ["Shuyang Hao", "Yiwei Wang", "Bryan Hooi", "Jun Liu", "Muhao Chen", "Zi Huang", "Yujun Cai"], "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11780", "pdf": "https://arxiv.org/pdf/2503.11780", "abs": "https://arxiv.org/abs/2503.11780", "authors": ["Tianyi Zhao", "Boyang Liu", "Yanglei Gao", "Yiming Sun", "Maoxun Yuan", "Xingxing Wei"], "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem that the\ndecreased feature extraction capability in multi-modal joint learning. This\nleads to an unreasonable but prevalent phenomenon--Fusion Degradation, which\nhinders the performance improvement of the MMOD model. Motivated by this, in\nthis paper, we introduce linear probing evaluation to the multi-modal detectors\nand rethink the multi-modal object detection task from the mono-modality\nlearning perspective. Therefore, we construct an novel framework called\nM$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method\nand the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework\nfacilitates the sufficient learning of mono-modality during multi-modal joint\ntraining and explores a lightweight yet effective feature fusion manner to\nachieve superior object detection performance. Extensive experiments conducted\non three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates\nthe Fusion Degradation phenomenon and outperforms the previous SOTA detectors.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11787", "pdf": "https://arxiv.org/pdf/2503.11787", "abs": "https://arxiv.org/abs/2503.11787", "authors": ["Samuel W. Remedios", "Shuwen Wei", "Shuo Han", "Jinwei Zhang", "Aaron Carass", "Kurt G. Schilling", "Dzung L. Pham", "Jerry L. Prince", "Blake E. Dewey"], "title": "ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In clinical imaging, magnetic resonance (MR) image volumes are often acquired\nas stacks of 2D slices, permitting decreased scan times, improved\nsignal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.\nWhile this is sufficient for clinical evaluation, automated algorithms designed\nfor 3D analysis perform sub-optimally on 2D-acquired scans, especially those\nwith thick slices and gaps between slices. Super-resolution (SR) methods aim to\naddress this problem, but previous methods do not address all of the following:\nslice profile shape estimation, slice gap, domain shift, and non-integer /\narbitrary upsampling factors. In this paper, we propose ECLARE (Efficient\nCross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method\nthat addresses each of these factors. ECLARE estimates the slice profile from\nthe 2D-acquired multi-slice MR volume, trains a network to learn the mapping\nfrom low-resolution to high-resolution in-plane patches from the same volume,\nand performs SR with anti-aliasing. We compared ECLARE to cubic B-spline\ninterpolation, SMORE, and other contemporary SR methods. We used realistic and\nrepresentative simulations so that quantitative performance against a ground\ntruth could be computed, and ECLARE outperformed all other methods in both\nsignal recovery and downstream tasks. On real data for which there is no ground\ntruth, ECLARE demonstrated qualitative superiority over other methods as well.\nImportantly, as ECLARE does not use external training data it cannot suffer\nfrom domain shift between training and testing. Our code is open-source and\navailable at https://www.github.com/sremedios/eclare.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11930", "pdf": "https://arxiv.org/pdf/2503.11930", "abs": "https://arxiv.org/abs/2503.11930", "authors": ["Jingxuan Zhang", "Robert J. Hart", "Ziqian Bi", "Shiaofen Fang", "Susan Walsh"], "title": "Generating a Biometrically Unique and Realistic Iris Database", "categories": ["cs.CV", "cs.LG"], "comment": "for associated iris database, see\n  https://huggingface.co/datasets/fatdove/Iris_Database", "summary": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11937", "pdf": "https://arxiv.org/pdf/2503.11937", "abs": "https://arxiv.org/abs/2503.11937", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11945", "pdf": "https://arxiv.org/pdf/2503.11945", "abs": "https://arxiv.org/abs/2503.11945", "authors": ["Naresh Kumar Devulapally", "Mingzhen Huang", "Vishal Asnani", "Shruti Agarwal", "Siwei Lyu", "Vishnu Suresh Lokhande"], "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Invisible watermarking of AI-generated images can help with copyright\nprotection, enabling detection and identification of AI-generated media. In\nthis work, we present a novel approach to watermark images of T2I Latent\nDiffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we\nenable watermarking in selected objects or parts of the image, offering greater\nflexibility compared to traditional full-image watermarking. Our method\nleverages the text encoder's compatibility across various LDMs, allowing\nplug-and-play integration for different LDMs. Moreover, introducing the\nwatermark early in the encoding stage improves robustness to adversarial\nperturbations in later stages of the pipeline. Our approach achieves $99\\%$ bit\naccuracy ($48$ bits) with a $10^5 \\times$ reduction in model parameters,\nenabling efficient watermarking.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11953", "pdf": "https://arxiv.org/pdf/2503.11953", "abs": "https://arxiv.org/abs/2503.11953", "authors": ["Priyanka Mandikal", "Tushar Nagarajan", "Alex Stoken", "Zihui Xue", "Kristen Grauman"], "title": "SPOC: Spatially-Progressing Object State Change Segmentation in Video", "categories": ["cs.CV"], "comment": null, "summary": "Object state changes in video reveal critical information about human and\nagent activity. However, existing methods are limited to temporal localization\nof when the object is in its initial state (e.g., the unchopped avocado) versus\nwhen it has completed a state change (e.g., the chopped avocado), which limits\napplicability for any task requiring detailed information about the progress of\nthe actions and its spatial localization. We propose to deepen the problem by\nintroducing the spatially-progressing object state change segmentation task.\nThe goal is to segment at the pixel-level those regions of an object that are\nactionable and those that are transformed. We introduce the first model to\naddress this task, designing a VLM-based pseudo-labeling approach, state-change\ndynamics constraints, and a novel WhereToChange benchmark built on in-the-wild\nInternet videos. Experiments on two datasets validate both the challenge of the\nnew task as well as the promise of our model for localizing exactly where and\nhow fast objects are changing in video. We further demonstrate useful\nimplications for tracking activity progress to benefit robotic agents. Project\npage: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz Öztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11969", "pdf": "https://arxiv.org/pdf/2503.11969", "abs": "https://arxiv.org/abs/2503.11969", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "The registration between the pre-operative model and the intra-operative\nsurface is crucial in image-guided liver surgery, as it facilitates the\neffective use of pre-operative information during the procedure. However, the\nintra-operative surface, usually represented as a point cloud, often has\nlimited coverage, especially in laparoscopic surgery, and is prone to holes and\nnoise, posing significant challenges for registration methods. Point cloud\ncompletion methods have the potential to alleviate these issues. Thus, we\nexplore six state-of-the-art point cloud completion methods to identify the\noptimal completion method for liver surgery applications. We focus on a\npatient-specific approach for liver point cloud completion from a partial liver\nsurface under three cases: canonical pose, non-canonical pose, and canonical\npose with noise. The transformer-based method, AdaPoinTr, outperforms all other\nmethods to generate a complete point cloud from the given partial liver point\ncloud under the canonical pose. On the other hand, our findings reveal\nsubstantial performance degradation of these methods under non-canonical poses\nand noisy settings, highlighting the limitations of these methods, which\nsuggests the need for a robust point completion method for its application in\nimage-guided liver surgery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11981", "pdf": "https://arxiv.org/pdf/2503.11981", "abs": "https://arxiv.org/abs/2503.11981", "authors": ["Utkarsh Nath", "Rajeev Goel", "Rahul Khurana", "Kyle Min", "Mark Ollila", "Pavan Turaga", "Varun Jampani", "Tejaswi Gowda"], "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation saw dramatic advances in recent years by leveraging\nText-to-Image models. However, most existing techniques struggle with\ncompositional prompts, which describe multiple objects and their spatial\nrelationships. They often fail to capture fine-grained inter-object\ninteractions. We introduce DecompDreamer, a Gaussian splatting-based training\nroutine designed to generate high-quality 3D compositions from such complex\nprompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose\nscenes into structured components and their relationships. We propose a\nprogressive optimization strategy that first prioritizes joint relationship\nmodeling before gradually shifting toward targeted object refinement. Our\nqualitative and quantitative evaluations against state-of-the-art text-to-3D\nmodels demonstrate that DecompDreamer effectively generates intricate 3D\ncompositions with superior object disentanglement, offering enhanced control\nand flexibility in 3D generation. Project page :\nhttps://decompdreamer3d.github.io", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12052", "pdf": "https://arxiv.org/pdf/2503.12052", "abs": "https://arxiv.org/abs/2503.12052", "authors": ["Zhiyao Sun", "Yu-Hui Wen", "Matthieu Lin", "Ho-Jui Fang", "Sheng Ye", "Tian Lv", "Yong-Jin Liu"], "title": "Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://human-tailor.github.com", "summary": "Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12127", "pdf": "https://arxiv.org/pdf/2503.12127", "abs": "https://arxiv.org/abs/2503.12127", "authors": ["Tobia Poppi", "Tejaswi Kasarla", "Pascal Mettes", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Hyperbolic Safety-Aware Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "CVPR 2025", "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettré", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12218", "pdf": "https://arxiv.org/pdf/2503.12218", "abs": "https://arxiv.org/abs/2503.12218", "authors": ["Chengxuan Qian", "Kai Han", "Siqi Ma", "Chongwen Lyu", "Zhenlong Yuan", "Jun Chen", "Zhe Liu"], "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12232", "pdf": "https://arxiv.org/pdf/2503.12232", "abs": "https://arxiv.org/abs/2503.12232", "authors": ["Yan Jiang", "Hao Yu", "Xu Cheng", "Haoyu Chen", "Zhaodong Sun", "Guoying Zhao"], "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12249", "pdf": "https://arxiv.org/pdf/2503.12249", "abs": "https://arxiv.org/abs/2503.12249", "authors": ["Boyu Chen", "Ameenat L. Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "categories": ["cs.CV"], "comment": null, "summary": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging\ntechnique with great potential for diagnosing anterior uveitis, a\nvision-threatening ocular inflammatory condition. A hallmark of this condition\nis the presence of inflammatory cells in the eye's anterior chamber, and\ndetecting these cells using AS-OCT images has attracted research interest.\nWhile recent efforts aim to replace manual cell detection with automated\ncomputer vision approaches, detecting extremely small (minuscule) objects in\nhigh-resolution images, such as AS-OCT, poses substantial challenges: (1) each\ncell appears as a minuscule particle, representing less than 0.005\\% of the\nimage, making the detection difficult, and (2) OCT imaging introduces\npixel-level noise that can be mistaken for cells, leading to false positive\ndetections. To overcome these challenges, we propose a minuscule cell detection\nframework through a progressive field-of-view focusing strategy. This strategy\nsystematically refines the detection scope from the whole image to a target\nregion where cells are likely to be present, and further to minuscule regions\npotentially containing individual cells. Our framework consists of two modules.\nFirst, a Field-of-Focus module uses a vision foundation model to segment the\ntarget region. Subsequently, a Fine-grained Object Detection module introduces\na specialized Minuscule Region Proposal followed by a Spatial Attention Network\nto distinguish individual cells from noise within the segmented region.\nExperimental results demonstrate that our framework outperforms\nstate-of-the-art methods for cell detection, providing enhanced efficacy for\nclinical applications. Our code is publicly available at:\nhttps://github.com/joeybyc/MCD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12261", "pdf": "https://arxiv.org/pdf/2503.12261", "abs": "https://arxiv.org/abs/2503.12261", "authors": ["R. Gnana Praveen", "Jahangir Alam"], "title": "Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Submission to valence arousal track of 8th ABAW competition. arXiv\n  admin note: substantial text overlap with arXiv:2403.13659", "summary": "Multimodal emotion recognition has recently drawn a lot of interest in\naffective computing as it has immense potential to outperform isolated unimodal\napproaches. Audio and visual modalities are two predominant contact-free\nchannels in videos, which are often expected to carry a complementary\nrelationship with each other. However, audio and visual channels may not always\nbe complementary with each other, resulting in poor audio-visual feature\nrepresentations, thereby degrading the performance of the system. In this\npaper, we propose a flexible audio-visual fusion model that can adapt to weak\ncomplementary relationships using a gated attention mechanism. Specifically, we\nextend the recursive joint cross-attention model by introducing gating\nmechanism in every iteration to control the flow of information between the\ninput features and the attended features depending on the strength of their\ncomplementary relationship. For instance, if the modalities exhibit strong\ncomplementary relationships, the gating mechanism chooses cross-attended\nfeatures, otherwise non-attended features. To further improve the performance\nof the system, we further introduce stage gating mechanism, which is used to\ncontrol the flow of information across the gated outputs of each iteration.\nTherefore, the proposed model improves the performance of the system even when\nthe audio and visual modalities do not have a strong complementary relationship\nwith each other by adding more flexibility to the recursive joint cross\nattention mechanism. The proposed model has been evaluated on the challenging\nAffwild2 dataset and significantly outperforms the state-of-the-art fusion\napproaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12281", "pdf": "https://arxiv.org/pdf/2503.12281", "abs": "https://arxiv.org/abs/2503.12281", "authors": ["Paola Natalia Cañas", "Marcos Nieto", "Oihana Otaegui", "Igor Rodríguez"], "title": "Exploration of VLMs for Driver Monitoring Systems Applications", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 16th ITS European Congress, Seville, Spain, 19-21 May\n  2025", "summary": "In recent years, we have witnessed significant progress in emerging deep\nlearning models, particularly Large Language Models (LLMs) and Vision-Language\nModels (VLMs). These models have demonstrated promising results, indicating a\nnew era of Artificial Intelligence (AI) that surpasses previous methodologies.\nTheir extensive knowledge and zero-shot capabilities suggest a paradigm shift\nin developing deep learning solutions, moving from data capturing and algorithm\ntraining to just writing appropriate prompts. While the application of these\ntechnologies has been explored across various industries, including automotive,\nthere is a notable gap in the scientific literature regarding their use in\nDriver Monitoring Systems (DMS). This paper presents our initial approach to\nimplementing VLMs in this domain, utilising the Driver Monitoring Dataset to\nevaluate their performance and discussing their advantages and challenges when\nimplemented in real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12343", "pdf": "https://arxiv.org/pdf/2503.12343", "abs": "https://arxiv.org/abs/2503.12343", "authors": ["Xiaoyu Xiong", "Changyu Hu", "Chunru Lin", "Pingchuan Ma", "Chuang Gan", "Tao Du"], "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues", "categories": ["cs.CV"], "comment": null, "summary": "We present TopoGaussian, a holistic, particle-based pipeline for inferring\nthe interior structure of an opaque object from easily accessible photos and\nvideos as input. Traditional mesh-based approaches require tedious and\nerror-prone mesh filling and fixing process, while typically output rough\nboundary surface. Our pipeline combines Gaussian Splatting with a novel,\nversatile particle-based differentiable simulator that simultaneously\naccommodates constitutive model, actuator, and collision, without interference\nwith mesh. Based on the gradients from this simulator, we provide flexible\nchoice of topology representation for optimization, including particle, neural\nimplicit surface, and quadratic surface. The resultant pipeline takes easily\naccessible photos and videos as input and outputs the topology that matches the\nphysical characteristics of the input. We demonstrate the efficacy of our\npipeline on a synthetic dataset and four real-world tasks with 3D-printed\nprototypes. Compared with existing mesh-based method, our pipeline is 5.26x\nfaster on average with improved shape quality. These results highlight the\npotential of our pipeline in 3D vision, soft robotics, and manufacturing\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12381", "pdf": "https://arxiv.org/pdf/2503.12381", "abs": "https://arxiv.org/abs/2503.12381", "authors": ["Ruchika Sharma", "Rudresh Dwivedi"], "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN", "categories": ["cs.CV", "cs.MM"], "comment": "Submiited to journal", "summary": "Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12399", "pdf": "https://arxiv.org/pdf/2503.12399", "abs": "https://arxiv.org/abs/2503.12399", "authors": ["Jiangdong Cai", "Yan Chen", "Zhenrong Shen", "Haotian Jiang", "Honglin Xiong", "Kai Xuan", "Lichi Zhang", "Qian Wang"], "title": "Pathology Image Restoration via Mixture of Prompts", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In digital pathology, acquiring all-in-focus images is essential to\nhigh-quality imaging and high-efficient clinical workflow. Traditional scanners\nachieve this by scanning at multiple focal planes of varying depths and then\nmerging them, which is relatively slow and often struggles with complex tissue\ndefocus. Recent prevailing image restoration technique provides a means to\nrestore high-quality pathology images from scans of single focal planes.\nHowever, existing image restoration methods are inadequate, due to intricate\ndefocus patterns in pathology images and their domain-specific semantic\ncomplexities. In this work, we devise a two-stage restoration solution\ncascading a transformer and a diffusion model, to benefit from their powers in\npreserving image fidelity and perceptual quality, respectively. We particularly\npropose a novel mixture of prompts for the two-stage solution. Given initial\nprompt that models defocus in microscopic imaging, we design two prompts that\ndescribe the high-level image semantics from pathology foundation model and the\nfine-grained tissue structures via edge extraction. We demonstrate that, by\nfeeding the prompt mixture to our method, we can restore high-quality pathology\nimages from single-focal-plane scans, implying high potentials of the mixture\nof prompts to clinical usage. Code will be publicly available at\nhttps://github.com/caijd2000/MoP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12401", "pdf": "https://arxiv.org/pdf/2503.12401", "abs": "https://arxiv.org/abs/2503.12401", "authors": ["Jianwei Zhao", "Xin Li", "Fan Yang", "Qiang Zhai", "Ao Luo", "Yang Zhao", "Hong Cheng", "Huazhu Fu"], "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12441", "pdf": "https://arxiv.org/pdf/2503.12441", "abs": "https://arxiv.org/abs/2503.12441", "authors": ["Yuda Zou", "Zelong Liu", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "title": "Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "Crowd counting and localization are important in applications such as public\nsecurity and traffic management. Existing methods have achieved impressive\nresults thanks to extensive laborious annotations. This paper propose a novel\npoint-localization-based semi-supervised crowd counting and localization method\ntermed Consistent-Point. We identify and address two inconsistencies of\npseudo-points, which have not been adequately explored. To enhance their\nposition consistency, we aggregate the positions of neighboring auxiliary\nproposal-points. Additionally, an instance-wise uncertainty calibration is\nproposed to improve the class consistency of pseudo-points. By generating more\nconsistent pseudo-points, Consistent-Point provides more stable supervision to\nthe training process, yielding improved results. Extensive experiments across\nfive widely used datasets and three different labeled ratio settings\ndemonstrate that our method achieves state-of-the-art performance in crowd\nlocalization while also attaining impressive crowd counting results. The code\nwill be available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12447", "pdf": "https://arxiv.org/pdf/2503.12447", "abs": "https://arxiv.org/abs/2503.12447", "authors": ["Li Yicong"], "title": "Causality Model for Semantic Understanding on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "PhD Thesis", "summary": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12453", "pdf": "https://arxiv.org/pdf/2503.12453", "abs": "https://arxiv.org/abs/2503.12453", "authors": ["Edgar Heinert", "Thomas Gottwald", "Annika Mütze", "Matthias Rottmann"], "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12485", "pdf": "https://arxiv.org/pdf/2503.12485", "abs": "https://arxiv.org/abs/2503.12485", "authors": ["Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Hezhen Hu", "Wengang Zhou", "Houqiang Li"], "title": "Cross-Modal Consistency Learning for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminate background perturbation but\ninevitably suffer from insufficient semantic cues compared to raw RGB videos.\nNevertheless, direct representation learning only from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12490", "pdf": "https://arxiv.org/pdf/2503.12490", "abs": "https://arxiv.org/abs/2503.12490", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Bin Chen", "Zian Guan", "Yuhao Wang", "Xu Jia", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12507", "pdf": "https://arxiv.org/pdf/2503.12507", "abs": "https://arxiv.org/abs/2503.12507", "authors": ["Guangqian Guo", "Yoong Guo", "Xuehui Yu", "Wenbo Li", "Yaoxing Wang", "Shan Gao"], "title": "Segment Any-Quality Images with Generative Latent Space Enhancement", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12527", "pdf": "https://arxiv.org/pdf/2503.12527", "abs": "https://arxiv.org/abs/2503.12527", "authors": ["Yang Yi", "Kunqing Wang", "Jinpu Zhang", "Zhen Tan", "Xiangke Wang", "Hui Shen", "Dewen Hu"], "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12535", "pdf": "https://arxiv.org/pdf/2503.12535", "abs": "https://arxiv.org/abs/2503.12535", "authors": ["Guibiao Liao", "Qing Li", "Zhenyu Bao", "Guoping Qiu", "Kanglin Liu"], "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. The project page is available at\n  https://gbliao.github.io/SPC-GS.github.io/", "summary": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches\nhave shown significant performance with dense input images. However, they\nexhibit poor performance when confronted with sparse inputs, primarily due to\nthe sparse distribution of Gaussian points and insufficient view supervision.\nTo relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based\nGaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)\nRegularization for open-world free view synthesis with sparse inputs.\nSpecifically, SGI provides a dense, scene-layout-based Gaussian distribution by\nutilizing view-changed images generated from the video generation model and\nview-constraint Gaussian points densification. Additionally, SPC mitigates\nlimited view supervision by employing semantic-prompt-based consistency\nconstraints developed by SAM2. This approach leverages available semantics from\ntraining views, serving as instructive prompts, to optimize visually\noverlapping regions in novel views with 2D and 3D consistency constraints.\nExtensive experiments demonstrate the superior performance of SPC-GS across\nReplica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in\nPSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world\nsemantic segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12590", "pdf": "https://arxiv.org/pdf/2503.12590", "abs": "https://arxiv.org/abs/2503.12590", "authors": ["Haoran Feng", "Zehuan Huang", "Lin Li", "Hairong Lv", "Lu Sheng"], "title": "Personalize Anything for Free with Diffusion Transformer", "categories": ["cs.CV"], "comment": "https://fenghora.github.io/Personalize-Anything-Page/", "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose \\textbf{Personalize\nAnything}, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11750", "pdf": "https://arxiv.org/pdf/2503.11750", "abs": "https://arxiv.org/abs/2503.11750", "authors": ["Shuyang Hao", "Yiwei Wang", "Bryan Hooi", "Jun Liu", "Muhao Chen", "Zi Huang", "Yujun Cai"], "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11780", "pdf": "https://arxiv.org/pdf/2503.11780", "abs": "https://arxiv.org/abs/2503.11780", "authors": ["Tianyi Zhao", "Boyang Liu", "Yanglei Gao", "Yiming Sun", "Maoxun Yuan", "Xingxing Wei"], "title": "Rethinking Multi-modal Object Detection from the Perspective of Mono-Modality Feature Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Multi-Modal Object Detection (MMOD), due to its stronger adaptability to\nvarious complex environments, has been widely applied in various applications.\nExtensive research is dedicated to the RGB-IR object detection, primarily\nfocusing on how to integrate complementary features from RGB-IR modalities.\nHowever, they neglect the mono-modality insufficient learning problem that the\ndecreased feature extraction capability in multi-modal joint learning. This\nleads to an unreasonable but prevalent phenomenon--Fusion Degradation, which\nhinders the performance improvement of the MMOD model. Motivated by this, in\nthis paper, we introduce linear probing evaluation to the multi-modal detectors\nand rethink the multi-modal object detection task from the mono-modality\nlearning perspective. Therefore, we construct an novel framework called\nM$^2$D-LIF, which consists of the Mono-Modality Distillation (M$^2$D) method\nand the Local Illumination-aware Fusion (LIF) module. The M$^2$D-LIF framework\nfacilitates the sufficient learning of mono-modality during multi-modal joint\ntraining and explores a lightweight yet effective feature fusion manner to\nachieve superior object detection performance. Extensive experiments conducted\non three MMOD datasets demonstrate that our M$^2$D-LIF effectively mitigates\nthe Fusion Degradation phenomenon and outperforms the previous SOTA detectors.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11781", "pdf": "https://arxiv.org/pdf/2503.11781", "abs": "https://arxiv.org/abs/2503.11781", "authors": ["Artem Nikonorov", "Georgy Perevozchikov", "Andrei Korepanov", "Nancy Mehta", "Mahmoud Afifi", "Egor Ershov", "Radu Timofte"], "title": "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": null, "summary": "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps://github.com/gosha20777/cmKAN", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11787", "pdf": "https://arxiv.org/pdf/2503.11787", "abs": "https://arxiv.org/abs/2503.11787", "authors": ["Samuel W. Remedios", "Shuwen Wei", "Shuo Han", "Jinwei Zhang", "Aaron Carass", "Kurt G. Schilling", "Dzung L. Pham", "Jerry L. Prince", "Blake E. Dewey"], "title": "ECLARE: Efficient cross-planar learning for anisotropic resolution enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In clinical imaging, magnetic resonance (MR) image volumes are often acquired\nas stacks of 2D slices, permitting decreased scan times, improved\nsignal-to-noise ratio, and image contrasts unique to 2D MR pulse sequences.\nWhile this is sufficient for clinical evaluation, automated algorithms designed\nfor 3D analysis perform sub-optimally on 2D-acquired scans, especially those\nwith thick slices and gaps between slices. Super-resolution (SR) methods aim to\naddress this problem, but previous methods do not address all of the following:\nslice profile shape estimation, slice gap, domain shift, and non-integer /\narbitrary upsampling factors. In this paper, we propose ECLARE (Efficient\nCross-planar Learning for Anisotropic Resolution Enhancement), a self-SR method\nthat addresses each of these factors. ECLARE estimates the slice profile from\nthe 2D-acquired multi-slice MR volume, trains a network to learn the mapping\nfrom low-resolution to high-resolution in-plane patches from the same volume,\nand performs SR with anti-aliasing. We compared ECLARE to cubic B-spline\ninterpolation, SMORE, and other contemporary SR methods. We used realistic and\nrepresentative simulations so that quantitative performance against a ground\ntruth could be computed, and ECLARE outperformed all other methods in both\nsignal recovery and downstream tasks. On real data for which there is no ground\ntruth, ECLARE demonstrated qualitative superiority over other methods as well.\nImportantly, as ECLARE does not use external training data it cannot suffer\nfrom domain shift between training and testing. Our code is open-source and\navailable at https://www.github.com/sremedios/eclare.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11792", "pdf": "https://arxiv.org/pdf/2503.11792", "abs": "https://arxiv.org/abs/2503.11792", "authors": ["Peizhi Yan", "Rabab K. Ward", "Dan Wang", "Qiang Tang", "Shan Du"], "title": "StyleMorpheus: A Style-Based 3D-Aware Morphable Face Model", "categories": ["cs.CV"], "comment": "13 pages, work was completed in 2023", "summary": "For 3D face modeling, the recently developed 3D-aware neural rendering\nmethods are able to render photorealistic face images with arbitrary viewing\ndirections. The training of the parametric controllable 3D-aware face models,\nhowever, still relies on a large-scale dataset that is lab-collected. To\naddress this issue, this paper introduces \"StyleMorpheus\", the first\nstyle-based neural 3D Morphable Face Model (3DMM) that is trained on\nin-the-wild images. It inherits 3DMM's disentangled controllability (over face\nidentity, expression, and appearance) but without the need for accurately\nreconstructed explicit 3D shapes. StyleMorpheus employs an auto-encoder\nstructure. The encoder aims at learning a representative disentangled\nparametric code space and the decoder improves the disentanglement using shape\nand appearance-related style codes in the different sub-modules of the network.\nFurthermore, we fine-tune the decoder through style-based generative\nadversarial learning to achieve photorealistic 3D rendering quality. The\nproposed style-based design enables StyleMorpheus to achieve state-of-the-art\n3D-aware face reconstruction results, while also allowing disentangled control\nof the reconstructed face. Our model achieves real-time rendering speed,\nallowing its use in virtual reality applications. We also demonstrate the\ncapability of the proposed style-based design in face editing applications such\nas style mixing and color editing. Project homepage:\nhttps://github.com/ubc-3d-vision-lab/StyleMorpheus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11930", "pdf": "https://arxiv.org/pdf/2503.11930", "abs": "https://arxiv.org/abs/2503.11930", "authors": ["Jingxuan Zhang", "Robert J. Hart", "Ziqian Bi", "Shiaofen Fang", "Susan Walsh"], "title": "Generating a Biometrically Unique and Realistic Iris Database", "categories": ["cs.CV", "cs.LG"], "comment": "for associated iris database, see\n  https://huggingface.co/datasets/fatdove/Iris_Database", "summary": "The use of the iris as a biometric identifier has increased dramatically over\nthe last 30 years, prompting privacy and security concerns about the use of\niris images in research. It can be difficult to acquire iris image databases\ndue to ethical concerns, and this can be a barrier for those performing\nbiometrics research. In this paper, we describe and show how to create a\ndatabase of realistic, biometrically unidentifiable colored iris images by\ntraining a diffusion model within an open-source diffusion framework. Not only\nwere we able to verify that our model is capable of creating iris textures that\nare biometrically unique from the training data, but we were also able to\nverify that our model output creates a full distribution of realistic iris\npigmentations. We highlight the fact that the utility of diffusion networks to\nachieve these criteria with relative ease, warrants additional research in its\nuse within the context of iris database generation and presentation attack\nsecurity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11937", "pdf": "https://arxiv.org/pdf/2503.11937", "abs": "https://arxiv.org/abs/2503.11937", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11945", "pdf": "https://arxiv.org/pdf/2503.11945", "abs": "https://arxiv.org/abs/2503.11945", "authors": ["Naresh Kumar Devulapally", "Mingzhen Huang", "Vishal Asnani", "Shruti Agarwal", "Siwei Lyu", "Vishnu Suresh Lokhande"], "title": "Your Text Encoder Can Be An Object-Level Watermarking Controller", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Invisible watermarking of AI-generated images can help with copyright\nprotection, enabling detection and identification of AI-generated media. In\nthis work, we present a novel approach to watermark images of T2I Latent\nDiffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we\nenable watermarking in selected objects or parts of the image, offering greater\nflexibility compared to traditional full-image watermarking. Our method\nleverages the text encoder's compatibility across various LDMs, allowing\nplug-and-play integration for different LDMs. Moreover, introducing the\nwatermark early in the encoding stage improves robustness to adversarial\nperturbations in later stages of the pipeline. Our approach achieves $99\\%$ bit\naccuracy ($48$ bits) with a $10^5 \\times$ reduction in model parameters,\nenabling efficient watermarking.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11953", "pdf": "https://arxiv.org/pdf/2503.11953", "abs": "https://arxiv.org/abs/2503.11953", "authors": ["Priyanka Mandikal", "Tushar Nagarajan", "Alex Stoken", "Zihui Xue", "Kristen Grauman"], "title": "SPOC: Spatially-Progressing Object State Change Segmentation in Video", "categories": ["cs.CV"], "comment": null, "summary": "Object state changes in video reveal critical information about human and\nagent activity. However, existing methods are limited to temporal localization\nof when the object is in its initial state (e.g., the unchopped avocado) versus\nwhen it has completed a state change (e.g., the chopped avocado), which limits\napplicability for any task requiring detailed information about the progress of\nthe actions and its spatial localization. We propose to deepen the problem by\nintroducing the spatially-progressing object state change segmentation task.\nThe goal is to segment at the pixel-level those regions of an object that are\nactionable and those that are transformed. We introduce the first model to\naddress this task, designing a VLM-based pseudo-labeling approach, state-change\ndynamics constraints, and a novel WhereToChange benchmark built on in-the-wild\nInternet videos. Experiments on two datasets validate both the challenge of the\nnew task as well as the promise of our model for localizing exactly where and\nhow fast objects are changing in video. We further demonstrate useful\nimplications for tracking activity progress to benefit robotic agents. Project\npage: https://vision.cs.utexas.edu/projects/spoc-spatially-progressing-osc", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11958", "pdf": "https://arxiv.org/pdf/2503.11958", "abs": "https://arxiv.org/abs/2503.11958", "authors": ["Chong Su", "Yingbin Fu", "Zheyuan Hu", "Jing Yang", "Param Hanji", "Shaojun Wang", "Xuan Zhao", "Cengiz Öztireli", "Fangcheng Zhong"], "title": "CHOrD: Generation of Collision-Free, House-Scale, and Organized Digital Twins for 3D Indoor Scenes with Controllable Floor Plans and Optimal Layouts", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Chong Su and Yingbin Fu contributed equally to this work", "summary": "We introduce CHOrD, a novel framework for scalable synthesis of 3D indoor\nscenes, designed to create house-scale, collision-free, and hierarchically\nstructured indoor digital twins. In contrast to existing methods that directly\nsynthesize the scene layout as a scene graph or object list, CHOrD incorporates\na 2D image-based intermediate layout representation, enabling effective\nprevention of collision artifacts by successfully capturing them as\nout-of-distribution (OOD) scenarios during generation. Furthermore, unlike\nexisting methods, CHOrD is capable of generating scene layouts that adhere to\ncomplex floor plans with multi-modal controls, enabling the creation of\ncoherent, house-wide layouts robust to both geometric and semantic variations\nin room structures. Additionally, we propose a novel dataset with expanded\ncoverage of household items and room configurations, as well as significantly\nimproved data quality. CHOrD demonstrates state-of-the-art performance on both\nthe 3D-FRONT and our proposed datasets, delivering photorealistic, spatially\ncoherent indoor scene synthesis adaptable to arbitrary floor plan variations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11969", "pdf": "https://arxiv.org/pdf/2503.11969", "abs": "https://arxiv.org/abs/2503.11969", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Evaluation of Intra-operative Patient-specific Methods for Point Cloud Completion for Minimally Invasive Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "The registration between the pre-operative model and the intra-operative\nsurface is crucial in image-guided liver surgery, as it facilitates the\neffective use of pre-operative information during the procedure. However, the\nintra-operative surface, usually represented as a point cloud, often has\nlimited coverage, especially in laparoscopic surgery, and is prone to holes and\nnoise, posing significant challenges for registration methods. Point cloud\ncompletion methods have the potential to alleviate these issues. Thus, we\nexplore six state-of-the-art point cloud completion methods to identify the\noptimal completion method for liver surgery applications. We focus on a\npatient-specific approach for liver point cloud completion from a partial liver\nsurface under three cases: canonical pose, non-canonical pose, and canonical\npose with noise. The transformer-based method, AdaPoinTr, outperforms all other\nmethods to generate a complete point cloud from the given partial liver point\ncloud under the canonical pose. On the other hand, our findings reveal\nsubstantial performance degradation of these methods under non-canonical poses\nand noisy settings, highlighting the limitations of these methods, which\nsuggests the need for a robust point completion method for its application in\nimage-guided liver surgery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11981", "pdf": "https://arxiv.org/pdf/2503.11981", "abs": "https://arxiv.org/abs/2503.11981", "authors": ["Utkarsh Nath", "Rajeev Goel", "Rahul Khurana", "Kyle Min", "Mark Ollila", "Pavan Turaga", "Varun Jampani", "Tejaswi Gowda"], "title": "DecompDreamer: Advancing Structured 3D Asset Generation with Multi-Object Decomposition and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-3D generation saw dramatic advances in recent years by leveraging\nText-to-Image models. However, most existing techniques struggle with\ncompositional prompts, which describe multiple objects and their spatial\nrelationships. They often fail to capture fine-grained inter-object\ninteractions. We introduce DecompDreamer, a Gaussian splatting-based training\nroutine designed to generate high-quality 3D compositions from such complex\nprompts. DecompDreamer leverages Vision-Language Models (VLMs) to decompose\nscenes into structured components and their relationships. We propose a\nprogressive optimization strategy that first prioritizes joint relationship\nmodeling before gradually shifting toward targeted object refinement. Our\nqualitative and quantitative evaluations against state-of-the-art text-to-3D\nmodels demonstrate that DecompDreamer effectively generates intricate 3D\ncompositions with superior object disentanglement, offering enhanced control\nand flexibility in 3D generation. Project page :\nhttps://decompdreamer3d.github.io", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12001", "pdf": "https://arxiv.org/pdf/2503.12001", "abs": "https://arxiv.org/abs/2503.12001", "authors": ["Peizhen Zheng", "Longfei Wei", "Dongjing Jiang", "Jianfei Zhang"], "title": "3D Gaussian Splatting against Moving Objects for High-Fidelity Street Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate reconstruction of dynamic street scenes is critical for\napplications in autonomous driving, augmented reality, and virtual reality.\nTraditional methods relying on dense point clouds and triangular meshes\nstruggle with moving objects, occlusions, and real-time processing constraints,\nlimiting their effectiveness in complex urban environments. While multi-view\nstereo and neural radiance fields have advanced 3D reconstruction, they face\nchallenges in computational efficiency and handling scene dynamics. This paper\nproposes a novel 3D Gaussian point distribution method for dynamic street scene\nreconstruction. Our approach introduces an adaptive transparency mechanism that\neliminates moving objects while preserving high-fidelity static scene details.\nAdditionally, iterative refinement of Gaussian point distribution enhances\ngeometric accuracy and texture representation. We integrate directional\nencoding with spatial position optimization to optimize storage and rendering\nefficiency, reducing redundancy while maintaining scene integrity. Experimental\nresults demonstrate that our method achieves high reconstruction quality,\nimproved rendering performance, and adaptability in large-scale dynamic\nenvironments. These contributions establish a robust framework for real-time,\nhigh-precision 3D reconstruction, advancing the practicality of dynamic scene\nmodeling across multiple applications. The source code for this work is\navailable to the public at https://github.com/deepcoxcom/3dgs", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12006", "pdf": "https://arxiv.org/pdf/2503.12006", "abs": "https://arxiv.org/abs/2503.12006", "authors": ["Zhe Shan", "Yang Liu", "Lei Zhou", "Cheng Yan", "Heng Wang", "Xia Xie"], "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The availability of large-scale remote sensing video data underscores the\nimportance of high-quality interactive segmentation. However, challenges such\nas small object sizes, ambiguous features, and limited generalization make it\ndifficult for current methods to achieve this goal. In this work, we propose\nROS-SAM, a method designed to achieve high-quality interactive segmentation\nwhile preserving generalization across diverse remote sensing data. The ROS-SAM\nis built upon three key innovations: 1) LoRA-based fine-tuning, which enables\nefficient domain adaptation while maintaining SAM's generalization ability, 2)\nEnhancement of deep network layers to improve the discriminability of extracted\nfeatures, thereby reducing misclassifications, and 3) Integration of global\ncontext with local boundary details in the mask decoder to generate\nhigh-quality segmentation masks. Additionally, we design the data pipeline to\nensure the model learns to better handle objects at varying scales during\ntraining while focusing on high-quality predictions during inference.\nExperiments on remote sensing video datasets show that the redesigned data\npipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally,\nwhen evaluated on existing remote sensing object tracking datasets, ROS-SAM\ndemonstrates impressive zero-shot capabilities, generating masks that closely\nresemble manual annotations. These results confirm ROS-SAM as a powerful tool\nfor fine-grained segmentation in remote sensing applications. Code is available\nat https://github.com/ShanZard/ROS-SAM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12009", "pdf": "https://arxiv.org/pdf/2503.12009", "abs": "https://arxiv.org/abs/2503.12009", "authors": ["Xin Jin", "Haisheng Su", "Kai Liu", "Cong Ma", "Wei Wu", "Fei Hui", "Junchi Yan"], "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12014", "pdf": "https://arxiv.org/pdf/2503.12014", "abs": "https://arxiv.org/abs/2503.12014", "authors": ["Shun Zou", "Yi Zou", "Mingya Zhang", "Shipeng Luo", "Guangwei Gao", "Guojun Qi"], "title": "Learning Dual-Domain Multi-Scale Representations for Single Image Deraining", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, code: https://zs1314.github.io/DMSR", "summary": "Existing image deraining methods typically rely on single-input,\nsingle-output, and single-scale architectures, which overlook the joint\nmulti-scale information between external and internal features. Furthermore,\nsingle-domain representations are often too restrictive, limiting their ability\nto handle the complexities of real-world rain scenarios. To address these\nchallenges, we propose a novel Dual-Domain Multi-Scale Representation Network\n(DMSR). The key idea is to exploit joint multi-scale representations from both\nexternal and internal domains in parallel while leveraging the strengths of\nboth spatial and frequency domains to capture more comprehensive properties.\nSpecifically, our method consists of two main components: the Multi-Scale\nProgressive Spatial Refinement Module (MPSRM) and the Frequency Domain Scale\nMixer (FDSM). The MPSRM enables the interaction and coupling of multi-scale\nexpert information within the internal domain using a hierarchical modulation\nand fusion strategy. The FDSM extracts multi-scale local information in the\nspatial domain, while also modeling global dependencies in the frequency\ndomain. Extensive experiments show that our model achieves state-of-the-art\nperformance across six benchmark datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12052", "pdf": "https://arxiv.org/pdf/2503.12052", "abs": "https://arxiv.org/abs/2503.12052", "authors": ["Zhiyao Sun", "Yu-Hui Wen", "Matthieu Lin", "Ho-Jui Fang", "Sheng Ye", "Tian Lv", "Yong-Jin Liu"], "title": "Tailor: An Integrated Text-Driven CG-Ready Human and Garment Generation System", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://human-tailor.github.com", "summary": "Creating detailed 3D human avatars with garments typically requires\nspecialized expertise and labor-intensive processes. Although recent advances\nin generative AI have enabled text-to-3D human/clothing generation, current\nmethods fall short in offering accessible, integrated pipelines for producing\nready-to-use clothed avatars. To solve this, we introduce Tailor, an integrated\ntext-to-avatar system that generates high-fidelity, customizable 3D humans with\nsimulation-ready garments. Our system includes a three-stage pipeline. We first\nemploy a large language model to interpret textual descriptions into\nparameterized body shapes and semantically matched garment templates. Next, we\ndevelop topology-preserving deformation with novel geometric losses to adapt\ngarments precisely to body geometries. Furthermore, an enhanced texture\ndiffusion module with a symmetric local attention mechanism ensures both view\nconsistency and photorealistic details. Quantitative and qualitative\nevaluations demonstrate that Tailor outperforms existing SoTA methods in terms\nof fidelity, usability, and diversity. Code will be available for academic use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12061", "pdf": "https://arxiv.org/pdf/2503.12061", "abs": "https://arxiv.org/abs/2503.12061", "authors": ["Yuqing Yan", "Yirui Wu"], "title": "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https://anonymous.4open.science/r/EHNet.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12127", "pdf": "https://arxiv.org/pdf/2503.12127", "abs": "https://arxiv.org/abs/2503.12127", "authors": ["Tobia Poppi", "Tejaswi Kasarla", "Pascal Mettes", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Hyperbolic Safety-Aware Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "CVPR 2025", "summary": "Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettré", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12218", "pdf": "https://arxiv.org/pdf/2503.12218", "abs": "https://arxiv.org/abs/2503.12218", "authors": ["Chengxuan Qian", "Kai Han", "Siqi Ma", "Chongwen Lyu", "Zhenlong Yuan", "Jun Chen", "Zhe Liu"], "title": "Adaptive Label Correction for Robust Medical Image Segmentation with Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has shown remarkable success in medical image analysis, but its\nreliance on large volumes of high-quality labeled data limits its\napplicability. While noisy labeled data are easier to obtain, directly\nincorporating them into training can degrade model performance. To address this\nchallenge, we propose a Mean Teacher-based Adaptive Label Correction (ALC)\nself-ensemble framework for robust medical image segmentation with noisy\nlabels. The framework leverages the Mean Teacher architecture to ensure\nconsistent learning under noise perturbations. It includes an adaptive label\nrefinement mechanism that dynamically captures and weights differences across\nmultiple disturbance versions to enhance the quality of noisy labels.\nAdditionally, a sample-level uncertainty-based label selection algorithm is\nintroduced to prioritize high-confidence samples for network updates,\nmitigating the impact of noisy annotations. Consistency learning is integrated\nto align the predictions of the student and teacher networks, further enhancing\nmodel robustness. Extensive experiments on two public datasets demonstrate the\neffectiveness of the proposed framework, showing significant improvements in\nsegmentation performance. By fully exploiting the strengths of the Mean Teacher\nstructure, the ALC framework effectively processes noisy labels, adapts to\nchallenging scenarios, and achieves competitive results compared to\nstate-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12232", "pdf": "https://arxiv.org/pdf/2503.12232", "abs": "https://arxiv.org/abs/2503.12232", "authors": ["Yan Jiang", "Hao Yu", "Xu Cheng", "Haoyu Chen", "Zhaodong Sun", "Guoying Zhao"], "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Aiming to match pedestrian images captured under varying lighting conditions,\nvisible-infrared person re-identification (VI-ReID) has drawn intensive\nresearch attention and achieved promising results. However, in real-world\nsurveillance contexts, data is distributed across multiple devices/entities,\nraising privacy and ownership concerns that make existing centralized training\nimpractical for VI-ReID. To tackle these challenges, we propose L2RW, a\nbenchmark that brings VI-ReID closer to real-world applications. The rationale\nof L2RW is that integrating decentralized training into VI-ReID can address\nprivacy concerns in scenarios with limited data-sharing regulation.\nSpecifically, we design protocols and corresponding algorithms for different\nprivacy sensitivity levels. In our new benchmark, we ensure the model training\nis done in the conditions that: 1) data from each camera remains completely\nisolated, or 2) different data entities (e.g., data controllers of a certain\nregion) can selectively share the data. In this way, we simulate scenarios with\nstrict privacy constraints which is closer to real-world conditions. Intensive\nexperiments with various server-side federated algorithms are conducted,\nshowing the feasibility of decentralized VI-ReID training. Notably, when\nevaluated in unseen domains (i.e., new data entities), our L2RW, trained with\nisolated data (privacy-preserved), achieves performance comparable to SOTAs\ntrained with shared data (privacy-unrestricted). We hope this work offers a\nnovel research entry for deploying VI-ReID that fits real-world scenarios and\ncan benefit the community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12249", "pdf": "https://arxiv.org/pdf/2503.12249", "abs": "https://arxiv.org/abs/2503.12249", "authors": ["Boyu Chen", "Ameenat L. Solebo", "Daqian Shi", "Jinge Wu", "Paul Taylor"], "title": "Minuscule Cell Detection in AS-OCT Images with Progressive Field-of-View Focusing", "categories": ["cs.CV"], "comment": null, "summary": "Anterior Segment Optical Coherence Tomography (AS-OCT) is an emerging imaging\ntechnique with great potential for diagnosing anterior uveitis, a\nvision-threatening ocular inflammatory condition. A hallmark of this condition\nis the presence of inflammatory cells in the eye's anterior chamber, and\ndetecting these cells using AS-OCT images has attracted research interest.\nWhile recent efforts aim to replace manual cell detection with automated\ncomputer vision approaches, detecting extremely small (minuscule) objects in\nhigh-resolution images, such as AS-OCT, poses substantial challenges: (1) each\ncell appears as a minuscule particle, representing less than 0.005\\% of the\nimage, making the detection difficult, and (2) OCT imaging introduces\npixel-level noise that can be mistaken for cells, leading to false positive\ndetections. To overcome these challenges, we propose a minuscule cell detection\nframework through a progressive field-of-view focusing strategy. This strategy\nsystematically refines the detection scope from the whole image to a target\nregion where cells are likely to be present, and further to minuscule regions\npotentially containing individual cells. Our framework consists of two modules.\nFirst, a Field-of-Focus module uses a vision foundation model to segment the\ntarget region. Subsequently, a Fine-grained Object Detection module introduces\na specialized Minuscule Region Proposal followed by a Spatial Attention Network\nto distinguish individual cells from noise within the segmented region.\nExperimental results demonstrate that our framework outperforms\nstate-of-the-art methods for cell detection, providing enhanced efficacy for\nclinical applications. Our code is publicly available at:\nhttps://github.com/joeybyc/MCD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12261", "pdf": "https://arxiv.org/pdf/2503.12261", "abs": "https://arxiv.org/abs/2503.12261", "authors": ["R. Gnana Praveen", "Jahangir Alam"], "title": "Handling Weak Complementary Relationships for Audio-Visual Emotion Recognition", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Submission to valence arousal track of 8th ABAW competition. arXiv\n  admin note: substantial text overlap with arXiv:2403.13659", "summary": "Multimodal emotion recognition has recently drawn a lot of interest in\naffective computing as it has immense potential to outperform isolated unimodal\napproaches. Audio and visual modalities are two predominant contact-free\nchannels in videos, which are often expected to carry a complementary\nrelationship with each other. However, audio and visual channels may not always\nbe complementary with each other, resulting in poor audio-visual feature\nrepresentations, thereby degrading the performance of the system. In this\npaper, we propose a flexible audio-visual fusion model that can adapt to weak\ncomplementary relationships using a gated attention mechanism. Specifically, we\nextend the recursive joint cross-attention model by introducing gating\nmechanism in every iteration to control the flow of information between the\ninput features and the attended features depending on the strength of their\ncomplementary relationship. For instance, if the modalities exhibit strong\ncomplementary relationships, the gating mechanism chooses cross-attended\nfeatures, otherwise non-attended features. To further improve the performance\nof the system, we further introduce stage gating mechanism, which is used to\ncontrol the flow of information across the gated outputs of each iteration.\nTherefore, the proposed model improves the performance of the system even when\nthe audio and visual modalities do not have a strong complementary relationship\nwith each other by adding more flexibility to the recursive joint cross\nattention mechanism. The proposed model has been evaluated on the challenging\nAffwild2 dataset and significantly outperforms the state-of-the-art fusion\napproaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12271", "pdf": "https://arxiv.org/pdf/2503.12271", "abs": "https://arxiv.org/abs/2503.12271", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Arsh Koneru", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "The predominant approach to advancing text-to-image generation has been\ntraining-time scaling, where larger models are trained on more data using\ngreater computational resources. While effective, this approach is\ncomputationally expensive, leading to growing interest in inference-time\nscaling to improve performance. Currently, inference-time scaling for\ntext-to-image diffusion models is largely limited to best-of-N sampling, where\nmultiple images are generated per prompt and a selection model chooses the best\noutput. Inspired by the recent success of reasoning models like DeepSeek-R1 in\nthe language domain, we introduce an alternative to naive best-of-N sampling by\nequipping text-to-image Diffusion Transformers with in-context reflection\ncapabilities. We propose Reflect-DiT, a method that enables Diffusion\nTransformers to refine their generations using in-context examples of\npreviously generated images alongside textual feedback describing necessary\nimprovements. Instead of passively relying on random sampling and hoping for a\nbetter result in a future generation, Reflect-DiT explicitly tailors its\ngenerations to address specific aspects requiring enhancement. Experimental\nresults demonstrate that Reflect-DiT improves performance on the GenEval\nbenchmark (+0.19) using SANA-1.0-1.6B as a base model. Additionally, it\nachieves a new state-of-the-art score of 0.81 on GenEval while generating only\n20 samples per prompt, surpassing the previous best score of 0.80, which was\nobtained using a significantly larger model (SANA-1.5-4.8B) with 2048 samples\nunder the best-of-N approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12281", "pdf": "https://arxiv.org/pdf/2503.12281", "abs": "https://arxiv.org/abs/2503.12281", "authors": ["Paola Natalia Cañas", "Marcos Nieto", "Oihana Otaegui", "Igor Rodríguez"], "title": "Exploration of VLMs for Driver Monitoring Systems Applications", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 16th ITS European Congress, Seville, Spain, 19-21 May\n  2025", "summary": "In recent years, we have witnessed significant progress in emerging deep\nlearning models, particularly Large Language Models (LLMs) and Vision-Language\nModels (VLMs). These models have demonstrated promising results, indicating a\nnew era of Artificial Intelligence (AI) that surpasses previous methodologies.\nTheir extensive knowledge and zero-shot capabilities suggest a paradigm shift\nin developing deep learning solutions, moving from data capturing and algorithm\ntraining to just writing appropriate prompts. While the application of these\ntechnologies has been explored across various industries, including automotive,\nthere is a notable gap in the scientific literature regarding their use in\nDriver Monitoring Systems (DMS). This paper presents our initial approach to\nimplementing VLMs in this domain, utilising the Driver Monitoring Dataset to\nevaluate their performance and discussing their advantages and challenges when\nimplemented in real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12343", "pdf": "https://arxiv.org/pdf/2503.12343", "abs": "https://arxiv.org/abs/2503.12343", "authors": ["Xiaoyu Xiong", "Changyu Hu", "Chunru Lin", "Pingchuan Ma", "Chuang Gan", "Tao Du"], "title": "TopoGaussian: Inferring Internal Topology Structures from Visual Clues", "categories": ["cs.CV"], "comment": null, "summary": "We present TopoGaussian, a holistic, particle-based pipeline for inferring\nthe interior structure of an opaque object from easily accessible photos and\nvideos as input. Traditional mesh-based approaches require tedious and\nerror-prone mesh filling and fixing process, while typically output rough\nboundary surface. Our pipeline combines Gaussian Splatting with a novel,\nversatile particle-based differentiable simulator that simultaneously\naccommodates constitutive model, actuator, and collision, without interference\nwith mesh. Based on the gradients from this simulator, we provide flexible\nchoice of topology representation for optimization, including particle, neural\nimplicit surface, and quadratic surface. The resultant pipeline takes easily\naccessible photos and videos as input and outputs the topology that matches the\nphysical characteristics of the input. We demonstrate the efficacy of our\npipeline on a synthetic dataset and four real-world tasks with 3D-printed\nprototypes. Compared with existing mesh-based method, our pipeline is 5.26x\nfaster on average with improved shape quality. These results highlight the\npotential of our pipeline in 3D vision, soft robotics, and manufacturing\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12348", "pdf": "https://arxiv.org/pdf/2503.12348", "abs": "https://arxiv.org/abs/2503.12348", "authors": ["Mo Zhou", "Jianwei Wang", "Xuanmeng Zhang", "Dylan Campbell", "Kai Wang", "Long Yuan", "Wenjie Zhang", "Xuemin Lin"], "title": "ProbDiffFlow: An Efficient Learning-Free Framework for Probabilistic Single-Image Optical Flow Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper studies optical flow estimation, a critical task in motion\nanalysis with applications in autonomous navigation, action recognition, and\nfilm production. Traditional optical flow methods require consecutive frames,\nwhich are often unavailable due to limitations in data acquisition or\nreal-world scene disruptions. Thus, single-frame optical flow estimation is\nemerging in the literature. However, existing single-frame approaches suffer\nfrom two major limitations: (1) they rely on labeled training data, making them\ntask-specific, and (2) they produce deterministic predictions, failing to\ncapture motion uncertainty. To overcome these challenges, we propose\nProbDiffFlow, a training-free framework that estimates optical flow\ndistributions from a single image. Instead of directly predicting motion,\nProbDiffFlow follows an estimation-by-synthesis paradigm: it first generates\ndiverse plausible future frames using a diffusion-based model, then estimates\nmotion from these synthesized samples using a pre-trained optical flow model,\nand finally aggregates the results into a probabilistic flow distribution. This\ndesign eliminates the need for task-specific training while capturing multiple\nplausible motions. Experiments on both synthetic and real-world datasets\ndemonstrate that ProbDiffFlow achieves superior accuracy, diversity, and\nefficiency, outperforming existing single-image and two-frame baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12355", "pdf": "https://arxiv.org/pdf/2503.12355", "abs": "https://arxiv.org/abs/2503.12355", "authors": ["Kumar Krishna Agrawal", "Long Lian", "Longchao Liu", "Natalia Harguindeguy", "Boyi Li", "Alexander Bick", "Maggie Chung", "Trevor Darrell", "Adam Yala"], "title": "Atlas: Multi-Scale Attention Improves Long Context Image Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Efficiently modeling massive images is a long-standing challenge in machine\nlearning. To this end, we introduce Multi-Scale Attention (MSA). MSA relies on\ntwo key ideas, (i) multi-scale representations (ii) bi-directional cross-scale\ncommunication. MSA creates O(log N) scales to represent the image across\nprogressively coarser features and leverages cross-attention to propagate\ninformation across scales. We then introduce Atlas, a novel neural network\narchitecture based on MSA. We demonstrate that Atlas significantly improves the\ncompute-performance tradeoff of long-context image modeling in a\nhigh-resolution variant of ImageNet 100. At 1024px resolution, Atlas-B achieves\n91.04% accuracy, comparable to ConvNext-B (91.92%) while being 4.3x faster.\nAtlas is 2.95x faster and 7.38% better than FasterViT, 2.25x faster and 4.96%\nbetter than LongViT. In comparisons against MambaVision-S, we find Atlas-S\nachieves 5%, 16% and 32% higher accuracy at 1024px, 2048px and 4096px\nrespectively, while obtaining similar runtimes. Code for reproducing our\nexperiments and pretrained models is available at\nhttps://github.com/yalalab/atlas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12381", "pdf": "https://arxiv.org/pdf/2503.12381", "abs": "https://arxiv.org/abs/2503.12381", "authors": ["Ruchika Sharma", "Rudresh Dwivedi"], "title": "Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor via Improved RCNN", "categories": ["cs.CV", "cs.MM"], "comment": "Submiited to journal", "summary": "Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12382", "pdf": "https://arxiv.org/pdf/2503.12382", "abs": "https://arxiv.org/abs/2503.12382", "authors": ["Kang You", "Tong Chen", "Dandan Ding", "M. Salman Asif", "Zhan Ma"], "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Despite the substantial advancements demonstrated by learning-based neural\nmodels in the LiDAR Point Cloud Compression (LPCC) task, realizing real-time\ncompression - an indispensable criterion for numerous industrial applications -\nremains a formidable challenge. This paper proposes RENO, the first real-time\nneural codec for 3D LiDAR point clouds, achieving superior performance with a\nlightweight model. RENO skips the octree construction and directly builds upon\nthe multiscale sparse tensor representation. Instead of the multi-stage\ninferring, RENO devises sparse occupancy codes, which exploit cross-scale\ncorrelation and derive voxels' occupancy in a one-shot manner, greatly saving\nprocessing time. Experimental results demonstrate that the proposed RENO\nachieves real-time coding speed, 10 fps at 14-bit depth on a desktop platform\n(e.g., one RTX 3090 GPU) for both encoding and decoding processes, while\nproviding 12.25% and 48.34% bit-rate savings compared to G-PCCv23 and Draco,\nrespectively, at a similar quality. RENO model size is merely 1MB, making it\nattractive for practical applications. The source code is available at\nhttps://github.com/NJUVISION/RENO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12399", "pdf": "https://arxiv.org/pdf/2503.12399", "abs": "https://arxiv.org/abs/2503.12399", "authors": ["Jiangdong Cai", "Yan Chen", "Zhenrong Shen", "Haotian Jiang", "Honglin Xiong", "Kai Xuan", "Lichi Zhang", "Qian Wang"], "title": "Pathology Image Restoration via Mixture of Prompts", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In digital pathology, acquiring all-in-focus images is essential to\nhigh-quality imaging and high-efficient clinical workflow. Traditional scanners\nachieve this by scanning at multiple focal planes of varying depths and then\nmerging them, which is relatively slow and often struggles with complex tissue\ndefocus. Recent prevailing image restoration technique provides a means to\nrestore high-quality pathology images from scans of single focal planes.\nHowever, existing image restoration methods are inadequate, due to intricate\ndefocus patterns in pathology images and their domain-specific semantic\ncomplexities. In this work, we devise a two-stage restoration solution\ncascading a transformer and a diffusion model, to benefit from their powers in\npreserving image fidelity and perceptual quality, respectively. We particularly\npropose a novel mixture of prompts for the two-stage solution. Given initial\nprompt that models defocus in microscopic imaging, we design two prompts that\ndescribe the high-level image semantics from pathology foundation model and the\nfine-grained tissue structures via edge extraction. We demonstrate that, by\nfeeding the prompt mixture to our method, we can restore high-quality pathology\nimages from single-focal-plane scans, implying high potentials of the mixture\nof prompts to clinical usage. Code will be publicly available at\nhttps://github.com/caijd2000/MoP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12401", "pdf": "https://arxiv.org/pdf/2503.12401", "abs": "https://arxiv.org/abs/2503.12401", "authors": ["Jianwei Zhao", "Xin Li", "Fan Yang", "Qiang Zhai", "Ao Luo", "Yang Zhao", "Hong Cheng", "Huazhu Fu"], "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12441", "pdf": "https://arxiv.org/pdf/2503.12441", "abs": "https://arxiv.org/abs/2503.12441", "authors": ["Yuda Zou", "Zelong Liu", "Yuliang Gu", "Bo Du", "Yongchao Xu"], "title": "Consistent-Point: Consistent Pseudo-Points for Semi-Supervised Crowd Counting and Localization", "categories": ["cs.CV"], "comment": null, "summary": "Crowd counting and localization are important in applications such as public\nsecurity and traffic management. Existing methods have achieved impressive\nresults thanks to extensive laborious annotations. This paper propose a novel\npoint-localization-based semi-supervised crowd counting and localization method\ntermed Consistent-Point. We identify and address two inconsistencies of\npseudo-points, which have not been adequately explored. To enhance their\nposition consistency, we aggregate the positions of neighboring auxiliary\nproposal-points. Additionally, an instance-wise uncertainty calibration is\nproposed to improve the class consistency of pseudo-points. By generating more\nconsistent pseudo-points, Consistent-Point provides more stable supervision to\nthe training process, yielding improved results. Extensive experiments across\nfive widely used datasets and three different labeled ratio settings\ndemonstrate that our method achieves state-of-the-art performance in crowd\nlocalization while also attaining impressive crowd counting results. The code\nwill be available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12447", "pdf": "https://arxiv.org/pdf/2503.12447", "abs": "https://arxiv.org/abs/2503.12447", "authors": ["Li Yicong"], "title": "Causality Model for Semantic Understanding on Videos", "categories": ["cs.CV", "cs.AI"], "comment": "PhD Thesis", "summary": "After a decade of prosperity, the development of video understanding has\nreached a critical juncture, where the sole reliance on massive data and\ncomplex architectures is no longer a one-size-fits-all solution to all\nsituations. The presence of ubiquitous data imbalance hampers DNNs from\neffectively learning the underlying causal mechanisms, leading to significant\nperformance drops when encountering distribution shifts, such as long-tail\nimbalances and perturbed imbalances. This realization has prompted researchers\nto seek alternative methodologies to capture causal patterns in video data. To\ntackle these challenges and increase the robustness of DNNs, causal modeling\nemerged as a principle to discover the true causal patterns behind the observed\ncorrelations. This thesis focuses on the domain of semantic video understanding\nand explores the potential of causal modeling to advance two fundamental tasks:\nVideo Relation Detection (VidVRD) and Video Question Answering (VideoQA).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12453", "pdf": "https://arxiv.org/pdf/2503.12453", "abs": "https://arxiv.org/abs/2503.12453", "authors": ["Edgar Heinert", "Thomas Gottwald", "Annika Mütze", "Matthias Rottmann"], "title": "Shape Bias and Robustness Evaluation via Cue Decomposition for Image Classification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Previous works studied how deep neural networks (DNNs) perceive image content\nin terms of their biases towards different image cues, such as texture and\nshape. Previous methods to measure shape and texture biases are typically\nstyle-transfer-based and limited to DNNs for image classification. In this\nwork, we provide a new evaluation procedure consisting of 1) a\ncue-decomposition method that comprises two AI-free data pre-processing methods\nextracting shape and texture cues, respectively, and 2) a novel\ncue-decomposition shape bias evaluation metric that leverages the\ncue-decomposition data. For application purposes we introduce a corresponding\ncue-decomposition robustness metric that allows for the estimation of the\nrobustness of a DNN w.r.t. image corruptions. In our numerical experiments, our\nfindings for biases in image classification DNNs align with those of previous\nevaluation metrics. However, our cue-decomposition robustness metric shows\nsuperior results in terms of estimating the robustness of DNNs. Furthermore,\nour results for DNNs on the semantic segmentation datasets Cityscapes and\nADE20k for the first time shed light into the biases of semantic segmentation\nDNNs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12485", "pdf": "https://arxiv.org/pdf/2503.12485", "abs": "https://arxiv.org/abs/2503.12485", "authors": ["Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Hezhen Hu", "Wengang Zhou", "Houqiang Li"], "title": "Cross-Modal Consistency Learning for Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training has been proven to be effective in boosting the performance of\nIsolated Sign Language Recognition (ISLR). Existing pre-training methods solely\nfocus on the compact pose data, which eliminate background perturbation but\ninevitably suffer from insufficient semantic cues compared to raw RGB videos.\nNevertheless, direct representation learning only from RGB videos remains\nchallenging due to the presence of sign-independent visual features. To address\nthis dilemma, we propose a Cross-modal Consistency Learning framework\n(CCL-SLR), which leverages the cross-modal consistency from both RGB and pose\nmodalities based on self-supervised pre-training. First, CCL-SLR employs\ncontrastive learning for instance discrimination within and across modalities.\nThrough the single-modal and cross-modal contrastive learning, CCL-SLR\ngradually aligns the feature spaces of RGB and pose modalities, thereby\nextracting consistent sign representations. Second, we further introduce\nMotion-Preserving Masking (MPM) and Semantic Positive Mining (SPM) techniques\nto improve cross-modal consistency from the perspective of data augmentation\nand sample similarity, respectively. Extensive experiments on four ISLR\nbenchmarks show that CCL-SLR achieves impressive performance, demonstrating its\neffectiveness. The code will be released to the public.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12490", "pdf": "https://arxiv.org/pdf/2503.12490", "abs": "https://arxiv.org/abs/2503.12490", "authors": ["Zilun Zhang", "Haozhan Shen", "Tiancheng Zhao", "Bin Chen", "Zian Guan", "Yuhao Wang", "Xu Jia", "Yuxiang Cai", "Yongheng Shang", "Jianwei Yin"], "title": "GeoRSMLLM: A Multimodal Large Language Model for Vision-Language Tasks in Geoscience and Remote Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The application of Vision-Language Models (VLMs) in remote sensing (RS) has\ndemonstrated significant potential in traditional tasks such as scene\nclassification, object detection, and image captioning. However, current\nmodels, which excel in Referring Expression Comprehension (REC), struggle with\ntasks involving complex instructions (e.g., exists multiple conditions) or\npixel-level operations like segmentation and change detection. In this white\npaper, we provide a comprehensive hierarchical summary of vision-language tasks\nin RS, categorized by the varying levels of cognitive capability required. We\nintroduce the Remote Sensing Vision-Language Task Set (RSVLTS), which includes\nOpen-Vocabulary Tasks (OVT), Referring Expression Tasks (RET), and Described\nObject Tasks (DOT) with increased difficulty, and Visual Question Answering\n(VQA) aloneside. Moreover, we propose a novel unified data representation using\na set-of-points approach for RSVLTS, along with a condition parser and a\nself-augmentation strategy based on cyclic referring. These features are\nintegrated into the GeoRSMLLM model, and this enhanced model is designed to\nhandle a broad range of tasks of RSVLTS, paving the way for a more generalized\nsolution for vision-language tasks in geoscience and remote sensing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12496", "pdf": "https://arxiv.org/pdf/2503.12496", "abs": "https://arxiv.org/abs/2503.12496", "authors": ["Tianyuan Qu", "Longxiang Tang", "Bohao Peng", "Senqiao Yang", "Bei Yu", "Jiaya Jia"], "title": "Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?", "categories": ["cs.CV"], "comment": null, "summary": "The rise of Large Vision-Language Models (LVLMs) has significantly advanced\nvideo understanding. However, efficiently processing long videos remains a\nchallenge due to the ``Sampling Dilemma'': low-density sampling risks missing\ncritical information, while high-density sampling introduces redundancy. To\naddress this issue, we introduce LSDBench, the first benchmark designed to\nevaluate LVLMs on long-video tasks by constructing high Necessary Sampling\nDensity (NSD) questions, where NSD represents the minimum sampling density\nrequired to accurately answer a given question. LSDBench focuses on dense,\nshort-duration actions to rigorously assess the sampling strategies employed by\nLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novel\nReasoning-Driven Hierarchical Sampling (RHS) framework, which combines global\nlocalization of question-relevant cues with local dense sampling for precise\ninference. Additionally, we develop a lightweight Semantic-Guided Frame\nSelector to prioritize informative frames, enabling RHS to achieve comparable\nor superior performance with significantly fewer sampled frames. Together, our\nLSDBench and RHS framework address the unique challenges of high-NSD long-video\ntasks, setting a new standard for evaluating and improving LVLMs in this\ndomain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12507", "pdf": "https://arxiv.org/pdf/2503.12507", "abs": "https://arxiv.org/abs/2503.12507", "authors": ["Guangqian Guo", "Yoong Guo", "Xuehui Yu", "Wenbo Li", "Yaoxing Wang", "Shan Gao"], "title": "Segment Any-Quality Images with Generative Latent Space Enhancement", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12527", "pdf": "https://arxiv.org/pdf/2503.12527", "abs": "https://arxiv.org/abs/2503.12527", "authors": ["Yang Yi", "Kunqing Wang", "Jinpu Zhang", "Zhen Tan", "Xiangke Wang", "Hui Shen", "Dewen Hu"], "title": "A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "The bias of low-cost Inertial Measurement Units (IMU) is a critical factor\naffecting the performance of Visual-Inertial Odometry (VIO). In particular,\nwhen visual tracking encounters errors, the optimized bias results may deviate\nsignificantly from the true values, adversely impacting the system's stability\nand localization precision. In this paper, we propose a novel plug-and-play\nframework featuring the Inertial Prior Network (IPNet), which is designed to\naccurately estimate IMU bias. Recognizing the substantial impact of initial\nbias errors in low-cost inertial devices on system performance, our network\ndirectly leverages raw IMU data to estimate the mean bias, eliminating the\ndependency on historical estimates in traditional recursive predictions and\neffectively preventing error propagation. Furthermore, we introduce an\niterative approach to calculate the mean value of the bias for network\ntraining, addressing the lack of bias labels in many visual-inertial datasets.\nThe framework is evaluated on two public datasets and one self-collected\ndataset. Extensive experiments demonstrate that our method significantly\nenhances both localization precision and robustness, with the ATE-RMSE metric\nimproving on average by 46\\%. The source code and video will be available at\n\\textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12535", "pdf": "https://arxiv.org/pdf/2503.12535", "abs": "https://arxiv.org/abs/2503.12535", "authors": ["Guibiao Liao", "Qing Li", "Zhenyu Bao", "Guoping Qiu", "Kanglin Liu"], "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025. The project page is available at\n  https://gbliao.github.io/SPC-GS.github.io/", "summary": "3D Gaussian Splatting-based indoor open-world free-view synthesis approaches\nhave shown significant performance with dense input images. However, they\nexhibit poor performance when confronted with sparse inputs, primarily due to\nthe sparse distribution of Gaussian points and insufficient view supervision.\nTo relieve these challenges, we propose SPC-GS, leveraging Scene-layout-based\nGaussian Initialization (SGI) and Semantic-Prompt Consistency (SPC)\nRegularization for open-world free view synthesis with sparse inputs.\nSpecifically, SGI provides a dense, scene-layout-based Gaussian distribution by\nutilizing view-changed images generated from the video generation model and\nview-constraint Gaussian points densification. Additionally, SPC mitigates\nlimited view supervision by employing semantic-prompt-based consistency\nconstraints developed by SAM2. This approach leverages available semantics from\ntraining views, serving as instructive prompts, to optimize visually\noverlapping regions in novel views with 2D and 3D consistency constraints.\nExtensive experiments demonstrate the superior performance of SPC-GS across\nReplica and ScanNet benchmarks. Notably, our SPC-GS achieves a 3.06 dB gain in\nPSNR for reconstruction quality and a 7.3% improvement in mIoU for open-world\nsemantic segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12590", "pdf": "https://arxiv.org/pdf/2503.12590", "abs": "https://arxiv.org/abs/2503.12590", "authors": ["Haoran Feng", "Zehuan Huang", "Lin Li", "Hairong Lv", "Lu Sheng"], "title": "Personalize Anything for Free with Diffusion Transformer", "categories": ["cs.CV"], "comment": "https://fenghora.github.io/Personalize-Anything-Page/", "summary": "Personalized image generation aims to produce images of user-specified\nconcepts while enabling flexible editing. Recent training-free approaches,\nwhile exhibit higher computational efficiency than training-based methods,\nstruggle with identity preservation, applicability, and compatibility with\ndiffusion transformers (DiTs). In this paper, we uncover the untapped potential\nof DiT, where simply replacing denoising tokens with those of a reference\nsubject achieves zero-shot subject reconstruction. This simple yet effective\nfeature injection technique unlocks diverse scenarios, from personalization to\nimage editing. Building upon this observation, we propose \\textbf{Personalize\nAnything}, a training-free framework that achieves personalized image\ngeneration in DiT through: 1) timestep-adaptive token replacement that enforces\nsubject consistency via early-stage injection and enhances flexibility through\nlate-stage regularization, and 2) patch perturbation strategies to boost\nstructural diversity. Our method seamlessly supports layout-guided generation,\nmulti-subject personalization, and mask-controlled editing. Evaluations\ndemonstrate state-of-the-art performance in identity preservation and\nversatility. Our work establishes new insights into DiTs while delivering a\npractical paradigm for efficient personalization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12615", "pdf": "https://arxiv.org/pdf/2503.12615", "abs": "https://arxiv.org/abs/2503.12615", "authors": ["Alessio Spagnoletti", "Jean Prost", "Andrés Almansa", "Nicolas Papadakis", "Marcelo Pereyra"], "title": "LATINO-PRO: LAtent consisTency INverse sOlver with PRompt Optimization", "categories": ["cs.CV", "cs.LG"], "comment": "27 pages, 20 figures", "summary": "Text-to-image latent diffusion models (LDMs) have recently emerged as\npowerful generative models with great potential for solving inverse problems in\nimaging. However, leveraging such models in a Plug & Play (PnP), zero-shot\nmanner remains challenging because it requires identifying a suitable text\nprompt for the unknown image of interest. Also, existing text-to-image PnP\napproaches are highly computationally expensive. We herein address these\nchallenges by proposing a novel PnP inference paradigm specifically designed\nfor embedding generative models within stochastic inverse solvers, with special\nattention to Latent Consistency Models (LCMs), which distill LDMs into fast\ngenerators. We leverage our framework to propose LAtent consisTency INverse\nsOlver (LATINO), the first zero-shot PnP framework to solve inverse problems\nwith priors encoded by LCMs. Our conditioning mechanism avoids automatic\ndifferentiation and reaches SOTA quality in as little as 8 neural function\nevaluations. As a result, LATINO delivers remarkably accurate solutions and is\nsignificantly more memory and computationally efficient than previous\napproaches. We then embed LATINO within an empirical Bayesian framework that\nautomatically calibrates the text prompt from the observed measurements by\nmarginal maximum likelihood estimation. Extensive experiments show that prompt\nself-calibration greatly improves estimation, allowing LATINO with PRompt\nOptimization to define new SOTAs in image reconstruction quality and\ncomputational efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12617", "pdf": "https://arxiv.org/pdf/2503.12617", "abs": "https://arxiv.org/abs/2503.12617", "authors": ["Anthony Lamelas", "Harrison Muchnic"], "title": "Scaling Semantic Categories: Investigating the Impact on Vision Transformer Labeling Performance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 7 figures, submitted to CVPR (feedback pending)", "summary": "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12678", "pdf": "https://arxiv.org/pdf/2503.12678", "abs": "https://arxiv.org/abs/2503.12678", "authors": ["Partho Ghosh", "Raisa Bentay Hossain", "Mohammad Zunaed", "Taufiq Hasan"], "title": "Domain Generalization for Improved Human Activity Recognition in Office Space Videos Using Adaptive Pre-processing", "categories": ["cs.CV"], "comment": null, "summary": "Automatic video activity recognition is crucial across numerous domains like\nsurveillance, healthcare, and robotics. However, recognizing human activities\nfrom video data becomes challenging when training and test data stem from\ndiverse domains. Domain generalization, adapting to unforeseen domains, is thus\nessential. This paper focuses on office activity recognition amidst\nenvironmental variability. We propose three pre-processing techniques\napplicable to any video encoder, enhancing robustness against environmental\nvariations. Our study showcases the efficacy of MViT, a leading\nstate-of-the-art video classification model, and other video encoders combined\nwith our techniques, outperforming state-of-the-art domain adaptation methods.\nOur approach significantly boosts accuracy, precision, recall and F1 score on\nunseen domains, emphasizing its adaptability in real-world scenarios with\ndiverse video data sources. This method lays a foundation for more reliable\nvideo activity recognition systems across heterogeneous data domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12763", "pdf": "https://arxiv.org/pdf/2503.12763", "abs": "https://arxiv.org/abs/2503.12763", "authors": ["Kewei Sui", "Anindita Ghosh", "Inwoo Hwang", "Jian Wang", "Chuan Guo"], "title": "A Survey on Human Interaction Motion Generation", "categories": ["cs.CV", "cs.LG"], "comment": "The repository listing relevant papers is accessible at:\n  https://github.com/soraproducer/Awesome-Human-Interaction-Motion-Generation", "summary": "Humans inhabit a world defined by interactions -- with other humans, objects,\nand environments. These interactive movements not only convey our relationships\nwith our surroundings but also demonstrate how we perceive and communicate with\nthe real world. Therefore, replicating these interaction behaviors in digital\nsystems has emerged as an important topic for applications in robotics, virtual\nreality, and animation. While recent advances in deep generative models and new\ndatasets have accelerated progress in this field, significant challenges remain\nin modeling the intricate human dynamics and their interactions with entities\nin the external world. In this survey, we present, for the first time, a\ncomprehensive overview of the literature in human interaction motion\ngeneration. We begin by establishing foundational concepts essential for\nunderstanding the research background. We then systematically review existing\nsolutions and datasets across three primary interaction tasks -- human-human,\nhuman-object, and human-scene interactions -- followed by evaluation metrics.\nFinally, we discuss open research directions and future opportunities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12764", "pdf": "https://arxiv.org/pdf/2503.12764", "abs": "https://arxiv.org/abs/2503.12764", "authors": ["Yidi Liu", "Dong Li", "Yuxin Ma", "Jie Huang", "Wenlong Zhang", "Xueyang Fu", "Zheng-jun Zha"], "title": "Decouple to Reconstruct: High Quality UHD Restoration via Active Feature Disentanglement and Reversible Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-definition (UHD) image restoration often faces computational\nbottlenecks and information loss due to its extremely high resolution. Existing\nstudies based on Variational Autoencoders (VAE) improve efficiency by\ntransferring the image restoration process from pixel space to latent space.\nHowever, degraded components are inherently coupled with background elements in\ndegraded images, both information loss during compression and information gain\nduring compensation remain uncontrollable. These lead to restored images often\nexhibiting image detail loss and incomplete degradation removal. To address\nthis issue, we propose a Controlled Differential Disentangled VAE, which\nutilizes Hierarchical Contrastive Disentanglement Learning and an Orthogonal\nGated Projection Module to guide the VAE to actively discard easily recoverable\nbackground information while encoding more difficult-to-recover degraded\ninformation into the latent space. Additionally, we design a Complex Invertible\nMultiscale Fusion Network to handle background features, ensuring their\nconsistency, and utilize a latent space restoration network to transform the\ndegraded latent features, leading to more accurate restoration results.\nExtensive experimental results demonstrate that our method effectively\nalleviates the information loss problem in VAE models while ensuring\ncomputational efficiency, significantly improving the quality of UHD image\nrestoration, and achieves state-of-the-art results in six UHD restoration tasks\nwith only 1M parameters.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12786", "pdf": "https://arxiv.org/pdf/2503.12786", "abs": "https://arxiv.org/abs/2503.12786", "authors": ["Peirong Zhang", "Yuliang Liu", "Songxuan Lai", "Hongliang Li", "Lianwen Jin"], "title": "Privacy-Preserving Biometric Verification with Handwritten Random Digit String", "categories": ["cs.CV"], "comment": null, "summary": "Handwriting verification has stood as a steadfast identity authentication\nmethod for decades. However, this technique risks potential privacy breaches\ndue to the inclusion of personal information in handwritten biometrics such as\nsignatures. To address this concern, we propose using the Random Digit String\n(RDS) for privacy-preserving handwriting verification. This approach allows\nusers to authenticate themselves by writing an arbitrary digit sequence,\neffectively ensuring privacy protection. To evaluate the effectiveness of RDS,\nwe construct a new HRDS4BV dataset composed of online naturally handwritten\nRDS. Unlike conventional handwriting, RDS encompasses unconstrained and\nvariable content, posing significant challenges for modeling consistent\npersonal writing style. To surmount this, we propose the Pattern Attentive\nVErification Network (PAVENet), along with a Discriminative Pattern Mining\n(DPM) module. DPM adaptively enhances the recognition of consistent and\ndiscriminative writing patterns, thus refining handwriting style\nrepresentation. Through comprehensive evaluations, we scrutinize the\napplicability of online RDS verification and showcase a pronounced\noutperformance of our model over existing methods. Furthermore, we discover a\nnoteworthy forgery phenomenon that deviates from prior findings and discuss its\npositive impact in countering malicious impostor attacks. Substantially, our\nwork underscores the feasibility of privacy-preserving biometric verification\nand propels the prospects of its broader acceptance and application.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12820", "pdf": "https://arxiv.org/pdf/2503.12820", "abs": "https://arxiv.org/abs/2503.12820", "authors": ["Kailin Li", "Zhenxin Li", "Shiyi Lan", "Yuan Xie", "Zhizhong Zhang", "Jiayi Liu", "Zuxuan Wu", "Zhiding Yu", "Jose M. Alvarez"], "title": "Hydra-MDP++: Advancing End-to-End Driving via Expert-Guided Hydra-Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Hydra-MDP++ introduces a novel teacher-student knowledge distillation\nframework with a multi-head decoder that learns from human demonstrations and\nrule-based experts. Using a lightweight ResNet-34 network without complex\ncomponents, the framework incorporates expanded evaluation metrics, including\ntraffic light compliance (TL), lane-keeping ability (LK), and extended comfort\n(EC) to address unsafe behaviors not captured by traditional NAVSIM-derived\nteachers. Like other end-to-end autonomous driving approaches, \\hydra processes\nraw images directly without relying on privileged perception signals.\nHydra-MDP++ achieves state-of-the-art performance by integrating these\ncomponents with a 91.0% drive score on NAVSIM through scaling to a V2-99 image\nencoder, demonstrating its effectiveness in handling diverse driving scenarios\nwhile maintaining computational efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12821", "pdf": "https://arxiv.org/pdf/2503.12821", "abs": "https://arxiv.org/abs/2503.12821", "authors": ["Mingyang Song", "Xiaoye Qu", "Jiawei Zhou", "Yu Cheng"], "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved significant progress in\ncombining visual comprehension with language generation. Despite this success,\nthe training data of LVLMs still suffers from Long-Tail (LT) problems, where\nthe data distribution is highly imbalanced. Previous works have mainly focused\non traditional VLM architectures, i.e., CLIP or ViT, and specific tasks such as\nrecognition and classification. Nevertheless, the exploration of LVLM (e.g.\nLLaVA) and more general tasks (e.g. Visual Question Answering and Visual\nReasoning) remains under-explored. In this paper, we first conduct an in-depth\nanalysis of the LT issues in LVLMs and identify two core causes: the\noverrepresentation of head concepts and the underrepresentation of tail\nconcepts. Based on the above observation, we propose an $\\textbf{A}$daptive\n$\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which\nconsists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing ($\\textbf{DR}$)\nand $\\textbf{D}$ata $\\textbf{S}$ynthesis ($\\textbf{DS}$). In the DR stage, we\nadaptively rebalance the redundant data based on entity distributions, while in\nthe DS stage, we leverage Denoising Diffusion Probabilistic Models (DDPMs) and\nscarce images to supplement underrepresented portions. Through comprehensive\nevaluations across eleven benchmarks, our proposed ADR effectively mitigates\nthe long-tail problem in the training data, improving the average performance\nof LLaVA 1.5 relatively by 4.36%, without increasing the training data volume.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12834", "pdf": "https://arxiv.org/pdf/2503.12834", "abs": "https://arxiv.org/abs/2503.12834", "authors": ["Seunggwan Lee", "Hwanhee Jung", "Byoungsoo Koh", "Qixing Huang", "Sangho Yoon", "Sangpil Kim"], "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 18 figures", "summary": "A fundamental challenge in conditional 3D shape generation is to minimize the\ninformation loss and maximize the intention of user input. Existing approaches\nhave predominantly focused on two types of isolated conditional signals, i.e.,\nuser sketches and text descriptions, each of which does not offer flexible\ncontrol of the generated shape. In this paper, we introduce PASTA, the flexible\napproach that seamlessly integrates a user sketch and a text description for 3D\nshape generation. The key idea is to use text embeddings from a vision-language\nmodel to enrich the semantic representation of sketches. Specifically, these\ntext-derived priors specify the part components of the object, compensating for\nmissing visual cues from ambiguous sketches. In addition, we introduce ISG-Net\nwhich employs two types of graph convolutional networks: IndivGCN, which\nprocesses fine-grained details, and PartGCN, which aggregates these details\ninto parts and refines the structure of objects. Extensive experiments\ndemonstrate that PASTA outperforms existing methods in part-level editing and\nachieves state-of-the-art results in sketch-to-3D shape generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12836", "pdf": "https://arxiv.org/pdf/2503.12836", "abs": "https://arxiv.org/abs/2503.12836", "authors": ["Sumin In", "Youngdong Jang", "Utae Jeong", "MinHyuk Jang", "Hyeongcheol Park", "Eunbyung Park", "Sangpil Kim"], "title": "CompMarkGS: Robust Watermarking for Compression 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 17 figures", "summary": "3D Gaussian Splatting (3DGS) enables rapid differentiable rendering for 3D\nreconstruction and novel view synthesis, leading to its widespread commercial\nuse. Consequently, copyright protection via watermarking has become critical.\nHowever, because 3DGS relies on millions of Gaussians, which require gigabytes\nof storage, efficient transfer and storage require compression. Existing 3DGS\nwatermarking methods are vulnerable to quantization-based compression, often\nresulting in the loss of the embedded watermark. To address this challenge, we\npropose a novel watermarking method that ensures watermark robustness after\nmodel compression while maintaining high rendering quality. In detail, we\nincorporate a quantization distortion layer that simulates compression during\ntraining, preserving the watermark under quantization-based compression. Also,\nwe propose a learnable watermark embedding feature that embeds the watermark\ninto the anchor feature, ensuring structural consistency and seamless\nintegration into the 3D scene. Furthermore, we present a frequency-aware anchor\ngrowing mechanism to enhance image quality in high-frequency regions by\neffectively identifying Guassians within these regions. Experimental results\nconfirm that our method preserves the watermark and maintains superior image\nquality under high compression, validating it as a promising approach for a\nsecure 3DGS model.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12838", "pdf": "https://arxiv.org/pdf/2503.12838", "abs": "https://arxiv.org/abs/2503.12838", "authors": ["Junjia Huang", "Pengxiang Yan", "Jinhang Cai", "Jiyang Liu", "Zhao Wang", "Yitong Wang", "Xinglong Wu", "Guanbin Li"], "title": "DreamLayer: Simultaneous Multi-Layer Generation via Diffusion Mode", "categories": ["cs.CV"], "comment": "Under submission", "summary": "Text-driven image generation using diffusion models has recently gained\nsignificant attention. To enable more flexible image manipulation and editing,\nrecent research has expanded from single image generation to transparent layer\ngeneration and multi-layer compositions. However, existing approaches often\nfail to provide a thorough exploration of multi-layer structures, leading to\ninconsistent inter-layer interactions, such as occlusion relationships, spatial\nlayout, and shadowing. In this paper, we introduce DreamLayer, a novel\nframework that enables coherent text-driven generation of multiple image\nlayers, by explicitly modeling the relationship between transparent foreground\nand background layers. DreamLayer incorporates three key components, i.e.,\nContext-Aware Cross-Attention (CACA) for global-local information exchange,\nLayer-Shared Self-Attention (LSSA) for establishing robust inter-layer\nconnections, and Information Retained Harmonization (IRH) for refining fusion\ndetails at the latent level. By leveraging a coherent full-image context,\nDreamLayer builds inter-layer connections through attention mechanisms and\napplies a harmonization step to achieve seamless layer fusion. To facilitate\nresearch in multi-layer generation, we construct a high-quality, diverse\nmulti-layer dataset including 400k samples. Extensive experiments and user\nstudies demonstrate that DreamLayer generates more coherent and well-aligned\nlayers, with broad applicability, including latent-space image editing and\nimage-to-layer decomposition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12843", "pdf": "https://arxiv.org/pdf/2503.12843", "abs": "https://arxiv.org/abs/2503.12843", "authors": ["Haozhe Si", "Yuxuan Wan", "Minh Do", "Deepak Vasisht", "Han Zhao", "Hendrik F. Hamann"], "title": "Towards Scalable Foundation Model for Multi-modal and Hyperspectral Geospatial Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Geospatial raster (imagery) data, such as that collected by satellite-based\nimaging systems at different times and spectral bands, hold immense potential\nfor enabling a wide range of high-impact applications. This potential stems\nfrom the rich information that is spatially and temporally contextualized\nacross multiple channels and sensing modalities. Recent work has adapted\nexisting self-supervised learning approaches for such geospatial data. However,\nthey fall short of scalable model architectures, leading to inflexibility and\ncomputational inefficiencies when faced with an increasing number of channels\nand modalities. To address these limitations, we introduce Low-rank Efficient\nSpatial-Spectral Vision Transformer (LESS ViT) with three key innovations: i)\nthe LESS Attention Block that approximates high-dimensional spatial-spectral\nattention through Kronecker's product of the low-dimensional spatial and\nspectral attention components; ii) the Continuous Positional-Channel Embedding\nLayer that preserves both spatial and spectral continuity and physical\ncharacteristics of each patch; and iii) the Perception Field Mask that exploits\nlocal spatial dependencies by constraining attention to neighboring patches. To\nevaluate the proposed innovations, we construct a benchmark, GFM-Bench, which\nserves as a comprehensive benchmark for such geospatial raster data. We\npretrain LESS ViT using a Hyperspectral Masked Autoencoder framework with\nintegrated positional and channel masking strategies. Experimental results\ndemonstrate that our proposed method surpasses current state-of-the-art\nmulti-modal geospatial foundation models, achieving superior performance with\nless computation and fewer parameters. The flexibility and extensibility of our\nframework make it a promising direction for future geospatial data analysis\ntasks that involve a wide range of modalities and channels.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12853", "pdf": "https://arxiv.org/pdf/2503.12853", "abs": "https://arxiv.org/abs/2503.12853", "authors": ["Yanlin Xiang", "Qingyuan He", "Ting Xu", "Ran Hao", "Jiacheng Hu", "Hanchao Zhang"], "title": "Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12855", "pdf": "https://arxiv.org/pdf/2503.12855", "abs": "https://arxiv.org/abs/2503.12855", "authors": ["Yujie Lu", "Yale Song", "William Wang", "Lorenzo Torresani", "Tushar Nagarajan"], "title": "VITED: Video Temporal Evidence Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12862", "pdf": "https://arxiv.org/pdf/2503.12862", "abs": "https://arxiv.org/abs/2503.12862", "authors": ["Yu-Ting Zhan", "He-bi Yang", "Cheng-Yuan Ho", "Jui-Chiu Chiang", "Wen-Hsiao Peng"], "title": "CAT-3DGS Pro: A New Benchmark for Efficient 3DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has shown immense potential for novel view\nsynthesis. However, achieving rate-distortion-optimized compression of 3DGS\nrepresentations for transmission and/or storage applications remains a\nchallenge. CAT-3DGS introduces a context-adaptive triplane hyperprior for\nend-to-end optimized compression, delivering state-of-the-art coding\nperformance. Despite this, it requires prolonged training and decoding time. To\naddress these limitations, we propose CAT-3DGS Pro, an enhanced version of\nCAT-3DGS that improves both compression performance and computational\nefficiency. First, we introduce a PCA-guided vector-matrix hyperprior, which\nreplaces the triplane-based hyperprior to reduce redundant parameters. To\nachieve a more balanced rate-distortion trade-off and faster encoding, we\npropose an alternate optimization strategy (A-RDO). Additionally, we refine the\nsampling rate optimization method in CAT-3DGS, leading to significant\nimprovements in rate-distortion performance. These enhancements result in a\n46.6% BD-rate reduction and 3x speedup in training time on BungeeNeRF, while\nachieving 5x acceleration in decoding speed for the Amsterdam scene compared to\nCAT-3DGS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12866", "pdf": "https://arxiv.org/pdf/2503.12866", "abs": "https://arxiv.org/abs/2503.12866", "authors": ["Chenyu Zhang", "Kunlun Xu", "Zichen Liu", "Yuxin Peng", "Jiahuan Zhou"], "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Vision-language models (VLMs) encounter considerable challenges when adapting\nto domain shifts stemming from changes in data distribution. Test-time\nadaptation (TTA) has emerged as a promising approach to enhance VLM performance\nunder such conditions. In practice, test data often arrives in batches, leading\nto increasing interest in the transductive TTA setting. However, existing TTA\nmethods primarily focus on individual test samples, overlooking crucial\ncross-sample correlations within a batch. While recent ViT-based TTA methods\nhave introduced batch-level adaptation, they remain suboptimal for VLMs due to\ninadequate integration of the text modality. To address these limitations, we\npropose a novel transductive TTA framework, Supportive Clique-based Attribute\nPrompting (SCAP), which effectively combines visual and textual information to\nenhance adaptation by generating fine-grained attribute prompts across test\nbatches. SCAP first forms supportive cliques of test samples in an unsupervised\nmanner based on visual similarity and learns an attribute prompt for each\nclique, capturing shared attributes critical for adaptation. For each test\nsample, SCAP aggregates attribute prompts from its associated cliques,\nproviding enriched contextual information. To ensure adaptability over time, we\nincorporate a retention module that dynamically updates attribute prompts and\ntheir associated attributes as new data arrives. Comprehensive experiments\nacross multiple benchmarks demonstrate that SCAP outperforms existing\nstate-of-the-art methods, significantly advancing VLM generalization under\ndomain shifts. Our code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-SCAP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12888", "pdf": "https://arxiv.org/pdf/2503.12888", "abs": "https://arxiv.org/abs/2503.12888", "authors": ["Siyuan Yao", "Yang Guo", "Yanyang Yan", "Wenqi Ren", "Xiaochun Cao"], "title": "UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network", "categories": ["cs.CV"], "comment": "14 pages,11 figures,references added", "summary": "Transformer-based trackers have achieved promising success and become the\ndominant tracking paradigm due to their accuracy and efficiency. Despite the\nsubstantial progress, most of the existing approaches tackle object tracking as\na deterministic coordinate regression problem, while the target localization\nuncertainty has been greatly overlooked, which hampers trackers' ability to\nmaintain reliable target state prediction in challenging scenarios. To address\nthis issue, we propose UncTrack, a novel uncertainty-aware transformer tracker\nthat predicts the target localization uncertainty and incorporates this\nuncertainty information for accurate target state inference. Specifically,\nUncTrack utilizes a transformer encoder to perform feature interaction between\ntemplate and search images. The output features are passed into an\nuncertainty-aware localization decoder (ULD) to coarsely predict the\ncorner-based localization and the corresponding localization uncertainty. Then\nthe localization uncertainty is sent into a prototype memory network (PMN) to\nexcavate valuable historical information to identify whether the target state\nprediction is reliable or not. To enhance the template representation, the\nsamples with high confidence are fed back into the prototype memory bank for\nmemory updating, making the tracker more robust to challenging appearance\nvariations. Extensive experiments demonstrate that our method outperforms other\nstate-of-the-art methods. Our code is available at\nhttps://github.com/ManOfStory/UncTrack.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12929", "pdf": "https://arxiv.org/pdf/2503.12929", "abs": "https://arxiv.org/abs/2503.12929", "authors": ["Xuying Zhang", "Yupeng Zhou", "Kai Wang", "Yikai Wang", "Zhen Li", "Xiuli Shao", "Daquan Zhou", "Qibin Hou", "Ming-Ming Cheng"], "title": "AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.\nHowever, existing works still struggle to maintain consistency between the\ngenerated views and the input views, especially when there is a significant\ncamera pose difference, leading to poor-quality 3D geometries and textures. We\nattribute this issue to their treatment of all target views with equal priority\naccording to our empirical observation that the target views closer to the\ninput views exhibit higher fidelity. With this inspiration, we propose\nAR-1-to-3, a novel next-view prediction paradigm based on diffusion models that\nfirst generates views close to the input views, which are then utilized as\ncontextual information to progressively synthesize farther views. To encode the\ngenerated view subsequences as local and global conditions for the next-view\nprediction, we accordingly develop a stacked local feature encoding strategy\n(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).\nExtensive experiments demonstrate that our method significantly improves the\nconsistency between the generated views and the input views, producing\nhigh-fidelity 3D assets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12935", "pdf": "https://arxiv.org/pdf/2503.12935", "abs": "https://arxiv.org/abs/2503.12935", "authors": ["Guoliang Xu", "Jianqin Yin", "Ren Zhang", "Yonghao Dang", "Feng Zhou", "Bo Yu"], "title": "L2HCount:Generalizing Crowd Counting from Low to High Crowd Density via Density Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Since COVID-19, crowd-counting tasks have gained wide applications. While\nsupervised methods are reliable, annotation is more challenging in high-density\nscenes due to small head sizes and severe occlusion, whereas it's simpler in\nlow-density scenes. Interestingly, can we train the model in low-density scenes\nand generalize it to high-density scenes? Therefore, we propose a low- to\nhigh-density generalization framework (L2HCount) that learns the pattern\nrelated to high-density scenes from low-density ones, enabling it to generalize\nwell to high-density scenes. Specifically, we first introduce a High-Density\nSimulation Module and a Ground-Truth Generation Module to construct fake\nhigh-density images along with their corresponding ground-truth crowd\nannotations respectively by image-shifting technique, effectively simulating\nhigh-density crowd patterns. However, the simulated images have two issues:\nimage blurring and loss of low-density image characteristics. Therefore, we\nsecond propose a Head Feature Enhancement Module to extract clear features in\nthe simulated high-density scene. Third, we propose a Dual-Density Memory\nEncoding Module that uses two crowd memories to learn scene-specific patterns\nfrom low- and simulated high-density scenes, respectively. Extensive\nexperiments on four challenging datasets have shown the promising performance\nof L2HCount.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12944", "pdf": "https://arxiv.org/pdf/2503.12944", "abs": "https://arxiv.org/abs/2503.12944", "authors": ["Jianzheng Huang", "Xianyu Mo", "Ziling Liu", "Jinyu Yang", "Feng Zheng"], "title": "GIFT: Generated Indoor video frames for Texture-less point tracking", "categories": ["cs.CV"], "comment": null, "summary": "Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12947", "pdf": "https://arxiv.org/pdf/2503.12947", "abs": "https://arxiv.org/abs/2503.12947", "authors": ["Ingyun Lee", "Jae Won Jang", "Seunghyeon Seo", "Nojun Kwak"], "title": "DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency for Few-shot View Synthesis", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view\nsynthesis but requires many multiview images, making it impractical for\nfew-shot scenarios. Ray augmentation was proposed to prevent overfitting for\nsparse training data by generating additional rays. However, existing methods,\nwhich generate augmented rays only near the original rays, produce severe\nfloaters and appearance distortion due to limited viewpoints and inconsistent\nrays obstructed by nearby obstacles and complex surfaces. To address these\nproblems, we propose DivCon-NeRF, which significantly enhances both diversity\nand consistency. It employs surface-sphere augmentation, which preserves the\ndistance between the original camera and the predicted surface point. This\nallows the model to compare the order of high-probability surface points and\nfilter out inconsistent rays easily without requiring the exact depth. By\nintroducing inner-sphere augmentation, DivCon-NeRF randomizes angles and\ndistances for diverse viewpoints, further increasing diversity. Consequently,\nour method significantly reduces floaters and visual distortions, achieving\nstate-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code\nwill be publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12963", "pdf": "https://arxiv.org/pdf/2503.12963", "abs": "https://arxiv.org/abs/2503.12963", "authors": ["Chaolong Yang", "Kai Yao", "Yuyao Yan", "Chenru Jiang", "Weiguang Zhao", "Jie Sun", "Guangliang Cheng", "Yifei Zhang", "Bin Dong", "Kaizhu Huang"], "title": "Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based Spatiotemporal Diffusion for Audio-driven Talking Portrait", "categories": ["cs.CV"], "comment": null, "summary": "Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12964", "pdf": "https://arxiv.org/pdf/2503.12964", "abs": "https://arxiv.org/abs/2503.12964", "authors": ["Zeeshan Patel", "Ethan He", "Parth Mannan", "Xiaowei Ren", "Ryan Wolf", "Niket Agarwal", "Jacob Huffman", "Zhuoyao Wang", "Carl Wang", "Jack Chang", "Yan Bai", "Tommy Huang", "Linnan Wang", "Sahil Jain", "Shanmugam Ramasamy", "Joseph Jennings", "Ekaterina Sirazitdinova", "Oleg Sudakov", "Mingyuan Ma", "Bobby Chen", "Forrest Lin", "Hao Wang", "Vasanth Rao Naik Sabavat", "Sriharsha Niverty", "Rong Ou", "Pallab Bhattacharya", "David Page", "Nima Tajbakhsh", "Ashwath Aithal"], "title": "Training Video Foundation Models with NVIDIA NeMo", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12972", "pdf": "https://arxiv.org/pdf/2503.12972", "abs": "https://arxiv.org/abs/2503.12972", "authors": ["Junming Liu", "Siyuan Meng", "Yanting Gao", "Song Mao", "Pinlong Cai", "Guohang Yan", "Yirong Chen", "Zilin Bian", "Botian Shi", "Ding Wang"], "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 7 figures, 6 tables", "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12973", "pdf": "https://arxiv.org/pdf/2503.12973", "abs": "https://arxiv.org/abs/2503.12973", "authors": ["Colin Prieur", "Nassim Ait Ali Braham", "Paul Tresson", "Grégoire Vincent", "Jocelyn Chanussot"], "title": "Prospects for Mitigating Spectral Variability in Tropical Species Classification Using Self-Supervised Learning", "categories": ["cs.CV"], "comment": "5 pages, 3 figures, published as proceeding of the \"2024 14th\n  Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote\n  Sensing (WHISPERS)\"", "summary": "Airborne hyperspectral imaging is a promising method for identifying tropical\nspecies, but spectral variability between acquisitions hinders consistent\nresults. This paper proposes using Self-Supervised Learning (SSL) to encode\nspectral features that are robust to abiotic variability and relevant for\nspecies identification. By employing the state-of-the-art Barlow-Twins approach\non repeated spectral acquisitions, we demonstrate the ability to develop stable\nfeatures. For the classification of 40 tropical species, experiments show that\nthese features can outperform typical reflectance products in terms of\nrobustness to spectral variability by 10 points of accuracy across dates.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12981", "pdf": "https://arxiv.org/pdf/2503.12981", "abs": "https://arxiv.org/abs/2503.12981", "authors": ["Thu Tran", "Kenny Tsu Wei Choo", "Shaohui Foong", "Hitesh Bhardwaj", "Shane Kyi Hla Win", "Wei Jun Ang", "Kenneth Goh", "Rajesh Krishna Balan"], "title": "Analyzing Swimming Performance Using Drone Captured Aerial Videos", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, published to ACM Dronet'24", "summary": "Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m/s for stroke duration and velocity, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12982", "pdf": "https://arxiv.org/pdf/2503.12982", "abs": "https://arxiv.org/abs/2503.12982", "authors": ["Yunshuang Yuan", "Yan Xia", "Daniel Cremers", "Monika Sester"], "title": "SparseAlign: A Fully Sparse Framework for Cooperative Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception can increase the view field and decrease the occlusion\nof an ego vehicle, hence improving the perception performance and safety of\nautonomous driving. Despite the success of previous works on cooperative object\ndetection, they mostly operate on dense Bird's Eye View (BEV) feature maps,\nwhich are computationally demanding and can hardly be extended to long-range\ndetection problems. More efficient fully sparse frameworks are rarely explored.\nIn this work, we design a fully sparse framework, SparseAlign, with three key\nfeatures: an enhanced sparse 3D backbone, a query-based temporal context\nlearning module, and a robust detection head specially tailored for sparse\nfeatures. Extensive experimental results on both OPV2V and DairV2X datasets\nshow that our framework, despite its sparsity, outperforms the state of the art\nwith less communication bandwidth requirements. In addition, experiments on the\nOPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also\nshow a significant performance gain compared to the baseline works.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13004", "pdf": "https://arxiv.org/pdf/2503.13004", "abs": "https://arxiv.org/abs/2503.13004", "authors": ["Jiaxu Liu", "Li Li", "Hubert P. H. Shum", "Toby P. Breckon"], "title": "TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13012", "pdf": "https://arxiv.org/pdf/2503.13012", "abs": "https://arxiv.org/abs/2503.13012", "authors": ["Xingguo Lv", "Xingbo Dong", "Liwen Wang", "Jiewen Yang", "Lei Zhao", "Bin Pu", "Zhe Jin", "Xuejun Li"], "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13016", "pdf": "https://arxiv.org/pdf/2503.13016", "abs": "https://arxiv.org/abs/2503.13016", "authors": ["Zijia Zhao", "Yuqi Huo", "Tongtian Yue", "Longteng Guo", "Haoyu Lu", "Bingning Wang", "Weipeng Chen", "Jing Liu"], "title": "Efficient Motion-Aware Video MLLM", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13023", "pdf": "https://arxiv.org/pdf/2503.13023", "abs": "https://arxiv.org/abs/2503.13023", "authors": ["Michal Danilowicz", "Tomasz Kryjak"], "title": "Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA", "categories": ["cs.CV"], "comment": "Accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025", "summary": "Multi-object tracking (MOT) is one of the most important problems in computer\nvision and a key component of any vision-based perception system used in\nadvanced autonomous mobile robotics. Therefore, its implementation on low-power\nand real-time embedded platforms is highly desirable. Modern MOT algorithms\nshould be able to track objects of a given class (e.g. people or vehicles). In\naddition, the number of objects to be tracked is not known in advance, and they\nmay appear and disappear at any time, as well as be obscured. For these\nreasons, the most popular and successful approaches have recently been based on\nthe tracking paradigm. Therefore, the presence of a high quality object\ndetector is essential, which in practice accounts for the vast majority of the\ncomputational and memory complexity of the whole MOT system. In this paper, we\npropose an FPGA (Field-Programmable Gate Array) implementation of an embedded\nMOT system based on a quantized YOLOv8 detector and the SORT (Simple Online\nRealtime Tracker) tracker. We use a modified version of the FINN framework to\nutilize external memory for model parameters and to support operations\nnecessary required by YOLOv8. We discuss the evaluation of detection and\ntracking performance using the COCO and MOT15 datasets, where we achieve 0.21\nmAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC\nsystem (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed\nin reprogrammable logic and the tracking algorithm is implemented in the\nprocessor system.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13028", "pdf": "https://arxiv.org/pdf/2503.13028", "abs": "https://arxiv.org/abs/2503.13028", "authors": ["Tony Danjun Wang", "Lennart Bastian", "Tobias Czempiel", "Christian Heiliger", "Nassir Navab"], "title": "Beyond Role-Based Surgical Domain Modeling: Generalizable Re-Identification in the Operating Room", "categories": ["cs.CV", "J.3"], "comment": "26 pages, 14 figures, Submitted to Medical Image Analysis", "summary": "Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13045", "pdf": "https://arxiv.org/pdf/2503.13045", "abs": "https://arxiv.org/abs/2503.13045", "authors": ["Gabriele Berton", "Kevin Musgrave", "Carlo Masone"], "title": "All You Need to Know About Training Image Retrieval Models", "categories": ["cs.CV"], "comment": null, "summary": "Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https://github.com/gmberton/image-retrieval", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13047", "pdf": "https://arxiv.org/pdf/2503.13047", "abs": "https://arxiv.org/abs/2503.13047", "authors": ["Ruiqi Song", "Xianda Guo", "Hangbin Wu", "Qinggong Wei", "Long Chen"], "title": "InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Directly generating planning results from raw sensors has become increasingly\nprevalent due to its adaptability and robustness in complex scenarios. Scene\nrepresentation, as a key module in the pipeline, has traditionally relied on\nconventional perception, which focus on the global scene. However, in driving\nscenarios, human drivers typically focus only on regions that directly impact\ndriving, which often coincide with those required for end-to-end autonomous\ndriving. In this paper, a novel end-to-end autonomous driving method called\nInsightDrive is proposed, which organizes perception by language-guided scene\nrepresentation. We introduce an instance-centric scene tokenizer that\ntransforms the surrounding environment into map- and object-aware instance\ntokens. Scene attention language descriptions, which highlight key regions and\nobstacles affecting the ego vehicle's movement, are generated by a\nvision-language model that leverages the cognitive reasoning capabilities of\nfoundation models. We then align scene descriptions with visual features using\nthe vision-language model, guiding visual attention through these descriptions\nto give effectively scene representation. Furthermore, we employ self-attention\nand cross-attention mechanisms to model the ego-agents and ego-map\nrelationships to comprehensively build the topological relationships of the\nscene. Finally, based on scene understanding, we jointly perform motion\nprediction and planning. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate that the proposed InsightDrive achieves state-of-the-art\nperformance in end-to-end autonomous driving. The code is available at\nhttps://github.com/songruiqi/InsightDrive", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13058", "pdf": "https://arxiv.org/pdf/2503.13058", "abs": "https://arxiv.org/abs/2503.13058", "authors": ["Zeyi Huang", "Utkarsh Ojha", "Yuyang Ji", "Donghyun Lee", "Yong Jae Lee"], "title": "Do Vision Models Develop Human-Like Progressive Difficulty Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "When a human undertakes a test, their responses likely follow a pattern: if\nthey answered an easy question $(2 \\times 3)$ incorrectly, they would likely\nanswer a more difficult one $(2 \\times 3 \\times 4)$ incorrectly; and if they\nanswered a difficult question correctly, they would likely answer the easy one\ncorrectly. Anything else hints at memorization. Do current visual recognition\nmodels exhibit a similarly structured learning capacity? In this work, we\nconsider the task of image classification and study if those models' responses\nfollow that pattern. Since real images aren't labeled with difficulty, we first\ncreate a dataset of 100 categories, 10 attributes, and 3 difficulty levels\nusing recent generative models: for each category (e.g., dog) and attribute\n(e.g., occlusion), we generate images of increasing difficulty (e.g., a dog\nwithout occlusion, a dog only partly visible). We find that most of the models\ndo in fact behave similarly to the aforementioned pattern around 80-90% of the\ntime. Using this property, we then explore a new way to evaluate those models.\nInstead of testing the model on every possible test image, we create an\nadaptive test akin to GRE, in which the model's performance on the current\nround of images determines the test images in the next round. This allows the\nmodel to skip over questions too easy/hard for itself, and helps us get its\noverall performance in fewer steps.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13060", "pdf": "https://arxiv.org/pdf/2503.13060", "abs": "https://arxiv.org/abs/2503.13060", "authors": ["Harshal Kausadikar", "Tanvi Kale", "Onkar Susladkar", "Sparsh Mittal"], "title": "Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework for Transliteration of Modi Script to Devanagari", "categories": ["cs.CV"], "comment": "Under submission at a conference", "summary": "In medieval India, the Marathi language was written using the Modi script.\nThe texts written in Modi script include extensive knowledge about medieval\nsciences, medicines, land records and authentic evidence about Indian history.\nAround 40 million documents are in poor condition and have not yet been\ntransliterated. Furthermore, only a few experts in this domain can\ntransliterate this script into English or Devanagari. Most of the past research\npredominantly focuses on individual character recognition. A system that can\ntransliterate Modi script documents to Devanagari script is needed. We propose\nthe MoDeTrans dataset, comprising 2,043 images of Modi script documents\naccompanied by their corresponding textual transliterations in Devanagari. We\nfurther introduce MoScNet (\\textbf{Mo}di \\textbf{Sc}ript \\textbf{Net}work), a\nnovel Vision-Language Model (VLM) framework for transliterating Modi script\nimages into Devanagari text. MoScNet leverages Knowledge Distillation, where a\nstudent model learns from a teacher model to enhance transliteration\nperformance. The final student model of MoScNet has better performance than the\nteacher model while having 163$\\times$ lower parameters. Our work is the first\nto perform direct transliteration from the handwritten Modi script to the\nDevanagari script. MoScNet also shows competitive results on the optical\ncharacter recognition (OCR) task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13063", "pdf": "https://arxiv.org/pdf/2503.13063", "abs": "https://arxiv.org/abs/2503.13063", "authors": ["Zheng Wang", "Zihui Wang", "Zheng Wang", "Xiaoliang Fan", "Cheng Wang"], "title": "Federated Learning with Domain Shift Eraser", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Federated learning (FL) is emerging as a promising technique for\ncollaborative learning without local data leaving their devices. However,\nclients' data originating from diverse domains may degrade model performance\ndue to domain shifts, preventing the model from learning consistent\nrepresentation space. In this paper, we propose a novel FL framework, Federated\nDomain Shift Eraser (FDSE), to improve model performance by differently erasing\neach client's domain skew and enhancing their consensus. First, we formulate\nthe model forward passing as an iterative deskewing process that extracts and\nthen deskews features alternatively. This is efficiently achieved by\ndecomposing each original layer in the neural network into a Domain-agnostic\nFeature Extractor (DFE) and a Domain-specific Skew Eraser (DSE). Then, a\nregularization term is applied to promise the effectiveness of feature\ndeskewing by pulling local statistics of DSE's outputs close to the globally\nconsistent ones. Finally, DFE modules are fairly aggregated and broadcast to\nall the clients to maximize their consensus, and DSE modules are personalized\nfor each client via similarity-aware aggregation to erase their domain skew\ndifferently. Comprehensive experiments were conducted on three datasets to\nconfirm the advantages of our method in terms of accuracy, efficiency, and\ngeneralizability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13068", "pdf": "https://arxiv.org/pdf/2503.13068", "abs": "https://arxiv.org/abs/2503.13068", "authors": ["Henghui Du", "Guangyao Li", "Chang Zhou", "Chunjie Zhang", "Alan Zhao", "Di Hu"], "title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, numerous tasks have been proposed to encourage model to\ndevelop specified capability in understanding audio-visual scene, primarily\ncategorized into temporal localization, spatial localization, spatio-temporal\nreasoning, and pixel-level understanding. Instead, human possesses a unified\nunderstanding ability for diversified tasks. Therefore, designing an\naudio-visual model with general capability to unify these tasks is of great\nvalue. However, simply joint training for all tasks can lead to interference\ndue to the heterogeneity of audiovisual data and complex relationship among\ntasks. We argue that this problem can be solved through explicit cooperation\namong tasks. To achieve this goal, we propose a unified learning method which\nachieves explicit inter-task cooperation from both the perspectives of data and\nmodel thoroughly. Specifically, considering the labels of existing datasets are\nsimple words, we carefully refine these datasets and construct an Audio-Visual\nUnified Instruction-tuning dataset with Explicit reasoning process (AV-UIE),\nwhich clarifies the cooperative relationship among tasks. Subsequently, to\nfacilitate concrete cooperation in learning stage, an interaction-aware LoRA\nstructure with multiple LoRA heads is designed to learn different aspects of\naudiovisual data interaction. By unifying the explicit cooperation across the\ndata and model aspect, our method not only surpasses existing unified\naudio-visual model on multiple tasks, but also outperforms most specialized\nmodels for certain tasks. Furthermore, we also visualize the process of\nexplicit cooperation and surprisingly find that each LoRA head has certain\naudio-visual understanding ability. Code and dataset:\nhttps://github.com/GeWu-Lab/Crab", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13086", "pdf": "https://arxiv.org/pdf/2503.13086", "abs": "https://arxiv.org/abs/2503.13086", "authors": ["Yiwei Xu", "Yifei Yu", "Wentian Gan", "Tengfei Wang", "Zongqian Zhan", "Hao Cheng", "Xin Wang"], "title": "Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near Real-Time 3DGS Optimization", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13107", "pdf": "https://arxiv.org/pdf/2503.13107", "abs": "https://arxiv.org/abs/2503.13107", "authors": ["Hao Yin", "Guangzong Si", "Zilei Wang"], "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13110", "pdf": "https://arxiv.org/pdf/2503.13110", "abs": "https://arxiv.org/abs/2503.13110", "authors": ["Jing Li", "Yihang Fu", "Falai Chen"], "title": "DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry", "categories": ["cs.CV"], "comment": null, "summary": "Boundary representation (B-rep) of geometric models is a fundamental format\nin Computer-Aided Design (CAD). However, automatically generating valid and\nhigh-quality B-rep models remains challenging due to the complex\ninterdependence between the topology and geometry of the models. Existing\nmethods tend to prioritize geometric representation while giving insufficient\nattention to topological constraints, making it difficult to maintain\nstructural validity and geometric accuracy. In this paper, we propose\nDTGBrepGen, a novel topology-geometry decoupled framework for B-rep generation\nthat explicitly addresses both aspects. Our approach first generates valid\ntopological structures through a two-stage process that independently models\nedge-face and edge-vertex adjacency relationships. Subsequently, we employ\nTransformer-based diffusion models for sequential geometry generation,\nprogressively generating vertex coordinates, followed by edge geometries and\nface geometries which are represented as B-splines. Extensive experiments on\ndiverse CAD datasets show that DTGBrepGen significantly outperforms existing\nmethods in both topological validity and geometric accuracy, achieving higher\nvalidity rates and producing more diverse and realistic B-reps. Our code is\npublicly available at https://github.com/jinli99/DTGBrepGen.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13120", "pdf": "https://arxiv.org/pdf/2503.13120", "abs": "https://arxiv.org/abs/2503.13120", "authors": ["Siyuan Fan", "Wenke Huang", "Xiantao Cai", "Bo Du"], "title": "3D Human Interaction Generation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "3D human interaction generation has emerged as a key research area, focusing\non producing dynamic and contextually relevant interactions between humans and\nvarious interactive entities. Recent rapid advancements in 3D model\nrepresentation methods, motion capture technologies, and generative models have\nlaid a solid foundation for the growing interest in this domain. Existing\nresearch in this field can be broadly categorized into three areas: human-scene\ninteraction, human-object interaction, and human-human interaction. Despite the\nrapid advancements in this area, challenges remain due to the need for\nnaturalness in human motion generation and the accurate interaction between\nhumans and interactive entities. In this survey, we present a comprehensive\nliterature review of human interaction generation, which, to the best of our\nknowledge, is the first of its kind. We begin by introducing the foundational\ntechnologies, including model representations, motion capture methods, and\ngenerative models. Subsequently, we introduce the approaches proposed for the\nthree sub-tasks, along with their corresponding datasets and evaluation\nmetrics. Finally, we discuss potential future research directions in this area\nand conclude the survey. Through this survey, we aim to offer a comprehensive\noverview of the current advancements in the field, highlight key challenges,\nand inspire future research works.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13139", "pdf": "https://arxiv.org/pdf/2503.13139", "abs": "https://arxiv.org/abs/2503.13139", "authors": ["Weiyu Guo", "Ziyang Chen", "Shaoguang Wang", "Jianxiang He", "Yijie Xu", "Jinhui Ye", "Ying Sun", "Hui Xiong"], "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "comment": "18 pages, under review", "summary": "Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13147", "pdf": "https://arxiv.org/pdf/2503.13147", "abs": "https://arxiv.org/abs/2503.13147", "authors": ["Jiayi Fu", "Siyu Liu", "Zikun Liu", "Chun-Le Guo", "Hyunhee Park", "Ruiqi Wu", "Guoqing Wang", "Chongyi Li"], "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Acceptted by CVPR 2025", "summary": "We propose a novel Iterative Predictor-Critic Code Decoding framework for\nreal-world image dehazing, abbreviated as IPC-Dehaze, which leverages the\nhigh-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from\nprevious codebook-based methods that rely on one-shot decoding, our method\nutilizes high-quality codes obtained in the previous iteration to guide the\nprediction of the Code-Predictor in the subsequent iteration, improving code\nprediction accuracy and ensuring stable dehazing performance. Our idea stems\nfrom the observations that 1) the degradation of hazy images varies with haze\ndensity and scene depth, and 2) clear regions play crucial cues in restoring\ndense haze regions. However, it is non-trivial to progressively refine the\nobtained codes in subsequent iterations, owing to the difficulty in determining\nwhich codes should be retained or replaced at each iteration. Another key\ninsight of our study is to propose Code-Critic to capture interrelations among\ncodes. The Code-Critic is used to evaluate code correlations and then resample\na set of codes with the highest mask scores, i.e., a higher score indicates\nthat the code is more likely to be rejected, which helps retain more accurate\ncodes and predict difficult ones. Extensive experiments demonstrate the\nsuperiority of our method over state-of-the-art methods in real-world dehazing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13160", "pdf": "https://arxiv.org/pdf/2503.13160", "abs": "https://arxiv.org/abs/2503.13160", "authors": ["Zihao Liu", "Xiaoyu Wu", "Jianqin Wu", "Xuxu Wang", "Linlin Yang"], "title": "Language-guided Open-world Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13185", "pdf": "https://arxiv.org/pdf/2503.13185", "abs": "https://arxiv.org/abs/2503.13185", "authors": ["Dingning Liu", "Cheng Wang", "Peng Gao", "Renrui Zhang", "Xinzhu Ma", "Yuan Meng", "Zhihui Wang"], "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt\nengineering techniques.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13188", "pdf": "https://arxiv.org/pdf/2503.13188", "abs": "https://arxiv.org/abs/2503.13188", "authors": ["Matteo Sodano", "Federico Magistri", "Elias Marks", "Fares Hosn", "Aibek Zurbayev", "Rodrigo Marcuzzi", "Meher V. R. Malladi", "Jens Behley", "Cyrill Stachniss"], "title": "3D Hierarchical Panoptic Segmentation in Real Orchard Environments Across Different Sensors", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to IROS", "summary": "Crop yield estimation is a relevant problem in agriculture, because an\naccurate crop yield estimate can support farmers' decisions on harvesting or\nprecision intervention. Robots can help to automate this process. To do so,\nthey need to be able to perceive the surrounding environment to identify target\nobjects. In this paper, we introduce a novel approach to address the problem of\nhierarchical panoptic segmentation of apple orchards on 3D data from different\nsensors. Our approach is able to simultaneously provide semantic segmentation,\ninstance segmentation of trunks and fruits, and instance segmentation of plants\n(a single trunk with its fruits). This allows us to identify relevant\ninformation such as individual plants, fruits, and trunks, and capture the\nrelationship among them, such as precisely estimate the number of fruits\nassociated to each tree in an orchard. Additionally, to efficiently evaluate\nour approach for hierarchical panoptic segmentation, we provide a dataset\ndesigned specifically for this task. Our dataset is recorded in Bonn in a real\napple orchard with a variety of sensors, spanning from a terrestrial laser\nscanner to a RGB-D camera mounted on different robotic platforms. The\nexperiments show that our approach surpasses state-of-the-art approaches in 3D\npanoptic segmentation in the agricultural domain, while also providing full\nhierarchical panoptic segmentation. Our dataset has been made publicly\navailable at https://www.ipb.uni-bonn.de/data/hops/. We will provide the\nopen-source implementation of our approach and public competiton for\nhierarchical panoptic segmentation on the hidden test sets upon paper\nacceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13211", "pdf": "https://arxiv.org/pdf/2503.13211", "abs": "https://arxiv.org/abs/2503.13211", "authors": ["Marvin Seyfarth", "Salman Ul Hassan Dar", "Isabelle Ayx", "Matthias Alexander Fink", "Stefan O. Schoenberg", "Hans-Ulrich Kauczor", "Sandy Engelhardt"], "title": "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D CT Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13214", "pdf": "https://arxiv.org/pdf/2503.13214", "abs": "https://arxiv.org/abs/2503.13214", "authors": ["Jie Huang", "Haorui Chen", "Jiaxuan Ren", "Siran Peng", "Liangjian Deng"], "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": "This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025", "summary": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13229", "pdf": "https://arxiv.org/pdf/2503.13229", "abs": "https://arxiv.org/abs/2503.13229", "authors": ["Yongkang Cheng", "Shaoli Huang"], "title": "HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures", "categories": ["cs.CV"], "comment": "Accepted by 3DV 2025", "summary": "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13260", "pdf": "https://arxiv.org/pdf/2503.13260", "abs": "https://arxiv.org/abs/2503.13260", "authors": ["Amit Zalcher", "Navve Wasserman", "Roman Beliy", "Oliver Heinimann", "Michal Irani"], "title": "Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual perceptual tasks aim to predict human judgment of images (e.g.,\nemotions invoked by images, image quality assessment). Unlike objective tasks\nsuch as object/scene recognition, perceptual tasks rely on subjective human\nassessments, making its data-labeling difficult. The scarcity of such\nhuman-annotated data results in small datasets leading to poor generalization.\nTypically, specialized models were designed for each perceptual task, tailored\nto its unique characteristics and its own training dataset. We propose a\nunified architectural framework for solving multiple different perceptual tasks\nleveraging CLIP as a prior. Our approach is based on recent cognitive findings\nwhich indicate that CLIP correlates well with human judgment. While CLIP was\nexplicitly trained to align images and text, it implicitly also learned human\ninclinations. We attribute this to the inclusion of human-written image\ncaptions in CLIP's training data, which contain not only factual image\ndescriptions, but inevitably also human sentiments and emotions. This makes\nCLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest\nthat minimal adaptation of CLIP suffices for solving a variety of perceptual\ntasks. Our simple unified framework employs a lightweight adaptation to\nfine-tune CLIP to each task, without requiring any task-specific architectural\nchanges. We evaluate our approach on three tasks: (i) Image Memorability\nPrediction, (ii) No-reference Image Quality Assessment, and (iii) Visual\nEmotion Analysis. Our model achieves state-of-the-art results on all three\ntasks, while demonstrating improved generalization across different datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13385", "pdf": "https://arxiv.org/pdf/2503.13385", "abs": "https://arxiv.org/abs/2503.13385", "authors": ["Qing Zhou", "Junyu Gao", "Qi Wang"], "title": "Scale Efficient Training for Large Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13424", "pdf": "https://arxiv.org/pdf/2503.13424", "abs": "https://arxiv.org/abs/2503.13424", "authors": ["Xinyu Lian", "Zichao Yu", "Ruiming Liang", "Yitong Wang", "Li Ray Luo", "Kaixu Chen", "Yuanzhen Zhou", "Qihong Tang", "Xudong Xu", "Zhaoyang Lyu", "Bo Dai", "Jiangmiao Pang"], "title": "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated Objects via Procedural Generation", "categories": ["cs.CV"], "comment": "Project page: https://infinite-mobility.github.io 10 pages,12 figures", "summary": "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13429", "pdf": "https://arxiv.org/pdf/2503.13429", "abs": "https://arxiv.org/abs/2503.13429", "authors": ["Nhi Pham", "Bernt Schiele", "Adam Kortylewski", "Jonas Fischer"], "title": "Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes", "categories": ["cs.CV"], "comment": null, "summary": "With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13434", "pdf": "https://arxiv.org/pdf/2503.13434", "abs": "https://arxiv.org/abs/2503.13434", "authors": ["Yaowei Li", "Lingen Li", "Zhaoyang Zhang", "Xiaoyu Li", "Guangzhi Wang", "Hongxiang Li", "Xiaodong Cun", "Ying Shan", "Yuexian Zou"], "title": "BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/", "summary": "Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13436", "pdf": "https://arxiv.org/pdf/2503.13436", "abs": "https://arxiv.org/abs/2503.13436", "authors": ["Lijie Fan", "Luming Tang", "Siyang Qin", "Tianhong Li", "Xuan Yang", "Siyuan Qiao", "Andreas Steiner", "Chen Sun", "Yuanzhen Li", "Tao Zhu", "Michael Rubinstein", "Michalis Raptis", "Deqing Sun", "Radu Soricut"], "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Tech report", "summary": "We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13439", "pdf": "https://arxiv.org/pdf/2503.13439", "abs": "https://arxiv.org/abs/2503.13439", "authors": ["Tianhao Wu", "Chuanxia Zheng", "Frank Guan", "Andrea Vedaldi", "Tat-Jen Cham"], "title": "Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images", "categories": ["cs.CV"], "comment": "Project Page: https://sm0kywu.github.io/Amodal3R/", "summary": "Most image-based 3D object reconstructors assume that objects are fully\nvisible, ignoring occlusions that commonly occur in real-world scenarios. In\nthis paper, we introduce Amodal3R, a conditional 3D generative model designed\nto reconstruct 3D objects from partial observations. We start from a\n\"foundation\" 3D generative model and extend it to recover plausible 3D geometry\nand appearance from occluded objects. We introduce a mask-weighted multi-head\ncross-attention mechanism followed by an occlusion-aware attention layer that\nexplicitly leverages occlusion priors to guide the reconstruction process. We\ndemonstrate that, by training solely on synthetic data, Amodal3R learns to\nrecover full 3D objects even in the presence of occlusions in real scenes. It\nsubstantially outperforms existing methods that independently perform 2D amodal\ncompletion followed by 3D reconstruction, thereby establishing a new benchmark\nfor occlusion-aware 3D reconstruction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11685", "pdf": "https://arxiv.org/pdf/2503.11685", "abs": "https://arxiv.org/abs/2503.11685", "authors": ["Omkar Kokane", "Adam Teman", "Anushka Jha", "Guru Prasath SL", "Gopal Raut", "Mukul Lokhande", "S. V. Jaya Chand", "Tanushree Dewangan", "Santosh Kumar Vishvakarma"], "title": "CORDIC Is All You Need", "categories": ["cs.AR", "cs.CV", "eess.IV"], "comment": null, "summary": "Artificial intelligence necessitates adaptable hardware accelerators for\nefficient high-throughput million operations. We present pipelined architecture\nwith CORDIC block for linear MAC computations and nonlinear iterative\nActivation Functions (AF) such as $tanh$, $sigmoid$, and $softmax$. This\napproach focuses on a Reconfigurable Processing Engine (RPE) based systolic\narray, with 40\\% pruning rate, enhanced throughput up to 4.64$\\times$, and\nreduction in power and area by 5.02 $\\times$ and 4.06 $\\times$ at CMOS 28 nm,\nwith minor accuracy loss. FPGA implementation achieves a reduction of up to 2.5\n$\\times$ resource savings and 3 $\\times$ power compared to prior works. The\nSystolic CORDIC engine for Reconfigurability and Enhanced throughput (SYCore)\ndeploys an output stationary dataflow with the CAESAR control engine for\ndiverse AI workloads such as Transformers, RNNs/LSTMs, and DNNs for\napplications like image detection, LLMs, and speech recognition. The\nenergy-efficient and flexible approach extends the enhanced approach for edge\nAI accelerators supporting emerging workloads.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11692", "pdf": "https://arxiv.org/pdf/2503.11692", "abs": "https://arxiv.org/abs/2503.11692", "authors": ["Rashik Shrestha", "Madhav Rijal", "Trevor Smith", "Yu Gu"], "title": "FloPE: Flower Pose Estimation for Precision Pollination", "categories": ["cs.RO", "cs.CV"], "comment": "IROS2025 under review", "summary": "This study presents Flower Pose Estimation (FloPE), a real-time flower pose\nestimation framework for computationally constrained robotic pollination\nsystems. Robotic pollination has been proposed to supplement natural\npollination to ensure global food security due to the decreased population of\nnatural pollinators. However, flower pose estimation for pollination is\nchallenging due to natural variability, flower clusters, and high accuracy\ndemands due to the flowers' fragility when pollinating. This method leverages\n3D Gaussian Splatting to generate photorealistic synthetic datasets with\nprecise pose annotations, enabling effective knowledge distillation from a\nhigh-capacity teacher model to a lightweight student model for efficient\ninference. The approach was evaluated on both single and multi-arm robotic\nplatforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees\nwithin a low computational cost. Our experiments validate the effectiveness of\nFloPE, achieving up to 78.75% pollination success rate and outperforming prior\nrobotic pollination techniques.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12030", "pdf": "https://arxiv.org/pdf/2503.12030", "abs": "https://arxiv.org/abs/2503.12030", "authors": ["Zhenxin Li", "Shihao Wang", "Shiyi Lan", "Zhiding Yu", "Zuxuan Wu", "Jose M. Alvarez"], "title": "Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving research currently faces a critical challenge\nin bridging the gap between open-loop training and closed-loop deployment.\nCurrent approaches are trained to predict trajectories in an open-loop\nenvironment, which struggle with quick reactions to other agents in closed-loop\nenvironments and risk generating kinematically infeasible plans due to the gap\nbetween open-loop training and closed-loop driving. In this paper, we introduce\nHydra-NeXt, a novel multi-branch planning framework that unifies trajectory\nprediction, control prediction, and a trajectory refinement network in one\nmodel. Unlike current open-loop trajectory prediction models that only handle\ngeneral-case planning, Hydra-NeXt further utilizes a control decoder to focus\non short-term actions, which enables faster responses to dynamic situations and\nreactive agents. Moreover, we propose the Trajectory Refinement module to\naugment and refine the planning decisions by effectively adhering to kinematic\nconstraints in closed-loop environments. This unified approach bridges the gap\nbetween open-loop training and closed-loop driving, demonstrating superior\nperformance of 65.89 Driving Score (DS) and 48.20% Success Rate (SR) on the\nBench2Drive dataset without relying on external experts for data collection.\nHydra-NeXt surpasses the previous state-of-the-art by 22.98 DS and 17.49 SR,\nmarking a significant advancement in autonomous driving. Code will be available\nat https://github.com/woxihuanjiangguo/Hydra-NeXt.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12141", "pdf": "https://arxiv.org/pdf/2503.12141", "abs": "https://arxiv.org/abs/2503.12141", "authors": ["Shayan Rokhva", "Babak Teimourpour", "Romina Babaei"], "title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12365", "pdf": "https://arxiv.org/pdf/2503.12365", "abs": "https://arxiv.org/abs/2503.12365", "authors": ["Xiangfei Fang", "Boying Wang", "Chengying Huan", "Shaonan Ma", "Heng Zhang", "Chen Zhao"], "title": "HyperKAN: Hypergraph Representation Learning with Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.CV", "cs.SI"], "comment": "Accepted by ICASSP2025", "summary": "Hypergraph representation learning has garnered increasing attention across\nvarious domains due to its capability to model high-order relationships.\nTraditional methods often rely on hypergraph neural networks (HNNs) employing\nmessage passing mechanisms to aggregate vertex and hyperedge features. However,\nthese methods are constrained by their dependence on hypergraph topology,\nleading to the challenge of imbalanced information aggregation, where\nhigh-degree vertices tend to aggregate redundant features, while low-degree\nvertices often struggle to capture sufficient structural features. To overcome\nthe above challenges, we introduce HyperKAN, a novel framework for hypergraph\nrepresentation learning that transcends the limitations of message-passing\ntechniques. HyperKAN begins by encoding features for each vertex and then\nleverages Kolmogorov-Arnold Networks (KANs) to capture complex nonlinear\nrelationships. By adjusting structural features based on similarity, our\napproach generates refined vertex representations that effectively addresses\nthe challenge of imbalanced information aggregation. Experiments conducted on\nthe real-world datasets demonstrate that HyperKAN significantly outperforms\nstate of-the-art HNN methods, achieving nearly a 9% performance improvement on\nthe Senate dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12642", "pdf": "https://arxiv.org/pdf/2503.12642", "abs": "https://arxiv.org/abs/2503.12642", "authors": ["Anjali Dharmik"], "title": "COVID 19 Diagnosis Analysis using Transfer Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Coronaviruses transmit COVID-19, a rapidly spreading disease. A Coronavirus\ninfection (COVID-19) was first discovered in December 2019 in Wuhan, China, and\nspread rapidly throughout the planet in exactly some months. because of this,\nthe virus can cause severe symptoms and even death, especially within the\nelderly and in people with medical conditions. The virus causes acute\nrespiratory infections in humans. the primary case was diagnosed in China in\n2019 and the pandemic started in 2020. Since the quantity of cases of COVID-19\nis increasing daily, there are only a limited number of test kits available in\nhospitals. So, to stop COVID-19 from spreading among people, an automatic\ndiagnosis system must be implemented. during this study, three pre-trained\nneural networks supported convolutional neural networks (VGG16, VGG19,\nResNet50) are proposed for detecting Coronavirus pneumonia infected patients\nthrough X-rays and computerized tomography (CT). By using cross-validation,\nwe've got implemented binary classifications with two classes (COVID-19, Normal\n(healthy)). Taking into consideration the results obtained, the pre-trained\nResNet50 model provides the simplest classification performance (97.77%\naccuracy, 100% sensitivity, 93.33% specificity, 98.00% F1-score) among the\nopposite three used models over 6259 images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12768", "pdf": "https://arxiv.org/pdf/2503.12768", "abs": "https://arxiv.org/abs/2503.12768", "authors": ["Tatsuro Sakai", "Kanji Tanaka", "Jonathan Tay Yu Liang", "Muhammad Adil Luqman", "Daiki Iwata"], "title": "Dynamic-Dark SLAM: RGB-Thermal Cooperative Robot Vision Strategy for Multi-Person Tracking in Both Well-Lit and Low-Light Scenes", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 4 figures, technical report", "summary": "In robot vision, thermal cameras have significant potential for recognizing\nhumans even in complete darkness. However, their application to multi-person\ntracking (MPT) has lagged due to data scarcity and difficulties in individual\nidentification. In this study, we propose a cooperative MPT system that\nutilizes co-located RGB and thermal cameras, using pseudo-annotations (bounding\nboxes + person IDs) to train RGB and T trackers. Evaluation experiments\ndemonstrate that the T tracker achieves remarkable performance in both bright\nand dark scenes. Furthermore, results suggest that a tracker-switching approach\nusing a binary brightness classifier is more suitable than a tracker-fusion\napproach for information integration. This study marks a crucial first step\ntoward ``Dynamic-Dark SLAM,\" enabling effective recognition, understanding, and\nreconstruction of individuals, occluding objects, and traversable areas in\ndynamic environments, both bright and dark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12793", "pdf": "https://arxiv.org/pdf/2503.12793", "abs": "https://arxiv.org/abs/2503.12793", "authors": ["Yechao Zhang", "Yingzhe Xu", "Junyu Shi", "Leo Yu Zhang", "Shengshan Hu", "Minghui Li", "Yanjun Zhang"], "title": "Improving Generalization of Universal Adversarial Perturbation via Dynamic Maximin Optimization", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted in AAAI 2025", "summary": "Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12847", "pdf": "https://arxiv.org/pdf/2503.12847", "abs": "https://arxiv.org/abs/2503.12847", "authors": ["Chen Liu", "Peike Li", "Liying Yang", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12926", "pdf": "https://arxiv.org/pdf/2503.12926", "abs": "https://arxiv.org/abs/2503.12926", "authors": ["Cheng Yuan", "Zhening Liu", "Jiashu Lv", "Jiawei Shao", "Yufei Jiang", "Jun Zhang", "Xuelong Li"], "title": "Task-Oriented Feature Compression for Multimodal Understanding via Device-Edge Co-Inference", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "With the rapid development of large multimodal models (LMMs), multimodal\nunderstanding applications are emerging. As most LMM inference requests\noriginate from edge devices with limited computational capabilities, the\npredominant inference pipeline involves directly forwarding the input data to\nan edge server which handles all computations. However, this approach\nintroduces high transmission latency due to limited uplink bandwidth of edge\ndevices and significant computation latency caused by the prohibitive number of\nvisual tokens, thus hindering delay-sensitive tasks and degrading user\nexperience. To address this challenge, we propose a task-oriented feature\ncompression (TOFC) method for multimodal understanding in a device-edge\nco-inference framework, where visual features are merged by clustering and\nencoded by a learnable and selective entropy model before feature projection.\nSpecifically, we employ density peaks clustering based on K nearest neighbors\nto reduce the number of visual features, thereby minimizing both data\ntransmission and computational complexity. Subsequently, a learnable entropy\nmodel with hyperprior is utilized to encode and decode merged features, further\nreducing transmission overhead. To enhance compression efficiency, multiple\nentropy models are adaptively selected based on the characteristics of the\nvisual features, enabling a more accurate estimation of the probability\ndistribution. Comprehensive experiments on seven visual question answering\nbenchmarks validate the effectiveness of the proposed TOFC method. Results show\nthat TOFC achieves up to 60% reduction in data transmission overhead and 50%\nreduction in system latency while maintaining identical task performance,\ncompared with traditional image compression methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13008", "pdf": "https://arxiv.org/pdf/2503.13008", "abs": "https://arxiv.org/abs/2503.13008", "authors": ["David E. Hernandez", "Jose Ramon Chang", "Torbjörn E. M. Nordling"], "title": "Knowledge Distillation: Enhancing Neural Network Compression with Integrated Gradients", "categories": ["cs.LG", "cs.CV", "68T05, 68T07", "I.2.6; I.4.2; I.4.9"], "comment": "15 pages, 3 figures, conference", "summary": "Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13051", "pdf": "https://arxiv.org/pdf/2503.13051", "abs": "https://arxiv.org/abs/2503.13051", "authors": ["Kai Uwe Barthel", "Florian Barthel", "Peter Eisert"], "title": "Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13082", "pdf": "https://arxiv.org/pdf/2503.13082", "abs": "https://arxiv.org/abs/2503.13082", "authors": ["Runyu Jiao", "Alice Fasoli", "Francesco Giuliari", "Matteo Bortolon", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Free-form language-based robotic reasoning and grasping", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://tev-fbk.github.io/FreeGrasp/", "summary": "Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13090", "pdf": "https://arxiv.org/pdf/2503.13090", "abs": "https://arxiv.org/abs/2503.13090", "authors": ["Václav Truhlařík", "Tomáš Pivoňka", "Michal Kasarda", "Libor Přeučil"], "title": "Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 5 figures", "summary": "Uniform and variable environments still remain a challenge for stable visual\nlocalization and mapping in mobile robot navigation. One of the possible\napproaches suitable for such environments is appearance-based teach-and-repeat\nnavigation, relying on simplified localization and reactive robot motion\ncontrol - all without a need for standard mapping. This work brings an\ninnovative solution to such a system based on visual place recognition\ntechniques. Here, the major contributions stand in the employment of a new\nvisual place recognition technique, a novel horizontal shift computation\napproach, and a multi-platform system design for applications across various\ntypes of mobile robots. Secondly, a new public dataset for experimental testing\nof appearance-based navigation methods is introduced. Moreover, the work also\nprovides real-world experimental testing and performance comparison of the\nintroduced navigation system against other state-of-the-art methods. The\nresults confirm that the new system outperforms existing methods in several\ntesting scenarios, is capable of operation indoors and outdoors, and exhibits\nrobustness to day and night scene variations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13227", "pdf": "https://arxiv.org/pdf/2503.13227", "abs": "https://arxiv.org/abs/2503.13227", "authors": ["Yijie Liu", "Xinyi Shang", "Yiqun Zhang", "Yang Lu", "Chen Gong", "Jing-Hao Xue", "Hanzi Wang"], "title": "Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data\nacross clients with limited labeled data to train a global model with strong\ngeneralization ability. Most FSSL methods rely on consistency regularization\nwith pseudo-labels, converting predictions from local or global models into\nhard pseudo-labels as supervisory signals. However, we discover that the\nquality of pseudo-label is largely deteriorated by data heterogeneity, an\nintrinsic facet of federated learning. In this paper, we study the problem of\nFSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label\nmismatches, further degrading model performance and convergence, and (2) local\nand global models' predictive tendencies diverge as heterogeneity increases.\nMotivated by these findings, we propose a simple and effective method called\nSemi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can\nflexibly correct pseudo-labels based on confidence discrepancies. This strategy\neffectively mitigates performance degradation caused by incorrect pseudo-labels\nand enhances consensus between local and global models. Experimental results\ndemonstrate that SAGE outperforms existing FSSL methods in both performance and\nconvergence. Our code is available at https://github.com/Jay-Codeman/SAGE", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13309", "pdf": "https://arxiv.org/pdf/2503.13309", "abs": "https://arxiv.org/abs/2503.13309", "authors": ["Farnoush Bayatmakou", "Reza Taleei", "Milad Amir Toutounchian", "Arash Mohammadi"], "title": "Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13330", "pdf": "https://arxiv.org/pdf/2503.13330", "abs": "https://arxiv.org/abs/2503.13330", "authors": ["Ricardo Bigolin Lanfredi", "Yan Zhuang", "Mark Finkelstein", "Praveen Thoppey Srinivasan Balamuralikrishna", "Luke Krembs", "Brandon Khoury", "Arthi Reddy", "Pritam Mukherjee", "Neil M. Rofsky", "Ronald M. Summers"], "title": "LEAVS: An LLM-based Labeler for Abdominal CT Supervision", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13441", "pdf": "https://arxiv.org/pdf/2503.13441", "abs": "https://arxiv.org/abs/2503.13441", "authors": ["Ri-Zhao Qiu", "Shiqi Yang", "Xuxin Cheng", "Chaitanya Chawla", "Jialong Li", "Tairan He", "Ge Yan", "Lars Paulsen", "Ge Yang", "Sha Yi", "Guanya Shi", "Xiaolong Wang"], "title": "Humanoid Policy ~ Human Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Code and data: https://human-as-robot.github.io/", "summary": "Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
