{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289", "abs": "https://arxiv.org/abs/2503.15289", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "categories": ["cs.CL"], "comment": "15 pages", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "reliability", "summarization", "fine-grained"], "score": 6}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289", "abs": "https://arxiv.org/abs/2503.15289", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "categories": ["cs.CL"], "comment": "15 pages", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "reliability", "summarization", "fine-grained"], "score": 6}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14671", "pdf": "https://arxiv.org/pdf/2503.14671", "abs": "https://arxiv.org/abs/2503.14671", "authors": ["Xiangyong Chen", "Xiaochuan Lin"], "title": "Generating Medically-Informed Explanations for Depression Detection using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14862", "pdf": "https://arxiv.org/pdf/2503.14862", "abs": "https://arxiv.org/abs/2503.14862", "authors": ["Ying Liu", "Yijing Hua", "Haojiang Chai", "Yanbo Wang", "TengQi Ye"], "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Open-vocabulary detectors are proposed to locate and recognize objects in\nnovel classes. However, variations in vision-aware language vocabulary data\nused for open-vocabulary learning can lead to unfair and unreliable\nevaluations. Recent evaluation methods have attempted to address this issue by\nincorporating object properties or adding locations and characteristics to the\ncaptions. Nevertheless, since these properties and locations depend on the\nspecific details of the images instead of classes, detectors can not make\naccurate predictions without precise descriptions provided through human\nannotation. This paper introduces 3F-OVD, a novel task that extends supervised\nfine-grained object detection to the open-vocabulary setting. Our task is\nintuitive and challenging, requiring a deep understanding of Fine-grained\ncaptions and careful attention to Fine-grained details in images in order to\naccurately detect Fine-grained objects. Additionally, due to the scarcity of\nqualified fine-grained object detection datasets, we have created a new\ndataset, NEU-171K, tailored for both supervised and open-vocabulary settings.\nWe benchmark state-of-the-art object detectors on our dataset for both\nsettings. Furthermore, we propose a simple yet effective post-processing\ntechnique.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation", "fine-grained"], "score": 5}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14671", "pdf": "https://arxiv.org/pdf/2503.14671", "abs": "https://arxiv.org/abs/2503.14671", "authors": ["Xiangyong Chen", "Xiaochuan Lin"], "title": "Generating Medically-Informed Explanations for Depression Detection using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "criteria"], "score": 5}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14862", "pdf": "https://arxiv.org/pdf/2503.14862", "abs": "https://arxiv.org/abs/2503.14862", "authors": ["Ying Liu", "Yijing Hua", "Haojiang Chai", "Yanbo Wang", "TengQi Ye"], "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Open-vocabulary detectors are proposed to locate and recognize objects in\nnovel classes. However, variations in vision-aware language vocabulary data\nused for open-vocabulary learning can lead to unfair and unreliable\nevaluations. Recent evaluation methods have attempted to address this issue by\nincorporating object properties or adding locations and characteristics to the\ncaptions. Nevertheless, since these properties and locations depend on the\nspecific details of the images instead of classes, detectors can not make\naccurate predictions without precise descriptions provided through human\nannotation. This paper introduces 3F-OVD, a novel task that extends supervised\nfine-grained object detection to the open-vocabulary setting. Our task is\nintuitive and challenging, requiring a deep understanding of Fine-grained\ncaptions and careful attention to Fine-grained details in images in order to\naccurately detect Fine-grained objects. Additionally, due to the scarcity of\nqualified fine-grained object detection datasets, we have created a new\ndataset, NEU-171K, tailored for both supervised and open-vocabulary settings.\nWe benchmark state-of-the-art object detectors on our dataset for both\nsettings. Furthermore, we propose a simple yet effective post-processing\ntechnique.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation", "fine-grained"], "score": 5}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14513", "pdf": "https://arxiv.org/pdf/2503.14513", "abs": "https://arxiv.org/abs/2503.14513", "authors": ["Seyed Muhammad Hossein Mousavi"], "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI", "eess.IV", "A.I"], "comment": "18 pages", "summary": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14827", "pdf": "https://arxiv.org/pdf/2503.14827", "abs": "https://arxiv.org/abs/2503.14827", "authors": ["Chejian Xu", "Jiawei Zhang", "Zhaorun Chen", "Chulin Xie", "Mintong Kang", "Yujin Potter", "Zhun Wang", "Zhuowen Yuan", "Alexander Xiong", "Zidi Xiong", "Chenhui Zhang", "Lingzhi Yuan", "Yi Zeng", "Peiyang Xu", "Chengquan Guo", "Andy Zhou", "Jeffrey Ziwei Tan", "Xuandong Zhao", "Francesco Pinto", "Zhen Xiang", "Yu Gai", "Zinan Lin", "Dan Hendrycks", "Bo Li", "Dawn Song"], "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ICLR 2025", "summary": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "helpfulness", "safety"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044", "abs": "https://arxiv.org/abs/2503.15044", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "9 pages", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15133", "pdf": "https://arxiv.org/pdf/2503.15133", "abs": "https://arxiv.org/abs/2503.15133", "authors": ["Christina Zorenböhmer", "Sebastian Schmidt", "Bernd Resch"], "title": "EmoGRACE: Aspect-based emotion analysis for social media data", "categories": ["cs.CL"], "comment": null, "summary": "While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "aspect-based"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14935", "pdf": "https://arxiv.org/pdf/2503.14935", "abs": "https://arxiv.org/abs/2503.14935", "authors": ["Chongjun Tu", "Lin Zhang", "Pengtao Chen", "Peng Ye", "Xianfang Zeng", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "FAVOR-Bench project page: https://favor-bench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "question answering"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14534", "pdf": "https://arxiv.org/pdf/2503.14534", "abs": "https://arxiv.org/abs/2503.14534", "authors": ["Bibi Erum Ayesha", "T. Satyanarayana Murthy", "Palamakula Ramesh Babu", "Ramu Kuchipudi"], "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14513", "pdf": "https://arxiv.org/pdf/2503.14513", "abs": "https://arxiv.org/abs/2503.14513", "authors": ["Seyed Muhammad Hossein Mousavi"], "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI", "eess.IV", "A.I"], "comment": "18 pages", "summary": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14827", "pdf": "https://arxiv.org/pdf/2503.14827", "abs": "https://arxiv.org/abs/2503.14827", "authors": ["Chejian Xu", "Jiawei Zhang", "Zhaorun Chen", "Chulin Xie", "Mintong Kang", "Yujin Potter", "Zhun Wang", "Zhuowen Yuan", "Alexander Xiong", "Zidi Xiong", "Chenhui Zhang", "Lingzhi Yuan", "Yi Zeng", "Peiyang Xu", "Chengquan Guo", "Andy Zhou", "Jeffrey Ziwei Tan", "Xuandong Zhao", "Francesco Pinto", "Zhen Xiang", "Yu Gai", "Zinan Lin", "Dan Hendrycks", "Bo Li", "Dawn Song"], "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ICLR 2025", "summary": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "helpfulness", "safety"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044", "abs": "https://arxiv.org/abs/2503.15044", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "9 pages", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15133", "pdf": "https://arxiv.org/pdf/2503.15133", "abs": "https://arxiv.org/abs/2503.15133", "authors": ["Christina Zorenböhmer", "Sebastian Schmidt", "Bernd Resch"], "title": "EmoGRACE: Aspect-based emotion analysis for social media data", "categories": ["cs.CL"], "comment": null, "summary": "While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "aspect-based"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14935", "pdf": "https://arxiv.org/pdf/2503.14935", "abs": "https://arxiv.org/abs/2503.14935", "authors": ["Chongjun Tu", "Lin Zhang", "Pengtao Chen", "Peng Ye", "Xianfang Zeng", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "FAVOR-Bench project page: https://favor-bench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "question answering"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14534", "pdf": "https://arxiv.org/pdf/2503.14534", "abs": "https://arxiv.org/abs/2503.14534", "authors": ["Bibi Erum Ayesha", "T. Satyanarayana Murthy", "Palamakula Ramesh Babu", "Ramu Kuchipudi"], "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14996", "pdf": "https://arxiv.org/pdf/2503.14996", "abs": "https://arxiv.org/abs/2503.14996", "authors": ["Francesco Maria Molfese", "Luca Moroni", "Luca Gioffrè", "Alessandro Scirè", "Simone Conia", "Roberto Navigli"], "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering", "categories": ["cs.CL"], "comment": "17 pages (9 main), 11 figures, 21 tables", "summary": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14674", "pdf": "https://arxiv.org/pdf/2503.14674", "abs": "https://arxiv.org/abs/2503.14674", "authors": ["Liu Jing", "Amirul Rahman"], "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14786", "pdf": "https://arxiv.org/pdf/2503.14786", "abs": "https://arxiv.org/abs/2503.14786", "authors": ["Haiyang Ying", "Matthias Zwicker"], "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14837", "pdf": "https://arxiv.org/pdf/2503.14837", "abs": "https://arxiv.org/abs/2503.14837", "authors": ["Yinqi Chen", "Meiying Zhang", "Qi Hao", "Guang Zhou"], "title": "SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate perception of dynamic traffic scenes is crucial for high-level\nautonomous driving systems, requiring robust object motion estimation and\ninstance segmentation. However, traditional methods often treat them as\nseparate tasks, leading to suboptimal performance, spatio-temporal\ninconsistencies, and inefficiency in complex scenarios due to the absence of\ninformation sharing. This paper proposes a multi-task SemanticFlow framework to\nsimultaneously predict scene flow and instance segmentation of full-resolution\npoint clouds. The novelty of this work is threefold: 1) developing a\ncoarse-to-fine prediction based multi-task scheme, where an initial coarse\nsegmentation of static backgrounds and dynamic objects is used to provide\ncontextual information for refining motion and semantic information through a\nshared feature processing module; 2) developing a set of loss functions to\nenhance the performance of scene flow estimation and instance segmentation,\nwhile can help ensure spatial and temporal consistency of both static and\ndynamic objects within traffic scenes; 3) developing a self-supervised learning\nscheme, which utilizes coarse segmentation to detect rigid objects and compute\ntheir transformation matrices between sequential frames, enabling the\ngeneration of self-supervised labels. The proposed framework is validated on\nthe Argoverse and Waymo datasets, demonstrating superior performance in\ninstance segmentation accuracy, scene flow estimation, and computational\nefficiency, establishing a new benchmark for self-supervised methods in dynamic\nscene understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15374", "pdf": "https://arxiv.org/pdf/2503.15374", "abs": "https://arxiv.org/abs/2503.15374", "authors": ["Anatole Callies", "Quentin Bodinier", "Philippe Ravaud", "Kourosh Davarpanah"], "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14905", "pdf": "https://arxiv.org/pdf/2503.14905", "abs": "https://arxiv.org/abs/2503.14905", "authors": ["Siwei Wen", "Junyan Ye", "Peilin Feng", "Hengrui Kang", "Zichen Wen", "Yize Chen", "Jiang Wu", "Wenjun Wu", "Conghui He", "Weijia Li"], "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15338", "pdf": "https://arxiv.org/pdf/2503.15338", "abs": "https://arxiv.org/abs/2503.15338", "authors": ["Junyi Ao", "Dekun Chen", "Xiaohai Tian", "Wenjie Feng", "Jun Zhang", "Lu Lu", "Yuxuan Wang", "Haizhou Li", "Zhizheng Wu"], "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14950", "pdf": "https://arxiv.org/pdf/2503.14950", "abs": "https://arxiv.org/abs/2503.14950", "authors": ["Joseph Emmanuel DL Dayo", "Prospero C. Naval Jr"], "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14957", "pdf": "https://arxiv.org/pdf/2503.14957", "abs": "https://arxiv.org/abs/2503.14957", "authors": ["Thanh-Son Nguyen", "Hong Yang", "Tzeh Yuan Neoh", "Hao Zhang", "Ee Yeo Keat", "Basura Fernando"], "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14979", "pdf": "https://arxiv.org/pdf/2503.14979", "abs": "https://arxiv.org/abs/2503.14979", "authors": ["Yaxiong Chen", "Junjian Hu", "Chunlei Li", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15004", "pdf": "https://arxiv.org/pdf/2503.15004", "abs": "https://arxiv.org/abs/2503.15004", "authors": ["Annalena Blänsdorf", "Tristan Wirth", "Arne Rak", "Thomas Pöllabauer", "Volker Knauthe", "Arjan Kuijper"], "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15197", "pdf": "https://arxiv.org/pdf/2503.15197", "abs": "https://arxiv.org/abs/2503.15197", "authors": ["Feifei Li", "Mi Zhang", "Yiming Sun", "Min Yang"], "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15435", "pdf": "https://arxiv.org/pdf/2503.15435", "abs": "https://arxiv.org/abs/2503.15435", "authors": ["Baolu Li", "Zongzhe Xu", "Jinlong Li", "Xinyu Liu", "Jianwu Fang", "Xiaopeng Li", "Hongkai Yu"], "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception", "categories": ["cs.CV"], "comment": "accepted by ICRA 2025", "summary": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "consistency"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14523", "pdf": "https://arxiv.org/pdf/2503.14523", "abs": "https://arxiv.org/abs/2503.14523", "authors": ["Siyi Wu", "Leyi Zhao", "Haitian Ma", "Xinyuan Song"], "title": "SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation via SDF Pre-training and Topology-Aware Fine-Tuning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular and curvilinear structures, such as blood\nvessels, neurons, and road networks, is crucial in various applications. A key\nchallenge is ensuring topological correctness while maintaining computational\nefficiency. Existing approaches often employ topological loss functions based\non persistent homology, such as Betti error, to enforce structural consistency.\nHowever, these methods suffer from high computational costs and are insensitive\nto pixel-level accuracy, often requiring additional loss terms like Dice or MSE\nto compensate. To address these limitations, we propose \\textbf{SDF-TopoNet},\nan improved topology-aware segmentation framework that enhances both\nsegmentation accuracy and training efficiency. Our approach introduces a novel\ntwo-stage training strategy. In the pre-training phase, we utilize the signed\ndistance function (SDF) as an auxiliary learning target, allowing the model to\nencode topological information without directly relying on computationally\nexpensive topological loss functions. In the fine-tuning phase, we incorporate\na dynamic adapter alongside a refined topological loss to ensure topological\ncorrectness while mitigating overfitting and computational overhead. We\nevaluate our method on five benchmark datasets. Experimental results\ndemonstrate that SDF-TopoNet outperforms existing methods in both topological\naccuracy and quantitative segmentation metrics, while significantly reducing\ntraining complexity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14996", "pdf": "https://arxiv.org/pdf/2503.14996", "abs": "https://arxiv.org/abs/2503.14996", "authors": ["Francesco Maria Molfese", "Luca Moroni", "Luca Gioffrè", "Alessandro Scirè", "Simone Conia", "Roberto Navigli"], "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering", "categories": ["cs.CL"], "comment": "17 pages (9 main), 11 figures, 21 tables", "summary": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14674", "pdf": "https://arxiv.org/pdf/2503.14674", "abs": "https://arxiv.org/abs/2503.14674", "authors": ["Liu Jing", "Amirul Rahman"], "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14786", "pdf": "https://arxiv.org/pdf/2503.14786", "abs": "https://arxiv.org/abs/2503.14786", "authors": ["Haiyang Ying", "Matthias Zwicker"], "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14837", "pdf": "https://arxiv.org/pdf/2503.14837", "abs": "https://arxiv.org/abs/2503.14837", "authors": ["Yinqi Chen", "Meiying Zhang", "Qi Hao", "Guang Zhou"], "title": "SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate perception of dynamic traffic scenes is crucial for high-level\nautonomous driving systems, requiring robust object motion estimation and\ninstance segmentation. However, traditional methods often treat them as\nseparate tasks, leading to suboptimal performance, spatio-temporal\ninconsistencies, and inefficiency in complex scenarios due to the absence of\ninformation sharing. This paper proposes a multi-task SemanticFlow framework to\nsimultaneously predict scene flow and instance segmentation of full-resolution\npoint clouds. The novelty of this work is threefold: 1) developing a\ncoarse-to-fine prediction based multi-task scheme, where an initial coarse\nsegmentation of static backgrounds and dynamic objects is used to provide\ncontextual information for refining motion and semantic information through a\nshared feature processing module; 2) developing a set of loss functions to\nenhance the performance of scene flow estimation and instance segmentation,\nwhile can help ensure spatial and temporal consistency of both static and\ndynamic objects within traffic scenes; 3) developing a self-supervised learning\nscheme, which utilizes coarse segmentation to detect rigid objects and compute\ntheir transformation matrices between sequential frames, enabling the\ngeneration of self-supervised labels. The proposed framework is validated on\nthe Argoverse and Waymo datasets, demonstrating superior performance in\ninstance segmentation accuracy, scene flow estimation, and computational\nefficiency, establishing a new benchmark for self-supervised methods in dynamic\nscene understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15374", "pdf": "https://arxiv.org/pdf/2503.15374", "abs": "https://arxiv.org/abs/2503.15374", "authors": ["Anatole Callies", "Quentin Bodinier", "Philippe Ravaud", "Kourosh Davarpanah"], "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14905", "pdf": "https://arxiv.org/pdf/2503.14905", "abs": "https://arxiv.org/abs/2503.14905", "authors": ["Siwei Wen", "Junyan Ye", "Peilin Feng", "Hengrui Kang", "Zichen Wen", "Yize Chen", "Jiang Wu", "Wenjun Wu", "Conghui He", "Weijia Li"], "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15338", "pdf": "https://arxiv.org/pdf/2503.15338", "abs": "https://arxiv.org/abs/2503.15338", "authors": ["Junyi Ao", "Dekun Chen", "Xiaohai Tian", "Wenjie Feng", "Jun Zhang", "Lu Lu", "Yuxuan Wang", "Haizhou Li", "Zhizheng Wu"], "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14950", "pdf": "https://arxiv.org/pdf/2503.14950", "abs": "https://arxiv.org/abs/2503.14950", "authors": ["Joseph Emmanuel DL Dayo", "Prospero C. Naval Jr"], "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14957", "pdf": "https://arxiv.org/pdf/2503.14957", "abs": "https://arxiv.org/abs/2503.14957", "authors": ["Thanh-Son Nguyen", "Hong Yang", "Tzeh Yuan Neoh", "Hao Zhang", "Ee Yeo Keat", "Basura Fernando"], "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14979", "pdf": "https://arxiv.org/pdf/2503.14979", "abs": "https://arxiv.org/abs/2503.14979", "authors": ["Yaxiong Chen", "Junjian Hu", "Chunlei Li", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15004", "pdf": "https://arxiv.org/pdf/2503.15004", "abs": "https://arxiv.org/abs/2503.15004", "authors": ["Annalena Blänsdorf", "Tristan Wirth", "Arne Rak", "Thomas Pöllabauer", "Volker Knauthe", "Arjan Kuijper"], "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15197", "pdf": "https://arxiv.org/pdf/2503.15197", "abs": "https://arxiv.org/abs/2503.15197", "authors": ["Feifei Li", "Mi Zhang", "Yiming Sun", "Min Yang"], "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15435", "pdf": "https://arxiv.org/pdf/2503.15435", "abs": "https://arxiv.org/abs/2503.15435", "authors": ["Baolu Li", "Zongzhe Xu", "Jinlong Li", "Xinyu Liu", "Jianwu Fang", "Xiaopeng Li", "Hongkai Yu"], "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception", "categories": ["cs.CV"], "comment": "accepted by ICRA 2025", "summary": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "consistency"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14523", "pdf": "https://arxiv.org/pdf/2503.14523", "abs": "https://arxiv.org/abs/2503.14523", "authors": ["Siyi Wu", "Leyi Zhao", "Haitian Ma", "Xinyuan Song"], "title": "SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation via SDF Pre-training and Topology-Aware Fine-Tuning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular and curvilinear structures, such as blood\nvessels, neurons, and road networks, is crucial in various applications. A key\nchallenge is ensuring topological correctness while maintaining computational\nefficiency. Existing approaches often employ topological loss functions based\non persistent homology, such as Betti error, to enforce structural consistency.\nHowever, these methods suffer from high computational costs and are insensitive\nto pixel-level accuracy, often requiring additional loss terms like Dice or MSE\nto compensate. To address these limitations, we propose \\textbf{SDF-TopoNet},\nan improved topology-aware segmentation framework that enhances both\nsegmentation accuracy and training efficiency. Our approach introduces a novel\ntwo-stage training strategy. In the pre-training phase, we utilize the signed\ndistance function (SDF) as an auxiliary learning target, allowing the model to\nencode topological information without directly relying on computationally\nexpensive topological loss functions. In the fine-tuning phase, we incorporate\na dynamic adapter alongside a refined topological loss to ensure topological\ncorrectness while mitigating overfitting and computational overhead. We\nevaluate our method on five benchmark datasets. Experimental results\ndemonstrate that SDF-TopoNet outperforms existing methods in both topological\naccuracy and quantitative segmentation metrics, while significantly reducing\ntraining complexity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14755", "pdf": "https://arxiv.org/pdf/2503.14755", "abs": "https://arxiv.org/abs/2503.14755", "authors": ["Omar E. Rakha", "Hazem M. Abbas"], "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors", "categories": ["cs.CL", "cs.AI"], "comment": "Paper was initially released in 2017 but was never published", "summary": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14797", "pdf": "https://arxiv.org/pdf/2503.14797", "abs": "https://arxiv.org/abs/2503.14797", "authors": ["Varich Boonsanong", "Vidhisha Balachandran", "Xiaochuang Han", "Shangbin Feng", "Lucy Lu Wang", "Yulia Tsvetkov"], "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14552", "pdf": "https://arxiv.org/pdf/2503.14552", "abs": "https://arxiv.org/abs/2503.14552", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Fatemeh Afghah", "Connor Peter McGrath", "Danish Bhatkar", "Mithilesh Anil Biradar", "Abolfazl Razi"], "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14891", "pdf": "https://arxiv.org/pdf/2503.14891", "abs": "https://arxiv.org/abs/2503.14891", "authors": ["Honglin Lin", "Zhuoshi Pan", "Yu Li", "Qizhi Pei", "Xin Gao", "Mengzhang Cai", "Conghui He", "Lijun Wu"], "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14917", "pdf": "https://arxiv.org/pdf/2503.14917", "abs": "https://arxiv.org/abs/2503.14917", "authors": ["Jiazheng Li", "Lu Yu", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Yanfang Ye", "Chuxu Zhang"], "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14607", "pdf": "https://arxiv.org/pdf/2503.14607", "abs": "https://arxiv.org/abs/2503.14607", "authors": ["Shuo Xing", "Zezhou Sun", "Shuangyu Xie", "Kaiyuan Chen", "Yanjia Huang", "Yuping Wang", "Jiachen Li", "Dezhen Song", "Zhengzhong Tu"], "title": "Can Large Vision Language Models Read Maps Like a Human?", "categories": ["cs.CV"], "comment": "35 pages", "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14716", "pdf": "https://arxiv.org/pdf/2503.14716", "abs": "https://arxiv.org/abs/2503.14716", "authors": ["Pei-Hsin Lin", "Jacob J. Lin", "Shang-Hsien Hsieh"], "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform", "categories": ["cs.CV", "cs.AI"], "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering", "summary": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14757", "pdf": "https://arxiv.org/pdf/2503.14757", "abs": "https://arxiv.org/abs/2503.14757", "authors": ["Marcelo Sanchez", "Gil Triginer", "Ignacio Sarasua", "Lara Raad", "Coloma Ballester"], "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing image inpainting methods have shown impressive completion results\nfor low-resolution images. However, most of these algorithms fail at high\nresolutions and require powerful hardware, limiting their deployment on edge\ndevices. Motivated by this, we propose the first baseline for REal-Time\nHigh-resolution image INpainting on Edge Devices (RETHINED) that is able to\ninpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a\nwide variety of mobile devices. A simple, yet effective novel method formed by\na lightweight Convolutional Neural Network (CNN) to recover structure, followed\nby a resolution-agnostic patch replacement mechanism to provide detailed\ntexture. Specially our pipeline leverages the structural capacity of CNN and\nthe high-level detail of patch-based methods, which is a key component for\nhigh-resolution image inpainting. To demonstrate the real application of our\nmethod, we conduct an extensive analysis on various mobile-friendly devices and\ndemonstrate similar inpainting performance while being $\\mathrm{100 \\times\nfaster}$ than existing state-of-the-art methods. Furthemore, we realease\nDF8K-Inpainting, the first free-form mask UHD inpainting dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15222", "pdf": "https://arxiv.org/pdf/2503.15222", "abs": "https://arxiv.org/abs/2503.15222", "authors": ["Pritam Kadasi", "Sriman Reddy", "Srivathsa Vamsi Chaturvedula", "Rudranshu Sen", "Agnish Saha", "Soumavo Sikdar", "Sayani Sarkar", "Suhani Mittal", "Rohit Jindal", "Mayank Singh"], "title": "Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation", "categories": ["cs.CL"], "comment": "Accepted to ICWSM'25", "summary": "With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15272", "pdf": "https://arxiv.org/pdf/2503.15272", "abs": "https://arxiv.org/abs/2503.15272", "authors": ["David Wan", "Justin Chih-Yao Chen", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine", "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14880", "pdf": "https://arxiv.org/pdf/2503.14880", "abs": "https://arxiv.org/abs/2503.14880", "authors": ["Henrique Morimitsu", "Xiaobin Zhu", "Roberto M. Cesar Jr.", "Xiangyang Ji", "Xu-Cheng Yin"], "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. The code and dataset are available at\n  https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow. 24\n  pages, 17 figures", "summary": "Optical flow estimation is essential for video processing tasks, such as\nrestoration and action recognition. The quality of videos is constantly\nincreasing, with current standards reaching 8K resolution. However, optical\nflow methods are usually designed for low resolution and do not generalize to\nlarge inputs due to their rigid architectures. They adopt downscaling or input\ntiling to reduce the input size, causing a loss of details and global\ninformation. There is also a lack of optical flow benchmarks to judge the\nactual performance of existing methods on high-resolution samples. Previous\nworks only conducted qualitative high-resolution evaluations on hand-picked\nsamples. This paper fills this gap in optical flow estimation in two ways. We\npropose DPFlow, an adaptive optical flow architecture capable of generalizing\nup to 8K resolution inputs while trained with only low-resolution samples. We\nalso introduce Kubric-NK, a new benchmark for evaluating optical flow methods\nwith input resolutions ranging from 1K to 8K. Our high-resolution evaluation\npushes the boundaries of existing methods and reveals new insights about their\ngeneralization capabilities. Extensive experimental results show that DPFlow\nachieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and\nother high-resolution benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15204", "pdf": "https://arxiv.org/pdf/2503.15204", "abs": "https://arxiv.org/abs/2503.15204", "authors": ["Tittaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Sorrawit Treesuk"], "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR", "cs.MA"], "comment": "14 pages, 2 figures", "summary": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14958", "pdf": "https://arxiv.org/pdf/2503.14958", "abs": "https://arxiv.org/abs/2503.14958", "authors": ["Zixuan Zheng", "Yilei Shi", "Chunlei Li", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15019", "pdf": "https://arxiv.org/pdf/2503.15019", "abs": "https://arxiv.org/abs/2503.15019", "authors": ["Shengqiong Wu", "Hao Fei", "Jingkang Yang", "Xiangtai Li", "Juncheng Li", "Hanwang Zhang", "Tat-seng Chua"], "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dimension"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15023", "pdf": "https://arxiv.org/pdf/2503.15023", "abs": "https://arxiv.org/abs/2503.15023", "authors": ["Chaouki Boufenar", "Mehdi Ayoub Rabiai", "Boualem Nadjib Zahaf", "Khelil Rafik Ouaras"], "title": "Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15024", "pdf": "https://arxiv.org/pdf/2503.15024", "abs": "https://arxiv.org/abs/2503.15024", "authors": ["Jin Wang", "Chenghui Lv", "Xian Li", "Shichao Dong", "Huadong Li", "kelu Yao", "Chao Li", "Wenqi Shao", "Ping Luo"], "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models", "categories": ["cs.CV"], "comment": "31 pages, 19 figures", "summary": "Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15056", "pdf": "https://arxiv.org/pdf/2503.15056", "abs": "https://arxiv.org/abs/2503.15056", "authors": ["Suhyeon Lee", "Kwanyoung Kim", "Jong Chul Ye"], "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation", "categories": ["cs.CV"], "comment": "25 pages, 16 figures", "summary": "Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15144", "pdf": "https://arxiv.org/pdf/2503.15144", "abs": "https://arxiv.org/abs/2503.15144", "authors": ["Xing He", "Zhe Zhu", "Liangliang Nan", "Honghua Chen", "Jing Qin", "Mingqiang Wei"], "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15283", "pdf": "https://arxiv.org/pdf/2503.15283", "abs": "https://arxiv.org/abs/2503.15283", "authors": ["Teng-Fang Hsiao", "Bo-Kai Ruan", "Yi-Lun Wu", "Tzu-Ling Lin", "Hong-Han Shuai"], "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15284", "pdf": "https://arxiv.org/pdf/2503.15284", "abs": "https://arxiv.org/abs/2503.15284", "authors": ["Yuanchao Yue", "Hui Yuan", "Qinglong Miao", "Xiaolong Mao", "Raouf Hamzaoui", "Peter Eisert"], "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15426", "pdf": "https://arxiv.org/pdf/2503.15426", "abs": "https://arxiv.org/abs/2503.15426", "authors": ["Wei Tang", "Yanpeng Sun", "Qinying Gu", "Zechao Li"], "title": "Visual Position Prompt for MLLM based Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14525", "pdf": "https://arxiv.org/pdf/2503.14525", "abs": "https://arxiv.org/abs/2503.14525", "authors": ["Frans Zdyb", "Albert Alonso", "Julius B. Kirkegaard"], "title": "Spline refinement with differentiable rendering", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Detecting slender, overlapping structures remains a challenge in\ncomputational microscopy. While recent coordinate-based approaches improve\ndetection, they often produce less accurate splines than pixel-based methods.\nWe introduce a training-free differentiable rendering approach to spline\nrefinement, achieving both high reliability and sub-pixel accuracy. Our method\nimproves spline quality, enhances robustness to distribution shifts, and\nshrinks the gap between synthetic and real-world data. Being fully\nunsupervised, the method is a drop-in replacement for the popular active\ncontour model for spline refinement. Evaluated on C. elegans nematodes, a\npopular model organism for drug discovery and biomedical research, we\ndemonstrate that our approach combines the strengths of both coordinate- and\npixel-based methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14536", "pdf": "https://arxiv.org/pdf/2503.14536", "abs": "https://arxiv.org/abs/2503.14536", "authors": ["Praveen Shastry", "Sowmya Chowdary Muthulur", "Naveen Kumarasami", "Anandakumar D", "Mounigasri M", "Keerthana R", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam", "Revathi Ezhumalai", "Abitha Marimuthu"], "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 92C55, 68U10, 92C50, 60G35"], "comment": "10 pages , 3 figures", "summary": "Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14538", "pdf": "https://arxiv.org/pdf/2503.14538", "abs": "https://arxiv.org/abs/2503.14538", "authors": ["Ananya Ganapthy", "Praveen Shastry", "Naveen Kumarasami", "Anandakumar D", "Keerthana R", "Mounigasri M", "Varshinipriya M", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam"], "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 68T45, 92C55, 92C50, 68U10"], "comment": "11 pages, 3 figures", "summary": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14562", "pdf": "https://arxiv.org/pdf/2503.14562", "abs": "https://arxiv.org/abs/2503.14562", "authors": ["A. I. Medvedeva", "V. V. Bakutkin"], "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "in Russian language", "summary": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14779", "pdf": "https://arxiv.org/pdf/2503.14779", "abs": "https://arxiv.org/abs/2503.14779", "authors": ["Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh"], "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14836", "pdf": "https://arxiv.org/pdf/2503.14836", "abs": "https://arxiv.org/abs/2503.14836", "authors": ["Kunyang Li", "Jean-Charles Noirot Ferrand", "Ryan Sheatsley", "Blaine Hoak", "Yohan Beugin", "Eric Pauley", "Patrick McDaniel"], "title": "On the Robustness Tradeoff in Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14908", "pdf": "https://arxiv.org/pdf/2503.14908", "abs": "https://arxiv.org/abs/2503.14908", "authors": ["Haoyu Chen", "Xiaojie Xu", "Wenbo Li", "Jingjing Ren", "Tian Ye", "Songhua Liu", "Ying-Cong Chen", "Lei Zhu", "Xinchao Wang"], "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14933", "pdf": "https://arxiv.org/pdf/2503.14933", "abs": "https://arxiv.org/abs/2503.14933", "authors": ["Yi Luo", "Hamed Hooshangnejad", "Xue Feng", "Gaofeng Huang", "Xiaojian Chen", "Rui Zhang", "Quan Chen", "Wil Ngwa", "Kai Ding"], "title": "A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "19 pages, 4 figures", "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15008", "pdf": "https://arxiv.org/pdf/2503.15008", "abs": "https://arxiv.org/abs/2503.15008", "authors": ["Aamir Mehmood", "Yue Hu", "Saddam Hussain Khan"], "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986", "summary": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15402", "pdf": "https://arxiv.org/pdf/2503.15402", "abs": "https://arxiv.org/abs/2503.15402", "authors": ["Alejandro Pequeño-Zurro", "Lyes Khacef", "Stefano Panzeri", "Elisabetta Chicca"], "title": "Towards efficient keyword spotting using spike-based time difference encoders", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.ET"], "comment": "26 pages, 9 figures", "summary": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14755", "pdf": "https://arxiv.org/pdf/2503.14755", "abs": "https://arxiv.org/abs/2503.14755", "authors": ["Omar E. Rakha", "Hazem M. Abbas"], "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors", "categories": ["cs.CL", "cs.AI"], "comment": "Paper was initially released in 2017 but was never published", "summary": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14797", "pdf": "https://arxiv.org/pdf/2503.14797", "abs": "https://arxiv.org/abs/2503.14797", "authors": ["Varich Boonsanong", "Vidhisha Balachandran", "Xiaochuang Han", "Shangbin Feng", "Lucy Lu Wang", "Yulia Tsvetkov"], "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14552", "pdf": "https://arxiv.org/pdf/2503.14552", "abs": "https://arxiv.org/abs/2503.14552", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Fatemeh Afghah", "Connor Peter McGrath", "Danish Bhatkar", "Mithilesh Anil Biradar", "Abolfazl Razi"], "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14891", "pdf": "https://arxiv.org/pdf/2503.14891", "abs": "https://arxiv.org/abs/2503.14891", "authors": ["Honglin Lin", "Zhuoshi Pan", "Yu Li", "Qizhi Pei", "Xin Gao", "Mengzhang Cai", "Conghui He", "Lijun Wu"], "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14917", "pdf": "https://arxiv.org/pdf/2503.14917", "abs": "https://arxiv.org/abs/2503.14917", "authors": ["Jiazheng Li", "Lu Yu", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Yanfang Ye", "Chuxu Zhang"], "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14607", "pdf": "https://arxiv.org/pdf/2503.14607", "abs": "https://arxiv.org/abs/2503.14607", "authors": ["Shuo Xing", "Zezhou Sun", "Shuangyu Xie", "Kaiyuan Chen", "Yanjia Huang", "Yuping Wang", "Jiachen Li", "Dezhen Song", "Zhengzhong Tu"], "title": "Can Large Vision Language Models Read Maps Like a Human?", "categories": ["cs.CV"], "comment": "35 pages", "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14716", "pdf": "https://arxiv.org/pdf/2503.14716", "abs": "https://arxiv.org/abs/2503.14716", "authors": ["Pei-Hsin Lin", "Jacob J. Lin", "Shang-Hsien Hsieh"], "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform", "categories": ["cs.CV", "cs.AI"], "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering", "summary": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14757", "pdf": "https://arxiv.org/pdf/2503.14757", "abs": "https://arxiv.org/abs/2503.14757", "authors": ["Marcelo Sanchez", "Gil Triginer", "Ignacio Sarasua", "Lara Raad", "Coloma Ballester"], "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing image inpainting methods have shown impressive completion results\nfor low-resolution images. However, most of these algorithms fail at high\nresolutions and require powerful hardware, limiting their deployment on edge\ndevices. Motivated by this, we propose the first baseline for REal-Time\nHigh-resolution image INpainting on Edge Devices (RETHINED) that is able to\ninpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a\nwide variety of mobile devices. A simple, yet effective novel method formed by\na lightweight Convolutional Neural Network (CNN) to recover structure, followed\nby a resolution-agnostic patch replacement mechanism to provide detailed\ntexture. Specially our pipeline leverages the structural capacity of CNN and\nthe high-level detail of patch-based methods, which is a key component for\nhigh-resolution image inpainting. To demonstrate the real application of our\nmethod, we conduct an extensive analysis on various mobile-friendly devices and\ndemonstrate similar inpainting performance while being $\\mathrm{100 \\times\nfaster}$ than existing state-of-the-art methods. Furthemore, we realease\nDF8K-Inpainting, the first free-form mask UHD inpainting dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15222", "pdf": "https://arxiv.org/pdf/2503.15222", "abs": "https://arxiv.org/abs/2503.15222", "authors": ["Pritam Kadasi", "Sriman Reddy", "Srivathsa Vamsi Chaturvedula", "Rudranshu Sen", "Agnish Saha", "Soumavo Sikdar", "Sayani Sarkar", "Suhani Mittal", "Rohit Jindal", "Mayank Singh"], "title": "Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation", "categories": ["cs.CL"], "comment": "Accepted to ICWSM'25", "summary": "With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15272", "pdf": "https://arxiv.org/pdf/2503.15272", "abs": "https://arxiv.org/abs/2503.15272", "authors": ["David Wan", "Justin Chih-Yao Chen", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine", "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14880", "pdf": "https://arxiv.org/pdf/2503.14880", "abs": "https://arxiv.org/abs/2503.14880", "authors": ["Henrique Morimitsu", "Xiaobin Zhu", "Roberto M. Cesar Jr.", "Xiangyang Ji", "Xu-Cheng Yin"], "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. The code and dataset are available at\n  https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow. 24\n  pages, 17 figures", "summary": "Optical flow estimation is essential for video processing tasks, such as\nrestoration and action recognition. The quality of videos is constantly\nincreasing, with current standards reaching 8K resolution. However, optical\nflow methods are usually designed for low resolution and do not generalize to\nlarge inputs due to their rigid architectures. They adopt downscaling or input\ntiling to reduce the input size, causing a loss of details and global\ninformation. There is also a lack of optical flow benchmarks to judge the\nactual performance of existing methods on high-resolution samples. Previous\nworks only conducted qualitative high-resolution evaluations on hand-picked\nsamples. This paper fills this gap in optical flow estimation in two ways. We\npropose DPFlow, an adaptive optical flow architecture capable of generalizing\nup to 8K resolution inputs while trained with only low-resolution samples. We\nalso introduce Kubric-NK, a new benchmark for evaluating optical flow methods\nwith input resolutions ranging from 1K to 8K. Our high-resolution evaluation\npushes the boundaries of existing methods and reveals new insights about their\ngeneralization capabilities. Extensive experimental results show that DPFlow\nachieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and\nother high-resolution benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15204", "pdf": "https://arxiv.org/pdf/2503.15204", "abs": "https://arxiv.org/abs/2503.15204", "authors": ["Tittaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Sorrawit Treesuk"], "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR", "cs.MA"], "comment": "14 pages, 2 figures", "summary": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14958", "pdf": "https://arxiv.org/pdf/2503.14958", "abs": "https://arxiv.org/abs/2503.14958", "authors": ["Zixuan Zheng", "Yilei Shi", "Chunlei Li", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15019", "pdf": "https://arxiv.org/pdf/2503.15019", "abs": "https://arxiv.org/abs/2503.15019", "authors": ["Shengqiong Wu", "Hao Fei", "Jingkang Yang", "Xiangtai Li", "Juncheng Li", "Hanwang Zhang", "Tat-seng Chua"], "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dimension"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15023", "pdf": "https://arxiv.org/pdf/2503.15023", "abs": "https://arxiv.org/abs/2503.15023", "authors": ["Chaouki Boufenar", "Mehdi Ayoub Rabiai", "Boualem Nadjib Zahaf", "Khelil Rafik Ouaras"], "title": "Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15024", "pdf": "https://arxiv.org/pdf/2503.15024", "abs": "https://arxiv.org/abs/2503.15024", "authors": ["Jin Wang", "Chenghui Lv", "Xian Li", "Shichao Dong", "Huadong Li", "kelu Yao", "Chao Li", "Wenqi Shao", "Ping Luo"], "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models", "categories": ["cs.CV"], "comment": "31 pages, 19 figures", "summary": "Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15056", "pdf": "https://arxiv.org/pdf/2503.15056", "abs": "https://arxiv.org/abs/2503.15056", "authors": ["Suhyeon Lee", "Kwanyoung Kim", "Jong Chul Ye"], "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation", "categories": ["cs.CV"], "comment": "25 pages, 16 figures", "summary": "Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15144", "pdf": "https://arxiv.org/pdf/2503.15144", "abs": "https://arxiv.org/abs/2503.15144", "authors": ["Xing He", "Zhe Zhu", "Liangliang Nan", "Honghua Chen", "Jing Qin", "Mingqiang Wei"], "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15283", "pdf": "https://arxiv.org/pdf/2503.15283", "abs": "https://arxiv.org/abs/2503.15283", "authors": ["Teng-Fang Hsiao", "Bo-Kai Ruan", "Yi-Lun Wu", "Tzu-Ling Lin", "Hong-Han Shuai"], "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15284", "pdf": "https://arxiv.org/pdf/2503.15284", "abs": "https://arxiv.org/abs/2503.15284", "authors": ["Yuanchao Yue", "Hui Yuan", "Qinglong Miao", "Xiaolong Mao", "Raouf Hamzaoui", "Peter Eisert"], "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15426", "pdf": "https://arxiv.org/pdf/2503.15426", "abs": "https://arxiv.org/abs/2503.15426", "authors": ["Wei Tang", "Yanpeng Sun", "Qinying Gu", "Zechao Li"], "title": "Visual Position Prompt for MLLM based Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14525", "pdf": "https://arxiv.org/pdf/2503.14525", "abs": "https://arxiv.org/abs/2503.14525", "authors": ["Frans Zdyb", "Albert Alonso", "Julius B. Kirkegaard"], "title": "Spline refinement with differentiable rendering", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Detecting slender, overlapping structures remains a challenge in\ncomputational microscopy. While recent coordinate-based approaches improve\ndetection, they often produce less accurate splines than pixel-based methods.\nWe introduce a training-free differentiable rendering approach to spline\nrefinement, achieving both high reliability and sub-pixel accuracy. Our method\nimproves spline quality, enhances robustness to distribution shifts, and\nshrinks the gap between synthetic and real-world data. Being fully\nunsupervised, the method is a drop-in replacement for the popular active\ncontour model for spline refinement. Evaluated on C. elegans nematodes, a\npopular model organism for drug discovery and biomedical research, we\ndemonstrate that our approach combines the strengths of both coordinate- and\npixel-based methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14536", "pdf": "https://arxiv.org/pdf/2503.14536", "abs": "https://arxiv.org/abs/2503.14536", "authors": ["Praveen Shastry", "Sowmya Chowdary Muthulur", "Naveen Kumarasami", "Anandakumar D", "Mounigasri M", "Keerthana R", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam", "Revathi Ezhumalai", "Abitha Marimuthu"], "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 92C55, 68U10, 92C50, 60G35"], "comment": "10 pages , 3 figures", "summary": "Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14538", "pdf": "https://arxiv.org/pdf/2503.14538", "abs": "https://arxiv.org/abs/2503.14538", "authors": ["Ananya Ganapthy", "Praveen Shastry", "Naveen Kumarasami", "Anandakumar D", "Keerthana R", "Mounigasri M", "Varshinipriya M", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam"], "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 68T45, 92C55, 92C50, 68U10"], "comment": "11 pages, 3 figures", "summary": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14562", "pdf": "https://arxiv.org/pdf/2503.14562", "abs": "https://arxiv.org/abs/2503.14562", "authors": ["A. I. Medvedeva", "V. V. Bakutkin"], "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "in Russian language", "summary": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14779", "pdf": "https://arxiv.org/pdf/2503.14779", "abs": "https://arxiv.org/abs/2503.14779", "authors": ["Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh"], "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14836", "pdf": "https://arxiv.org/pdf/2503.14836", "abs": "https://arxiv.org/abs/2503.14836", "authors": ["Kunyang Li", "Jean-Charles Noirot Ferrand", "Ryan Sheatsley", "Blaine Hoak", "Yohan Beugin", "Eric Pauley", "Patrick McDaniel"], "title": "On the Robustness Tradeoff in Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14908", "pdf": "https://arxiv.org/pdf/2503.14908", "abs": "https://arxiv.org/abs/2503.14908", "authors": ["Haoyu Chen", "Xiaojie Xu", "Wenbo Li", "Jingjing Ren", "Tian Ye", "Songhua Liu", "Ying-Cong Chen", "Lei Zhu", "Xinchao Wang"], "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14933", "pdf": "https://arxiv.org/pdf/2503.14933", "abs": "https://arxiv.org/abs/2503.14933", "authors": ["Yi Luo", "Hamed Hooshangnejad", "Xue Feng", "Gaofeng Huang", "Xiaojian Chen", "Rui Zhang", "Quan Chen", "Wil Ngwa", "Kai Ding"], "title": "A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "19 pages, 4 figures", "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15008", "pdf": "https://arxiv.org/pdf/2503.15008", "abs": "https://arxiv.org/abs/2503.15008", "authors": ["Aamir Mehmood", "Yue Hu", "Saddam Hussain Khan"], "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986", "summary": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15402", "pdf": "https://arxiv.org/pdf/2503.15402", "abs": "https://arxiv.org/abs/2503.15402", "authors": ["Alejandro Pequeño-Zurro", "Lyes Khacef", "Stefano Panzeri", "Elisabetta Chicca"], "title": "Towards efficient keyword spotting using spike-based time difference encoders", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.ET"], "comment": "26 pages, 9 figures", "summary": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14603", "pdf": "https://arxiv.org/pdf/2503.14603", "abs": "https://arxiv.org/abs/2503.14603", "authors": ["Yazeed Alnumay", "Alexandre Barbet", "Anna Bialas", "William Darling", "Shaan Desai", "Joan Devassy", "Kyle Duffy", "Stephanie Howe", "Olivia Lasche", "Justin Lee", "Anirudh Shrinivason", "Jennifer Tracey"], "title": "Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14620", "pdf": "https://arxiv.org/pdf/2503.14620", "abs": "https://arxiv.org/abs/2503.14620", "authors": ["Hikaru Shimadzu", "Takehito Utsuro", "Daisuke Kitayama"], "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14626", "pdf": "https://arxiv.org/pdf/2503.14626", "abs": "https://arxiv.org/abs/2503.14626", "authors": ["Ramon Ruiz-Dolz", "John Lawrence"], "title": "An Explainable Framework for Misinformation Identification via Critical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14718", "pdf": "https://arxiv.org/pdf/2503.14718", "abs": "https://arxiv.org/abs/2503.14718", "authors": ["Hakyung Sung", "Gyu-Ho Shin"], "title": "Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement", "categories": ["cs.CL"], "comment": null, "summary": "We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14530", "pdf": "https://arxiv.org/pdf/2503.14530", "abs": "https://arxiv.org/abs/2503.14530", "authors": ["Qing Li", "Jiahui Geng", "Derui Zhu", "Fengyu Cai", "Chenyang Lyu", "Fakhri Karray"], "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14537", "pdf": "https://arxiv.org/pdf/2503.14537", "abs": "https://arxiv.org/abs/2503.14537", "authors": ["Liewen Liao", "Weihao Yan", "Ming Yang", "Songan Zhang"], "title": "Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite augmenting\nperception, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. Commencing with an examination of input modalities, we investigates\nthe details of 3D reconstruction and conducts a multi-perspective, in-depth\nanalysis of recent advancements. Specifically, we first provide a systematic\nintroduction of preliminaries, including data formats, benchmarks and technical\npreliminaries of learning-based 3D reconstruction, facilitating instant\nidentification of suitable methods based on hardware configurations and sensor\nsuites. Then, we systematically review learning-based 3D reconstruction methods\nin autonomous driving, categorizing approaches by subtasks and conducting\nmulti-dimensional analysis and summary to establish a comprehensive technical\nreference. The development trends and existing challenges is summarized in the\ncontext of learning-based 3D reconstruction in autonomous driving. We hope that\nour review will inspire future researches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14547", "pdf": "https://arxiv.org/pdf/2503.14547", "abs": "https://arxiv.org/abs/2503.14547", "authors": ["Shuheng Li", "Jiayun Zhang", "Xiaohan Fu", "Xiyuan Zhang", "Jingbo Shang", "Rajesh K. Gupta"], "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted by SenSys 2025", "summary": "In human activity recognition (HAR), activity labels have typically been\nencoded in one-hot format, which has a recent shift towards using textual\nrepresentations to provide contextual knowledge. Here, we argue that HAR should\nbe anchored to physical motion data, as motion forms the basis of activity and\napplies effectively across sensing systems, whereas text is inherently limited.\nWe propose SKELAR, a novel HAR framework that pretrains activity\nrepresentations from skeleton data and matches them with heterogeneous HAR\nsignals. Our method addresses two major challenges: (1) capturing core motion\nknowledge without context-specific details. We achieve this through a\nself-supervised coarse angle reconstruction task that recovers joint rotation\nangles, invariant to both users and deployments; (2) adapting the\nrepresentations to downstream tasks with varying modalities and focuses. To\naddress this, we introduce a self-attention matching module that dynamically\nprioritizes relevant body parts in a data-driven manner. Given the lack of\ncorresponding labels in existing skeleton data, we establish MASD, a new HAR\ndataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27\nactivities. This is the first broadly applicable HAR dataset with\ntime-synchronized data across three modalities. Experiments show that SKELAR\nachieves the state-of-the-art performance in both full-shot and few-shot\nsettings. We also demonstrate that SKELAR can effectively leverage synthetic\nskeleton data to extend its use in scenarios without skeleton collections.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14553", "pdf": "https://arxiv.org/pdf/2503.14553", "abs": "https://arxiv.org/abs/2503.14553", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Naji Khosravan", "Seyyedali Hosseinalipour"], "title": "Redefining non-IID Data in Federated Learning for Computer Vision Tasks: Migrating from Labels to Embeddings for Task-Specific Data Distributions", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 9 figures, 1 table, (implementations are included at our\n  GitHub repository: https://github.com/KasraBorazjani/task-perspective-het)", "summary": "Federated Learning (FL) represents a paradigm shift in distributed machine\nlearning (ML), enabling clients to train models collaboratively while keeping\ntheir raw data private. This paradigm shift from traditional centralized ML\nintroduces challenges due to the non-iid (non-independent and identically\ndistributed) nature of data across clients, significantly impacting FL's\nperformance. Existing literature, predominantly model data heterogeneity by\nimposing label distribution skew across clients. In this paper, we show that\nlabel distribution skew fails to fully capture the real-world data\nheterogeneity among clients in computer vision tasks beyond classification.\nSubsequently, we demonstrate that current approaches overestimate FL's\nperformance by relying on label/class distribution skew, exposing an overlooked\ngap in the literature. By utilizing pre-trained deep neural networks to extract\ntask-specific data embeddings, we define task-specific data heterogeneity\nthrough the lens of each vision task and introduce a new level of data\nheterogeneity called embedding-based data heterogeneity. Our methodology\ninvolves clustering data points based on embeddings and distributing them among\nclients using the Dirichlet distribution. Through extensive experiments, we\nevaluate the performance of different FL methods under our revamped notion of\ndata heterogeneity, introducing new benchmark performance measures to the\nliterature. We further unveil a series of open research directions that can be\npursued.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14640", "pdf": "https://arxiv.org/pdf/2503.14640", "abs": "https://arxiv.org/abs/2503.14640", "authors": ["Yi Liao", "Yongsheng Gao", "Weichuan Zhang"], "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14991", "pdf": "https://arxiv.org/pdf/2503.14991", "abs": "https://arxiv.org/abs/2503.14991", "authors": ["Stefan Arnold"], "title": "Inspecting the Representation Manifold of Differentially-Private Text", "categories": ["cs.CL"], "comment": null, "summary": "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14665", "pdf": "https://arxiv.org/pdf/2503.14665", "abs": "https://arxiv.org/abs/2503.14665", "authors": ["Parker Ewen", "Hao Chen", "Seth Isaacson", "Joey Wilson", "Katherine A. Skinner", "Ram Vasudevan"], "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing.Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15055", "pdf": "https://arxiv.org/pdf/2503.15055", "abs": "https://arxiv.org/abs/2503.15055", "authors": ["Arina Razmyslovich", "Kseniia Murasheva", "Sofia Sedlova", "Julien Capitaine", "Eugene Dmitriev"], "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15117", "pdf": "https://arxiv.org/pdf/2503.15117", "abs": "https://arxiv.org/abs/2503.15117", "authors": ["Shichen Li", "Zhongqing Wang", "Zheyu Zhao", "Yue Zhang", "Peifeng Li"], "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification", "categories": ["cs.CL"], "comment": "AAAI2025", "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["aspect-based"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14736", "pdf": "https://arxiv.org/pdf/2503.14736", "abs": "https://arxiv.org/abs/2503.14736", "authors": ["Yilan Dong", "Haohe Liu", "Qing Wang", "Jiahao Yang", "Wenqing Wang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14783", "pdf": "https://arxiv.org/pdf/2503.14783", "abs": "https://arxiv.org/abs/2503.14783", "authors": ["Ge Yan", "Tsui-Wei Weng"], "title": "RAT: Boosting Misclassification Detection Ability without Extra Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15235", "pdf": "https://arxiv.org/pdf/2503.15235", "abs": "https://arxiv.org/abs/2503.15235", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "title": "Exploring Large Language Models for Word Games:Who is the Spy?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15242", "pdf": "https://arxiv.org/pdf/2503.15242", "abs": "https://arxiv.org/abs/2503.15242", "authors": ["Pierre Chambon", "Baptiste Roziere", "Benoit Sagot", "Gabriel Synnaeve"], "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?", "categories": ["cs.CL", "cs.AI", "cs.CC"], "comment": null, "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14853", "pdf": "https://arxiv.org/pdf/2503.14853", "abs": "https://arxiv.org/abs/2503.14853", "authors": ["Peipeng Yu", "Jianwei Fei", "Hui Gao", "Xuan Feng", "Zhihua Xia", "Chip Hong Chang"], "title": "Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14863", "pdf": "https://arxiv.org/pdf/2503.14863", "abs": "https://arxiv.org/abs/2503.14863", "authors": ["Hengkang Wang", "Yang Liu", "Huidong Liu", "Chien-Chih Wang", "Yanhui Guo", "Hongdong Li", "Bryan Wang", "Ju Sun"], "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video restoration (VR) aims to recover high-quality videos from degraded\nones. Although recent zero-shot VR methods using pre-trained diffusion models\n(DMs) show good promise, they suffer from approximation errors during reverse\ndiffusion and insufficient temporal consistency. Moreover, dealing with 3D\nvideo data, VR is inherently computationally intensive. In this paper, we\nadvocate viewing the reverse process in DMs as a function and present a novel\nMaximum a Posterior (MAP) framework that directly parameterizes video frames in\nthe seed space of DMs, eliminating approximation errors. We also introduce\nstrategies to promote bilevel temporal consistency: semantic consistency by\nleveraging clustering structures in the seed space, and pixel-level consistency\nby progressive warping with optical flow refinements. Extensive experiments on\nmultiple virtual reality tasks demonstrate superior visual quality and temporal\nconsistency achieved by our method compared to the state-of-the-art.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15450", "pdf": "https://arxiv.org/pdf/2503.15450", "abs": "https://arxiv.org/abs/2503.15450", "authors": ["Tongyao Zhu", "Qian Liu", "Haonan Wang", "Shiqi Chen", "Xiangming Gu", "Tianyu Pang", "Min-Yen Kan"], "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling", "categories": ["cs.CL"], "comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models", "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14912", "pdf": "https://arxiv.org/pdf/2503.14912", "abs": "https://arxiv.org/abs/2503.14912", "authors": ["Gahye Lee", "Hyejeong Yoon", "Jungeon Kim", "Seungyong Lee"], "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes", "categories": ["cs.CV", "I.4.8; I.3.5"], "comment": "Accepted to 3DV 2025", "summary": "This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14938", "pdf": "https://arxiv.org/pdf/2503.14938", "abs": "https://arxiv.org/abs/2503.14938", "authors": ["Zhong Ji", "Ci Liu", "Jingren Liu", "Chen Tang", "Yanwei Pang", "Xuelong Li"], "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14649", "pdf": "https://arxiv.org/pdf/2503.14649", "abs": "https://arxiv.org/abs/2503.14649", "authors": ["Wenqi Jiang", "Suvinay Subramanian", "Cat Graves", "Gustavo Alonso", "Amir Yazdanbakhsh", "Vidushi Dadu"], "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14939", "pdf": "https://arxiv.org/pdf/2503.14939", "abs": "https://arxiv.org/abs/2503.14939", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14944", "pdf": "https://arxiv.org/pdf/2503.14944", "abs": "https://arxiv.org/abs/2503.14944", "authors": ["Zihan Cao", "Yu Zhong", "Ziqi Wang", "Liang-Jian Deng"], "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14948", "pdf": "https://arxiv.org/pdf/2503.14948", "abs": "https://arxiv.org/abs/2503.14948", "authors": ["Hao Liang", "Zhipeng Dong", "Yi Yang", "Mengyin Fu"], "title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14966", "pdf": "https://arxiv.org/pdf/2503.14966", "abs": "https://arxiv.org/abs/2503.14966", "authors": ["Tingxiu Chen", "Yilei Shi", "Zixuan Zheng", "Bingcong Yan", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "MICCAI 2024", "summary": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14990", "pdf": "https://arxiv.org/pdf/2503.14990", "abs": "https://arxiv.org/abs/2503.14990", "authors": ["Kévin Polisano", "Sylvain Meignen", "Nils Laurent", "Hubert Leterme"], "title": "Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15001", "pdf": "https://arxiv.org/pdf/2503.15001", "abs": "https://arxiv.org/abs/2503.15001", "authors": ["Michael Neri", "Federica Battisti"], "title": "Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA", "summary": "During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15016", "pdf": "https://arxiv.org/pdf/2503.15016", "abs": "https://arxiv.org/abs/2503.15016", "authors": ["Fethi Harkat", "Tiphaine Deuberet", "Guillaume Gey", "Valérie Perrier", "Kévin Polisano"], "title": "Manifold Learning for Hyperspectral Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15087", "pdf": "https://arxiv.org/pdf/2503.15087", "abs": "https://arxiv.org/abs/2503.15087", "authors": ["Christoph Griesbacher", "Christian Fruhwirth-Reisinger"], "title": "An Investigation of Beam Density on LiDAR Object Detection Performance", "categories": ["cs.CV"], "comment": "Accepted by CVWW 2025", "summary": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15138", "pdf": "https://arxiv.org/pdf/2503.15138", "abs": "https://arxiv.org/abs/2503.15138", "authors": ["Mingzhe Zheng", "Yongqi Xu", "Haojian Huang", "Xuran Ma", "Yexin Liu", "Wenjie Shu", "Yatian Pang", "Feilong Tang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention", "categories": ["cs.CV"], "comment": "Code: https://github.com/DuNGEOnmassster/VideoGen-of-Thought.git;\n  Webpage: https://cheliosoops.github.io/VGoT/", "summary": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15208", "pdf": "https://arxiv.org/pdf/2503.15208", "abs": "https://arxiv.org/abs/2503.15208", "authors": ["Jiazhe Guo", "Yikang Ding", "Xiwu Chen", "Shuo Chen", "Bohan Li", "Yingshuang Zou", "Xiaoyang Lyu", "Feiyang Tan", "Xiaojuan Qi", "Zhiheng Li", "Hao Zhao"], "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15211", "pdf": "https://arxiv.org/pdf/2503.15211", "abs": "https://arxiv.org/abs/2503.15211", "authors": ["Zechuan Li", "Hongshan Yu", "Yihao Ding", "Jinhao Qiao", "Basim Azam", "Naveed Akhtar"], "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15260", "pdf": "https://arxiv.org/pdf/2503.15260", "abs": "https://arxiv.org/abs/2503.15260", "authors": ["Lei Shi", "Xi Fang", "Naiyu Wang", "Junxing Zhang"], "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15342", "pdf": "https://arxiv.org/pdf/2503.15342", "abs": "https://arxiv.org/abs/2503.15342", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Ali Khaleghi Rahimian", "Thomas MacDougall"], "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15415", "pdf": "https://arxiv.org/pdf/2503.15415", "abs": "https://arxiv.org/abs/2503.15415", "authors": ["Giovanni Floreale", "Piero Baraldi", "Enrico Zio", "Olga Fink"], "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14542", "pdf": "https://arxiv.org/pdf/2503.14542", "abs": "https://arxiv.org/abs/2503.14542", "authors": ["Agnieszka Sroka-Oleksiak", "Adam Pardyl", "Dawid Rymarczyk", "Aldona Olechowska-Jarząb", "Katarzyna Biegun-Drożdż", "Dorota Ochońska", "Michał Wronka", "Adriana Borowa", "Tomasz Gosiewski", "Miłosz Adamczyk", "Henryk Telega", "Bartosz Zieliński", "Monika Brzychczy-Włoch"], "title": "AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Sepsis is a life-threatening condition which requires rapid diagnosis and\ntreatment. Traditional microbiological methods are time-consuming and\nexpensive. In response to these challenges, deep learning algorithms were\ndeveloped to identify 14 bacteria species and 3 yeast-like fungi from\nmicroscopic images of Gram-stained smears of positive blood samples from sepsis\npatients.\n  A total of 16,637 Gram-stained microscopic images were used in the study. The\nanalysis used the Cellpose 3 model for segmentation and Attention-based Deep\nMultiple Instance Learning for classification. Our model achieved an accuracy\nof 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88,\nrespectively. The highest values, reaching up to 96.2%, were obtained for\nCutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and\nNakaseomyces glabratus. Classification difficulties were observed in closely\nrelated species, such as Staphylococcus hominis and Staphylococcus\nhaemolyticus, due to morphological similarity, and within Candida albicans due\nto high morphotic diversity.\n  The study confirms the potential of our model for microbial classification,\nbut it also indicates the need for further optimisation and expansion of the\ntraining data set. In the future, this technology could support microbial\ndiagnosis, reducing diagnostic time and improving the effectiveness of sepsis\ntreatment due to its simplicity and accessibility. Part of the results\npresented in this publication was covered by a patent application at the\nEuropean Patent Office EP24461637.1 \"A computer implemented method for\nidentifying a microorganism in a blood and a data processing system therefor\".", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14573", "pdf": "https://arxiv.org/pdf/2503.14573", "abs": "https://arxiv.org/abs/2503.14573", "authors": ["Wanxin Yu", "Zhemin Zhu", "Cong Wang", "Yihang Bao", "Chunjie Xia", "Rongshan Cheng", "Yan Yu", "Tsung-Yuan Tsai"], "title": "Three-dimensional Reconstruction of the Lumbar Spine with Submillimeter Accuracy Using Biplanar X-ray Images", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "21 pages, 10 figures, 4 tables", "summary": "Three-dimensional reconstruction of the spine under weight-bearing conditions\nfrom biplanar X-ray images is of great importance for the clinical assessment\nof spinal diseases. However, the current fully automated reconstruction methods\nhave low accuracy and fail to meet the clinical application standards. This\nstudy developed and validated a fully automated method for high-accuracy 3D\nreconstruction of the lumbar spine from biplanar X-ray images. The method\ninvolves lumbar decomposition and landmark detection from the raw X-ray images,\nfollowed by a deformable model and landmark-weighted 2D-3D registration\napproach. The reconstruction accuracy was validated by the gold standard\nobtained through the registration of CT-segmented vertebral models with the\nbiplanar X-ray images. The proposed method achieved a 3D reconstruction\naccuracy of 0.80 mm, representing a significant improvement over the mainstream\napproaches. This study will contribute to the clinical diagnosis of lumbar in\nweight-bearing positions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14655", "pdf": "https://arxiv.org/pdf/2503.14655", "abs": "https://arxiv.org/abs/2503.14655", "authors": ["Minheng Chen", "Xiaowei Yu", "Jing Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14892", "pdf": "https://arxiv.org/pdf/2503.14892", "abs": "https://arxiv.org/abs/2503.14892", "authors": ["He Huang", "Yong Chen", "Yujun Guo", "Wei He"], "title": "Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) fusion is an efficient technique that combines\nlow-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)\nto generate high-resolution HSI (HR-HSI). Existing supervised learning methods\n(SLMs) can yield promising results when test data degradation matches the\ntraining ones, but they face challenges in generalizing to unknown\ndegradations. To unleash the potential and generalization ability of SLMs, we\npropose a novel self-supervised unknown-to-known degradation transformation\nframework (U2K) for blind HSI fusion, which adaptively transforms unknown\ndegradation into the same type of degradation as those handled by pre-trained\nSLMs. Specifically, the proposed U2K framework consists of: (1) spatial and\nspectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded\nHR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert\nthese wrapped data into predefined degradation patterns. The transformed HR-MSI\nand LR-HSI pairs are then processed by a pre-trained network to reconstruct the\ntarget HR-HSI. We train the U2K framework in a self-supervised manner using\nconsistency loss and greedy alternating optimization, significantly improving\nthe flexibility of blind HSI fusion. Extensive experiments confirm the\neffectiveness of our proposed U2K framework in boosting the adaptability of\nfive existing SLMs under various degradation settings and surpassing\nstate-of-the-art blind methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15029", "pdf": "https://arxiv.org/pdf/2503.15029", "abs": "https://arxiv.org/abs/2503.15029", "authors": ["Jianbo Zhao", "Taiyu Ban", "Zhihao Liu", "Hangning Zhou", "Xiyang Wang", "Qibin Zhou", "Hailong Qin", "Mu Yang", "Lei Liu", "Bin Li"], "title": "DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15288", "pdf": "https://arxiv.org/pdf/2503.15288", "abs": "https://arxiv.org/abs/2503.15288", "authors": ["Justin Le Louëdec", "Maike Bauer", "Tanja Amerstorfer", "Jackie A. Davies"], "title": "Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking", "categories": ["physics.space-ph", "cs.CV", "cs.LG"], "comment": "24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025", "summary": "Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14603", "pdf": "https://arxiv.org/pdf/2503.14603", "abs": "https://arxiv.org/abs/2503.14603", "authors": ["Yazeed Alnumay", "Alexandre Barbet", "Anna Bialas", "William Darling", "Shaan Desai", "Joan Devassy", "Kyle Duffy", "Stephanie Howe", "Olivia Lasche", "Justin Lee", "Anirudh Shrinivason", "Jennifer Tracey"], "title": "Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14620", "pdf": "https://arxiv.org/pdf/2503.14620", "abs": "https://arxiv.org/abs/2503.14620", "authors": ["Hikaru Shimadzu", "Takehito Utsuro", "Daisuke Kitayama"], "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14626", "pdf": "https://arxiv.org/pdf/2503.14626", "abs": "https://arxiv.org/abs/2503.14626", "authors": ["Ramon Ruiz-Dolz", "John Lawrence"], "title": "An Explainable Framework for Misinformation Identification via Critical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14718", "pdf": "https://arxiv.org/pdf/2503.14718", "abs": "https://arxiv.org/abs/2503.14718", "authors": ["Hakyung Sung", "Gyu-Ho Shin"], "title": "Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement", "categories": ["cs.CL"], "comment": null, "summary": "We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14530", "pdf": "https://arxiv.org/pdf/2503.14530", "abs": "https://arxiv.org/abs/2503.14530", "authors": ["Qing Li", "Jiahui Geng", "Derui Zhu", "Fengyu Cai", "Chenyang Lyu", "Fakhri Karray"], "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14537", "pdf": "https://arxiv.org/pdf/2503.14537", "abs": "https://arxiv.org/abs/2503.14537", "authors": ["Liewen Liao", "Weihao Yan", "Ming Yang", "Songan Zhang"], "title": "Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite augmenting\nperception, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. Commencing with an examination of input modalities, we investigates\nthe details of 3D reconstruction and conducts a multi-perspective, in-depth\nanalysis of recent advancements. Specifically, we first provide a systematic\nintroduction of preliminaries, including data formats, benchmarks and technical\npreliminaries of learning-based 3D reconstruction, facilitating instant\nidentification of suitable methods based on hardware configurations and sensor\nsuites. Then, we systematically review learning-based 3D reconstruction methods\nin autonomous driving, categorizing approaches by subtasks and conducting\nmulti-dimensional analysis and summary to establish a comprehensive technical\nreference. The development trends and existing challenges is summarized in the\ncontext of learning-based 3D reconstruction in autonomous driving. We hope that\nour review will inspire future researches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14547", "pdf": "https://arxiv.org/pdf/2503.14547", "abs": "https://arxiv.org/abs/2503.14547", "authors": ["Shuheng Li", "Jiayun Zhang", "Xiaohan Fu", "Xiyuan Zhang", "Jingbo Shang", "Rajesh K. Gupta"], "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted by SenSys 2025", "summary": "In human activity recognition (HAR), activity labels have typically been\nencoded in one-hot format, which has a recent shift towards using textual\nrepresentations to provide contextual knowledge. Here, we argue that HAR should\nbe anchored to physical motion data, as motion forms the basis of activity and\napplies effectively across sensing systems, whereas text is inherently limited.\nWe propose SKELAR, a novel HAR framework that pretrains activity\nrepresentations from skeleton data and matches them with heterogeneous HAR\nsignals. Our method addresses two major challenges: (1) capturing core motion\nknowledge without context-specific details. We achieve this through a\nself-supervised coarse angle reconstruction task that recovers joint rotation\nangles, invariant to both users and deployments; (2) adapting the\nrepresentations to downstream tasks with varying modalities and focuses. To\naddress this, we introduce a self-attention matching module that dynamically\nprioritizes relevant body parts in a data-driven manner. Given the lack of\ncorresponding labels in existing skeleton data, we establish MASD, a new HAR\ndataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27\nactivities. This is the first broadly applicable HAR dataset with\ntime-synchronized data across three modalities. Experiments show that SKELAR\nachieves the state-of-the-art performance in both full-shot and few-shot\nsettings. We also demonstrate that SKELAR can effectively leverage synthetic\nskeleton data to extend its use in scenarios without skeleton collections.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14553", "pdf": "https://arxiv.org/pdf/2503.14553", "abs": "https://arxiv.org/abs/2503.14553", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Naji Khosravan", "Seyyedali Hosseinalipour"], "title": "Redefining non-IID Data in Federated Learning for Computer Vision Tasks: Migrating from Labels to Embeddings for Task-Specific Data Distributions", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 9 figures, 1 table, (implementations are included at our\n  GitHub repository: https://github.com/KasraBorazjani/task-perspective-het)", "summary": "Federated Learning (FL) represents a paradigm shift in distributed machine\nlearning (ML), enabling clients to train models collaboratively while keeping\ntheir raw data private. This paradigm shift from traditional centralized ML\nintroduces challenges due to the non-iid (non-independent and identically\ndistributed) nature of data across clients, significantly impacting FL's\nperformance. Existing literature, predominantly model data heterogeneity by\nimposing label distribution skew across clients. In this paper, we show that\nlabel distribution skew fails to fully capture the real-world data\nheterogeneity among clients in computer vision tasks beyond classification.\nSubsequently, we demonstrate that current approaches overestimate FL's\nperformance by relying on label/class distribution skew, exposing an overlooked\ngap in the literature. By utilizing pre-trained deep neural networks to extract\ntask-specific data embeddings, we define task-specific data heterogeneity\nthrough the lens of each vision task and introduce a new level of data\nheterogeneity called embedding-based data heterogeneity. Our methodology\ninvolves clustering data points based on embeddings and distributing them among\nclients using the Dirichlet distribution. Through extensive experiments, we\nevaluate the performance of different FL methods under our revamped notion of\ndata heterogeneity, introducing new benchmark performance measures to the\nliterature. We further unveil a series of open research directions that can be\npursued.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14640", "pdf": "https://arxiv.org/pdf/2503.14640", "abs": "https://arxiv.org/abs/2503.14640", "authors": ["Yi Liao", "Yongsheng Gao", "Weichuan Zhang"], "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14991", "pdf": "https://arxiv.org/pdf/2503.14991", "abs": "https://arxiv.org/abs/2503.14991", "authors": ["Stefan Arnold"], "title": "Inspecting the Representation Manifold of Differentially-Private Text", "categories": ["cs.CL"], "comment": null, "summary": "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14665", "pdf": "https://arxiv.org/pdf/2503.14665", "abs": "https://arxiv.org/abs/2503.14665", "authors": ["Parker Ewen", "Hao Chen", "Seth Isaacson", "Joey Wilson", "Katherine A. Skinner", "Ram Vasudevan"], "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing.Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15055", "pdf": "https://arxiv.org/pdf/2503.15055", "abs": "https://arxiv.org/abs/2503.15055", "authors": ["Arina Razmyslovich", "Kseniia Murasheva", "Sofia Sedlova", "Julien Capitaine", "Eugene Dmitriev"], "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15117", "pdf": "https://arxiv.org/pdf/2503.15117", "abs": "https://arxiv.org/abs/2503.15117", "authors": ["Shichen Li", "Zhongqing Wang", "Zheyu Zhao", "Yue Zhang", "Peifeng Li"], "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification", "categories": ["cs.CL"], "comment": "AAAI2025", "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["aspect-based"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14736", "pdf": "https://arxiv.org/pdf/2503.14736", "abs": "https://arxiv.org/abs/2503.14736", "authors": ["Yilan Dong", "Haohe Liu", "Qing Wang", "Jiahao Yang", "Wenqing Wang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14783", "pdf": "https://arxiv.org/pdf/2503.14783", "abs": "https://arxiv.org/abs/2503.14783", "authors": ["Ge Yan", "Tsui-Wei Weng"], "title": "RAT: Boosting Misclassification Detection Ability without Extra Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15235", "pdf": "https://arxiv.org/pdf/2503.15235", "abs": "https://arxiv.org/abs/2503.15235", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "title": "Exploring Large Language Models for Word Games:Who is the Spy?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15242", "pdf": "https://arxiv.org/pdf/2503.15242", "abs": "https://arxiv.org/abs/2503.15242", "authors": ["Pierre Chambon", "Baptiste Roziere", "Benoit Sagot", "Gabriel Synnaeve"], "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?", "categories": ["cs.CL", "cs.AI", "cs.CC"], "comment": null, "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14853", "pdf": "https://arxiv.org/pdf/2503.14853", "abs": "https://arxiv.org/abs/2503.14853", "authors": ["Peipeng Yu", "Jianwei Fei", "Hui Gao", "Xuan Feng", "Zhihua Xia", "Chip Hong Chang"], "title": "Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14863", "pdf": "https://arxiv.org/pdf/2503.14863", "abs": "https://arxiv.org/abs/2503.14863", "authors": ["Hengkang Wang", "Yang Liu", "Huidong Liu", "Chien-Chih Wang", "Yanhui Guo", "Hongdong Li", "Bryan Wang", "Ju Sun"], "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video restoration (VR) aims to recover high-quality videos from degraded\nones. Although recent zero-shot VR methods using pre-trained diffusion models\n(DMs) show good promise, they suffer from approximation errors during reverse\ndiffusion and insufficient temporal consistency. Moreover, dealing with 3D\nvideo data, VR is inherently computationally intensive. In this paper, we\nadvocate viewing the reverse process in DMs as a function and present a novel\nMaximum a Posterior (MAP) framework that directly parameterizes video frames in\nthe seed space of DMs, eliminating approximation errors. We also introduce\nstrategies to promote bilevel temporal consistency: semantic consistency by\nleveraging clustering structures in the seed space, and pixel-level consistency\nby progressive warping with optical flow refinements. Extensive experiments on\nmultiple virtual reality tasks demonstrate superior visual quality and temporal\nconsistency achieved by our method compared to the state-of-the-art.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15450", "pdf": "https://arxiv.org/pdf/2503.15450", "abs": "https://arxiv.org/abs/2503.15450", "authors": ["Tongyao Zhu", "Qian Liu", "Haonan Wang", "Shiqi Chen", "Xiangming Gu", "Tianyu Pang", "Min-Yen Kan"], "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling", "categories": ["cs.CL"], "comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models", "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14912", "pdf": "https://arxiv.org/pdf/2503.14912", "abs": "https://arxiv.org/abs/2503.14912", "authors": ["Gahye Lee", "Hyejeong Yoon", "Jungeon Kim", "Seungyong Lee"], "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes", "categories": ["cs.CV", "I.4.8; I.3.5"], "comment": "Accepted to 3DV 2025", "summary": "This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14938", "pdf": "https://arxiv.org/pdf/2503.14938", "abs": "https://arxiv.org/abs/2503.14938", "authors": ["Zhong Ji", "Ci Liu", "Jingren Liu", "Chen Tang", "Yanwei Pang", "Xuelong Li"], "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14649", "pdf": "https://arxiv.org/pdf/2503.14649", "abs": "https://arxiv.org/abs/2503.14649", "authors": ["Wenqi Jiang", "Suvinay Subramanian", "Cat Graves", "Gustavo Alonso", "Amir Yazdanbakhsh", "Vidushi Dadu"], "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14939", "pdf": "https://arxiv.org/pdf/2503.14939", "abs": "https://arxiv.org/abs/2503.14939", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14944", "pdf": "https://arxiv.org/pdf/2503.14944", "abs": "https://arxiv.org/abs/2503.14944", "authors": ["Zihan Cao", "Yu Zhong", "Ziqi Wang", "Liang-Jian Deng"], "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14948", "pdf": "https://arxiv.org/pdf/2503.14948", "abs": "https://arxiv.org/abs/2503.14948", "authors": ["Hao Liang", "Zhipeng Dong", "Yi Yang", "Mengyin Fu"], "title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14966", "pdf": "https://arxiv.org/pdf/2503.14966", "abs": "https://arxiv.org/abs/2503.14966", "authors": ["Tingxiu Chen", "Yilei Shi", "Zixuan Zheng", "Bingcong Yan", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "MICCAI 2024", "summary": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14990", "pdf": "https://arxiv.org/pdf/2503.14990", "abs": "https://arxiv.org/abs/2503.14990", "authors": ["Kévin Polisano", "Sylvain Meignen", "Nils Laurent", "Hubert Leterme"], "title": "Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15001", "pdf": "https://arxiv.org/pdf/2503.15001", "abs": "https://arxiv.org/abs/2503.15001", "authors": ["Michael Neri", "Federica Battisti"], "title": "Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA", "summary": "During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15016", "pdf": "https://arxiv.org/pdf/2503.15016", "abs": "https://arxiv.org/abs/2503.15016", "authors": ["Fethi Harkat", "Tiphaine Deuberet", "Guillaume Gey", "Valérie Perrier", "Kévin Polisano"], "title": "Manifold Learning for Hyperspectral Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15087", "pdf": "https://arxiv.org/pdf/2503.15087", "abs": "https://arxiv.org/abs/2503.15087", "authors": ["Christoph Griesbacher", "Christian Fruhwirth-Reisinger"], "title": "An Investigation of Beam Density on LiDAR Object Detection Performance", "categories": ["cs.CV"], "comment": "Accepted by CVWW 2025", "summary": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15138", "pdf": "https://arxiv.org/pdf/2503.15138", "abs": "https://arxiv.org/abs/2503.15138", "authors": ["Mingzhe Zheng", "Yongqi Xu", "Haojian Huang", "Xuran Ma", "Yexin Liu", "Wenjie Shu", "Yatian Pang", "Feilong Tang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention", "categories": ["cs.CV"], "comment": "Code: https://github.com/DuNGEOnmassster/VideoGen-of-Thought.git;\n  Webpage: https://cheliosoops.github.io/VGoT/", "summary": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15208", "pdf": "https://arxiv.org/pdf/2503.15208", "abs": "https://arxiv.org/abs/2503.15208", "authors": ["Jiazhe Guo", "Yikang Ding", "Xiwu Chen", "Shuo Chen", "Bohan Li", "Yingshuang Zou", "Xiaoyang Lyu", "Feiyang Tan", "Xiaojuan Qi", "Zhiheng Li", "Hao Zhao"], "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15211", "pdf": "https://arxiv.org/pdf/2503.15211", "abs": "https://arxiv.org/abs/2503.15211", "authors": ["Zechuan Li", "Hongshan Yu", "Yihao Ding", "Jinhao Qiao", "Basim Azam", "Naveed Akhtar"], "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15260", "pdf": "https://arxiv.org/pdf/2503.15260", "abs": "https://arxiv.org/abs/2503.15260", "authors": ["Lei Shi", "Xi Fang", "Naiyu Wang", "Junxing Zhang"], "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15342", "pdf": "https://arxiv.org/pdf/2503.15342", "abs": "https://arxiv.org/abs/2503.15342", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Ali Khaleghi Rahimian", "Thomas MacDougall"], "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15415", "pdf": "https://arxiv.org/pdf/2503.15415", "abs": "https://arxiv.org/abs/2503.15415", "authors": ["Giovanni Floreale", "Piero Baraldi", "Enrico Zio", "Olga Fink"], "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14542", "pdf": "https://arxiv.org/pdf/2503.14542", "abs": "https://arxiv.org/abs/2503.14542", "authors": ["Agnieszka Sroka-Oleksiak", "Adam Pardyl", "Dawid Rymarczyk", "Aldona Olechowska-Jarząb", "Katarzyna Biegun-Drożdż", "Dorota Ochońska", "Michał Wronka", "Adriana Borowa", "Tomasz Gosiewski", "Miłosz Adamczyk", "Henryk Telega", "Bartosz Zieliński", "Monika Brzychczy-Włoch"], "title": "AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Sepsis is a life-threatening condition which requires rapid diagnosis and\ntreatment. Traditional microbiological methods are time-consuming and\nexpensive. In response to these challenges, deep learning algorithms were\ndeveloped to identify 14 bacteria species and 3 yeast-like fungi from\nmicroscopic images of Gram-stained smears of positive blood samples from sepsis\npatients.\n  A total of 16,637 Gram-stained microscopic images were used in the study. The\nanalysis used the Cellpose 3 model for segmentation and Attention-based Deep\nMultiple Instance Learning for classification. Our model achieved an accuracy\nof 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88,\nrespectively. The highest values, reaching up to 96.2%, were obtained for\nCutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and\nNakaseomyces glabratus. Classification difficulties were observed in closely\nrelated species, such as Staphylococcus hominis and Staphylococcus\nhaemolyticus, due to morphological similarity, and within Candida albicans due\nto high morphotic diversity.\n  The study confirms the potential of our model for microbial classification,\nbut it also indicates the need for further optimisation and expansion of the\ntraining data set. In the future, this technology could support microbial\ndiagnosis, reducing diagnostic time and improving the effectiveness of sepsis\ntreatment due to its simplicity and accessibility. Part of the results\npresented in this publication was covered by a patent application at the\nEuropean Patent Office EP24461637.1 \"A computer implemented method for\nidentifying a microorganism in a blood and a data processing system therefor\".", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14573", "pdf": "https://arxiv.org/pdf/2503.14573", "abs": "https://arxiv.org/abs/2503.14573", "authors": ["Wanxin Yu", "Zhemin Zhu", "Cong Wang", "Yihang Bao", "Chunjie Xia", "Rongshan Cheng", "Yan Yu", "Tsung-Yuan Tsai"], "title": "Three-dimensional Reconstruction of the Lumbar Spine with Submillimeter Accuracy Using Biplanar X-ray Images", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "21 pages, 10 figures, 4 tables", "summary": "Three-dimensional reconstruction of the spine under weight-bearing conditions\nfrom biplanar X-ray images is of great importance for the clinical assessment\nof spinal diseases. However, the current fully automated reconstruction methods\nhave low accuracy and fail to meet the clinical application standards. This\nstudy developed and validated a fully automated method for high-accuracy 3D\nreconstruction of the lumbar spine from biplanar X-ray images. The method\ninvolves lumbar decomposition and landmark detection from the raw X-ray images,\nfollowed by a deformable model and landmark-weighted 2D-3D registration\napproach. The reconstruction accuracy was validated by the gold standard\nobtained through the registration of CT-segmented vertebral models with the\nbiplanar X-ray images. The proposed method achieved a 3D reconstruction\naccuracy of 0.80 mm, representing a significant improvement over the mainstream\napproaches. This study will contribute to the clinical diagnosis of lumbar in\nweight-bearing positions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14655", "pdf": "https://arxiv.org/pdf/2503.14655", "abs": "https://arxiv.org/abs/2503.14655", "authors": ["Minheng Chen", "Xiaowei Yu", "Jing Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14892", "pdf": "https://arxiv.org/pdf/2503.14892", "abs": "https://arxiv.org/abs/2503.14892", "authors": ["He Huang", "Yong Chen", "Yujun Guo", "Wei He"], "title": "Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) fusion is an efficient technique that combines\nlow-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)\nto generate high-resolution HSI (HR-HSI). Existing supervised learning methods\n(SLMs) can yield promising results when test data degradation matches the\ntraining ones, but they face challenges in generalizing to unknown\ndegradations. To unleash the potential and generalization ability of SLMs, we\npropose a novel self-supervised unknown-to-known degradation transformation\nframework (U2K) for blind HSI fusion, which adaptively transforms unknown\ndegradation into the same type of degradation as those handled by pre-trained\nSLMs. Specifically, the proposed U2K framework consists of: (1) spatial and\nspectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded\nHR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert\nthese wrapped data into predefined degradation patterns. The transformed HR-MSI\nand LR-HSI pairs are then processed by a pre-trained network to reconstruct the\ntarget HR-HSI. We train the U2K framework in a self-supervised manner using\nconsistency loss and greedy alternating optimization, significantly improving\nthe flexibility of blind HSI fusion. Extensive experiments confirm the\neffectiveness of our proposed U2K framework in boosting the adaptability of\nfive existing SLMs under various degradation settings and surpassing\nstate-of-the-art blind methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15029", "pdf": "https://arxiv.org/pdf/2503.15029", "abs": "https://arxiv.org/abs/2503.15029", "authors": ["Jianbo Zhao", "Taiyu Ban", "Zhihao Liu", "Hangning Zhou", "Xiyang Wang", "Qibin Zhou", "Hailong Qin", "Mu Yang", "Lei Liu", "Bin Li"], "title": "DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15288", "pdf": "https://arxiv.org/pdf/2503.15288", "abs": "https://arxiv.org/abs/2503.15288", "authors": ["Justin Le Louëdec", "Maike Bauer", "Tanja Amerstorfer", "Jackie A. Davies"], "title": "Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking", "categories": ["physics.space-ph", "cs.CV", "cs.LG"], "comment": "24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025", "summary": "Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
