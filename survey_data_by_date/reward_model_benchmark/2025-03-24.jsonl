{"id": "2503.16566", "pdf": "https://arxiv.org/pdf/2503.16566", "abs": "https://arxiv.org/abs/2503.16566", "authors": ["Jie Zhang", "Zheng Yuan", "Zhongqi Wang", "Bei Yan", "Sibo Wang", "Xiangkui Cao", "Zonghui Guo", "Shiguang Shan", "Xilin Chen"], "title": "REVAL: A Comprehension Evaluation on Reliability and Values of Large Vision-Language Models", "categories": ["cs.CV"], "comment": "45 pages, 5 figures, 18 tables", "summary": "The rapid evolution of Large Vision-Language Models (LVLMs) has highlighted\nthe necessity for comprehensive evaluation frameworks that assess these models\nacross diverse dimensions. While existing benchmarks focus on specific aspects\nsuch as perceptual abilities, cognitive capabilities, and safety against\nadversarial attacks, they often lack the breadth and depth required to provide\na holistic understanding of LVLMs' strengths and limitations. To address this\ngap, we introduce REVAL, a comprehensive benchmark designed to evaluate the\n\\textbf{RE}liability and \\textbf{VAL}ue of LVLMs. REVAL encompasses over 144K\nimage-text Visual Question Answering (VQA) samples, structured into two primary\nsections: Reliability, which assesses truthfulness (\\eg, perceptual accuracy\nand hallucination tendencies) and robustness (\\eg, resilience to adversarial\nattacks, typographic attacks, and image corruption), and Values, which\nevaluates ethical concerns (\\eg, bias and moral understanding), safety issues\n(\\eg, toxicity and jailbreak vulnerabilities), and privacy problems (\\eg,\nprivacy awareness and privacy leakage). We evaluate 26 models, including\nmainstream open-source LVLMs and prominent closed-source models like GPT-4o and\nGemini-1.5-Pro. Our findings reveal that while current LVLMs excel in\nperceptual tasks and toxicity avoidance, they exhibit significant\nvulnerabilities in adversarial scenarios, privacy preservation, and ethical\nreasoning. These insights underscore critical areas for future improvements,\nguiding the development of more secure, reliable, and ethically aligned LVLMs.\nREVAL provides a robust framework for researchers to systematically assess and\ncompare LVLMs, fostering advancements in the field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "truthfulness", "safety", "reliability", "accuracy", "question answering"], "score": 7}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17039", "pdf": "https://arxiv.org/pdf/2503.17039", "abs": "https://arxiv.org/abs/2503.17039", "authors": ["Jeremy Barnes", "Naiara Perez", "Alba Bonet-Jover", "Bego√±a Altuna"], "title": "Summarization Metrics for Spanish and Basque: Do Automatic Scores and LLM-Judges Correlate with Humans?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Studies on evaluation metrics and LLM-as-a-Judge models for automatic text\nsummarization have largely been focused on English, limiting our understanding\nof their effectiveness in other languages. Through our new dataset BASSE\n(BAsque and Spanish Summarization Evaluation), we address this situation by\ncollecting human judgments on 2,040 abstractive summaries in Basque and\nSpanish, generated either manually or by five LLMs with four different prompts.\nFor each summary, annotators evaluated five criteria on a 5-point Likert scale:\ncoherence, consistency, fluency, relevance, and 5W1H. We use these data to\nreevaluate traditional automatic metrics used for evaluating summaries, as well\nas several LLM-as-a-Judge models that show strong performance on this task in\nEnglish. Our results show that currently proprietary judge LLMs have the\nhighest correlation with human judgments, followed by criteria-specific\nautomatic metrics, while open-sourced judge LLMs perform poorly. We release\nBASSE and our code publicly, along with the first large-scale Basque\nsummarization dataset containing 22,525 news articles with their subheads.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "consistency", "summarization", "criteria"], "score": 6}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16541", "pdf": "https://arxiv.org/pdf/2503.16541", "abs": "https://arxiv.org/abs/2503.16541", "authors": ["Hanzhi Zhang", "Sumera Anjum", "Heng Fan", "Weijian Zheng", "Yan Huang", "Yunhe Feng"], "title": "Poly-FEVER: A Multilingual Fact Verification Benchmark for Hallucination Detection in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations in generative AI, particularly in Large Language Models\n(LLMs), pose a significant challenge to the reliability of multilingual\napplications. Existing benchmarks for hallucination detection focus primarily\non English and a few widely spoken languages, lacking the breadth to assess\ninconsistencies in model performance across diverse linguistic contexts. To\naddress this gap, we introduce Poly-FEVER, a large-scale multilingual fact\nverification benchmark specifically designed for evaluating hallucination\ndetection in LLMs. Poly-FEVER comprises 77,973 labeled factual claims spanning\n11 languages, sourced from FEVER, Climate-FEVER, and SciFact. It provides the\nfirst large-scale dataset tailored for analyzing hallucination patterns across\nlanguages, enabling systematic evaluation of LLMs such as ChatGPT and the LLaMA\nseries. Our analysis reveals how topic distribution and web resource\navailability influence hallucination frequency, uncovering language-specific\nbiases that impact model accuracy. By offering a multilingual benchmark for\nfact verification, Poly-FEVER facilitates cross-linguistic comparisons of\nhallucination detection and contributes to the development of more reliable,\nlanguage-inclusive AI systems. The dataset is publicly available to advance\nresearch in responsible AI, fact-checking methodologies, and multilingual NLP,\npromoting greater transparency and robustness in LLM performance. The proposed\nPoly-FEVER is available at:\nhttps://huggingface.co/datasets/HanzhiZhang/Poly-FEVER.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16867", "pdf": "https://arxiv.org/pdf/2503.16867", "abs": "https://arxiv.org/abs/2503.16867", "authors": ["Kaisi Guan", "Zhengfeng Lai", "Yuchong Sun", "Peng Zhang", "Wei Liu", "Kieran Liu", "Meng Cao", "Ruihua Song"], "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering", "categories": ["cs.CV"], "comment": null, "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17136", "pdf": "https://arxiv.org/pdf/2503.17136", "abs": "https://arxiv.org/abs/2503.17136", "authors": ["Brihi Joshi", "Sriram Venkatapathy", "Mohit Bansal", "Nanyun Peng", "Haw-Shiuan Chang"], "title": "CoKe: Customizable Fine-Grained Story Evaluation via Chain-of-Keyword Rationalization", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating creative text such as human-written stories using language models\nhas always been a challenging task -- owing to the subjectivity of\nmulti-annotator ratings. To mimic the thinking process of humans, chain of\nthought (CoT) generates free-text explanations that help guide a model's\npredictions and Self-Consistency (SC) marginalizes predictions over multiple\ngenerated explanations. In this study, we discover that the widely-used\nself-consistency reasoning methods cause suboptimal results due to an objective\nmismatch between generating 'fluent-looking' explanations vs. actually leading\nto a good rating prediction for an aspect of a story. To overcome this\nchallenge, we propose $\\textbf{C}$hain-$\\textbf{o}$f-$\\textbf{Ke}$ywords\n(CoKe), that generates a sequence of keywords $\\textit{before}$ generating a\nfree-text rationale, that guide the rating prediction of our evaluation\nlanguage model. Then, we generate a diverse set of such keywords, and aggregate\nthe scores corresponding to these generations. On the StoryER dataset, CoKe\nbased on our small fine-tuned evaluation models not only reach human-level\nperformance and significantly outperform GPT-4 with a 2x boost in correlation\nwith human annotators, but also requires drastically less number of parameters.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "consistency", "fine-grained"], "score": 5}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17229", "pdf": "https://arxiv.org/pdf/2503.17229", "abs": "https://arxiv.org/abs/2503.17229", "authors": ["Albert Sawczyn", "Jakub Binkowski", "Denis Janiak", "Bogdan Gabrys", "Tomasz Kajdanowicz"], "title": "FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) frequently generate hallucinated content, posing\nsignificant challenges for applications where factuality is crucial. While\nexisting hallucination detection methods typically operate at the sentence\nlevel or passage level, we propose FactSelfCheck, a novel black-box\nsampling-based method that enables fine-grained fact-level detection. Our\napproach represents text as knowledge graphs consisting of facts in the form of\ntriples. Through analyzing factual consistency across multiple LLM responses,\nwe compute fine-grained hallucination scores without requiring external\nresources or training data. Our evaluation demonstrates that FactSelfCheck\nperforms competitively with leading sampling-based methods while providing more\ndetailed insights. Most notably, our fact-level approach significantly improves\nhallucination correction, achieving a 35% increase in factual content compared\nto the baseline, while sentence-level SelfCheckGPT yields only an 8%\nimprovement. The granular nature of our detection enables more precise\nidentification and correction of hallucinated content.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "factuality", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17350", "pdf": "https://arxiv.org/pdf/2503.17350", "abs": "https://arxiv.org/abs/2503.17350", "authors": ["Qingyu Shi", "Jianzong Wu", "Jinbin Bai", "Jiangning Zhang", "Lu Qi", "Xiangtai Li", "Yunhai Tong"], "title": "Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer", "categories": ["cs.CV"], "comment": null, "summary": "The motion transfer task involves transferring motion from a source video to\nnewly generated videos, requiring the model to decouple motion from appearance.\nPrevious diffusion-based methods primarily rely on separate spatial and\ntemporal attention mechanisms within 3D U-Net. In contrast, state-of-the-art\nvideo Diffusion Transformers (DiT) models use 3D full attention, which does not\nexplicitly separate temporal and spatial information. Thus, the interaction\nbetween spatial and temporal dimensions makes decoupling motion and appearance\nmore challenging for DiT models. In this paper, we propose DeT, a method that\nadapts DiT models to improve motion transfer ability. Our approach introduces a\nsimple yet effective temporal kernel to smooth DiT features along the temporal\ndimension, facilitating the decoupling of foreground motion from background\nappearance. Meanwhile, the temporal kernel effectively captures temporal\nvariations in DiT features, which are closely related to motion. Moreover, we\nintroduce explicit supervision along dense trajectories in the latent feature\nspace to further enhance motion consistency. Additionally, we present MTBench,\na general and challenging benchmark for motion transfer. We also introduce a\nhybrid motion fidelity metric that considers both the global and local motion\nsimilarity. Therefore, our work provides a more comprehensive evaluation than\nprevious works. Extensive experiments on MTBench demonstrate that DeT achieves\nthe best trade-off between motion fidelity and edit fidelity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "dimension"], "score": 4}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16529", "pdf": "https://arxiv.org/pdf/2503.16529", "abs": "https://arxiv.org/abs/2503.16529", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Limin Han", "Jiaojiao Zhao", "Beibei Huang", "Zhenhong Long", "Junting Guo", "Meijuan An", "Rongjia Du", "Ning Wang", "Kai Wang", "Shiguo Lian"], "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "21 pages,13 figures", "summary": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for six distilled models. Evaluation results indicate that\nthe enhanced models achieve significant improvements in safety while\nmaintaining reasoning capabilities without notable degradation. We open-source\nthe safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Distill-Safe/tree/main to serve as a\nvaluable resource for future research and optimization of DeepSeek models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16578", "pdf": "https://arxiv.org/pdf/2503.16578", "abs": "https://arxiv.org/abs/2503.16578", "authors": ["Yang Chen", "Hui Wang", "Shiyao Wang", "Junyang Chen", "Jiabei He", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While voice technologies increasingly serve aging populations, current\nsystems exhibit significant performance gaps due to inadequate training data\ncapturing elderly-specific vocal characteristics like presbyphonia and\ndialectal variations. The limited data available on super-aged individuals in\nexisting elderly speech datasets, coupled with overly simple recording styles\nand annotation dimensions, exacerbates this issue. To address the critical\nscarcity of speech data from individuals aged 75 and above, we introduce\nSeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset\ncontains 55.53 hours of speech from 101 natural conversations involving 202\nparticipants, ensuring a strategic balance across gender, region, and age.\nThrough detailed annotation across multiple dimensions, it can support a wide\nrange of speech tasks. We perform extensive experiments on speaker\nverification, speaker diarization, speech recognition, and speech editing\ntasks, offering crucial insights for the development of speech technologies\ntargeting this age group.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "dialogue"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16742", "pdf": "https://arxiv.org/pdf/2503.16742", "abs": "https://arxiv.org/abs/2503.16742", "authors": ["Esther Y. H. Lin", "Yimin Ding", "Jogendra Kundu", "Yatong An", "Mohamed T. El-Haddad", "Alexander Fix"], "title": "Digitally Prototype Your Eye Tracker: Simulating Hardware Performance using 3D Synthetic Data", "categories": ["cs.CV"], "comment": "14 pages, 12 figures", "summary": "Eye tracking (ET) is a key enabler for Augmented and Virtual Reality (AR/VR).\nPrototyping new ET hardware requires assessing the impact of hardware choices\non eye tracking performance. This task is compounded by the high cost of\nobtaining data from sufficiently many variations of real hardware, especially\nfor machine learning, which requires large training datasets. We propose a\nmethod for end-to-end evaluation of how hardware changes impact machine\nlearning-based ET performance using only synthetic data. We utilize a dataset\nof real 3D eyes, reconstructed from light dome data using neural radiance\nfields (NeRF), to synthesize captured eyes from novel viewpoints and camera\nparameters. Using this framework, we demonstrate that we can predict the\nrelative performance across various hardware configurations, accounting for\nvariations in sensor noise, illumination brightness, and optical blur. We also\ncompare our simulator with the publicly available eye tracking dataset from the\nProject Aria glasses, demonstrating a strong correlation with real-world\nperformance. Finally, we present a first-of-its-kind analysis in which we vary\nET camera positions, evaluating ET performance ranging from on-axis direct\nviews of the eye to peripheral views on the frame. Such an analysis would have\npreviously required manufacturing physical devices to capture evaluation data.\nIn short, our method enables faster prototyping of ET hardware.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16581", "pdf": "https://arxiv.org/pdf/2503.16581", "abs": "https://arxiv.org/abs/2503.16581", "authors": ["Zahra Khalila", "Arbi Haza Nasution", "Winda Monika", "Aytug Onan", "Yohei Murakami", "Yasir Bin Ismail Radi", "Noor Mohammad Osmani"], "title": "Investigating Retrieval-Augmented Generation in Quranic Studies: A Study of 13 Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "11 pages, keywords: Large-language-models; retrieval-augmented\n  generation; question answering; Quranic studies; Islamic teachings", "summary": "Accurate and contextually faithful responses are critical when applying large\nlanguage models (LLMs) to sensitive and domain-specific tasks, such as\nanswering queries related to quranic studies. General-purpose LLMs often\nstruggle with hallucinations, where generated responses deviate from\nauthoritative sources, raising concerns about their reliability in religious\ncontexts. This challenge highlights the need for systems that can integrate\ndomain-specific knowledge while maintaining response accuracy, relevance, and\nfaithfulness. In this study, we investigate 13 open-source LLMs categorized\ninto large (e.g., Llama3:70b, Gemma2:27b, QwQ:32b), medium (e.g., Gemma2:9b,\nLlama3:8b), and small (e.g., Llama3.2:3b, Phi3:3.8b). A Retrieval-Augmented\nGeneration (RAG) is used to make up for the problems that come with using\nseparate models. This research utilizes a descriptive dataset of Quranic surahs\nincluding the meanings, historical context, and qualities of the 114 surahs,\nallowing the model to gather relevant knowledge before responding. The models\nare evaluated using three key metrics set by human evaluators: context\nrelevance, answer faithfulness, and answer relevance. The findings reveal that\nlarge models consistently outperform smaller models in capturing query\nsemantics and producing accurate, contextually grounded responses. The\nLlama3.2:3b model, even though it is considered small, does very well on\nfaithfulness (4.619) and relevance (4.857), showing the promise of smaller\narchitectures that have been well optimized. This article examines the\ntrade-offs between model size, computational efficiency, and response quality\nwhile using LLMs in domain-specific applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16614", "pdf": "https://arxiv.org/pdf/2503.16614", "abs": "https://arxiv.org/abs/2503.16614", "authors": ["Maria de Lourdes M. Silva", "Andr√© L. C. Mendon√ßa", "Eduardo R. D. Neto", "Iago C. Chaves", "Felipe T. Brito", "Victor A. E. Farias", "Javam C. Machado"], "title": "Classification of User Reports for Detection of Faulty Computer Components using NLP Models: A Case Study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 2 figures", "summary": "Computer manufacturers typically offer platforms for users to report faults.\nHowever, there remains a significant gap in these platforms' ability to\neffectively utilize textual reports, which impedes users from describing their\nissues in their own words. In this context, Natural Language Processing (NLP)\noffers a promising solution, by enabling the analysis of user-generated text.\nThis paper presents an innovative approach that employs NLP models to classify\nuser reports for detecting faulty computer components, such as CPU, memory,\nmotherboard, video card, and more. In this work, we build a dataset of 341 user\nreports obtained from many sources. Additionally, through extensive\nexperimental evaluation, our approach achieved an accuracy of 79% with our\ndataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16826", "pdf": "https://arxiv.org/pdf/2503.16826", "abs": "https://arxiv.org/abs/2503.16826", "authors": ["Jun Seong Kim", "Kyaw Ye Thu", "Javad Ismayilzada", "Junyeong Park", "Eunsu Kim", "Huzama Ahmad", "Na Min An", "James Thorne", "Alice Oh"], "title": "When Tom Eats Kimchi: Evaluating Cultural Bias of Multimodal Large Language Models in Cultural Mixture Contexts", "categories": ["cs.CL"], "comment": "12 pages", "summary": "In a highly globalized world, it is important for multi-modal large language\nmodels (MLLMs) to recognize and respond correctly to mixed-cultural inputs. For\nexample, a model should correctly identify kimchi (Korean food) in an image\nboth when an Asian woman is eating it, as well as an African man is eating it.\nHowever, current MLLMs show an over-reliance on the visual features of the\nperson, leading to misclassification of the entities. To examine the robustness\nof MLLMs to different ethnicity, we introduce MixCuBe, a cross-cultural bias\nbenchmark, and study elements from five countries and four ethnicities. Our\nfindings reveal that MLLMs achieve both higher accuracy and lower sensitivity\nto such perturbation for high-resource cultures, but not for low-resource\ncultures. GPT-4o, the best-performing model overall, shows up to 58% difference\nin accuracy between the original and perturbed cultural settings in\nlow-resource cultures. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/kyawyethu/MixCuBe.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16858", "pdf": "https://arxiv.org/pdf/2503.16858", "abs": "https://arxiv.org/abs/2503.16858", "authors": ["Jialin Chen", "Aosong Feng", "Ziyu Zhao", "Juan Garza", "Gaukhar Nurbek", "Cheng Qin", "Ali Maatouk", "Leandros Tassiulas", "Yifeng Gao", "Rex Ying"], "title": "MTBench: A Multimodal Time Series Benchmark for Temporal Reasoning and Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "Understanding the relationship between textual news and time-series evolution\nis a critical yet under-explored challenge in applied data science. While\nmultimodal learning has gained traction, existing multimodal time-series\ndatasets fall short in evaluating cross-modal reasoning and complex question\nanswering, which are essential for capturing complex interactions between\nnarrative information and temporal patterns. To bridge this gap, we introduce\nMultimodal Time Series Benchmark (MTBench), a large-scale benchmark designed to\nevaluate large language models (LLMs) on time series and text understanding\nacross financial and weather domains. MTbench comprises paired time series and\ntextual data, including financial news with corresponding stock price movements\nand weather reports aligned with historical temperature records. Unlike\nexisting benchmarks that focus on isolated modalities, MTbench provides a\ncomprehensive testbed for models to jointly reason over structured numerical\ntrends and unstructured textual narratives. The richness of MTbench enables\nformulation of diverse tasks that require a deep understanding of both text and\ntime-series data, including time-series forecasting, semantic and technical\ntrend analysis, and news-driven question answering (QA). These tasks target the\nmodel's ability to capture temporal dependencies, extract key insights from\ntextual context, and integrate cross-modal information. We evaluate\nstate-of-the-art LLMs on MTbench, analyzing their effectiveness in modeling the\ncomplex relationships between news narratives and temporal patterns. Our\nfindings reveal significant challenges in current models, including\ndifficulties in capturing long-term dependencies, interpreting causality in\nfinancial and weather trends, and effectively fusing multimodal information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed", "question answering"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16910", "pdf": "https://arxiv.org/pdf/2503.16910", "abs": "https://arxiv.org/abs/2503.16910", "authors": ["Yu Qiu", "Yuhang Sun", "Jie Mei", "Lin Xiao", "Jing Xu"], "title": "Salient Object Detection in Traffic Scene through the TSOD10K Dataset", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Traffic Salient Object Detection (TSOD) aims to segment the objects critical\nto driving safety by combining semantic (e.g., collision risks) and visual\nsaliency. Unlike SOD in natural scene images (NSI-SOD), which prioritizes\nvisually distinctive regions, TSOD emphasizes the objects that demand immediate\ndriver attention due to their semantic impact, even with low visual contrast.\nThis dual criterion, i.e., bridging perception and contextual risk, re-defines\nsaliency for autonomous and assisted driving systems. To address the lack of\ntask-specific benchmarks, we collect the first large-scale TSOD dataset with\npixel-wise saliency annotations, named TSOD10K. TSOD10K covers the diverse\nobject categories in various real-world traffic scenes under various\nchallenging weather/illumination variations (e.g., fog, snowstorms,\nlow-contrast, and low-light). Methodologically, we propose a Mamba-based TSOD\nmodel, termed Tramba. Considering the challenge of distinguishing inconspicuous\nvisual information from complex traffic backgrounds, Tramba introduces a novel\nDual-Frequency Visual State Space module equipped with shifted window\npartitioning and dilated scanning to enhance the perception of fine details and\nglobal structure by hierarchically decomposing high/low-frequency components.\nTo emphasize critical regions in traffic scenes, we propose a traffic-oriented\nHelix 2D-Selective-Scan (Helix-SS2D) mechanism that injects driving attention\npriors while effectively capturing global multi-direction spatial dependencies.\nWe establish a comprehensive benchmark by evaluating Tramba and 22 existing\nNSI-SOD models on TSOD10K, demonstrating Tramba's superiority. Our research\nestablishes the first foundation for safety-aware saliency analysis in\nintelligent transportation systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16945", "pdf": "https://arxiv.org/pdf/2503.16945", "abs": "https://arxiv.org/abs/2503.16945", "authors": ["Ibtissam Saadi", "Abdenour Hadid", "Douglas W. Cunningham", "Abdelmalik Taleb-Ahmed", "Yassin El Hillali"], "title": "PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16480", "pdf": "https://arxiv.org/pdf/2503.16480", "abs": "https://arxiv.org/abs/2503.16480", "authors": ["Yara Kyrychenko", "Jon Roozenbeek", "Brandon Davidson", "Sander van der Linden", "Ramit Debnath"], "title": "Human Preferences for Constructive Interactions in Language Model Alignment", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "1 Figure, 1 Table, 11 pages", "summary": "As large language models (LLMs) enter the mainstream, aligning them to foster\nconstructive dialogue rather than exacerbate societal divisions is critical.\nUsing an individualized and multicultural alignment dataset of over 7,500\nconversations of individuals from 74 countries engaging with 21 LLMs, we\nexamined how linguistic attributes linked to constructive interactions are\nreflected in human preference data used for training AI. We found that users\nconsistently preferred well-reasoned and nuanced responses while rejecting\nthose high in personal storytelling. However, users who believed that AI should\nreflect their values tended to place less preference on reasoning in LLM\nresponses and more on curiosity. Encouragingly, we observed that users could\nset the tone for how constructive their conversation would be, as LLMs mirrored\nlinguistic attributes, including toxicity, in user queries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16976", "pdf": "https://arxiv.org/pdf/2503.16976", "abs": "https://arxiv.org/abs/2503.16976", "authors": ["Weihao Yu", "Xiaoqing Guo", "Chenxin Li", "Yifan Liu", "Yixuan Yuan"], "title": "GeoT: Geometry-guided Instance-dependent Transition Matrix for Semi-supervised Tooth Point Cloud Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "IPMI2025", "summary": "Achieving meticulous segmentation of tooth point clouds from intra-oral scans\nstands as an indispensable prerequisite for various orthodontic applications.\nGiven the labor-intensive nature of dental annotation, a significant amount of\ndata remains unlabeled, driving increasing interest in semi-supervised\napproaches. One primary challenge of existing semi-supervised medical\nsegmentation methods lies in noisy pseudo labels generated for unlabeled data.\nTo address this challenge, we propose GeoT, the first framework that employs\ninstance-dependent transition matrix (IDTM) to explicitly model noise in pseudo\nlabels for semi-supervised dental segmentation. Specifically, to handle the\nextensive solution space of IDTM arising from tens of thousands of dental\npoints, we introduce tooth geometric priors through two key components:\npoint-level geometric regularization (PLGR) to enhance consistency between\npoint adjacency relationships in 3D and IDTM spaces, and class-level geometric\nsmoothing (CLGS) to leverage the fixed spatial distribution of tooth categories\nfor optimal IDTM estimation. Extensive experiments performed on the public\nTeeth3DS dataset and private dataset demonstrate that our method can make full\nutilization of unlabeled data to facilitate segmentation, achieving performance\ncomparable to fully supervised methods with only $20\\%$ of the labeled data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17044", "pdf": "https://arxiv.org/pdf/2503.17044", "abs": "https://arxiv.org/abs/2503.17044", "authors": ["Chandan Yeshwanth", "David Rozenberszki", "Angela Dai"], "title": "ExCap3D: Expressive 3D Scene Understanding via Object Captioning with Varying Detail", "categories": ["cs.CV"], "comment": "Project page: https://cy94.github.io/excap3d/, Video:\n  https://www.youtube.com/watch?v=SQRV1l_0oY0", "summary": "Generating text descriptions of objects in 3D indoor scenes is an important\nbuilding block of embodied understanding. Existing methods do this by\ndescribing objects at a single level of detail, which often does not capture\nfine-grained details such as varying textures, materials, and shapes of the\nparts of objects. We propose the task of expressive 3D captioning: given an\ninput 3D scene, describe objects at multiple levels of detail: a high-level\nobject description, and a low-level description of the properties of its parts.\nTo produce such captions, we present ExCap3D, an expressive 3D captioning model\nwhich takes as input a 3D scan, and for each detected object in the scan,\ngenerates a fine-grained collective description of the parts of the object,\nalong with an object-level description conditioned on the part-level\ndescription. We design ExCap3D to encourage semantic consistency between the\ngenerated text descriptions, as well as textual similarity in the latent space,\nto further increase the quality of the generated captions. To enable this task,\nwe generated the ExCap3D Dataset by leveraging a visual-language model (VLM)\nfor multi-view captioning. The ExCap3D Dataset contains captions on the\nScanNet++ dataset with varying levels of detail, comprising 190k text\ndescriptions of 34k 3D objects in 947 indoor scenes. Our experiments show that\nthe object- and part-level of detail captions generated by ExCap3D are of\nhigher quality than those produced by state-of-the-art methods, with a Cider\nscore improvement of 17% and 124% for object- and part-level details\nrespectively. Our code, dataset and models will be made publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16851", "pdf": "https://arxiv.org/pdf/2503.16851", "abs": "https://arxiv.org/abs/2503.16851", "authors": ["Zeqing He", "Zhibo Wang", "Huiyu Xu", "Kui Ren"], "title": "Towards LLM Guardrails via Sparse Representation Steering", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nnatural language generation tasks, yet their uncontrolled outputs pose\nsignificant ethical and safety risks. Recently, representation engineering\nmethods have shown promising results in steering model behavior by modifying\nthe rich semantic information encoded in activation vectors. However, due to\nthe difficulty of precisely disentangling semantic directions within\nhigh-dimensional representation space, existing approaches suffer from three\nmajor limitations: lack of fine-grained control, quality degradation of\ngenerated content, and poor interpretability. To address these challenges, we\npropose a sparse encoding-based representation engineering method, named SRE,\nwhich decomposes polysemantic activations into a structured, monosemantic\nfeature space. By leveraging sparse autoencoding, our approach isolates and\nadjusts only task-specific sparse feature dimensions, enabling precise and\ninterpretable steering of model behavior while preserving content quality. We\nvalidate our method on three critical domains, i.e., safety, fairness, and\ntruthfulness using the open-source LLM Gemma-2-2B-it. Experimental results show\nthat SRE achieves superior controllability while maintaining the overall\nquality of generated content (i.e., controllability and quality), demonstrating\nits effectiveness as a fine-grained and interpretable activation steering\nframework.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["truthfulness", "safety", "fine-grained"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17071", "pdf": "https://arxiv.org/pdf/2503.17071", "abs": "https://arxiv.org/abs/2503.17071", "authors": ["Pablo Garcia-Fernandez", "Lorenzo Vaquero", "Mingxuan Liu", "Feng Xue", "Daniel Cores", "Nicu Sebe", "Manuel Mucientes", "Elisa Ricci"], "title": "Superpowering Open-Vocabulary Object Detectors for X-ray Vision", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detection (OvOD) is set to revolutionize security\nscreening by enabling systems to recognize any item in X-ray scans. However,\ndeveloping effective OvOD models for X-ray imaging presents unique challenges\ndue to data scarcity and the modality gap that prevents direct adoption of\nRGB-based solutions. To overcome these limitations, we propose RAXO, a\ntraining-free framework that repurposes off-the-shelf RGB OvOD detectors for\nrobust X-ray detection. RAXO builds high-quality X-ray class descriptors using\na dual-source retrieval strategy. It gathers relevant RGB images from the web\nand enriches them via a novel X-ray material transfer mechanism, eliminating\nthe need for labeled databases. These visual descriptors replace text-based\nclassification in OvOD, leveraging intra-modal feature distances for robust\ndetection. Extensive experiments demonstrate that RAXO consistently improves\nOvOD performance, providing an average mAP increase of up to 17.0 points over\nbase detectors. To further support research in this emerging field, we also\nintroduce DET-COMPASS, a new benchmark featuring bounding box annotations for\nover 300 object categories, enabling large-scale evaluation of OvOD in X-ray.\nCode and dataset available at: https://github.com/PAGF188/RAXO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16974", "pdf": "https://arxiv.org/pdf/2503.16974", "abs": "https://arxiv.org/abs/2503.16974", "authors": ["Julian Junyan Wang", "Victor Xiaoqi Wang"], "title": "Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks", "categories": ["q-fin.GN", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": "96 pages", "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. Simulation analysis reveals that despite measurable\ninconsistency in LLM outputs, downstream statistical inferences remain\nremarkably robust. These findings address concerns about what we term\n\"G-hacking,\" the selective reporting of favorable outcomes from multiple\nGenerative AI runs, by demonstrating that such risks are relatively low for\nfinance and accounting tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "consistency", "summarization"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17122", "pdf": "https://arxiv.org/pdf/2503.17122", "abs": "https://arxiv.org/abs/2503.17122", "authors": ["Jonas Mirlach", "Lei Wan", "Andreas Wiedholz", "Hannan Ejaz Keen", "Andreas Eich"], "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception", "categories": ["cs.CV"], "comment": "10 pages, 7 figures, submitted to ICCV2025", "summary": "In autonomous driving, the integration of roadside perception systems is\nessential for overcoming occlusion challenges and enhancing the safety of\nVulnerable Road Users (VRUs). While LiDAR and visual (RGB) sensors are commonly\nused, thermal imaging remains underrepresented in datasets, despite its\nacknowledged advantages for VRU detection in extreme lighting conditions. In\nthis paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and\nthermal imaging from a roadside perspective, with a strong focus on VRUs.\nR-LiViT captures three intersections during both day and night, ensuring a\ndiverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and\nspatially aligned RGB and thermal images across over 150 traffic scenarios,\nwith 6 and 8 annotated classes respectively, providing a comprehensive resource\nfor tasks such as object detection and tracking. The dataset1 and the code for\nreproducing our evaluation results2 are made publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17226", "pdf": "https://arxiv.org/pdf/2503.17226", "abs": "https://arxiv.org/abs/2503.17226", "authors": ["Aryan Yazdan Parast", "Basim Azam", "Naveed Akhtar"], "title": "Leveraging Text-to-Image Generation for Handling Spurious Correlation", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks trained with Empirical Risk Minimization (ERM) perform\nwell when both training and test data come from the same domain, but they often\nfail to generalize to out-of-distribution samples. In image classification,\nthese models may rely on spurious correlations that often exist between labels\nand irrelevant features of images, making predictions unreliable when those\nfeatures do not exist. We propose a technique to generate training samples with\ntext-to-image (T2I) diffusion models for addressing the spurious correlation\nproblem. First, we compute the best describing token for the visual features\npertaining to the causal components of samples by a textual inversion\nmechanism. Then, leveraging a language segmentation method and a diffusion\nmodel, we generate new samples by combining the causal component with the\nelements from other classes. We also meticulously prune the generated samples\nbased on the prediction probabilities and attribution scores of the ERM model\nto ensure their correct composition for our objective. Finally, we retrain the\nERM model on our augmented dataset. This process reduces the model's reliance\non spurious correlations by learning from carefully crafted samples for in\nwhich this correlation does not exist. Our experiments show that across\ndifferent benchmarks, our technique achieves better worst-group accuracy than\nthe existing state-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16556", "pdf": "https://arxiv.org/pdf/2503.16556", "abs": "https://arxiv.org/abs/2503.16556", "authors": ["Sabeen Ahmed", "Nathan Parker", "Margaret Park", "Daniel Jeong", "Lauren Peres", "Evan W. Davis", "Jennifer B. Permuth", "Erin Siegel", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "Reliable Radiologic Skeletal Muscle Area Assessment -- A Biomarker for Cancer Cachexia Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV"], "comment": "47 pages, 19 figures, 9 Tables", "summary": "Cancer cachexia is a common metabolic disorder characterized by severe muscle\natrophy which is associated with poor prognosis and quality of life. Monitoring\nskeletal muscle area (SMA) longitudinally through computed tomography (CT)\nscans, an imaging modality routinely acquired in cancer care, is an effective\nway to identify and track this condition. However, existing tools often lack\nfull automation and exhibit inconsistent accuracy, limiting their potential for\nintegration into clinical workflows. To address these challenges, we developed\nSMAART-AI (Skeletal Muscle Assessment-Automated and Reliable Tool-based on AI),\nan end-to-end automated pipeline powered by deep learning models (nnU-Net 2D)\ntrained on mid-third lumbar level CT images with 5-fold cross-validation,\nensuring generalizability and robustness. SMAART-AI incorporates an\nuncertainty-based mechanism to flag high-error SMA predictions for expert\nreview, enhancing reliability. We combined the SMA, skeletal muscle index, BMI,\nand clinical data to train a multi-layer perceptron (MLP) model designed to\npredict cachexia at the time of cancer diagnosis. Tested on the\ngastroesophageal cancer dataset, SMAART-AI achieved a Dice score of 97.80% +/-\n0.93%, with SMA estimated across all four datasets in this study at a median\nabsolute error of 2.48% compared to manual annotations with SliceOmatic.\nUncertainty metrics-variance, entropy, and coefficient of variation-strongly\ncorrelated with SMA prediction errors (0.83, 0.76, and 0.73 respectively). The\nMLP model predicts cachexia with 79% precision, providing clinicians with a\nreliable tool for early diagnosis and intervention. By combining automation,\naccuracy, and uncertainty awareness, SMAART-AI bridges the gap between research\nand clinical application, offering a transformative approach to managing cancer\ncachexia.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16630", "pdf": "https://arxiv.org/pdf/2503.16630", "abs": "https://arxiv.org/abs/2503.16630", "authors": ["Dana Cohen-Bar", "Daniel Cohen-Or", "Gal Chechik", "Yoni Kasten"], "title": "TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://danacohen95.github.io/TriTex/", "summary": "As 3D content creation continues to grow, transferring semantic textures\nbetween 3D meshes remains a significant challenge in computer graphics. While\nrecent methods leverage text-to-image diffusion models for texturing, they\noften struggle to preserve the appearance of the source texture during texture\ntransfer. We present \\ourmethod, a novel approach that learns a volumetric\ntexture field from a single textured mesh by mapping semantic features to\nsurface colors. Using an efficient triplane-based architecture, our method\nenables semantic-aware texture transfer to a novel target mesh. Despite\ntraining on just one example, it generalizes effectively to diverse shapes\nwithin the same category. Extensive evaluation on our newly created benchmark\ndataset shows that \\ourmethod{} achieves superior texture transfer quality and\nfast inference times compared to existing methods. Our approach advances\nsingle-example texture transfer, providing a practical solution for maintaining\nvisual coherence across related 3D models in applications like game development\nand simulation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16513", "pdf": "https://arxiv.org/pdf/2503.16513", "abs": "https://arxiv.org/abs/2503.16513", "authors": ["Nadia Saeed"], "title": "Medifact at PerAnsSumm 2025: Leveraging Lightweight Models for Perspective-Specific Summarization of Clinical Q&A Forums", "categories": ["cs.CL", "cs.AI"], "comment": "This paper accepted in PerAnsSumm: Perspective-aware Healthcare\n  answer summarization, a shared task organized at the CL4Health workshop\n  colocated with NAACL 2025", "summary": "The PerAnsSumm 2025 challenge focuses on perspective-aware healthcare answer\nsummarization (Agarwal et al., 2025). This work proposes a few-shot learning\nframework using a Snorkel-BART-SVM pipeline for classifying and summarizing\nopen-ended healthcare community question-answering (CQA). An SVM model is\ntrained with weak supervision via Snorkel, enhancing zero-shot learning.\nExtractive classification identifies perspective-relevant sentences, which are\nthen summarized using a pretrained BART-CNN model. The approach achieved 12th\nplace among 100 teams in the shared task, demonstrating computational\nefficiency and contextual accuracy. By leveraging pretrained summarization\nmodels, this work advances medical CQA research and contributes to clinical\ndecision support systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16515", "pdf": "https://arxiv.org/pdf/2503.16515", "abs": "https://arxiv.org/abs/2503.16515", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) were used to assist four Commonwealth Scientific\nand Industrial Research Organisation (CSIRO) researchers to perform systematic\nliterature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in\nthese case studies. In each, we explore the impact of changing parameters on\nthe accuracy of LLM responses. The LLM was tasked with extracting evidence from\nchosen academic papers to answer specific research questions. We evaluate the\nmodels' performance in faithfully reproducing quotes from the literature and\nsubject experts were asked to assess the model performance in answering the\nresearch questions. We developed a semantic text highlighting tool to\nfacilitate expert review of LLM responses.\n  We found that state of the art LLMs were able to reproduce quotes from texts\nwith greater than 95% accuracy and answer research questions with an accuracy\nof approximately 83%. We use two methods to determine the correctness of LLM\nresponses; expert review and the cosine similarity of transformer embeddings of\nLLM and expert answers. The correlation between these methods ranged from 0.48\nto 0.77, providing evidence that the latter is a valid metric for measuring\nsemantic similarity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16520", "pdf": "https://arxiv.org/pdf/2503.16520", "abs": "https://arxiv.org/abs/2503.16520", "authors": ["Ji-Eun Han", "Yoonseok Heo"], "title": "Not All Personas Are Worth It: Culture-Reflective Persona Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Incorporating personas into conversational AI models is crucial for achieving\nauthentic and engaging interactions. However, the cultural diversity and\nadaptability of existing persona datasets is often overlooked, reducing their\nefficacy in building culturally aware AI systems. To address this issue, we\npropose a two-step pipeline for generating culture-specific personas and\nintroduce KoPersona, a dataset comprising 200,000 personas designed to capture\nKorean cultural values, behaviors, and social nuances. A comprehensive\nevaluation through various metrics validates the quality of KoPersona and its\nrelevance to Korean culture. This work not only contributes to persona-based\nresearch, but also establishes a scalable approach for creating culturally\nrelevant personas adaptable to various languages and cultural contexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16527", "pdf": "https://arxiv.org/pdf/2503.16527", "abs": "https://arxiv.org/abs/2503.16527", "authors": ["Ang Li", "Haozhe Chen", "Hongseok Namkoong", "Tianyi Peng"], "title": "LLM Generated Persona is a Promise with a Catch", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": null, "summary": "The use of large language models (LLMs) to simulate human behavior has gained\nsignificant attention, particularly through personas that approximate\nindividual characteristics. Persona-based simulations hold promise for\ntransforming disciplines that rely on population-level feedback, including\nsocial science, economic analysis, marketing research, and business operations.\nTraditional methods to collect realistic persona data face significant\nchallenges. They are prohibitively expensive and logistically challenging due\nto privacy constraints, and often fail to capture multi-dimensional attributes,\nparticularly subjective qualities. Consequently, synthetic persona generation\nwith LLMs offers a scalable, cost-effective alternative. However, current\napproaches rely on ad hoc and heuristic generation techniques that do not\nguarantee methodological rigor or simulation precision, resulting in systematic\nbiases in downstream tasks. Through extensive large-scale experiments including\npresidential election forecasts and general opinion surveys of the U.S.\npopulation, we reveal that these biases can lead to significant deviations from\nreal-world outcomes. Our findings underscore the need to develop a rigorous\nscience of persona generation and outline the methodological innovations,\norganizational and institutional support, and empirical foundations required to\nenhance the reliability and scalability of LLM-driven persona simulations. To\nsupport further research and development in this area, we have open-sourced\napproximately one million generated personas, available for public access and\nanalysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "multi-dimensional"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16528", "pdf": "https://arxiv.org/pdf/2503.16528", "abs": "https://arxiv.org/abs/2503.16528", "authors": ["Heng Ping", "Shixuan Li", "Peiyu Zhang", "Anzhe Cheng", "Shukai Duan", "Nikos Kanakaris", "Xiongye Xiao", "Wei Yang", "Shahin Nazarian", "Andrei Irimia", "Paul Bogdan"], "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "code generation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16549", "pdf": "https://arxiv.org/pdf/2503.16549", "abs": "https://arxiv.org/abs/2503.16549", "authors": ["Felix Chen", "Hangjie Yuan", "Yunqiu Xu", "Tao Feng", "Jun Cen", "Pengwei Liu", "Zeying Huang", "Yi Yang"], "title": "MathFlow: Enhancing the Perceptual Flow of MLLMs for Visual Mathematical Problems", "categories": ["cs.CV"], "comment": "https://github.com/MathFlow-zju/MathFlow", "summary": "Despite impressive performance across diverse tasks, Multimodal Large\nLanguage Models (MLLMs) have yet to fully demonstrate their potential in visual\nmathematical problem-solving, particularly in accurately perceiving and\ninterpreting diagrams. Inspired by typical processes of humans, we hypothesize\nthat the perception capabilities to extract meaningful information from\ndiagrams is crucial, as it directly impacts subsequent inference processes. To\nvalidate this hypothesis, we developed FlowVerse, a comprehensive benchmark\nthat categorizes all information used during problem-solving into four\ncomponents, which are then combined into six problem versions for evaluation.\nOur preliminary results on FlowVerse reveal that existing MLLMs exhibit\nsubstantial limitations when extracting essential information and reasoned\nproperty from diagrams and performing complex reasoning based on these visual\ninputs. In response, we introduce MathFlow, a modular problem-solving pipeline\nthat decouples perception and inference into distinct stages, thereby\noptimizing each independently. Given the perceptual limitations observed in\ncurrent MLLMs, we trained MathFlow-P-7B as a dedicated perception model.\nExperimental results indicate that MathFlow-P-7B yields substantial performance\ngains when integrated with various closed-source and open-source inference\nmodels. This demonstrates the effectiveness of the MathFlow pipeline and its\ncompatibility to diverse inference frameworks. The FlowVerse benchmark and code\nare available at https://github.com/MathFlow-zju/MathFlow.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16591", "pdf": "https://arxiv.org/pdf/2503.16591", "abs": "https://arxiv.org/abs/2503.16591", "authors": ["Luigi Piccinelli", "Christos Sakaridis", "Mattia Segu", "Yung-Hsu Yang", "Siyuan Li", "Wim Abbeloos", "Luc Van Gool"], "title": "UniK3D: Universal Camera Monocular 3D Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D estimation is crucial for visual perception. However, current\nmethods fall short by relying on oversimplified assumptions, such as pinhole\ncamera models or rectified images. These limitations severely restrict their\ngeneral applicability, causing poor performance in real-world scenarios with\nfisheye or panoramic images and resulting in substantial context loss. To\naddress this, we present UniK3D, the first generalizable method for monocular\n3D estimation able to model any camera. Our method introduces a spherical 3D\nrepresentation which allows for better disentanglement of camera and scene\ngeometry and enables accurate metric 3D reconstruction for unconstrained camera\nmodels. Our camera component features a novel, model-independent representation\nof the pencil of rays, achieved through a learned superposition of spherical\nharmonics. We also introduce an angular loss, which, together with the camera\nmodule design, prevents the contraction of the 3D outputs for wide-view\ncameras. A comprehensive zero-shot evaluation on 13 diverse datasets\ndemonstrates the state-of-the-art performance of UniK3D across 3D, depth, and\ncamera metrics, with substantial gains in challenging large-field-of-view and\npanoramic settings, while maintaining top accuracy in conventional pinhole\nsmall-field-of-view domains. Code and models are available at\ngithub.com/lpiccinelli-eth/unik3d .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16544", "pdf": "https://arxiv.org/pdf/2503.16544", "abs": "https://arxiv.org/abs/2503.16544", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16664", "pdf": "https://arxiv.org/pdf/2503.16664", "abs": "https://arxiv.org/abs/2503.16664", "authors": ["Martin Kosteln√≠k", "Karel Bene≈°", "Michal Hradi≈°"], "title": "TextBite: A Historical Czech Document Dataset for Logical Page Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Logical page segmentation is an important step in document analysis, enabling\nbetter semantic representations, information retrieval, and text understanding.\nPrevious approaches define logical segmentation either through text or\ngeometric objects, relying on OCR or precise geometry. To avoid the need for\nOCR, we define the task purely as segmentation in the image domain.\nFurthermore, to ensure the evaluation remains unaffected by geometrical\nvariations that do not impact text segmentation, we propose to use only\nforeground text pixels in the evaluation metric and disregard all background\npixels. To support research in logical document segmentation, we introduce\nTextBite, a dataset of historical Czech documents spanning the 18th to 20th\ncenturies, featuring diverse layouts from newspapers, dictionaries, and\nhandwritten records. The dataset comprises 8,449 page images with 78,863\nannotated segments of logically and thematically coherent text. We propose a\nset of baseline methods combining text region detection and relation\nprediction. The dataset, baselines and evaluation framework can be accessed at\nhttps://github.com/DCGM/textbite-dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16561", "pdf": "https://arxiv.org/pdf/2503.16561", "abs": "https://arxiv.org/abs/2503.16561", "authors": ["Ibrahim Al Azher", "Miftahul Jannat Mokarrama", "Zhishuai Guo", "Sagnik Ray Choudhury", "Hamed Alhoori"], "title": "FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article", "categories": ["cs.CL", "cs.LG"], "comment": "19 pages, 5 figures", "summary": "The future work section of a scientific article outlines potential research\ndirections by identifying gaps and limitations of a current study. This section\nserves as a valuable resource for early-career researchers seeking unexplored\nareas and experienced researchers looking for new projects or collaborations.\nIn this study, we generate future work suggestions from key sections of a\nscientific article alongside related papers and analyze how the trends have\nevolved. We experimented with various Large Language Models (LLMs) and\nintegrated Retrieval-Augmented Generation (RAG) to enhance the generation\nprocess. We incorporate a LLM feedback mechanism to improve the quality of the\ngenerated content and propose an LLM-as-a-judge approach for evaluation. Our\nresults demonstrated that the RAG-based approach with LLM feedback outperforms\nother methods evaluated through qualitative and quantitative metrics. Moreover,\nwe conduct a human evaluation to assess the LLM as an extractor and judge. The\ncode and dataset for this project are here, code: HuggingFace", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16575", "pdf": "https://arxiv.org/pdf/2503.16575", "abs": "https://arxiv.org/abs/2503.16575", "authors": ["Bo Hu", "Han Yuan", "Vlad Pandelea", "Wuqiong Luo", "Yingzhu Zhao", "Zheng Ma"], "title": "Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has sparked widespread\nadoption across diverse applications, making robust evaluation frameworks\ncrucial for assessing their performance. While conventional evaluation metrics\nremain applicable for shorter texts, their efficacy diminishes when evaluating\nthe quality of long-form answers. This limitation is particularly critical in\nreal-world scenarios involving extended questions, extensive context, and\nlong-form answers, such as financial analysis or regulatory compliance. In this\npaper, we use a practical financial use case to illustrate applications that\nhandle \"long question-context-answer triplets\". We construct a real-world\nfinancial dataset comprising long triplets and demonstrate the inadequacies of\ntraditional metrics. To address this, we propose an effective Extract, Match,\nand Score (EMS) evaluation approach tailored to the complexities of long-form\nLLMs' outputs, providing practitioners with a reliable methodology for\nassessing LLMs' performance in complex real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16795", "pdf": "https://arxiv.org/pdf/2503.16795", "abs": "https://arxiv.org/abs/2503.16795", "authors": ["Yihan Hu", "Jianing Peng", "Yiheng Lin", "Ting Liu", "Xiaochao Qu", "Luoqi Liu", "Yao Zhao", "Yunchao Wei"], "title": "DCEdit: Dual-Level Controlled Image Editing via Precisely Localized Semantics", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel approach to improving text-guided image editing\nusing diffusion-based models. Text-guided image editing task poses key\nchallenge of precisly locate and edit the target semantic, and previous methods\nfall shorts in this aspect. Our method introduces a Precise Semantic\nLocalization strategy that leverages visual and textual self-attention to\nenhance the cross-attention map, which can serve as a regional cues to improve\nediting performance. Then we propose a Dual-Level Control mechanism for\nincorporating regional cues at both feature and latent levels, offering\nfine-grained control for more precise edits. To fully compare our methods with\nother DiT-based approaches, we construct the RW-800 benchmark, featuring high\nresolution images, long descriptive texts, real-world images, and a new text\nediting task. Experimental results on the popular PIE-Bench and RW-800\nbenchmarks demonstrate the superior performance of our approach in preserving\nbackground and providing accurate edits.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16779", "pdf": "https://arxiv.org/pdf/2503.16779", "abs": "https://arxiv.org/abs/2503.16779", "authors": ["Mengsong Wu", "Tong Zhu", "Han Han", "Xiang Zhang", "Wenbiao Shao", "Wenliang Chen"], "title": "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of Frozen Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https://github.com/fairyshine/Chain-of-Tools .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16811", "pdf": "https://arxiv.org/pdf/2503.16811", "abs": "https://arxiv.org/abs/2503.16811", "authors": ["Maoji Zheng", "Ziyu Xu", "Qiming Xia", "Hai Wu", "Chenglu Wen", "Cheng Wang"], "title": "Seg2Box: 3D Object Detection by Point-Wise Semantics Supervision", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "LiDAR-based 3D object detection and semantic segmentation are critical tasks\nin 3D scene understanding. Traditional detection and segmentation methods\nsupervise their models through bounding box labels and semantic mask labels.\nHowever, these two independent labels inherently contain significant\nredundancy. This paper aims to eliminate the redundancy by supervising 3D\nobject detection using only semantic labels. However, the challenge arises due\nto the incomplete geometry structure and boundary ambiguity of point-cloud\ninstances, leading to inaccurate pseudo labels and poor detection results. To\naddress these challenges, we propose a novel method, named Seg2Box. We first\nintroduce a Multi-Frame Multi-Scale Clustering (MFMS-C) module, which leverages\nthe spatio-temporal consistency of point clouds to generate accurate box-level\npseudo-labels. Additionally, the Semantic?Guiding Iterative-Mining\nSelf-Training (SGIM-ST) module is proposed to enhance the performance by\nprogressively refining the pseudo-labels and mining the instances without\ngenerating pseudo-labels. Experiments on the Waymo Open Dataset and nuScenes\nDataset show that our method significantly outperforms other competitive\nmethods by 23.7\\% and 10.3\\% in mAP, respectively. The results demonstrate the\ngreat label-efficient potential and advancement of our method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16856", "pdf": "https://arxiv.org/pdf/2503.16856", "abs": "https://arxiv.org/abs/2503.16856", "authors": ["Yang Tian", "Zheng Lu", "Mingqi Gao", "Zheng Liu", "Bo Zhao"], "title": "MMCR: Benchmarking Cross-Source Reasoning in Scientific Papers", "categories": ["cs.CL"], "comment": null, "summary": "Fully comprehending scientific papers by machines reflects a high level of\nArtificial General Intelligence, requiring the ability to reason across\nfragmented and heterogeneous sources of information, presenting a complex and\npractically significant challenge. While Vision-Language Models (VLMs) have\nmade remarkable strides in various tasks, particularly those involving\nreasoning with evidence source from single image or text page, their ability to\nuse cross-source information for reasoning remains an open problem. This work\npresents MMCR, a high-difficulty benchmark designed to evaluate VLMs' capacity\nfor reasoning with cross-source information from scientific papers. The\nbenchmark comprises 276 high-quality questions, meticulously annotated by\nhumans across 7 subjects and 10 task types. Experiments with 18 VLMs\ndemonstrate that cross-source reasoning presents a substantial challenge for\nexisting models. Notably, even the top-performing model, GPT-4o, achieved only\n48.55% overall accuracy, with only 20% accuracy in multi-table comprehension\ntasks, while the second-best model, Qwen2.5-VL-72B, reached 39.86% overall\naccuracy. Furthermore, we investigated the impact of the Chain-of-Thought (CoT)\ntechnique on cross-source reasoning and observed a detrimental effect on small\nmodels, whereas larger models demonstrated substantially enhanced performance.\nThese results highlight the pressing need to develop VLMs capable of\neffectively utilizing cross-source information for reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16868", "pdf": "https://arxiv.org/pdf/2503.16868", "abs": "https://arxiv.org/abs/2503.16868", "authors": ["Mengsay Loem", "Taiju Hosaka"], "title": "Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visual question answering (VQA) has emerged as a flexible approach for\nextracting specific pieces of information from document images. However,\nexisting work typically queries each field in isolation, overlooking potential\ndependencies across multiple items. This paper investigates the merits of\nextracting multiple fields jointly versus separately. Through experiments on\nmultiple large vision language models and datasets, we show that jointly\nextracting fields often improves accuracy, especially when the fields share\nstrong numeric or contextual dependencies. We further analyze how performance\nscales with the number of requested items and use a regression based metric to\nquantify inter field relationships. Our results suggest that multi field\nprompts can mitigate confusion arising from similar surface forms and related\nnumeric values, providing practical methods for designing robust VQA systems in\ndocument information extraction tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16855", "pdf": "https://arxiv.org/pdf/2503.16855", "abs": "https://arxiv.org/abs/2503.16855", "authors": ["Koki Hirooka", "Abu Saleh Musa Miah", "Tatsuya Murakami", "Yuto Akiba", "Yong Seok Hwang", "Jungpil Shin"], "title": "Stack Transformer Based Spatial-Temporal Attention Model for Dynamic Multi-Culture Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Hand gesture-based Sign Language Recognition (SLR) serves as a crucial\ncommunication bridge between deaf and non-deaf individuals. Existing SLR\nsystems perform well for their cultural SL but may struggle with multi-cultural\nsign languages (McSL). To address these challenges, this paper proposes a Stack\nSpatial-Temporal Transformer Network that leverages multi-head attention\nmechanisms to capture both spatial and temporal dependencies with hierarchical\nfeatures using the Stack Transfer concept. In the proceed, firstly, we applied\na fully connected layer to make a embedding vector which has high expressive\npower from the original dataset, then fed them a stack newly proposed\ntransformer to achieve hierarchical features with short-range and long-range\ndependency. The network architecture is composed of several stages that process\nspatial and temporal relationships sequentially, ensuring effective feature\nextraction. After making the fully connected layer, the embedding vector is\nprocessed by the Spatial Multi-Head Attention Transformer, which captures\nspatial dependencies between joints. In the next stage, the Temporal Multi-Head\nAttention Transformer captures long-range temporal dependencies, and again, the\nfeatures are concatenated with the output using another skip connection. The\nprocessed features are then passed to the Feed-Forward Network (FFN), which\nrefines the feature representations further. After the FFN, additional skip\nconnections are applied to combine the output with earlier layers, followed by\na final normalization layer to produce the final output feature tensor. This\nprocess is repeated for 10 transformer blocks. The extensive experiment shows\nthat the JSL, KSL and ASL datasets achieved good performance accuracy. Our\napproach demonstrates improved performance in McSL, and it will be consider as\na novel work in this domain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17126", "pdf": "https://arxiv.org/pdf/2503.17126", "abs": "https://arxiv.org/abs/2503.17126", "authors": ["John Joon Young Chung", "Vishakh Padmakumar", "Melissa Roemmele", "Yuqian Sun", "Max Kreminski"], "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279", "abs": "https://arxiv.org/abs/2503.17279", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17336", "pdf": "https://arxiv.org/pdf/2503.17336", "abs": "https://arxiv.org/abs/2503.17336", "authors": ["Reem Gody", "Mohamed Abdelghaffar", "Mohammed Jabreel", "Ahmed Tawfik"], "title": "Efficient Intent-Based Filtering for Multi-Party Conversations Using Knowledge Distillation from LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have showcased remarkable capabilities in\nconversational AI, enabling open-domain responses in chat-bots, as well as\nadvanced processing of conversations like summarization, intent classification,\nand insights generation. However, these models are resource-intensive,\ndemanding substantial memory and computational power. To address this, we\npropose a cost-effective solution that filters conversational snippets of\ninterest for LLM processing, tailored to the target downstream application,\nrather than processing every snippet. In this work, we introduce an innovative\napproach that leverages knowledge distillation from LLMs to develop an\nintent-based filter for multi-party conversations, optimized for compute power\nconstrained environments. Our method combines different strategies to create a\ndiverse multi-party conversational dataset, that is annotated with the target\nintents and is then used to fine-tune the MobileBERT model for multi-label\nintent classification. This model achieves a balance between efficiency and\nperformance, effectively filtering conversation snippets based on their\nintents. By passing only the relevant snippets to the LLM for further\nprocessing, our approach significantly reduces overall operational costs\ndepending on the intents and the data distribution as demonstrated in our\nexperiments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16948", "pdf": "https://arxiv.org/pdf/2503.16948", "abs": "https://arxiv.org/abs/2503.16948", "authors": ["Yinhan Zhang", "Yue Ma", "Bingyuan Wang", "Qifeng Chen", "Zeyu Wang"], "title": "MagicColor: Multi-Instance Sketch Colorization", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textit{MagicColor}, a diffusion-based framework for\nmulti-instance sketch colorization. The production of multi-instance 2D line\nart colorization adheres to an industry-standard workflow, which consists of\nthree crucial stages: the design of line art characters, the coloring of\nindividual objects, and the refinement process. The artists are required to\nrepeat the process of coloring each instance one by one, which is inaccurate\nand inefficient. Meanwhile, current generative methods fail to solve this task\ndue to the challenge of multi-instance pair data collection. To tackle these\nchallenges, we incorporate three technical designs to ensure precise character\ndetail transcription and achieve multi-instance sketch colorization in a single\nforward. Specifically, we first propose the self-play training strategy to\nsolve the lack of training data. Then we introduce an instance guider to feed\nthe color of the instance. To achieve accurate color matching, we present\nfine-grained color matching with edge loss to enhance visual quality. Equipped\nwith the proposed modules, MagicColor enables automatically transforming\nsketches into vividly-colored images with accurate consistency and\nmulti-instance control. Experiments on our collected datasets show that our\nmodel outperforms existing methods regarding chromatic precision. Specifically,\nour model critically automates the colorization process with zero manual\nadjustments, so novice users can produce stylistically consistent artwork by\nproviding reference instances and the original line art. Our code and\nadditional details are available at https://yinhan-zhang.github.io/color", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16432", "pdf": "https://arxiv.org/pdf/2503.16432", "abs": "https://arxiv.org/abs/2503.16432", "authors": ["Young-Ho Bae", "Casey C. Bennett"], "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "36 pages", "summary": "This study investigates multimodal turn-taking prediction within human-agent\ninteractions (HAI), particularly focusing on cooperative gaming environments.\nIt comprises both model development and subsequent user study, aiming to refine\nour understanding and improve conversational dynamics in spoken dialogue\nsystems (SDSs). For the modeling phase, we introduce a novel transformer-based\ndeep learning (DL) model that simultaneously integrates multiple modalities -\ntext, vision, audio, and contextual in-game data to predict turn-taking events\nin real-time. Our model employs a Crossmodal Transformer architecture to\neffectively fuse information from these diverse modalities, enabling more\ncomprehensive turn-taking predictions. The model demonstrates superior\nperformance compared to baseline models, achieving 87.3% accuracy and 83.0%\nmacro F1 score. A human user study was then conducted to empirically evaluate\nthe turn-taking DL model in an interactive scenario with a virtual avatar while\nplaying the game \"Dont Starve Together\", comparing a control condition without\nturn-taking prediction (n=20) to an experimental condition with our model\ndeployed (n=40). Both conditions included a mix of English and Korean speakers,\nsince turn-taking cues are known to vary by culture. We then analyzed the\ninteraction quality, examining aspects such as utterance counts, interruption\nfrequency, and participant perceptions of the avatar. Results from the user\nstudy suggest that our multimodal turn-taking model not only enhances the\nfluidity and naturalness of human-agent conversations, but also maintains a\nbalanced conversational dynamic without significantly altering dialogue\nfrequency. The study provides in-depth insights into the influence of\nturn-taking abilities on user perceptions and interaction quality, underscoring\nthe potential for more contextually adaptive and responsive conversational\nagents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dialogue"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16964", "pdf": "https://arxiv.org/pdf/2503.16964", "abs": "https://arxiv.org/abs/2503.16964", "authors": ["Jiadong Tang", "Yu Gao", "Dianyi Yang", "Liqi Yan", "Yufeng Yue", "Yi Yang"], "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16970", "pdf": "https://arxiv.org/pdf/2503.16970", "abs": "https://arxiv.org/abs/2503.16970", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16498", "pdf": "https://arxiv.org/pdf/2503.16498", "abs": "https://arxiv.org/abs/2503.16498", "authors": ["Enzo Sinacola", "Arnault Pachot", "Thierry Petit"], "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Accepted, proceedings of the 17th International Conference on Machine\n  Learning and Computing", "summary": "Large Language Models (LLMs) offer a promising alternative to traditional\nsurvey methods, potentially enhancing efficiency and reducing costs. In this\nstudy, we use LLMs to create virtual populations that answer survey questions,\nenabling us to predict outcomes comparable to human responses. We evaluate\nseveral LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the\nLlama and Mistral models-comparing their performance to that of a traditional\nRandom Forests algorithm using demographic data from the World Values Survey\n(WVS). LLMs demonstrate competitive performance overall, with the significant\nadvantage of requiring no additional training data. However, they exhibit\nbiases when predicting responses for certain religious and population groups,\nunderperforming in these areas. On the other hand, Random Forests demonstrate\nstronger performance than LLMs when trained with sufficient data. We observe\nthat removing censorship mechanisms from LLMs significantly improves predictive\naccuracy, particularly for underrepresented demographic segments where censored\nmodels struggle. These findings highlight the importance of addressing biases\nand reconsidering censorship approaches in LLMs to enhance their reliability\nand fairness in public opinion research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505", "abs": "https://arxiv.org/abs/2503.16505", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "title": "Scalable Evaluation of Online Moderation Strategies via Synthetic Simulations", "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 9 figures", "summary": "Despite the ever-growing importance of online moderation, there has been no\nlarge-scale study evaluating the effectiveness of alternative moderation\nstrategies. This is largely due to the lack of appropriate datasets, and the\ndifficulty of getting human discussants, moderators, and evaluators involved in\nmultiple experiments. In this paper, we propose a methodology for leveraging\nsynthetic experiments performed exclusively by Large Language Models (LLMs) to\ninitially bypass the need for human participation in experiments involving\nonline moderation. We evaluate six LLM moderation configurations; two currently\nused real-life moderation strategies (guidelines issued for human moderators\nfor online moderation and real-life facilitation), two baseline strategies\n(guidelines elicited for LLM alignment work, and LLM moderation with minimal\nprompting) a baseline with no moderator at all, as well as our own proposed\nstrategy inspired by a Reinforcement Learning (RL) formulation of the problem.\nWe find that our own moderation strategy significantly outperforms established\nmoderation guidelines, as well as out-of-the-box LLM moderation. We also find\nthat smaller LLMs, with less intensive instruction-tuning, can create more\nvaried discussions than larger models. In order to run these experiments, we\ncreate and release an efficient, purpose-built, open-source Python framework,\ndubbed \"SynDisco\" to easily simulate hundreds of discussions using LLM\nuser-agents and moderators. Additionally, we release the Virtual Moderation\nDataset (VMD), a large dataset of LLM-generated and LLM-annotated discussions,\ngenerated by three families of open-source LLMs accompanied by an exploratory\nanalysis of the dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16545", "pdf": "https://arxiv.org/pdf/2503.16545", "abs": "https://arxiv.org/abs/2503.16545", "authors": ["Xinyan Chen", "Jiaxin Ge", "Hongming Dai", "Qiang Zhou", "Qiuxuan Feng", "Jingtong Hu", "Yizhou Wang", "Jiaming Liu", "Shanghang Zhang"], "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Empathy is fundamental to human interactions, yet it remains unclear whether\nembodied agents can provide human-like empathetic support. Existing works have\nstudied agents' tasks solving and social interactions abilities, but whether\nagents can understand empathetic needs and conduct empathetic behaviors remains\noverlooked. To address this, we introduce EmpathyAgent, the first benchmark to\nevaluate and enhance agents' empathetic actions across diverse scenarios.\nEmpathyAgent contains 10,000 multimodal samples with corresponding empathetic\ntask plans and three different challenges. To systematically evaluate the\nagents' empathetic actions, we propose an empathy-specific evaluation suite\nthat evaluates the agents' empathy process. We benchmark current models and\nfound that exhibiting empathetic actions remains a significant challenge.\nMeanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially\nenhance empathetic behavior. By establishing a standard benchmark for\nevaluating empathetic actions, we hope to advance research in empathetic\nembodied agents. Our code and data are publicly available at\nhttps://github.com/xinyan-cxy/EmpathyAgent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16997", "pdf": "https://arxiv.org/pdf/2503.16997", "abs": "https://arxiv.org/abs/2503.16997", "authors": ["Qinghe Ma", "Jian Zhang", "Zekun Li", "Lei Qi", "Qian Yu", "Yinghuan Shi"], "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Large pretrained visual foundation models exhibit impressive general\ncapabilities. However, the extensive prior knowledge inherent in these models\ncan sometimes be a double-edged sword when adapting them to downstream tasks in\nspecific domains. In the context of semi-supervised medical image segmentation\nwith domain shift, foundation models like MedSAM tend to make overconfident\npredictions, some of which are incorrect. The error accumulation hinders the\neffective utilization of unlabeled data and limits further improvements. In\nthis paper, we introduce a Synergistic training framework for Foundation and\nConventional models (SynFoC) to address the issue. We observe that a\nconventional model trained from scratch has the ability to correct the\nhigh-confidence mispredictions of the foundation model, while the foundation\nmodel can supervise it with high-quality pseudo-labels in the early training\nstages. Furthermore, to enhance the collaborative training effectiveness of\nboth models and promote reliable convergence towards optimization, the\nconsensus-divergence consistency regularization is proposed. We demonstrate the\nsuperiority of our method across four public multi-domain datasets. In\nparticular, our method improves the Dice score by 10.31\\% on the Prostate\ndataset. Our code is available at https://github.com/MQinghe/SynFoC .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17034", "pdf": "https://arxiv.org/pdf/2503.17034", "abs": "https://arxiv.org/abs/2503.17034", "authors": ["Stephen Lloyd-Brown", "Susan Francis", "Caroline Hoad", "Penny Gowland", "Karen Mullinger", "Andrew French", "Xin Chen"], "title": "An Attentive Representative Sample Selection Strategy Combined with Balanced Batch Training for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ISBI 2025", "summary": "An often overlooked problem in medical image segmentation research is the\neffective selection of training subsets to annotate from a complete set of\nunlabelled data. Many studies select their training sets at random, which may\nlead to suboptimal model performance, especially in the minimal supervision\nsetting where each training image has a profound effect on performance\noutcomes. This work aims to address this issue. We use prototypical contrasting\nlearning and clustering to extract representative and diverse samples for\nannotation. We improve upon prior works with a bespoke cluster-based image\nselection process. Additionally, we introduce the concept of unsupervised\nbalanced batch dataloading to medical image segmentation, which aims to improve\nmodel learning with minimally annotated data. We evaluated our method on a\npublic skin lesion dataset (ISIC 2018) and compared it to another\nstate-of-the-art data sampling method. Our method achieved superior performance\nin a low annotation budget scenario.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17069", "pdf": "https://arxiv.org/pdf/2503.17069", "abs": "https://arxiv.org/abs/2503.17069", "authors": ["Yufei Shi", "Weilong Yan", "Gang Xu", "Yumeng Li", "Yuchen Li", "Zhenxi Li", "Fei Richard Yu", "Ming Li", "Si Yong Yeo"], "title": "PVChat: Personalized Video Chat with One-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video large language models (ViLLMs) excel in general video understanding,\ne.g., recognizing activities like talking and eating, but struggle with\nidentity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or\n\"Tom is discussing with Sarah\", limiting their applicability in smart\nhealthcare and smart home environments. To address this limitation, we propose\na one-shot learning framework PVChat, the first personalized ViLLM that enables\nsubject-aware question answering (QA) from a single video for each subject. Our\napproach optimizes a Mixture-of-Heads (MoH) enhanced ViLLM on a synthetically\naugmented video-QA dataset, leveraging a progressive image-to-video learning\nstrategy. Specifically, we introduce an automated augmentation pipeline that\nsynthesizes identity-preserving positive samples and retrieves hard negatives\nfrom existing video corpora, generating a diverse training dataset with four QA\ntypes: existence, appearance, action, and location inquiries. To enhance\nsubject-specific learning, we propose a ReLU Routing MoH attention mechanism,\nalongside two novel objectives: (1) Smooth Proximity Regularization for\nprogressive learning through exponential distance scaling and (2) Head\nActivation Enhancement for balanced attention routing. Finally, we adopt a\ntwo-stage training strategy, transitioning from image pre-training to video\nfine-tuning, enabling a gradual learning process from static attributes to\ndynamic representations. We evaluate PVChat on diverse datasets covering\nmedical scenarios, TV series, anime, and real-world footage, demonstrating its\nsuperiority in personalized feature understanding after learning from a single\nvideo, compared to state-of-the-art ViLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17074", "pdf": "https://arxiv.org/pdf/2503.17074", "abs": "https://arxiv.org/abs/2503.17074", "authors": ["Vittorio Pippi", "Fabio Quattrini", "Silvia Cascianelli", "Alessio Tonioni", "Rita Cucchiara"], "title": "Zero-Shot Styled Text Image Generation, but Make It Autoregressive", "categories": ["cs.CV"], "comment": "Accepted at CVPR2025", "summary": "Styled Handwritten Text Generation (HTG) has recently received attention from\nthe computer vision and document analysis communities, which have developed\nseveral solutions, either GAN- or diffusion-based, that achieved promising\nresults. Nonetheless, these strategies fail to generalize to novel styles and\nhave technical constraints, particularly in terms of maximum output length and\ntraining efficiency. To overcome these limitations, in this work, we propose a\nnovel framework for text image generation, dubbed Emuru. Our approach leverages\na powerful text image representation model (a variational autoencoder) combined\nwith an autoregressive Transformer. Our approach enables the generation of\nstyled text images conditioned on textual content and style examples, such as\nspecific fonts or handwriting styles. We train our model solely on a diverse,\nsynthetic dataset of English text rendered in over 100,000 typewritten and\ncalligraphy fonts, which gives it the capability to reproduce unseen styles\n(both fonts and users' handwriting) in zero-shot. To the best of our knowledge,\nEmuru is the first autoregressive model for HTG, and the first designed\nspecifically for generalization to novel styles. Moreover, our model generates\nimages without background artifacts, which are easier to use for downstream\napplications. Extensive evaluation on both typewritten and handwritten,\nany-length text image generation scenarios demonstrates the effectiveness of\nour approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17076", "pdf": "https://arxiv.org/pdf/2503.17076", "abs": "https://arxiv.org/abs/2503.17076", "authors": ["Victor Besnier", "Mickael Chen", "David Hurych", "Eduardo Valle", "Matthieu Cord"], "title": "Halton Scheduler For Masked Generative Image Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Masked Generative Image Transformers (MaskGIT) have emerged as a scalable and\nefficient image generation framework, able to deliver high-quality visuals with\nlow inference costs. However, MaskGIT's token unmasking scheduler, an essential\ncomponent of the framework, has not received the attention it deserves. We\nanalyze the sampling objective in MaskGIT, based on the mutual information\nbetween tokens, and elucidate its shortcomings. We then propose a new sampling\nstrategy based on our Halton scheduler instead of the original Confidence\nscheduler. More precisely, our method selects the token's position according to\na quasi-random, low-discrepancy Halton sequence. Intuitively, that method\nspreads the tokens spatially, progressively covering the image uniformly at\neach step. Our analysis shows that it allows reducing non-recoverable sampling\nerrors, leading to simpler hyper-parameters tuning and better quality images.\nOur scheduler does not require retraining or noise injection and may serve as a\nsimple drop-in replacement for the original sampling strategy. Evaluation of\nboth class-to-image synthesis on ImageNet and text-to-image generation on the\nCOCO dataset demonstrates that the Halton scheduler outperforms the Confidence\nscheduler quantitatively by reducing the FID and qualitatively by generating\nmore diverse and more detailed images. Our code is at\nhttps://github.com/valeoai/Halton-MaskGIT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17093", "pdf": "https://arxiv.org/pdf/2503.17093", "abs": "https://arxiv.org/abs/2503.17093", "authors": ["Johan Edstedt", "Andr√© Mateus", "Alberto Jaenal"], "title": "ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Structure-from-Motion (SfM) is the task of estimating 3D structure and camera\nposes from images. We define Collaborative SfM (ColabSfM) as sharing\ndistributed SfM reconstructions. Sharing maps requires estimating a joint\nreference frame, which is typically referred to as registration. However, there\nis a lack of scalable methods and training datasets for registering SfM\nreconstructions. In this paper, we tackle this challenge by proposing the\nscalable task of point cloud registration for SfM reconstructions. We find that\ncurrent registration methods cannot register SfM point clouds when trained on\nexisting datasets. To this end, we propose a SfM registration dataset\ngeneration pipeline, leveraging partial reconstructions from synthetically\ngenerated camera trajectories for each scene. Finally, we propose a simple but\nimpactful neural refiner on top of the SotA registration method RoITr that\nyields significant improvements, which we call RefineRoITr. Our extensive\nexperimental evaluation shows that our proposed pipeline and model enables\nColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17096", "pdf": "https://arxiv.org/pdf/2503.17096", "abs": "https://arxiv.org/abs/2503.17096", "authors": ["Ruiyang Ha", "Songyi Jiang", "Bin Li", "Bikang Pan", "Yihang Zhu", "Junjie Zhang", "Xiatian Zhu", "Shaogang Gong", "Jingya Wang"], "title": "Multi-modal Multi-platform Person Re-Identification: Benchmark and Method", "categories": ["cs.CV"], "comment": null, "summary": "Conventional person re-identification (ReID) research is often limited to\nsingle-modality sensor data from static cameras, which fails to address the\ncomplexities of real-world scenarios where multi-modal signals are increasingly\nprevalent. For instance, consider an urban ReID system integrating stationary\nRGB cameras, nighttime infrared sensors, and UAVs equipped with dynamic\ntracking capabilities. Such systems face significant challenges due to\nvariations in camera perspectives, lighting conditions, and sensor modalities,\nhindering effective person ReID. To address these challenges, we introduce the\nMP-ReID benchmark, a novel dataset designed specifically for multi-modality and\nmulti-platform ReID. This benchmark uniquely compiles data from 1,930\nidentities across diverse modalities, including RGB, infrared, and thermal\nimaging, captured by both UAVs and ground-based cameras in indoor and outdoor\nenvironments. Building on this benchmark, we introduce Uni-Prompt ReID, a\nframework with specific-designed prompts, tailored for cross-modality and\ncross-platform scenarios. Our method consistently outperforms state-of-the-art\napproaches, establishing a robust foundation for future research in complex and\ndynamic ReID environments. Our dataset are available\nat:https://mp-reid.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17110", "pdf": "https://arxiv.org/pdf/2503.17110", "abs": "https://arxiv.org/abs/2503.17110", "authors": ["Robin Hesse", "Doƒüukan Baƒücƒ±", "Bernt Schiele", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Beyond Accuracy: What Matters in Designing Well-Behaved Models?", "categories": ["cs.CV", "cs.LG"], "comment": "Code: https://github.com/visinf/beyond-accuracy", "summary": "Deep learning has become an essential part of computer vision, with deep\nneural networks (DNNs) excelling in predictive performance. However, they often\nfall short in other critical quality dimensions, such as robustness,\ncalibration, or fairness. While existing studies have focused on a subset of\nthese quality dimensions, none have explored a more general form of\n\"well-behavedness\" of DNNs. With this work, we address this gap by\nsimultaneously studying nine different quality dimensions for image\nclassification. Through a large-scale study, we provide a bird's-eye view by\nanalyzing 326 backbone models and how different training paradigms and model\narchitectures affect the quality dimensions. We reveal various new insights\nsuch that (i) vision-language models exhibit high fairness on ImageNet-1k\nclassification and strong robustness against domain changes; (ii)\nself-supervised learning is an effective training paradigm to improve almost\nall considered quality dimensions; and (iii) the training dataset size is a\nmajor driver for most of the quality dimensions. We conclude our study by\nintroducing the QUBA score (Quality Understanding Beyond Accuracy), a novel\nmetric that ranks models across multiple dimensions of quality, enabling\ntailored recommendations based on specific user needs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17155", "pdf": "https://arxiv.org/pdf/2503.17155", "abs": "https://arxiv.org/abs/2503.17155", "authors": ["Panpan Wang", "Liqiang Niu", "Fandong Meng", "Jinan Xu", "Yufeng Chen", "Jie Zhou"], "title": "D2C: Unlocking the Potential of Continuous Autoregressive Image Generation with Discrete Tokens", "categories": ["cs.CV"], "comment": null, "summary": "In the domain of image generation, latent-based generative models occupy a\ndominant status; however, these models rely heavily on image tokenizer. To meet\nmodeling requirements, autoregressive models possessing the characteristics of\nscalability and flexibility embrace a discrete-valued tokenizer, but face the\nchallenge of poor image generation quality. In contrast, diffusion models take\nadvantage of the continuous-valued tokenizer to achieve better generation\nquality but are subject to low efficiency and complexity. The existing hybrid\nmodels are mainly to compensate for information loss and simplify the diffusion\nlearning process. The potential of merging discrete-valued and\ncontinuous-valued tokens in the field of image generation has not yet been\nexplored. In this paper, we propose D2C, a novel two-stage method to enhance\nmodel generation capacity. In the first stage, the discrete-valued tokens\nrepresenting coarse-grained image features are sampled by employing a small\ndiscrete-valued generator. Then in the second stage, the continuous-valued\ntokens representing fine-grained image features are learned conditioned on the\ndiscrete token sequence. In addition, we design two kinds of fusion modules for\nseamless interaction. On the ImageNet-256 benchmark, extensive experiment\nresults validate that our model achieves superior performance compared with\nseveral continuous-valued and discrete-valued generative models on the\nclass-conditional image generation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17162", "pdf": "https://arxiv.org/pdf/2503.17162", "abs": "https://arxiv.org/abs/2503.17162", "authors": ["Tonmoy Hossain ana Miaomiao Zhang"], "title": "CoRLD: Contrastive Representation Learning Of Deformable Shapes In Images", "categories": ["cs.CV"], "comment": null, "summary": "Deformable shape representations, parameterized by deformations relative to a\ngiven template, have proven effective for improved image analysis tasks.\nHowever, their broader applicability is hindered by two major challenges.\nFirst, existing methods mainly rely on a known template during testing, which\nis impractical and limits flexibility. Second, they often struggle to capture\nfine-grained, voxel-level distinctions between similar shapes (e.g., anatomical\nvariations among healthy individuals, those with mild cognitive impairment, and\ndiseased states). To address these limitations, we propose a novel framework -\nContrastive Representation Learning of Deformable shapes (CoRLD) in learned\ndeformation spaces and demonstrate its effectiveness in the context of image\nclassification. Our CoRLD leverages a class-aware contrastive supervised\nlearning objective in latent deformation spaces, promoting proximity among\nrepresentations of similar classes while ensuring separation of dissimilar\ngroups. In contrast to previous deep learning networks that require a reference\nimage as input to predict deformation changes, our approach eliminates this\ndependency. Instead, template images are utilized solely as ground truth in the\nloss function during the training process, making our model more flexible and\ngeneralizable to a wide range of medical applications. We validate CoRLD on\ndiverse datasets, including real brain magnetic resonance imaging (MRIs) and\nadrenal shapes derived from computed tomography (CT) scans. Experimental\nresults show that our model effectively extracts deformable shape features,\nwhich can be easily integrated with existing classifiers to substantially boost\nthe classification accuracy. Our code is available at GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17193", "pdf": "https://arxiv.org/pdf/2503.17193", "abs": "https://arxiv.org/abs/2503.17193", "authors": ["Xiaojin Lu", "Taoran yue", "Jiaxi cai", "Shibing Chu"], "title": "MSCA-Net:Multi-Scale Context Aggregation Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting infrared small targets in complex backgrounds remains a challenging\ntask because of the low contrast and high noise levels inherent in infrared\nimages. These factors often lead to the loss of crucial details during feature\nextraction. Moreover, existing detection methods have limitations in adequately\nintegrating global and local information, which constrains the efficiency and\naccuracy of infrared small target detection. To address these challenges, this\npaper proposes a novel network architecture named MSCA-Net, which integrates\nthree key components: Multi-Scale Enhanced Detection Attention\nmechanism(MSEDA), Positional Convolutional Block Attention Module (PCBAM), and\nChannel Aggregation Block (CAB). Specifically, MSEDA employs a multi-scale\nfeature fusion attention mechanism to adaptively aggregate information across\ndifferent scales, enriching feature representation. PCBAM captures the\ncorrelation between global and local features through a correlation\nmatrix-based strategy, enabling deep feature interaction. Moreover, CAB\nredistributes input feature channels, facilitating the efficient transmission\nof beneficial features and further enhancing the model detection capability in\ncomplex backgrounds. The experimental results demonstrate that MSCA-Net\nachieves outstanding small target detection performance in complex backgrounds.\nSpecifically, it attains mIoU scores of 78.43\\%, 94.56\\%, and 67.08\\% on the\nNUAA-SIRST, NUDT-SIRST, and IRTSD-1K datasets, respectively, underscoring its\neffectiveness and strong potential for real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17347", "pdf": "https://arxiv.org/pdf/2503.17347", "abs": "https://arxiv.org/abs/2503.17347", "authors": ["Jichen Hu", "Chen Yang", "Zanwei Zhou", "Jiemin Fang", "Xiaokang Yang", "Qi Tian", "Wei Shen"], "title": "Dereflection Any Image with Diffusion Priors and Diversified Data", "categories": ["cs.CV"], "comment": null, "summary": "Reflection removal of a single image remains a highly challenging task due to\nthe complex entanglement between target scenes and unwanted reflections.\nDespite significant progress, existing methods are hindered by the scarcity of\nhigh-quality, diverse data and insufficient restoration priors, resulting in\nlimited generalization across various real-world scenarios. In this paper, we\npropose Dereflection Any Image, a comprehensive solution with an efficient data\npreparation pipeline and a generalizable model for robust reflection removal.\nFirst, we introduce a dataset named Diverse Reflection Removal (DRR) created by\nrandomly rotating reflective mediums in target scenes, enabling variation of\nreflection angles and intensities, and setting a new benchmark in scale,\nquality, and diversity. Second, we propose a diffusion-based framework with\none-step diffusion for deterministic outputs and fast inference. To ensure\nstable learning, we design a three-stage progressive training strategy,\nincluding reflection-invariant finetuning to encourage consistent outputs\nacross varying reflection patterns that characterize our dataset. Extensive\nexperiments show that our method achieves SOTA performance on both common\nbenchmarks and challenging in-the-wild images, showing superior generalization\nacross diverse real-world scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16818", "pdf": "https://arxiv.org/pdf/2503.16818", "abs": "https://arxiv.org/abs/2503.16818", "authors": ["Shunki Tatsumi", "Ryo Hayakawa", "Youji Iiguni"], "title": "Depth-Aided Color Image Inpainting in Quaternion Domain", "categories": ["eess.IV", "cs.CV"], "comment": "accepted to IEEE Signal Processing Letters", "summary": "In this paper, we propose a depth-aided color image inpainting method in the\nquaternion domain, called depth-aided low-rank quaternion matrix completion\n(D-LRQMC). In conventional quaternion-based inpainting techniques, the color\nimage is expressed as a quaternion matrix by using the three imaginary parts as\nthe color channels, whereas the real part is set to zero and has no\ninformation. Our approach incorporates depth information as the real part of\nthe quaternion representations, leveraging the correlation between color and\ndepth to improve the result of inpainting. In the proposed method, we first\nrestore the observed image with the conventional LRQMC and estimate the depth\nof the restored result. We then incorporate the estimated depth into the real\npart of the observed image and perform LRQMC again. Simulation results\ndemonstrate that the proposed D-LRQMC can improve restoration accuracy and\nvisual quality for various images compared to the conventional LRQMC. These\nresults suggest the effectiveness of the depth information for color image\nprocessing in quaternion domain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16868", "pdf": "https://arxiv.org/pdf/2503.16868", "abs": "https://arxiv.org/abs/2503.16868", "authors": ["Mengsay Loem", "Taiju Hosaka"], "title": "Joint Extraction Matters: Prompt-Based Visual Question Answering for Multi-Field Document Information Extraction", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visual question answering (VQA) has emerged as a flexible approach for\nextracting specific pieces of information from document images. However,\nexisting work typically queries each field in isolation, overlooking potential\ndependencies across multiple items. This paper investigates the merits of\nextracting multiple fields jointly versus separately. Through experiments on\nmultiple large vision language models and datasets, we show that jointly\nextracting fields often improves accuracy, especially when the fields share\nstrong numeric or contextual dependencies. We further analyze how performance\nscales with the number of requested items and use a regression based metric to\nquantify inter field relationships. Our results suggest that multi field\nprompts can mitigate confusion arising from similar surface forms and related\nnumeric values, providing practical methods for designing robust VQA systems in\ndocument information extraction tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16988", "pdf": "https://arxiv.org/pdf/2503.16988", "abs": "https://arxiv.org/abs/2503.16988", "authors": ["Ying Ming", "Shaoze Luo", "Longfei Zhao", "Qiqi Xu", "Wei Song"], "title": "High Accuracy Pulmonary Vessel Segmentation for Contrast and Non-contrast CT Images and Its Clinical Evaluation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of pulmonary vessels plays a very critical role in\ndiagnosing and assessing various lung diseases. In clinical practice, diagnosis\nis typically carried out using CTPA images. However, there is a lack of\nhigh-precision pulmonary vessel segmentation algorithms for CTPA, and pulmonary\nvessel segmentation for NCCT poses an even greater challenge. In this study, we\npropose a 3D image segmentation algorithm for automated pulmonary vessel\nsegmentation from both contrast and non-contrast CT images. In the network, we\ndesigned a Vessel Lumen Structure Optimization Module (VLSOM), which extracts\nthe centerline of vessels and adjusts the weights based on the positional\ninformation and adds a Cl-Dice-Loss to supervise the stability of the vessels\nstructure. In addition, we designed a method for generating vessel GT from CTPA\nto NCCT for training models that support both CTPA and NCCT. In this work, we\nused 427 sets of high-precision annotated CT data from multiple vendors and\ncountries. Finally, our experimental model achieved Cl-Recall, Cl-DICE and\nRecall values of 0.879, 0.909, 0.934 (CTPA) and 0.928, 0.936, 0.955 (NCCT)\nrespectively. This shows that our model has achieved good performance in both\naccuracy and completeness of pulmonary vessel segmentation. In clinical visual\nevaluation, our model also had good segmentation performance on various disease\ntypes and can assist doctors in medical diagnosis, verifying the great\npotential of this method in clinical application.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17046", "pdf": "https://arxiv.org/pdf/2503.17046", "abs": "https://arxiv.org/abs/2503.17046", "authors": ["Dongsheng Yang", "Qianying Liu", "Wataru Sato", "Takashi Minato", "Chaoran Liu", "Shin'ya Nishida"], "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference", "comparison", "pairwise"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17261", "pdf": "https://arxiv.org/pdf/2503.17261", "abs": "https://arxiv.org/abs/2503.17261", "authors": ["Jie Mei", "Chenyu Lin", "Yu Qiu", "Yaonan Wang", "Hui Zhang", "Ziyang Wang", "Dong Dai"], "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Lung cancer is a leading cause of cancer-related deaths globally. PET-CT is\ncrucial for imaging lung tumors, providing essential metabolic and anatomical\ninformation, while it faces challenges such as poor image quality, motion\nartifacts, and complex tumor morphology. Deep learning-based models are\nexpected to address these problems, however, existing small-scale and private\ndatasets limit significant performance improvements for these methods. Hence,\nwe introduce a large-scale PET-CT lung tumor segmentation dataset, termed\nPCLT20K, which comprises 21,930 pairs of PET-CT images from 605 patients.\nFurthermore, we propose a cross-modal interactive perception network with Mamba\n(CIPA) for lung tumor segmentation in PET-CT images. Specifically, we design a\nchannel-wise rectification module (CRM) that implements a channel state space\nblock across multi-modal features to learn correlated representations and helps\nfilter out modality-specific noise. A dynamic cross-modality interaction module\n(DCIM) is designed to effectively integrate position and context information,\nwhich employs PET images to learn regional position information and serves as a\nbridge to assist in modeling the relationships between local features of CT\nimages. Extensive experiments on a comprehensive benchmark demonstrate the\neffectiveness of our CIPA compared to the current state-of-the-art segmentation\nmethods. We hope our research can provide more exploration opportunities for\nmedical image segmentation. The dataset and code are available at\nhttps://github.com/mj129/CIPA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16511", "pdf": "https://arxiv.org/pdf/2503.16511", "abs": "https://arxiv.org/abs/2503.16511", "authors": ["Tingkai Liu", "Ari S. Benjamin", "Anthony M. Zador"], "title": "Token-Level Uncertainty-Aware Objective for Language Model Post-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the current work, we connect token-level uncertainty in causal language\nmodeling to two types of training objectives: 1) masked maximum likelihood\n(MLE), 2) self-distillation. We show that masked MLE is effective in reducing\nepistemic uncertainty, and serve as an effective token-level automatic\ncurriculum learning technique. However, masked MLE is prone to overfitting and\nrequires self-distillation regularization to improve or maintain performance on\nout-of-distribution tasks. We demonstrate significant performance gain via the\nproposed training objective - combined masked MLE and self-distillation -\nacross multiple architectures (Gemma, LLaMA, Phi) and datasets (Alpaca,\nShareGPT, GSM8K), mitigating overfitting while maintaining adaptability during\npost-training. Our findings suggest that uncertainty-aware training provides an\neffective mechanism for enhancing language model training.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["Alpaca"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16516", "pdf": "https://arxiv.org/pdf/2503.16516", "abs": "https://arxiv.org/abs/2503.16516", "authors": ["Yuxin Chen", "Peng Tang", "Weidong Qiu", "Shujun Li"], "title": "Using LLMs for Automated Privacy Policy Analysis: Prompt Engineering, Fine-Tuning and Explainability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Privacy policies are widely used by digital services and often required for\nlegal purposes. Many machine learning based classifiers have been developed to\nautomate detection of different concepts in a given privacy policy, which can\nhelp facilitate other automated tasks such as producing a more reader-friendly\nsummary and detecting legal compliance issues. Despite the successful\napplications of large language models (LLMs) to many NLP tasks in various\ndomains, there is very little work studying the use of LLMs for automated\nprivacy policy analysis, therefore, if and how LLMs can help automate privacy\npolicy analysis remains under-explored. To fill this research gap, we conducted\na comprehensive evaluation of LLM-based privacy policy concept classifiers,\nemploying both prompt engineering and LoRA (low-rank adaptation) fine-tuning,\non four state-of-the-art (SOTA) privacy policy corpora and taxonomies. Our\nexperimental results demonstrated that combining prompt engineering and\nfine-tuning can make LLM-based classifiers outperform other SOTA methods,\n\\emph{significantly} and \\emph{consistently} across privacy policy\ncorpora/taxonomies and concepts. Furthermore, we evaluated the explainability\nof the LLM-based classifiers using three metrics: completeness, logicality, and\ncomprehensibility. For all three metrics, a score exceeding 91.1\\% was observed\nin our evaluation, indicating that LLMs are not only useful to improve the\nclassification performance, but also to enhance the explainability of detection\nresults.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16522", "pdf": "https://arxiv.org/pdf/2503.16522", "abs": "https://arxiv.org/abs/2503.16522", "authors": ["Yongjia Ma", "Donglin Di", "Xuan Liu", "Xiaokai Chen", "Lei Fan", "Wei Chen", "Tonghua Su"], "title": "Adams Bashforth Moulton Solver for Inversion and Editing in Rectified Flow", "categories": ["cs.CV"], "comment": null, "summary": "Rectified flow models have achieved remarkable performance in image and video\ngeneration tasks. However, existing numerical solvers face a trade-off between\nfast sampling and high-accuracy solutions, limiting their effectiveness in\ndownstream applications such as reconstruction and editing. To address this\nchallenge, we propose leveraging the Adams-Bashforth-Moulton (ABM)\npredictor-corrector method to enhance the accuracy of ODE solving in rectified\nflow models. Specifically, we introduce ABM-Solver, which integrates a multi\nstep predictor corrector approach to reduce local truncation errors and employs\nAdaptive Step Size Adjustment to improve sampling speed. Furthermore, to\neffectively preserve non edited regions while facilitating semantic\nmodifications, we introduce a Mask Guided Feature Injection module. We estimate\nself-similarity to generate a spatial mask that differentiates preserved\nregions from those available for editing. Extensive experiments on multiple\nhigh-resolution image datasets validate that ABM-Solver significantly improves\ninversion precision and editing quality, outperforming existing solvers without\nrequiring additional training or optimization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16523", "pdf": "https://arxiv.org/pdf/2503.16523", "abs": "https://arxiv.org/abs/2503.16523", "authors": ["Shi Yin Hong", "Uttamasha Oyshi", "Quan Mai", "Gibson Nkhata", "Susan Gauch"], "title": "Mind2: Mind-to-Mind Emotional Support System with Bidirectional Cognitive Discourse Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 2 figures, and 3 tables; WI-IAT 2024", "summary": "Emotional support (ES) systems alleviate users' mental distress by generating\nstrategic supportive dialogues based on diverse user situations. However, ES\nsystems are limited in their ability to generate effective ES dialogues that\ninclude timely context and interpretability, hindering them from earning public\ntrust. Driven by cognitive models, we propose Mind-to-Mind (Mind2), an ES\nframework that approaches interpretable ES context modeling for the ES dialogue\ngeneration task from a discourse analysis perspective. Specifically, we perform\ncognitive discourse analysis on ES dialogues according to our dynamic discourse\ncontext propagation window, which accommodates evolving context as the\nconversation between the ES system and user progresses. To enhance\ninterpretability, Mind2 prioritizes details that reflect each speaker's belief\nabout the other speaker with bidirectionality, integrating Theory-of-Mind,\nphysiological expected utility, and cognitive rationality to extract cognitive\nknowledge from ES conversations. Experimental results support that Mind2\nachieves competitive performance versus state-of-the-art ES systems while\ntrained with only 10\\% of the available training data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16538", "pdf": "https://arxiv.org/pdf/2503.16538", "abs": "https://arxiv.org/abs/2503.16538", "authors": ["Bastian P√§tzold", "Jan Nogga", "Sven Behnke"], "title": "Leveraging Vision-Language Models for Open-Vocabulary Instance Segmentation and Tracking", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to IEEE Robotics and Automation Letters (RA-L)", "summary": "This paper introduces a novel approach that leverages the capabilities of\nvision-language models (VLMs) by integrating them with established approaches\nfor open-vocabulary detection (OVD), instance segmentation, and tracking. We\nutilize VLM-generated structured descriptions to identify visible object\ninstances, collect application-relevant attributes, and inform an\nopen-vocabulary detector to extract corresponding bounding boxes that are\npassed to a video segmentation model providing precise segmentation masks and\ntracking capabilities. Once initialized, this model can then directly extract\nsegmentation masks, allowing processing of image streams in real time with\nminimal computational overhead. Tracks can be updated online as needed by\ngenerating new structured descriptions and corresponding open-vocabulary\ndetections. This combines the descriptive power of VLMs with the grounding\ncapability of OVD and the pixel-level understanding and speed of video\nsegmentation. Our evaluation across datasets and robotics platforms\ndemonstrates the broad applicability of this approach, showcasing its ability\nto extract task-specific attributes from non-standard objects in dynamic\nenvironments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16525", "pdf": "https://arxiv.org/pdf/2503.16525", "abs": "https://arxiv.org/abs/2503.16525", "authors": ["Huan Yang", "Renji Zhang", "Deyu Zhang"], "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large Language Model Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16542", "pdf": "https://arxiv.org/pdf/2503.16542", "abs": "https://arxiv.org/abs/2503.16542", "authors": ["Shiyi Jiang", "Farshad Firouzi", "Krishnendu Chakrabarty"], "title": "Defending Against Gradient Inversion Attacks for Biomedical Images via Learnable Data Perturbation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16536", "pdf": "https://arxiv.org/pdf/2503.16536", "abs": "https://arxiv.org/abs/2503.16536", "authors": ["Shuo Huang", "Muhammad Umair Nasir", "Steven James", "Julian Togelius"], "title": "Word2Minecraft: Generating 3D Game Levels through Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We present Word2Minecraft, a system that leverages large language models to\ngenerate playable game levels in Minecraft based on structured stories. The\nsystem transforms narrative elements-such as protagonist goals, antagonist\nchallenges, and environmental settings-into game levels with both spatial and\ngameplay constraints. We introduce a flexible framework that allows for the\ncustomization of story complexity, enabling dynamic level generation. The\nsystem employs a scaling algorithm to maintain spatial consistency while\nadapting key game elements. We evaluate Word2Minecraft using both metric-based\nand human-based methods. Our results show that GPT-4-Turbo outperforms\nGPT-4o-Mini in most areas, including story coherence and objective enjoyment,\nwhile the latter excels in aesthetic appeal. We also demonstrate the system' s\nability to generate levels with high map enjoyment, offering a promising step\nforward in the intersection of story generation and game design. We open-source\nthe code at https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16537", "pdf": "https://arxiv.org/pdf/2503.16537", "abs": "https://arxiv.org/abs/2503.16537", "authors": ["Grigorii Khvatskii", "Yong Suk Lee", "Corey Angst", "Maria Gibbs", "Robert Landers", "Nitesh V. Chawla"], "title": "Do Multimodal Large Language Models Understand Welding?", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages", "summary": "This paper examines the performance of Multimodal LLMs (MLLMs) in skilled\nproduction work, with a focus on welding. Using a novel data set of real-world\nand online weld images, annotated by a domain expert, we evaluate the\nperformance of two state-of-the-art MLLMs in assessing weld acceptability\nacross three contexts: RV \\& Marine, Aeronautical, and Farming. While both\nmodels perform better on online images, likely due to prior exposure or\nmemorization, they also perform relatively well on unseen, real-world weld\nimages. Additionally, we introduce WeldPrompt, a prompting strategy that\ncombines Chain-of-Thought generation with in-context learning to mitigate\nhallucinations and improve reasoning. WeldPrompt improves model recall in\ncertain contexts but exhibits inconsistent performance across others. These\nresults underscore the limitations and potentials of MLLMs in high-stakes\ntechnical domains and highlight the importance of fine-tuning, domain-specific\ndata, and more sophisticated prompting strategies to improve model reliability.\nThe study opens avenues for further research into multimodal learning in\nindustry applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16553", "pdf": "https://arxiv.org/pdf/2503.16553", "abs": "https://arxiv.org/abs/2503.16553", "authors": ["Zhenlin Qin", "Leizhen Wang", "Francisco Camara Pereira", "Zhenlinag Ma"], "title": "A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely applied to domain-specific tasks due\nto their massive general knowledge and remarkable inference capacities. Current\nstudies on LLMs have shown immense potential in applying LLMs to model\nindividual mobility prediction problems. However, most LLM-based mobility\nprediction models only train on specific datasets or use single well-designed\nprompts, leading to difficulty in adapting to different cities and users with\ndiverse contexts. To fill these gaps, this paper proposes a unified fine-tuning\nframework to train a foundational open source LLM-based mobility prediction\nmodel. We conducted extensive experiments on six real-world mobility datasets\nto validate the proposed model. The results showed that the proposed model\nachieved the best performance in prediction accuracy and transferability over\nstate-of-the-art models based on deep learning and LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16554", "pdf": "https://arxiv.org/pdf/2503.16554", "abs": "https://arxiv.org/abs/2503.16554", "authors": ["Brian Keith", "Fausto German", "Eric Krokos", "Sarah Joseph", "Chris North"], "title": "Explainable AI Components for Narrative Map Extraction", "categories": ["cs.CL", "cs.HC"], "comment": "Text2Story Workshop 2025 at ECIR 2025", "summary": "As narrative extraction systems grow in complexity, establishing user trust\nthrough interpretable and explainable outputs becomes increasingly critical.\nThis paper presents an evaluation of an Explainable Artificial Intelligence\n(XAI) system for narrative map extraction that provides meaningful explanations\nacross multiple levels of abstraction. Our system integrates explanations based\non topical clusters for low-level document relationships, connection\nexplanations for event relationships, and high-level structure explanations for\noverall narrative patterns. In particular, we evaluate the XAI system through a\nuser study involving 10 participants that examined narratives from the 2021\nCuban protests. The analysis of results demonstrates that participants using\nthe explanations made the users trust in the system's decisions, with\nconnection explanations and important event detection proving particularly\neffective at building user confidence. Survey responses indicate that the\nmulti-level explanation approach helped users develop appropriate trust in the\nsystem's narrative extraction capabilities. This work advances the\nstate-of-the-art in explainable narrative extraction while providing practical\ninsights for developing reliable narrative extraction systems that support\neffective human-AI collaboration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16709", "pdf": "https://arxiv.org/pdf/2503.16709", "abs": "https://arxiv.org/abs/2503.16709", "authors": ["Xuan Shen", "Weize Ma", "Jing Liu", "Changdi Yang", "Rui Ding", "Quanyi Wang", "Henghui Ding", "Wei Niu", "Yanzhi Wang", "Pu Zhao", "Jun Lin", "Jiuxiang Gu"], "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Monocular Depth Estimation (MDE) has emerged as a pivotal task in computer\nvision, supporting numerous real-world applications. However, deploying\naccurate depth estimation models on resource-limited edge devices, especially\nApplication-Specific Integrated Circuits (ASICs), is challenging due to the\nhigh computational and memory demands. Recent advancements in foundational\ndepth estimation deliver impressive results but further amplify the difficulty\nof deployment on ASICs. To address this, we propose QuartDepth which adopts\npost-training quantization to quantize MDE models with hardware accelerations\nfor ASICs. Our approach involves quantizing both weights and activations to\n4-bit precision, reducing the model size and computation cost. To mitigate the\nperformance degradation, we introduce activation polishing and compensation\nalgorithm applied before and after activation quantization, as well as a weight\nreconstruction method for minimizing errors in weight quantization.\nFurthermore, we design a flexible and programmable hardware accelerator by\nsupporting kernel fusion and customized instruction programmability, enhancing\nthroughput and efficiency. Experimental results demonstrate that our framework\nachieves competitive accuracy while enabling fast inference and higher energy\nefficiency on ASICs, bridging the gap between high-performance depth estimation\nand practical edge-device applicability. Code:\nhttps://github.com/shawnricecake/quart-depth", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16710", "pdf": "https://arxiv.org/pdf/2503.16710", "abs": "https://arxiv.org/abs/2503.16710", "authors": ["Yanyan Li", "Youxu Fang", "Zunjie Zhu", "Kunyi Li", "Yong Ding", "Federico Tombari"], "title": "4D Gaussian Splatting SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Simultaneously localizing camera poses and constructing Gaussian radiance\nfields in dynamic scenes establish a crucial bridge between 2D images and the\n4D real world. Instead of removing dynamic objects as distractors and\nreconstructing only static environments, this paper proposes an efficient\narchitecture that incrementally tracks camera poses and establishes the 4D\nGaussian radiance fields in unknown scenarios by using a sequence of RGB-D\nimages. First, by generating motion masks, we obtain static and dynamic priors\nfor each pixel. To eliminate the influence of static scenes and improve the\nefficiency on learning the motion of dynamic objects, we classify the Gaussian\nprimitives into static and dynamic Gaussian sets, while the sparse control\npoints along with an MLP is utilized to model the transformation fields of the\ndynamic Gaussians. To more accurately learn the motion of dynamic Gaussians, a\nnovel 2D optical flow map reconstruction algorithm is designed to render\noptical flows of dynamic objects between neighbor images, which are further\nused to supervise the 4D Gaussian radiance fields along with traditional\nphotometric and geometric constraints. In experiments, qualitative and\nquantitative evaluation results show that the proposed method achieves robust\ntracking and high-quality view synthesis performance in real-world\nenvironments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16622", "pdf": "https://arxiv.org/pdf/2503.16622", "abs": "https://arxiv.org/abs/2503.16622", "authors": ["Michele Fiori", "Gabriele Civitarese", "Priyankar Choudhary", "Claudio Bettini"], "title": "Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning\nof machine learning models. In IoT systems, XAI improves the transparency of\nmodels processing sensor data from multiple heterogeneous devices, ensuring\nend-users understand and trust their outputs. Among the many applications, XAI\nhas also been applied to sensor-based Activities of Daily Living (ADLs)\nrecognition in smart homes. Existing approaches highlight which sensor events\nare most important for each predicted activity, using simple rules to convert\nthese events into natural language explanations for non-expert users. However,\nthese methods produce rigid explanations lacking natural language flexibility\nand are not scalable. With the recent rise of Large Language Models (LLMs), it\nis worth exploring whether they can enhance explanation generation, considering\ntheir proven knowledge of human activities. This paper investigates potential\napproaches to combine XAI and LLMs for sensor-based ADL recognition. We\nevaluate if LLMs can be used: a) as explainable zero-shot ADL recognition\nmodels, avoiding costly labeled data collection, and b) to automate the\ngeneration of explanations for existing data-driven XAI approaches when\ntraining data is available and the goal is higher recognition rates. Our\ncritical evaluation provides insights into the benefits and challenges of using\nLLMs for explainable ADL recognition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16776", "pdf": "https://arxiv.org/pdf/2503.16776", "abs": "https://arxiv.org/abs/2503.16776", "authors": ["Valentin Bieri", "Marco Zamboni", "Nicolas S. Blumer", "Qingxuan Chen", "Francis Engelmann"], "title": "OpenCity3D: What do Vision-Language Models know about Urban Environments?", "categories": ["cs.CV"], "comment": "Published at WACV 2025", "summary": "Vision-language models (VLMs) show great promise for 3D scene understanding\nbut are mainly applied to indoor spaces or autonomous driving, focusing on\nlow-level tasks like segmentation. This work expands their use to urban-scale\nenvironments by leveraging 3D reconstructions from multi-view aerial imagery.\nWe propose OpenCity3D, an approach that addresses high-level tasks, such as\npopulation density estimation, building age classification, property price\nprediction, crime rate assessment, and noise pollution evaluation. Our findings\nhighlight OpenCity3D's impressive zero-shot and few-shot capabilities,\nshowcasing adaptability to new contexts. This research establishes a new\nparadigm for language-driven urban analytics, enabling applications in\nplanning, policy, and environmental monitoring. See our project page:\nopencity3d.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16655", "pdf": "https://arxiv.org/pdf/2503.16655", "abs": "https://arxiv.org/abs/2503.16655", "authors": ["Maxime Delmas", "Magdalena Wysocka", "Danilo Gusicuma", "Andr√© Freitas"], "title": "Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs", "categories": ["cs.CL"], "comment": "11 pages, 9 figures, 3 tables", "summary": "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16780", "pdf": "https://arxiv.org/pdf/2503.16780", "abs": "https://arxiv.org/abs/2503.16780", "authors": ["Uihyun Cho", "Namhun Kim"], "title": "A-IDE : Agent-Integrated Denoising Experts", "categories": ["cs.CV"], "comment": "10 pages, 11 figures", "summary": "Recent advances in deep-learning based denoising methods have improved\nLow-Dose CT image quality. However, due to distinct HU distributions and\ndiverse anatomical characteristics, a single model often struggles to\ngeneralize across multiple anatomies. To address this limitation, we introduce\n\\textbf{Agent-Integrated Denoising Experts (A-IDE)} framework, which integrates\nthree anatomical region-specialized RED-CNN models under the management of\ndecision-making LLM agent. The agent analyzes semantic cues from BiomedCLIP to\ndynamically route incoming LDCT scans to the most appropriate expert model. We\nhighlight three major advantages of our approach. A-IDE excels in\nheterogeneous, data-scarce environments. The framework automatically prevents\noverfitting by distributing tasks among multiple experts. Finally, our\nLLM-driven agentic pipeline eliminates the need for manual interventions.\nExperimental evaluations on the Mayo-2016 dataset confirm that A-IDE achieves\nsuperior performance in RMSE, PSNR, and SSIM compared to a single unified\ndenoiser.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16674", "pdf": "https://arxiv.org/pdf/2503.16674", "abs": "https://arxiv.org/abs/2503.16674", "authors": ["Molly Kennedy", "Ayyoob Imani", "Timo Spinde", "Hinrich Sch√ºtze"], "title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets", "categories": ["cs.CL"], "comment": null, "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16782", "pdf": "https://arxiv.org/pdf/2503.16782", "abs": "https://arxiv.org/abs/2503.16782", "authors": ["Enguang Wang", "Zhimao Peng", "Zhengyuan Xie", "Haori Lu", "Fei Yang", "Xialei Liu"], "title": "Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data\ncontaining both seen and novel categories. Although existing methods perform\nwell on generic datasets, they struggle in fine-grained scenarios. We attribute\nthis difficulty to their reliance on contrastive learning over global image\nfeatures to automatically capture discriminative cues, which fails to capture\nthe subtle local differences essential for distinguishing fine-grained\ncategories. Therefore, in this paper, we propose incorporating part knowledge\nto address fine-grained GCD, which introduces two key challenges: the absence\nof annotations for novel classes complicates the extraction of the part\nfeatures, and global contrastive learning prioritizes holistic feature\ninvariance, inadvertently suppressing discriminative local part patterns. To\naddress these challenges, we propose PartGCD, including 1) Adaptive Part\nDecomposition, which automatically extracts class-specific semantic parts via\nGaussian Mixture Models, and 2) Part Discrepancy Regularization, enforcing\nexplicit separation between part features to amplify fine-grained local part\ndistinctions.\n  Experiments demonstrate state-of-the-art performance across multiple\nfine-grained benchmarks while maintaining competitiveness on generic datasets,\nvalidating the effectiveness and robustness of our approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16728", "pdf": "https://arxiv.org/pdf/2503.16728", "abs": "https://arxiv.org/abs/2503.16728", "authors": ["Emiel van Miltenburg", "Chenghua Lin"], "title": "Natural Language Generation", "categories": ["cs.CL"], "comment": "3 pages + references. Submitted for publication in the Encyclopedia\n  of Language & Linguistics", "summary": "This article provides a brief overview of the field of Natural Language\nGeneration. The term Natural Language Generation (NLG), in its broadest\ndefinition, refers to the study of systems that verbalize some form of\ninformation through natural language. That information could be stored in a\nlarge database or knowledge graph (in data-to-text applications), but NLG\nresearchers may also study summarisation (text-to-text) or image captioning\n(image-to-text), for example. As a subfield of Natural Language Processing, NLG\nis closely related to other sub-disciplines such as Machine Translation (MT)\nand Dialog Systems. Some NLG researchers exclude MT from their definition of\nthe field, since there is no content selection involved where the system has to\ndetermine what to say. Conversely, dialog systems do not typically fall under\nthe header of Natural Language Generation since NLG is just one component of\ndialog systems (the others being Natural Language Understanding and Dialog\nManagement). However, with the rise of Large Language Models (LLMs), different\nsubfields of Natural Language Processing have converged on similar\nmethodologies for the production of natural language and the evaluation of\nautomatically generated text.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16745", "pdf": "https://arxiv.org/pdf/2503.16745", "abs": "https://arxiv.org/abs/2503.16745", "authors": ["Shiva Upadhye", "Jiaxuan Li", "Richard Futrell"], "title": "SPACER: A Parallel Dataset of Speech Production And Comprehension of Error Repairs", "categories": ["cs.CL"], "comment": "11 pages, 11 figures", "summary": "Speech errors are a natural part of communication, yet they rarely lead to\ncomplete communicative failure because both speakers and comprehenders can\ndetect and correct errors. Although prior research has examined error\nmonitoring and correction in production and comprehension separately,\nintegrated investigation of both systems has been impeded by the scarcity of\nparallel data. In this study, we present SPACER, a parallel dataset that\ncaptures how naturalistic speech errors are corrected by both speakers and\ncomprehenders. We focus on single-word substitution errors extracted from the\nSwitchboard corpus, accompanied by speaker's self-repairs and comprehenders'\nresponses from an offline text-editing experiment. Our exploratory analysis\nsuggests asymmetries in error correction strategies: speakers are more likely\nto repair errors that introduce greater semantic and phonemic deviations,\nwhereas comprehenders tend to correct errors that are phonemically similar to\nmore plausible alternatives or do not fit into prior contexts. Our dataset\nenables future research on integrated approaches toward studying language\nproduction and comprehension.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16816", "pdf": "https://arxiv.org/pdf/2503.16816", "abs": "https://arxiv.org/abs/2503.16816", "authors": ["Yi Niu", "Jiashuai Liu", "Yingkang Zhan", "Jiangbo Shi", "Di Zhang", "Ines Machado", "Mireia Crispin-Ortuzar", "Chen Li", "Zeyu Gao"], "title": "ST-Prompt Guided Histological Hypergraph Learning for Spatial Gene Expression Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Spatial Transcriptomics (ST) reveals the spatial distribution of gene\nexpression in tissues, offering critical insights into biological processes and\ndisease mechanisms. However, predicting ST from H\\&E-stained histology images\nis challenging due to the heterogeneous relationship between histomorphology\nand gene expression, which arises from substantial variability across different\npatients and tissue sections. A more practical and valuable approach is to\nutilize ST data from a few local regions to predict the spatial transcriptomic\nlandscape across the remaining regions in H&E slides. In response, we propose\nPHG2ST, an ST-prompt guided histological hypergraph learning framework, which\nleverages sparse ST signals as prompts to guide histological hypergraph\nlearning for global spatial gene expression prediction. Our framework fuses\nhistological hypergraph representations at multiple scales through a masked\nST-prompt encoding mechanism, improving robustness and generalizability.\nBenchmark evaluations on two public ST datasets demonstrate that PHG2ST\noutperforms the existing state-of-the-art methods and closely aligns with the\nground truth. These results underscore the potential of leveraging sparse local\nST data for scalable and cost-effective spatial gene expression mapping in\nreal-world biomedical applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16883", "pdf": "https://arxiv.org/pdf/2503.16883", "abs": "https://arxiv.org/abs/2503.16883", "authors": ["Deniss Ruder", "Andero Uusberg", "Kairit Sirts"], "title": "Assessing the Reliability and Validity of GPT-4 in Annotating Emotion Appraisal Ratings", "categories": ["cs.CL"], "comment": null, "summary": "Appraisal theories suggest that emotions arise from subjective evaluations of\nevents, referred to as appraisals. The taxonomy of appraisals is quite diverse,\nand they are usually given ratings on a Likert scale to be annotated in an\nexperiencer-annotator or reader-annotator paradigm. This paper studies GPT-4 as\na reader-annotator of 21 specific appraisal ratings in different prompt\nsettings, aiming to evaluate and improve its performance compared to human\nannotators. We found that GPT-4 is an effective reader-annotator that performs\nclose to or even slightly better than human annotators, and its results can be\nsignificantly improved by using a majority voting of five completions. GPT-4\nalso effectively predicts appraisal ratings and emotion labels using a single\nprompt, but adding instruction complexity results in poorer performance. We\nalso found that longer event descriptions lead to more accurate annotations for\nboth model and human annotator ratings. This work contributes to the growing\nusage of LLMs in psychology and the strategies for improving GPT-4 performance\nin annotating appraisals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16852", "pdf": "https://arxiv.org/pdf/2503.16852", "abs": "https://arxiv.org/abs/2503.16852", "authors": ["Jiaxi Li", "Di Lin", "Hao Chen", "Hongying Liu", "Liang Wan", "Wei Feng"], "title": "Casual Inference via Style Bias Deconfounding for Domain Generalization", "categories": ["cs.CV", "cs.AI"], "comment": "under review", "summary": "Deep neural networks (DNNs) often struggle with out-of-distribution data,\nlimiting their reliability in diverse realworld applications. To address this\nissue, domain generalization methods have been developed to learn\ndomain-invariant features from single or multiple training domains, enabling\ngeneralization to unseen testing domains. However, existing approaches usually\noverlook the impact of style frequency within the training set. This oversight\npredisposes models to capture spurious visual correlations caused by style\nconfounding factors, rather than learning truly causal representations, thereby\nundermining inference reliability. In this work, we introduce Style\nDeconfounding Causal Learning (SDCL), a novel causal inference-based framework\ndesigned to explicitly address style as a confounding factor. Our approaches\nbegins with constructing a structural causal model (SCM) tailored to the domain\ngeneralization problem and applies a backdoor adjustment strategy to account\nfor style influence. Building on this foundation, we design a style-guided\nexpert module (SGEM) to adaptively clusters style distributions during\ntraining, capturing the global confounding style. Additionally, a back-door\ncausal learning module (BDCL) performs causal interventions during feature\nextraction, ensuring fair integration of global confounding styles into sample\npredictions, effectively reducing style bias. The SDCL framework is highly\nversatile and can be seamlessly integrated with state-of-the-art data\naugmentation techniques. Extensive experiments across diverse natural and\nmedical image recognition tasks validate its efficacy, demonstrating superior\nperformance in both multi-domain and the more challenging single-domain\ngeneralization scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17073", "pdf": "https://arxiv.org/pdf/2503.17073", "abs": "https://arxiv.org/abs/2503.17073", "authors": ["Jonas Wallat", "Abdelrahman Abdallah", "Adam Jatowt", "Avishek Anand"], "title": "A Study into Investigating Temporal Robustness of LLMs", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "8 pages", "summary": "Large Language Models (LLMs) encapsulate a surprising amount of factual world\nknowledge. However, their performance on temporal questions and historical\nknowledge is limited because they often cannot understand temporal scope and\norientation or neglect the temporal aspect altogether. In this study, we aim to\nmeasure precisely how robust LLMs are for question answering based on their\nability to process temporal information and perform tasks requiring temporal\nreasoning and temporal factual knowledge. Specifically, we design eight\ntime-sensitive robustness tests for factual information to check the\nsensitivity of six popular LLMs in the zero-shot setting. Overall, we find LLMs\nlacking temporal robustness, especially to temporal reformulations and the use\nof different granularities of temporal references. We show how a selection of\nthese eight tests can be used automatically to judge a model's temporal\nrobustness for user questions on the fly. Finally, we apply the findings of\nthis study to improve the temporal QA performance by up to 55 percent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16916", "pdf": "https://arxiv.org/pdf/2503.16916", "abs": "https://arxiv.org/abs/2503.16916", "authors": ["Xiaoyong Chen", "Yong Guo", "Jiaming Liang", "Sitong Zhuang", "Runhao Zeng", "Xiping Hu"], "title": "Temporal Action Detection Model Compression by Progressive Block Drop", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Temporal action detection (TAD) aims to identify and localize action\ninstances in untrimmed videos, which is essential for various video\nunderstanding tasks. However, recent improvements in model performance, driven\nby larger feature extractors and datasets, have led to increased computational\ndemands. This presents a challenge for applications like autonomous driving and\nrobotics, which rely on limited computational resources. While existing channel\npruning methods can compress these models, reducing the number of channels\noften hinders the parallelization efficiency of GPU, due to the inefficient\nmultiplication between small matrices. Instead of pruning channels, we propose\na Progressive Block Drop method that reduces model depth while retaining layer\nwidth. In this way, we still use large matrices for computation but reduce the\nnumber of multiplications. Our approach iteratively removes redundant blocks in\ntwo steps: first, we drop blocks with minimal impact on model performance; and\nsecond, we employ a parameter-efficient cross-depth alignment technique,\nfine-tuning the pruned model to restore model accuracy. Our method achieves a\n25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and\nActivityNet-1.3) to achieve lossless compression. More critically, we\nempirically show that our method is orthogonal to channel pruning methods and\ncan be combined with it to yield further efficiency gains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17222", "pdf": "https://arxiv.org/pdf/2503.17222", "abs": "https://arxiv.org/abs/2503.17222", "authors": ["Sonish Sivarajkumar", "Kimia Ameri", "Chuqin Li", "Yanshan Wang", "Min Jiang"], "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading\ncause of mortality globally, necessitating meticulous monitoring and\nadjudication in clinical trials. This process, traditionally performed manually\nby clinical experts, is time-consuming, resource-intensive, and prone to\ninter-reviewer variability, potentially introducing bias and hindering trial\nprogress. This study addresses these critical limitations by presenting a novel\nframework for automating the adjudication of cardiovascular events in clinical\ntrials using Large Language Models (LLMs). We developed a two-stage approach:\nfirst, employing an LLM-based pipeline for event information extraction from\nunstructured clinical data and second, using an LLM-based adjudication process\nguided by a Tree of Thoughts approach and clinical endpoint committee (CEC)\nguidelines. Using cardiovascular event-specific clinical trial data, the\nframework achieved an F1-score of 0.82 for event extraction and an accuracy of\n0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel,\nautomated metric specifically designed for evaluating the quality of\nAI-generated clinical reasoning in adjudicating cardiovascular events. This\napproach demonstrates significant potential for substantially reducing\nadjudication time and costs while maintaining high-quality, consistent, and\nauditable outcomes in clinical trials. The reduced variability and enhanced\nstandardization also allow for faster identification and mitigation of risks\nassociated with cardiovascular therapies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17239", "pdf": "https://arxiv.org/pdf/2503.17239", "abs": "https://arxiv.org/abs/2503.17239", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche"], "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16929", "pdf": "https://arxiv.org/pdf/2503.16929", "abs": "https://arxiv.org/abs/2503.16929", "authors": ["Shicheng Li", "Lei Li", "Kun Ouyang", "Shuhuai Ren", "Yuanxin Liu", "Yuanxing Zhang", "Fuzheng Zhang", "Lingpeng Kong", "Qi Liu", "Xu Sun"], "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16930", "pdf": "https://arxiv.org/pdf/2503.16930", "abs": "https://arxiv.org/abs/2503.16930", "authors": ["Haijin Zeng", "Xiangming Wang", "Yongyong Chen", "Jingyong Su", "Jie Liu"], "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Dynamic image degradations, including noise, blur and lighting\ninconsistencies, pose significant challenges in image restoration, often due to\nsensor limitations or adverse environmental conditions. Existing Deep Unfolding\nNetworks (DUNs) offer stable restoration performance but require manual\nselection of degradation matrices for each degradation type, limiting their\nadaptability across diverse scenarios. To address this issue, we propose the\nVision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for\nhandling multiple degradation types simultaneously. VLU-Net leverages a\nVision-Language Model (VLM) refined on degraded image-text pairs to align image\nfeatures with degradation descriptions, selecting the appropriate transform for\ntarget degradation. By integrating an automatic VLM-based gradient estimation\nstrategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net\neffectively tackles complex multi-degradation restoration tasks while\nmaintaining interpretability. Furthermore, we design a hierarchical feature\nunfolding structure to enhance VLU-Net framework, efficiently synthesizing\ndegradation patterns across various levels. VLU-Net is the first all-in-one DUN\nframework and outperforms current leading one-by-one and all-in-one end-to-end\nmethods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L\nderaining dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16457", "pdf": "https://arxiv.org/pdf/2503.16457", "abs": "https://arxiv.org/abs/2503.16457", "authors": ["Iago Alves Brito", "Julia Soares Dollis", "Fernanda Bufon F√§rber", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galv√£o Filho"], "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "The integration of large language models (LLMs) into virtual reality (VR)\nenvironments has opened new pathways for creating more immersive and\ninteractive digital humans. By leveraging the generative capabilities of LLMs\nalongside multimodal outputs such as facial expressions and gestures, virtual\nagents can simulate human-like personalities and emotions, fostering richer and\nmore engaging user experiences. This paper provides a comprehensive review of\nmethods for enabling digital humans to adopt nuanced personality traits,\nexploring approaches such as zero-shot, few-shot, and fine-tuning.\nAdditionally, it highlights the challenges of integrating LLM-driven\npersonality traits into VR, including computational demands, latency issues,\nand the lack of standardized evaluation frameworks for multimodal interactions.\nBy addressing these gaps, this work lays a foundation for advancing\napplications in education, therapy, and gaming, while fostering\ninterdisciplinary collaboration to redefine human-computer interaction in VR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16463", "pdf": "https://arxiv.org/pdf/2503.16463", "abs": "https://arxiv.org/abs/2503.16463", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "30 pages", "summary": "Recent advances in large language models (LLMs) have shown promising results\nin medical diagnosis, with some studies indicating superior performance\ncompared to human physicians in specific scenarios. However, the diagnostic\ncapabilities of LLMs are often overestimated, as their performance\nsignificantly deteriorates in interactive diagnostic settings that require\nactive information gathering. This study investigates the underlying mechanisms\nbehind the performance degradation phenomenon and proposes a solution. We\nidentified that the primary deficiency of LLMs lies in the initial diagnosis\nphase, particularly in information-gathering efficiency and initial diagnosis\nformation, rather than in the subsequent differential diagnosis phase. To\naddress this limitation, we developed a plug-and-play method enhanced (PPME)\nLLM agent, leveraging over 3.5 million electronic medical records from Chinese\nand American healthcare facilities. Our approach integrates specialized models\nfor initial disease diagnosis and inquiry into the history of the present\nillness, trained through supervised and reinforcement learning techniques. The\nexperimental results indicate that the PPME LLM achieved over 30% improvement\ncompared to baselines. The final diagnostic accuracy of the PPME LLM in\ninteractive diagnostic scenarios approached levels comparable to those achieved\nusing complete clinical data. These findings suggest a promising potential for\ndeveloping autonomous diagnostic systems, although further validation studies\nare needed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16464", "pdf": "https://arxiv.org/pdf/2503.16464", "abs": "https://arxiv.org/abs/2503.16464", "authors": ["Shinnosuke Sawano", "Satoshi Kodera"], "title": "Human-Centered AI in Multidisciplinary Medical Discussions: Evaluating the Feasibility of a Chat-Based Approach to Case Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "11 pages, 2 figures, 3 tables, 2 supplemental figures", "summary": "In this study, we investigate the feasibility of using a human-centered\nartificial intelligence (AI) chat platform where medical specialists\ncollaboratively assess complex cases. As the target population for this\nplatform, we focus on patients with cardiovascular diseases who are in a state\nof multimorbidity, that is, suffering from multiple chronic conditions. We\nevaluate simulated cases with multiple diseases using a chat application by\ncollaborating with physicians to assess feasibility, efficiency gains through\nAI utilization, and the quantification of discussion content. We constructed\nsimulated cases based on past case reports, medical errors reports and complex\ncases of cardiovascular diseases experienced by the physicians. The analysis of\ndiscussions across five simulated cases demonstrated a significant reduction in\nthe time required for summarization using AI, with an average reduction of\n79.98\\%. Additionally, we examined hallucination rates in AI-generated\nsummaries used in multidisciplinary medical discussions. The overall\nhallucination rate ranged from 1.01\\% to 5.73\\%, with an average of 3.62\\%,\nwhereas the harmful hallucination rate varied from 0.00\\% to 2.09\\%, with an\naverage of 0.49\\%. Furthermore, morphological analysis demonstrated that\nmultidisciplinary assessments enabled a more complex and detailed\nrepresentation of medical knowledge compared with single physician assessments.\nWe examined structural differences between multidisciplinary and single\nphysician assessments using centrality metrics derived from the knowledge\ngraph. In this study, we demonstrated that AI-assisted summarization\nsignificantly reduced the time required for medical discussions while\nmaintaining structured knowledge representation. These findings can support the\nfeasibility of AI-assisted chat-based discussions as a human-centered approach\nto multidisciplinary medical decision-making.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16975", "pdf": "https://arxiv.org/pdf/2503.16975", "abs": "https://arxiv.org/abs/2503.16975", "authors": ["Xiaofeng Mao", "Yuefeng Chen", "Rong Zhang", "Hui Xue", "Zhao Li", "Hang Su"], "title": "EasyRobust: A Comprehensive and Easy-to-use Toolkit for Robust and Generalized Vision", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) has shown great promise in computer vision tasks.\nHowever, machine vision achieved by DNNs cannot be as robust as human\nperception. Adversarial attacks and data distribution shifts have been known as\ntwo major scenarios which degrade machine performance and obstacle the wide\ndeployment of machines \"in the wild\". In order to break these obstructions and\nfacilitate the research of model robustness, we develop EasyRobust, a\ncomprehensive and easy-to-use toolkit for training, evaluation and analysis of\nrobust vision models. EasyRobust targets at two types of robustness: 1)\nAdversarial robustness enables the model to defense against malicious inputs\ncrafted by worst-case perturbations, also known as adversarial examples; 2)\nNon-adversarial robustness enhances the model performance on natural test\nimages with corruptions or distribution shifts. Thorough benchmarks on image\nclassification enable EasyRobust to provide an accurate robustness evaluation\non vision models. We wish our EasyRobust can help for training\npractically-robust models and promote academic and industrial progress in\nclosing the gap between human and machine vision. Codes and models of\nEasyRobust have been open-sourced in https://github.com/alibaba/easyrobust.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16983", "pdf": "https://arxiv.org/pdf/2503.16983", "abs": "https://arxiv.org/abs/2503.16983", "authors": ["Xu Zhang", "Hao Zhou", "Haoming Qin", "Xiaobin Lu", "Jiaxing Yan", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Enabling Versatile Controls for Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Codes and Supplementary Material:\n  http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl", "summary": "Despite substantial progress in text-to-video generation, achieving precise\nand flexible control over fine-grained spatiotemporal attributes remains a\nsignificant unresolved challenge in video generation research. To address these\nlimitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework\ndesigned to enable fine-grained control over pre-trained video diffusion models\nin a unified manner. VCtrl integrates diverse user-specified control\nsignals-such as Canny edges, segmentation masks, and human keypoints-into\npretrained video diffusion models via a generalizable conditional module\ncapable of uniformly encoding multiple types of auxiliary signals without\nmodifying the underlying generator. Additionally, we design a unified control\nsignal encoding pipeline and a sparse residual connection mechanism to\nefficiently incorporate control representations. Comprehensive experiments and\nhuman evaluations demonstrate that VCtrl effectively enhances controllability\nand generation quality. The source code and pre-trained models are publicly\navailable and implemented using the PaddlePaddle framework at\nhttp://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16563", "pdf": "https://arxiv.org/pdf/2503.16563", "abs": "https://arxiv.org/abs/2503.16563", "authors": ["Aahan Singh", "Engin Tekin", "Maryam Nadeem", "Nancy A. ElNaker", "Mohammad Amaan Sayeed", "Natalia Vassilieva", "Boulbaba Ben Amor"], "title": "Chem42: a Family of chemical Language Models for Target-aware Ligand Generation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Revolutionizing drug discovery demands more than just understanding molecular\ninteractions - it requires generative models that can design novel ligands\ntailored to specific biological targets. While chemical Language Models (cLMs)\nhave made strides in learning molecular properties, most fail to incorporate\ntarget-specific insights, restricting their ability to drive de-novo ligand\ngeneration. Chem42, a cutting-edge family of generative chemical Language\nModels, is designed to bridge this gap. By integrating atomic-level\ninteractions with multimodal inputs from Prot42, a complementary protein\nLanguage Model, Chem42 achieves a sophisticated cross-modal representation of\nmolecular structures, interactions, and binding patterns. This innovative\nframework enables the creation of structurally valid, synthetically accessible\nligands with enhanced target specificity. Evaluations across diverse protein\ntargets confirm that Chem42 surpasses existing approaches in chemical validity,\ntarget-aware design, and predicted binding affinity. By reducing the search\nspace of viable drug candidates, Chem42 could accelerate the drug discovery\npipeline, offering a powerful generative AI tool for precision medicine. Our\nChem42 models set a new benchmark in molecule property prediction, conditional\nmolecule generation, and target-aware ligand design. The models are publicly\navailable at huggingface.co/inceptionai.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17027", "pdf": "https://arxiv.org/pdf/2503.17027", "abs": "https://arxiv.org/abs/2503.17027", "authors": ["Ziteng Cui", "Jianfei Yang", "Tatsuya Harada"], "title": "RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark", "categories": ["cs.CV"], "comment": "23 pages, 17 figures, extension of ECCV 2024 work: arXiv:2408.14802", "summary": "In the computer vision community, the preference for pre-training visual\nmodels has largely shifted toward sRGB images due to their ease of acquisition\nand compact storage. However, camera RAW images preserve abundant physical\ndetails across diverse real-world scenarios. Despite this, most existing visual\nperception methods that utilize RAW data directly integrate image signal\nprocessing (ISP) stages with subsequent network modules, often overlooking\npotential synergies at the model level. Building on recent advances in\nadapter-based methodologies in both NLP and computer vision, we propose\nRAW-Adapter, a novel framework that incorporates learnable ISP modules as\ninput-level adapters to adjust RAW inputs. At the same time, it employs\nmodel-level adapters to seamlessly bridge ISP processing with high-level\ndownstream architectures. Moreover, RAW-Adapter serves as a general framework\napplicable to various computer vision frameworks.\n  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based\ncommon corruptions, including lightness degradations, weather effects,\nblurriness, camera imaging degradations, and variations in camera color\nresponse. Using this benchmark, we systematically compare the performance of\nRAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based\nhigh-level vision algorithms. Additionally, we propose a RAW-based data\naugmentation strategy to further enhance RAW-Adapter's performance and improve\nits out-of-domain (OOD) generalization ability. Extensive experiments\nsubstantiate the effectiveness and efficiency of RAW-Adapter, highlighting its\nrobust performance across diverse scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16565", "pdf": "https://arxiv.org/pdf/2503.16565", "abs": "https://arxiv.org/abs/2503.16565", "authors": ["Kirill Vishniakov", "Boulbaba Ben Amor", "Engin Tekin", "Nancy A. ElNaker", "Karthik Viswanathan", "Aleksandr Medvedev", "Aahan Singh", "Maryam Nadeem", "Mohammad Amaan Sayeed", "Praveenkumar Kanithi", "Tiago Magalhaes", "Natalia Vassilieva", "Dwarikanath Mahapatra", "Marco Pimentel", "and Shadab Khan"], "title": "Gene42: Long-Range Genomic Foundation Model With Dense Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Gene42, a novel family of Genomic Foundation Models (GFMs)\ndesigned to manage context lengths of up to 192,000 base pairs (bp) at a\nsingle-nucleotide resolution. Gene42 models utilize a decoder-only\n(LLaMA-style) architecture with a dense self-attention mechanism. Initially\ntrained on fixed-length sequences of 4,096 bp, our models underwent continuous\npretraining to extend the context length to 192,000 bp. This iterative\nextension allowed for the comprehensive processing of large-scale genomic data\nand the capture of intricate patterns and dependencies within the human genome.\nGene42 is the first dense attention model capable of handling such extensive\nlong context lengths in genomics, challenging state-space models that often\nrely on convolutional operators among other mechanisms. Our pretrained models\nexhibit notably low perplexity values and high reconstruction accuracy,\nhighlighting their strong ability to model genomic data. Extensive experiments\non various genomic benchmarks have demonstrated state-of-the-art performance\nacross multiple tasks, including biotype classification, regulatory region\nidentification, chromatin profiling prediction, variant pathogenicity\nprediction, and species classification. The models are publicly available at\nhuggingface.co/inceptionai.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17029", "pdf": "https://arxiv.org/pdf/2503.17029", "abs": "https://arxiv.org/abs/2503.17029", "authors": ["Junjie Hu", "Shuyong Gao", "Qianyu Guo", "Yan Wang", "Qishan Wang", "Yuang Feng", "Wenqiang Zhang"], "title": "AnimatePainter: A Self-Supervised Rendering Framework for Reconstructing Painting Process", "categories": ["cs.CV"], "comment": null, "summary": "Humans can intuitively decompose an image into a sequence of strokes to\ncreate a painting, yet existing methods for generating drawing processes are\nlimited to specific data types and often rely on expensive human-annotated\ndatasets. We propose a novel self-supervised framework for generating drawing\nprocesses from any type of image, treating the task as a video generation\nproblem. Our approach reverses the drawing process by progressively removing\nstrokes from a reference image, simulating a human-like creation sequence.\nCrucially, our method does not require costly datasets of real human drawing\nprocesses; instead, we leverage depth estimation and stroke rendering to\nconstruct a self-supervised dataset. We model human drawings as \"refinement\"\nand \"layering\" processes and introduce depth fusion layers to enable video\ngeneration models to learn and replicate human drawing behavior. Extensive\nexperiments validate the effectiveness of our approach, demonstrating its\nability to generate realistic drawings without the need for real drawing\nprocess data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16689", "pdf": "https://arxiv.org/pdf/2503.16689", "abs": "https://arxiv.org/abs/2503.16689", "authors": ["Tianze Luo", "Xingchen Miao", "Wenbo Duan"], "title": "WaveFM: A High-Fidelity and Efficient Vocoder Based on Flow Matching", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to the main conference of NAACL 2025. The codes are\n  available at https://github.com/luotianze666/WaveFM", "summary": "Flow matching offers a robust and stable approach to training diffusion\nmodels. However, directly applying flow matching to neural vocoders can result\nin subpar audio quality. In this work, we present WaveFM, a reparameterized\nflow matching model for mel-spectrogram conditioned speech synthesis, designed\nto enhance both sample quality and generation speed for diffusion vocoders.\nSince mel-spectrograms represent the energy distribution of waveforms, WaveFM\nadopts a mel-conditioned prior distribution instead of a standard Gaussian\nprior to minimize unnecessary transportation costs during synthesis. Moreover,\nwhile most diffusion vocoders rely on a single loss function, we argue that\nincorporating auxiliary losses, including a refined multi-resolution STFT loss,\ncan further improve audio quality. To speed up inference without degrading\nsample quality significantly, we introduce a tailored consistency distillation\nmethod for WaveFM. Experiment results demonstrate that our model achieves\nsuperior performance in both quality and efficiency compared to previous\ndiffusion vocoders, while enabling waveform generation in a single inference\nstep.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17032", "pdf": "https://arxiv.org/pdf/2503.17032", "abs": "https://arxiv.org/abs/2503.17032", "authors": ["Jianchuan Chen", "Jingchuan Hu", "Gaige Wang", "Zhonghua Jiang", "Tiansong Zhou", "Zhiwen Chen", "Chengfei Lv"], "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, project page:\n  https://PixelAI-Team.github.io/TaoAvatar", "summary": "Realistic 3D full-body talking avatars hold great potential in AR, with\napplications ranging from e-commerce live streaming to holographic\ncommunication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike\navatar creation, existing methods struggle with fine-grained control of facial\nexpressions and body movements in full-body talking tasks. Additionally, they\noften lack sufficient details and cannot run in real-time on mobile devices. We\npresent TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking\navatar driven by various signals. Our approach starts by creating a\npersonalized clothed human parametric template that binds Gaussians to\nrepresent appearances. We then pre-train a StyleUnet-based network to handle\ncomplex pose-dependent non-rigid deformation, which can capture high-frequency\nappearance details but is too resource-intensive for mobile devices. To\novercome this, we \"bake\" the non-rigid deformations into a lightweight\nMLP-based network using a distillation technique and develop blend shapes to\ncompensate for details. Extensive experiments show that TaoAvatar achieves\nstate-of-the-art rendering quality while running in real-time across various\ndevices, maintaining 90 FPS on high-definition stereo devices such as the Apple\nVision Pro.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16833", "pdf": "https://arxiv.org/pdf/2503.16833", "abs": "https://arxiv.org/abs/2503.16833", "authors": ["Luxi He", "Xiangyu Qi", "Michel Liao", "Inyoung Cheong", "Prateek Mittal", "Danqi Chen", "Peter Henderson"], "title": "The Deployment of End-to-End Audio Language Models Should Take into Account the Principle of Least Privilege", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CY", "eess.AS"], "comment": null, "summary": "We are at a turning point for language models that accept audio input. The\nlatest end-to-end audio language models (Audio LMs) process speech directly\ninstead of relying on a separate transcription step. This shift preserves\ndetailed information, such as intonation or the presence of multiple speakers,\nthat would otherwise be lost in transcription. However, it also introduces new\nsafety risks, including the potential misuse of speaker identity cues and other\nsensitive vocal attributes, which could have legal implications. In this\nposition paper, we urge a closer examination of how these models are built and\ndeployed. We argue that the principle of least privilege should guide decisions\non whether to deploy cascaded or end-to-end models. Specifically, evaluations\nshould assess (1) whether end-to-end modeling is necessary for a given\napplication; and (2), the appropriate scope of information access. Finally, We\nhighlight related gaps in current audio LM benchmarks and identify key open\nresearch questions, both technical and policy-related, that must be addressed\nto enable the responsible deployment of end-to-end Audio LMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17050", "pdf": "https://arxiv.org/pdf/2503.17050", "abs": "https://arxiv.org/abs/2503.17050", "authors": ["Yuang Feng", "Shuyong Gao", "Fuzhen Yan", "Yicheng Song", "Lingyi Hong", "Junjie Hu", "Wenqiang Zhang"], "title": "Scoring, Remember, and Reference: Catching Camouflaged Objects in Videos", "categories": ["cs.CV"], "comment": null, "summary": "Video Camouflaged Object Detection (VCOD) aims to segment objects whose\nappearances closely resemble their surroundings, posing a challenging and\nemerging task. Existing vision models often struggle in such scenarios due to\nthe indistinguishable appearance of camouflaged objects and the insufficient\nexploitation of dynamic information in videos. To address these challenges, we\npropose an end-to-end VCOD framework inspired by human memory-recognition,\nwhich leverages historical video information by integrating memory reference\nframes for camouflaged sequence processing. Specifically, we design a\ndual-purpose decoder that simultaneously generates predicted masks and scores,\nenabling reference frame selection based on scores while introducing auxiliary\nsupervision to enhance feature extraction.Furthermore, this study introduces a\nnovel reference-guided multilevel asymmetric attention mechanism, effectively\nintegrating long-term reference information with short-term motion cues for\ncomprehensive feature extraction. By combining these modules, we develop the\nScoring, Remember, and Reference (SRR) framework, which efficiently extracts\ninformation to locate targets and employs memory guidance to improve subsequent\nprocessing. With its optimized module design and effective utilization of video\ndata, our model achieves significant performance improvements, surpassing\nexisting approaches by 10% on benchmark datasets while requiring fewer\nparameters (54M) and only a single pass through the video. The code will be\nmade publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16875", "pdf": "https://arxiv.org/pdf/2503.16875", "abs": "https://arxiv.org/abs/2503.16875", "authors": ["Jiangcheng Qin", "Xueyuan Zhang", "Baisong Liu", "Jiangbo Qian", "Yangyang Wang"], "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation", "categories": ["cs.IR", "cs.CL", "cs.DC"], "comment": null, "summary": "Accurately predicting click-through rates (CTR) under stringent privacy\nconstraints poses profound challenges, particularly when user-item interactions\nare sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)\nmethods frequently assume homogeneous feature spaces and rely on centralized\ndata sharing, neglecting complex inter-domain discrepancies and the subtle\ntrade-offs imposed by privacy-preserving protocols. Here, we present Federated\nCross-Domain CTR Prediction with Large Language Model Augmentation\n(FedCCTR-LM), a federated framework engineered to address these limitations by\nsynchronizing data augmentation, representation disentanglement, and adaptive\nprivacy protection. Our approach integrates three core innovations. First, the\nPrivacy-Preserving Augmentation Network (PrivAugNet) employs large language\nmodels to enrich user and item representations and expand interaction\nsequences, mitigating data sparsity and feature incompleteness. Second, the\nIndependent Domain-Specific Transformer with Contrastive Learning (IDST-CL)\nmodule disentangles domain-specific and shared user preferences, employing\nintra-domain representation alignment (IDRA) and crossdomain representation\ndisentanglement (CDRD) to refine the learned embeddings and enhance knowledge\ntransfer across domains. Finally, the Adaptive Local Differential Privacy\n(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal\nbalance between rigorous privacy guarantees and predictive accuracy. Empirical\nevaluations on four real-world datasets demonstrate that FedCCTR-LM\nsubstantially outperforms existing baselines, offering robust,\nprivacy-preserving, and generalizable cross-domain CTR prediction in\nheterogeneous, federated environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17080", "pdf": "https://arxiv.org/pdf/2503.17080", "abs": "https://arxiv.org/abs/2503.17080", "authors": ["Gensheng Pei", "Tao Chen", "Yujia Wang", "Xinhao Cai", "Xiangbo Shu", "Tianfei Zhou", "Yazhou Yao"], "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17004", "pdf": "https://arxiv.org/pdf/2503.17004", "abs": "https://arxiv.org/abs/2503.17004", "authors": ["Sophia Rupprecht", "Yassine Hounat", "Monisha Kumar", "Giacomo Lastrucci", "Artur M. Schweidtmann"], "title": "Text2Model: Generating dynamic chemical reactor models using large language models (LLMs)", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "As large language models have shown remarkable capabilities in conversing via\nnatural language, the question arises as to how LLMs could potentially assist\nchemical engineers in research and industry with domain-specific tasks. We\ngenerate dynamic chemical reactor models in Modelica code format from textual\ndescriptions as user input. We fine-tune Llama 3.1 8B Instruct on synthetically\ngenerated Modelica code for different reactor scenarios. We compare the\nperformance of our fine-tuned model to the baseline Llama 3.1 8B Instruct model\nand GPT4o. We manually assess the models' predictions regarding the syntactic\nand semantic accuracy of the generated dynamic models. We find that\nconsiderable improvements are achieved by the fine-tuned model with respect to\nboth the semantic and the syntactic accuracy of the Modelica models. However,\nthe fine-tuned model lacks a satisfactory ability to generalize to unseen\nscenarios compared to GPT4o.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17097", "pdf": "https://arxiv.org/pdf/2503.17097", "abs": "https://arxiv.org/abs/2503.17097", "authors": ["Boyuan Zheng", "Shouyi Lu", "Renbo Huang", "Minqing Huang", "Fan Lu", "Wei Tian", "Guirong Zhuo", "Lu Xiong"], "title": "R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "We introduce R2LDM, an innovative approach for generating dense and accurate\n4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of\nutilizing range images or bird's eye view (BEV) images, we represent both LiDAR\nand 4D radar point clouds using voxel features, which more effectively capture\n3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model\n(LVDM), which performs the diffusion process in the latent space. Additionally,\na novel Latent Point Cloud Reconstruction (LPCR) module is utilized to\nreconstruct point clouds from high-dimensional latent voxel features. As a\nresult, R2LDM effectively generates LiDAR-like point clouds from paired raw\nradar data. We evaluate our approach on two different datasets, and the\nexperimental results demonstrate that our model achieves 6- to 10-fold\ndensification of radar point clouds, outperforming state-of-the-art baselines\nin 4D radar point cloud super-resolution. Furthermore, the enhanced radar point\nclouds generated by our method significantly improve downstream tasks,\nachieving up to 31.7% improvement in point cloud registration recall rate and\n24.9% improvement in object detection accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17106", "pdf": "https://arxiv.org/pdf/2503.17106", "abs": "https://arxiv.org/abs/2503.17106", "authors": ["Yizhe Liu", "Tong Jia", "Da Cai", "Hao Wang", "Dongyue Chen"], "title": "GAA-TSO: Geometry-Aware Assisted Depth Completion for Transparent and Specular Objects", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Transparent and specular objects are frequently encountered in daily life,\nfactories, and laboratories. However, due to the unique optical properties, the\ndepth information on these objects is usually incomplete and inaccurate, which\nposes significant challenges for downstream robotics tasks. Therefore, it is\ncrucial to accurately restore the depth information of transparent and specular\nobjects. Previous depth completion methods for these objects usually use RGB\ninformation as an additional channel of the depth image to perform depth\nprediction. Due to the poor-texture characteristics of transparent and specular\nobjects, these methods that rely heavily on color information tend to generate\nstructure-less depth predictions. Moreover, these 2D methods cannot effectively\nexplore the 3D structure hidden in the depth channel, resulting in depth\nambiguity. To this end, we propose a geometry-aware assisted depth completion\nmethod for transparent and specular objects, which focuses on exploring the 3D\nstructural cues of the scene. Specifically, besides extracting 2D features from\nRGB-D input, we back-project the input depth to a point cloud and build the 3D\nbranch to extract hierarchical scene-level 3D structural features. To exploit\n3D geometric information, we design several gated cross-modal fusion modules to\neffectively propagate multi-level 3D geometric features to the image branch. In\naddition, we propose an adaptive correlation aggregation strategy to\nappropriately assign 3D features to the corresponding 2D features. Extensive\nexperiments on ClearGrasp, OOD, TransCG, and STD datasets show that our method\noutperforms other state-of-the-art methods. We further demonstrate that our\nmethod significantly enhances the performance of downstream robotic grasping\ntasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17132", "pdf": "https://arxiv.org/pdf/2503.17132", "abs": "https://arxiv.org/abs/2503.17132", "authors": ["Siyuan Yang", "Shilin Lu", "Shizheng Wang", "Meng Hwa Er", "Zengwei Zheng", "Alex C. Kot"], "title": "Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.NE"], "comment": null, "summary": "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17153", "pdf": "https://arxiv.org/pdf/2503.17153", "abs": "https://arxiv.org/abs/2503.17153", "authors": ["Fouad Makiyeh", "Huy-Dung Nguyen", "Patrick Chareyre", "Ramin Hasani", "Marc Blanchon", "Daniela Rus"], "title": "Enhancing Steering Estimation with Semantic-Aware GNNs", "categories": ["cs.CV"], "comment": "Submitted to ICCV 2025", "summary": "Steering estimation is a critical task in autonomous driving, traditionally\nrelying on 2D image-based models. In this work, we explore the advantages of\nincorporating 3D spatial information through hybrid architectures that combine\n3D neural network models with recurrent neural networks (RNNs) for temporal\nmodeling, using LiDAR-based point clouds as input. We systematically evaluate\nfour hybrid 3D models, all of which outperform the 2D-only baseline, with the\nGraph Neural Network (GNN) - RNN model yielding the best results.\n  To reduce reliance on LiDAR, we leverage a pretrained unified model to\nestimate depth from monocular images, reconstructing pseudo-3D point clouds. We\nthen adapt the GNN-RNN model, originally designed for LiDAR-based point clouds,\nto work with these pseudo-3D representations, achieving comparable or even\nsuperior performance compared to the LiDAR-based model. Additionally, the\nunified model provides semantic labels for each point, enabling a more\nstructured scene representation. To further optimize graph construction, we\nintroduce an efficient connectivity strategy where connections are\npredominantly formed between points of the same semantic class, with only 20\\%\nof inter-class connections retained. This targeted approach reduces graph\ncomplexity and computational cost while preserving critical spatial\nrelationships.\n  Finally, we validate our approach on the KITTI dataset, achieving a 71%\nimprovement over 2D-only models. Our findings highlight the advantages of 3D\nspatial information and efficient graph construction for steering estimation,\nwhile maintaining the cost-effectiveness of monocular images and avoiding the\nexpense of LiDAR-based systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17182", "pdf": "https://arxiv.org/pdf/2503.17182", "abs": "https://arxiv.org/abs/2503.17182", "authors": ["Patrick Rim", "Hyoungseob Park", "Vadim Ezhov", "Jeffrey Moon", "Alex Wong"], "title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We propose PolyRad, a novel radar-guided depth estimation method that\nintroduces polynomial fitting to transform scaleless depth predictions from\npretrained monocular depth estimation (MDE) models into metric depth maps.\nUnlike existing approaches that rely on complex architectures or expensive\nsensors, our method is grounded in a simple yet fundamental insight: using\npolynomial coefficients predicted from cheap, ubiquitous radar data to\nadaptively adjust depth predictions non-uniformly across depth ranges. Although\nMDE models often infer reasonably accurate local depth structure within each\nobject or local region, they may misalign these regions relative to one\nanother, making a linear scale-and-shift transformation insufficient given\nthree or more of these regions. In contrast, PolyRad generalizes beyond linear\ntransformations and is able to correct such misalignments by introducing\ninflection points. Importantly, our polynomial fitting framework preserves\nstructural consistency through a novel training objective that enforces\nmonotonicity via first-derivative regularization. PolyRad achieves\nstate-of-the-art performance on the nuScenes, ZJU-4DRadarCam, and View-of-Delft\ndatasets, outperforming existing methods by 30.3% in MAE and 37.2% in RMSE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17184", "pdf": "https://arxiv.org/pdf/2503.17184", "abs": "https://arxiv.org/abs/2503.17184", "authors": ["Xueqi Qiu", "Xingyu Miao", "Fan Wan", "Haoran Duan", "Tejal Shah", "Varun Ojhab", "Yang Longa", "Rajiv Ranjan"], "title": "D2Fusion: Dual-domain Fusion with Feature Superposition for Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deepfake detection is crucial for curbing the harm it causes to society.\nHowever, current Deepfake detection methods fail to thoroughly explore artifact\ninformation across different domains due to insufficient intrinsic\ninteractions. These interactions refer to the fusion and coordination after\nfeature extraction processes across different domains, which are crucial for\nrecognizing complex forgery clues. Focusing on more generalized Deepfake\ndetection, in this work, we introduce a novel bi-directional attention module\nto capture the local positional information of artifact clues from the spatial\ndomain. This enables accurate artifact localization, thus addressing the coarse\nprocessing with artifact features. To further address the limitation that the\nproposed bi-directional attention module may not well capture global subtle\nforgery information in the artifact feature (e.g., textures or edges), we\nemploy a fine-grained frequency attention module in the frequency domain. By\ndoing so, we can obtain high-frequency information in the fine-grained\nfeatures, which contains the global and subtle forgery information. Although\nthese features from the diverse domains can be effectively and independently\nimproved, fusing them directly does not effectively improve the detection\nperformance. Therefore, we propose a feature superposition strategy that\ncomplements information from spatial and frequency domains. This strategy turns\nthe feature components into the form of wave-like tokens, which are updated\nbased on their phase, such that the distinctions between authentic and artifact\nfeatures can be amplified. Our method demonstrates significant improvements\nover state-of-the-art (SOTA) methods on five public Deepfake datasets in\ncapturing abnormalities across different manipulated operations and real-life.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17197", "pdf": "https://arxiv.org/pdf/2503.17197", "abs": "https://arxiv.org/abs/2503.17197", "authors": ["Xingchao Yang", "Takafumi Taketomi", "Yuki Endo", "Yoshihiro Kanamori"], "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy", "categories": ["cs.CV"], "comment": "CVPR 2025. Project: https://yangxingchao.github.io/FreeUV-page/", "summary": "Recovering high-quality 3D facial textures from single-view 2D images is a\nchallenging task, especially under constraints of limited data and complex\nfacial details such as makeup, wrinkles, and occlusions. In this paper, we\nintroduce FreeUV, a novel ground-truth-free UV texture recovery framework that\neliminates the need for annotated or synthetic UV data. FreeUV leverages\npre-trained stable diffusion model alongside a Cross-Assembly inference\nstrategy to fulfill this objective. In FreeUV, separate networks are trained\nindependently to focus on realistic appearance and structural consistency, and\nthese networks are combined during inference to generate coherent textures. Our\napproach accurately captures intricate facial features and demonstrates robust\nperformance across diverse poses and occlusions. Extensive experiments validate\nFreeUV's effectiveness, with results surpassing state-of-the-art methods in\nboth quantitative and qualitative metrics. Additionally, FreeUV enables new\napplications, including local editing, facial feature interpolation, and\nmulti-view texture recovery. By reducing data requirements, FreeUV offers a\nscalable solution for generating high-fidelity 3D facial textures suitable for\nreal-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17224", "pdf": "https://arxiv.org/pdf/2503.17224", "abs": "https://arxiv.org/abs/2503.17224", "authors": ["Giacomo Savazzi", "Eugenio Lomurno", "Cristian Sbrolli", "Agnese Chiatti", "Matteo Matteucci"], "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As machine learning models increase in scale and complexity, obtaining\nsufficient training data has become a critical bottleneck due to acquisition\ncosts, privacy constraints, and data scarcity in specialised domains. While\nsynthetic data generation has emerged as a promising alternative, a notable\nperformance gap remains compared to models trained on real data, particularly\nas task complexity grows. Concurrently, Neuro-Symbolic methods, which combine\nneural networks' learning strengths with symbolic reasoning's structured\nrepresentations, have demonstrated significant potential across various\ncognitive tasks. This paper explores the utility of Neuro-Symbolic conditioning\nfor synthetic image dataset generation, focusing specifically on improving the\nperformance of Scene Graph Generation models. The research investigates whether\nstructured symbolic representations in the form of scene graphs can enhance\nsynthetic data quality through explicit encoding of relational constraints. The\nresults demonstrate that Neuro-Symbolic conditioning yields significant\nimprovements of up to +2.59% in standard Recall metrics and +2.83% in No Graph\nConstraint Recall metrics when used for dataset augmentation. These findings\nestablish that merging Neuro-Symbolic and generative approaches produces\nsynthetic data with complementary structural information that enhances model\nperformance when combined with real data, providing a novel approach to\novercome data scarcity limitations even for complex visual reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17349", "pdf": "https://arxiv.org/pdf/2503.17349", "abs": "https://arxiv.org/abs/2503.17349", "authors": ["Jianing Qi", "Jiawei Liu", "Hao Tang", "Zhigang Zhu"], "title": "Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) excel at identifying and describing objects but\nstruggle with spatial reasoning such as accurately understanding the relative\npositions of objects. Inspired by the dual-pathway (ventral-dorsal) model of\nhuman vision, we investigate why VLMs fail spatial tasks despite strong object\nrecognition capabilities. Our interpretability-driven analysis reveals a\ncritical underlying cause: vision embeddings in VLMs are treated primarily as\nsemantic ``bag-of-tokens,\" overshadowing subtle yet crucial positional cues due\nto their disproportionately large embedding norms. We validate this insight\nthrough extensive diagnostic experiments, demonstrating minimal performance\nimpact when token orders or fine-grained spatial details are removed. Guided by\nthese findings, we propose simple, interpretable interventions, including\nnormalizing vision embedding norms and extracting mid-layer spatially rich\nfeatures, to restore spatial awareness. Empirical results on both our synthetic\ndata and standard benchmarks demonstrate improved spatial reasoning\ncapabilities, highlighting the value of interpretability-informed design\nchoices. Our study not only uncovers fundamental limitations in current VLM\narchitectures but also provides actionable insights for enhancing structured\nperception of visual scenes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17358", "pdf": "https://arxiv.org/pdf/2503.17358", "abs": "https://arxiv.org/abs/2503.17358", "authors": ["Jerred Chen", "Ronald Clark"], "title": "Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image", "categories": ["cs.CV"], "comment": "Project page: https://jerredchen.github.io/image-as-imu/", "summary": "In many robotics and VR/AR applications, fast camera motions cause a high\nlevel of motion blur, causing existing camera pose estimation methods to fail.\nIn this work, we propose a novel framework that leverages motion blur as a rich\ncue for motion estimation rather than treating it as an unwanted artifact. Our\napproach works by predicting a dense motion flow field and a monocular depth\nmap directly from a single motion-blurred image. We then recover the\ninstantaneous camera velocity by solving a linear least squares problem under\nthe small motion assumption. In essence, our method produces an IMU-like\nmeasurement that robustly captures fast and aggressive camera movements. To\ntrain our model, we construct a large-scale dataset with realistic synthetic\nmotion blur derived from ScanNet++v2 and further refine our model by training\nend-to-end on real data using our fully differentiable pipeline. Extensive\nevaluations on real-world benchmarks demonstrate that our method achieves\nstate-of-the-art angular and translational velocity estimates, outperforming\ncurrent methods like MASt3R and COLMAP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16488", "pdf": "https://arxiv.org/pdf/2503.16488", "abs": "https://arxiv.org/abs/2503.16488", "authors": ["Kunal Chavan", "Keertan Balaji", "Spoorti Barigidad", "Samba Raju Chiluveru"], "title": "VocalEyes: Enhancing Environmental Perception for the Visually Impaired through Vision-Language Models and Distance-Aware Object Detection", "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": null, "summary": "With an increasing demand for assistive technologies that promote the\nindependence and mobility of visually impaired people, this study suggests an\ninnovative real-time system that gives audio descriptions of a user's\nsurroundings to improve situational awareness. The system acquires live video\ninput and processes it with a quantized and fine-tuned Florence-2 big model,\nadjusted to 4-bit accuracy for efficient operation on low-power edge devices\nsuch as the NVIDIA Jetson Orin Nano. By transforming the video signal into\nframes with a 5-frame latency, the model provides rapid and contextually\npertinent descriptions of objects, pedestrians, and barriers, together with\ntheir estimated distances. The system employs Parler TTS Mini, a lightweight\nand adaptable Text-to-Speech (TTS) solution, for efficient audio feedback. It\naccommodates 34 distinct speaker types and enables customization of speech\ntone, pace, and style to suit user requirements. This study examines the\nquantization and fine-tuning techniques utilized to modify the Florence-2 model\nfor this application, illustrating how the integration of a compact model\narchitecture with a versatile TTS component improves real-time performance and\nuser experience. The proposed system is assessed based on its accuracy,\nefficiency, and usefulness, providing a viable option to aid vision-impaired\nusers in navigating their surroundings securely and successfully.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16537", "pdf": "https://arxiv.org/pdf/2503.16537", "abs": "https://arxiv.org/abs/2503.16537", "authors": ["Grigorii Khvatskii", "Yong Suk Lee", "Corey Angst", "Maria Gibbs", "Robert Landers", "Nitesh V. Chawla"], "title": "Do Multimodal Large Language Models Understand Welding?", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages", "summary": "This paper examines the performance of Multimodal LLMs (MLLMs) in skilled\nproduction work, with a focus on welding. Using a novel data set of real-world\nand online weld images, annotated by a domain expert, we evaluate the\nperformance of two state-of-the-art MLLMs in assessing weld acceptability\nacross three contexts: RV \\& Marine, Aeronautical, and Farming. While both\nmodels perform better on online images, likely due to prior exposure or\nmemorization, they also perform relatively well on unseen, real-world weld\nimages. Additionally, we introduce WeldPrompt, a prompting strategy that\ncombines Chain-of-Thought generation with in-context learning to mitigate\nhallucinations and improve reasoning. WeldPrompt improves model recall in\ncertain contexts but exhibits inconsistent performance across others. These\nresults underscore the limitations and potentials of MLLMs in high-stakes\ntechnical domains and highlight the importance of fine-tuning, domain-specific\ndata, and more sophisticated prompting strategies to improve model reliability.\nThe study opens avenues for further research into multimodal learning in\nindustry applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16635", "pdf": "https://arxiv.org/pdf/2503.16635", "abs": "https://arxiv.org/abs/2503.16635", "authors": ["Yinchi Zhou", "Huidong Xie", "Menghua Xia", "Qiong Liu", "Bo Zhou", "Tianqi Chen", "Jun Hou", "Liang Guo", "Xinyuan Zheng", "Hanzhong Wang", "Biao Li", "Axel Rominger", "Kuangyu Shi", "Nicha C. Dvorneka", "Chi Liu"], "title": "Fed-NDIF: A Noise-Embedded Federated Diffusion Model For Low-Count Whole-Body PET Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-count positron emission tomography (LCPET) imaging can reduce patients'\nexposure to radiation but often suffers from increased image noise and reduced\nlesion detectability, necessitating effective denoising techniques. Diffusion\nmodels have shown promise in LCPET denoising for recovering degraded image\nquality. However, training such models requires large and diverse datasets,\nwhich are challenging to obtain in the medical domain. To address data scarcity\nand privacy concerns, we combine diffusion models with federated learning -- a\ndecentralized training approach where models are trained individually at\ndifferent sites, and their parameters are aggregated on a central server over\nmultiple iterations. The variation in scanner types and image noise levels\nwithin and across institutions poses additional challenges for federated\nlearning in LCPET denoising. In this study, we propose a novel noise-embedded\nfederated learning diffusion model (Fed-NDIF) to address these challenges,\nleveraging a multicenter dataset and varying count levels. Our approach\nincorporates liver normalized standard deviation (NSTD) noise embedding into a\n2.5D diffusion model and utilizes the Federated Averaging (FedAvg) algorithm to\naggregate locally trained models into a global model, which is subsequently\nfine-tuned on local datasets to optimize performance and obtain personalized\nmodels. Extensive validation on datasets from the University of Bern, Ruijin\nHospital in Shanghai, and Yale-New Haven Hospital demonstrates the superior\nperformance of our method in enhancing image quality and improving lesion\nquantification. The Fed-NDIF model shows significant improvements in PSNR,\nSSIM, and NMSE of the entire 3D volume, as well as enhanced lesion\ndetectability and quantification, compared to local diffusion models and\nfederated UNet-based models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16711", "pdf": "https://arxiv.org/pdf/2503.16711", "abs": "https://arxiv.org/abs/2503.16711", "authors": ["Mihaela-Larisa Clement", "M√≥nika Farsang", "Felix Resch", "Radu Grosu"], "title": "Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Submitted to IROS 2025", "summary": "Autonomous agents that rely purely on perception to make real-time control\ndecisions require efficient and robust architectures. In this work, we\ndemonstrate that augmenting RGB input with depth information significantly\nenhances our agents' ability to predict steering commands compared to using RGB\nalone. We benchmark lightweight recurrent controllers that leverage the fused\nRGB-D features for sequential decision-making. To train our models, we collect\nhigh-quality data using a small-scale autonomous car controlled by an expert\ndriver via a physical steering wheel, capturing varying levels of steering\ndifficulty. Our models, trained under diverse configurations, were successfully\ndeployed on real hardware. Specifically, our findings reveal that the early\nfusion of depth data results in a highly robust controller, which remains\neffective even with frame drops and increased noise levels, without\ncompromising the network's focus on the task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16759", "pdf": "https://arxiv.org/pdf/2503.16759", "abs": "https://arxiv.org/abs/2503.16759", "authors": ["Yancheng Cai", "Ali Bozorgian", "Maliha Ashraf", "Robert Wanat", "Rafa≈Ç K. Mantiuk"], "title": "elaTCSF: A Temporal Contrast Sensitivity Function for Flicker Detection and Modeling Variable Refresh Rate Flicker", "categories": ["cs.GR", "cs.CV"], "comment": "Published at SIGGRAPH Asia 2024", "summary": "The perception of flicker has been a prominent concern in illumination and\nelectronic display fields for over a century. Traditional approaches often rely\non Critical Flicker Frequency (CFF), primarily suited for high-contrast\n(full-on, full-off) flicker. To tackle varying contrast flicker, the\nInternational Committee for Display Metrology (ICDM) introduced a Temporal\nContrast Sensitivity Function TCSF$_{IDMS}$ within the Information Display\nMeasurements Standard (IDMS). Nevertheless, this standard overlooks crucial\nparameters: luminance, eccentricity, and area. Existing models incorporating\nthese parameters are inadequate for flicker detection, especially at low\nspatial frequencies. To address these limitations, we extend the TCSF$_{IDMS}$\nand combine it with a new spatial probability summation model to incorporate\nthe effects of luminance, eccentricity, and area (elaTCSF). We train the\nelaTCSF on various flicker detection datasets and establish the first variable\nrefresh rate flicker detection dataset for further verification. Additionally,\nwe contribute to resolving a longstanding debate on whether the flicker is more\nvisible in peripheral vision. We demonstrate how elaTCSF can be used to predict\nflicker due to low-persistence in VR headsets, identify flicker-free VRR\noperational ranges, and determine flicker sensitivity in lighting design.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16801", "pdf": "https://arxiv.org/pdf/2503.16801", "abs": "https://arxiv.org/abs/2503.16801", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Saeed Mian"], "title": "Auto-Regressive Diffusion for Generating 3D Human-Object Interactions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-driven Human-Object Interaction (Text-to-HOI) generation is an emerging\nfield with applications in animation, video games, virtual reality, and\nrobotics. A key challenge in HOI generation is maintaining interaction\nconsistency in long sequences. Existing Text-to-Motion-based approaches, such\nas discrete motion tokenization, cannot be directly applied to HOI generation\ndue to limited data in this domain and the complexity of the modality. To\naddress the problem of interaction consistency in long sequences, we propose an\nautoregressive diffusion model (ARDHOI) that predicts the next continuous\ntoken. Specifically, we introduce a Contrastive Variational Autoencoder (cVAE)\nto learn a physically plausible space of continuous HOI tokens, thereby\nensuring that generated human-object motions are realistic and natural. For\ngenerating sequences autoregressively, we develop a Mamba-based context encoder\nto capture and maintain consistent sequential actions. Additionally, we\nimplement an MLP-based denoiser to generate the subsequent token conditioned on\nthe encoded context. Our model has been evaluated on the OMOMO and BEHAVE\ndatasets, where it outperforms existing state-of-the-art methods in terms of\nboth performance and inference speed. This makes ARDHOI a robust and efficient\nsolution for text-driven HOI tasks", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16862", "pdf": "https://arxiv.org/pdf/2503.16862", "abs": "https://arxiv.org/abs/2503.16862", "authors": ["Yiqiang Cai", "Yizhou Tan", "Peihong Zhang", "Yuxuan Liu", "Shengchen Li", "Xi Shao", "Mark D. Plumbley"], "title": "City2Scene: Improving Acoustic Scene Classification with City Features", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Acoustic scene recordings are often collected from a diverse range of cities.\nMost existing acoustic scene classification (ASC) approaches focus on\nidentifying common acoustic scene patterns across cities to enhance\ngeneralization. In contrast, we hypothesize that city-specific environmental\nand cultural differences in acoustic features are beneficial for the ASC task.\nIn this paper, we introduce City2Scene, a novel framework that leverages city\nfeatures to improve ASC. City2Scene transfers the city-specific knowledge from\ncity classification models to a scene classification model using knowledge\ndistillation. We evaluated City2Scene on the DCASE Challenge Task 1 datasets,\nwhere each audio clip is annotated with both scene and city labels.\nExperimental results demonstrate that city features provide valuable\ninformation for classifying scenes. By distilling the city-specific knowledge,\nCity2Scene effectively improves accuracy for various state-of-the-art ASC\nbackbone models, including both CNNs and Transformers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16872", "pdf": "https://arxiv.org/pdf/2503.16872", "abs": "https://arxiv.org/abs/2503.16872", "authors": ["Xuan Wang", "Siyuan Liang", "Dongping Liao", "Han Fang", "Aishan Liu", "Xiaochun Cao", "Yu-liang Lu", "Ee-Chien Chang", "Xitong Gao"], "title": "Lie Detector: Unified Backdoor Detection via Cross-Examination Framework", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Institutions with limited data and computing resources often outsource model\ntraining to third-party providers in a semi-honest setting, assuming adherence\nto prescribed training protocols with pre-defined learning paradigm (e.g.,\nsupervised or semi-supervised learning). However, this practice can introduce\nsevere security risks, as adversaries may poison the training data to embed\nbackdoors into the resulting model. Existing detection approaches predominantly\nrely on statistical analyses, which often fail to maintain universally accurate\ndetection accuracy across different learning paradigms. To address this\nchallenge, we propose a unified backdoor detection framework in the semi-honest\nsetting that exploits cross-examination of model inconsistencies between two\nindependent service providers. Specifically, we integrate central kernel\nalignment to enable robust feature similarity measurements across different\nmodel architectures and learning paradigms, thereby facilitating precise\nrecovery and identification of backdoor triggers. We further introduce backdoor\nfine-tuning sensitivity analysis to distinguish backdoor triggers from\nadversarial perturbations, substantially reducing false positives. Extensive\nexperiments demonstrate that our method achieves superior detection\nperformance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines\nacross supervised, semi-supervised, and autoregressive learning tasks,\nrespectively. Notably, it is the first to effectively detect backdoors in\nmultimodal large language models, further highlighting its broad applicability\nand advancing secure deep learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17017", "pdf": "https://arxiv.org/pdf/2503.17017", "abs": "https://arxiv.org/abs/2503.17017", "authors": ["Aoting Zhang", "Dongbao Yang", "Chang Liu", "Xiaopeng Hong", "Yu Zhou"], "title": "Specifying What You Know or Not for Multi-Label Class-Incremental Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Existing class incremental learning is mainly designed for single-label\nclassification task, which is ill-equipped for multi-label scenarios due to the\ninherent contradiction of learning objectives for samples with incomplete\nlabels. We argue that the main challenge to overcome this contradiction in\nmulti-label class-incremental learning (MLCIL) lies in the model's inability to\nclearly distinguish between known and unknown knowledge. This ambiguity hinders\nthe model's ability to retain historical knowledge, master current classes, and\nprepare for future learning simultaneously. In this paper, we target at\nspecifying what is known or not to accommodate Historical, Current, and\nProspective knowledge for MLCIL and propose a novel framework termed as HCP.\nSpecifically, (i) we clarify the known classes by dynamic feature purification\nand recall enhancement with distribution prior, enhancing the precision and\nretention of known information. (ii) We design prospective knowledge mining to\nprobe the unknown, preparing the model for future learning. Extensive\nexperiments validate that our method effectively alleviates catastrophic\nforgetting in MLCIL, surpassing the previous state-of-the-art by 3.3% on\naverage accuracy for MS-COCO B0-C10 setting without replay buffers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17024", "pdf": "https://arxiv.org/pdf/2503.17024", "abs": "https://arxiv.org/abs/2503.17024", "authors": ["David Mildenberger", "Paul Hager", "Daniel Rueckert", "Martin J Menten"], "title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Supervised contrastive learning (SupCon) has proven to be a powerful\nalternative to the standard cross-entropy loss for classification of\nmulti-class balanced datasets. However, it struggles to learn well-conditioned\nrepresentations of datasets with long-tailed class distributions. This problem\nis potentially exacerbated for binary imbalanced distributions, which are\ncommonly encountered during many real-world problems such as medical diagnosis.\nIn experiments on seven binary datasets of natural and medical images, we show\nthat the performance of SupCon decreases with increasing class imbalance. To\nsubstantiate these findings, we introduce two novel metrics that evaluate the\nquality of the learned representation space. By measuring the class\ndistribution in local neighborhoods, we are able to uncover structural\ndeficiencies of the representation space that classical metrics cannot detect.\nInformed by these insights, we propose two new supervised contrastive learning\nstrategies tailored to binary imbalanced datasets that improve the structure of\nthe representation space and increase downstream classification accuracy over\nstandard SupCon by up to 35%. We make our code available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17030", "pdf": "https://arxiv.org/pdf/2503.17030", "abs": "https://arxiv.org/abs/2503.17030", "authors": ["Snigdha Paul", "Sambit Mallick", "Anindya Sen"], "title": "Exploring the Efficacy of Partial Denoising Using Bit Plane Slicing for Enhanced Fracture Identification: A Comparative Study of Deep Learning-Based Approaches and Handcrafted Feature Extraction Techniques", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computer vision has transformed medical diagnosis, treatment, and research\nthrough advanced image processing and machine learning techniques. Fracture\nclassification, a critical area in healthcare, has greatly benefited from these\nadvancements, yet accurate detection is challenged by complex patterns and\nimage noise. Bit plane slicing enhances medical images by reducing noise\ninterference and extracting informative features. This research explores\npartial denoising techniques to provide practical solutions for improved\nfracture analysis, ultimately enhancing patient care. The study explores deep\nlearning model DenseNet and handcrafted feature extraction. Decision Tree and\nRandom Forest, were employed to train and evaluate distinct image\nrepresentations. These include the original image, the concatenation of the\nfour bit planes from the LSB as well as MSB, the fully denoised image, and an\nimage consisting of 6 bit planes from MSB and 2 denoised bit planes from LSB.\nThe purpose of forming these diverse image representations is to analyze SNR as\nwell as classification accuracy and identify the bit planes that contain the\nmost informative features. Moreover, the study delves into the significance of\npartial denoising techniques in preserving crucial features, leading to\nimprovements in classification results. Notably, this study shows that\nemploying the Random Forest classifier, the partially denoised image\nrepresentation exhibited a testing accuracy of 95.61% surpassing the\nperformance of other image representations. The outcomes of this research\nprovide valuable insights into the development of efficient preprocessing,\nfeature extraction and classification approaches for fracture identification.\nBy enhancing diagnostic accuracy, these advancements hold the potential to\npositively impact patient care and overall medical outcomes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17095", "pdf": "https://arxiv.org/pdf/2503.17095", "abs": "https://arxiv.org/abs/2503.17095", "authors": ["Kwan Yun", "Chaelin Kim", "Hangyeul Shin", "Junyong Noh"], "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.3; I.3.8"], "comment": "CVPR2025, 11 pages, 14 figures", "summary": "Recent 3D face editing methods using masks have produced high-quality edited\nimages by leveraging Neural Radiance Fields (NeRF). Despite their impressive\nperformance, existing methods often provide limited user control due to the use\nof pre-trained segmentation masks. To utilize masks with a desired layout, an\nextensive training dataset is required, which is challenging to gather. We\npresent FFaceNeRF, a NeRF-based face editing technique that can overcome the\nchallenge of limited user control due to the use of fixed mask layouts. Our\nmethod employs a geometry adapter with feature injection, allowing for\neffective manipulation of geometry attributes. Additionally, we adopt latent\nmixing for tri-plane augmentation, which enables training with a few samples.\nThis facilitates rapid model adaptation to desired mask layouts, crucial for\napplications in fields like personalized medical imaging or creative face\nediting. Our comparative evaluations demonstrate that FFaceNeRF surpasses\nexisting mask based face editing methods in terms of flexibility, control, and\ngenerated image quality, paving the way for future advancements in customized\nand high-fidelity 3D face editing. The code is available on the\n{\\href{https://kwanyun.github.io/FFaceNeRF_page/}{project-page}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17105", "pdf": "https://arxiv.org/pdf/2503.17105", "abs": "https://arxiv.org/abs/2503.17105", "authors": ["Marco Usai", "Andrea Loddo", "Alessandra Perniciano", "Maurizio Atzori", "Cecilia Di Ruberto"], "title": "A Comparative Analysis of Image Descriptors for Histopathological Classification of Gastric Cancer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Gastric cancer ranks as the fifth most common and fourth most lethal cancer\nglobally, with a dismal 5-year survival rate of approximately 20%. Despite\nextensive research on its pathobiology, the prognostic predictability remains\ninadequate, compounded by pathologists' high workload and potential diagnostic\nerrors. Thus, automated, accurate histopathological diagnosis tools are\ncrucial. This study employs Machine Learning and Deep Learning techniques to\nclassify histopathological images into healthy and cancerous categories. Using\nhandcrafted and deep features with shallow learning classifiers on the\nGasHisSDB dataset, we offer a comparative analysis and insights into the most\nrobust and high-performing combinations of features and classifiers for\ndistinguishing between normal and abnormal histopathological images without\nfine-tuning strategies. With the RF classifier, our approach can reach F1 of\n93.4%, demonstrating its validity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17107", "pdf": "https://arxiv.org/pdf/2503.17107", "abs": "https://arxiv.org/abs/2503.17107", "authors": ["Davide Antonio Mura", "Michela Pinna", "Lorenzo Putzu", "Andrea Loddo", "Alessandra Perniciano", "Olga Mulas", "Cecilia Di Ruberto"], "title": "Exploring Few-Shot Object Detection on Blood Smear Images: A Case Study of Leukocytes and Schistocytes", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The detection of blood disorders often hinges upon the quantification of\nspecific blood cell types. Variations in cell counts may indicate the presence\nof pathological conditions. Thus, the significance of developing precise\nautomatic systems for blood cell enumeration is underscored. The investigation\nfocuses on a novel approach termed DE-ViT. This methodology is employed in a\nFew-Shot paradigm, wherein training relies on a limited number of images. Two\ndistinct datasets are utilised for experimental purposes: the Raabin-WBC\ndataset for Leukocyte detection and a local dataset for Schistocyte\nidentification. In addition to the DE-ViT model, two baseline models, Faster\nR-CNN 50 and Faster R-CNN X 101, are employed, with their outcomes being\ncompared against those of the proposed model. While DE-ViT has demonstrated\nstate-of-the-art performance on the COCO and LVIS datasets, both baseline\nmodels surpassed its performance on the Raabin-WBC dataset. Moreover, only\nFaster R-CNN X 101 yielded satisfactory results on the SC-IDB. The observed\ndisparities in performance may possibly be attributed to domain shift\nphenomena.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17116", "pdf": "https://arxiv.org/pdf/2503.17116", "abs": "https://arxiv.org/abs/2503.17116", "authors": ["Luca Rossetto", "Werner Bailer", "Duc-Tien Dang-Nguyen", "Graham Healy", "Bj√∂rn √û√≥r J√≥nsson", "Onanong Kongmeesub", "Hoang-Bao Le", "Stevan Rudinac", "Klaus Sch√∂ffmann", "Florian Spiess", "Allie Tran", "Minh-Triet Tran", "Quang-Linh Tran", "Cathal Gurrin"], "title": "The CASTLE 2024 Dataset: Advancing the Art of Multimodal Understanding", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.IR"], "comment": "7 pages, 6 figures, dataset available via\n  https://castle-dataset.github.io/", "summary": "Egocentric video has seen increased interest in recent years, as it is used\nin a range of areas. However, most existing datasets are limited to a single\nperspective. In this paper, we present the CASTLE 2024 dataset, a multimodal\ncollection containing ego- and exo-centric (i.e., first- and third-person\nperspective) video and audio from 15 time-aligned sources, as well as other\nsensor streams and auxiliary data. The dataset was recorded by volunteer\nparticipants over four days in a fixed location and includes the point of view\nof 10 participants, with an additional 5 fixed cameras providing an exocentric\nperspective. The entire dataset contains over 600 hours of UHD video recorded\nat 50 frames per second. In contrast to other datasets, CASTLE 2024 does not\ncontain any partial censoring, such as blurred faces or distorted audio. The\ndataset is available via https://castle-dataset.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17198", "pdf": "https://arxiv.org/pdf/2503.17198", "abs": "https://arxiv.org/abs/2503.17198", "authors": ["Yongli Xiang", "Ziming Hong", "Lina Yao", "Dadong Wang", "Tongliang Liu"], "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Code is released at https://github.com/tmllab/2025_CVPR_JailNTL", "summary": "Non-transferable learning (NTL) has been proposed to protect model\nintellectual property (IP) by creating a \"non-transferable barrier\" to restrict\ngeneralization from authorized to unauthorized domains. Recently, well-designed\nattack, which restores the unauthorized-domain performance by fine-tuning NTL\nmodels on few authorized samples, highlights the security risks of NTL-based\napplications. However, such attack requires modifying model weights, thus being\ninvalid in the black-box scenario. This raises a critical question: can we\ntrust the security of NTL models deployed as black-box systems? In this work,\nwe reveal the first loophole of black-box NTL models by proposing a novel\nattack method (dubbed as JailNTL) to jailbreak the non-transferable barrier\nthrough test-time data disguising. The main idea of JailNTL is to disguise\nunauthorized data so it can be identified as authorized by the NTL model,\nthereby bypassing the non-transferable barrier without modifying the NTL model\nweights. Specifically, JailNTL encourages unauthorized-domain disguising in two\nlevels, including: (i) data-intrinsic disguising (DID) for eliminating domain\ndiscrepancy and preserving class-related content at the input-level, and (ii)\nmodel-guided disguising (MGD) for mitigating output-level statistics difference\nof the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL\nmodels in the black-box scenario, JailNTL achieves an accuracy increase of up\nto 55.7% in the unauthorized domain by using only 1% authorized samples,\nlargely exceeding existing SOTA white-box attacks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17244", "pdf": "https://arxiv.org/pdf/2503.17244", "abs": "https://arxiv.org/abs/2503.17244", "authors": ["Jyothi Rikhab Chand", "Mathews Jacob"], "title": "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17275", "pdf": "https://arxiv.org/pdf/2503.17275", "abs": "https://arxiv.org/abs/2503.17275", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Jazib", "Zeeshan Alam", "Muhmmad Farhan Khan", "Muhammad Saad", "Muhammad Ali Jamshed"], "title": "Vision Transformer Based Semantic Communications for Next Generation Wireless Networks", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted @ ICC 2025", "summary": "In the evolving landscape of 6G networks, semantic communications are poised\nto revolutionize data transmission by prioritizing the transmission of semantic\nmeaning over raw data accuracy. This paper presents a Vision Transformer\n(ViT)-based semantic communication framework that has been deliberately\ndesigned to achieve high semantic similarity during image transmission while\nsimultaneously minimizing the demand for bandwidth. By equipping ViT as the\nencoder-decoder framework, the proposed architecture can proficiently encode\nimages into a high semantic content at the transmitter and precisely\nreconstruct the images, considering real-world fading and noise consideration\nat the receiver. Building on the attention mechanisms inherent to ViTs, our\nmodel outperforms Convolution Neural Network (CNNs) and Generative Adversarial\nNetworks (GANs) tailored for generating such images. The architecture based on\nthe proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38\ndB, which is higher than other Deep Learning (DL) approaches in maintaining\nsemantic similarity across different communication environments. These findings\nestablish our ViT-based approach as a significant breakthrough in semantic\ncommunications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
