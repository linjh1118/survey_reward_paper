{"id": "2503.17662", "pdf": "https://arxiv.org/pdf/2503.17662", "abs": "https://arxiv.org/abs/2503.17662", "authors": ["Ke Ji", "Yixin Lian", "Linxu Li", "Jingsheng Gao", "Weiyuan Li", "Bin Dai"], "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning", "categories": ["cs.CL"], "comment": "18 pages, 4 figures", "summary": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "alignment", "human alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "safety", "consistency", "dialogue", "fine-grained"], "score": 6}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18454", "pdf": "https://arxiv.org/pdf/2503.18454", "abs": "https://arxiv.org/abs/2503.18454", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xierui Wang", "Xiaoyin Xu", "Min Zhang"], "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "preference dataset", "human preference", "correlation"], "score": 5}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17660", "pdf": "https://arxiv.org/pdf/2503.17660", "abs": "https://arxiv.org/abs/2503.17660", "authors": ["Kun Li", "Jianhui Wang", "Miao Zhang", "Xueqian Wang"], "title": "OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Intent Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Generative AI has significantly advanced text-driven image generation, but it\nstill faces challenges in producing outputs that consistently align with\nevolving user preferences and intents, particularly in multi-turn dialogue\nscenarios. In this research, We present a Visual Co-Adaptation (VCA) framework\nthat incorporates human-in-the-loop feedback, utilizing a well-trained reward\nmodel specifically designed to closely align with human preferences. Using a\ndiverse multi-turn dialogue dataset, the framework applies multiple reward\nfunctions (such as diversity, consistency, and preference feedback) to refine\nthe diffusion model through LoRA, effectively optimizing image generation based\non user input. We also constructed multi-round dialogue datasets with prompts\nand image pairs that well-fit user intent. Experiments show the model achieves\n508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It\nalso achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and\nexcels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments\ndemonstrate the effectiveness of the proposed method over state-of-the-art\nbaselines, with significant improvements in image consistency and alignment\nwith user intent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "dialogue"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18172", "pdf": "https://arxiv.org/pdf/2503.18172", "abs": "https://arxiv.org/abs/2503.18172", "authors": ["Zixin Chen", "Sicheng Song", "Kashun Shum", "Yanna Lin", "Rui Sheng", "Huamin Qu"], "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages in total. Under Review For ARR", "summary": "Misleading chart visualizations, which intentionally manipulate data\nrepresentations to support specific claims, can distort perceptions and lead to\nincorrect conclusions. Despite decades of research, misleading visualizations\nremain a widespread and pressing issue. Recent advances in multimodal large\nlanguage models (MLLMs) have demonstrated strong chart comprehension\ncapabilities, yet no existing work has systematically evaluated their ability\nto detect and interpret misleading charts. This paper introduces the Misleading\nChart Question Answering (Misleading ChartQA) Benchmark, a large-scale\nmultimodal dataset designed to assess MLLMs in identifying and reasoning about\nmisleading charts. It contains over 3,000 curated examples, covering 21 types\nof misleaders and 10 chart types. Each example includes standardized chart\ncode, CSV data, and multiple-choice questions with labeled explanations,\nvalidated through multi-round MLLM checks and exhausted expert human review. We\nbenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations\nin identifying visually deceptive practices. We also propose a novel pipeline\nthat detects and localizes misleaders, enhancing MLLMs' accuracy in misleading\nchart interpretation. Our work establishes a foundation for advancing\nMLLM-driven misleading chart comprehension. We publicly release the sample\ndataset to support further research in this critical area.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17856", "pdf": "https://arxiv.org/pdf/2503.17856", "abs": "https://arxiv.org/abs/2503.17856", "authors": ["Radu Beche", "Sergiu Nedevschi"], "title": "ClaraVid: A Holistic Scene Reconstruction Benchmark From Aerial Perspective With Delentropy-Based Complexity Profiling", "categories": ["cs.CV"], "comment": "Currently under review", "summary": "The development of aerial holistic scene understanding algorithms is hindered\nby the scarcity of comprehensive datasets that enable both semantic and\ngeometric reconstruction. While synthetic datasets offer an alternative,\nexisting options exhibit task-specific limitations, unrealistic scene\ncompositions, and rendering artifacts that compromise real-world applicability.\nWe introduce ClaraVid, a synthetic aerial dataset specifically designed to\novercome these limitations. Comprising 16,917 high-resolution images captured\nat 4032x3024 from multiple viewpoints across diverse landscapes, ClaraVid\nprovides dense depth maps, panoptic segmentation, sparse point clouds, and\ndynamic object masks, while mitigating common rendering artifacts. To further\nadvance neural reconstruction, we introduce the Delentropic Scene Profile\n(DSP), a novel complexity metric derived from differential entropy analysis,\ndesigned to quantitatively assess scene difficulty and inform reconstruction\ntasks. Utilizing DSP, we systematically benchmark neural reconstruction\nmethods, uncovering a consistent, measurable correlation between scene\ncomplexity and reconstruction accuracy. Empirical results indicate that higher\ndelentropy strongly correlates with increased reconstruction errors, validating\nDSP as a reliable complexity prior. Currently under review, upon acceptance the\ndata and code will be available at\n$\\href{https://rdbch.github.io/claravid}{rdbch.github.io/ClaraVid}$.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "correlation", "accuracy"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17871", "pdf": "https://arxiv.org/pdf/2503.17871", "abs": "https://arxiv.org/abs/2503.17871", "authors": ["Pranavi Kolouju", "Eric Xing", "Robert Pless", "Nathan Jacobs", "Abby Stylianou"], "title": "good4cir: Generating Detailed Synthetic Captions for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Composed image retrieval (CIR) enables users to search images using a\nreference image combined with textual modifications. Recent advances in\nvision-language models have improved CIR, but dataset limitations remain a\nbarrier. Existing datasets often rely on simplistic, ambiguous, or insufficient\nmanual annotations, hindering fine-grained retrieval. We introduce good4cir, a\nstructured pipeline leveraging vision-language models to generate high-quality\nsynthetic annotations. Our method involves: (1) extracting fine-grained object\ndescriptions from query images, (2) generating comparable descriptions for\ntarget images, and (3) synthesizing textual instructions capturing meaningful\ntransformations between images. This reduces hallucination, enhances\nmodification diversity, and ensures object-level consistency. Applying our\nmethod improves existing datasets and enables creating new datasets across\ndiverse domains. Results demonstrate improved retrieval accuracy for CIR models\ntrained on our pipeline-generated datasets. We release our dataset construction\nframework to support further research in CIR and multi-modal retrieval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17877", "pdf": "https://arxiv.org/pdf/2503.17877", "abs": "https://arxiv.org/abs/2503.17877", "authors": ["Samira Alkaee Taleghan", "Andrew P. Barrett", "Walter N. Meier", "Farnoush Banaei-Kashani"], "title": "IceBench: A Benchmark for Deep Learning based Sea Ice Type Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Sea ice plays a critical role in the global climate system and maritime\noperations, making timely and accurate classification essential. However,\ntraditional manual methods are time-consuming, costly, and have inherent\nbiases. Automating sea ice type classification addresses these challenges by\nenabling faster, more consistent, and scalable analysis. While both traditional\nand deep learning approaches have been explored, deep learning models offer a\npromising direction for improving efficiency and consistency in sea ice\nclassification. However, the absence of a standardized benchmark and\ncomparative study prevents a clear consensus on the best-performing models. To\nbridge this gap, we introduce \\textit{IceBench}, a comprehensive benchmarking\nframework for sea ice type classification. Our key contributions are threefold:\nFirst, we establish the IceBench benchmarking framework which leverages the\nexisting AI4Arctic Sea Ice Challenge dataset as a standardized dataset,\nincorporates a comprehensive set of evaluation metrics, and includes\nrepresentative models from the entire spectrum of sea ice type classification\nmethods categorized in two distinct groups, namely, pixel-based classification\nmethods and patch-based classification methods. IceBench is open-source and\nallows for convenient integration and evaluation of other sea ice type\nclassification methods; hence, facilitating comparative evaluation of new\nmethods and improving reproducibility in the field. Second, we conduct an\nin-depth comparative study on representative models to assess their strengths\nand limitations, providing insights for both practitioners and researchers.\nThird, we leverage IceBench for systematic experiments addressing key research\nquestions on model transferability across seasons (time) and locations (space),\ndata downscaling, and preprocessing strategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17975", "pdf": "https://arxiv.org/pdf/2503.17975", "abs": "https://arxiv.org/abs/2503.17975", "authors": ["Yuzhi Li", "Haojun Xu", "Feng Tian"], "title": "Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rising popularity of short video platforms, the demand for video\nproduction has increased substantially. However, high-quality video creation\ncontinues to rely heavily on professional editing skills and a nuanced\nunderstanding of visual language. To address this challenge, the Shot Sequence\nOrdering (SSO) task in AI-assisted video editing has emerged as a pivotal\napproach for enhancing video storytelling and the overall viewing experience.\nNevertheless, the progress in this field has been impeded by a lack of publicly\navailable benchmark datasets. In response, this paper introduces two novel\nbenchmark datasets, AVE-Order and ActivityNet-Order. Additionally, we employ\nthe Kendall Tau distance as an evaluation metric for the SSO task and propose\nthe Kendall Tau Distance-Cross Entropy Loss. We further introduce the concept\nof Cinematology Embedding, which incorporates movie metadata and shot labels as\nprior knowledge into the SSO model, and constructs the AVE-Meta dataset to\nvalidate the method's effectiveness. Experimental results indicate that the\nproposed loss function and method substantially enhance SSO task accuracy. All\ndatasets are publicly accessible at https://github.com/litchiar/ShotSeqBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18665", "pdf": "https://arxiv.org/pdf/2503.18665", "abs": "https://arxiv.org/abs/2503.18665", "authors": ["Bingchen Miao", "Yang Wu", "Minghe Gao", "Qifan Yu", "Wendong Bu", "Wenqiao Zhang", "Yunfei Li", "Siliang Tang", "Tat-Seng Chua", "Juncheng Li"], "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional", "fine-grained"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18755", "pdf": "https://arxiv.org/pdf/2503.18755", "abs": "https://arxiv.org/abs/2503.18755", "authors": ["Nathan Darjana", "Ryo Fujii", "Hideo Saito", "Hiroki Kajita"], "title": "EgoSurgery-HTS: A Dataset for Egocentric Hand-Tool Segmentation in Open Surgery Videos", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Egocentric open-surgery videos capture rich, fine-grained details essential\nfor accurately modeling surgical procedures and human behavior in the operating\nroom. A detailed, pixel-level understanding of hands and surgical tools is\ncrucial for interpreting a surgeon's actions and intentions. We introduce\nEgoSurgery-HTS, a new dataset with pixel-wise annotations and a benchmark suite\nfor segmenting surgical tools, hands, and interacting tools in egocentric\nopen-surgery videos. Specifically, we provide a labeled dataset for (1) tool\ninstance segmentation of 14 distinct surgical tools, (2) hand instance\nsegmentation, and (3) hand-tool segmentation to label hands and the tools they\nmanipulate. Using EgoSurgery-HTS, we conduct extensive evaluations of\nstate-of-the-art segmentation methods and demonstrate significant improvements\nin the accuracy of hand and hand-tool segmentation in egocentric open-surgery\nvideos compared to existing datasets. The dataset will be released at\nhttps://github.com/Fujiry0/EgoSurgery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18923", "pdf": "https://arxiv.org/pdf/2503.18923", "abs": "https://arxiv.org/abs/2503.18923", "authors": ["Meng Cao", "Pengfei Hu", "Yingyao Wang", "Jihao Gu", "Haoran Tang", "Haoze Zhao", "Jiahua Dong", "Wangbo Yu", "Ge Zhang", "Ian Reid", "Xiaodan Liang"], "title": "Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Recent advancements in Large Video Language Models (LVLMs) have highlighted\ntheir potential for multi-modal understanding, yet evaluating their factual\ngrounding in video contexts remains a critical unsolved challenge. To address\nthis gap, we introduce Video SimpleQA, the first comprehensive benchmark\ntailored for factuality evaluation of LVLMs. Our work distinguishes from\nexisting video benchmarks through the following key features: 1) Knowledge\nrequired: demanding integration of external knowledge beyond the explicit\nnarrative; 2) Fact-seeking question: targeting objective, undisputed events or\nrelationships, avoiding subjective interpretation; 3) Definitive & short-form\nanswer: Answers are crafted as unambiguous and definitively correct in a short\nformat, enabling automated evaluation through LLM-as-a-judge frameworks with\nminimal scoring variance; 4) External-source verified: All annotations undergo\nrigorous validation against authoritative external references to ensure the\nreliability; 5) Temporal reasoning required: The annotated question types\nencompass both static single-frame understanding and dynamic temporal\nreasoning, explicitly evaluating LVLMs factuality under the long-context\ndependencies. We extensively evaluate 41 state-of-the-art LVLMs and summarize\nkey findings as follows: 1) Current LVLMs exhibit notable deficiencies in\nfactual adherence, particularly for open-source models. The best-performing\nmodel Gemini-1.5-Pro achieves merely an F-score of 54.4%; 2) Test-time compute\nparadigms show insignificant performance gains, revealing fundamental\nconstraints for enhancing factuality through post-hoc computation; 3)\nRetrieval-Augmented Generation demonstrates consistent improvements at the cost\nof additional inference time overhead, presenting a critical\nefficiency-performance trade-off.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "factuality", "reliability"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17485", "pdf": "https://arxiv.org/pdf/2503.17485", "abs": "https://arxiv.org/abs/2503.17485", "authors": ["Lama Ayash", "Hassan Alhuzali", "Ashwag Alasmari", "Sultan Aloufi"], "title": "SaudiCulture: A Benchmark for Evaluating Large Language Models Cultural Competence within Saudi Arabia", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, under-review", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing; however, they often struggle to accurately capture\nand reflect cultural nuances. This research addresses this challenge by\nfocusing on Saudi Arabia, a country characterized by diverse dialects and rich\ncultural traditions. We introduce SaudiCulture, a novel benchmark designed to\nevaluate the cultural competence of LLMs within the distinct geographical and\ncultural contexts of Saudi Arabia. SaudiCulture is a comprehensive dataset of\nquestions covering five major geographical regions, such as West, East, South,\nNorth, and Center, along with general questions applicable across all regions.\nThe dataset encompasses a broad spectrum of cultural domains, including food,\nclothing, entertainment, celebrations, and crafts. To ensure a rigorous\nevaluation, SaudiCulture includes questions of varying complexity, such as\nopen-ended, single-choice, and multiple-choice formats, with some requiring\nmultiple correct answers. Additionally, the dataset distinguishes between\ncommon cultural knowledge and specialized regional aspects. We conduct\nextensive evaluations on five LLMs, such as GPT-4, Llama 3.3, FANAR, Jais, and\nAceGPT, analyzing their performance across different question types and\ncultural contexts. Our findings reveal that all models experience significant\nperformance declines when faced with highly specialized or region-specific\nquestions, particularly those requiring multiple correct responses.\nAdditionally, certain cultural categories are more easily identifiable than\nothers, further highlighting inconsistencies in LLMs cultural understanding.\nThese results emphasize the importance of incorporating region-specific\nknowledge into LLMs training to enhance their cultural competence.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "testbed"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17599", "pdf": "https://arxiv.org/pdf/2503.17599", "abs": "https://arxiv.org/abs/2503.17599", "authors": ["Zheqing Li", "Yiying Yang", "Jiping Lang", "Wenhao Jiang", "Yuhang Zhao", "Shuang Li", "Dingqian Wang", "Zhu Lin", "Xuanna Li", "Yuze Tang", "Jiexian Qiu", "Xiaolin Lu", "Hongji Yu", "Shuang Chen", "Yuhua Bi", "Xiaofei Zeng", "Yixian Chen", "Junrong Chen", "Lin Yao"], "title": "GPBench: A Comprehensive and Fine-Grained Benchmark for Evaluating Large Language Models as General Practitioners", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "General practitioners (GPs) serve as the cornerstone of primary healthcare\nsystems by providing continuous and comprehensive medical services. However,\ndue to community-oriented nature of their practice, uneven training and\nresource gaps, the clinical proficiency among GPs can vary significantly across\nregions and healthcare settings. Currently, Large Language Models (LLMs) have\ndemonstrated great potential in clinical and medical applications, making them\na promising tool for supporting general practice. However, most existing\nbenchmarks and evaluation frameworks focus on exam-style assessments-typically\nmultiple-choice question-lack comprehensive assessment sets that accurately\nmirror the real-world scenarios encountered by GPs. To evaluate how effectively\nLLMs can make decisions in the daily work of GPs, we designed GPBench, which\nconsists of both test questions from clinical practice and a novel evaluation\nframework. The test set includes multiple-choice questions that assess\nfundamental knowledge of general practice, as well as realistic, scenario-based\nproblems. All questions are meticulously annotated by experts, incorporating\nrich fine-grained information related to clinical management. The proposed LLM\nevaluation framework is based on the competency model for general practice,\nproviding a comprehensive methodology for assessing LLM performance in\nreal-world settings. As the first large-model evaluation set targeting GP\ndecision-making scenarios, GPBench allows us to evaluate current mainstream\nLLMs. Expert assessment and evaluation reveal that in areas such as disease\nstaging, complication recognition, treatment detail, and medication usage,\nthese models exhibit at least ten major shortcomings. Overall, existing LLMs\nare not yet suitable for independent use in real-world GP working scenarios\nwithout human oversight.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17499", "pdf": "https://arxiv.org/pdf/2503.17499", "abs": "https://arxiv.org/abs/2503.17499", "authors": ["Joey Mulé", "Dhandeep Challagundla", "Rachit Saini", "Riadul Islam"], "title": "Event-Based Crossing Dataset (EBCD)", "categories": ["cs.CV"], "comment": null, "summary": "Event-based vision revolutionizes traditional image sensing by capturing\nasynchronous intensity variations rather than static frames, enabling ultrafast\ntemporal resolution, sparse data encoding, and enhanced motion perception.\nWhile this paradigm offers significant advantages, conventional event-based\ndatasets impose a fixed thresholding constraint to determine pixel activations,\nseverely limiting adaptability to real-world environmental fluctuations. Lower\nthresholds retain finer details but introduce pervasive noise, whereas higher\nthresholds suppress extraneous activations at the expense of crucial object\ninformation. To mitigate these constraints, we introduce the Event-Based\nCrossing Dataset (EBCD), a comprehensive dataset tailored for pedestrian and\nvehicle detection in dynamic outdoor environments, incorporating a\nmulti-thresholding framework to refine event representations. By capturing\nevent-based images at ten distinct threshold levels (4, 8, 12, 16, 20, 30, 40,\n50, 60, and 75), this dataset facilitates an extensive assessment of object\ndetection performance under varying conditions of sparsity and noise\nsuppression. We benchmark state-of-the-art detection architectures-including\nYOLOv4, YOLOv7, EfficientDet-b0, MobileNet-v1, and Histogram of Oriented\nGradients (HOG)-to experiment upon the nuanced impact of threshold selection on\ndetection performance. By offering a systematic approach to threshold\nvariation, we foresee that EBCD fosters a more adaptive evaluation of\nevent-based object detection, aligning diverse neuromorphic vision with\nreal-world scene dynamics. We present the dataset as publicly available to\npropel further advancements in low-latency, high-fidelity neuromorphic imaging:\nhttps://ieee-dataport.org/documents/event-based-crossing-dataset-ebcd", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17633", "pdf": "https://arxiv.org/pdf/2503.17633", "abs": "https://arxiv.org/abs/2503.17633", "authors": ["Tejas Panambur", "Mario Parente"], "title": "Enhancing Martian Terrain Recognition with Deep Constrained Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Martian terrain recognition is pivotal for advancing our understanding of\ntopography, geomorphology, paleoclimate, and habitability. While deep\nclustering methods have shown promise in learning semantically homogeneous\nfeature embeddings from Martian rover imagery, the natural variations in\nintensity, scale, and rotation pose significant challenges for accurate terrain\nclassification. To address these limitations, we propose Deep Constrained\nClustering with Metric Learning (DCCML), a novel algorithm that leverages\nmultiple constraint types to guide the clustering process. DCCML incorporates\nsoft must-link constraints derived from spatial and depth similarities between\nneighboring patches, alongside hard constraints from stereo camera pairs and\ntemporally adjacent images. Experimental evaluation on the Curiosity rover\ndataset (with 150 clusters) demonstrates that DCCML increases homogeneous\nclusters by 16.7 percent while reducing the Davies-Bouldin Index from 3.86 to\n1.82 and boosting retrieval accuracy from 86.71 percent to 89.86 percent. This\nimprovement enables more precise classification of Martian geological features,\nadvancing our capacity to analyze and understand the planet's landscape.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17651", "pdf": "https://arxiv.org/pdf/2503.17651", "abs": "https://arxiv.org/abs/2503.17651", "authors": ["Zhuo Tao", "Liang Li", "Qi Chen", "Yunbin Tu", "Zheng-Jun Zha", "Ming-Hsuan Yang", "Yuankai Qi", "Qingming Huang"], "title": "Collaborative Temporal Consistency Learning for Point-supervised Natural Language Video Localization", "categories": ["cs.CV"], "comment": "Under review", "summary": "Natural language video localization (NLVL) is a crucial task in video\nunderstanding that aims to localize the target moment in videos specified by a\ngiven language description. Recently, a point-supervised paradigm has been\npresented to address this task, requiring only a single annotated frame within\nthe target moment rather than complete temporal boundaries. Compared with the\nfully-supervised paradigm, it offers a balance between localization accuracy\nand annotation cost. However, due to the absence of complete annotation, it is\nchallenging to align the video content with language descriptions, consequently\nhindering accurate moment prediction. To address this problem, we propose a new\nCOllaborative Temporal consistEncy Learning (COTEL) framework that leverages\nthe synergy between saliency detection and moment localization to strengthen\nthe video-language alignment. Specifically, we first design a frame- and a\nsegment-level Temporal Consistency Learning (TCL) module that models semantic\nalignment across frame saliencies and sentence-moment pairs. Then, we design a\ncross-consistency guidance scheme, including a Frame-level Consistency Guidance\n(FCG) and a Segment-level Consistency Guidance (SCG), that enables the two\ntemporal consistency learning paths to reinforce each other mutually. Further,\nwe introduce a Hierarchical Contrastive Alignment Loss (HCAL) to\ncomprehensively align the video and text query. Extensive experiments on two\nbenchmarks demonstrate that our method performs favorably against SoTA\napproaches. We will release all the source codes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17736", "pdf": "https://arxiv.org/pdf/2503.17736", "abs": "https://arxiv.org/abs/2503.17736", "authors": ["Yiming Zhao", "Yu Zeng", "Yukun Qi", "YaoYang Liu", "Lin Chen", "Zehui Chen", "Xikun Bao", "Jie Zhao", "Feng Zhao"], "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in the\nfield of video understanding recently. However, current benchmarks uniformly\nlean on text prompts for evaluation, which often necessitate complex\nreferential language and fail to provide precise spatial and temporal\nreferences. This limitation diminishes the experience and efficiency of\nhuman-model interaction. To address this limitation, we propose the Video\nVisual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically\ndesigned to evaluate LVLMs' video understanding capabilities in multimodal\nhuman-model interaction scenarios. V2P-Bench includes 980 unique videos and\n1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating\ninstance-level fine-grained understanding aligned with human cognition.\nBenchmarking results reveal that even the most powerful models perform poorly\non V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly\nlower than the human experts' 88.3%, highlighting the current shortcomings of\nLVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a\nfoundation for advancing multimodal human-model interaction and video\nunderstanding evaluation. Project page:\nhttps://github.com/gaotiexinqu/V2P-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18174", "pdf": "https://arxiv.org/pdf/2503.18174", "abs": "https://arxiv.org/abs/2503.18174", "authors": ["Weronika Łajewska", "Krisztian Balog"], "title": "GINGER: Grounded Information Nugget-Based Generation of Responses", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. To address them, we\npropose a modular pipeline for grounded response generation that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. The multistage pipeline encompasses nugget detection,\nclustering, ranking, top cluster summarization, and fluency enhancement. It\nguarantees grounding in specific facts, facilitates source attribution, and\nensures maximum information inclusion within length constraints. Extensive\nexperiments on the TREC RAG'24 dataset evaluated with the AutoNuggetizer\nframework demonstrate that GINGER achieves state-of-the-art performance on this\nbenchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "summarization"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17794", "pdf": "https://arxiv.org/pdf/2503.17794", "abs": "https://arxiv.org/abs/2503.17794", "authors": ["Ketan Suhaas Saichandran", "Xavier Thomas", "Prakhar Kaushik", "Deepti Ghadiyaram"], "title": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of up to +4% in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n85% of the prompts from the GenAI-Bench dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17820", "pdf": "https://arxiv.org/pdf/2503.17820", "abs": "https://arxiv.org/abs/2503.17820", "authors": ["Zheng Lin", "Nan Zhou", "Chen-Xi Du", "Deng-Ping Fan", "Shi-Min Hu"], "title": "RefCut: Interactive Segmentation with Reference Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation aims to segment the specified target on the image\nwith positive and negative clicks from users. Interactive ambiguity is a\ncrucial issue in this field, which refers to the possibility of multiple\ncompliant outcomes with the same clicks, such as selecting a part of an object\nversus the entire object, a single object versus a combination of multiple\nobjects, and so on. The existing methods cannot provide intuitive guidance to\nthe model, which leads to unstable output results and makes it difficult to\nmeet the large-scale and efficient annotation requirements for specific targets\nin some scenarios. To bridge this gap, we introduce RefCut, a reference-based\ninteractive segmentation framework designed to address part ambiguity and\nobject ambiguity in segmenting specific targets. Users only need to provide a\nreference image and corresponding reference masks, and the model will be\noptimized based on them, which greatly reduces the interactive burden on users\nwhen annotating a large number of such targets. In addition, to enrich these\ntwo kinds of ambiguous data, we propose a new Target Disassembly Dataset which\ncontains two subsets of part disassembly and object disassembly for evaluation.\nIn the combination evaluation of multiple datasets, our RefCut achieved\nstate-of-the-art performance. Extensive experiments and visualized results\ndemonstrate that RefCut advances the field of intuitive and controllable\ninteractive segmentation. Our code will be publicly available and the demo\nvideo is in https://www.lin-zheng.com/refcut.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18290", "pdf": "https://arxiv.org/pdf/2503.18290", "abs": "https://arxiv.org/abs/2503.18290", "authors": ["Paul K. Mandal"], "title": "When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; I.5.1"], "comment": "5 pages, 3 figures, 4 tables", "summary": "In this paper, I investigate the effectiveness of dataset cartography for\nextractive question answering on the SQuAD dataset. I begin by analyzing\nannotation artifacts in SQuAD and evaluate the impact of two adversarial\ndatasets, AddSent and AddOneSent, on an ELECTRA-small model. Using training\ndynamics, I partition SQuAD into easy-to-learn, ambiguous, and hard-to-learn\nsubsets. I then compare the performance of models trained on these subsets to\nthose trained on randomly selected samples of equal size. Results show that\ntraining on cartography-based subsets does not improve generalization to the\nSQuAD validation set or the AddSent adversarial set. While the hard-to-learn\nsubset yields a slightly higher F1 score on the AddOneSent dataset, the overall\ngains are limited. These findings suggest that dataset cartography provides\nlittle benefit for adversarial robustness in SQuAD-style QA tasks. I conclude\nby comparing these results to prior findings on SNLI and discuss possible\nreasons for the observed differences.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "question answering"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17827", "pdf": "https://arxiv.org/pdf/2503.17827", "abs": "https://arxiv.org/abs/2503.17827", "authors": ["Wenxuan Zhu", "Bing Li", "Cheng Zheng", "Jinjie Mai", "Jun Chen", "Letian Jiang", "Abdullah Hamdi", "Sara Rojas Martinez", "Chia-Wen Lin", "Mohamed Elhoseiny", "Bernard Ghanem"], "title": "4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D\nimage/video understanding capabilities. However, there are no publicly\nstandardized benchmarks to assess the abilities of MLLMs in understanding the\n4D objects (3D objects with temporal evolution over time). In this paper, we\nintroduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs\nin 4D object understanding, featuring tasks in 4D object Question Answering (4D\nobject QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse\ncategories, high-quality annotations, and tasks necessitating multi-view\nspatial-temporal understanding, different from existing 2D image/video-based\nbenchmarks. With 4D-Bench, we evaluate a wide range of open-source and\nclosed-source MLLMs. The results from the 4D object captioning experiment\nindicate that MLLMs generally exhibit weaker temporal understanding compared to\ntheir appearance understanding, notably, while open-source models approach\nclosed-source performance in appearance understanding, they show larger\nperformance gaps in temporal understanding. 4D object QA yields surprising\nfindings: even with simple single-object videos, MLLMs perform poorly, with\nstate-of-the-art GPT-4o achieving only 63\\% accuracy compared to the human\nbaseline of 91\\%. These findings highlight a substantial gap in 4D object\nunderstanding and the need for further advancements in MLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18485", "pdf": "https://arxiv.org/pdf/2503.18485", "abs": "https://arxiv.org/abs/2503.18485", "authors": ["Dawit Ketema Gete", "Bedru Yimam Ahamed", "Tadesse Destaw Belay", "Yohannes Ayana Ejigu", "Sukairaj Hafiz Imam", "Alemu Belay Tessema", "Mohammed Oumer Adem", "Tadesse Amare Belay", "Robert Geislinger", "Umma Aliyu Musa", "Martin Semmann", "Shamsuddeen Hassan Muhammad", "Henning Schreiber", "Seid Muhie Yimam"], "title": "Whispering in Amharic: Fine-tuning Whisper for Low-resource Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work explores fine-tuning OpenAI's Whisper automatic speech recognition\n(ASR) model for Amharic, a low-resource language, to improve transcription\naccuracy. While the foundational Whisper model struggles with Amharic due to\nlimited representation in its training data, we fine-tune it using datasets\nlike Mozilla Common Voice, FLEURS, and the BDU-speech dataset. The\nbest-performing model, Whispersmall-am, significantly improves when finetuned\non a mix of existing FLEURS data and new, unseen Amharic datasets. Training\nsolely on new data leads to poor performance, but combining it with FLEURS data\nreinforces the model, enabling better specialization in Amharic. We also\ndemonstrate that normalizing Amharic homophones significantly enhances Word\nError Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This study\nunderscores the importance of fine-tuning strategies and dataset composition\nfor improving ASR in low-resource languages, providing insights for future\nAmharic speech recognition research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17937", "pdf": "https://arxiv.org/pdf/2503.17937", "abs": "https://arxiv.org/abs/2503.17937", "authors": ["Zhi Zhang", "Daoyi Chen"], "title": "Cross-Domain Underwater Image Enhancement Guided by No-Reference Image Quality Assessment: A Transfer Learning Approach", "categories": ["cs.CV"], "comment": null, "summary": "Single underwater image enhancement (UIE) is a challenging ill-posed problem,\nbut its development is hindered by two major issues: (1) The labels in\nunderwater reference datasets are pseudo labels, relying on these pseudo ground\ntruths in supervised learning leads to domain discrepancy. (2) Underwater\nreference datasets are scarce, making training on such small datasets prone to\noverfitting and distribution shift. To address these challenges, we propose\nTrans-UIE, a transfer learning-based UIE model that captures the fundamental\nparadigms of UIE through pretraining and utilizes a dataset composed of both\nreference and non-reference datasets for fine-tuning. However, fine-tuning the\nmodel using only reconstruction loss may introduce confirmation bias. To\nmitigate this, our method leverages no-reference image quality assessment\n(NR-IQA) metrics from above-water scenes to guide the transfer learning process\nacross domains while generating enhanced images with the style of the\nabove-water image domain. Additionally, to reduce the risk of overfitting\nduring the pretraining stage, we introduce Pearson correlation loss.\nExperimental results on both full-reference and no-reference underwater\nbenchmark datasets demonstrate that Trans-UIE significantly outperforms\nstate-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "correlation"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18082", "pdf": "https://arxiv.org/pdf/2503.18082", "abs": "https://arxiv.org/abs/2503.18082", "authors": ["Nachuan Ma", "Zhengfei Song", "Qiang Hu", "Chuang-Wei Liu", "Yu Han", "Yanting Zhang", "Rui Fan", "Lihua Xie"], "title": "Vehicular Road Crack Detection with Deep Learning: A New Online Benchmark for Comprehensive Evaluation of Existing Algorithms", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In the emerging field of urban digital twins (UDTs), advancing intelligent\nroad inspection (IRI) vehicles with automatic road crack detection systems is\nessential for maintaining civil infrastructure. Over the past decade, deep\nlearning-based road crack detection methods have been developed to detect\ncracks more efficiently, accurately, and objectively, with the goal of\nreplacing manual visual inspection. Nonetheless, there is a lack of systematic\nreviews on state-of-the-art (SoTA) deep learning techniques, especially\ndata-fusion and label-efficient algorithms for this task. This paper thoroughly\nreviews the SoTA deep learning-based algorithms, including (1) supervised, (2)\nunsupervised, (3) semi-supervised, and (4) weakly-supervised methods developed\nfor road crack detection. Also, we create a dataset called UDTIRI-Crack,\ncomprising $2,500$ high-quality images from seven public annotated sources, as\nthe first extensive online benchmark in this field. Comprehensive experiments\nare conducted to compare the detection performance, computational efficiency,\nand generalizability of public SoTA deep learning-based algorithms for road\ncrack detection. In addition, the feasibility of foundation models and large\nlanguage models (LLMs) for road crack detection is explored. Afterwards, the\nexisting challenges and future development trends of deep learning-based road\ncrack detection algorithms are discussed. We believe this review can serve as\npractical guidance for developing intelligent road detection vehicles with the\nnext-generation road condition assessment systems. The released benchmark\nUDTIRI-Crack is available at https://udtiri.com/submission/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17736", "pdf": "https://arxiv.org/pdf/2503.17736", "abs": "https://arxiv.org/abs/2503.17736", "authors": ["Yiming Zhao", "Yu Zeng", "Yukun Qi", "YaoYang Liu", "Lin Chen", "Zehui Chen", "Xikun Bao", "Jie Zhao", "Feng Zhao"], "title": "V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in the\nfield of video understanding recently. However, current benchmarks uniformly\nlean on text prompts for evaluation, which often necessitate complex\nreferential language and fail to provide precise spatial and temporal\nreferences. This limitation diminishes the experience and efficiency of\nhuman-model interaction. To address this limitation, we propose the Video\nVisual Prompt Benchmark(V2P-Bench), a comprehensive benchmark specifically\ndesigned to evaluate LVLMs' video understanding capabilities in multimodal\nhuman-model interaction scenarios. V2P-Bench includes 980 unique videos and\n1,172 QA pairs, covering 5 main tasks and 12 dimensions, facilitating\ninstance-level fine-grained understanding aligned with human cognition.\nBenchmarking results reveal that even the most powerful models perform poorly\non V2P-Bench (65.4% for GPT-4o and 67.9% for Gemini-1.5-Pro), significantly\nlower than the human experts' 88.3%, highlighting the current shortcomings of\nLVLMs in understanding video visual prompts. We hope V2P-Bench will serve as a\nfoundation for advancing multimodal human-model interaction and video\nunderstanding evaluation. Project page:\nhttps://github.com/gaotiexinqu/V2P-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17979", "pdf": "https://arxiv.org/pdf/2503.17979", "abs": "https://arxiv.org/abs/2503.17979", "authors": ["Weixiang Zhao", "Xingyu Sui", "Jiahe Guo", "Yulin Hu", "Yang Deng", "Yanyan Zhao", "Bing Qin", "Wanxiang Che", "Tat-Seng Chua", "Ting Liu"], "title": "Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages. Work in progress", "summary": "Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3\nand DeepSeek-R1, have demonstrated remarkable performance in specialized\nreasoning tasks through human-like deliberative thinking and long\nchain-of-thought reasoning. However, our systematic evaluation across various\nmodel families (DeepSeek, Qwen, and LLaMA) and scales (7B to 671B) reveals that\nacquiring these deliberative reasoning capabilities significantly reduces the\nfoundational capabilities of LRMs, including notable declines in helpfulness\nand harmlessness, alongside substantially increased inference costs.\nImportantly, we demonstrate that adaptive reasoning -- employing modes like\nZero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate\nthese drawbacks. Our empirical insights underline the critical need for\ndeveloping more versatile LRMs capable of dynamically allocating inference-time\ncompute according to specific task characteristics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "helpfulness", "harmlessness"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18141", "pdf": "https://arxiv.org/pdf/2503.18141", "abs": "https://arxiv.org/abs/2503.18141", "authors": ["Diwei Wang", "Cédric Bobenrieth", "Hyewon Seo"], "title": "AGIR: Assessing 3D Gait Impairment with Reasoning based on LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Assessing gait impairment plays an important role in early diagnosis, disease\nmonitoring, and treatment evaluation for neurodegenerative diseases. Despite\nits widespread use in clinical practice, it is limited by subjectivity and a\nlack of precision. While recent deep learning-based approaches have\nconsistently improved classification accuracies, they often lack\ninterpretability, hindering their utility in clinical decision-making. To\novercome these challenges, we introduce AGIR, a novel pipeline consisting of a\npre-trained VQ-VAE motion tokenizer and a subsequent Large Language Model (LLM)\nfine-tuned over pairs of motion tokens and Chain-of-Thought (CoT) reasonings.\nTo fine-tune an LLM for pathological gait analysis, we first introduce a\nmultimodal dataset by adding rationales dedicated to MDS-UPDRS gait score\nassessment to an existing PD gait dataset. We then introduce a two-stage\nsupervised fine-tuning (SFT) strategy to enhance the LLM's motion comprehension\nwith pathology-specific knowledge. This strategy includes: 1) a generative\nstage that aligns gait motions with analytic descriptions through bidirectional\nmotion-description generation, 2) a reasoning stage that integrates logical\nChain-of-Thought (CoT) reasoning for impairment assessment with UPDRS gait\nscore. Validation on an existing dataset and comparisons with state-of-the-art\nmethods confirm the robustness and accuracy of our pipeline, demonstrating its\nability to assign gait impairment scores from motion input with clinically\nmeaningful rationales.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18484", "pdf": "https://arxiv.org/pdf/2503.18484", "abs": "https://arxiv.org/abs/2503.18484", "authors": ["Junyuan Gao", "Jiahe Song", "Jiang Wu", "Runchuan Zhu", "Guanlin Shen", "Shasha Wang", "Xingjian Wei", "Haote Yang", "Songyang Zhang", "Weijia Li", "Bin Wang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Equal contribution: Junyuan Gao, Jiahe Song, Jiang Wu; Corresponding\n  author: Conghui He", "summary": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18177", "pdf": "https://arxiv.org/pdf/2503.18177", "abs": "https://arxiv.org/abs/2503.18177", "authors": ["Gulnaz Gimaletdinova", "Dim Shaiakhmetov", "Madina Akpaeva", "Mukhammadmuso Abduzhabbarov", "Kadyrmamat Momunov"], "title": "Training A Neural Network For Partially Occluded Road Sign Identification In The Context Of Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "The increasing number of autonomous vehicles and the rapid development of\ncomputer vision technologies underscore the particular importance of conducting\nresearch on the accuracy of traffic sign recognition. Numerous studies in this\nfield have already achieved significant results, demonstrating high\neffectiveness in addressing traffic sign recognition tasks. However, the task\nbecomes considerably more complex when a sign is partially obscured by\nsurrounding objects, such as tree branches, billboards, or other elements of\nthe urban environment. In our study, we investigated how partial occlusion of\ntraffic signs affects their recognition. For this purpose, we collected a\ndataset comprising 5,746 images, including both fully visible and partially\noccluded signs, and made it publicly available. Using this dataset, we compared\nthe performance of our custom convolutional neural network (CNN), which\nachieved 96% accuracy, with models trained using transfer learning. The best\nresult was obtained by VGG16 with full layer unfreezing, reaching 99% accuracy.\nAdditional experiments revealed that models trained solely on fully visible\nsigns lose effectiveness when recognizing occluded signs. This highlights the\ncritical importance of incorporating real-world data with partial occlusion\ninto training sets to ensure robust model performance in complex practical\nscenarios and to enhance the safety of autonomous driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18227", "pdf": "https://arxiv.org/pdf/2503.18227", "abs": "https://arxiv.org/abs/2503.18227", "authors": ["Yiheng Zhong", "Zihong Luo", "Chengzhi Liu", "Feilong Tang", "Zelin Peng", "Ming Hu", "Yingzhen Hu", "Jionglong Su", "Zongyuan Geand", "Imran Razzak"], "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18282", "pdf": "https://arxiv.org/pdf/2503.18282", "abs": "https://arxiv.org/abs/2503.18282", "authors": ["Kazuhiro Yamada", "Li Yin", "Qingrui Hu", "Ning Ding", "Shunsuke Iwashita", "Jun Ichikawa", "Kiwamu Kotani", "Calvin Yeung", "Keisuke Fujii"], "title": "TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking, player identification, and pose estimation are\nfundamental components of sports analytics, essential for analyzing player\nmovements, performance, and tactical strategies. However, existing datasets and\nmethodologies primarily target mainstream team sports such as soccer and\nconventional 5-on-5 basketball, often overlooking scenarios involving\nfixed-camera setups commonly used at amateur levels, less mainstream sports, or\ndatasets that explicitly incorporate pose annotations. In this paper, we\npropose the TrackID3x3 dataset, the first publicly available comprehensive\ndataset specifically designed for multi-player tracking, player identification,\nand pose estimation in 3x3 basketball scenarios. The dataset comprises three\ndistinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera\nfootage), capturing diverse full-court camera perspectives and environments. We\nalso introduce the Track-ID task, a simplified variant of the game state\nreconstruction task that excludes field detection and focuses exclusively on\nfixed-camera scenarios. To evaluate performance, we propose a baseline\nalgorithm called Track-ID algorithm, tailored to assess tracking and\nidentification quality. Furthermore, our benchmark experiments, utilizing\nrecent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose\nestimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results\nand highlight remaining challenges. Our dataset and evaluation benchmarks\nprovide a solid foundation for advancing automated analytics in 3x3 basketball.\nDataset and code will be available at\nhttps://github.com/open-starlab/TrackID3x3.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18286", "pdf": "https://arxiv.org/pdf/2503.18286", "abs": "https://arxiv.org/abs/2503.18286", "authors": ["Siyuan Cheng", "Lingjuan Lyu", "Zhenting Wang", "Xiangyu Zhang", "Vikash Sehwag"], "title": "CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative AI, it is now possible to synthesize\nhigh-quality images in a few seconds. Despite the power of these technologies,\nthey raise significant concerns regarding misuse. Current efforts to\ndistinguish between real and AI-generated images may lack generalization, being\neffective for only certain types of generative models and susceptible to\npost-processing techniques like JPEG compression. To overcome these\nlimitations, we propose a novel framework, Co-Spy, that first enhances existing\nsemantic features (e.g., the number of fingers in a hand) and artifact features\n(e.g., pixel value differences), and then adaptively integrates them to achieve\nmore general and robust synthetic image detection. Additionally, we create\nCo-Spy-Bench, a comprehensive dataset comprising 5 real image datasets and 22\nstate-of-the-art generative models, including the latest models like FLUX. We\nalso collect 50k synthetic images in the wild from the Internet to enable\nevaluation in a more practical setting. Our extensive evaluations demonstrate\nthat our detector outperforms existing methods under identical training\nconditions, achieving an average accuracy improvement of approximately 11% to\n34%. The code is available at https://github.com/Megum1/Co-Spy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18294", "pdf": "https://arxiv.org/pdf/2503.18294", "abs": "https://arxiv.org/abs/2503.18294", "authors": ["Fiseha B. Tesema", "Alejandro Guerra Manzanares", "Tianxiang Cui", "Qian Zhang", "Moses Solomon", "Sean He"], "title": "LGPS: A Lightweight GAN-Based Approach for Polyp Segmentation in Colonoscopy Images", "categories": ["cs.CV"], "comment": "10 pages, 6 Figures", "summary": "Colorectal cancer (CRC) is a major global cause of cancer-related deaths,\nwith early polyp detection and removal during colonoscopy being crucial for\nprevention. While deep learning methods have shown promise in polyp\nsegmentation, challenges such as high computational costs, difficulty in\nsegmenting small or low-contrast polyps, and limited generalizability across\ndatasets persist. To address these issues, we propose LGPS, a lightweight\nGAN-based framework for polyp segmentation. LGPS incorporates three key\ninnovations: (1) a MobileNetV2 backbone enhanced with modified residual blocks\nand Squeeze-and-Excitation (ResE) modules for efficient feature extraction; (2)\nConvolutional Conditional Random Fields (ConvCRF) for precise boundary\nrefinement; and (3) a hybrid loss function combining Binary Cross-Entropy,\nWeighted IoU Loss, and Dice Loss to address class imbalance and enhance\nsegmentation accuracy. LGPS is validated on five benchmark datasets and\ncompared with state-of-the-art(SOTA) methods. On the largest and challenging\nPolypGen test dataset, LGPS achieves a Dice of 0.7299 and an IoU of 0.7867,\noutperformed all SOTA works and demonstrating robust generalization. With only\n1.07 million parameters, LGPS is 17 times smaller than the smallest existing\nmodel, making it highly suitable for real-time clinical applications. Its\nlightweight design and strong performance underscore its potential for\nimproving early CRC diagnosis. Code is available at\nhttps://github.com/Falmi/LGPS/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18484", "pdf": "https://arxiv.org/pdf/2503.18484", "abs": "https://arxiv.org/abs/2503.18484", "authors": ["Junyuan Gao", "Jiahe Song", "Jiang Wu", "Runchuan Zhu", "Guanlin Shen", "Shasha Wang", "Xingjian Wei", "Haote Yang", "Songyang Zhang", "Weijia Li", "Bin Wang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "Equal contribution: Junyuan Gao, Jiahe Song, Jiang Wu; Corresponding\n  author: Conghui He", "summary": "Existing multilingual benchmarks for Large Vision Language Models (LVLMs)\nsuffer from limitations including language-specific content biases, disjointed\nmultimodal input formats, and a lack of safety evaluation. To address these\ngaps, we propose PM4Bench, the first Parallel Multilingual Multi-Modal\nMulti-task Benchmark for LVLMs. PM4Bench features a parallel corpus design\nacross 10 languages, enabling fair and accurate cross-lingual comparisons. It\nincludes the vision setting where text and queries are embedded in images,\nrequiring LVLMs to simultaneously \"see\", \"read\", and \"think\", aligning with\nreal-world applications. Additionally, PM\\textsuperscript{4}Bench incorporates\nsafety evaluations, addressing critical oversight in existing multilingual\nbenchmarks. Using PM4Bench, we evaluate 11 mainstream LVLMs, revealing\nsignificant cross-linguistic performance disparities, particularly in vision\nsettings, and identifying OCR capability as a key determinant of these\nimbalances. We will release PM4Bench at https://github.com/opendatalab/PM4Bench .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18536", "pdf": "https://arxiv.org/pdf/2503.18536", "abs": "https://arxiv.org/abs/2503.18536", "authors": ["Erjian Guo", "Zhen Zhao", "Zicheng Wang", "Tong Chen", "Yunyi Liu", "Luping Zhou"], "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels", "categories": ["cs.CV"], "comment": null, "summary": "Medical Visual Question Answering (Med-VQA) systems benefit the\ninterpretation of medical images containing critical clinical information.\nHowever, the challenge of noisy labels and limited high-quality datasets\nremains underexplored. To address this, we establish the first benchmark for\nnoisy labels in Med-VQA by simulating human mislabeling with semantically\ndesigned noise types. More importantly, we introduce the DiN framework, which\nleverages a diffusion model to handle noisy labels in Med-VQA. Unlike the\ndominant classification-based VQA approaches that directly predict answers, our\nAnswer Diffuser (AD) module employs a coarse-to-fine process, refining answer\ncandidates with a diffusion model for improved accuracy. The Answer Condition\nGenerator (ACG) further enhances this process by generating task-specific\nconditional information via integrating answer embeddings with fused\nimage-question features. To address label noise, our Noisy Label\nRefinement(NLR) module introduces a robust loss function and dynamic answer\nadjustment to further boost the performance of the AD module.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18705", "pdf": "https://arxiv.org/pdf/2503.18705", "abs": "https://arxiv.org/abs/2503.18705", "authors": ["Inseung Hwang", "Kiseok Choi", "Hyunho Ha", "Min H. Kim"], "title": "Benchmarking Burst Super-Resolution for Polarization Images: Noise Dataset and Analysis", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Snapshot polarization imaging calculates polarization states from linearly\npolarized subimages. To achieve this, a polarization camera employs a double\nBayer-patterned sensor to capture both color and polarization. It demonstrates\nlow light efficiency and low spatial resolution, resulting in increased noise\nand compromised polarization measurements. Although burst super-resolution\neffectively reduces noise and enhances spatial resolution, applying it to\npolarization imaging poses challenges due to the lack of tailored datasets and\nreliable ground truth noise statistics. To address these issues, we introduce\nPolarNS and PolarBurstSR, two innovative datasets developed specifically for\npolarization imaging. PolarNS provides characterization of polarization noise\nstatistics, facilitating thorough analysis, while PolarBurstSR functions as a\nbenchmark for burst super-resolution in polarization images. These datasets,\ncollected under various real-world conditions, enable comprehensive evaluation.\nAdditionally, we present a model for analyzing polarization noise to quantify\nnoise propagation, tested on a large dataset captured in a darkroom\nenvironment. As part of our application, we compare the latest burst\nsuper-resolution models, highlighting the advantages of training tailored to\npolarization compared to RGB-based methods. This work establishes a benchmark\nfor polarization burst super-resolution and offers critical insights into noise\npropagation, thereby enhancing polarization image reconstruction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "testbed"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17391", "pdf": "https://arxiv.org/pdf/2503.17391", "abs": "https://arxiv.org/abs/2503.17391", "authors": ["Atharva Deo", "Nicholas Matsumoto", "Sun Kim", "Peter Wager", "Randy G. Tsai", "Aaron Denmark", "Cherine Yang", "Xi Li", "Jay Moran", "Miguel Hernandez", "Andrew J. Hung"], "title": "AI-driven Automation of End-to-end Assessment of Suturing Expertise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present an AI based approach to automate the End-to-end Assessment of\nSuturing Expertise (EASE), a suturing skills assessment tool that\ncomprehensively defines criteria around relevant sub-skills.1 While EASE\nprovides granular skills assessment related to suturing to provide trainees\nwith an objective evaluation of their aptitude along with actionable insights,\nthe scoring process is currently performed by human evaluators, which is time\nand resource consuming. The AI based approach solves this by enabling real-time\nscore prediction with minimal resources during model inference. This enables\nthe possibility of real-time feedback to the surgeons/trainees, potentially\naccelerating the learning process for the suturing task and mitigating critical\nerrors during the surgery, improving patient outcomes. In this study, we focus\non the following 7 EASE domains that come under 3 suturing phases: 1) Needle\nHandling: Number of Repositions, Needle Hold Depth, Needle Hold Ratio, and\nNeedle Hold Angle; 2) Needle Driving: Driving Smoothness, and Wrist Rotation;\n3) Needle Withdrawal: Wrist Rotation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17460", "pdf": "https://arxiv.org/pdf/2503.17460", "abs": "https://arxiv.org/abs/2503.17460", "authors": ["Reem Gody", "Mahmoud Goudy", "Ahmed Y. Tawfik"], "title": "ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we present ConvoGen: an innovative framework for generating\nsynthetic conversational data using multi-agent systems. Our method leverages\nfew-shot learning and introduces iterative sampling from a dynamically updated\nfew-shot hub to create diverse and realistic conversational scenarios. The\ngenerated data has numerous applications, including training and evaluating\nconversational AI models, and augmenting existing datasets for tasks like\nconversational intent classification or conversation summarization. Our\nexperiments demonstrate the effectiveness of this method in producing\nhigh-quality diverse synthetic conversational data, highlighting its potential\nto enhance the development and evaluation of conversational AI systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "summarization"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17406", "pdf": "https://arxiv.org/pdf/2503.17406", "abs": "https://arxiv.org/abs/2503.17406", "authors": ["Haochen Zhang", "Nader Zantout", "Pujith Kachana", "Ji Zhang", "Wenshan Wang"], "title": "IRef-VLA: A Benchmark for Interactive Referential Grounding with Imperfect Language in 3D Scenes", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025. Code available at\n  https://github.com/HaochenZ11/IRef-VLA. arXiv admin note: text overlap with\n  arXiv:2411.03540", "summary": "With the recent rise of large language models, vision-language models, and\nother general foundation models, there is growing potential for multimodal,\nmulti-task robotics that can operate in diverse environments given natural\nlanguage input. One such application is indoor navigation using natural\nlanguage instructions. However, despite recent progress, this problem remains\nchallenging due to the 3D spatial reasoning and semantic understanding\nrequired. Additionally, the language used may be imperfect or misaligned with\nthe scene, further complicating the task. To address this challenge, we curate\na benchmark dataset, IRef-VLA, for Interactive Referential Vision and\nLanguage-guided Action in 3D Scenes with imperfect references. IRef-VLA is the\nlargest real-world dataset for the referential grounding task, consisting of\nover 11.5K scanned 3D rooms from existing datasets, 7.6M heuristically\ngenerated semantic relations, and 4.7M referential statements. Our dataset also\ncontains semantic object and room annotations, scene graphs, navigable free\nspace annotations, and is augmented with statements where the language has\nimperfections or ambiguities. We verify the generalizability of our dataset by\nevaluating with state-of-the-art models to obtain a performance baseline and\nalso develop a graph-search baseline to demonstrate the performance bound and\ngeneration of alternatives using scene-graph knowledge. With this benchmark, we\naim to provide a resource for 3D scene understanding that aids the development\nof robust, interactive navigation systems. The dataset and all source code is\npublicly released at https://github.com/HaochenZ11/IRef-VLA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17509", "pdf": "https://arxiv.org/pdf/2503.17509", "abs": "https://arxiv.org/abs/2503.17509", "authors": ["Joseph Gatto", "Parker Seegmiller", "Timothy Burdick", "Inas S. Khayal", "Sarah DeLozier", "Sarah M. Preum"], "title": "Follow-up Question Generation For Enhanced Patient-Provider Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "17 Pages, 7 Figures, 6 Tables", "summary": "Follow-up question generation is an essential feature of dialogue systems as\nit can reduce conversational ambiguity and enhance modeling complex\ninteractions. Conversational contexts often pose core NLP challenges such as\n(i) extracting relevant information buried in fragmented data sources, and (ii)\nmodeling parallel thought processes. These two challenges occur frequently in\nmedical dialogue as a doctor asks questions based not only on patient\nutterances but also their prior EHR data and current diagnostic hypotheses.\nAsking medical questions in asynchronous conversations compounds these issues\nas doctors can only rely on static EHR information to motivate follow-up\nquestions.\n  To address these challenges, we introduce FollowupQ, a novel framework for\nenhancing asynchronous medical conversation. FollowupQ is a multi-agent\nframework that processes patient messages and EHR data to generate personalized\nfollow-up questions, clarifying patient-reported medical conditions. FollowupQ\nreduces requisite provider follow-up communications by 34%. It also improves\nperformance by 17% and 5% on real and synthetic data, respectively. We also\nrelease the first public dataset of asynchronous medical messages with linked\nEHR data alongside 2,300 follow-up questions written by clinical experts for\nthe wider NLP research community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17493", "pdf": "https://arxiv.org/pdf/2503.17493", "abs": "https://arxiv.org/abs/2503.17493", "authors": ["Aidos Konyspay", "Pakizar Shamoi", "Malika Ziyada", "Zhusup Smambayev"], "title": "Meme Similarity and Emotion Detection using Multimodal Analysis", "categories": ["cs.CV"], "comment": "Have been submitted to IEEE for consideration", "summary": "Internet memes are a central element of online culture, blending images and\ntext. While substantial research has focused on either the visual or textual\ncomponents of memes, little attention has been given to their interplay. This\ngap raises a key question: What methodology can effectively compare memes and\nthe emotions they elicit? Our study employs a multimodal methodological\napproach, analyzing both the visual and textual elements of memes.\nSpecifically, we perform a multimodal CLIP (Contrastive Language-Image\nPre-training) model for grouping similar memes based on text and visual content\nembeddings, enabling robust similarity assessments across modalities. Using the\nReddit Meme Dataset and Memotion Dataset, we extract low-level visual features\nand high-level semantic features to identify similar meme pairs. To validate\nthese automated similarity assessments, we conducted a user study with 50\nparticipants, asking them to provide yes/no responses regarding meme similarity\nand their emotional reactions. The comparison of experimental results with\nhuman judgments showed a 67.23\\% agreement, suggesting that the computational\napproach aligns well with human perception. Additionally, we implemented a\ntext-based classifier using the DistilBERT model to categorize memes into one\nof six basic emotions. The results indicate that anger and joy are the dominant\nemotions in memes, with motivational memes eliciting stronger emotional\nresponses. This research contributes to the study of multimodal memes,\nenhancing both language-based and visual approaches to analyzing and improving\nonline visual communication and user experiences. Furthermore, it provides\ninsights for better content moderation strategies in online platforms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "agreement"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17497", "pdf": "https://arxiv.org/pdf/2503.17497", "abs": "https://arxiv.org/abs/2503.17497", "authors": ["Daniel Kuhse", "Harun Teper", "Sebastian Buschjäger", "Chien-Yao Wang", "Jian-Jia Chen"], "title": "You Only Look Once at Anytime (AnytimeYOLO): Analysis and Optimization of Early-Exits for Object-Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce AnytimeYOLO, a family of variants of the YOLO architecture that\nenables anytime object detection. Our AnytimeYOLO networks allow for\ninterruptible inference, i.e., they provide a prediction at any point in time,\na property desirable for safety-critical real-time applications.\n  We present structured explorations to modify the YOLO architecture, enabling\nearly termination to obtain intermediate results. We focus on providing\nfine-grained control through high granularity of available termination points.\nFirst, we formalize Anytime Models as a special class of prediction models that\noffer anytime predictions. Then, we discuss a novel transposed variant of the\nYOLO architecture, that changes the architecture to enable better early\npredictions and greater freedom for the order of processing stages. Finally, we\npropose two optimization algorithms that, given an anytime model, can be used\nto determine the optimal exit execution order and the optimal subset of\nearly-exits to select for deployment in low-resource environments. We evaluate\nthe anytime performance and trade-offs of design choices, proposing a new\nanytime quality metric for this purpose. In particular, we also discuss key\nchallenges for anytime inference that currently make its deployment costly.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17739", "pdf": "https://arxiv.org/pdf/2503.17739", "abs": "https://arxiv.org/abs/2503.17739", "authors": ["Chatrine Qwaider", "Bashar Alhafni", "Kirill Chirkunov", "Nizar Habash", "Ted Briscoe"], "title": "Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection", "categories": ["cs.CL"], "comment": null, "summary": "Automated Essay Scoring (AES) plays a crucial role in assessing language\nlearners' writing quality, reducing grading workload, and providing real-time\nfeedback. Arabic AES systems are particularly challenged by the lack of\nannotated essay datasets. This paper presents a novel framework leveraging\nLarge Language Models (LLMs) and Transformers to generate synthetic Arabic\nessay datasets for AES. We prompt an LLM to generate essays across CEFR\nproficiency levels and introduce controlled error injection using a fine-tuned\nStandard Arabic BERT model for error type prediction. Our approach produces\nrealistic human-like essays, contributing a dataset of 3,040 annotated essays.\nAdditionally, we develop a BERT-based auto-marking system for accurate and\nscalable Arabic essay evaluation. Experimental results demonstrate the\neffectiveness of our framework in improving Arabic AES performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17536", "pdf": "https://arxiv.org/pdf/2503.17536", "abs": "https://arxiv.org/abs/2503.17536", "authors": ["Nusrat Munia", "Abdullah-Al-Zubaer Imran"], "title": "DermDiff: Generative Diffusion Model for Mitigating Racial Biases in Dermatology Diagnosis", "categories": ["cs.CV"], "comment": "Paper presented at ADSMI@MICCAI 2024", "summary": "Skin diseases, such as skin cancer, are a significant public health issue,\nand early diagnosis is crucial for effective treatment. Artificial intelligence\n(AI) algorithms have the potential to assist in triaging benign vs malignant\nskin lesions and improve diagnostic accuracy. However, existing AI models for\nskin disease diagnosis are often developed and tested on limited and biased\ndatasets, leading to poor performance on certain skin tones. To address this\nproblem, we propose a novel generative model, named DermDiff, that can generate\ndiverse and representative dermoscopic image data for skin disease diagnosis.\nLeveraging text prompting and multimodal image-text learning, DermDiff improves\nthe representation of underrepresented groups (patients, diseases, etc.) in\nhighly imbalanced datasets. Our extensive experimentation showcases the\neffectiveness of DermDiff in terms of high fidelity and diversity. Furthermore,\ndownstream evaluation suggests the potential of DermDiff in mitigating racial\nbiases for dermatology diagnosis. Our code is available at\nhttps://github.com/Munia03/DermDiff", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17574", "pdf": "https://arxiv.org/pdf/2503.17574", "abs": "https://arxiv.org/abs/2503.17574", "authors": ["Simona Kocour", "Assia Benbihi", "Aikaterini Adam", "Torsten Sattler"], "title": "Is there anything left? Measuring semantic residuals of objects removed from 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Searching in and editing 3D scenes has become extremely intuitive with\ntrainable scene representations that allow linking human concepts to elements\nin the scene. These operations are often evaluated on the basis of how\naccurately the searched element is segmented or extracted from the scene. In\nthis paper, we address the inverse problem, that is, how much of the searched\nelement remains in the scene after it is removed. This question is particularly\nimportant in the context of privacy-preserving mapping when a user reconstructs\na 3D scene and wants to remove private elements before sharing the map. To the\nbest of our knowledge, this is the first work to address this question. To\nanswer this, we propose a quantitative evaluation that measures whether a\nremoval operation leaves object residuals that can be reasoned over. The scene\nis not private when such residuals are present. Experiments on state-of-the-art\nscene representations show that the proposed metrics are meaningful and\nconsistent with the user study that we also present. We also propose a method\nto refine the removal based on spatial and semantic consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17882", "pdf": "https://arxiv.org/pdf/2503.17882", "abs": "https://arxiv.org/abs/2503.17882", "authors": ["Shengyun Si", "Xinpeng Wang", "Guangyao Zhai", "Nassir Navab", "Barbara Plank"], "title": "Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 23 figures", "summary": "Recent advancements in large language models (LLMs) have demonstrated that\nfine-tuning and human alignment can render LLMs harmless. In practice, such\n\"harmlessness\" behavior is mainly achieved by training models to reject harmful\nrequests, such as \"Explain how to burn down my neighbor's house\", where the\nmodel appropriately declines to respond. However, this approach can\ninadvertently result in false refusal, where models reject benign queries as\nwell, such as \"Tell me how to kill a Python process\". In this work, we\ndemonstrate that prompting safety reflection before generating a response can\nmitigate false refusal behavior. Building on this finding, we introduce the\nThink-Before-Refusal (TBR) schema and conduct safety-aware instruction\nfine-tuning incorporating safety reflection. In an ablation study across 15\npre-trained models, we show that models fine-tuned with safety reflection\nsignificantly reduce false refusal behavior while maintaining safety and\noverall performance compared to those fine-tuned without safety reflection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["harmlessness", "safety"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17900", "pdf": "https://arxiv.org/pdf/2503.17900", "abs": "https://arxiv.org/abs/2503.17900", "authors": ["Hsin-Ling Hsu", "Cong-Tinh Dao", "Luning Wang", "Zitao Shuai", "Thao Nguyen Minh Phan", "Jun-En Ding", "Chun-Chieh Liao", "Pengfei Hu", "Xiaoxue Han", "Chih-Ho Hsu", "Dongsheng Luo", "Wen-Chih Peng", "Feng Liu", "Fang-Ming Hung", "Chenwei Wu"], "title": "MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation", "categories": ["cs.CL"], "comment": null, "summary": "Despite recent success in applying large language models (LLMs) to electronic\nhealth records (EHR), most systems focus primarily on assessment rather than\ntreatment planning. We identify three critical limitations in current\napproaches: they generate treatment plans in a single pass rather than\nfollowing the sequential reasoning process used by clinicians; they rarely\nincorporate patient-specific historical context; and they fail to effectively\ndistinguish between subjective and objective clinical information. Motivated by\nthe SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce\nMedPlan, a novel framework that structures LLM reasoning to align with\nreal-life clinician workflows. Our approach employs a two-stage architecture\nthat first generates a clinical assessment based on patient symptoms and\nobjective data, then formulates a structured treatment plan informed by this\nassessment and enriched with patient-specific information through\nretrieval-augmented generation. Comprehensive evaluation demonstrates that our\nmethod significantly outperforms baseline approaches in both assessment\naccuracy and treatment plan quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17641", "pdf": "https://arxiv.org/pdf/2503.17641", "abs": "https://arxiv.org/abs/2503.17641", "authors": ["Chi Zhang", "Chengjian Feng", "Feng Yan", "Qiming Zhang", "Mingjin Zhang", "Yujie Zhong", "Jing Zhang", "Lin Ma"], "title": "InstructVEdit: A Holistic Approach for Instructional Video Editing", "categories": ["cs.CV"], "comment": "https://o937-blip.github.io/InstructVEdit", "summary": "Video editing according to instructions is a highly challenging task due to\nthe difficulty in collecting large-scale, high-quality edited video pair data.\nThis scarcity not only limits the availability of training data but also\nhinders the systematic exploration of model architectures and training\nstrategies. While prior work has improved specific aspects of video editing\n(e.g., synthesizing a video dataset using image editing techniques or\ndecomposed video editing training), a holistic framework addressing the above\nchallenges remains underexplored. In this study, we introduce InstructVEdit, a\nfull-cycle instructional video editing approach that: (1) establishes a\nreliable dataset curation workflow to initialize training, (2) incorporates two\nmodel architectural improvements to enhance edit quality while preserving\ntemporal consistency, and (3) proposes an iterative refinement strategy\nleveraging real-world data to enhance generalization and minimize train-test\ndiscrepancies. Extensive experiments show that InstructVEdit achieves\nstate-of-the-art performance in instruction-based video editing, demonstrating\nrobust adaptability to diverse real-world scenarios. Project page:\nhttps://o937-blip.github.io/InstructVEdit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "iterative refinement"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17922", "pdf": "https://arxiv.org/pdf/2503.17922", "abs": "https://arxiv.org/abs/2503.17922", "authors": ["Youhui Zuo", "Sibo Wei", "Chen Zhang", "Zhuorui Liu", "Wenpeng Lu", "Dawei Song"], "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference", "categories": ["cs.CL"], "comment": null, "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17933", "pdf": "https://arxiv.org/pdf/2503.17933", "abs": "https://arxiv.org/abs/2503.17933", "authors": ["Justice Ou", "Tinglin Huang", "Yilun Zhao", "Ziyang Yu", "Peiqing Lu", "Rex Ying"], "title": "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "To improve the reliability of Large Language Models (LLMs) in clinical\napplications, retrieval-augmented generation (RAG) is extensively applied to\nprovide factual medical knowledge. However, beyond general medical knowledge\nfrom open-ended datasets, clinical case-based knowledge is also critical for\neffective medical reasoning, as it provides context grounded in real-world\npatient experiences. Motivated by this, we propose Experience Retrieval\nAugmentation - ExpRAG framework based on Electronic Health Record (EHR), aiming\nto offer the relevant context from other patients' discharge reports. ExpRAG\nperforms retrieval through a coarse-to-fine process, utilizing an EHR-based\nreport ranker to efficiently identify similar patients, followed by an\nexperience retriever to extract task-relevant content for enhanced medical\nreasoning. To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset\nwith 1,280 discharge-related questions across diagnosis, medication, and\ninstruction tasks. Each problem is generated using EHR data to ensure realistic\nand challenging scenarios. Experimental results demonstrate that ExpRAG\nconsistently outperforms a text-based ranker, achieving an average relative\nimprovement of 5.2%, highlighting the importance of case-based knowledge for\nmedical reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17963", "pdf": "https://arxiv.org/pdf/2503.17963", "abs": "https://arxiv.org/abs/2503.17963", "authors": ["Guijin Son", "Hyunwoo Ko", "Haneral Jung", "Chami Hwang"], "title": "Won: Establishing Best Practices for Korean Financial NLP", "categories": ["cs.CL"], "comment": "The training dataset is uploaded here:\n  https://huggingface.co/datasets/KRX-Data/Won-Instruct. The model will be\n  updated shortly", "summary": "In this work, we present the first open leaderboard for evaluating Korean\nlarge language models focused on finance. Operated for about eight weeks, the\nleaderboard evaluated 1,119 submissions on a closed benchmark covering five\nMCQA categories: finance and accounting, stock price prediction, domestic\ncompany analysis, financial markets, and financial agent tasks and one\nopen-ended qa task. Building on insights from these evaluations, we release an\nopen instruction dataset of 80k instances and summarize widely used training\nstrategies observed among top-performing models. Finally, we introduce Won, a\nfully open and transparent LLM built using these best practices. We hope our\ncontributions help advance the development of better and safer financial LLMs\nfor Korean and other languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17669", "pdf": "https://arxiv.org/pdf/2503.17669", "abs": "https://arxiv.org/abs/2503.17669", "authors": ["Yuheng Feng", "Jianhui Wang", "Kun Li", "Sida Li", "Tianyu Shi", "Haoyue Han", "Miao Zhang", "Xueqian Wang"], "title": "TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Although text-to-image generation technologies have made significant\nadvancements, they still face challenges when dealing with ambiguous prompts\nand aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase\nDialogue Refinement and Co-Adaptation), addresses these issues by enhancing\nimage generation through iterative user interaction. It consists of two phases:\nthe Initial Generation Phase, which creates base images based on user prompts,\nand the Interactive Refinement Phase, which integrates user feedback through\nthree key modules. The Dialogue-to-Prompt (D2P) module ensures that user\nfeedback is effectively transformed into actionable prompts, which improves the\nalignment between user intent and model input. By evaluating generated outputs\nagainst user expectations, the Feedback-Reflection (FR) module identifies\ndiscrepancies and facilitates improvements. In an effort to ensure consistently\nhigh-quality results, the Adaptive Optimization (AO) module fine-tunes the\ngeneration process by balancing user preferences and maintaining prompt\nfidelity. Experimental results show that TDRI outperforms existing methods by\nachieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and\nthe highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In\niterative feedback tasks, user satisfaction increased to 88% after 8 rounds,\nwith diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to\nreduce the number of iterations and improve personalization in the creation of\nfashion products. TDRI exhibits a strong potential for a wide range of\napplications in the creative and industrial domains, as it streamlines the\ncreative process and improves alignment with user preferences", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "dialogue"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17672", "pdf": "https://arxiv.org/pdf/2503.17672", "abs": "https://arxiv.org/abs/2503.17672", "authors": ["Qing Zhong", "Peng-Tao Jiang", "Wen Wang", "Guodong Ding", "Lin Wu", "Kaiqi Huang"], "title": "A Temporal Modeling Framework for Video Pre-Training on Video Instance Segmentation", "categories": ["cs.CV"], "comment": "7 pages, 5figures, 6 tables, Accepted to ICME 2025", "summary": "Contemporary Video Instance Segmentation (VIS) methods typically adhere to a\npre-train then fine-tune regime, where a segmentation model trained on images\nis fine-tuned on videos. However, the lack of temporal knowledge in the\npre-trained model introduces a domain gap which may adversely affect the VIS\nperformance. To effectively bridge this gap, we present a novel video\npre-training approach to enhance VIS models, especially for videos with\nintricate instance relationships. Our crucial innovation focuses on reducing\ndisparities between the pre-training and fine-tuning stages. Specifically, we\nfirst introduce consistent pseudo-video augmentations to create diverse\npseudo-video samples for pre-training while maintaining the instance\nconsistency across frames. Then, we incorporate a multi-scale temporal module\nto enhance the model's ability to model temporal relations through self- and\ncross-attention at short- and long-term temporal spans. Our approach does not\nset constraints on model architecture and can integrate seamlessly with various\nVIS methods. Experiment results on commonly adopted VIS benchmarks show that\nour method consistently outperforms state-of-the-art methods. Our approach\nachieves a notable 4.0% increase in average precision on the challenging OVIS\ndataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17673", "pdf": "https://arxiv.org/pdf/2503.17673", "abs": "https://arxiv.org/abs/2503.17673", "authors": ["Jinyuan Liu", "Bowei Zhang", "Qingyun Mei", "Xingyuan Li", "Yang Zou", "Zhiying Jiang", "Long Ma", "Risheng Liu", "Xin Fan"], "title": "DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion", "categories": ["cs.CV", "68T45", "I.4.3"], "comment": "Accepted by CVPR 2025", "summary": "Infrared and visible image fusion integrates information from distinct\nspectral bands to enhance image quality by leveraging the strengths and\nmitigating the limitations of each modality. Existing approaches typically\ntreat image fusion and subsequent high-level tasks as separate processes,\nresulting in fused images that offer only marginal gains in task performance\nand fail to provide constructive feedback for optimizing the fusion process. To\novercome these limitations, we propose a Discriminative Cross-Dimension\nEvolutionary Learning Framework, termed DCEvo, which simultaneously enhances\nvisual quality and perception accuracy. Leveraging the robust search\ncapabilities of Evolutionary Learning, our approach formulates the optimization\nof dual tasks as a multi-objective problem by employing an Evolutionary\nAlgorithm (EA) to dynamically balance loss function parameters. Inspired by\nvisual neuroscience, we integrate a Discriminative Enhancer (DE) within both\nthe encoder and decoder, enabling the effective learning of complementary\nfeatures from different modalities. Additionally, our Cross-Dimensional\nEmbedding (CDE) block facilitates mutual enhancement between high-dimensional\ntask features and low-dimensional fusion features, ensuring a cohesive and\nefficient feature integration process. Experimental results on three benchmarks\ndemonstrate that our method significantly outperforms state-of-the-art\napproaches, achieving an average improvement of 9.32% in visual quality while\nalso enhancing subsequent high-level tasks. The code is available at\nhttps://github.com/Beate-Suy-Zhang/DCEvo.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18069", "pdf": "https://arxiv.org/pdf/2503.18069", "abs": "https://arxiv.org/abs/2503.18069", "authors": ["Si Shen", "Fei Huang", "Zhixiao Zhao", "Chang Liu", "Tiansheng Zheng", "Danhao Zhu"], "title": "Long Is More Important Than Difficult for Training Reasoning Models", "categories": ["cs.CL"], "comment": "15 pages,6 figures", "summary": "Difficult problems, which often result in long reasoning traces, are widely\nrecognized as key factors for enhancing the performance of reasoning models.\nHowever, such high-challenge problems are scarce, limiting the size of\navailable datasets. In this paper, we propose a simple method to decouple the\nreliance on problem difficulty. First, we empirically demonstrate that\nreasoning length, rather than problem difficulty, primarily influences the\nperformance of trained models. Second, we identify a scaling law on reasoning\nlength, showing that model performance increases in a log-linear fashion as the\nreasoning data length grows. Finally, we introduce a straightforward technique\nto generate reasoning data of arbitrary length, and show that synthesized data\nis effective for training reasoning models. After fine-tuning the\nQwen2.5-32B-Instruct language model on our Long1K dataset, we present our\nmodel, Long1K-32B, which achieves remarkable performance with only 1,000\ntraining samples, achieving 95.6\\% accuracy on MATH, and 71.1\\% on GPQA\noutperforming DeepSeek-R1-Distill-Qwen-32B. The model, code, and dataset are\nall open-sourced, available at https://huggingface.co/ZTss/LONG1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17699", "pdf": "https://arxiv.org/pdf/2503.17699", "abs": "https://arxiv.org/abs/2503.17699", "authors": ["Haolin Qin", "Tingfa Xu", "Tianhao Li", "Zhenxiang Chen", "Tao Feng", "Jianan Li"], "title": "MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "UAV tracking faces significant challenges in real-world scenarios, such as\nsmall-size targets and occlusions, which limit the performance of RGB-based\ntrackers. Multispectral images (MSI), which capture additional spectral\ninformation, offer a promising solution to these challenges. However, progress\nin this field has been hindered by the lack of relevant datasets. To address\nthis gap, we introduce the first large-scale Multispectral UAV Single Object\nTracking dataset (MUST), which includes 250 video sequences spanning diverse\nenvironments and challenges, providing a comprehensive data foundation for\nmultispectral UAV tracking. We also propose a novel tracking framework,\nUNTrack, which encodes unified spectral, spatial, and temporal features from\nspectrum prompts, initial templates, and sequential searches. UNTrack employs\nan asymmetric transformer with a spectral background eliminate mechanism for\noptimal relationship modeling and an encoder that continuously updates the\nspectrum prompt to refine tracking, improving both accuracy and efficiency.\nExtensive experiments show that our proposed UNTrack outperforms\nstate-of-the-art UAV trackers. We believe our dataset and framework will drive\nfuture research in this area. The dataset is available on\nhttps://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18071", "pdf": "https://arxiv.org/pdf/2503.18071", "abs": "https://arxiv.org/abs/2503.18071", "authors": ["Zhiyu Lin", "Yifei Gao", "Xian Zhao", "Yunfan Yang", "Jitao Sang"], "title": "Mind with Eyes: from Language Reasoning to Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Language models have recently advanced into the realm of reasoning, yet it is\nthrough multimodal reasoning that we can fully unlock the potential to achieve\nmore comprehensive, human-like cognitive capabilities. This survey provides a\nsystematic overview of the recent multimodal reasoning approaches, categorizing\nthem into two levels: language-centric multimodal reasoning and collaborative\nmultimodal reasoning. The former encompasses one-pass visual perception and\nactive visual perception, where vision primarily serves a supporting role in\nlanguage reasoning. The latter involves action generation and state update\nwithin reasoning process, enabling a more dynamic interaction between\nmodalities. Furthermore, we analyze the technical evolution of these methods,\ndiscuss their inherent challenges, and introduce key benchmark tasks and\nevaluation metrics for assessing multimodal reasoning performance. Finally, we\nprovide insights into future research directions from the following two\nperspectives: (i) from visual-language reasoning to omnimodal reasoning and\n(ii) from multimodal reasoning to multimodal agents. This survey aims to\nprovide a structured overview that will inspire further advancements in\nmultimodal reasoning research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18072", "pdf": "https://arxiv.org/pdf/2503.18072", "abs": "https://arxiv.org/abs/2503.18072", "authors": ["Germán Capdehourat", "Isabel Amigo", "Brian Lorenzo", "Joaquín Trigo"], "title": "On the effectiveness of LLMs for automatic grading of open-ended questions in Spanish", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Grading is a time-consuming and laborious task that educators must face. It\nis an important task since it provides feedback signals to learners, and it has\nbeen demonstrated that timely feedback improves the learning process. In recent\nyears, the irruption of LLMs has shed light on the effectiveness of automatic\ngrading. In this paper, we explore the performance of different LLMs and\nprompting techniques in automatically grading short-text answers to open-ended\nquestions. Unlike most of the literature, our study focuses on a use case where\nthe questions, answers, and prompts are all in Spanish. Experimental results\ncomparing automatic scores to those of human-expert evaluators show good\noutcomes in terms of accuracy, precision and consistency for advanced LLMs,\nboth open and proprietary. Results are notably sensitive to prompt styles,\nsuggesting biases toward certain words or content in the prompt. However, the\nbest combinations of models and prompt strategies, consistently surpasses an\naccuracy of 95% in a three-level grading task, which even rises up to more than\n98% when the it is simplified to a binary right or wrong rating problem, which\ndemonstrates the potential that LLMs have to implement this type of automation\nin education applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18076", "pdf": "https://arxiv.org/pdf/2503.18076", "abs": "https://arxiv.org/abs/2503.18076", "authors": ["Somnath Roy", "Padharthi Sreekar", "Srivatsa Narasimha", "Anubhav Anand"], "title": "A Multi-Model Adaptation of Speculative Decoding for Classification", "categories": ["cs.CL"], "comment": null, "summary": "The current study introduces a novel adaptation of speculative decoding,\nrepurposed from generation to classification tasks. We propose a multi-model\nframework employing up to three lightweight worker models and a single, more\nrobust judge model analogous to draft models and target model, respectively, in\nspeculative decoding. The worker models, tasked with the bulk of the\ncomputation, independently predict discrete class labels for a given input.\nWhen majority worker models agree on a label, it is accepted as the final\nlabel, optimizing efficiency by bypassing the computationally expensive judge\nmodel. In cases of disagreement, the judge model intervenes to resolve the\nlabel. This approach minimizes redundant computation, leverages the redundancy\nof multiple workers for confidence, and confines the judge model's role to\nchallenging cases, offering a practical balance of efficiency and accuracy. Our\nanalysis suggests that smaller out of the box instruction/chat finetuned worker\nmodels with 3 billion parameters (hereafter, 3B) demonstrate a level of\nalignment with judge models comparable to that of larger finetuned worker\nmodels with 7 billion parameters (hereafter, 7B) across both simple and higher\norder reasoning tasks. The top performing 3B worker model pair achieve an\nagreement rate of approximately 80-83% for sentiment and around 50-80% for\nsimilar ticket when compared to judge models. Additionally, 3B worker models\nprovide a speedup ranging from 2.8x to 9x relative to the judge models, while\n7B worker model combinations achieve a speedup ranging from 1.28x to 0.28x", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18117", "pdf": "https://arxiv.org/pdf/2503.18117", "abs": "https://arxiv.org/abs/2503.18117", "authors": ["Muhidin A. Mohamed", "Shuab D. Ahmed", "Yahye A. Isse", "Hanad M. Mohamed", "Fuad M. Hassan", "Houssein A. Assowe"], "title": "Detection of Somali-written Fake News and Toxic Messages on the Social Media Using Transformer-based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18129", "pdf": "https://arxiv.org/pdf/2503.18129", "abs": "https://arxiv.org/abs/2503.18129", "authors": ["Varvara Krechetova", "Denis Kochedykov"], "title": "GeoBenchX: Benchmarking LLMs for Multistep Geospatial Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "Github with code and benchmark set:\n  https://github.com/Solirinai/GeoBenchX", "summary": "In this paper, we establish a benchmark for evaluating large language models\n(LLMs) on multi-step geospatial tasks relevant to commercial GIS practitioners.\nWe assess seven leading commercial LLMs (Sonnet 3.5 and 3.7, Haiku 3.5, Gemini\n2.0, GPT-4o, GPT-4o mini, and o3-mini) using a simple tool-calling agent\nequipped with 23 geospatial functions. Our benchmark comprises tasks across\nfour categories of increasing complexity, with both solvable and intentionally\nunsolvable tasks to test hallucination rejection. We develop an LLM-as-Judge\nevaluation framework to compare agent solutions against reference\nimplementations. Results show Sonnet 3.5 and GPT-4o achieve the best overall\nperformance, with Claude models excelling on solvable tasks while OpenAI models\nbetter identify unsolvable scenarios. We observe significant differences in\ntoken usage, with Anthropic models consuming substantially more tokens than\ncompetitors. Common errors include misunderstanding geometrical relationships,\nrelying on outdated knowledge, and inefficient data manipulation. The resulting\nbenchmark set, evaluation framework, and data generation pipeline are released\nas open-source resources, providing one more standardized method for ongoing\nevaluation of LLMs for GeoAI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18132", "pdf": "https://arxiv.org/pdf/2503.18132", "abs": "https://arxiv.org/abs/2503.18132", "authors": ["Yibo Yan", "Shen Wang", "Jiahao Huo", "Philip S. Yu", "Xuming Hu", "Qingsong Wen"], "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection", "categories": ["cs.CL"], "comment": "Work In Progress", "summary": "Mathematical error detection in educational settings presents a significant\nchallenge for Multimodal Large Language Models (MLLMs), requiring a\nsophisticated understanding of both visual and textual mathematical content\nalong with complex reasoning capabilities. Though effective in mathematical\nproblem-solving, MLLMs often struggle with the nuanced task of identifying and\ncategorizing student errors in multimodal mathematical contexts. Therefore, we\nintroduce MathAgent, a novel Mixture-of-Math-Agent framework designed\nspecifically to address these challenges. Our approach decomposes error\ndetection into three phases, each handled by a specialized agent: an image-text\nconsistency validator, a visual semantic interpreter, and an integrative error\nanalyzer. This architecture enables more accurate processing of mathematical\ncontent by explicitly modeling relationships between multimodal problems and\nstudent solution steps. We evaluate MathAgent on real-world educational data,\ndemonstrating approximately 5% higher accuracy in error step identification and\n3% improvement in error categorization compared to baseline models. Besides,\nMathAgent has been successfully deployed in an educational platform that has\nserved over one million K-12 students, achieving nearly 90% student\nsatisfaction while generating significant cost savings by reducing manual error\ndetection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18242", "pdf": "https://arxiv.org/pdf/2503.18242", "abs": "https://arxiv.org/abs/2503.18242", "authors": ["Aneesh Vathul", "Daniel Lee", "Sheryl Chen", "Arthi Tasmia"], "title": "ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities on a\nbroad array of NLP tasks, but their tendency to produce\nhallucinations$\\unicode{x2013}$plausible-sounding but factually incorrect\ncontent$\\unicode{x2013}$poses severe challenges in high-stakes domains.\nExisting hallucination detection methods either bear the computational cost of\nmultiple inference passes or sacrifice accuracy for efficiency with single-pass\napproaches, neither of which is ideal in resource-constrained environments such\nas edge devices. We propose the Shannon Entropy Distribution Hallucination\nDetector (ShED-HD), a novel hallucination detection framework that bridges this\ngap by classifying sequence-level entropy patterns using a lightweight BiLSTM\narchitecture with single-headed attention. In contrast to prior approaches,\nShED-HD efficiently detects distinctive uncertainty patterns across entire\noutput sequences, preserving contextual awareness. Through in-depth evaluation\non three datasets (BioASQ, TriviaQA, and Jeopardy Questions), we show that\nShED-HD significantly outperforms other computationally efficient approaches in\nthe out-of-distribution setting, while achieving comparable performance in the\nin-distribution setting. ShED-HD facilitates hallucination detection that is\nlow-cost, accurate, and generalizable, improving the credibility of content\ngenerated by LLMs in resource-constrained environments where trustworthy AI\nfunctionality is crucial.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18260", "pdf": "https://arxiv.org/pdf/2503.18260", "abs": "https://arxiv.org/abs/2503.18260", "authors": ["Mahak Shah", "Akaash Vishal Hazarika", "Meetu Malhotra", "Sachin C. Patil", "Joshit Mohanty"], "title": "Bridging Emotions and Architecture: Sentiment Analysis in Modern Distributed Systems", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "IEEE 3rd International Conference on Advancements in Smart, Secure\n  and Intelligent Computing (ASSIC)", "summary": "Sentiment analysis is a field within NLP that has gained importance because\nit is applied in various areas such as; social media surveillance, customer\nfeedback evaluation and market research. At the same time, distributed systems\nallow for effective processing of large amounts of data. Therefore, this paper\nexamines how sentiment analysis converges with distributed systems by\nconcentrating on different approaches, challenges and future investigations.\nFurthermore, we do an extensive experiment where we train sentiment analysis\nmodels using both single node configuration and distributed architecture to\nbring out the benefits and shortcomings of each method in terms of performance\nand accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18293", "pdf": "https://arxiv.org/pdf/2503.18293", "abs": "https://arxiv.org/abs/2503.18293", "authors": ["Jiayi Yao", "Haibo Sun", "Nianwen Xue"], "title": "Fact-checking AI-generated news reports: Can LLMs catch their own lies?", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we evaluate the ability of Large Language Models (LLMs) to\nassess the veracity of claims in ''news reports'' generated by themselves or\nother LLMs. Our goal is to determine whether LLMs can effectively fact-check\ntheir own content, using methods similar to those used to verify claims made by\nhumans. Our findings indicate that LLMs are more effective at assessing claims\nin national or international news stories than in local news stories, better at\nevaluating static information than dynamic information, and better at verifying\ntrue claims compared to false ones. We hypothesize that this disparity arises\nbecause the former types of claims are better represented in the training data.\nAdditionally, we find that incorporating retrieved results from a search engine\nin a Retrieval-Augmented Generation (RAG) setting significantly reduces the\nnumber of claims an LLM cannot assess. However, this approach also increases\nthe occurrence of incorrect assessments, partly due to irrelevant or\nlow-quality search results. This diagnostic study highlights the need for\nfuture research on fact-checking machine-generated reports to prioritize\nimproving the precision and relevance of retrieved information to better\nsupport fact-checking efforts. Furthermore, claims about dynamic events and\nlocal news may require human-in-the-loop fact-checking systems to ensure\naccuracy and reliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18296", "pdf": "https://arxiv.org/pdf/2503.18296", "abs": "https://arxiv.org/abs/2503.18296", "authors": ["Mengya Xu", "Zhongzhen Huang", "Jie Zhang", "Xiaofan Zhang", "Qi Dou"], "title": "Surgical Action Planning with Large Language Models", "categories": ["cs.CL"], "comment": "10 pages,4 figures", "summary": "In robot-assisted minimally invasive surgery, we introduce the Surgical\nAction Planning (SAP) task, which generates future action plans from visual\ninputs to address the absence of intraoperative predictive planning in current\nintelligent applications. SAP shows great potential for enhancing\nintraoperative guidance and automating procedures. However, it faces challenges\nsuch as understanding instrument-action relationships and tracking surgical\nprogress. Large Language Models (LLMs) show promise in understanding surgical\nvideo content but remain underexplored for predictive decision-making in SAP,\nas they focus mainly on retrospective analysis. Challenges like data privacy,\ncomputational demands, and modality-specific constraints further highlight\nsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, a\nLarge Language Models-based Surgical Action Planning framework that predicts\nfuture actions and generates text responses by interpreting natural language\nprompts of surgical goals. The text responses potentially support surgical\neducation, intraoperative decision-making, procedure documentation, and skill\nanalysis. LLM-SAP integrates two novel modules: the Near-History Focus Memory\nModule (NHF-MM) for modeling historical states and the prompts factory for\naction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP dataset\nusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness in\nnext-action prediction. Pre-trained LLMs are tested zero-shot, and supervised\nfine-tuning (SFT) with LoRA is implemented to address data privacy concerns.\nOur experiments show that Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3%\nhigher accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18432", "pdf": "https://arxiv.org/pdf/2503.18432", "abs": "https://arxiv.org/abs/2503.18432", "authors": ["Junsong Li", "Jie Zhou", "Yutao Yang", "Bihao Zhan", "Qianjun Pan", "Yuyang Ding", "Qin Chen", "Jiang Bo", "Xin Lin", "Liang He"], "title": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17899", "pdf": "https://arxiv.org/pdf/2503.17899", "abs": "https://arxiv.org/abs/2503.17899", "authors": ["Dongheng Lin", "Han Hu", "Jianbo Jiao"], "title": "What Time Tells Us? An Explorative Study of Time Awareness Learned from Static Images", "categories": ["cs.CV"], "comment": null, "summary": "Time becomes visible through illumination changes in what we see. Inspired by\nthis, in this paper we explore the potential to learn time awareness from\nstatic images, trying to answer: what time tells us? To this end, we first\nintroduce a Time-Oriented Collection (TOC) dataset, which contains 130,906\nimages with reliable timestamps. Leveraging this dataset, we propose a\nTime-Image Contrastive Learning (TICL) approach to jointly model timestamps and\nrelated visual representations through cross-modal contrastive learning. We\nfound that the proposed TICL, 1) not only achieves state-of-the-art performance\non the timestamp estimation task, over various benchmark metrics, 2) but also,\ninterestingly, though only seeing static images, the time-aware embeddings\nlearned from TICL show strong capability in several time-aware downstream tasks\nsuch as time-based image retrieval, video scene classification, and time-aware\nimage editing. Our findings suggest that time-related visual cues can be\nlearned from static images and are beneficial for various vision tasks, laying\na foundation for future research on understanding time-related visual context.\nProject page:https://rathgrith.github.io/timetells/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491", "abs": "https://arxiv.org/abs/2503.18491", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "categories": ["cs.CL"], "comment": "8 Pages, 5 figures", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18502", "pdf": "https://arxiv.org/pdf/2503.18502", "abs": "https://arxiv.org/abs/2503.18502", "authors": ["Andrés García-Silva", "José Manuel Gómez-Pérez"], "title": "Autoregressive Language Models for Knowledge Base Population: A case study in the space mission domain", "categories": ["cs.CL"], "comment": "Pre-print version", "summary": "Knowledge base population KBP plays a crucial role in populating and\nmaintaining knowledge bases up-to-date in organizations by leveraging domain\ncorpora. Motivated by the increasingly large context windows supported by large\nlanguage models, we propose to fine-tune an autoregressive language model for\nend-toend KPB. Our case study involves the population of a space mission\nknowledge graph. To fine-tune the model we generate a dataset for end-to-end\nKBP tapping into existing domain resources. Our case study shows that\nfine-tuned language models of limited size can achieve competitive and even\nhigher accuracy than larger models in the KBP task. Smaller models specialized\nfor KBP offer affordable deployment and lower-cost inference. Moreover, KBP\nspecialist models do not require the ontology to be included in the prompt,\nallowing for more space in the context for additional input text or output\nserialization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18526", "pdf": "https://arxiv.org/pdf/2503.18526", "abs": "https://arxiv.org/abs/2503.18526", "authors": ["Raúl Ortega", "José Manuel Gómez-Pérez"], "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "Pre-print version", "summary": "Validating key claims in scientific literature, particularly in biomedical\nresearch, is essential for ensuring accuracy and advancing knowledge. This\nprocess is critical in sectors like the pharmaceutical industry, where rapid\nscientific progress requires automation and deep domain expertise. However,\ncurrent solutions have significant limitations. They lack end-to-end pipelines\nencompassing all claim extraction, evidence retrieval, and verification steps;\nrely on complex NLP and information retrieval pipelines prone to multiple\nfailure points; and often fail to provide clear, user-friendly justifications\nfor claim verification outcomes. To address these challenges, we introduce\nSciClaims, an advanced system powered by state-of-the-art large language models\n(LLMs) that seamlessly integrates the entire scientific claim analysis process.\nSciClaims outperforms previous approaches in both claim extraction and\nverification without requiring additional fine-tuning, setting a new benchmark\nfor automated scientific claim analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17966", "pdf": "https://arxiv.org/pdf/2503.17966", "abs": "https://arxiv.org/abs/2503.17966", "authors": ["Zeng-Hui Zhu", "Wei Lu", "Si-Bao Chen", "Chris H. Q. Ding", "Jin Tang", "Bin Luo"], "title": "Real-World Remote Sensing Image Dehazing: Benchmark and Baseline", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 9 figures, real-world remote sensing image dehazing dataset", "summary": "Remote Sensing Image Dehazing (RSID) poses significant challenges in\nreal-world scenarios due to the complex atmospheric conditions and severe color\ndistortions that degrade image quality. The scarcity of real-world remote\nsensing hazy image pairs has compelled existing methods to rely primarily on\nsynthetic datasets. However, these methods struggle with real-world\napplications due to the inherent domain gap between synthetic and real data. To\naddress this, we introduce Real-World Remote Sensing Hazy Image Dataset\n(RRSHID), the first large-scale dataset featuring real-world hazy and dehazed\nimage pairs across diverse atmospheric conditions. Based on this, we propose\nMCAF-Net, a novel framework tailored for real-world RSID. Its effectiveness\narises from three innovative components: Multi-branch Feature Integration Block\nAggregator (MFIBA), which enables robust feature extraction through cascaded\nintegration blocks and parallel multi-branch processing; Color-Calibrated\nSelf-Supervised Attention Module (CSAM), which mitigates complex color\ndistortions via self-supervised learning and attention-guided refinement; and\nMulti-Scale Feature Adaptive Fusion Module (MFAFM), which integrates features\neffectively while preserving local details and global context. Extensive\nexperiments validate that MCAF-Net demonstrates state-of-the-art performance in\nreal-world RSID, while maintaining competitive performance on synthetic\ndatasets. The introduction of RRSHID and MCAF-Net sets new benchmarks for\nreal-world RSID research, advancing practical solutions for this complex task.\nThe code and dataset are publicly available at\n\\url{https://github.com/lwCVer/RRSHID}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18646", "pdf": "https://arxiv.org/pdf/2503.18646", "abs": "https://arxiv.org/abs/2503.18646", "authors": ["Zhen-Song Chen", "Hong-Wei Ding", "Xian-Jia Wang", "Witold Pedrycz"], "title": "ZeroLM: Data-Free Transformer Architecture Search for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Neural architecture search (NAS) provides a systematic framework for\nautomating the design of neural network architectures, yet its widespread\nadoption is hindered by prohibitive computational requirements. Existing\nzero-cost proxy methods, while reducing search overhead, demonstrate inadequate\nperformance in architecture ranking tasks, particularly for Transformer-based\nmodels where they often underperform simple parameter counting metrics. Current\nautomated proxy discovery approaches suffer from extended search times,\nsusceptibility to data overfitting, and structural complexity. This paper\nintroduces a novel zero-cost proxy methodology that quantifies model capacity\nthrough efficient weight statistics computation while decomposing Transformer\narchitectures into functionally distinct sub-modules, thereby optimizing the\nbalance of their contributions to overall performance. Our comprehensive\nevaluation demonstrates the superiority of this approach, achieving a\nSpearman's rho of 0.76 and Kendall's tau of 0.53 on the FlexiBERT benchmark.\nThe proposed method exhibits exceptional computational efficiency while\nmaintaining robust performance across diverse NAS benchmark tasks, offering a\npractical solution for large-scale architecture search.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17978", "pdf": "https://arxiv.org/pdf/2503.17978", "abs": "https://arxiv.org/abs/2503.17978", "authors": ["Dominique Nshimyimana", "Vitor Fortes Rey", "Sungho Suh", "Bo Zhou", "Paul Lukowicz"], "title": "PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human activity recognition (HAR) with deep learning models relies on large\namounts of labeled data, often challenging to obtain due to associated cost,\ntime, and labor. Self-supervised learning (SSL) has emerged as an effective\napproach to leverage unlabeled data through pretext tasks, such as masked\nreconstruction and multitask learning with signal processing-based data\naugmentations, to pre-train encoder models. However, such methods are often\nderived from computer vision approaches that disregard physical mechanisms and\nconstraints that govern wearable sensor data and the phenomena they reflect. In\nthis paper, we propose a physics-informed multi-task pre-training (PIM)\nframework for IMU-based HAR. PIM generates pre-text tasks based on the\nunderstanding of basic physical aspects of human motion: including movement\nspeed, angles of movement, and symmetry between sensor placements. Given a\nsensor signal, we calculate corresponding features using physics-based\nequations and use them as pretext tasks for SSL. This enables the model to\ncapture fundamental physical characteristics of human activities, which is\nespecially relevant for multi-sensor systems. Experimental evaluations on four\nHAR benchmark datasets demonstrate that the proposed method outperforms\nexisting state-of-the-art methods, including data augmentation and masked\nreconstruction, in terms of accuracy and F1 score. We have observed gains of\nalmost 10\\% in macro f1 score and accuracy with only 2 to 8 labeled examples\nper class and up to 3% when there is no reduction in the amount of training\ndata.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17983", "pdf": "https://arxiv.org/pdf/2503.17983", "abs": "https://arxiv.org/abs/2503.17983", "authors": ["Baizhi Wang", "Rui Yan", "Wenxin Ma", "Xu Zhang", "Yuhao Wang", "Xiaolong Li", "Yunjie Gu", "Zihang Jiang", "S. Kevin Zhou"], "title": "Histomorphology-driven multi-instance learning for breast cancer WSI classification", "categories": ["cs.CV"], "comment": "10 pages,5 figures", "summary": "Histomorphology is crucial in breast cancer diagnosis. However, existing\nwhole slide image (WSI) classification methods struggle to effectively\nincorporate histomorphology information, limiting their ability to capture key\nand fine-grained pathological features. To address this limitation, we propose\na novel framework that explicitly incorporates histomorphology (tumor\ncellularity, cellular morphology, and tissue architecture) into WSI\nclassification. Specifically, our approach consists of three key components:\n(1) estimating the importance of tumor-related histomorphology information at\nthe patch level based on medical prior knowledge; (2) generating representative\ncluster-level features through histomorphology-driven cluster pooling; and (3)\nenabling WSI-level classification through histomorphology-driven multi-instance\naggregation. With the incorporation of histomorphological information, our\nframework strengthens the model's ability to capture key and fine-grained\npathological patterns, thereby enhancing WSI classification performance.\nExperimental results demonstrate its effectiveness, achieving high diagnostic\naccuracy for molecular subtyping and cancer subtyping. The code will be made\navailable at https://github.com/Badgewho/HMDMIL.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18730", "pdf": "https://arxiv.org/pdf/2503.18730", "abs": "https://arxiv.org/abs/2503.18730", "authors": ["Hongkuan Zhou", "Stefan Schmid", "Yicong Li", "Lavdim Halilaj", "Xiangtong Yao", "Wei cao"], "title": "Predicting the Road Ahead: A Knowledge Graph based Foundation Model for Scene Understanding in Autonomous Driving", "categories": ["cs.CL"], "comment": null, "summary": "The autonomous driving field has seen remarkable advancements in various\ntopics, such as object recognition, trajectory prediction, and motion planning.\nHowever, current approaches face limitations in effectively comprehending the\ncomplex evolutions of driving scenes over time. This paper proposes FM4SU, a\nnovel methodology for training a symbolic foundation model (FM) for scene\nunderstanding in autonomous driving. It leverages knowledge graphs (KGs) to\ncapture sensory observation along with domain knowledge such as road topology,\ntraffic rules, or complex interactions between traffic participants. A bird's\neye view (BEV) symbolic representation is extracted from the KG for each\ndriving scene, including the spatio-temporal information among the objects\nacross the scenes. The BEV representation is serialized into a sequence of\ntokens and given to pre-trained language models (PLMs) for learning an inherent\nunderstanding of the co-occurrence among driving scene elements and generating\npredictions on the next scenes. We conducted a number of experiments using the\nnuScenes dataset and KG in various scenarios. The results demonstrate that\nfine-tuned models achieve significantly higher accuracy in all tasks. The\nfine-tuned T5 model achieved a next scene prediction accuracy of 86.7%. This\npaper concludes that FM4SU offers a promising foundation for developing more\ncomprehensive models for scene understanding in autonomous driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17984", "pdf": "https://arxiv.org/pdf/2503.17984", "abs": "https://arxiv.org/abs/2503.17984", "authors": ["Maochen Yang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Semi-supervised crowd counting is crucial for addressing the high annotation\ncosts of densely populated scenes. Although several methods based on\npseudo-labeling have been proposed, it remains challenging to effectively and\naccurately utilize unlabeled data. In this paper, we propose a novel framework\ncalled Taste More Taste Better (TMTB), which emphasizes both data and model\naspects. Firstly, we explore a data augmentation technique well-suited for the\ncrowd counting task. By inpainting the background regions, this technique can\neffectively enhance data diversity while preserving the fidelity of the entire\nscenes. Secondly, we introduce the Visual State Space Model as backbone to\ncapture the global context information from crowd scenes, which is crucial for\nextremely crowded, low-light, and adverse weather scenarios. In addition to the\ntraditional regression head for exact prediction, we employ an Anti-Noise\nclassification head to provide less exact but more accurate supervision, since\nthe regression head is sensitive to noise in manual annotations. We conduct\nextensive experiments on four benchmark datasets and show that our method\noutperforms state-of-the-art methods by a large margin. Code is publicly\navailable on https://github.com/syhien/taste_more_taste_better.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18751", "pdf": "https://arxiv.org/pdf/2503.18751", "abs": "https://arxiv.org/abs/2503.18751", "authors": ["Wesley Scivetti", "Nathan Schneider"], "title": "Construction Identification and Disambiguation Using BERT: A Case Study of NPN", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, ACL long-paper format (preprint)", "summary": "Construction Grammar hypothesizes that knowledge of a language consists\nchiefly of knowledge of form-meaning pairs (''constructions'') that include\nvocabulary, general grammar rules, and even idiosyncratic patterns. Recent work\nhas shown that transformer language models represent at least some\nconstructional patterns, including ones where the construction is rare overall.\nIn this work, we probe BERT's representation of the form and meaning of a minor\nconstruction of English, the NPN (noun-preposition-noun) construction --\nexhibited in such expressions as face to face and day to day -- which is known\nto be polysemous. We construct a benchmark dataset of semantically annotated\ncorpus instances (including distractors that superficially resemble the\nconstruction). With this dataset, we train and evaluate probing classifiers.\nThey achieve decent discrimination of the construction from distractors, as\nwell as sense disambiguation among true instances of the construction,\nrevealing that BERT embeddings carry indications of the construction's\nsemantics. Moreover, artificially permuting the word order of true construction\ninstances causes them to be rejected, indicating sensitivity to matters of\nform. We conclude that BERT does latently encode at least some knowledge of the\nNPN construction going beyond a surface syntactic pattern and lexical cues.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18007", "pdf": "https://arxiv.org/pdf/2503.18007", "abs": "https://arxiv.org/abs/2503.18007", "authors": ["Hongyu Yan", "Zijun Li", "Kunming Luo", "Li Lu", "Ping Tan"], "title": "SymmCompletion: High-Fidelity and High-Consistency Point Cloud Completion with Symmetry Guidance", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025 (Oral presentation), Code:\n  https://github.com/HongyuYann/SymmCompletion", "summary": "Point cloud completion aims to recover a complete point shape from a partial\npoint cloud. Although existing methods can form satisfactory point clouds in\nglobal completeness, they often lose the original geometry details and face the\nproblem of geometric inconsistency between existing point clouds and\nreconstructed missing parts. To tackle this problem, we introduce\nSymmCompletion, a highly effective completion method based on symmetry\nguidance. Our method comprises two primary components: a Local Symmetry\nTransformation Network (LSTNet) and a Symmetry-Guidance Transformer (SGFormer).\nFirst, LSTNet efficiently estimates point-wise local symmetry transformation to\ntransform key geometries of partial inputs into missing regions, thereby\ngenerating geometry-align partial-missing pairs and initial point clouds.\nSecond, SGFormer leverages the geometric features of partial-missing pairs as\nthe explicit symmetric guidance that can constrain the refinement process for\ninitial point clouds. As a result, SGFormer can exploit provided priors to form\nhigh-fidelity and geometry-consistency final point clouds. Qualitative and\nquantitative evaluations on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art completion networks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18013", "pdf": "https://arxiv.org/pdf/2503.18013", "abs": "https://arxiv.org/abs/2503.18013", "authors": ["Yufei Zhan", "Yousong Zhu", "Shurong Zheng", "Hongyin Zhao", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Project in development. Github:\n  https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1", "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "preference", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional", "criteria"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18893", "pdf": "https://arxiv.org/pdf/2503.18893", "abs": "https://arxiv.org/abs/2503.18893", "authors": ["Chi-Chih Chang", "Chien-Yu Lin", "Yash Akhauri", "Wei-Cheng Lin", "Kai-Chiang Wu", "Luis Ceze", "Mohamed S. Abdelfattah"], "title": "xKV: Cross-Layer SVD for KV-Cache Compression", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.16586", "pdf": "https://arxiv.org/pdf/2503.16586", "abs": "https://arxiv.org/abs/2503.16586", "authors": ["Yash Vekaria", "Aurelio Loris Canino", "Jonathan Levitsky", "Alex Ciechonski", "Patricia Callejo", "Anna Maria Mandalari", "Zubair Shafiq"], "title": "Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CR", "cs.CY", "I.2; I.2.1; I.2.7; H.3.4; K.4; K.4.1; H.1; H.1.2; H.5.2; H.4.3"], "comment": null, "summary": "Generative AI (GenAI) browser assistants integrate powerful capabilities of\nGenAI in web browsers to provide rich experiences such as question answering,\ncontent summarization, and agentic navigation. These assistants, available\ntoday as browser extensions, can not only track detailed browsing activity such\nas search and click data, but can also autonomously perform tasks such as\nfilling forms, raising significant privacy concerns. It is crucial to\nunderstand the design and operation of GenAI browser extensions, including how\nthey collect, store, process, and share user data. To this end, we study their\nability to profile users and personalize their responses based on explicit or\ninferred demographic attributes and interests of users. We perform network\ntraffic analysis and use a novel prompting framework to audit tracking,\nprofiling, and personalization by the ten most popular GenAI browser assistant\nextensions. We find that instead of relying on local in-browser models, these\nassistants largely depend on server-side APIs, which can be auto-invoked\nwithout explicit user interaction. When invoked, they collect and share webpage\ncontent, often the full HTML DOM and sometimes even the user's form inputs,\nwith their first-party servers. Some assistants also share identifiers and user\nprompts with third-party trackers such as Google Analytics. The collection and\nsharing continues even if a webpage contains sensitive information such as\nhealth or personal information such as name or SSN entered in a web form. We\nfind that several GenAI browser assistants infer demographic attributes such as\nage, gender, income, and interests and use this profile--which carries across\nbrowsing contexts--to personalize responses. In summary, our work shows that\nGenAI browser assistants can and do collect personal and sensitive information\nfor profiling and personalization with little to no safeguards.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "question answering"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18055", "pdf": "https://arxiv.org/pdf/2503.18055", "abs": "https://arxiv.org/abs/2503.18055", "authors": ["Mingde Yao", "Menglu Wang", "King-Man Tam", "Lingen Li", "Tianfan Xue", "Jinwei Gu"], "title": "PolarFree: Polarization-based Reflection-free Imaging", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https://github.com/mdyao/PolarFree.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17955", "pdf": "https://arxiv.org/pdf/2503.17955", "abs": "https://arxiv.org/abs/2503.17955", "authors": ["Stefan Pasch", "Sun-Young Ha"], "title": "Human-AI Interaction and User Satisfaction: Empirical Evidence from Online Reviews of AI Products", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Human-AI Interaction (HAI) guidelines and design principles have become\nincreasingly important in both industry and academia to guide the development\nof AI systems that align with user needs and expectations. However, large-scale\nempirical evidence on how HAI principles shape user satisfaction in practice\nremains limited. This study addresses that gap by analyzing over 100,000 user\nreviews of AI-related products from G2.com, a leading review platform for\nbusiness software and services. Based on widely adopted industry guidelines, we\nidentify seven core HAI dimensions and examine their coverage and sentiment\nwithin the reviews. We find that the sentiment on four HAI\ndimensions-adaptability, customization, error recovery, and security-is\npositively associated with overall user satisfaction. Moreover, we show that\nengagement with HAI dimensions varies by professional background: Users with\ntechnical job roles are more likely to discuss system-focused aspects, such as\nreliability, while non-technical users emphasize interaction-focused features\nlike customization and feedback. Interestingly, the relationship between HAI\nsentiment and overall satisfaction is not moderated by job role, suggesting\nthat once an HAI dimension has been identified by users, its effect on\nsatisfaction is consistent across job roles.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "dimension"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18123", "pdf": "https://arxiv.org/pdf/2503.18123", "abs": "https://arxiv.org/abs/2503.18123", "authors": ["Alexander Gielisse", "Jan van Gemert"], "title": "End-to-End Implicit Neural Representations for Classification", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. 8 pages, supplementary material included", "summary": "Implicit neural representations (INRs) such as NeRF and SIREN encode a signal\nin neural network parameters and show excellent results for signal\nreconstruction. Using INRs for downstream tasks, such as classification, is\nhowever not straightforward. Inherent symmetries in the parameters pose\nchallenges and current works primarily focus on designing architectures that\nare equivariant to these symmetries. However, INR-based classification still\nsignificantly under-performs compared to pixel-based methods like CNNs. This\nwork presents an end-to-end strategy for initializing SIRENs together with a\nlearned learning-rate scheme, to yield representations that improve\nclassification accuracy. We show that a simple, straightforward, Transformer\nmodel applied to a meta-learned SIREN, without incorporating explicit symmetry\nequivariances, outperforms the current state-of-the-art. On the CIFAR-10 SIREN\nclassification task, we improve the state-of-the-art without augmentations from\n38.8% to 59.6%, and from 63.4% to 64.7% with augmentations. We demonstrate\nscalability on the high-resolution Imagenette dataset achieving reasonable\nreconstruction quality with a classification accuracy of 60.8% and are the\nfirst to do INR classification on the full ImageNet-1K dataset where we achieve\na SIREN classification performance of 23.6%. To the best of our knowledge, no\nother SIREN classification approach has managed to set a classification\nbaseline for any high-resolution image dataset. Our code is available at\nhttps://github.com/SanderGielisse/MWT", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18050", "pdf": "https://arxiv.org/pdf/2503.18050", "abs": "https://arxiv.org/abs/2503.18050", "authors": ["Hanwool Lee"], "title": "(G)I-DLE: Generative Inference via Distribution-preserving Logit Exclusion with KL Divergence Minimization for Constrained Decoding", "categories": ["cs.CE", "cs.CL"], "comment": "preprint", "summary": "We propose (G)I-DLE, a new approach to constrained decoding that leverages KL\ndivergence minimization to preserve the intrinsic conditional probability\ndistribution of autoregressive language models while excluding undesirable\ntokens. Unlike conventional methods that naively set banned tokens' logits to\n$-\\infty$, which can distort the conversion from raw logits to posterior\nprobabilities and increase output variance, (G)I-DLE re-normalizes the allowed\ntoken probabilities to minimize such distortion. We validate our method on the\nK2-Eval dataset, specifically designed to assess Korean language fluency,\nlogical reasoning, and cultural appropriateness. Experimental results on\nQwen2.5 models (ranging from 1.5B to 14B) demonstrate that G-IDLE not only\nboosts mean evaluation scores but also substantially reduces the variance of\noutput quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18147", "pdf": "https://arxiv.org/pdf/2503.18147", "abs": "https://arxiv.org/abs/2503.18147", "authors": ["Ke Niu", "Yuwen Chen", "Haiyang Yu", "Zhuofan Chen", "Xianghui Que", "Bin Li", "Xiangyang Xue"], "title": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive Hierarchical Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18155", "pdf": "https://arxiv.org/pdf/2503.18155", "abs": "https://arxiv.org/abs/2503.18155", "authors": ["Kelly O. Marshall", "Omid Poursaeed", "Sergiu Oprea", "Amit Kumar", "Anushrut Jignasu", "Chinmay Hegde", "Yilei Li", "Rakesh Ranjan"], "title": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of Indoor 3D Scenes", "categories": ["cs.CV"], "comment": null, "summary": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18223", "pdf": "https://arxiv.org/pdf/2503.18223", "abs": "https://arxiv.org/abs/2503.18223", "authors": ["Valentin Gabeff", "Haozhe Qi", "Brendan Flaherty", "Gencer Sumbül", "Alexander Mathis", "Devis Tuia"], "title": "MammAlps: A multi-view video behavior monitoring dataset of wild mammals in the Swiss Alps", "categories": ["cs.CV", "cs.IR", "q-bio.NC", "q-bio.QM"], "comment": "CVPR 2025; Benchmark and code at:\n  https://github.com/eceo-epfl/MammAlps", "summary": "Monitoring wildlife is essential for ecology and ethology, especially in\nlight of the increasing human impact on ecosystems. Camera traps have emerged\nas habitat-centric sensors enabling the study of wildlife populations at scale\nwith minimal disturbance. However, the lack of annotated video datasets limits\nthe development of powerful video understanding models needed to process the\nvast amount of fieldwork data collected. To advance research in wild animal\nbehavior monitoring we present MammAlps, a multimodal and multi-view dataset of\nwildlife behavior monitoring from 9 camera-traps in the Swiss National Park.\nMammAlps contains over 14 hours of video with audio, 2D segmentation maps and\n8.5 hours of individual tracks densely labeled for species and behavior. Based\non 6135 single animal clips, we propose the first hierarchical and multimodal\nanimal behavior recognition benchmark using audio, video and reference scene\nsegmentation maps as inputs. Furthermore, we also propose a second\necology-oriented benchmark aiming at identifying activities, species, number of\nindividuals and meteorological conditions from 397 multi-view and long-term\necological events, including false positive triggers. We advocate that both\ntasks are complementary and contribute to bridging the gap between machine\nlearning and ecology. Code and data are available at:\nhttps://github.com/eceo-epfl/MammAlps", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18666", "pdf": "https://arxiv.org/pdf/2503.18666", "abs": "https://arxiv.org/abs/2503.18666", "authors": ["Haoyu Wang", "Christopher M. Poskitt", "Jun Sun"], "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Agents built on LLMs are increasingly deployed across diverse domains,\nautomating complex decision-making and task execution. However, their autonomy\nintroduces safety risks, including security vulnerabilities, legal violations,\nand unintended harmful actions. Existing mitigation methods, such as\nmodel-based safeguards and early enforcement strategies, fall short in\nrobustness, interpretability, and adaptability. To address these challenges, we\npropose AgentSpec, a lightweight domain-specific language for specifying and\nenforcing runtime constraints on LLM agents. With AgentSpec, users define\nstructured rules that incorporate triggers, predicates, and enforcement\nmechanisms, ensuring agents operate within predefined safety boundaries. We\nimplement AgentSpec across multiple domains, including code execution, embodied\nagents, and autonomous driving, demonstrating its adaptability and\neffectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe\nexecutions in over 90% of code agent cases, eliminates all hazardous actions in\nembodied agent tasks, and enforces 100% compliance by autonomous vehicles\n(AVs). Despite its strong safety guarantees, AgentSpec remains computationally\nlightweight, with overheads in milliseconds. By combining interpretability,\nmodularity, and efficiency, AgentSpec provides a practical and scalable\nsolution for enforcing LLM agent safety across diverse applications. We also\nautomate the generation of rules using LLMs and assess their effectiveness. Our\nevaluation shows that the rules generated by OpenAI o1 achieve a precision of\n95.56% and recall of 70.96% for embodied agents, successfully identifying\n87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8\nscenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18267", "pdf": "https://arxiv.org/pdf/2503.18267", "abs": "https://arxiv.org/abs/2503.18267", "authors": ["Minh-Tuan Tran", "Trung Le", "Xuan-May Le", "Thanh-Toan Do", "Dinh Phung"], "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Dataset distillation has become a popular method for compressing large\ndatasets into smaller, more efficient representations while preserving critical\ninformation for model training. Data features are broadly categorized into two\ntypes: instance-specific features, which capture unique, fine-grained details\nof individual examples, and class-general features, which represent shared,\nbroad patterns across a class. However, previous approaches often struggle to\nbalance these features-some focus solely on class-general patterns, neglecting\nfiner instance details, while others prioritize instance-specific features,\noverlooking the shared characteristics essential for class-level understanding.\nIn this paper, we introduce the Non-Critical Region Refinement Dataset\nDistillation (NRR-DD) method, which preserves instance-specific details and\nfine-grained regions in synthetic data while enriching non-critical regions\nwith class-general information. This approach enables models to leverage all\npixel information, capturing both feature types and enhancing overall\nperformance. Additionally, we present Distance-Based Representative (DBR)\nknowledge transfer, which eliminates the need for soft labels in training by\nrelying on the distance between synthetic data predictions and one-hot encoded\nlabels. Experimental results show that NRR-DD achieves state-of-the-art\nperformance on both small- and large-scale datasets. Furthermore, by storing\nonly two distances per instance, our method delivers comparable results across\nvarious settings. The code is available at\nhttps://github.com/tmtuan1307/NRR-DD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18773", "pdf": "https://arxiv.org/pdf/2503.18773", "abs": "https://arxiv.org/abs/2503.18773", "authors": ["Dayou Du", "Shijie Cao", "Jianyi Cheng", "Ting Cao", "Mao Yang"], "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.PF"], "comment": null, "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18339", "pdf": "https://arxiv.org/pdf/2503.18339", "abs": "https://arxiv.org/abs/2503.18339", "authors": ["Inpyo Hong", "Youngwan Jo", "Hyojeong Lee", "Sunghyun Ahn", "Sanghyun Park"], "title": "GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot quantization (ZSQ) enables neural network compression without\ntraining data, which is crucial in restricted data access environments.\nHowever, existing ZSQ methods suffer from significant activation loss in\nlow-bit environments owing to their coarse-grained scaling strategy. To address\nthis issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel\nawareness to minimize the quantization error. Unlike conventional layer- or\nchannel-wise quantization, GranQ dynamically adjusts quantization granularity\nby considering both layer- and channel-level activation distributions. This\nenables fine-grained quantization while minimizing activation distortion.\nAdditionally, we introduce vectorized activation quantization, which enables\nefficient parallel computation and reduces computational overhead while\npreserving accuracy. GranQ achieves superior performance compared with those of\nstate-of-the-art ZSQ methods that employ quantization-aware training. With\nthese findings, we anticipate that GranQ will inspire novel research directions\nbeyond conventional ZSQ approaches focused on data generation and model\ntraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18349", "pdf": "https://arxiv.org/pdf/2503.18349", "abs": "https://arxiv.org/abs/2503.18349", "authors": ["Zekai Deng", "Ye Shi", "Kaiyang Ji", "Lan Xu", "Shaoli Huang", "Jingya Wang"], "title": "Human-Object Interaction with Vision-Language Model Guided Relative Movement Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Human-Object Interaction (HOI) is vital for advancing simulation, animation,\nand robotics, enabling the generation of long-term, physically plausible\nmotions in 3D environments. However, existing methods often fall short of\nachieving physics realism and supporting diverse types of interactions. To\naddress these challenges, this paper introduces a unified Human-Object\nInteraction framework that provides unified control over interactions with\nstatic scenes and dynamic objects using language commands. The interactions\nbetween human and object parts can always be described as the continuous stable\nRelative Movement Dynamics (RMD) between human and object parts. By leveraging\nthe world knowledge and scene perception capabilities of Vision-Language Models\n(VLMs), we translate language commands into RMD diagrams, which are used to\nguide goal-conditioned reinforcement learning for sequential interaction with\nobjects. Our framework supports long-horizon interactions among dynamic,\narticulated, and static objects. To support the training and evaluation of our\nframework, we present a new dataset named Interplay, which includes multi-round\ntask plans generated by VLMs, covering both static and dynamic HOI tasks.\nExtensive experiments demonstrate that our proposed framework can effectively\nhandle a wide range of HOI tasks, showcasing its ability to maintain long-term,\nmulti-round transitions. For more details, please refer to our project webpage:\nhttps://rmd-hoi.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18352", "pdf": "https://arxiv.org/pdf/2503.18352", "abs": "https://arxiv.org/abs/2503.18352", "authors": ["Jinjin Zhang", "Qiuyu Huang", "Junjie Liu", "Xiefan Guo", "Di Huang"], "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In this paper, we present Diffusion-4K, a novel framework for direct\nultra-high-resolution image synthesis using text-to-image diffusion models. The\ncore advancements include: (1) Aesthetic-4K Benchmark: addressing the absence\nof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,\na comprehensive benchmark for ultra-high-resolution image generation. We\ncurated a high-quality 4K dataset with carefully selected images and captions\ngenerated by GPT-4o. Additionally, we introduce GLCM Score and Compression\nRatio metrics to evaluate fine details, combined with holistic measures such as\nFID, Aesthetics and CLIPScore for a comprehensive assessment of\nultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a\nwavelet-based fine-tuning approach for direct training with photorealistic 4K\nimages, applicable to various latent diffusion models, demonstrating its\neffectiveness in synthesizing highly detailed 4K images. Consequently,\nDiffusion-4K achieves impressive performance in high-quality image synthesis\nand text prompt adherence, especially when powered by modern large-scale\ndiffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results\nfrom our benchmark demonstrate the superiority of Diffusion-4K in\nultra-high-resolution image synthesis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18364", "pdf": "https://arxiv.org/pdf/2503.18364", "abs": "https://arxiv.org/abs/2503.18364", "authors": ["Chenxi Xie", "Minghan Li", "Hui Zeng", "Jun Luo", "Lei Zhang"], "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution semantic segmentation is essential for applications such as\nimage editing, bokeh imaging, AR/VR, etc. Unfortunately, existing datasets\noften have limited resolution and lack precise mask details and boundaries. In\nthis work, we build a large-scale, matting-level semantic segmentation dataset,\nnamed MaSS13K, which consists of 13,348 real-world images, all at 4K\nresolution. MaSS13K provides high-quality mask annotations of a number of\nobjects, which are categorized into seven categories: human, vegetation,\nground, sky, water, building, and others. MaSS13K features precise masks, with\nan average mask complexity 20-50 times higher than existing semantic\nsegmentation datasets. We consequently present a method specifically designed\nfor high-resolution semantic segmentation, namely MaSSFormer, which employs an\nefficient pixel decoder that aggregates high-level semantic features and\nlow-level texture features across three stages, aiming to produce\nhigh-resolution masks with minimal computational cost. Finally, we propose a\nnew learning paradigm, which integrates the high-quality masks of the seven\ngiven categories with pseudo labels from new classes, enabling MaSSFormer to\ntransfer its accurate segmentation capability to other classes of objects. Our\nproposed MaSSFormer is comprehensively evaluated on the MaSS13K benchmark\ntogether with 14 representative segmentation models. We expect that our\nmeticulously annotated MaSS13K dataset and the MaSSFormer model can facilitate\nthe research of high-resolution and high-quality semantic segmentation.\nDatasets and codes can be found at https://github.com/xiechenxi99/MaSS13K.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18393", "pdf": "https://arxiv.org/pdf/2503.18393", "abs": "https://arxiv.org/abs/2503.18393", "authors": ["Xinhua Xu", "Hong Liu", "Jianbing Wu", "Jinfu Liu"], "title": "PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes", "categories": ["cs.CV"], "comment": null, "summary": "The integration of RGB and depth modalities significantly enhances the\naccuracy of segmenting complex indoor scenes, with depth data from RGB-D\ncameras playing a crucial role in this improvement. However, collecting an\nRGB-D dataset is more expensive than an RGB dataset due to the need for\nspecialized depth sensors. Aligning depth and RGB images also poses challenges\ndue to sensor positioning and issues like missing data and noise. In contrast,\nPseudo Depth (PD) from high-precision depth estimation algorithms can eliminate\nthe dependence on RGB-D sensors and alignment processes, as well as provide\neffective depth information and show significant potential in semantic\nsegmentation. Therefore, to explore the practicality of utilizing pseudo depth\ninstead of real depth for semantic segmentation, we design an RGB-PD\nsegmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo\nDepth Aggregation Module (PDAM) for fully exploiting the informative clues\nprovided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo\ndepth maps into a single modality, making it easily adaptable to other RGB-D\nsegmentation methods. In addition, the pre-trained diffusion model serves as a\nstrong feature extractor for RGB segmentation tasks, but multi-modal\ndiffusion-based segmentation methods remain unexplored. Therefore, we present a\nPseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image\ndiffusion model as a feature extractor and a simple yet effective fusion\nstrategy to integrate pseudo depth. To verify the applicability of pseudo depth\nand our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D\ndatasets. The experimental results demonstrate that pseudo depth can\neffectively enhance segmentation performance, and our PDDM achieves\nstate-of-the-art performance, outperforming other methods by +6.98 mIoU on\nNYUv2 and +2.11 mIoU on SUNRGB-D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18430", "pdf": "https://arxiv.org/pdf/2503.18430", "abs": "https://arxiv.org/abs/2503.18430", "authors": ["Zhichao Sun", "Huazhang Hu", "Yidong Ma", "Gang Liu", "Nemo Chen", "Xu Tang", "Yongchao Xu"], "title": "CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast Vocabulary Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the exponential growth of data, traditional object detection methods are\nincreasingly struggling to handle vast vocabulary object detection tasks\neffectively. We analyze two key limitations of classification-based detectors:\npositive gradient dilution, where rare positive categories receive insufficient\nlearning signals, and hard negative gradient dilution, where discriminative\ngradients are overwhelmed by numerous easy negatives. To address these\nchallenges, we propose CQ-DINO, a category query-based object detection\nframework that reformulates classification as a contrastive task between object\nqueries and learnable category queries. Our method introduces image-guided\nquery selection, which reduces the negative space by adaptively retrieving\ntop-K relevant categories per image via cross-attention, thereby rebalancing\ngradient distributions and facilitating implicit hard example mining.\nFurthermore, CQ-DINO flexibly integrates explicit hierarchical category\nrelationships in structured datasets (e.g., V3Det) or learns implicit category\ncorrelations via self-attention in generic datasets (e.g., COCO). Experiments\ndemonstrate that CQ-DINO achieves superior performance on the challenging V3Det\nbenchmark (surpassing previous methods by 2.1% AP) while maintaining\ncompetitiveness in COCO. Our work provides a scalable solution for real-world\ndetection systems requiring wide category coverage. The dataset and code will\nbe publicly at https://github.com/RedAIGC/CQ-DINO.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18469", "pdf": "https://arxiv.org/pdf/2503.18469", "abs": "https://arxiv.org/abs/2503.18469", "authors": ["Hao Ni", "Lianli Gao", "Pengpeng Zeng", "Heng Tao Shen", "Jingkuan Song"], "title": "CFReID: Continual Few-shot Person Re-Identification", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Real-world surveillance systems are dynamically evolving, requiring a person\nRe-identification model to continuously handle newly incoming data from various\ndomains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed\nto learn and accumulate knowledge across multiple domains incrementally.\nHowever, LReID models need to be trained on large-scale labeled data for each\nunseen domain, which are typically inaccessible due to privacy and cost\nconcerns. In this paper, we propose a new paradigm called Continual Few-shot\nReID (CFReID), which requires models to be incrementally trained using few-shot\ndata and tested on all seen domains. Under few-shot conditions, CFREID faces\ntwo core challenges: 1) learning knowledge from few-shot data of unseen domain,\nand 2) avoiding catastrophic forgetting of seen domains. To tackle these two\nchallenges, we propose a Stable Distribution Alignment (SDA) framework from\nfeature distribution perspective. Specifically, our SDA is composed of two\nmodules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot\nAdaptation (PFA). To support the study of CFReID, we establish an evaluation\nbenchmark for CFReID on five publicly available ReID datasets. Extensive\nexperiments demonstrate that our SDA can enhance the few-shot learning and\nanti-forgetting capabilities under few-shot conditions. Notably, our approach,\nusing only 5\\% of the data, i.e., 32 IDs, significantly outperforms LReID's\nstate-of-the-art performance, which requires 700 to 1,000 IDs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18507", "pdf": "https://arxiv.org/pdf/2503.18507", "abs": "https://arxiv.org/abs/2503.18507", "authors": ["Luca Zanella", "Massimiliano Mancini", "Willi Menapace", "Sergey Tulyakov", "Yiming Wang", "Elisa Ricci"], "title": "Can Text-to-Video Generation help Video-Language Alignment?", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website at https://lucazanella.github.io/synvita/", "summary": "Recent video-language alignment models are trained on sets of videos, each\nwith an associated positive caption and a negative caption generated by large\nlanguage models. A problem with this procedure is that negative captions may\nintroduce linguistic biases, i.e., concepts are seen only as negatives and\nnever associated with a video. While a solution would be to collect videos for\nthe negative captions, existing databases lack the fine-grained variations\nneeded to cover all possible negatives. In this work, we study whether\nsynthetic videos can help to overcome this issue. Our preliminary analysis with\nmultiple generators shows that, while promising on some tasks, synthetic videos\nharm the performance of the model on others. We hypothesize this issue is\nlinked to noise (semantic and visual) in the generated videos and develop a\nmethod, SynViTA, that accounts for those. SynViTA dynamically weights the\ncontribution of each synthetic video based on how similar its target caption is\nw.r.t. the real counterpart. Moreover, a semantic consistency loss makes the\nmodel focus on fine-grained differences across captions, rather than\ndifferences in video appearance. Experiments show that, on average, SynViTA\nimproves over existing methods on VideoCon test sets and SSv2-Temporal,\nSSv2-Events, and ATP-Hard benchmarks, being a first promising step for using\nsynthetic videos when learning video-language models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18540", "pdf": "https://arxiv.org/pdf/2503.18540", "abs": "https://arxiv.org/abs/2503.18540", "authors": ["Guneet Mutreja", "Philipp Schuegraf", "Ksenia Bittner"], "title": "HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in self-supervised learning have led to the development of\nfoundation models that have significantly advanced performance in various\ncomputer vision tasks. However, despite their potential, these models often\noverlook the crucial role of high-resolution digital surface models (DSMs) in\nunderstanding urban environments, particularly for building-level analysis,\nwhich is essential for applications like digital twins. To address this gap, we\nintroduce HiRes-FusedMIM, a novel pre-trained model specifically designed to\nleverage the rich information contained within high-resolution RGB and DSM\ndata. HiRes-FusedMIM utilizes a dual-encoder simple masked image modeling\n(SimMIM) architecture with a multi-objective loss function that combines\nreconstruction and contrastive objectives, enabling it to learn powerful, joint\nrepresentations from both modalities. We conducted a comprehensive evaluation\nof HiRes-FusedMIM on a diverse set of downstream tasks, including\nclassification, semantic segmentation, and instance segmentation. Our results\ndemonstrate that: 1) HiRes-FusedMIM outperforms previous state-of-the-art\ngeospatial methods on several building-related datasets, including WHU Aerial\nand LoveDA, demonstrating its effectiveness in capturing and leveraging\nfine-grained building information; 2) Incorporating DSMs during pre-training\nconsistently improves performance compared to using RGB data alone,\nhighlighting the value of elevation information for building-level analysis; 3)\nThe dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB\nand DSM data, significantly outperforms a single-encoder model on the Vaihingen\nsegmentation task, indicating the benefits of learning specialized\nrepresentations for each modality. To facilitate further research and\napplications in this direction, we will publicly release the trained model\nweights.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18544", "pdf": "https://arxiv.org/pdf/2503.18544", "abs": "https://arxiv.org/abs/2503.18544", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "Distilling Stereo Networks for Performant and Efficient Leaner Networks", "categories": ["cs.CV"], "comment": "8 pages, 3 figures. Published in: 2023 International Joint Conference\n  on Neural Networks (IJCNN)", "summary": "Knowledge distillation has been quite popular in vision for tasks like\nclassification and segmentation however not much work has been done for\ndistilling state-of-the-art stereo matching methods despite their range of\napplications. One of the reasons for its lack of use in stereo matching\nnetworks is due to the inherent complexity of these networks, where a typical\nnetwork is composed of multiple two- and three-dimensional modules. In this\nwork, we systematically combine the insights from state-of-the-art stereo\nmethods with general knowledge-distillation techniques to develop a joint\nframework for stereo networks distillation with competitive results and faster\ninference. Moreover, we show, via a detailed empirical analysis, that\ndistilling knowledge from the stereo network requires careful design of the\ncomplete distillation pipeline starting from backbone to the right selection of\ndistillation points and corresponding loss functions. This results in the\nstudent networks that are not only leaner and faster but give excellent\nperformance . For instance, our student network while performing better than\nthe performance oriented methods like PSMNet [1], CFNet [2], and LEAStereo [3])\non benchmark SceneFlow dataset, is 8x, 5x, and 8x faster respectively.\nFurthermore, compared to speed oriented methods having inference time less than\n100ms, our student networks perform better than all the tested methods. In\naddition, our student network also shows better generalization capabilities\nwhen tested on unseen datasets like ETH3D and Middlebury.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18548", "pdf": "https://arxiv.org/pdf/2503.18548", "abs": "https://arxiv.org/abs/2503.18548", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Lorenzo Brigato", "Stavroula Mougiakakou"], "title": "Benchmarking Post-Hoc Unknown-Category Detection in Food Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Food recognition models often struggle to distinguish between seen and unseen\nsamples, frequently misclassifying samples from unseen categories by assigning\nthem an in-distribution (ID) label. This misclassification presents significant\nchallenges when deploying these models in real-world applications, particularly\nwithin automatic dietary assessment systems, where incorrect labels can lead to\ncascading errors throughout the system. Ideally, such models should prompt the\nuser when an unknown sample is encountered, allowing for corrective action.\nGiven no prior research exploring food recognition in real-world settings, in\nthis work we conduct an empirical analysis of various post-hoc\nout-of-distribution (OOD) detection methods for fine-grained food recognition.\nOur findings indicate that virtual logit matching (ViM) performed the best\noverall, likely due to its combination of logits and feature-space\nrepresentations. Additionally, our work reinforces prior notions in the OOD\ndomain, noting that models with higher ID accuracy performed better across the\nevaluated OOD detection methods. Furthermore, transformer-based architectures\nconsistently outperformed convolution-based models in detecting OOD samples\nacross various methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18552", "pdf": "https://arxiv.org/pdf/2503.18552", "abs": "https://arxiv.org/abs/2503.18552", "authors": ["Qiang Qu", "Ming Li", "Xiaoming Chen", "Tongliang Liu"], "title": "EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Conditional human animation transforms a static reference image into a\ndynamic sequence by applying motion cues such as poses. These motion cues are\ntypically derived from video data but are susceptible to limitations including\nlow temporal resolution, motion blur, overexposure, and inaccuracies under\nlow-light conditions. In contrast, event cameras provide data streams with\nexceptionally high temporal resolution, a wide dynamic range, and inherent\nresistance to motion blur and exposure issues. In this work, we propose\nEvAnimate, a framework that leverages event streams as motion cues to animate\nstatic human images. Our approach employs a specialized event representation\nthat transforms asynchronous event streams into 3-channel slices with\ncontrollable slicing rates and appropriate slice density, ensuring\ncompatibility with diffusion models. Subsequently, a dual-branch architecture\ngenerates high-quality videos by harnessing the inherent motion dynamics of the\nevent streams, thereby enhancing both video quality and temporal consistency.\nSpecialized data augmentation strategies further enhance cross-person\ngeneralization. Finally, we establish a new benchmarking, including simulated\nevent data for training and validation, and a real-world event dataset\ncapturing human actions under normal and extreme scenarios. The experiment\nresults demonstrate that EvAnimate achieves high temporal fidelity and robust\nperformance in scenarios where traditional video-derived cues fall short.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18557", "pdf": "https://arxiv.org/pdf/2503.18557", "abs": "https://arxiv.org/abs/2503.18557", "authors": ["Rafia Rahim", "Samuel Woerz", "Andreas Zell"], "title": "LeanStereo: A Leaner Backbone based Stereo Network", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Recently, end-to-end deep networks based stereo matching methods, mainly\nbecause of their performance, have gained popularity. However, this improvement\nin performance comes at the cost of increased computational and memory\nbandwidth requirements, thus necessitating specialized hardware (GPUs); even\nthen, these methods have large inference times compared to classical methods.\nThis limits their applicability in real-world applications. Although we desire\nhigh accuracy stereo methods albeit with reasonable inference time. To this\nend, we propose a fast end-to-end stereo matching method. Majority of this\nspeedup comes from integrating a leaner backbone. To recover the performance\nlost because of a leaner backbone, we propose to use learned attention weights\nbased cost volume combined with LogL1 loss for stereo matching. Using LogL1\nloss not only improves the overall performance of the proposed network but also\nleads to faster convergence. We do a detailed empirical evaluation of different\ndesign choices and show that our method requires 4x less operations and is also\nabout 9 to 14x faster compared to the state of the art methods like ACVNet [1],\nLEAStereo [2] and CFNet [3] while giving comparable performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18626", "pdf": "https://arxiv.org/pdf/2503.18626", "abs": "https://arxiv.org/abs/2503.18626", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Min Chang Jordan Ren", "Jianfei Yang"], "title": "Generative Dataset Distillation using Min-Max Diffusion Model", "categories": ["cs.CV"], "comment": "The paper is accepted as the ECCV2024 workshop paper and achieved\n  second place in the generative track of The First Dataset Distillation\n  Challenge of ECCV2024, https://www.dd-challenge.com/#/", "summary": "In this paper, we address the problem of generative dataset distillation that\nutilizes generative models to synthesize images. The generator may produce any\nnumber of images under a preserved evaluation time. In this work, we leverage\nthe popular diffusion model as the generator to compute a surrogate dataset,\nboosted by a min-max loss to control the dataset's diversity and\nrepresentativeness during training. However, the diffusion model is\ntime-consuming when generating images, as it requires an iterative generation\nprocess. We observe a critical trade-off between the number of image samples\nand the image quality controlled by the diffusion steps and propose Diffusion\nStep Reduction to achieve optimal performance. This paper details our\ncomprehensive method and its performance. Our model achieved $2^{nd}$ place in\nthe generative track of \\href{https://www.dd-challenge.com/#/}{The First\nDataset Distillation Challenge of ECCV2024}, demonstrating its superior\nperformance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18629", "pdf": "https://arxiv.org/pdf/2503.18629", "abs": "https://arxiv.org/abs/2503.18629", "authors": ["Arne Grobrügge", "Niklas Kühl", "Gerhard Satzger", "Philipp Spitzer"], "title": "Towards Human-Understandable Multi-Dimensional Concept Discovery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Concept-based eXplainable AI (C-XAI) aims to overcome the limitations of\ntraditional saliency maps by converting pixels into human-understandable\nconcepts that are consistent across an entire dataset. A crucial aspect of\nC-XAI is completeness, which measures how well a set of concepts explains a\nmodel's decisions. Among C-XAI methods, Multi-Dimensional Concept Discovery\n(MCD) effectively improves completeness by breaking down the CNN latent space\ninto distinct and interpretable concept subspaces. However, MCD's explanations\ncan be difficult for humans to understand, raising concerns about their\npractical utility. To address this, we propose Human-Understandable\nMulti-dimensional Concept Discovery (HU-MCD). HU-MCD uses the Segment Anything\nModel for concept identification and implements a CNN-specific input masking\ntechnique to reduce noise introduced by traditional masking methods. These\nchanges to MCD, paired with the completeness relation, enable HU-MCD to enhance\nconcept understandability while maintaining explanation faithfulness. Our\nexperiments, including human subject studies, show that HU-MCD provides more\nprecise and reliable explanations than existing C-XAI methods. The code is\navailable at https://github.com/grobruegge/hu-mcd.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "multi-dimensional"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18637", "pdf": "https://arxiv.org/pdf/2503.18637", "abs": "https://arxiv.org/abs/2503.18637", "authors": ["Nina Shvetsova", "Arsha Nagrani", "Bernt Schiele", "Hilde Kuehne", "Christian Rupprecht"], "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks", "categories": ["cs.CV"], "comment": "To be published at CVPR 2025, project webpage\n  https://utd-project.github.io/", "summary": "We propose a new \"Unbiased through Textual Description (UTD)\" video benchmark\nbased on unbiased subsets of existing video classification and retrieval\ndatasets to enable a more robust assessment of video understanding\ncapabilities. Namely, we tackle the problem that current video benchmarks may\nsuffer from different representation biases, e.g., object bias or single-frame\nbias, where mere recognition of objects or utilization of only a single frame\nis sufficient for correct prediction. We leverage VLMs and LLMs to analyze and\ndebias benchmarks from such representation biases. Specifically, we generate\nframe-wise textual descriptions of videos, filter them for specific information\n(e.g. only objects) and leverage them to examine representation biases across\nthree dimensions: 1) concept bias - determining if a specific concept (e.g.,\nobjects) alone suffice for prediction; 2) temporal bias - assessing if temporal\ninformation contributes to prediction; and 3) common sense vs. dataset bias -\nevaluating whether zero-shot reasoning or dataset correlations contribute to\nprediction. We conduct a systematic analysis of 12 popular video classification\nand retrieval datasets and create new object-debiased test splits for these\ndatasets. Moreover, we benchmark 30 state-of-the-art video models on original\nand debiased splits and analyze biases in the models. To facilitate the future\ndevelopment of more robust video understanding benchmarks and models, we\nrelease: \"UTD-descriptions\", a dataset with our rich structured descriptions\nfor each dataset, and \"UTD-splits\", a dataset of object-debiased test splits.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18711", "pdf": "https://arxiv.org/pdf/2503.18711", "abs": "https://arxiv.org/abs/2503.18711", "authors": ["Thomas Sugg", "Kyle O'Brien", "Lekh Poudel", "Alex Dumouchelle", "Michelle Jou", "Marc Bosch", "Deva Ramanan", "Srinivasa Narasimhan", "Shubham Tulsiani"], "title": "Accenture-NVS1: A Novel View Synthesis Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "6 pages, 7 figures", "summary": "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18725", "pdf": "https://arxiv.org/pdf/2503.18725", "abs": "https://arxiv.org/abs/2503.18725", "authors": ["Zimin Xia", "Alexandre Alahi"], "title": "FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel fine-grained cross-view localization method that estimates\nthe 3 Degrees of Freedom pose of a ground-level image in an aerial image of the\nsurroundings by matching fine-grained features between the two images. The pose\nis estimated by aligning a point plane generated from the ground image with a\npoint plane sampled from the aerial image. To generate the ground points, we\nfirst map ground image features to a 3D point cloud. Our method then learns to\nselect features along the height dimension to pool the 3D points to a\nBird's-Eye-View (BEV) plane. This selection enables us to trace which feature\nin the ground image contributes to the BEV representation. Next, we sample a\nset of sparse matches from computed point correspondences between the two point\nplanes and compute their relative pose using Procrustes alignment. Compared to\nthe previous state-of-the-art, our method reduces the mean localization error\nby 28% on the VIGOR cross-area test set. Qualitative results show that our\nmethod learns semantically consistent matches across ground and aerial views\nthrough weakly supervised learning from the camera pose.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained", "dimension"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18808", "pdf": "https://arxiv.org/pdf/2503.18808", "abs": "https://arxiv.org/abs/2503.18808", "authors": ["Yang Liu", "Hongjin Wang", "Zepu Wang", "Xiaoguang Zhu", "Jing Liu", "Peng Sun", "Rui Tang", "Jianwei Du", "Victor C. M. Leung", "Liang Song"], "title": "CRCL: Causal Representation Consistency Learning for Anomaly Detection in Surveillance Videos", "categories": ["cs.CV"], "comment": "Accepted for publication by IEEE Transactions on Image Processing", "summary": "Video Anomaly Detection (VAD) remains a fundamental yet formidable task in\nthe video understanding community, with promising applications in areas such as\ninformation forensics and public safety protection. Due to the rarity and\ndiversity of anomalies, existing methods only use easily collected regular\nevents to model the inherent normality of normal spatial-temporal patterns in\nan unsupervised manner. Previous studies have shown that existing unsupervised\nVAD models are incapable of label-independent data offsets (e.g., scene\nchanges) in real-world scenarios and may fail to respond to light anomalies due\nto the overgeneralization of deep neural networks. Inspired by causality\nlearning, we argue that there exist causal factors that can adequately\ngeneralize the prototypical patterns of regular events and present significant\ndeviations when anomalous instances occur. In this regard, we propose Causal\nRepresentation Consistency Learning (CRCL) to implicitly mine potential\nscene-robust causal variable in unsupervised video normality learning.\nSpecifically, building on the structural causal models, we propose\nscene-debiasing learning and causality-inspired normality learning to strip\naway entangled scene bias in deep representations and learn causal video\nnormality, respectively. Extensive experiments on benchmarks validate the\nsuperiority of our method over conventional deep representation learning.\nMoreover, ablation studies and extension validation show that the CRCL can cope\nwith label-independent biases in multi-scene settings and maintain stable\nperformance with only limited training data available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "consistency"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18817", "pdf": "https://arxiv.org/pdf/2503.18817", "abs": "https://arxiv.org/abs/2503.18817", "authors": ["Jeonghyeon Kim", "Sangheum Hwang"], "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18830", "pdf": "https://arxiv.org/pdf/2503.18830", "abs": "https://arxiv.org/abs/2503.18830", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Hangrui Xu", "Peng Jiao", "Haoqian Wang"], "title": "DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18854", "pdf": "https://arxiv.org/pdf/2503.18854", "abs": "https://arxiv.org/abs/2503.18854", "authors": ["Ruichuan An", "Sihan Yang", "Ming Lu", "Renrui Zhang", "Kai Zeng", "Yulin Luo", "Jiajun Cao", "Hao Liang", "Ying Chen", "Qi She", "Shanghang Zhang", "Wentao Zhang"], "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset will be publicly available at\n  $\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$", "summary": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at\n$\\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}$.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18872", "pdf": "https://arxiv.org/pdf/2503.18872", "abs": "https://arxiv.org/abs/2503.18872", "authors": ["Yanda Chen", "Gongwei Chen", "Miao Zhang", "Weili Guan", "Liqiang Nie"], "title": "Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Dataset distillation (DD) excels in synthesizing a small number of images per\nclass (IPC) but struggles to maintain its effectiveness in high-IPC settings.\nRecent works on dataset distillation demonstrate that combining distilled and\nreal data can mitigate the effectiveness decay. However, our analysis of the\ncombination paradigm reveals that the current one-shot and independent\nselection mechanism induces an incompatibility issue between distilled and real\nimages. To address this issue, we introduce a novel curriculum coarse-to-fine\nselection (CCFS) method for efficient high-IPC dataset distillation. CCFS\nemploys a curriculum selection framework for real data selection, where we\nleverage a coarse-to-fine strategy to select appropriate real data based on the\ncurrent synthetic dataset in each curriculum. Extensive experiments validate\nCCFS, surpassing the state-of-the-art by +6.6\\% on CIFAR-10, +5.8\\% on\nCIFAR-100, and +3.4\\% on Tiny-ImageNet under high-IPC settings. Notably, CCFS\nachieves 60.2\\% test accuracy on ResNet-18 with a 20\\% compression ratio of\nTiny-ImageNet, closely matching full-dataset training with only 0.3\\%\ndegradation. Code: https://github.com/CYDaaa30/CCFS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18897", "pdf": "https://arxiv.org/pdf/2503.18897", "abs": "https://arxiv.org/abs/2503.18897", "authors": ["Thomas Chabal", "Shizhe Chen", "Jean Ponce", "Cordelia Schmid"], "title": "Online 3D Scene Reconstruction Using Neural Object Priors", "categories": ["cs.CV", "cs.RO"], "comment": "3DV 2025. Project page:\n  https://www.di.ens.fr/willow/research/online-scene-reconstruction/", "summary": "This paper addresses the problem of reconstructing a scene online at the\nlevel of objects given an RGB-D video sequence. While current object-aware\nneural implicit representations hold promise, they are limited in online\nreconstruction efficiency and shape completion. Our main contributions to\nalleviate the above limitations are twofold. First, we propose a feature grid\ninterpolation mechanism to continuously update grid-based object-centric neural\nimplicit representations as new object parts are revealed. Second, we construct\nan object library with previously mapped objects in advance and leverage the\ncorresponding shape priors to initialize geometric object models in new videos,\nsubsequently completing them with novel views as well as synthesized past views\nto avoid losing original object details. Extensive experiments on synthetic\nenvironments from the Replica dataset, real-world ScanNet sequences and videos\ncaptured in our laboratory demonstrate that our approach outperforms\nstate-of-the-art neural implicit models for this task in terms of\nreconstruction accuracy and completeness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18931", "pdf": "https://arxiv.org/pdf/2503.18931", "abs": "https://arxiv.org/abs/2503.18931", "authors": ["Yitong Chen", "Lingchen Meng", "Wujian Peng", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "categories": ["cs.CV"], "comment": "Code is available in https://github.com/SliMM-X/CoMP-MM", "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18933", "pdf": "https://arxiv.org/pdf/2503.18933", "abs": "https://arxiv.org/abs/2503.18933", "authors": ["Enrico Pallotta", "Sina Mokhtarzadeh Azar", "Shuai Li", "Olga Zatsarynna", "Juergen Gall"], "title": "SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting future video frames is essential for decision-making systems, yet\nRGB frames alone often lack the information needed to fully capture the\nunderlying complexities of the real world. To address this limitation, we\npropose a multi-modal framework for Synchronous Video Prediction (SyncVP) that\nincorporates complementary data modalities, enhancing the richness and accuracy\nof future predictions. SyncVP builds on pre-trained modality-specific diffusion\nmodels and introduces an efficient spatio-temporal cross-attention module to\nenable effective information sharing across modalities. We evaluate SyncVP on\nstandard benchmark datasets, such as Cityscapes and BAIR, using depth as an\nadditional modality. We furthermore demonstrate its generalization to other\nmodalities on SYNTHIA with semantic information and ERA5-Land with climate\ndata. Notably, SyncVP achieves state-of-the-art performance, even in scenarios\nwhere only one modality is present, demonstrating its robustness and potential\nfor a wide range of applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17543", "pdf": "https://arxiv.org/pdf/2503.17543", "abs": "https://arxiv.org/abs/2503.17543", "authors": ["Moein Heidari", "Afshin Bozorgpour", "AmirHossein Zarif-Fakharnia", "Dorit Merhof", "Ilker Hacihaliloglu"], "title": "Echo-E$^3$Net: Efficient Endo-Epi Spatio-Temporal Network for Ejection Fraction Estimation", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted as a conference paper to MICCAI 2025", "summary": "Left ventricular ejection fraction (LVEF) is a critical metric for assessing\ncardiac function, widely used in diagnosing heart failure and guiding clinical\ndecisions. Despite its importance, conventional LVEF estimation remains\ntime-consuming and operator-dependent. Recent deep learning advancements have\nenhanced automation, yet many existing models are computationally demanding,\nhindering their feasibility for real-time clinical applications. Additionally,\nthe interplay between spatial and temporal features is crucial for accurate\nestimation but is often overlooked. In this work, we propose Echo-E$^3$Net, an\nefficient Endo-Epi spatio-temporal network tailored for LVEF estimation. Our\nmethod introduces the Endo-Epi Cardial Border Detector (E$^2$CBD) module, which\nenhances feature extraction by leveraging spatial and temporal landmark cues.\nComplementing this, the Endo-Epi Feature Aggregator (E$^2$FA) distills\nstatistical descriptors from backbone feature maps, refining the final EF\nprediction. These modules, along with a multi-component loss function tailored\nto align with the clinical definition of EF, collectively enhance\nspatial-temporal representation learning, ensuring robust and efficient EF\nestimation. We evaluate Echo-E$^3$Net on the EchoNet-Dynamic dataset, achieving\na RMSE of 5.15 and an R$^2$ score of 0.82, setting a new benchmark in\nefficiency with 6.8 million parameters and only 8.49G Flops. Our model operates\nwithout pre-training, data augmentation, or ensemble methods, making it\nwell-suited for real-time point-of-care ultrasound (PoCUS) applications. Our\nCode is publicly available\non~\\href{https://github.com/moeinheidari7829/Echo-E3Net}{\\textcolor{magenta}{GitHub}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17733", "pdf": "https://arxiv.org/pdf/2503.17733", "abs": "https://arxiv.org/abs/2503.17733", "authors": ["Bin Fu", "Jialin Li", "Bin Zhang", "Ruiping Wang", "Xilin Chen"], "title": "GS-LTS: 3D Gaussian Splatting-Based Adaptive Modeling for Long-Term Service Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has garnered significant attention in robotics\nfor its explicit, high fidelity dense scene representation, demonstrating\nstrong potential for robotic applications. However, 3DGS-based methods in\nrobotics primarily focus on static scenes, with limited attention to the\ndynamic scene changes essential for long-term service robots. These robots\ndemand sustained task execution and efficient scene updates-challenges current\napproaches fail to meet. To address these limitations, we propose GS-LTS\n(Gaussian Splatting for Long-Term Service), a 3DGS-based system enabling indoor\nrobots to manage diverse tasks in dynamic environments over time. GS-LTS\ndetects scene changes (e.g., object addition or removal) via single-image\nchange detection, employs a rule-based policy to autonomously collect\nmulti-view observations, and efficiently updates the scene representation\nthrough Gaussian editing. Additionally, we propose a simulation-based benchmark\nthat automatically generates scene change data as compact configuration\nscripts, providing a standardized, user-friendly evaluation benchmark.\nExperimental results demonstrate GS-LTS's advantages in reconstruction,\nnavigation, and superior scene updates-faster and higher quality than the image\ntraining baseline-advancing 3DGS for long-term robotic operations. Code and\nbenchmark are available at: https://vipl-vsu.github.io/3DGS-LTS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17970", "pdf": "https://arxiv.org/pdf/2503.17970", "abs": "https://arxiv.org/abs/2503.17970", "authors": ["Yang Luo", "Shiru Wang", "Jun Liu", "Jiaxuan Xiao", "Rundong Xue", "Zeyu Zhang", "Hao Zhang", "Yu Lu", "Yang Zhao", "Yutong Xie"], "title": "PathoHR: Breast Cancer Survival Prediction on High-Resolution Pathological Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Breast cancer survival prediction in computational pathology presents a\nremarkable challenge due to tumor heterogeneity. For instance, different\nregions of the same tumor in the pathology image can show distinct\nmorphological and molecular characteristics. This makes it difficult to extract\nrepresentative features from whole slide images (WSIs) that truly reflect the\ntumor's aggressive potential and likely survival outcomes. In this paper, we\npresent PathoHR, a novel pipeline for accurate breast cancer survival\nprediction that enhances any size of pathological images to enable more\neffective feature learning. Our approach entails (1) the incorporation of a\nplug-and-play high-resolution Vision Transformer (ViT) to enhance patch-wise\nWSI representation, enabling more detailed and comprehensive feature\nextraction, (2) the systematic evaluation of multiple advanced similarity\nmetrics for comparing WSI-extracted features, optimizing the representation\nlearning process to better capture tumor characteristics, (3) the demonstration\nthat smaller image patches enhanced follow the proposed pipeline can achieve\nequivalent or superior prediction accuracy compared to raw larger patches,\nwhile significantly reducing computational overhead. Experimental findings\nvalid that PathoHR provides the potential way of integrating enhanced image\nresolution with optimized feature learning to advance computational pathology,\noffering a promising direction for more accurate and efficient breast cancer\nsurvival prediction. Code will be available at\nhttps://github.com/AIGeeksGroup/PathoHR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18151", "pdf": "https://arxiv.org/pdf/2503.18151", "abs": "https://arxiv.org/abs/2503.18151", "authors": ["Siwon Kim", "Wooyung Yun", "Jeongbin Oh", "Soomok Lee"], "title": "Efficient Deep Learning Approaches for Processing Ultra-Widefield Retinal Imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning has emerged as the predominant solution for classifying medical\nimages. We intend to apply these developments to the ultra-widefield (UWF)\nretinal imaging dataset. Since UWF images can accurately diagnose various\nretina diseases, it is very important to clas sify them accurately and prevent\nthem with early treatment. However, processing images manually is\ntime-consuming and labor-intensive, and there are two challenges to automating\nthis process. First, high perfor mance usually requires high computational\nresources. Artificial intelli gence medical technology is better suited for\nplaces with limited medical resources, but using high-performance processing\nunits in such environ ments is challenging. Second, the problem of the accuracy\nof colour fun dus photography (CFP) methods. In general, the UWF method\nprovides more information for retinal diagnosis than the CFP method, but most\nof the research has been conducted based on the CFP method. Thus, we\ndemonstrate that these problems can be efficiently addressed in low performance\nunits using methods such as strategic data augmentation and model ensembles,\nwhich balance performance and computational re sources while utilizing UWF\nimages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18246", "pdf": "https://arxiv.org/pdf/2503.18246", "abs": "https://arxiv.org/abs/2503.18246", "authors": ["Feiran Wang", "Bin Duan", "Jiachen Tao", "Nikhil Sharma", "Dawen Cai", "Yan Yan"], "title": "ZECO: ZeroFusion Guided 3D MRI Conditional Generation", "categories": ["eess.IV", "cs.CV"], "comment": "Project page: \\url{https://brack-wang.github.io/ZECO_web/}; Github\n  Code: \\url{https://github.com/Brack-Wang/ZECO}", "summary": "Medical image segmentation is crucial for enhancing diagnostic accuracy and\ntreatment planning in Magnetic Resonance Imaging (MRI). However, acquiring\nprecise lesion masks for segmentation model training demands specialized\nexpertise and significant time investment, leading to a small dataset scale in\nclinical practice. In this paper, we present ZECO, a ZeroFusion guided 3D MRI\nconditional generation framework that extracts, compresses, and generates\nhigh-fidelity MRI images with corresponding 3D segmentation masks to mitigate\ndata scarcity. To effectively capture inter-slice relationships within volumes,\nwe introduce a Spatial Transformation Module that encodes MRI images into a\ncompact latent space for the diffusion process. Moving beyond unconditional\ngeneration, our novel ZeroFusion method progressively maps 3D masks to MRI\nimages in latent space, enabling robust training on limited datasets while\navoiding overfitting. ZECO outperforms state-of-the-art models in both\nquantitative and qualitative evaluations on Brain MRI datasets across various\nmodalities, showcasing its exceptional capability in synthesizing high-quality\nMRI images conditioned on segmentation masks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18642", "pdf": "https://arxiv.org/pdf/2503.18642", "abs": "https://arxiv.org/abs/2503.18642", "authors": ["Taejin Jeong", "Joohyeok Kim", "Jaehoon Joo", "Yeonwoo Jung", "Hyeonmin Kim", "Seong Jae Hwang"], "title": "Rethinking Glaucoma Calibration: Voting-Based Binocular and Metadata Integration", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glaucoma is an incurable ophthalmic disease that damages the optic nerve,\nleads to vision loss, and ranks among the leading causes of blindness\nworldwide. Diagnosing glaucoma typically involves fundus photography, optical\ncoherence tomography (OCT), and visual field testing. However, the high cost of\nOCT often leads to reliance on fundus photography and visual field testing,\nboth of which exhibit inherent inter-observer variability. This stems from\nglaucoma being a multifaceted disease that influenced by various factors. As a\nresult, glaucoma diagnosis is highly subjective, emphasizing the necessity of\ncalibration, which aligns predicted probabilities with actual disease\nlikelihood. Proper calibration is essential to prevent overdiagnosis or\nmisdiagnosis, which are critical concerns for high-risk diseases. Although AI\nhas significantly improved diagnostic accuracy, overconfidence in models have\nworsen calibration performance. Recent study has begun focusing on calibration\nfor glaucoma. Nevertheless, previous study has not fully considered glaucoma's\nsystemic nature and the high subjectivity in its diagnostic process. To\novercome these limitations, we propose V-ViT (Voting-based ViT), a novel\nframework that enhances calibration by incorporating disease-specific\ncharacteristics. V-ViT integrates binocular data and metadata, reflecting the\nmulti-faceted nature of glaucoma diagnosis. Additionally, we introduce a MC\ndropout-based Voting System to address high subjectivity. Our approach achieves\nstate-of-the-art performance across all metrics, including accuracy,\ndemonstrating that our proposed methods are effective in addressing calibration\nissues. We validate our method using a custom dataset including binocular data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17403", "pdf": "https://arxiv.org/pdf/2503.17403", "abs": "https://arxiv.org/abs/2503.17403", "authors": ["Azim Akhtarshenas", "Afshin Dini", "Navid Ayoobi"], "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revo lutionized natural language processing\nNatural Language Processing (NLP), with Chat Generative Pre-trained Transformer\n(ChatGPT) standing out as a notable exampledue to its advanced capabilities and\nwidespread applications. This survey provides a comprehensive analysis of\nChatGPT, exploring its architecture, training processes, and functionalities.\nWe examine its integration into various domains across industries such as\ncustomer service, education, healthcare, and entertainment. A comparative\nanalysis with other LLMs highlights ChatGPT's unique features and performance\nmetrics. Regarding benchmarks, the paper examines ChatGPT's comparative\nperformance against other LLMs and discusses potential risks such as\nmisinformation, bias, and data privacy concerns. Additionally, we offer a\nnumber of figures and tables that outline the backdrop of the discussion, the\nmain ideas of the article, the numerous LLM models, a thorough list of datasets\nused for pre-training, fine-tuning, and evaluation, as well as particular LLM\napplications with pertinent references. Finally, we identify future research\ndirections and technological advancements, underscoring the evolving landscape\nof LLMs and their profound impact on artificial intelligence Artificial\nIntelligence (AI) and society.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17407", "pdf": "https://arxiv.org/pdf/2503.17407", "abs": "https://arxiv.org/abs/2503.17407", "authors": ["Jiaheng Liu", "Dawei Zhu", "Zhiqi Bai", "Yancheng He", "Huanxuan Liao", "Haoran Que", "Zekun Wang", "Chenchen Zhang", "Ge Zhang", "Jiebin Zhang", "Yuanxing Zhang", "Zhuo Chen", "Hangyu Guo", "Shilong Li", "Ziqiang Liu", "Yong Shan", "Yifan Song", "Jiayi Tian", "Wenhao Wu", "Zhejian Zhou", "Ruijie Zhu", "Junlan Feng", "Yang Gao", "Shizhu He", "Zhoujun Li", "Tianyu Liu", "Fanyu Meng", "Wenbo Su", "Yingshui Tan", "Zili Wang", "Jian Yang", "Wei Ye", "Bo Zheng", "Wangchunshu Zhou", "Wenhao Huang", "Sujian Li", "Zhaoxiang Zhang"], "title": "A Comprehensive Survey on Long Context Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Efficient processing of long contexts has been a persistent pursuit in\nNatural Language Processing. With the growing number of long documents,\ndialogues, and other textual data, it is important to develop Long Context\nLanguage Models (LCLMs) that can process and analyze extensive inputs in an\neffective and efficient way. In this paper, we present a comprehensive survey\non recent advances in long-context modeling for large language models. Our\nsurvey is structured around three key aspects: how to obtain effective and\nefficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate\nand analyze LCLMs comprehensively. For the first aspect, we discuss data\nstrategies, architectural designs, and workflow approaches oriented with long\ncontext processing. For the second aspect, we provide a detailed examination of\nthe infrastructure required for LCLM training and inference. For the third\naspect, we present evaluation paradigms for long-context comprehension and\nlong-form generation, as well as behavioral analysis and mechanism\ninterpretability of LCLMs. Beyond these three key aspects, we thoroughly\nexplore the diverse application scenarios where existing LCLMs have been\ndeployed and outline promising future development directions. This survey\nprovides an up-to-date review of the literature on long-context LLMs, which we\nwish to serve as a valuable resource for both researchers and engineers. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at:\n\\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17425", "pdf": "https://arxiv.org/pdf/2503.17425", "abs": "https://arxiv.org/abs/2503.17425", "authors": ["Veysel Kocaman", "Yigit Gul", "M. Aytug Kaya", "Hasham Ul Haq", "Mehmet Butgul", "Cabir Celik", "David Talby"], "title": "Beyond Negation Detection: Comprehensive Assertion Detection Models for Clinical NLP", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3"], "comment": "accepted at Text2Story Workshop at ECIR 2025", "summary": "Assertion status detection is a critical yet often overlooked component of\nclinical NLP, essential for accurately attributing extracted medical facts.\nPast studies have narrowly focused on negation detection, leading to\nunderperforming commercial solutions such as AWS Medical Comprehend, Azure AI\nText Analytics, and GPT-4o due to their limited domain adaptation. To address\nthis gap, we developed state-of-the-art assertion detection models, including\nfine-tuned LLMs, transformer-based classifiers, few-shot classifiers, and deep\nlearning (DL) approaches. We evaluated these models against cloud-based\ncommercial API solutions, the legacy rule-based NegEx approach, and GPT-4o. Our\nfine-tuned LLM achieves the highest overall accuracy (0.962), outperforming\nGPT-4o (0.901) and commercial APIs by a notable margin, particularly excelling\nin Present (+4.2%), Absent (+8.4%), and Hypothetical (+23.4%) assertions. Our\nDL-based models surpass commercial solutions in Conditional (+5.3%) and\nAssociated-with-Someone-Else (+10.1%) categories, while the few-shot classifier\noffers a lightweight yet highly competitive alternative (0.929), making it\nideal for resource-constrained environments. Integrated within Spark NLP, our\nmodels consistently outperform black-box commercial solutions while enabling\nscalable inference and seamless integration with medical NER, Relation\nExtraction, and Terminology Resolution. These results reinforce the importance\nof domain-adapted, transparent, and customizable clinical NLP solutions over\ngeneral-purpose LLMs and proprietary APIs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17415", "pdf": "https://arxiv.org/pdf/2503.17415", "abs": "https://arxiv.org/abs/2503.17415", "authors": ["Yicheng Duan", "Xi Huang", "Duo Chen"], "title": "Enhancing Subsequent Video Retrieval via Vision-Language Models (VLMs)", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "The rapid growth of video content demands efficient and precise retrieval\nsystems. While vision-language models (VLMs) excel in representation learning,\nthey often struggle with adaptive, time-sensitive video retrieval. This paper\nintroduces a novel framework that combines vector similarity search with\ngraph-based data structures. By leveraging VLM embeddings for initial retrieval\nand modeling contextual relationships among video segments, our approach\nenables adaptive query refinement and improves retrieval accuracy. Experiments\ndemonstrate its precision, scalability, and robustness, offering an effective\nsolution for interactive video retrieval in dynamic environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17514", "pdf": "https://arxiv.org/pdf/2503.17514", "abs": "https://arxiv.org/abs/2503.17514", "authors": ["Ken Ziyu Liu", "Christopher A. Choquette-Choo", "Matthew Jagielski", "Peter Kairouz", "Sanmi Koyejo", "Percy Liang", "Nicolas Papernot"], "title": "Language Models May Verbatim Complete TextThey Were Not Explicitly Trained On", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20\n  tables, 15 figures", "summary": "An important question today is whether a given text was used to train a large\nlanguage model (LLM). A \\emph{completion} test is often employed: check if the\nLLM completes a sufficiently complex text. This, however, requires a\nground-truth definition of membership; most commonly, it is defined as a member\nbased on the $n$-gram overlap between the target text and any text in the\ndataset. In this work, we demonstrate that this $n$-gram based membership\ndefinition can be effectively gamed. We study scenarios where sequences are\n\\emph{non-members} for a given $n$ and we find that completion tests still\nsucceed. We find many natural cases of this phenomenon by retraining LLMs from\nscratch after removing all training samples that were completed; these cases\ninclude exact duplicates, near-duplicates, and even short overlaps. They\nshowcase that it is difficult to find a single viable choice of $n$ for\nmembership definitions. Using these insights, we design adversarial datasets\nthat can cause a given target sequence to be completed without containing it,\nfor any reasonable choice of $n$. Our findings highlight the inadequacy of\n$n$-gram membership, suggesting membership definitions fail to account for\nauxiliary information available to the training algorithm.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17579", "pdf": "https://arxiv.org/pdf/2503.17579", "abs": "https://arxiv.org/abs/2503.17579", "authors": ["Suet-Ying Lam", "Qingcheng Zeng", "Jingyi Wu", "Rob Voigt"], "title": "Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility", "categories": ["cs.CL"], "comment": null, "summary": "Whether large language models (LLMs) process language similarly to humans has\nbeen the subject of much theoretical and practical debate. We examine this\nquestion through the lens of the production-interpretation distinction found in\nhuman sentence processing and evaluate the extent to which instruction-tuned\nLLMs replicate this distinction. Using an empirically documented asymmetry\nbetween production and interpretation in humans for implicit causality verbs as\na testbed, we find that some LLMs do quantitatively and qualitatively reflect\nhuman-like asymmetries between production and interpretation. We demonstrate\nthat whether this behavior holds depends upon both model size - with larger\nmodels more likely to reflect human-like patterns and the choice of\nmeta-linguistic prompts used to elicit the behavior.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17684", "pdf": "https://arxiv.org/pdf/2503.17684", "abs": "https://arxiv.org/abs/2503.17684", "authors": ["Dhruv Sahnan", "David Corney", "Irene Larraz", "Giovanni Zagni", "Ruben Miguez", "Zhuohan Xie", "Iryna Gurevych", "Elizabeth Churchill", "Tanmoy Chakraborty", "Preslav Nakov"], "title": "Can LLMs Automate Fact-Checking Article Writing?", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 figures, 6 tables", "summary": "Automatic fact-checking aims to support professional fact-checkers by\noffering tools that can help speed up manual fact-checking. Yet, existing\nframeworks fail to address the key step of producing output suitable for\nbroader dissemination to the general public: while human fact-checkers\ncommunicate their findings through fact-checking articles, automated systems\ntypically produce little or no justification for their assessments. Here, we\naim to bridge this gap. We argue for the need to extend the typical automatic\nfact-checking pipeline with automatic generation of full fact-checking\narticles. We first identify key desiderata for such articles through a series\nof interviews with experts from leading fact-checking organizations. We then\ndevelop QRAFT, an LLM-based agentic framework that mimics the writing workflow\nof human fact-checkers. Finally, we assess the practical usefulness of QRAFT\nthrough human evaluations with professional fact-checkers. Our evaluation shows\nthat while QRAFT outperforms several previously proposed text-generation\napproaches, it lags considerably behind expert-written articles. We hope that\nour work will enable further research in this new and important direction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17753", "pdf": "https://arxiv.org/pdf/2503.17753", "abs": "https://arxiv.org/abs/2503.17753", "authors": ["Hojun Cho", "Donghu Kim", "Soyoung Yang", "Chan Lee", "Hunjoo Lee", "Jaegul Choo"], "title": "Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17530", "pdf": "https://arxiv.org/pdf/2503.17530", "abs": "https://arxiv.org/abs/2503.17530", "authors": ["Tianyu Zhang", "Fan Wan", "Haoran Duan", "Kevin W. Tong", "Jingjing Deng", "Yang Long"], "title": "FMDConv: Fast Multi-Attention Dynamic Convolution via Speed-Accuracy Trade-off", "categories": ["cs.CV"], "comment": null, "summary": "Spatial convolution is fundamental in constructing deep Convolutional Neural\nNetworks (CNNs) for visual recognition. While dynamic convolution enhances\nmodel accuracy by adaptively combining static kernels, it incurs significant\ncomputational overhead, limiting its deployment in resource-constrained\nenvironments such as federated edge computing. To address this, we propose Fast\nMulti-Attention Dynamic Convolution (FMDConv), which integrates input\nattention, temperature-degraded kernel attention, and output attention to\noptimize the speed-accuracy trade-off. FMDConv achieves a better balance\nbetween accuracy and efficiency by selectively enhancing feature extraction\nwith lower complexity. Furthermore, we introduce two novel quantitative\nmetrics, the Inverse Efficiency Score and Rate-Correct Score, to systematically\nevaluate this trade-off. Extensive experiments on CIFAR-10, CIFAR-100, and\nImageNet demonstrate that FMDConv reduces the computational cost by up to\n49.8\\% on ResNet-18 and 42.2\\% on ResNet-50 compared to prior multi-attention\ndynamic convolution methods while maintaining competitive accuracy. These\nadvantages make FMDConv highly suitable for real-world, resource-constrained\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17755", "pdf": "https://arxiv.org/pdf/2503.17755", "abs": "https://arxiv.org/abs/2503.17755", "authors": ["Sharan Maiya", "Yinhong Liu", "Ramit Debnath", "Anna Korhonen"], "title": "Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes", "categories": ["cs.CL", "cs.LG"], "comment": "preprint, submitted to ACL ARR 2025, 21 pages, 23 figures", "summary": "Large Language Models (LLMs) are often used as automated judges to evaluate\ntext, but their effectiveness can be hindered by various unintentional biases.\nWe propose using linear classifying probes, trained by leveraging differences\nbetween contrasting pairs of prompts, to directly access LLMs' latent knowledge\nand extract more accurate preferences. Through extensive experiments using\nmodels of varying size from four different families and six diverse datasets\nassessing text quality evaluation and common sense reasoning, we demonstrate\nthat both supervised and unsupervised probing approaches consistently\noutperform traditional generation-based judgement while maintaining similar\ncomputational costs. These probes generalise under domain shifts and can even\noutperform finetuned evaluators with the same training data size. Our results\nsuggest linear probing offers an accurate, robust and computationally efficient\napproach for LLM-as-judge tasks while providing interpretable insights into how\nmodels encode judgement-relevant knowledge. Our data and code will be openly\nreleased in the future.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17539", "pdf": "https://arxiv.org/pdf/2503.17539", "abs": "https://arxiv.org/abs/2503.17539", "authors": ["Bhishma Dedhia", "David Bourgin", "Krishna Kumar Singh", "Yuheng Li", "Yan Kang", "Zhan Xu", "Niraj K. Jha", "Yuchen Liu"], "title": "Generating, Fast and Slow: Scalable Parallel Video Generation with Video Interface Networks", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) can generate short photorealistic videos, yet\ndirectly training and sampling longer videos with full attention across the\nvideo remains computationally challenging. Alternative methods break long\nvideos down into sequential generation of short video segments, requiring\nmultiple sampling chain iterations and specialized consistency modules. To\novercome these challenges, we introduce a new paradigm called Video Interface\nNetworks (VINs), which augment DiTs with an abstraction module to enable\nparallel inference of video chunks. At each diffusion step, VINs encode global\nsemantics from the noisy input of local chunks and the encoded representations,\nin turn, guide DiTs in denoising chunks in parallel. The coupling of VIN and\nDiT is learned end-to-end on the denoising objective. Further, the VIN\narchitecture maintains fixed-size encoding tokens that encode the input via a\nsingle cross-attention step. Disentangling the encoding tokens from the input\nthus enables VIN to scale to long videos and learn essential semantics.\nExperiments on VBench demonstrate that VINs surpass existing chunk-based\nmethods in preserving background consistency and subject coherence. We then\nshow via an optical flow analysis that our approach attains state-of-the-art\nmotion smoothness while using 25-40% fewer FLOPs than full generation. Finally,\nhuman raters favorably assessed the overall video quality and temporal\nconsistency of our method in a user study.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17811", "pdf": "https://arxiv.org/pdf/2503.17811", "abs": "https://arxiv.org/abs/2503.17811", "authors": ["Wenqi Pei", "Hailing Xu", "Hengyuan Zhao", "Shizheng Hou", "Han Chen", "Zining Zhang", "Pingyi Luo", "Bingsheng He"], "title": "Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Natural Language to SQL (NL2SQL) has seen significant advancements with large\nlanguage models (LLMs). However, these models often depend on closed-source\nsystems and high computational resources, posing challenges in data privacy and\ndeployment. In contrast, small language models (SLMs) struggle with NL2SQL\ntasks, exhibiting poor performance and incompatibility with existing\nframeworks. To address these issues, we introduce Feather-SQL, a new\nlightweight framework tailored for SLMs. Feather-SQL improves SQL executability\nand accuracy through 1) schema pruning and linking, 2) multi-path and\nmulti-candidate generation. Additionally, we introduce the 1+1 Model\nCollaboration Paradigm, which pairs a strong general-purpose chat model with a\nfine-tuned SQL specialist, combining strong analytical reasoning with\nhigh-precision SQL generation. Experimental results on BIRD demonstrate that\nFeather-SQL improves NL2SQL performance on SLMs, with around 10% boost for\nmodels without fine-tuning. The proposed paradigm raises the accuracy ceiling\nof SLMs to 54.76%, highlighting its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17876", "pdf": "https://arxiv.org/pdf/2503.17876", "abs": "https://arxiv.org/abs/2503.17876", "authors": ["Kaiwen Zuo", "Jing Tang", "Hanbing Qin", "Binli Luo", "Ligang He", "Shiyan Tang"], "title": "Satisfactory Medical Consultation based on Terminology-Enhanced Information Retrieval and Emotional In-Context Learning", "categories": ["cs.CL", "cs.IR"], "comment": "The 46th European Conference on Information Retrieval Workshop", "summary": "Recent advancements in Large Language Models (LLMs) have marked significant\nprogress in understanding and responding to medical inquiries. However, their\nperformance still falls short of the standards set by professional\nconsultations. This paper introduces a novel framework for medical\nconsultation, comprising two main modules: Terminology-Enhanced Information\nRetrieval (TEIR) and Emotional In-Context Learning (EICL). TEIR ensures\nimplicit reasoning through the utilization of inductive knowledge and key\nterminology retrieval, overcoming the limitations of restricted domain\nknowledge in public databases. Additionally, this module features capabilities\nfor processing long context. The EICL module aids in generating sentences with\nhigh attribute relevance by memorizing semantic and attribute information from\nunlabelled corpora and applying controlled retrieval for the required\ninformation. Furthermore, a dataset comprising 803,564 consultation records was\ncompiled in China, significantly enhancing the model's capability for complex\ndialogues and proactive inquiry initiation. Comprehensive experiments\ndemonstrate the proposed method's effectiveness in extending the context window\nlength of existing LLMs. The experimental outcomes and extensive data validate\nthe framework's superiority over five baseline models in terms of BLEU and\nROUGE performance metrics, with substantial leads in certain capabilities.\nNotably, ablation studies confirm the significance of the TEIR and EICL\ncomponents. In addition, our new framework has the potential to significantly\nimprove patient satisfaction in real clinical consulting situations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17625", "pdf": "https://arxiv.org/pdf/2503.17625", "abs": "https://arxiv.org/abs/2503.17625", "authors": ["Karol Chlasta", "Katarzyna Wisiecka", "Krzysztof Krejtz", "Izabela Krejtz"], "title": "AI-Based Screening for Depression and Social Anxiety Through Eye Tracking: An Exploratory Study", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.HC", "cs.LG", "68U01", "J.3; I.2; I.5; H.4; C.3"], "comment": "17 pages, 11 figures", "summary": "Well-being is a dynamic construct that evolves over time and fluctuates\nwithin individuals, presenting challenges for accurate quantification. Reduced\nwell-being is often linked to depression or anxiety disorders, which are\ncharacterised by biases in visual attention towards specific stimuli, such as\nhuman faces. This paper introduces a novel approach to AI-assisted screening of\naffective disorders by analysing visual attention scan paths using\nconvolutional neural networks (CNNs). Data were collected from two studies\nexamining (1) attentional tendencies in individuals diagnosed with major\ndepression and (2) social anxiety. These data were processed using residual\nCNNs through images generated from eye-gaze patterns. Experimental results,\nobtained with ResNet architectures, demonstrated an average accuracy of 48% for\na three-class system and 62% for a two-class system. Based on these exploratory\nfindings, we propose that this method could be employed in rapid, ecological,\nand effective mental health screening systems to assess well-being through\neye-tracking.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17932", "pdf": "https://arxiv.org/pdf/2503.17932", "abs": "https://arxiv.org/abs/2503.17932", "authors": ["Xunguang Wang", "Wenxuan Wang", "Zhenlan Ji", "Zongjie Li", "Pingchuan Ma", "Daoyuan Wu", "Shuai Wang"], "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "11 pages", "summary": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17936", "pdf": "https://arxiv.org/pdf/2503.17936", "abs": "https://arxiv.org/abs/2503.17936", "authors": ["Riya Naik", "Ashwin Srinivasan", "Estrid He", "Swati Agarwal"], "title": "An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language as a medium for human-computer interaction has long been\nanticipated, has been undergoing a sea-change with the advent of Large Language\nModels (LLMs) with startling capacities for processing and generating language.\nMany of us now treat LLMs as modern-day oracles, asking it almost any kind of\nquestion. Unlike its Delphic predecessor, consulting an LLM does not have to be\na single-turn activity (ask a question, receive an answer, leave); and -- also\nunlike the Pythia -- it is widely acknowledged that answers from LLMs can be\nimproved with additional context. In this paper, we aim to study when we need\nmulti-turn interactions with LLMs to successfully get a question answered; or\nconclude that a question is unanswerable. We present a neural symbolic\nframework that models the interactions between human and LLM agents. Through\nthe proposed framework, we define incompleteness and ambiguity in the questions\nas properties deducible from the messages exchanged in the interaction, and\nprovide results from benchmark problems, in which the answer-correctness is\nshown to depend on whether or not questions demonstrate the presence of\nincompleteness or ambiguity (according to the properties we identify). Our\nresults show multi-turn interactions are usually required for datasets which\nhave a high proportion of incompleteness or ambiguous questions; and that that\nincreasing interaction length has the effect of reducing incompleteness or\nambiguity. The results also suggest that our measures of incompleteness and\nambiguity can be useful tools for characterising interactions with an LLM on\nquestion-answeringproblems", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17952", "pdf": "https://arxiv.org/pdf/2503.17952", "abs": "https://arxiv.org/abs/2503.17952", "authors": ["Divyansh Singh", "Manuel Nunez Martinez", "Bonnie J. Dorr", "Sonja Schmer Galunder"], "title": "SLIDE: Sliding Localized Information for Document Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Constructing accurate knowledge graphs from long texts and low-resource\nlanguages is challenging, as large language models (LLMs) experience degraded\nperformance with longer input chunks. This problem is amplified in low-resource\nsettings where data scarcity hinders accurate entity and relationship\nextraction. Contextual retrieval methods, while improving retrieval accuracy,\nstruggle with long documents. They truncate critical information in texts\nexceeding maximum context lengths of LLMs, significantly limiting knowledge\ngraph construction. We introduce SLIDE (Sliding Localized Information for\nDocument Extraction), a chunking method that processes long documents by\ngenerating local context through overlapping windows. SLIDE ensures that\nessential contextual information is retained, enhancing knowledge graph\nextraction from documents exceeding LLM context limits. It significantly\nimproves GraphRAG performance, achieving a 24% increase in entity extraction\nand a 39% improvement in relationship extraction for English. For Afrikaans, a\nlow-resource language, SLIDE achieves a 49% increase in entity extraction and\nan 82% improvement in relationship extraction. Furthermore, it improves upon\nstate-of-the-art in question-answering metrics such as comprehensiveness,\ndiversity and empowerment, demonstrating its effectiveness in multilingual and\nresource-constrained settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17668", "pdf": "https://arxiv.org/pdf/2503.17668", "abs": "https://arxiv.org/abs/2503.17668", "authors": ["Usha Kumari", "Shuvendu Rana"], "title": "3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System", "categories": ["cs.CV"], "comment": null, "summary": "Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18008", "pdf": "https://arxiv.org/pdf/2503.18008", "abs": "https://arxiv.org/abs/2503.18008", "authors": ["Kyuyoung Kim", "Jinwoo Shin", "Jaehyung Kim"], "title": "Personalized Language Models via Privacy-Preserving Evolutionary Model Merging", "categories": ["cs.CL", "cs.NE"], "comment": "Preprint", "summary": "Personalization in large language models (LLMs) seeks to tailor models to\nindividual user or user group preferences. Prompt-based methods augment queries\nwith user preference information, whereas training-based methods directly\nencode preferences into model parameters for more effective personalization.\nDespite achieving some success in personalizing LLMs, prior methods often fail\nto directly optimize task-specific metrics and lack explicit\nprivacy-preservation mechanisms. To address these limitations, we propose\nPrivacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel\napproach to personalization that employs gradient-free methods to directly\noptimize task-specific metrics while preserving user privacy. By incorporating\nprivacy preservation into optimization, PriME produces a personalized module\nthat effectively captures the target user's preferences while minimizing the\nprivacy risks for the users sharing their private information. Experiments on\nthe LaMP benchmark show that PriME outperforms both prompt-based and\ntraining-based methods, achieving up to a 45% performance improvement over the\nprior art. Further analysis shows that PriME achieves a significantly better\nprivacy-utility trade-off, highlighting the potential of evolutionary\napproaches for privacy-preserving LLM personalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17675", "pdf": "https://arxiv.org/pdf/2503.17675", "abs": "https://arxiv.org/abs/2503.17675", "authors": ["Shulei Wang", "Wang Lin", "Hai Huang", "Hanting Wang", "Sihang Cai", "WenKang Han", "Tao Jin", "Jingyuan Chen", "Jiacheng Sun", "Jieming Zhu", "Zhou Zhao"], "title": "Towards Transformer-Based Aligned Generation with Self-Coherence Guidance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "We introduce a novel, training-free approach for enhancing alignment in\nTransformer-based Text-Guided Diffusion Models (TGDMs). Existing TGDMs often\nstruggle to generate semantically aligned images, particularly when dealing\nwith complex text prompts or multi-concept attribute binding challenges.\nPrevious U-Net-based methods primarily optimized the latent space, but their\ndirect application to Transformer-based architectures has shown limited\neffectiveness. Our method addresses these challenges by directly optimizing\ncross-attention maps during the generation process. Specifically, we introduce\nSelf-Coherence Guidance, a method that dynamically refines attention maps using\nmasks derived from previous denoising steps, ensuring precise alignment without\nadditional training. To validate our approach, we constructed more challenging\nbenchmarks for evaluating coarse-grained attribute binding, fine-grained\nattribute binding, and style binding. Experimental results demonstrate the\nsuperior performance of our method, significantly surpassing other\nstate-of-the-art methods across all evaluated tasks. Our code is available at\nhttps://scg-diffusion.github.io/scg-diffusion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18062", "pdf": "https://arxiv.org/pdf/2503.18062", "abs": "https://arxiv.org/abs/2503.18062", "authors": ["Anh Duc Nguyen", "Hieu Minh Phi", "Anh Viet Ngo", "Long Hai Trieu", "Thai Phuong Nguyen"], "title": "Investigating Recent Large Language Models for Vietnamese Machine Reading Comprehension", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable proficiency in Machine\nReading Comprehension (MRC) tasks; however, their effectiveness for\nlow-resource languages like Vietnamese remains largely unexplored. In this\npaper, we fine-tune and evaluate two state-of-the-art LLMs: Llama 3 (8B\nparameters) and Gemma (7B parameters), on ViMMRC, a Vietnamese MRC dataset. By\nutilizing Quantized Low-Rank Adaptation (QLoRA), we efficiently fine-tune these\nmodels and compare their performance against powerful LLM-based baselines.\nAlthough our fine-tuned models are smaller than GPT-3 and GPT-3.5, they\noutperform both traditional BERT-based approaches and these larger models. This\ndemonstrates the effectiveness of our fine-tuning process, showcasing how\nmodern LLMs can surpass the capabilities of older models like BERT while still\nbeing suitable for deployment in resource-constrained environments. Through\nintensive analyses, we explore various aspects of model performance, providing\nvaluable insights into adapting LLMs for low-resource languages like\nVietnamese. Our study contributes to the advancement of natural language\nprocessing in low-resource languages, and we make our fine-tuned models\npublicly available at: https://huggingface.co/iaiuet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17690", "pdf": "https://arxiv.org/pdf/2503.17690", "abs": "https://arxiv.org/abs/2503.17690", "authors": ["Ziyu Yao", "Xuxin Cheng", "Zhiqi Huang", "Lei Li"], "title": "CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Repetitive action counting, which aims to count periodic movements in a\nvideo, is valuable for video analysis applications such as fitness monitoring.\nHowever, existing methods largely rely on regression networks with limited\nrepresentational capacity, which hampers their ability to accurately capture\nvariable periodic patterns. Additionally, their supervised learning on narrow,\nlimited training sets leads to overfitting and restricts their ability to\ngeneralize across diverse scenarios. To address these challenges, we propose\nCountLLM, the first large language model (LLM)-based framework that takes video\ndata and periodic text prompts as inputs and outputs the desired counting\nvalue. CountLLM leverages the rich clues from explicit textual instructions and\nthe powerful representational capabilities of pre-trained LLMs for repetitive\naction counting. To effectively guide CountLLM, we develop a periodicity-based\nstructured template for instructions that describes the properties of\nperiodicity and implements a standardized answer format to ensure consistency.\nAdditionally, we propose a progressive multimodal training paradigm to enhance\nthe periodicity-awareness of the LLM. Empirical evaluations on widely\nrecognized benchmarks demonstrate CountLLM's superior performance and\ngeneralization, particularly in handling novel and out-of-domain actions that\ndeviate significantly from the training data, offering a promising avenue for\nrepetitive action counting.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18063", "pdf": "https://arxiv.org/pdf/2503.18063", "abs": "https://arxiv.org/abs/2503.18063", "authors": ["Pieyi Zhang", "Richong Zhang", "Zhijie Nie"], "title": "Dynamic Task Vector Grouping for Efficient Multi-Task Prompt Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Multi-task prompt tuning utilizes multiple high-resource source tasks to\nimprove performance on low-source target tasks. Existing approaches transfer\nthe soft prompt trained by combining all source tasks or a single\n``high-similar'' source task one-time-only. However, we find that the optimal\ntransfer performance often comes from a combination of source tasks, which is\nneither one nor all. Further, we find that the similarity between source and\ntarget tasks also changes dynamically during fine-tuning after transfering,\nmaking similarity calculation in the initiation stage inadequate. To address\nthese issues, we propose a method called Dynamic Task Vector Grouping (DTVG),\nwhose core ideas contain (1) measuring the task similarity with task vectors\ninstead of soft prompt, (2) grouping the optimal source task combination based\non two metrics: {\\it target similarity} and {\\it knowledge consistency}; (3)\ndynamically updating the combination in each iteration step. Extensive\nexperiments on the 26 NLP datasets under different settings demonstrate that\nDTVG effectively groups similar source tasks while reducing negative transfer,\nachieving the start-of-art performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17695", "pdf": "https://arxiv.org/pdf/2503.17695", "abs": "https://arxiv.org/abs/2503.17695", "authors": ["Yikun Ma", "Yiqing Li", "Jiawei Wu", "Zhi Jin"], "title": "MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Generative models have made remarkable advancements and are capable of\nproducing high-quality content. However, performing controllable editing with\ngenerative models remains challenging, due to their inherent uncertainty in\noutputs. This challenge is praticularly pronounced in motion editing, which\ninvolves the processing of spatial information. While some physics-based\ngenerative methods have attempted to implement motion editing, they typically\noperate on single-view images with simple motions, such as translation and\ndragging. These methods struggle to handle complex rotation and stretching\nmotions and ensure multi-view consistency, often necessitating\nresource-intensive retraining. To address these challenges, we propose\nMotionDiff, a training-free zero-shot diffusion method that leverages optical\nflow for complex multi-view motion editing. Specifically, given a static scene,\nusers can interactively select objects of interest to add motion priors. The\nproposed Point Kinematic Model (PKM) then estimates corresponding multi-view\noptical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently,\nthese optical flows are utilized to generate multi-view motion results through\ndecoupled motion representation in the Multi-view Motion Diffusion Stage\n(MMDS). Extensive experiments demonstrate that MotionDiff outperforms other\nphysics-based generative motion editing methods in achieving high-quality\nmulti-view consistent motion results. Notably, MotionDiff does not require\nretraining, enabling users to conveniently adapt it for various down-stream\ntasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17700", "pdf": "https://arxiv.org/pdf/2503.17700", "abs": "https://arxiv.org/abs/2503.17700", "authors": ["Paul Hill", "Zhiming Liu", "Nantheera Anantrasirichai"], "title": "MAMAT: 3D Mamba-Based Atmospheric Turbulence Removal and its Object Detection Capability", "categories": ["cs.CV"], "comment": null, "summary": "Restoration and enhancement are essential for improving the quality of videos\ncaptured under atmospheric turbulence conditions, aiding visualization, object\ndetection, classification, and tracking in surveillance systems. In this paper,\nwe introduce a novel Mamba-based method, the 3D Mamba-Based Atmospheric\nTurbulence Removal (MAMAT), which employs a dual-module strategy to mitigate\nthese distortions. The first module utilizes deformable 3D convolutions for\nnon-rigid registration to minimize spatial shifts, while the second module\nenhances contrast and detail. Leveraging the advanced capabilities of the 3D\nMamba architecture, experimental results demonstrate that MAMAT outperforms\nstate-of-the-art learning-based methods, achieving up to a 3\\% improvement in\nvisual quality and a 15\\% boost in object detection. It not only enhances\nvisualization but also significantly improves object detection accuracy,\nbridging the gap between visual restoration and the effectiveness of\nsurveillance applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17709", "pdf": "https://arxiv.org/pdf/2503.17709", "abs": "https://arxiv.org/abs/2503.17709", "authors": ["Yuchen Sun", "Shanhui Zhao", "Tao Yu", "Hao Wen", "Samith Va", "Mengwei Xu", "Yuanchun Li", "Chongyang Zhang"], "title": "GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "GUI agents hold significant potential to enhance the experience and\nefficiency of human-device interaction. However, current methods face\nchallenges in generalizing across applications (apps) and tasks, primarily due\nto two fundamental limitations in existing datasets. First, these datasets\noverlook developer-induced structural variations among apps, limiting the\ntransferability of knowledge across diverse software environments. Second, many\nof them focus solely on navigation tasks, which restricts their capacity to\nrepresent comprehensive software architectures and complex user interactions.\nTo address these challenges, we introduce GUI-Xplore, a dataset meticulously\ndesigned to enhance cross-application and cross-task generalization via an\nexploration-and-reasoning framework. GUI-Xplore integrates pre-recorded\nexploration videos providing contextual insights, alongside five hierarchically\nstructured downstream tasks designed to comprehensively evaluate GUI agent\ncapabilities. To fully exploit GUI-Xplore's unique features, we propose\nXplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling\nwith Graph-Guided Environment Reasoning. Further experiments indicate that\nXplore-Agent achieves a 10% improvement over existing methods in unfamiliar\nenvironments, yet there remains significant potential for further enhancement\ntowards truly generalizable GUI agents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17712", "pdf": "https://arxiv.org/pdf/2503.17712", "abs": "https://arxiv.org/abs/2503.17712", "authors": ["Heng Gao", "Zhuolin He", "Shoumeng Qiu", "Xiangyang Xue", "Jian Pu"], "title": "Multi-modality Anomaly Segmentation on the Road", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation allows autonomous driving cars to understand the\nsurroundings of the vehicle comprehensively. However, it is also crucial for\nthe model to detect obstacles that may jeopardize the safety of autonomous\ndriving systems. Based on our experiments, we find that current uni-modal\nanomaly segmentation frameworks tend to produce high anomaly scores for\nnon-anomalous regions in images. Motivated by this empirical finding, we\ndevelop a multi-modal uncertainty-based anomaly segmentation framework, named\nMMRAS+, for autonomous driving systems. MMRAS+ effectively reduces the high\nanomaly outputs of non-anomalous classes by introducing text-modal using the\nCLIP text encoder. Indeed, MMRAS+ is the first multi-modal anomaly segmentation\nsolution for autonomous driving. Moreover, we develop an ensemble module to\nfurther boost the anomaly segmentation performance. Experiments on RoadAnomaly,\nSMIYC, and Fishyscapes validation datasets demonstrate the superior performance\nof our method. The code is available in\nhttps://github.com/HengGao12/MMRAS_plus.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18089", "pdf": "https://arxiv.org/pdf/2503.18089", "abs": "https://arxiv.org/abs/2503.18089", "authors": ["Javad SeraJ", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti"], "title": "$D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Tuning large language models is essential for optimizing their performance\nacross diverse applications, particularly in scenarios with limited data\navailability. Tuning large language models in scarce data scenarios is crucial,\nparticularly given that the convergence speed of the LoRA method is lower than\nthat of full fine-tuning. In this paper, we present an analysis of\npost-training methods including Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Odds Ratio Preference Optimization (ORPO) within the\ncontext of task-specific learning using the LoRA method. Next we introduce\n$D^2LoRA$, a data-driven approach for initializing LoRA metrics that enhances\ntraining efficiency, especially in limited-data settings. Our experiments\ncompare $D^2LoRA$ with vanilla LoRA in terms of performance and catastrophic\nforgetting under extremely data-constrained conditions. The results demonstrate\nthat $D^2LoRA$ achieves a 1% improvement GSM8K benchmark and a 2-point\nimprovement in ROUGE score in title generation tasks. $D^2LoRA$ facilitates the\nadaptation of LLMs to multiple tasks even when task-specific data is scarce,\nthereby reducing training expenses and offering data cost.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17716", "pdf": "https://arxiv.org/pdf/2503.17716", "abs": "https://arxiv.org/abs/2503.17716", "authors": ["Tim Alpherts", "Sennay Ghebreab", "Nanne van Noord"], "title": "EMPLACE: Self-Supervised Urban Scene Change Detection", "categories": ["cs.CV"], "comment": "7 pages, 7 figures, published at AAAI 2025", "summary": "Urban change is a constant process that influences the perception of\nneighbourhoods and the lives of the people within them. The field of Urban\nScene Change Detection (USCD) aims to capture changes in street scenes using\ncomputer vision and can help raise awareness of changes that make it possible\nto better understand the city and its residents. Traditionally, the field of\nUSCD has used supervised methods with small scale datasets. This constrains\nmethods when applied to new cities, as it requires labour-intensive labeling\nprocesses and forces a priori definitions of relevant change. In this paper we\nintroduce AC-1M the largest USCD dataset by far of over 1.1M images, together\nwith EMPLACE, a self-supervising method to train a Vision Transformer using our\nadaptive triplet loss. We show EMPLACE outperforms SOTA methods both as a\npre-training method for linear fine-tuning as well as a zero-shot setting.\nLastly, in a case study of Amsterdam, we show that we are able to detect both\nsmall and large changes throughout the city and that changes uncovered by\nEMPLACE, depending on size, correlate with housing prices - which in turn is\nindicative of inequity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18095", "pdf": "https://arxiv.org/pdf/2503.18095", "abs": "https://arxiv.org/abs/2503.18095", "authors": ["Lorena G Barberia", "Belinda Lombard", "Norton Trevisan Roman", "Tatiane C. M. Sousa"], "title": "Clarifying Misconceptions in COVID-19 Vaccine Sentiment and Stance Analysis and Their Implications for Vaccine Hesitancy Mitigation: A Systematic Review", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.7"], "comment": "14 pages, 3 figures, 4 tables", "summary": "Background Advances in machine learning (ML) models have increased the\ncapability of researchers to detect vaccine hesitancy in social media using\nNatural Language Processing (NLP). A considerable volume of research has\nidentified the persistence of COVID-19 vaccine hesitancy in discourse shared on\nvarious social media platforms. Methods Our objective in this study was to\nconduct a systematic review of research employing sentiment analysis or stance\ndetection to study discourse towards COVID-19 vaccines and vaccination spread\non Twitter (officially known as X since 2023). Following registration in the\nPROSPERO international registry of systematic reviews, we searched papers\npublished from 1 January 2020 to 31 December 2023 that used supervised machine\nlearning to assess COVID-19 vaccine hesitancy through stance detection or\nsentiment analysis on Twitter. We categorized the studies according to a\ntaxonomy of five dimensions: tweet sample selection approach, self-reported\nstudy type, classification typology, annotation codebook definitions, and\ninterpretation of results. We analyzed if studies using stance detection report\ndifferent hesitancy trends than those using sentiment analysis by examining how\nCOVID-19 vaccine hesitancy is measured, and whether efforts were made to avoid\nmeasurement bias. Results Our review found that measurement bias is widely\nprevalent in studies employing supervised machine learning to analyze sentiment\nand stance toward COVID-19 vaccines and vaccination. The reporting errors are\nsufficiently serious that they hinder the generalisability and interpretation\nof these studies to understanding whether individual opinions communicate\nreluctance to vaccinate against SARS-CoV-2. Conclusion Improving the reporting\nof NLP methods is crucial to addressing knowledge gaps in vaccine hesitancy\ndiscourse.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17724", "pdf": "https://arxiv.org/pdf/2503.17724", "abs": "https://arxiv.org/abs/2503.17724", "authors": ["Jie Zhang", "Zhongqi Wang", "Shiguang Shan", "Xilin Chen"], "title": "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https://github.com/Robin-WZQ/IBA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17728", "pdf": "https://arxiv.org/pdf/2503.17728", "abs": "https://arxiv.org/abs/2503.17728", "authors": ["Yongjin Choi", "Chanhun Park", "Seung Jun Baek"], "title": "DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "Recent advances in text-to-image diffusion models spurred research on\npersonalization, i.e., a customized image synthesis, of subjects within\nreference images. Although existing personalization methods are able to alter\nthe subjects' positions or to personalize multiple subjects simultaneously,\nthey often struggle to modify the behaviors of subjects or their dynamic\ninteractions. The difficulty is attributable to overfitting to reference\nimages, which worsens if only a single reference image is available. We propose\nDynASyn, an effective multi-subject personalization from a single reference\nimage addressing these challenges. DynASyn preserves the subject identity in\nthe personalization process by aligning concept-based priors with subject\nappearances and actions. This is achieved by regularizing the attention maps\nbetween the subject token and images through concept-based priors. In addition,\nwe propose concept-based prompt-and-image augmentation for an enhanced\ntrade-off between identity preservation and action diversity. We adopt an\nSDE-based editing guided by augmented prompts to generate diverse appearances\nand actions while maintaining identity consistency in the augmented images.\nExperiments show that DynASyn is capable of synthesizing highly realistic\nimages of subjects with novel contexts and dynamic interactions with the\nsurroundings, and outperforms baseline methods in both quantitative and\nqualitative aspects.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17731", "pdf": "https://arxiv.org/pdf/2503.17731", "abs": "https://arxiv.org/abs/2503.17731", "authors": ["Sungphill Moon", "Hyeontae Son", "Dongcheol Hur", "Sangwook Kim"], "title": "Co-op: Correspondence-based Novel Object Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "We propose Co-op, a novel method for accurately and robustly estimating the\n6DoF pose of objects unseen during training from a single RGB image. Our method\nrequires only the CAD model of the target object and can precisely estimate its\npose without any additional fine-tuning. While existing model-based methods\nsuffer from inefficiency due to using a large number of templates, our method\nenables fast and accurate estimation with a small number of templates. This\nimprovement is achieved by finding semi-dense correspondences between the input\nimage and the pre-rendered templates. Our method achieves strong generalization\nperformance by leveraging a hybrid representation that combines patch-level\nclassification and offset regression. Additionally, our pose refinement model\nestimates probabilistic flow between the input image and the rendered image,\nrefining the initial estimate to an accurate pose using a differentiable PnP\nlayer. We demonstrate that our method not only estimates object poses rapidly\nbut also outperforms existing methods by a large margin on the seven core\ndatasets of the BOP Challenge, achieving state-of-the-art accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17752", "pdf": "https://arxiv.org/pdf/2503.17752", "abs": "https://arxiv.org/abs/2503.17752", "authors": ["R. D. Lin", "Pengcheng Weng", "Yinqiao Wang", "Han Ding", "Jinsong Han", "Fei Wang"], "title": "HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "LiDAR point cloud semantic segmentation plays a crucial role in autonomous\ndriving. In recent years, semi-supervised methods have gained popularity due to\ntheir significant reduction in annotation labor and time costs. Current\nsemi-supervised methods typically focus on point cloud spatial distribution or\nconsider short-term temporal representations, e.g., only two adjacent frames,\noften overlooking the rich long-term temporal properties inherent in autonomous\ndriving scenarios. In driving experience, we observe that nearby objects, such\nas roads and vehicles, remain stable while driving, whereas distant objects\nexhibit greater variability in category and shape. This natural phenomenon is\nalso captured by LiDAR, which reflects lower temporal sensitivity for nearby\nobjects and higher sensitivity for distant ones. To leverage these\ncharacteristics, we propose HiLoTs, which learns high-temporal sensitivity and\nlow-temporal sensitivity representations from continuous LiDAR frames. These\nrepresentations are further enhanced and fused using a cross-attention\nmechanism. Additionally, we employ a teacher-student framework to align the\nrepresentations learned by the labeled and unlabeled branches, effectively\nutilizing the large amounts of unlabeled data. Experimental results on the\nSemanticKITTI and nuScenes datasets demonstrate that our proposed HiLoTs\noutperforms state-of-the-art semi-supervised methods, and achieves performance\nclose to LiDAR+Camera multimodal approaches. Code is available on\nhttps://github.com/rdlin118/HiLoTs", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18182", "pdf": "https://arxiv.org/pdf/2503.18182", "abs": "https://arxiv.org/abs/2503.18182", "authors": ["Divya Patel", "Vansh Parikh", "Om Patel", "Agam Shah", "Bhaskar Chaudhury"], "title": "Exploring Topic Trends in COVID-19 Research Literature using Non-Negative Matrix Factorization", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we apply topic modeling using Non-Negative Matrix Factorization\n(NMF) on the COVID-19 Open Research Dataset (CORD-19) to uncover the underlying\nthematic structure and its evolution within the extensive body of COVID-19\nresearch literature. NMF factorizes the document-term matrix into two\nnon-negative matrices, effectively representing the topics and their\ndistribution across the documents. This helps us see how strongly documents\nrelate to topics and how topics relate to words. We describe the complete\nmethodology which involves a series of rigorous pre-processing steps to\nstandardize the available text data while preserving the context of phrases,\nand subsequently feature extraction using the term frequency-inverse document\nfrequency (tf-idf), which assigns weights to words based on their frequency and\nrarity in the dataset. To ensure the robustness of our topic model, we conduct\na stability analysis. This process assesses the stability scores of the NMF\ntopic model for different numbers of topics, enabling us to select the optimal\nnumber of topics for our analysis. Through our analysis, we track the evolution\nof topics over time within the CORD-19 dataset. Our findings contribute to the\nunderstanding of the knowledge structure of the COVID-19 research landscape,\nproviding a valuable resource for future research in this field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17760", "pdf": "https://arxiv.org/pdf/2503.17760", "abs": "https://arxiv.org/abs/2503.17760", "authors": ["Zeyu Liu", "Zanlin Ni", "Yeguo Hua", "Xin Deng", "Xiao Ma", "Cheng Zhong", "Gao Huang"], "title": "CODA: Repurposing Continuous VAEs for Discrete Tokenization", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://lzy-tony.github.io/coda", "summary": "Discrete visual tokenizers transform images into a sequence of tokens,\nenabling token-based visual generation akin to language models. However, this\nprocess is inherently challenging, as it requires both compressing visual\nsignals into a compact representation and discretizing them into a fixed set of\ncodes. Traditional discrete tokenizers typically learn the two tasks jointly,\noften leading to unstable training, low codebook utilization, and limited\nreconstruction quality. In this paper, we introduce\n\\textbf{CODA}(\\textbf{CO}ntinuous-to-\\textbf{D}iscrete \\textbf{A}daptation), a\nframework that decouples compression and discretization. Instead of training\ndiscrete tokenizers from scratch, CODA adapts off-the-shelf continuous VAEs --\nalready optimized for perceptual compression -- into discrete tokenizers via a\ncarefully designed discretization process. By primarily focusing on\ndiscretization, CODA ensures stable and efficient training while retaining the\nstrong visual fidelity of continuous VAEs. Empirically, with $\\mathbf{6\n\\times}$ less training budget than standard VQGAN, our approach achieves a\nremarkable codebook utilization of 100% and notable reconstruction FID (rFID)\nof $\\mathbf{0.43}$ and $\\mathbf{1.34}$ for $8 \\times$ and $16 \\times$\ncompression on ImageNet 256$\\times$ 256 benchmark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18212", "pdf": "https://arxiv.org/pdf/2503.18212", "abs": "https://arxiv.org/abs/2503.18212", "authors": ["Kanishka Parankusham", "Rodrigue Rizk", "KC Santosh"], "title": "LakotaBERT: A Transformer-based Model for Low Resource Lakota Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Lakota, a critically endangered language of the Sioux people in North\nAmerica, faces significant challenges due to declining fluency among younger\ngenerations. This paper introduces LakotaBERT, the first large language model\n(LLM) tailored for Lakota, aiming to support language revitalization efforts.\nOur research has two primary objectives: (1) to create a comprehensive Lakota\nlanguage corpus and (2) to develop a customized LLM for Lakota. We compiled a\ndiverse corpus of 105K sentences in Lakota, English, and parallel texts from\nvarious sources, such as books and websites, emphasizing the cultural\nsignificance and historical context of the Lakota language. Utilizing the\nRoBERTa architecture, we pre-trained our model and conducted comparative\nevaluations against established models such as RoBERTa, BERT, and multilingual\nBERT. Initial results demonstrate a masked language modeling accuracy of 51%\nwith a single ground truth assumption, showcasing performance comparable to\nthat of English-based models. We also evaluated the model using additional\nmetrics, such as precision and F1 score, to provide a comprehensive assessment\nof its capabilities. By integrating AI and linguistic methodologies, we aspire\nto enhance linguistic diversity and cultural resilience, setting a valuable\nprecedent for leveraging technology in the revitalization of other endangered\nindigenous languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17782", "pdf": "https://arxiv.org/pdf/2503.17782", "abs": "https://arxiv.org/abs/2503.17782", "authors": ["Hyungyu Choi", "Young Kyun Jang", "Chanho Eom"], "title": "GOAL: Global-local Object Alignment Learning", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Vision-language models like CLIP have shown impressive capabilities in\naligning images and text, but they often struggle with lengthy and detailed\ntext descriptions because of their training focus on short and concise\ncaptions. We present GOAL (Global-local Object Alignment Learning), a novel\nfine-tuning method that enhances CLIP's ability to handle lengthy text by\nleveraging both global and local semantic alignments between image and lengthy\ntext. Our approach consists of two key components: Local Image-Sentence\nMatching (LISM), which identifies corresponding pairs between image segments\nand descriptive sentences, and Token Similarity-based Learning (TSL), which\nefficiently propagates local element attention through these matched pairs.\nEvaluating GOAL on three new benchmarks for image-lengthy text retrieval, we\ndemonstrate significant improvements over baseline CLIP fine-tuning,\nestablishing a simple yet effective approach for adapting CLIP to detailed\ntextual descriptions. Through extensive experiments, we show that our method's\nfocus on local semantic alignment alongside global context leads to more\nnuanced and representative embeddings, particularly beneficial for tasks\nrequiring fine-grained understanding of lengthy text descriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18226", "pdf": "https://arxiv.org/pdf/2503.18226", "abs": "https://arxiv.org/abs/2503.18226", "authors": ["Venkatesh Bollineni", "Igor Crk", "Eren Gultepe"], "title": "Mapping Hymns and Organizing Concepts in the Rigveda: Quantitatively Connecting the Vedic Suktas", "categories": ["cs.CL"], "comment": "Accepted to NLP4DH 2025 at NAACL 2025", "summary": "Accessing and gaining insight into the Rigveda poses a non-trivial challenge\ndue to its extremely ancient Sanskrit language, poetic structure, and large\nvolume of text. By using NLP techniques, this study identified topics and\nsemantic connections of hymns within the Rigveda that were corroborated by\nseven well-known groupings of hymns. The 1,028 suktas (hymns) from the modern\nEnglish translation of the Rigveda by Jamison and Brereton were preprocessed\nand sukta-level embeddings were obtained using, i) a novel adaptation of LSA,\npresented herein, ii) SBERT, and iii) Doc2Vec embeddings. Following an UMAP\ndimension reduction of the vectors, the network of suktas was formed using\nk-nearest neighbours. Then, community detection of topics in the sukta networks\nwas performed with the Louvain, Leiden, and label propagation methods, whose\nstatistical significance of the formed topics were determined using an\nappropriate null distribution. Only the novel adaptation of LSA using the\nLeiden method, had detected sukta topic networks that were significant (z =\n2.726, p < .01) with a modularity score of 0.944. Of the seven famous sukta\ngroupings analyzed (e.g., creation, funeral, water, etc.) the LSA derived\nnetwork was successful in all seven cases, while Doc2Vec was not significant\nand failed to detect the relevant suktas. SBERT detected four of the famous\nsuktas as separate groups, but mistakenly combined three of them into a single\nmixed group. Also, the SBERT network was not statistically significant.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17788", "pdf": "https://arxiv.org/pdf/2503.17788", "abs": "https://arxiv.org/abs/2503.17788", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "title": "Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a novel framework that\nattempts to precisely align hand poses and interactions by synergistically\nintegrating foundation model-driven 2D priors with diffusion-based interaction\nrefinement for occlusion-resistant two-hand reconstruction. First, we introduce\na Fusion Alignment Encoder that learns to align fused multimodal priors\nkeypoints, segmentation maps, and depth cues from foundation models during\ntraining. This provides robust structured guidance, further enabling efficient\ninference without foundation models at test time while maintaining high\nreconstruction accuracy. Second, we employ a two-hand diffusion model\nexplicitly trained to transform interpenetrated poses into plausible,\nnon-penetrated interactions, leveraging gradient-guided denoising to correct\nartifacts and ensure realistic spatial relations. Extensive evaluations\ndemonstrate that our method achieves state-of-the-art performance on\nInterHand2.6M, FreiHAND, and HIC datasets, significantly advancing occlusion\nhandling and interaction robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17792", "pdf": "https://arxiv.org/pdf/2503.17792", "abs": "https://arxiv.org/abs/2503.17792", "authors": ["Lingyun Deng", "Litong Liu", "Dong Wang", "Xiao-Ping Wang"], "title": "Topology preserving Image segmentation using the iterative convolution-thresholding method", "categories": ["cs.CV"], "comment": "22 pages, 14 figures", "summary": "Variational models are widely used in image segmentation, with various models\ndesigned to address different types of images by optimizing specific objective\nfunctionals. However, traditional segmentation models primarily focus on the\nvisual attributes of the image, often neglecting the topological properties of\nthe target objects. This limitation can lead to segmentation results that\ndeviate from the ground truth, particularly in images with complex topological\nstructures. In this paper, we introduce a topology-preserving constraint into\nthe iterative convolution-thresholding method (ICTM), resulting in the\ntopology-preserving ICTM (TP-ICTM). Extensive experiments demonstrate that, by\nexplicitly preserving the topological properties of target objects-such as\nconnectivity-the proposed algorithm achieves enhanced accuracy and robustness,\nparticularly in images with intricate structures or noise.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18247", "pdf": "https://arxiv.org/pdf/2503.18247", "abs": "https://arxiv.org/abs/2503.18247", "authors": ["Tadesse Destaw Belay", "Israel Abebe Azime", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Abinew Ali Ayele", "Shamsuddeen Hassan Muhammad", "Seid Muhie Yimam"], "title": "AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text", "categories": ["cs.CL"], "comment": null, "summary": "Pretrained Language Models (PLMs) built from various sources are the\nfoundation of today's NLP progress. Language representations learned by such\nmodels achieve strong performance across many tasks with datasets of varying\nsizes drawn from various sources. We explore a thorough analysis of domain and\ntask adaptive continual pretraining approaches for low-resource African\nlanguages and a promising result is shown for the evaluated tasks. We create\nAfriSocial, a corpus designed for domain adaptive finetuning that passes\nthrough quality pre-processing steps. Continual pretraining PLMs using\nAfriSocial as domain adaptive pretraining (DAPT) data, consistently improves\nperformance on fine-grained emotion classification task of 16 targeted\nlanguages from 1% to 28.27% macro F1 score. Likewise, using the task adaptive\npertaining (TAPT) approach, further finetuning with small unlabeled but similar\ntask data shows promising results. For example, unlabeled sentiment data\n(source) for fine-grained emotion classification task (target) improves the\nbase model results by an F1 score ranging from 0.55% to 15.11%. Combining the\ntwo methods, DAPT + TAPT, achieves also better results than base models. All\nthe resources will be available to improve low-resource NLP tasks, generally,\nas well as other similar domain tasks such as hate speech and sentiment tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18253", "pdf": "https://arxiv.org/pdf/2503.18253", "abs": "https://arxiv.org/abs/2503.18253", "authors": ["Tadesse Destaw Belay", "Dawit Ketema Gete", "Abinew Ali Ayele", "Olga Kolesnikova", "Grigori Sidorov", "Seid Muhie Yimam"], "title": "Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian Languages", "categories": ["cs.CL"], "comment": null, "summary": "In this digital world, people freely express their emotions using different\nsocial media platforms. As a result, modeling and integrating\nemotion-understanding models are vital for various human-computer interaction\ntasks such as decision-making, product and customer feedback analysis,\npolitical promotions, marketing research, and social media monitoring. As users\nexpress different emotions simultaneously in a single instance, annotating\nemotions in a multilabel setting such as the EthioEmo (Belay et al., 2025)\ndataset effectively captures this dynamic. Additionally, incorporating\nintensity, or the degree of emotion, is crucial, as emotions can significantly\ndiffer in their expressive strength and impact. This intensity is significant\nfor assessing whether further action is necessary in decision-making processes,\nespecially concerning negative emotions in applications such as healthcare and\nmental health studies. To enhance the EthioEmo dataset, we include annotations\nfor the intensity of each labeled emotion. Furthermore, we evaluate various\nstate-of-the-art encoder-only Pretrained Language Models (PLMs) and\ndecoder-only Large Language Models (LLMs) to provide comprehensive\nbenchmarking.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17814", "pdf": "https://arxiv.org/pdf/2503.17814", "abs": "https://arxiv.org/abs/2503.17814", "authors": ["Wen Li", "Chen Liu", "Shangshu Yu", "Dunqiang Liu", "Yin Zhou", "Siqi Shen", "Chenglu Wen", "Cheng Wang"], "title": "LightLoc: Learning Outdoor LiDAR Localization at Light Speed", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene coordinate regression achieves impressive results in outdoor LiDAR\nlocalization but requires days of training. Since training needs to be repeated\nfor each new scene, long training times make these methods impractical for\ntime-sensitive applications, such as autonomous driving, drones, and robotics.\nWe identify large coverage areas and vast data in large-scale outdoor scenes as\nkey challenges that limit fast training. In this paper, we propose LightLoc,\nthe first method capable of efficiently learning localization in a new scene at\nlight speed. LightLoc introduces two novel techniques to address these\nchallenges. First, we introduce sample classification guidance to assist\nregression learning, reducing ambiguity from similar samples and improving\ntraining efficiency. Second, we propose redundant sample downsampling to remove\nwell-learned frames during training, reducing training time without\ncompromising accuracy. Additionally, the fast training and confidence\nestimation capabilities of sample classification enable its integration into\nSLAM, effectively eliminating error accumulation. Extensive experiments on\nlarge-scale outdoor datasets demonstrate that LightLoc achieves\nstate-of-the-art performance with a 50x reduction in training time than\nexisting methods. Our code is available at https://github.com/liw95/LightLoc.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18288", "pdf": "https://arxiv.org/pdf/2503.18288", "abs": "https://arxiv.org/abs/2503.18288", "authors": ["Cheng Huang", "Fan Gao", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Yongbin Yu"], "title": "Sun-Shine: A Large Language Model for Tibetan Culture", "categories": ["cs.CL"], "comment": null, "summary": "Tibetan, a minority language in China, features a highly intricate\ngrammatical structure, characterized by four verb tenses and a tense system\nwith frequent irregularities, contributing to its extensive inflectional\ndiversity. Recently, advances in Large Language Models (LLMs) have transformed\nthe paradigm in many domains. Despite the success in other fields, current LLMs\noften fall short in catering to the needs of domain experts like Tibetans, and\nthe potential of LLMs for Tibetan culture is under-explored. The intrinsic\nreasons are the immense and intricate nature of Tibetan culture as well as the\nnecessity for higher granularity and richness in knowledge. Simultaneously, the\ncomplexity and uniqueness of its grammatical structure, coupled with its status\nas a minority ethnic language, contribute to data scarcity, which remains a\nfundamental challenge. To alleviate these issues, we introduce Llama-Sunshine\n(Sun-Shine), the first large language model for Tibetan culture, which is\nexpert in various Tibetan language processing tasks. Sun-Shine incorporates\nstate-of-the-art model architectures optimized for Tibetan's linguistic\nfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverse\nTibetan texts such as literature, religious scripts, news, and conversational\ndata, which is also the first large-scale dataset for Tibetan culture. Though\ncomprehensive experiments, Sun-Shine not only demonstrates a higher level of\nknowledge expertise for Tibetan culture but also gains preliminary embodied\nintelligence capabilities in Tibetan language processing tasks, like language\nmodeling, text classification, machine translation, and syntactic analysis.\nMoreover, it excels in low-resource scenarios, showcasing strong generalization\ncapabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18360", "pdf": "https://arxiv.org/pdf/2503.18360", "abs": "https://arxiv.org/abs/2503.18360", "authors": ["Yiran Hu", "Huanghai Liu", "Qingjing Chen", "Ning Zheng", "Chong Wang", "Yun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "J&H: Evaluating the Robustness of Large Language Models Under Knowledge-Injection Attacks in Legal Domain", "categories": ["cs.CL"], "comment": "10 pages, 5 figures", "summary": "As the scale and capabilities of Large Language Models (LLMs) increase, their\napplications in knowledge-intensive fields such as legal domain have garnered\nwidespread attention. However, it remains doubtful whether these LLMs make\njudgments based on domain knowledge for reasoning. If LLMs base their judgments\nsolely on specific words or patterns, rather than on the underlying logic of\nthe language, the ''LLM-as-judges'' paradigm poses substantial risks in the\nreal-world applications. To address this question, we propose a method of legal\nknowledge injection attacks for robustness testing, thereby inferring whether\nLLMs have learned legal knowledge and reasoning logic. In this paper, we\npropose J&H: an evaluation framework for detecting the robustness of LLMs under\nknowledge injection attacks in the legal domain. The aim of the framework is to\nexplore whether LLMs perform deductive reasoning when accomplishing legal\ntasks. To further this aim, we have attacked each part of the reasoning logic\nunderlying these tasks (major premise, minor premise, and conclusion\ngeneration). We have collected mistakes that legal experts might make in\njudicial decisions in the real world, such as typos, legal synonyms, inaccurate\nexternal legal statutes retrieval. However, in real legal practice, legal\nexperts tend to overlook these mistakes and make judgments based on logic.\nHowever, when faced with these errors, LLMs are likely to be misled by\ntypographical errors and may not utilize logic in their judgments. We conducted\nknowledge injection attacks on existing general and domain-specific LLMs.\nCurrent LLMs are not robust against the attacks employed in our experiments. In\naddition we propose and compare several methods to enhance the knowledge\nrobustness of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18471", "pdf": "https://arxiv.org/pdf/2503.18471", "abs": "https://arxiv.org/abs/2503.18471", "authors": ["Calvin Bao", "Yow-Ting Shiue", "Marine Carpuat", "Joel Chan"], "title": "Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "26 pages, 8 tables, 6 figures", "summary": "Scholars often explore literature outside of their home community of study.\nThis exploration process is frequently hampered by field-specific jargon. Past\ncomputational work often focuses on supporting translation work by removing\njargon through simplification and summarization; here, we explore a different\napproach that preserves jargon as useful bridges to new conceptual spaces.\nSpecifically, we cast different scholarly domains as different language-using\ncommunities, and explore how to adapt techniques from unsupervised\ncross-lingual alignment of word embeddings to explore conceptual alignments\nbetween domain-specific word embedding spaces.We developed a prototype\ncross-domain search engine that uses aligned domain-specific embeddings to\nsupport conceptual exploration, and tested this prototype in two case studies.\nWe discuss qualitative insights into the promises and pitfalls of this approach\nto translation work, and suggest design insights for future interfaces that\nprovide computational support for cross-domain information seeking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17934", "pdf": "https://arxiv.org/pdf/2503.17934", "abs": "https://arxiv.org/abs/2503.17934", "authors": ["Xuewei Chen", "Zhimin Chen", "Yiren Song"], "title": "TransAnimate: Taming Layer Diffusion to Generate RGBA Video", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have made remarkable advancements in recent\nyears. However, generating RGBA videos with alpha channels for transparency and\nvisual effects remains a significant challenge due to the scarcity of suitable\ndatasets and the complexity of adapting existing models for this purpose. To\naddress these limitations, we present TransAnimate, an innovative framework\nthat integrates RGBA image generation techniques with video generation modules,\nenabling the creation of dynamic and transparent videos. TransAnimate\nefficiently leverages pre-trained text-to-transparent image model weights and\ncombines them with temporal models and controllability plugins trained on RGB\nvideos, adapting them for controllable RGBA video generation tasks.\nAdditionally, we introduce an interactive motion-guided control mechanism,\nwhere directional arrows define movement and colors adjust scaling, offering\nprecise and intuitive control for designing game effects. To further alleviate\ndata scarcity, we have developed a pipeline for creating an RGBA video dataset,\nincorporating high-quality game effect videos, extracted foreground objects,\nand synthetic transparent videos. Comprehensive experiments demonstrate that\nTransAnimate generates high-quality RGBA videos, establishing it as a practical\nand effective tool for applications in gaming and visual effects.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18594", "pdf": "https://arxiv.org/pdf/2503.18594", "abs": "https://arxiv.org/abs/2503.18594", "authors": ["Guillem García Subies", "Álvaro Barbero Jiménez", "Paloma Martínez Fernández"], "title": "ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel contribution to Spanish clinical natural language\nprocessing by introducing the largest publicly available clinical corpus,\nClinText-SP, along with a state-of-the-art clinical encoder language model,\nRigoBERTa Clinical. Our corpus was meticulously curated from diverse open\nsources, including clinical cases from medical journals and annotated corpora\nfrom shared tasks, providing a rich and diverse dataset that was previously\ndifficult to access. RigoBERTa Clinical, developed through domain-adaptive\npretraining on this comprehensive dataset, significantly outperforms existing\nmodels on multiple clinical NLP benchmarks. By publicly releasing both the\ndataset and the model, we aim to empower the research community with robust\nresources that can drive further advancements in clinical NLP and ultimately\ncontribute to improved healthcare applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18596", "pdf": "https://arxiv.org/pdf/2503.18596", "abs": "https://arxiv.org/abs/2503.18596", "authors": ["Yihan Wang", "Peiyu Liu", "Xin Yang"], "title": "LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Schema linking is a critical bottleneck in achieving human-level performance\nin Text-to-SQL tasks, particularly in real-world large-scale multi-database\nscenarios. Addressing schema linking faces two major challenges: (1) Database\nRetrieval: selecting the correct database from a large schema pool in\nmulti-database settings, while filtering out irrelevant ones. (2) Schema Item\nGrounding: accurately identifying the relevant tables and columns from within a\nlarge and redundant schema for SQL generation. To address this, we introduce\nLinkAlign, a novel framework that can effectively adapt existing baselines to\nreal-world environments by systematically addressing schema linking. Our\nframework comprises three key steps: multi-round semantic enhanced retrieval\nand irrelevant information isolation for Challenge 1, and schema extraction\nenhancement for Challenge 2. We evaluate our method performance of schema\nlinking on the SPIDER and BIRD benchmarks, and the ability to adapt existing\nText-to-SQL models to real-world environments on the SPIDER 2.0-lite benchmark.\nExperiments show that LinkAlign outperforms existing baselines in\nmulti-database settings, demonstrating its effectiveness and robustness. On the\nother hand, our method ranks highest among models excluding those using long\nchain-of-thought reasoning LLMs. This work bridges the gap between current\nresearch and real-world scenarios, providing a practical solution for robust\nand scalable schema linking. The codes are available at\nhttps://github.com/Satissss/LinkAlign.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17982", "pdf": "https://arxiv.org/pdf/2503.17982", "abs": "https://arxiv.org/abs/2503.17982", "authors": ["Yara AlaaEldin", "Francesca Odone"], "title": "Co-SemDepth: Fast Joint Semantic Segmentation and Depth Estimation on Aerial Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the geometric and semantic properties of the scene is crucial\nin autonomous navigation and particularly challenging in the case of Unmanned\nAerial Vehicle (UAV) navigation. Such information may be by obtained by\nestimating depth and semantic segmentation maps of the surrounding environment\nand for their practical use in autonomous navigation, the procedure must be\nperformed as close to real-time as possible. In this paper, we leverage\nmonocular cameras on aerial robots to predict depth and semantic maps in\nlow-altitude unstructured environments. We propose a joint deep-learning\narchitecture that can perform the two tasks accurately and rapidly, and\nvalidate its effectiveness on MidAir and Aeroscapes benchmark datasets. Our\njoint-architecture proves to be competitive or superior to the other single and\njoint architecture methods while performing its task fast predicting 20.2 FPS\non a single NVIDIA quadro p5000 GPU and it has a low memory footprint. All\ncodes for training and prediction can be found on this link:\nhttps://github.com/Malga-Vision/Co-SemDepth", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17992", "pdf": "https://arxiv.org/pdf/2503.17992", "abs": "https://arxiv.org/abs/2503.17992", "authors": ["Xueying Liu", "Lianfang Wang", "Jun Liu", "Yong Wang", "Yuping Duan"], "title": "Geometric Constrained Non-Line-of-Sight Imaging", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Normal reconstruction is crucial in non-line-of-sight (NLOS) imaging, as it\nprovides key geometric and lighting information about hidden objects, which\nsignificantly improves reconstruction accuracy and scene understanding.\nHowever, jointly estimating normals and albedo expands the problem from\nmatrix-valued functions to tensor-valued functions that substantially\nincreasing complexity and computational difficulty. In this paper, we propose a\nnovel joint albedo-surface reconstruction method, which utilizes the Frobenius\nnorm of the shape operator to control the variation rate of the normal field.\nIt is the first attempt to apply regularization methods to the reconstruction\nof surface normals for hidden objects. By improving the accuracy of the normal\nfield, it enhances detail representation and achieves high-precision\nreconstruction of hidden object geometry. The proposed method demonstrates\nrobustness and effectiveness on both synthetic and experimental datasets. On\ntransient data captured within 15 seconds, our surface normal-regularized\nreconstruction model produces more accurate surfaces than recently proposed\nmethods and is 30 times faster than the existing surface reconstruction\napproach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18769", "pdf": "https://arxiv.org/pdf/2503.18769", "abs": "https://arxiv.org/abs/2503.18769", "authors": ["Alan Dao", "Dinh Bach Vu", "Bui Quang Huy"], "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning", "categories": ["cs.CL", "cs.RO"], "comment": null, "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the\nspatial reasoning capabilities of large language models (LLMs) for 3D Cartesian\nspace navigation. AlphaSpace employs a semantics-based tokenization strategy,\nencoding height information through specialized semantic tokens, and integrates\nprimarily symbolic synthetic reasoning data. This approach enables LLMs to\naccurately manipulate objects by positioning them at specific [x, y, z]\ncoordinates. Experimental results demonstrate that AlphaSpace significantly\noutperforms existing models on manipulation subtasks, achieving a total\naccuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5\nSonnet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18010", "pdf": "https://arxiv.org/pdf/2503.18010", "abs": "https://arxiv.org/abs/2503.18010", "authors": ["Thomas Dagès", "Simon Weber", "Ya-Wei Eileen Lin", "Ronen Talmon", "Daniel Cremers", "Michael Lindenbaum", "Alfred M. Bruckstein", "Ron Kimmel"], "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding", "categories": ["cs.CV"], "comment": "Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025", "summary": "Dimensionality reduction is a fundamental task that aims to simplify complex\ndata by reducing its feature dimensionality while preserving essential\npatterns, with core applications in data analysis and visualisation. To\npreserve the underlying data structure, multi-dimensional scaling (MDS) methods\nfocus on preserving pairwise dissimilarities, such as distances. They optimise\nthe embedding to have pairwise distances as close as possible to the data\ndissimilarities. However, the current standard is limited to embedding data in\nRiemannian manifolds. Motivated by the lack of asymmetry in the Riemannian\nmetric of the embedding space, this paper extends the MDS problem to a natural\nasymmetric generalisation of Riemannian manifolds called Finsler manifolds.\nInspired by Euclidean space, we define a canonical Finsler space for embedding\nasymmetric data. Due to its simplicity with respect to geodesics, data\nrepresentation in this space is both intuitive and simple to analyse. We\ndemonstrate that our generalisation benefits from the same theoretical\nconvergence guarantees. We reveal the effectiveness of our Finsler embedding\nacross various types of non-symmetric data, highlighting its value in\napplications such as data visualisation, dimensionality reduction, directed\ngraph embedding, and link prediction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18878", "pdf": "https://arxiv.org/pdf/2503.18878", "abs": "https://arxiv.org/abs/2503.18878", "authors": ["Andrey Galichin", "Alexey Dontsov", "Polina Druzhinina", "Anton Razzhigaev", "Oleg Y. Rogov", "Elena Tutubalina", "Ivan Oseledets"], "title": "I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing. Recent advances have led to the developing of a new class\nof reasoning LLMs; for example, open-source DeepSeek-R1 has achieved\nstate-of-the-art performance by integrating deep thinking and complex\nreasoning. Despite these impressive capabilities, the internal reasoning\nmechanisms of such models remain unexplored. In this work, we employ Sparse\nAutoencoders (SAEs), a method to learn a sparse decomposition of latent\nrepresentations of a neural network into interpretable features, to identify\nfeatures that drive reasoning in the DeepSeek-R1 series of models. First, we\npropose an approach to extract candidate ''reasoning features'' from SAE\nrepresentations. We validate these features through empirical analysis and\ninterpretability methods, demonstrating their direct correlation with the\nmodel's reasoning abilities. Crucially, we demonstrate that steering these\nfeatures systematically enhances reasoning performance, offering the first\nmechanistic account of reasoning in LLMs. Code available at\nhttps://github.com/AIRI-Institute/SAE-Reasoning", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18891", "pdf": "https://arxiv.org/pdf/2503.18891", "abs": "https://arxiv.org/abs/2503.18891", "authors": ["Zhexuan Wang", "Yutong Wang", "Xuebo Liu", "Liang Ding", "Miao Zhang", "Jie Liu", "Min Zhang"], "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have\ndemonstrated significant potential in collaborative problem-solving. However,\nthey still face substantial challenges of low communication efficiency and\nsuboptimal task performance, making the careful design of the agents'\ncommunication topologies particularly important. Inspired by the management\ntheory that roles in an efficient team are often dynamically adjusted, we\npropose AgentDropout, which identifies redundant agents and communication\nacross different communication rounds by optimizing the adjacency matrices of\nthe communication graphs and eliminates them to enhance both token efficiency\nand task performance. Compared to state-of-the-art methods, AgentDropout\nachieves an average reduction of 21.6% in prompt token consumption and 18.4% in\ncompletion token consumption, along with a performance improvement of 1.14 on\nthe tasks. Furthermore, the extended experiments demonstrate that AgentDropout\nachieves notable domain transferability and structure robustness, revealing its\nreliability and effectiveness. We release our code at\nhttps://github.com/wangzx1219/AgentDropout.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034", "abs": "https://arxiv.org/abs/2503.18034", "authors": ["Qiao Liang", "Yanjiang Liu", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of the\nvision encoder's prior knowledge on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient--particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18035", "pdf": "https://arxiv.org/pdf/2503.18035", "abs": "https://arxiv.org/abs/2503.18035", "authors": ["Tianyi Shang", "Zhenyu Li", "Pengjie Xu", "Zhaojun Deng", "Ruirui Zhang"], "title": "Text-Driven Cross-Modal Place Recognition Method for Remote Sensing Localization", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Environment description-based localization in large-scale point cloud maps\nconstructed through remote sensing is critically significant for the\nadvancement of large-scale autonomous systems, such as delivery robots\noperating in the last mile. However, current approaches encounter challenges\ndue to the inability of point cloud encoders to effectively capture local\ndetails and long-range spatial relationships, as well as a significant modality\ngap between text and point cloud representations. To address these challenges,\nwe present Des4Pos, a novel two-stage text-driven remote sensing localization\nframework. In the coarse stage, the point-cloud encoder utilizes the\nMulti-scale Fusion Attention Mechanism (MFAM) to enhance local geometric\nfeatures, followed by a bidirectional Long Short-Term Memory (LSTM) module to\nstrengthen global spatial relationships. Concurrently, the Stepped Text Encoder\n(STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and\npoint-cloud features using this prior knowledge, effectively bridging modality\ndiscrepancies. In the fine stage, we introduce a Cascaded Residual Attention\n(CRA) module to fuse cross-modal features and predict relative localization\noffsets, thereby achieving greater localization precision. Experiments on the\nKITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art\nperformance in text-to-point-cloud place recognition. Specifically, it attains\na top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius\nthreshold, surpassing the best existing methods by 7% and 7%, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17421", "pdf": "https://arxiv.org/pdf/2503.17421", "abs": "https://arxiv.org/abs/2503.17421", "authors": ["Junwei Kuang", "Liang Yang", "Shaoze Cui", "Weiguo Fan"], "title": "Understanding Social Support Needs in Questions: A Hybrid Approach Integrating Semi-Supervised Learning and LLM-based Data Augmentation", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "55 pages", "summary": "Patients are increasingly turning to online health Q&A communities for social\nsupport to improve their well-being. However, when this support received does\nnot align with their specific needs, it may prove ineffective or even\ndetrimental. This necessitates a model capable of identifying the social\nsupport needs in questions. However, training such a model is challenging due\nto the scarcity and class imbalance issues of labeled data. To overcome these\nchallenges, we follow the computational design science paradigm to develop a\nnovel framework, Hybrid Approach for SOcial Support need classification\n(HA-SOS). HA-SOS integrates an answer-enhanced semi-supervised learning\napproach, a text data augmentation technique leveraging large language models\n(LLMs) with reliability- and diversity-aware sample selection mechanism, and a\nunified training process to automatically label social support needs in\nquestions. Extensive empirical evaluations demonstrate that HA-SOS\nsignificantly outperforms existing question classification models and\nalternative semi-supervised learning approaches. This research contributes to\nthe literature on social support, question classification, semi-supervised\nlearning, and text data augmentation. In practice, our HA-SOS framework\nfacilitates online Q&A platform managers and answerers to better understand\nusers' social support needs, enabling them to provide timely, personalized\nanswers and interventions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18042", "pdf": "https://arxiv.org/pdf/2503.18042", "abs": "https://arxiv.org/abs/2503.18042", "authors": ["Qiang Wang", "Yuhang He", "SongLin Dong", "Xiang Song", "Jizhou Han", "Haoyu Luo", "Yihong Gong"], "title": "DualCP: Rehearsal-Free Domain-Incremental Learning via Dual-Level Concept Prototype", "categories": ["cs.CV"], "comment": "Accepted at AAAI 2025", "summary": "Domain-Incremental Learning (DIL) enables vision models to adapt to changing\nconditions in real-world environments while maintaining the knowledge acquired\nfrom previous domains. Given privacy concerns and training time, Rehearsal-Free\nDIL (RFDIL) is more practical. Inspired by the incremental cognitive process of\nthe human brain, we design Dual-level Concept Prototypes (DualCP) for each\nclass to address the conflict between learning new knowledge and retaining old\nknowledge in RFDIL. To construct DualCP, we propose a Concept Prototype\nGenerator (CPG) that generates both coarse-grained and fine-grained prototypes\nfor each class. Additionally, we introduce a Coarse-to-Fine calibrator (C2F) to\nalign image features with DualCP. Finally, we propose a Dual Dot-Regression\n(DDR) loss function to optimize our C2F module. Extensive experiments on the\nDomainNet, CDDB, and CORe50 datasets demonstrate the effectiveness of our\nmethod.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18052", "pdf": "https://arxiv.org/pdf/2503.18052", "abs": "https://arxiv.org/abs/2503.18052", "authors": ["Yue Li", "Qi Ma", "Runyi Yang", "Huapeng Li", "Mengjiao Ma", "Bin Ren", "Nikola Popovic", "Nicu Sebe", "Ender Konukoglu", "Theo Gevers", "Luc Van Gool", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining", "categories": ["cs.CV"], "comment": "Our code, model, and dataset will be released at\n  https://github.com/unique1i/SceneSplat", "summary": "Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training, or together at\ninference. This highlights a clear absence of a model capable of processing 3D\ndata alone for learning semantics end-to-end, along with the necessary data to\ntrain such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as the\nde facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable fashion remains an open challenge. To address these limitations\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. In order to power the proposed methods, we introduce\nSceneSplat-7K, the first large-scale 3DGS dataset for indoor scenes, comprising\nof 6868 scenes derived from 7 established datasets like ScanNet, Matterport3D,\netc. Generating SceneSplat-7K required computational resources equivalent to\n119 GPU-days on an L4 GPU, enabling standardized benchmarking for 3DGS-based\nreasoning for indoor scenes. Our exhaustive experiments on SceneSplat-7K\ndemonstrate the significant benefit of the proposed methods over the\nestablished baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17502", "pdf": "https://arxiv.org/pdf/2503.17502", "abs": "https://arxiv.org/abs/2503.17502", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Roozbeh Razavi-Far"], "title": "Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) and transformer-based architectures are\nincreasingly utilized for source code analysis. As software systems grow in\ncomplexity, integrating LLMs into code analysis workflows becomes essential for\nenhancing efficiency, accuracy, and automation. This paper explores the role of\nLLMs for different code analysis tasks, focusing on three key aspects: 1) what\nthey can analyze and their applications, 2) what models are used and 3) what\ndatasets are used, and the challenges they face. Regarding the goal of this\nresearch, we investigate scholarly articles that explore the use of LLMs for\nsource code analysis to uncover research developments, current trends, and the\nintellectual structure of this emerging field. Additionally, we summarize\nlimitations and highlight essential tools, datasets, and key challenges, which\ncould be valuable for future work.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18065", "pdf": "https://arxiv.org/pdf/2503.18065", "abs": "https://arxiv.org/abs/2503.18065", "authors": ["Ziming Wei", "Bingqian Lin", "Yunshuang Nie", "Jiaqi Chen", "Shikui Ma", "Hang Xu", "Xiaodan Liang"], "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18073", "pdf": "https://arxiv.org/pdf/2503.18073", "abs": "https://arxiv.org/abs/2503.18073", "authors": ["Yuxuan Xie", "Xuan Yu", "Changjian Jiang", "Sitong Mao", "Shunbo Zhou", "Rui Fan", "Rong Xiong", "Yue Wang"], "title": "PanopticSplatting: End-to-End Panoptic Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17632", "pdf": "https://arxiv.org/pdf/2503.17632", "abs": "https://arxiv.org/abs/2503.17632", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "FairFlow: Mitigating Dataset Biases through Undecided Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2024", "summary": "Language models are prone to dataset biases, known as shortcuts and spurious\ncorrelations in data, which often result in performance drop on new data. We\npresent a new debiasing framework called ``FairFlow'' that mitigates dataset\nbiases by learning to be undecided in its predictions for data samples or\nrepresentations associated with known or unknown biases. The framework\nintroduces two key components: a suite of data and model perturbation\noperations that generate different biased views of input samples, and a\ncontrastive objective that learns debiased and robust representations from the\nresulting biased views of samples. Experiments show that FairFlow outperforms\nexisting debiasing methods, particularly against out-of-domain and hard test\nsamples without compromising the in-domain performance", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17793", "pdf": "https://arxiv.org/pdf/2503.17793", "abs": "https://arxiv.org/abs/2503.17793", "authors": ["Codefuse", "Ling Team", ":", "Wenting Cai", "Yuchen Cao", "Chaoyu Chen", "Chen Chen", "Siba Chen", "Qing Cui", "Peng Di", "Junpeng Fang", "Zi Gong", "Ting Guo", "Zhengyu He", "Yang Huang", "Cong Li", "Jianguo Li", "Zheng Li", "Shijie Lian", "BingChang Liu", "Songshan Luo", "Shuo Mao", "Min Shen", "Jian Wu", "Jiaolong Yang", "Wenjie Yang", "Tong Ye", "Hang Yu", "Wei Zhang", "Zhenduo Zhang", "Hailin Zhao", "Xunjin Zheng", "Jun Zhou"], "title": "Every Sample Matters: Leveraging Mixture-of-Experts and High-Quality Data for Efficient and Accurate Code LLM", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "20 pages, 6 figures", "summary": "Recent advancements in code large language models (LLMs) have demonstrated\nremarkable capabilities in code generation and understanding. It is still\nchallenging to build a code LLM with comprehensive performance yet ultimate\nefficiency. Many attempts have been released in the open source community to\nbreak the trade-off between performance and efficiency, such as the Qwen Coder\nseries and the DeepSeek Coder series. This paper introduces yet another attempt\nin this area, namely Ling-Coder-Lite. We leverage the efficient\nMixture-of-Experts (MoE) architecture along with a set of high-quality data\ncuration methods (especially those based on program analytics) to build an\nefficient yet powerful code LLM. Ling-Coder-Lite exhibits on-par performance on\n12 representative coding benchmarks compared to state-of-the-art models of\nsimilar size, such as Qwen2.5-Coder-7B and DeepSeek-Coder-V2-Lite, while\noffering competitive latency and throughput. In practice, we achieve a 50\\%\nreduction in deployment resources compared to the similar-sized dense model\nwithout performance loss. To facilitate further research and development in\nthis area, we open-source our models as well as a substantial portion of\nhigh-quality data for the annealing and post-training stages. The models and\ndata can be accessed\nat~\\url{https://huggingface.co/inclusionAI/Ling-Coder-lite}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18034", "pdf": "https://arxiv.org/pdf/2503.18034", "abs": "https://arxiv.org/abs/2503.18034", "authors": ["Qiao Liang", "Yanjiang Liu", "Ben He", "Yaojie Lu", "Hongyu Lin", "Jia Zheng", "Xianpei Han", "Le Sun", "Yingfei Sun"], "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Does the prior knowledge of the vision encoder constrain the capability\nboundary of Multi-modal Large Language Models (MLLMs)? While most existing\nresearch treats MLLMs as unified systems optimized through end-to-end training,\nthe impact of vision encoder's prior knowledge is seldom investigated. In this\nwork, we introduce a novel metric, $Rank_e$, to quantify the effect of the\nvision encoder's prior knowledge on MLLM performance. Our analysis reveals a\npositive correlation between prior knowledge and MLLM performance. Moreover, we\nfind that domain-specific fine-tuning using solely end-to-end visual question\nanswering (VQA) data is insufficient--particularly for entities with low\ninherent visual prior knowledge. To address this issue, we propose VisPRE\n(Vision Prior Remediation), a two-stage training framework that explicitly\nincorporates prior knowledge at the vision encoder level. Experimental results\ndemonstrate that augmenting vision encoder's prior knowledge substantially\nboosts the visual understanding capabilities of MLLMs, offering a novel and\neffective strategy for improving performance, especially in scenarios involving\nuncommon visual entities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18135", "pdf": "https://arxiv.org/pdf/2503.18135", "abs": "https://arxiv.org/abs/2503.18135", "authors": ["Jiaxin Huang", "Runnan Chen", "Ziwen Li", "Zhengqing Gao", "Xiao He", "Yandong Guo", "Mingming Gong", "Tongliang Liu"], "title": "MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning segmentation aims to segment target objects in complex scenes based\non human intent and spatial reasoning. While recent multimodal large language\nmodels (MLLMs) have demonstrated impressive 2D image reasoning segmentation,\nadapting these capabilities to 3D scenes remains underexplored. In this paper,\nwe introduce MLLM-For3D, a simple yet effective framework that transfers\nknowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize\nMLLMs to generate multi-view pseudo segmentation masks and corresponding text\nembeddings, then unproject 2D masks into 3D space and align them with the text\nembeddings. The primary challenge lies in the absence of 3D context and spatial\nconsistency across multiple views, causing the model to hallucinate objects\nthat do not exist and fail to target objects consistently. Training the 3D\nmodel with such irrelevant objects leads to performance degradation. To address\nthis, we introduce a spatial consistency strategy to enforce that segmentation\nmasks remain coherent in the 3D space, effectively capturing the geometry of\nthe scene. Moreover, we develop a Token-for-Query approach for multimodal\nsemantic alignment, enabling consistent identification of the same object\nacross different views. Extensive evaluations on various challenging indoor\nscene benchmarks demonstrate that, even without any labeled 3D training data,\nMLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively\ninterpreting user intent, understanding 3D scenes, and reasoning about spatial\nrelationships.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18065", "pdf": "https://arxiv.org/pdf/2503.18065", "abs": "https://arxiv.org/abs/2503.18065", "authors": ["Ziming Wei", "Bingqian Lin", "Yunshuang Nie", "Jiaqi Chen", "Shikui Ma", "Hang Xu", "Xiaodan Liang"], "title": "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Data scarcity is a long-standing challenge in the Vision-Language Navigation\n(VLN) field, which extremely hinders the generalization of agents to unseen\nenvironments. Previous works primarily rely on additional simulator data or\nweb-collected images/videos to improve the generalization. However, the\nsimulator environments still face limited diversity, and the web-collected data\noften requires extensive labor to remove the noise. In this paper, we propose a\nRewriting-driven AugMentation (RAM) paradigm for VLN, which directly creates\nthe unseen observation-instruction pairs via rewriting human-annotated training\ndata. Benefiting from our rewriting mechanism, new observation-instruction can\nbe obtained in both simulator-free and labor-saving manners to promote\ngeneralization. Specifically, we first introduce Object-Enriched Observation\nRewriting, where we combine Vision-Language Models (VLMs) and Large Language\nModels (LLMs) to derive rewritten object-enriched scene descriptions, enabling\nobservation synthesis with diverse objects and spatial layouts via\nText-to-Image Generation Models (T2IMs). Then, we propose Observation-Contrast\nInstruction Rewriting, which generates observation-aligned rewritten\ninstructions by requiring LLMs to reason the difference between original and\nnew observations. We further develop a mixing-then-focusing training strategy\nwith a random observation cropping scheme, effectively enhancing data\ndistribution diversity while suppressing augmentation data noise during\ntraining. Experiments on both the discrete environments (R2R, REVERIE, and R4R\ndatasets) and continuous environments (R2R-CE dataset) show the superior\nperformance and impressive generalization ability of our method. Code is\navailable at https://github.com/SaDil13/VLN-RAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18102", "pdf": "https://arxiv.org/pdf/2503.18102", "abs": "https://arxiv.org/abs/2503.18102", "authors": ["Samuel Schmidgall", "Michael Moor"], "title": "AgentRxiv: Towards Collaborative Autonomous Research", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Progress in scientific discovery is rarely the result of a single \"Eureka\"\nmoment, but is rather the product of hundreds of scientists incrementally\nworking together toward a common goal. While existing agent workflows are\ncapable of producing research autonomously, they do so in isolation, without\nthe ability to continuously improve upon prior research results. To address\nthese challenges, we introduce AgentRxiv-a framework that lets LLM agent\nlaboratories upload and retrieve reports from a shared preprint server in order\nto collaborate, share insights, and iteratively build on each other's research.\nWe task agent laboratories to develop new reasoning and prompting techniques\nand find that agents with access to their prior research achieve higher\nperformance improvements compared to agents operating in isolation (11.4%\nrelative improvement over baseline on MATH-500). We find that the best\nperforming strategy generalizes to benchmarks in other domains (improving on\naverage by 3.3%). Multiple agent laboratories sharing research through\nAgentRxiv are able to work together towards a common goal, progressing more\nrapidly than isolated laboratories, achieving higher overall accuracy (13.7%\nrelative improvement over baseline on MATH-500). These findings suggest that\nautonomous agents may play a role in designing future AI systems alongside\nhumans. We hope that AgentRxiv allows agents to collaborate toward research\ngoals and enables researchers to accelerate discovery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18320", "pdf": "https://arxiv.org/pdf/2503.18320", "abs": "https://arxiv.org/abs/2503.18320", "authors": ["Dong Jing", "Nanyi Fei", "Zhiwu Lu"], "title": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18150", "pdf": "https://arxiv.org/pdf/2503.18150", "abs": "https://arxiv.org/abs/2503.18150", "authors": ["Zhuoling Li", "Hossein Rahmani", "Qiuhong Ke", "Jun Liu"], "title": "LongDiff: Training-Free Long Video Generation in One Go", "categories": ["cs.CV"], "comment": null, "summary": "Video diffusion models have recently achieved remarkable results in video\ngeneration. Despite their encouraging performance, most of these models are\nmainly designed and trained for short video generation, leading to challenges\nin maintaining temporal consistency and visual details in long video\ngeneration. In this paper, we propose LongDiff, a novel training-free method\nconsisting of carefully designed components \\ -- Position Mapping (PM) and\nInformative Frame Selection (IFS) \\ -- to tackle two key challenges that hinder\nshort-to-long video generation generalization: temporal position ambiguity and\ninformation dilution. Our LongDiff unlocks the potential of off-the-shelf video\ndiffusion models to achieve high-quality long video generation in one go.\nExtensive experiments demonstrate the efficacy of our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18394", "pdf": "https://arxiv.org/pdf/2503.18394", "abs": "https://arxiv.org/abs/2503.18394", "authors": ["Kun Li", "Xinwei Chen", "Tianyou Song", "Chengrui Zhou", "Zhuoran Liu", "Zhenyan Zhang", "Jiangjian Guo", "Qing Shan"], "title": "Solving Situation Puzzles with Large Language Model and External Reformulation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have shown an impressive\nability to perform arithmetic and symbolic reasoning tasks. However, we found\nthat LLMs (e.g., ChatGPT) cannot perform well on reasoning that requires\nmultiple rounds of dialogue, especially when solving situation puzzles.\nSpecifically, LLMs intend to ask very detailed questions focusing on a specific\naspect or same/similar questions after several rounds of Q&As. To help LLMs get\nout of the above dilemma, we propose a novel external reformulation\nmethodology, where the situation puzzle will be reformulated after several\nrounds of Q&A or when the LLMs raise an incorrect guess. Experiments show\nsuperior performance (e.g., win rate, number of question/guess attempts) of our\nmethod than directly using LLMs for solving situation puzzles, highlighting the\npotential of strategic problem reformulation to enhance the reasoning\ncapabilities of LLMs in complex interactive scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18435", "pdf": "https://arxiv.org/pdf/2503.18435", "abs": "https://arxiv.org/abs/2503.18435", "authors": ["Junteng Liu", "Weihao Zeng", "Xiwen Zhang", "Yijun Wang", "Zifei Shan", "Junxian He"], "title": "On the Perception Bottleneck of VLMs for Chart Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chart understanding requires models to effectively analyze and reason about\nnumerical data, textual elements, and complex visual components. Our\nobservations reveal that the perception capabilities of existing large\nvision-language models (LVLMs) constitute a critical bottleneck in this\nprocess. In this study, we delve into this perception bottleneck by decomposing\nit into two components: the vision encoder bottleneck, where the visual\nrepresentation may fail to encapsulate the correct information, and the\nextraction bottleneck, where the language model struggles to extract the\nnecessary information from the provided visual representations. Through\ncomprehensive experiments, we find that (1) the information embedded within\nvisual representations is substantially richer than what is typically captured\nby linear extractors, such as the widely used retrieval accuracy metric; (2)\nWhile instruction tuning effectively enhances the extraction capability of\nLVLMs, the vision encoder remains a critical bottleneck, demanding focused\nattention and improvement. Therefore, we further enhance the visual encoder to\nmitigate the vision encoder bottleneck under a contrastive learning framework.\nEmpirical results demonstrate that our approach significantly mitigates the\nperception bottleneck and improves the ability of LVLMs to comprehend charts.\nCode is publicly available at https://github.com/hkust-nlp/Vision4Chart.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18458", "pdf": "https://arxiv.org/pdf/2503.18458", "abs": "https://arxiv.org/abs/2503.18458", "authors": ["Luchao Wang", "Qian Ren", "Kaiming He", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent years have witnessed remarkable success of 3D Gaussian Splatting\n(3DGS) in novel view synthesis, surpassing prior differentiable rendering\nmethods in both quality and efficiency. However, its training process suffers\nfrom coupled opacity-color optimization that frequently converges to local\nminima, producing floater artifacts that degrade visual fidelity. We present\nStableGS, a framework that eliminates floaters through cross-view depth\nconsistency constraints while introducing a dual-opacity GS model to decouple\ngeometry and material properties of translucent objects. To further enhance\nreconstruction quality in weakly-textured regions, we integrate DUSt3R depth\nestimation, significantly improving geometric stability. Our method\nfundamentally addresses 3DGS training instabilities, outperforming existing\nstate-of-the-art methods across open-source datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18492", "pdf": "https://arxiv.org/pdf/2503.18492", "abs": "https://arxiv.org/abs/2503.18492", "authors": ["Jungjae Lee", "Dongjae Lee", "Chihun Choi", "Youngmin Im", "Jaeyoung Wi", "Kihong Heo", "Sangeun Oh", "Sunjae Lee", "Insik Shin"], "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Foundation Models (LFMs) have unlocked new possibilities in\nhuman-computer interaction, particularly with the rise of mobile Graphical User\nInterface (GUI) Agents capable of interpreting GUIs. These agents promise to\nrevolutionize mobile computing by allowing users to automate complex mobile\ntasks through simple natural language instructions. However, the inherent\nprobabilistic nature of LFMs, coupled with the ambiguity and context-dependence\nof mobile tasks, makes LFM-based automation unreliable and prone to errors. To\naddress this critical challenge, we introduce VeriSafe Agent (VSA): a formal\nverification system that serves as a logically grounded safeguard for Mobile\nGUI Agents. VSA is designed to deterministically ensure that an agent's actions\nstrictly align with user intent before conducting an action. At its core, VSA\nintroduces a novel autoformalization technique that translates natural language\nuser instructions into a formally verifiable specification, expressed in our\ndomain-specific language (DSL). This enables runtime, rule-based verification,\nallowing VSA to detect and prevent erroneous actions executing an action,\neither by providing corrective feedback or halting unsafe behavior. To the best\nof our knowledge, VSA is the first attempt to bring the rigor of formal\nverification to GUI agent. effectively bridging the gap between LFM-driven\nautomation and formal software verification. We implement VSA using\noff-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user\ninstructions across 18 widely used mobile apps. The results demonstrate that\nVSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a\nsignificant 20.4%-25.6% improvement over existing LLM-based verification\nmethods, and consequently increases the GUI agent's task completion rate by\n90%-130%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18211", "pdf": "https://arxiv.org/pdf/2503.18211", "abs": "https://arxiv.org/abs/2503.18211", "authors": ["Zhengyuan Li", "Kai Cheng", "Anindita Ghosh", "Uttaran Bhattacharya", "Liangyan Gui", "Aniket Bera"], "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction", "categories": ["cs.CV"], "comment": "Project URL: https://github.com/lzhyu/SimMotionEdit", "summary": "Text-based 3D human motion editing is a critical yet challenging task in\ncomputer vision and graphics. While training-free approaches have been\nexplored, the recent release of the MotionFix dataset, which includes\nsource-text-motion triplets, has opened new avenues for training, yielding\npromising results. However, existing methods struggle with precise control,\noften leading to misalignment between motion semantics and language\ninstructions. In this paper, we introduce a related task, motion similarity\nprediction, and propose a multi-task training paradigm, where we train the\nmodel jointly on motion editing and motion similarity prediction to foster the\nlearning of semantically meaningful representations. To complement this task,\nwe design an advanced Diffusion-Transformer-based architecture that separately\nhandles motion similarity prediction and motion editing. Extensive experiments\ndemonstrate the state-of-the-art performance of our approach in both editing\nalignment and fidelity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18494", "pdf": "https://arxiv.org/pdf/2503.18494", "abs": "https://arxiv.org/abs/2503.18494", "authors": ["Hao-Yuan Chen", "Cheng-Pong Huang", "Jui-Ming Yao"], "title": "Verbal Process Supervision Elicits Better Coding Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of large language models and their applications as AI agents\nhave significantly advanced state-of-the-art code generation benchmarks,\ntransforming modern software engineering tasks. However, even with test-time\ncomputed reasoning models, these systems still struggle with complex software\nengineering challenges. This work introduces CURA, a code understanding and\nreasoning agent system enhanced with verbal process supervision (VPS),\nachieving a 3.65\\% improvement over baseline models on challenging benchmarks\nlike BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and\nVPS techniques, attains state-of-the-art performance. This work represents a\nstep forward in integrating reasoning-driven architectures with LLM-based code\ngeneration, enabling agentic reasoning for language models to solve complex\nsoftware engineering tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18680", "pdf": "https://arxiv.org/pdf/2503.18680", "abs": "https://arxiv.org/abs/2503.18680", "authors": ["Danrui Li", "Yichao Shi", "Yaluo Wang", "Ziying Shi", "Mubbasir Kapadia"], "title": "ArchSeek: Retrieving Architectural Case Studies Using Vision-Language Models", "categories": ["cs.IR", "cs.CL"], "comment": "15 pages, 8 figures, 3 tables. Accepted by CAAD Futures 2025", "summary": "Efficiently searching for relevant case studies is critical in architectural\ndesign, as designers rely on precedent examples to guide or inspire their\nongoing projects. However, traditional text-based search tools struggle to\ncapture the inherently visual and complex nature of architectural knowledge,\noften leading to time-consuming and imprecise exploration. This paper\nintroduces ArchSeek, an innovative case study search system with recommendation\ncapability, tailored for architecture design professionals. Powered by the\nvisual understanding capabilities from vision-language models and cross-modal\nembeddings, it enables text and image queries with fine-grained control, and\ninteraction-based design case recommendations. It offers architects a more\nefficient, personalized way to discover design inspirations, with potential\napplications across other visually driven design fields. The source code is\navailable at https://github.com/danruili/ArchSeek.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18278", "pdf": "https://arxiv.org/pdf/2503.18278", "abs": "https://arxiv.org/abs/2503.18278", "authors": ["Cheng Yang", "Yang Sui", "Jinqi Xiao", "Lingyi Huang", "Yu Gong", "Chendi Li", "Jinghua Yan", "Yu Bai", "Ponnuswamy Sadayappan", "Xia Hu", "Bo Yuan"], "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792", "abs": "https://arxiv.org/abs/2503.18792", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "title": "REALM: A Dataset of Real-World LLM Use Cases", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9 pages, 5 figures", "summary": "Large Language Models, such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles. A dedicated\ndashboard https://realm-e7682.web.app/ presents the data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18825", "pdf": "https://arxiv.org/pdf/2503.18825", "abs": "https://arxiv.org/abs/2503.18825", "authors": ["Sara Fish", "Julia Shephard", "Minkai Li", "Ran I. Shorrer", "Yannai A. Gonczarowski"], "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "We develop benchmarks for LLM agents that act in, learn from, and strategize\nin unknown environments, the specifications of which the LLM agent must learn\nover time from deliberate exploration. Our benchmarks consist of\ndecision-making tasks derived from key problems in economics. To forestall\nsaturation, the benchmark tasks are synthetically generated with scalable\ndifficulty levels. Additionally, we propose litmus tests, a new kind of\nquantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests\nquantify differences in character, values, and tendencies of LLMs and LLM\nagents, by considering their behavior when faced with tradeoffs (e.g.,\nefficiency versus equality) where there is no objectively right or wrong\nbehavior. Overall, our benchmarks and litmus tests assess the abilities and\ntendencies of LLM agents in tackling complex economic problems in diverse\nsettings spanning procurement, scheduling, task allocation, and pricing --\napplications that should grow in importance as such agents are further\nintegrated into the economy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18892", "pdf": "https://arxiv.org/pdf/2503.18892", "abs": "https://arxiv.org/abs/2503.18892", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18312", "pdf": "https://arxiv.org/pdf/2503.18312", "abs": "https://arxiv.org/abs/2503.18312", "authors": ["Jianlong Jin", "Chenglong Zhao", "Ruixin Zhang", "Sheng Shang", "Jianqing Xu", "Jingyun Zhang", "ShaoMing Wang", "Yang Zhao", "Shouhong Ding", "Wei Jia", "Yunsheng Wu"], "title": "Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and Intra-Class Variation Controllable Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Palmprint recognition is significantly limited by the lack of large-scale\npublicly available datasets. Previous methods have adopted B\\'ezier curves to\nsimulate the palm creases, which then serve as input for conditional GANs to\ngenerate realistic palmprints. However, without employing real data\nfine-tuning, the performance of the recognition model trained on these\nsynthetic datasets would drastically decline, indicating a large gap between\ngenerated and real palmprints. This is primarily due to the utilization of an\ninaccurate palm crease representation and challenges in balancing intra-class\nvariation with identity consistency. To address this, we introduce a\npolynomial-based palm crease representation that provides a new palm crease\ngeneration mechanism more closely aligned with the real distribution. We also\npropose the palm creases conditioned diffusion model with a novel intra-class\nvariation control method. By applying our proposed $K$-step noise-sharing\nsampling, we are able to synthesize palmprint datasets with large intra-class\nvariation and high identity consistency. Experimental results show that, for\nthe first time, recognition models trained solely on our synthetic datasets,\nwithout any fine-tuning, outperform those trained on real datasets.\nFurthermore, our approach achieves superior recognition performance as the\nnumber of generated identities increases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18941", "pdf": "https://arxiv.org/pdf/2503.18941", "abs": "https://arxiv.org/abs/2503.18941", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18328", "pdf": "https://arxiv.org/pdf/2503.18328", "abs": "https://arxiv.org/abs/2503.18328", "authors": ["Chun Gu", "Xiaofei Wei", "Li Zhang", "Xiatian Zhu"], "title": "TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering", "categories": ["cs.CV"], "comment": "CVPR 2025. Code: https://github.com/fudan-zvg/tensoflow", "summary": "Inverse rendering aims to recover scene geometry, material properties, and\nlighting from multi-view images. Given the complexity of light-surface\ninteractions, importance sampling is essential for the evaluation of the\nrendering equation, as it reduces variance and enhances the efficiency of Monte\nCarlo sampling. Existing inverse rendering methods typically use pre-defined\nnon-learnable importance samplers in prior manually, struggling to effectively\nmatch the spatially and directionally varied integrand and resulting in high\nvariance and suboptimal performance. To address this limitation, we propose the\nconcept of learning a spatially and directionally aware importance sampler for\nthe rendering equation to accurately and flexibly capture the unconstrained\ncomplexity of a typical scene. We further formulate TensoFlow, a generic\napproach for sampler learning in inverse rendering, enabling to closely match\nthe integrand of the rendering equation spatially and directionally.\nConcretely, our sampler is parameterized by normalizing flows, allowing both\ndirectional sampling of incident light and probability density function (PDF)\ninference. To capture the characteristics of the sampler spatially, we learn a\ntensorial representation of the scene space, which imposes spatial conditions,\ntogether with reflected direction, leading to spatially and directionally aware\nsampling distributions. Our model can be optimized by minimizing the difference\nbetween the integrand and our normalizing flow. Extensive experiments validate\nthe superiority of TensoFlow over prior alternatives on both synthetic and\nreal-world benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18334", "pdf": "https://arxiv.org/pdf/2503.18334", "abs": "https://arxiv.org/abs/2503.18334", "authors": ["Haotian Zhai", "Xinyu Chen", "Can Zhang", "Tianming Sha", "Ruirui Li"], "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild", "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18338", "pdf": "https://arxiv.org/pdf/2503.18338", "abs": "https://arxiv.org/abs/2503.18338", "authors": ["Wenrui Cai", "Qingjie Liu", "Yunhong Wang"], "title": "SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Most state-of-the-art trackers adopt one-stream paradigm, using a single\nVision Transformer for joint feature extraction and relation modeling of\ntemplate and search region images. However, relation modeling between different\nimage patches exhibits significant variations. For instance, background regions\ndominated by target-irrelevant information require reduced attention\nallocation, while foreground, particularly boundary areas, need to be be\nemphasized. A single model may not effectively handle all kinds of relation\nmodeling simultaneously. In this paper, we propose a novel tracker called\nSPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE),\ncombining the capability of multiple experts to handle diverse relation\nmodeling more flexibly. Benefiting from TMoE, we extend relation modeling from\nimage pairs to spatio-temporal context, further improving tracking accuracy\nwith minimal increase in model parameters. Moreover, we employ TMoE as a\nparameter-efficient fine-tuning method, substantially reducing trainable\nparameters, which enables us to train SPMTrack of varying scales efficiently\nand preserve the generalization ability of pretrained models to achieve\nsuperior performance. We conduct experiments on seven datasets, and\nexperimental results demonstrate that our method significantly outperforms\ncurrent state-of-the-art trackers. The source code is available at\nhttps://github.com/WenRuiCai/SPMTrack.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18361", "pdf": "https://arxiv.org/pdf/2503.18361", "abs": "https://arxiv.org/abs/2503.18361", "authors": ["Wenyuan Zhang", "Emily Yue-ting Jia", "Junsheng Zhou", "Baorui Ma", "Kanle Shi", "Yu-Shen Liu"], "title": "NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://wen-yuan-zhang.github.io/NeRFPrior/", "summary": "Recently, it has shown that priors are vital for neural implicit functions to\nreconstruct high-quality surfaces from multi-view RGB images. However, current\npriors require large-scale pre-training, and merely provide geometric clues\nwithout considering the importance of color. In this paper, we present\nNeRFPrior, which adopts a neural radiance field as a prior to learn signed\ndistance fields using volume rendering for surface reconstruction. Our NeRF\nprior can provide both geometric and color clues, and also get trained fast\nunder the same scene without additional data. Based on the NeRF prior, we are\nenabled to learn a signed distance function (SDF) by explicitly imposing a\nmulti-view consistency constraint on each ray intersection for surface\ninference. Specifically, at each ray intersection, we use the density in the\nprior as a coarse geometry estimation, while using the color near the surface\nas a clue to check its visibility from another view angle. For the textureless\nareas where the multi-view consistency constraint does not work well, we\nfurther introduce a depth consistency loss with confidence weights to infer the\nSDF. Our experimental results outperform the state-of-the-art methods under the\nwidely used benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18370", "pdf": "https://arxiv.org/pdf/2503.18370", "abs": "https://arxiv.org/abs/2503.18370", "authors": ["Raquel Vidaurre", "Elena Garces", "Dan Casas"], "title": "DiffusedWrinkles: A Diffusion-Based Model for Data-Driven Garment Animation", "categories": ["cs.CV"], "comment": "BMVC 2024", "summary": "We present a data-driven method for learning to generate animations of 3D\ngarments using a 2D image diffusion model. In contrast to existing methods,\ntypically based on fully connected networks, graph neural networks, or\ngenerative adversarial networks, which have difficulties to cope with\nparametric garments with fine wrinkle detail, our approach is able to\nsynthesize high-quality 3D animations for a wide variety of garments and body\nshapes, while being agnostic to the garment mesh topology. Our key idea is to\nrepresent 3D garment deformations as a 2D layout-consistent texture that\nencodes 3D offsets with respect to a parametric garment template. Using this\nrepresentation, we encode a large dataset of garments simulated in various\nmotions and shapes and train a novel conditional diffusion model that is able\nto synthesize high-quality pose-shape-and-design dependent 3D garment\ndeformations. Since our model is generative, we can synthesize various\nplausible deformations for a given target pose, shape, and design.\nAdditionally, we show that we can further condition our model using an existing\ngarment state, which enables the generation of temporally coherent sequences.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18382", "pdf": "https://arxiv.org/pdf/2503.18382", "abs": "https://arxiv.org/abs/2503.18382", "authors": ["Hongen Liu", "Cheng Cui", "Yuning Du", "Yi Liu", "Gang Pan"], "title": "PP-FormulaNet: Bridging Accuracy and Efficiency in Advanced Formula Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Formula recognition is an important task in document intelligence. It\ninvolves converting mathematical expressions from document images into\nstructured symbolic formats that computers can easily work with. LaTeX is the\nmost common format used for this purpose. In this work, we present\nPP-FormulaNet, a state-of-the-art formula recognition model that excels in both\naccuracy and efficiency. To meet the diverse needs of applications, we have\ndeveloped two specialized models: PP-FormulaNet-L, tailored for high-accuracy\nscenarios, and PP-FormulaNet-S, optimized for high-efficiency contexts. Our\nextensive evaluations reveal that PP-FormulaNet-L attains accuracy levels that\nsurpass those of prominent models such as UniMERNet by a significant 6%.\nConversely, PP-FormulaNet-S operates at speeds that are over 16 times faster.\nThese advancements facilitate seamless integration of PP-FormulaNet into a\nbroad spectrum of document processing environments that involve intricate\nmathematical formulas. Furthermore, we introduce a Formula Mining System, which\nis capable of extracting a vast amount of high-quality formula data. This\nsystem further enhances the robustness and applicability of our formula\nrecognition model. Code and models are publicly available at\nPaddleOCR(https://github.com/PaddlePaddle/PaddleOCR) and\nPaddleX(https://github.com/PaddlePaddle/PaddleX).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18386", "pdf": "https://arxiv.org/pdf/2503.18386", "abs": "https://arxiv.org/abs/2503.18386", "authors": ["Sicong Feng", "Jielong Yang", "Li Peng"], "title": "Resource-Efficient Motion Control for Video Generation via Dynamic Mask Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion models bring new vitality to visual content\ncreation. However, current text-to-video generation models still face\nsignificant challenges such as high training costs, substantial data\nrequirements, and difficulties in maintaining consistency between given text\nand motion of the foreground object. To address these challenges, we propose\nmask-guided video generation, which can control video generation through mask\nmotion sequences, while requiring limited training data. Our model enhances\nexisting architectures by incorporating foreground masks for precise\ntext-position matching and motion trajectory control. Through mask motion\nsequences, we guide the video generation process to maintain consistent\nforeground objects throughout the sequence. Additionally, through a first-frame\nsharing strategy and autoregressive extension approach, we achieve more stable\nand longer video generation. Extensive qualitative and quantitative experiments\ndemonstrate that this approach excels in various video generation tasks, such\nas video editing and generating artistic videos, outperforming previous methods\nin terms of consistency and quality. Our generated results can be viewed in the\nsupplementary materials.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18406", "pdf": "https://arxiv.org/pdf/2503.18406", "abs": "https://arxiv.org/abs/2503.18406", "authors": ["Sherry X. Chen", "Misha Sra", "Pradeep Sen"], "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning", "categories": ["cs.CV"], "comment": "Computer Vision and Pattern Recognition 2025", "summary": "Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to challenges in creating large, high-quality\ntraining datasets. Previous work has typically relied on text-toimage (T2I)\ngenerative models to produce pairs of original and edited images that simulate\nthe input/output of an instruction-guided image-editing model. However, these\nimage pairs often fail to align with the specified edit instructions due to the\nlimitations of T2I models, which negatively impacts models trained on such\ndatasets. To address this, we present Instruct-CLIP, a self-supervised method\nthat learns the semantic changes between original and edited images to refine\nand better align the instructions in existing datasets. Furthermore, we adapt\nInstruct-CLIP to handle noisy latent images and diffusion timesteps so that it\ncan be used to train latent diffusion models (LDMs) [19] and efficiently\nenforce alignment between the edit instruction and the image changes in latent\nspace at any step of the diffusion pipeline. We use Instruct-CLIP to correct\nthe InstructPix2Pix dataset and get over 120K refined samples we then use to\nfine-tune their model, guided by our novel Instruct-CLIP-based loss function.\nThe resulting model can produce edits that are more aligned with the given\ninstructions. Our code and dataset are available at\nhttps://github.com/SherryXTChen/Instruct-CLIP.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18414", "pdf": "https://arxiv.org/pdf/2503.18414", "abs": "https://arxiv.org/abs/2503.18414", "authors": ["Yuchuan Tian", "Hanting Chen", "Mengyu Zheng", "Yuchen Liang", "Chao Xu", "Yunhe Wang"], "title": "U-REPA: Aligning Diffusion U-Nets to ViTs", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Representation Alignment (REPA) that aligns Diffusion Transformer (DiT)\nhidden-states with ViT visual encoders has proven highly effective in DiT\ntraining, demonstrating superior convergence properties, but it has not been\nvalidated on the canonical diffusion U-Net architecture that shows faster\nconvergence compared to DiTs. However, adapting REPA to U-Net architectures\npresents unique challenges: (1) different block functionalities necessitate\nrevised alignment strategies; (2) spatial-dimension inconsistencies emerge from\nU-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT\nhinder the effectiveness of tokenwise alignment. To encounter these challenges,\nwe propose U-REPA, a representation alignment paradigm that bridges U-Net\nhidden states and ViT features as follows: Firstly, we propose via observation\nthat due to skip connection, the middle stage of U-Net is the best alignment\noption. Secondly, we propose upsampling of U-Net features after passing them\nthrough MLPs. Thirdly, we observe difficulty when performing tokenwise\nsimilarity alignment, and further introduces a manifold loss that regularizes\nthe relative similarity between samples. Experiments indicate that the\nresulting U-REPA could achieve excellent generation quality and greatly\naccelerates the convergence speed. With CFG guidance interval, U-REPA could\nreach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\\times$ 256,\nand needs only half the total epochs to perform better than REPA. Codes are\navailable at https://github.com/YuchuanTian/U-REPA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18420", "pdf": "https://arxiv.org/pdf/2503.18420", "abs": "https://arxiv.org/abs/2503.18420", "authors": ["Dian Zheng", "Cheng Zhang", "Xiao-Ming Wu", "Cao Li", "Chengfei Lv", "Jian-Fang Hu", "Wei-Shi Zheng"], "title": "Panorama Generation From NFoV Image Done Right", "categories": ["cs.CV"], "comment": "CVPR2025. Project\n  page:https://isee-laboratory.github.io/PanoDecouple/\n  Code:https://github.com/iSEE-Laboratory/PanoDecouple/", "summary": "Generating 360-degree panoramas from narrow field of view (NFoV) image is a\npromising computer vision task for Virtual Reality (VR) applications. Existing\nmethods mostly assess the generated panoramas with InceptionNet or CLIP based\nmetrics, which tend to perceive the image quality and is \\textbf{not suitable\nfor evaluating the distortion}. In this work, we first propose a\ndistortion-specific CLIP, named Distort-CLIP to accurately evaluate the\npanorama distortion and discover the \\textbf{``visual cheating''} phenomenon in\nprevious works (\\ie, tending to improve the visual results by sacrificing\ndistortion accuracy). This phenomenon arises because prior methods employ a\nsingle network to learn the distinct panorama distortion and content completion\nat once, which leads the model to prioritize optimizing the latter. To address\nthe phenomenon, we propose \\textbf{PanoDecouple}, a decoupled diffusion model\nframework, which decouples the panorama generation into distortion guidance and\ncontent completion, aiming to generate panoramas with both accurate distortion\nand visual appeal. Specifically, we design a DistortNet for distortion guidance\nby imposing panorama-specific distortion prior and a modified condition\nregistration mechanism; and a ContentNet for content completion by imposing\nperspective image information. Additionally, a distortion correction loss\nfunction with Distort-CLIP is introduced to constrain the distortion\nexplicitly. The extensive experiments validate that PanoDecouple surpasses\nexisting methods both in distortion and visual metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18422", "pdf": "https://arxiv.org/pdf/2503.18422", "abs": "https://arxiv.org/abs/2503.18422", "authors": ["Handong Li", "Yiyuan Zhang", "Longteng Guo", "Xiangyu Yue", "Jing Liu"], "title": "Breaking the Encoder Barrier for Seamless Video-Language Understanding", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Most Video-Large Language Models (Video-LLMs) adopt an encoder-decoder\nframework, where a vision encoder extracts frame-wise features for processing\nby a language model. However, this approach incurs high computational costs,\nintroduces resolution biases, and struggles to capture fine-grained multimodal\ninteractions. To overcome these limitations, we propose ELVA, an encoder-free\nVideo-LLM that directly models nuanced video-language interactions without\nrelying on a vision encoder. ELVA employs token merging to construct a\nbottom-up hierarchical representation and incorporates a video guidance\nsupervisor for direct spatiotemporal representation learning. Additionally, a\nhybrid-resolution mechanism strategically integrates high- and low-resolution\nframes as inputs to achieve an optimal balance between performance and\nefficiency. With only 7M publicly available video-text pairs, ELVA achieves\nperformance on par with encoder-based Video-LLMs while reducing FLOPs by up to\n95\\% and inference latency by 92\\%, offering a scalable and efficient solution\nfor real-time video understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18429", "pdf": "https://arxiv.org/pdf/2503.18429", "abs": "https://arxiv.org/abs/2503.18429", "authors": ["Dingcheng Zhen", "Shunshun Yin", "Shiyang Qin", "Hou Yi", "Ziwei Zhang", "Siyuan Liu", "Gan Qi", "Ming Tao"], "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation", "categories": ["cs.CV"], "comment": "Accept in CVPR 2025 Conference Submission", "summary": "In this work, we introduce the first autoregressive framework for real-time,\naudio-driven portrait animation, a.k.a, talking head. Beyond the challenge of\nlengthy animation times, a critical challenge in realistic talking head\ngeneration lies in preserving the natural movement of diverse body parts. To\nthis end, we propose Teller, the first streaming audio-driven protrait\nanimation framework with autoregressive motion generation. Specifically, Teller\nfirst decomposes facial and body detail animation into two components: Facial\nMotion Latent Generation (FMLG) based on an autoregressive transfromer, and\nmovement authenticity refinement using a Efficient Temporal Module\n(ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion\nlatent from the implicit keypoint-based model into discrete motion tokens,\nwhich are then temporally sliced with audio embeddings. This enables the AR\ntranformer to learn real-time, stream-based mappings from audio to motion.\nFurthermore, Teller incorporate ETM to capture finer motion details. This\nmodule ensures the physical consistency of body parts and accessories, such as\nneck muscles and earrings, improving the realism of these movements. Teller is\ndesigned to be efficient, surpassing the inference speed of diffusion-based\nmodels (Hallo 20.93s vs. Teller 0.92s for one second video generation), and\nachieves a real-time streaming performance of up to 25 FPS. Extensive\nexperiments demonstrate that our method outperforms recent audio-driven\nportrait animation models, especially in small movements, as validated by human\nevaluations with a significant margin in quality and realism.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18435", "pdf": "https://arxiv.org/pdf/2503.18435", "abs": "https://arxiv.org/abs/2503.18435", "authors": ["Junteng Liu", "Weihao Zeng", "Xiwen Zhang", "Yijun Wang", "Zifei Shan", "Junxian He"], "title": "On the Perception Bottleneck of VLMs for Chart Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chart understanding requires models to effectively analyze and reason about\nnumerical data, textual elements, and complex visual components. Our\nobservations reveal that the perception capabilities of existing large\nvision-language models (LVLMs) constitute a critical bottleneck in this\nprocess. In this study, we delve into this perception bottleneck by decomposing\nit into two components: the vision encoder bottleneck, where the visual\nrepresentation may fail to encapsulate the correct information, and the\nextraction bottleneck, where the language model struggles to extract the\nnecessary information from the provided visual representations. Through\ncomprehensive experiments, we find that (1) the information embedded within\nvisual representations is substantially richer than what is typically captured\nby linear extractors, such as the widely used retrieval accuracy metric; (2)\nWhile instruction tuning effectively enhances the extraction capability of\nLVLMs, the vision encoder remains a critical bottleneck, demanding focused\nattention and improvement. Therefore, we further enhance the visual encoder to\nmitigate the vision encoder bottleneck under a contrastive learning framework.\nEmpirical results demonstrate that our approach significantly mitigates the\nperception bottleneck and improves the ability of LVLMs to comprehend charts.\nCode is publicly available at https://github.com/hkust-nlp/Vision4Chart.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18445", "pdf": "https://arxiv.org/pdf/2503.18445", "abs": "https://arxiv.org/abs/2503.18445", "authors": ["Chenfei Liao", "Kaiyu Lei", "Xu Zheng", "Junha Moon", "Zhixiong Wang", "Yixuan Wang", "Danda Pani Paudel", "Luc Van Gool", "Xuming Hu"], "title": "Benchmarking Multi-modal Semantic Segmentation under Sensor Failures: Missing and Noisy Modality Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal semantic segmentation (MMSS) addresses the limitations of\nsingle-modality data by integrating complementary information across\nmodalities. Despite notable progress, a significant gap persists between\nresearch and real-world deployment due to variability and uncertainty in\nmulti-modal data quality. Robustness has thus become essential for practical\nMMSS applications. However, the absence of standardized benchmarks for\nevaluating robustness hinders further advancement. To address this, we first\nsurvey existing MMSS literature and categorize representative methods to\nprovide a structured overview. We then introduce a robustness benchmark that\nevaluates MMSS models under three scenarios: Entire-Missing Modality (EMM),\nRandom-Missing Modality (RMM), and Noisy Modality (NM). From a probabilistic\nstandpoint, we model modality failure under two conditions: (1) all damaged\ncombinations are equally probable; (2) each modality fails independently\nfollowing a Bernoulli distribution. Based on these, we propose four\nmetrics-$mIoU^{Avg}_{EMM}$, $mIoU^{E}_{EMM}$, $mIoU^{Avg}_{RMM}$, and\n$mIoU^{E}_{RMM}$-to assess model robustness under EMM and RMM. This work\nprovides the first dedicated benchmark for MMSS robustness, offering new\ninsights and tools to advance the field. Source code is available at\nhttps://github.com/Chenfei-Liao/Multi-Modal-Semantic-Segmentation-Robustness-Benchmark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18458", "pdf": "https://arxiv.org/pdf/2503.18458", "abs": "https://arxiv.org/abs/2503.18458", "authors": ["Luchao Wang", "Qian Ren", "Kaiming He", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "StableGS: A Floater-Free Framework for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent years have witnessed remarkable success of 3D Gaussian Splatting\n(3DGS) in novel view synthesis, surpassing prior differentiable rendering\nmethods in both quality and efficiency. However, its training process suffers\nfrom coupled opacity-color optimization that frequently converges to local\nminima, producing floater artifacts that degrade visual fidelity. We present\nStableGS, a framework that eliminates floaters through cross-view depth\nconsistency constraints while introducing a dual-opacity GS model to decouple\ngeometry and material properties of translucent objects. To further enhance\nreconstruction quality in weakly-textured regions, we integrate DUSt3R depth\nestimation, significantly improving geometric stability. Our method\nfundamentally addresses 3DGS training instabilities, outperforming existing\nstate-of-the-art methods across open-source datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18459", "pdf": "https://arxiv.org/pdf/2503.18459", "abs": "https://arxiv.org/abs/2503.18459", "authors": ["Haoyu Chen", "Yunqiao Yang", "Nan Zhong", "Kede Ma"], "title": "Hiding Images in Diffusion Models by Editing Learned Score Functions", "categories": ["cs.CV"], "comment": null, "summary": "Hiding data using neural networks (i.e., neural steganography) has achieved\nremarkable success across both discriminative classifiers and generative\nadversarial networks. However, the potential of data hiding in diffusion models\nremains relatively unexplored. Current methods exhibit limitations in achieving\nhigh extraction accuracy, model fidelity, and hiding efficiency due primarily\nto the entanglement of the hiding and extraction processes with multiple\ndenoising diffusion steps. To address these, we describe a simple yet effective\napproach that embeds images at specific timesteps in the reverse diffusion\nprocess by editing the learned score functions. Additionally, we introduce a\nparameter-efficient fine-tuning method that combines gradient-based parameter\nselection with low-rank adaptation to enhance model fidelity and hiding\nefficiency. Comprehensive experiments demonstrate that our method extracts\nhigh-quality images at human-indistinguishable levels, replicates the original\nmodel behaviors at both sample and population levels, and embeds images orders\nof magnitude faster than prior methods. Besides, our method naturally supports\nmulti-recipient scenarios through independent extraction channels.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18470", "pdf": "https://arxiv.org/pdf/2503.18470", "abs": "https://arxiv.org/abs/2503.18470", "authors": ["Zhenyu Pan", "Han Liu"], "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse", "categories": ["cs.CV", "cs.AI"], "comment": "Working Paper", "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18478", "pdf": "https://arxiv.org/pdf/2503.18478", "abs": "https://arxiv.org/abs/2503.18478", "authors": ["Xiangrui Liu", "Yan Shu", "Zheng Liu", "Ao Li", "Yang Tian", "Bo Zhao"], "title": "Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Despite advanced token compression techniques, existing multimodal large\nlanguage models (MLLMs) still struggle with hour-long video understanding. In\nthis work, we propose Video-XL-Pro, an efficient method for extremely long\nvideo understanding, built upon Reconstructive Compression of Tokens (ReCoT), a\nlearnable module that leverages self-supervised learning to generate\ncomprehensive and compact video tokens. ReCoT introduces two key components:\n(i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from\nstatic image tokens by learning intra-token relationships, which are then used\nin masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively\nmasks redundant visual tokens to facilitate more effective reconstructive\nlearning. To improve training efficiency in MLLMs fine-tuning, we introduce a\nvideo-specific dataset pruning strategy and design a simple yet Query-aware\nSelector that enables the model to precisely locate query-relevant video\ntokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models\ntrained on larger datasets across multiple long video understanding benchmarks.\nMoreover, it can process over 8K frames on a single A100 GPU while maintaining\nhigh-quality performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18483", "pdf": "https://arxiv.org/pdf/2503.18483", "abs": "https://arxiv.org/abs/2503.18483", "authors": ["Zequn Zeng", "Yudi Su", "Jianqiao Sun", "Tiansheng Wen", "Hao Zhang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu", "Jiawei Ma"], "title": "Explaining Domain Shifts in Language: Concept erasing for Interpretable Image Classification", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Concept-based models can map black-box representations to\nhuman-understandable concepts, which makes the decision-making process more\ntransparent and then allows users to understand the reason behind predictions.\nHowever, domain-specific concepts often impact the final predictions, which\nsubsequently undermine the model generalization capabilities, and prevent the\nmodel from being used in high-stake applications. In this paper, we propose a\nnovel Language-guided Concept-Erasing (LanCE) framework. In particular, we\nempirically demonstrate that pre-trained vision-language models (VLMs) can\napproximate distinct visual domain shifts via domain descriptors while\nprompting large Language Models (LLMs) can easily simulate a wide range of\ndescriptors of unseen visual domains. Then, we introduce a novel plug-in domain\ndescriptor orthogonality (DDO) regularizer to mitigate the impact of these\ndomain-specific concepts on the final predictions. Notably, the DDO regularizer\nis agnostic to the design of concept-based models and we integrate it into\nseveral prevailing models. Through evaluation of domain generalization on four\nstandard benchmarks and three newly introduced benchmarks, we demonstrate that\nDDO can significantly improve the out-of-distribution (OOD) generalization over\nthe previous state-of-the-art concept-based models.Our code is available at\nhttps://github.com/joeyz0z/LanCE.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18527", "pdf": "https://arxiv.org/pdf/2503.18527", "abs": "https://arxiv.org/abs/2503.18527", "authors": ["Soulaimene Turki", "Daniel Panangian", "Houda Chaabouni-Chouayakh", "Ksenia Bittner"], "title": "AIM2PC: Aerial Image to 3D Building Point Cloud Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to ISPRS Geospatial Week 2025", "summary": "Three-dimensional urban reconstruction of buildings from single-view images\nhas attracted significant attention over the past two decades. However, recent\nmethods primarily focus on rooftops from aerial images, often overlooking\nessential geometrical details. Additionally, there is a notable lack of\ndatasets containing complete 3D point clouds for entire buildings, along with\nchallenges in obtaining reliable camera pose information for aerial images.\nThis paper addresses these challenges by presenting a novel methodology, AIM2PC\n, which utilizes our generated dataset that includes complete 3D point clouds\nand determined camera poses. Our approach takes features from a single aerial\nimage as input and concatenates them with essential additional conditions, such\nas binary masks and Sobel edge maps, to enable more edge-aware reconstruction.\nBy incorporating a point cloud diffusion model based on Centered denoising\nDiffusion Probabilistic Models (CDPM), we project these concatenated features\nonto the partially denoised point cloud using our camera poses at each\ndiffusion step. The proposed method is able to reconstruct the complete 3D\nbuilding point cloud, including wall information and demonstrates superior\nperformance compared to existing baseline techniques. To allow further\ncomparisons with our methodology the dataset has been made available at\nhttps://github.com/Soulaimene/AIM2PCDataset", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18553", "pdf": "https://arxiv.org/pdf/2503.18553", "abs": "https://arxiv.org/abs/2503.18553", "authors": ["Zihao Chen", "Hsuanyu Wu", "Chi-Hsi Kung", "Yi-Ting Chen", "Yan-Tsung Peng"], "title": "ATARS: An Aerial Traffic Atomic Activity Recognition and Temporal Segmentation Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Traffic Atomic Activity which describes traffic patterns for topological\nintersection dynamics is a crucial topic for the advancement of intelligent\ndriving systems. However, existing atomic activity datasets are collected from\nan egocentric view, which cannot support the scenarios where traffic activities\nin an entire intersection are required. Moreover, existing datasets only\nprovide video-level atomic activity annotations, which require exhausting\nefforts to manually trim the videos for recognition and limit their\napplications to untrimmed videos. To bridge this gap, we introduce the Aerial\nTraffic Atomic Activity Recognition and Segmentation (ATARS) dataset, the first\naerial dataset designed for multi-label atomic activity analysis. We offer\natomic activity labels for each frame, which accurately record the intervals\nfor traffic activities. Moreover, we propose a novel task, Multi-label Temporal\nAtomic Activity Recognition, enabling the study of accurate temporal\nlocalization for atomic activity and easing the burden of manual video trimming\nfor recognition. We conduct extensive experiments to evaluate existing\nstate-of-the-art models on both atomic activity recognition and temporal atomic\nactivity segmentation. The results highlight the unique challenges of our ATARS\ndataset, such as recognizing extremely small objects' activities. We further\nprovide comprehensive discussion analyzing these challenges and offer valuable\ninsights for future direction to improve recognizing atomic activity in aerial\nview. Our source code and dataset are available at\nhttps://github.com/magecliff96/ATARS/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18583", "pdf": "https://arxiv.org/pdf/2503.18583", "abs": "https://arxiv.org/abs/2503.18583", "authors": ["Alexander Holmberg", "Nils Mechtel", "Wei Ouyang"], "title": "Adapting Video Diffusion Models for Time-Lapse Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "We present a domain adaptation of video diffusion models to generate highly\nrealistic time-lapse microscopy videos of cell division in HeLa cells. Although\nstate-of-the-art generative video models have advanced significantly for\nnatural videos, they remain underexplored in microscopy domains. To address\nthis gap, we fine-tune a pretrained video diffusion model on\nmicroscopy-specific sequences, exploring three conditioning strategies: (1)\ntext prompts derived from numeric phenotypic measurements (e.g., proliferation\nrates, migration speeds, cell-death frequencies), (2) direct numeric embeddings\nof phenotype scores, and (3) image-conditioned generation, where an initial\nmicroscopy frame is extended into a complete video sequence. Evaluation using\nbiologically meaningful morphological, proliferation, and migration metrics\ndemonstrates that fine-tuning substantially improves realism and accurately\ncaptures critical cellular behaviors such as mitosis and migration. Notably,\nthe fine-tuned model also generalizes beyond the training horizon, generating\ncoherent cell dynamics even in extended sequences. However, precisely\ncontrolling specific phenotypic characteristics remains challenging,\nhighlighting opportunities for future work to enhance conditioning methods. Our\nresults demonstrate the potential for domain-specific fine-tuning of generative\nvideo models to produce biologically plausible synthetic microscopy data,\nsupporting applications such as in-silico hypothesis testing and data\naugmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18589", "pdf": "https://arxiv.org/pdf/2503.18589", "abs": "https://arxiv.org/abs/2503.18589", "authors": ["Guillem Capellera", "Antonio Rubio", "Luis Ferraz", "Antonio Agudo"], "title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 conference", "summary": "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https://youtu.be/ngw4D4eJToE", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18623", "pdf": "https://arxiv.org/pdf/2503.18623", "abs": "https://arxiv.org/abs/2503.18623", "authors": ["Deepayan Das", "Davide Talon", "Yiming Wang", "Massimiliano Mancini", "Elisa Ricci"], "title": "Training-Free Personalization via Retrieval and Reasoning on Fingerprints", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18631", "pdf": "https://arxiv.org/pdf/2503.18631", "abs": "https://arxiv.org/abs/2503.18631", "authors": ["Kunyang Li", "Ming Hou"], "title": "Robust Lane Detection with Wavelet-Enhanced Context Modeling and Adaptive Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Lane detection is critical for autonomous driving and ad-vanced driver\nassistance systems (ADAS). While recent methods like CLRNet achieve strong\nperformance, they struggle under adverse con-ditions such as extreme weather,\nillumination changes, occlusions, and complex curves. We propose a\nWavelet-Enhanced Feature Pyramid Net-work (WE-FPN) to address these challenges.\nA wavelet-based non-local block is integrated before the feature pyramid to\nimprove global context modeling, especially for occluded and curved lanes.\nAdditionally, we de-sign an adaptive preprocessing module to enhance lane\nvisibility under poor lighting. An attention-guided sampling strategy further\nreffnes spa-tial features, boosting accuracy on distant and curved lanes.\nExperiments on CULane and TuSimple demonstrate that our approach signiffcantly\noutperforms baselines in challenging scenarios, achieving better robust-ness\nand accuracy in real-world driving conditions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18640", "pdf": "https://arxiv.org/pdf/2503.18640", "abs": "https://arxiv.org/abs/2503.18640", "authors": ["Haoran Wang", "Jingwei Huang", "Lu Yang", "Tianchen Deng", "Gaojing Zhang", "Mingrui Li"], "title": "LLGS: Unsupervised Gaussian Splatting for Image Enhancement and Reconstruction in Pure Dark Environment", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has shown remarkable capabilities in novel view\nrendering tasks and exhibits significant potential for multi-view\noptimization.However, the original 3D Gaussian Splatting lacks color\nrepresentation for inputs in low-light environments. Simply using enhanced\nimages as inputs would lead to issues with multi-view consistency, and current\nsingle-view enhancement systems rely on pre-trained data, lacking scene\ngeneralization. These problems limit the application of 3D Gaussian Splatting\nin low-light conditions in the field of robotics, including high-fidelity\nmodeling and feature matching. To address these challenges, we propose an\nunsupervised multi-view stereoscopic system based on Gaussian Splatting, called\nLow-Light Gaussian Splatting (LLGS). This system aims to enhance images in\nlow-light environments while reconstructing the scene. Our method introduces a\ndecomposable Gaussian representation called M-Color, which separately\ncharacterizes color information for targeted enhancement. Furthermore, we\npropose an unsupervised optimization method with zero-knowledge priors, using\ndirection-based enhancement to ensure multi-view consistency. Experiments\nconducted on real-world datasets demonstrate that our system outperforms\nstate-of-the-art methods in both low-light enhancement and 3D Gaussian\nSplatting.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18658", "pdf": "https://arxiv.org/pdf/2503.18658", "abs": "https://arxiv.org/abs/2503.18658", "authors": ["Christopher Ummerle", "Antonio Giganti", "Sara Mandelli", "Paolo Bestagini", "Stefano Tubaro"], "title": "Leveraging Land Cover Priors for Isoprene Emission Super-Resolution", "categories": ["cs.CV"], "comment": "17 pages, 16 figures, 4 tables", "summary": "Remote sensing plays a crucial role in monitoring Earth's ecosystems, yet\nsatellite-derived data often suffer from limited spatial resolution,\nrestricting their applicability in atmospheric modeling and climate research.\nIn this work, we propose a deep learning-based Super-Resolution (SR) framework\nthat leverages land cover information to enhance the spatial accuracy of\nBiogenic Volatile Organic Compounds (BVOCs) emissions, with a particular focus\non isoprene. Our approach integrates land cover priors as emission drivers,\ncapturing spatial patterns more effectively than traditional methods. We\nevaluate the model's performance across various climate conditions and analyze\nstatistical correlations between isoprene emissions and key environmental\ninformation such as cropland and tree cover data. Additionally, we assess the\ngeneralization capabilities of our SR model by applying it to unseen climate\nzones and geographical regions. Experimental results demonstrate that\nincorporating land cover data significantly improves emission SR accuracy,\nparticularly in heterogeneous landscapes. This study contributes to atmospheric\nchemistry and climate modeling by providing a cost-effective, data-driven\napproach to refining BVOC emission maps. The proposed method enhances the\nusability of satellite-based emissions data, supporting applications in air\nquality forecasting, climate impact assessments, and environmental studies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18671", "pdf": "https://arxiv.org/pdf/2503.18671", "abs": "https://arxiv.org/abs/2503.18671", "authors": ["Yihan Chen", "Wenfei Yang", "Huan Ren", "Shifeng Zhang", "Tianzhu Zhang", "Feng Wu"], "title": "Structure-Aware Correspondence Learning for Relative Pose Estimation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18673", "pdf": "https://arxiv.org/pdf/2503.18673", "abs": "https://arxiv.org/abs/2503.18673", "authors": ["Taeyeop Lee", "Bowen Wen", "Minjun Kang", "Gyuree Kang", "In So Kweon", "Kuk-Jin Yoon"], "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025, Project Page: https://taeyeop.com/any6d", "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18674", "pdf": "https://arxiv.org/pdf/2503.18674", "abs": "https://arxiv.org/abs/2503.18674", "authors": ["Edoardo De Matteis", "Matteo Migliarini", "Alessio Sampieri", "Indro Spinelli", "Fabio Galasso"], "title": "Human Motion Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "We introduce the task of human motion unlearning to prevent the synthesis of\ntoxic animations while preserving the general text-to-motion generative\nperformance. Unlearning toxic motions is challenging as those can be generated\nfrom explicit text prompts and from implicit toxic combinations of safe motions\n(e.g., ``kicking\" is ``loading and swinging a leg\"). We propose the first\nmotion unlearning benchmark by filtering toxic motions from the large and\nrecent text-to-motion datasets of HumanML3D and Motion-X. We propose baselines,\nby adapting state-of-the-art image unlearning techniques to process\nspatio-temporal signals. Finally, we propose a novel motion unlearning model\nbased on Latent Code Replacement, which we dub LCR. LCR is training-free and\nsuitable to the discrete latent spaces of state-of-the-art text-to-motion\ndiffusion models. LCR is simple and consistently outperforms baselines\nqualitatively and quantitatively. Project page:\n\\href{https://www.pinlab.org/hmu}{https://www.pinlab.org/hmu}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18682", "pdf": "https://arxiv.org/pdf/2503.18682", "abs": "https://arxiv.org/abs/2503.18682", "authors": ["Samuel Rota Bulò", "Nemanja Bartolovic", "Lorenzo Porzi", "Peter Kontschieder"], "title": "Hardware-Rasterized Ray-Based Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present a novel, hardware rasterized rendering approach for ray-based 3D\nGaussian Splatting (RayGS), obtaining both fast and high-quality results for\nnovel view synthesis. Our work contains a mathematically rigorous and\ngeometrically intuitive derivation about how to efficiently estimate all\nrelevant quantities for rendering RayGS models, structured with respect to\nstandard hardware rasterization shaders. Our solution is the first enabling\nrendering RayGS models at sufficiently high frame rates to support\nquality-sensitive applications like Virtual and Mixed Reality. Our second\ncontribution enables alias-free rendering for RayGS, by addressing MIP-related\nissues arising when rendering diverging scales during training and testing. We\ndemonstrate significant performance gains, across different benchmark scenes,\nwhile retaining state-of-the-art appearance quality of RayGS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18703", "pdf": "https://arxiv.org/pdf/2503.18703", "abs": "https://arxiv.org/abs/2503.18703", "authors": ["Guanglu Dong", "Tianheng Zheng", "Yuanzhouhan Cao", "Linbo Qing", "Chao Ren"], "title": "Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Recently, deep image deraining models based on paired datasets have made a\nseries of remarkable progress. However, they cannot be well applied in\nreal-world applications due to the difficulty of obtaining real paired datasets\nand the poor generalization performance. In this paper, we propose a novel\nChannel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised\nImage Deraining framework, CSUD, to tackle the aforementioned challenges.\nDuring training with unpaired data, CSUD is capable of generating high-quality\npseudo clean and rainy image pairs which are used to enhance the performance of\nderaining network. Specifically, to preserve more image background details\nwhile transferring rain streaks from rainy images to the unpaired clean images,\nwe propose a novel Channel Consistency Loss (CCLoss) by introducing the Channel\nConsistency Prior (CCP) of rain streaks into training process, thereby ensuring\nthat the generated pseudo rainy images closely resemble the real ones.\nFurthermore, we propose a novel Self-Reconstruction (SR) strategy to alleviate\nthe redundant information transfer problem of the generator, further improving\nthe deraining performance and the generalization capability of our method.\nExtensive experiments on multiple synthetic and real-world datasets demonstrate\nthat the deraining performance of CSUD surpasses other state-of-the-art\nunsupervised methods and CSUD exhibits superior generalization capability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18712", "pdf": "https://arxiv.org/pdf/2503.18712", "abs": "https://arxiv.org/abs/2503.18712", "authors": ["Shaokai Ye", "Haozhe Qi", "Alexander Mathis", "Mackenzie W. Mathis"], "title": "LLaVAction: evaluating and training multi-modal large language models for action recognition", "categories": ["cs.CV"], "comment": "https://github.com/AdaptiveMotorControlLab/LLaVAction", "summary": "Understanding human behavior requires measuring behavioral actions. Due to\nits complexity, behavior is best mapped onto a rich, semantic structure such as\nlanguage. The recent development of multi-modal large language models (MLLMs)\nis a promising candidate for a wide range of action understanding tasks. In\nthis work, we focus on evaluating and then improving MLLMs to perform action\nrecognition. We reformulate EPIC-KITCHENS-100, one of the largest and most\nchallenging egocentric action datasets, to the form of video multiple question\nanswering (EPIC-KITCHENS-100-MQA). We show that when we sample difficult\nincorrect answers as distractors, leading MLLMs struggle to recognize the\ncorrect actions. We propose a series of methods that greatly improve the MLLMs'\nability to perform action recognition, achieving state-of-the-art on both the\nEPIC-KITCHENS-100 validation set, as well as outperforming GPT-4o by 21 points\nin accuracy on EPIC-KITCHENS-100-MQA. Lastly, we show improvements on other\naction-related video benchmarks such as EgoSchema, PerceptionTest,\nLongVideoBench, VideoMME and MVBench, suggesting that MLLMs are a promising\npath forward for complex action tasks. Code and models are available at:\nhttps://github.com/AdaptiveMotorControlLab/LLaVAction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18718", "pdf": "https://arxiv.org/pdf/2503.18718", "abs": "https://arxiv.org/abs/2503.18718", "authors": ["Lijiang Li", "Jinglu Wang", "Xiang Ming", "Yan Lu"], "title": "GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In the Generative AI era, safeguarding 3D models has become increasingly\nurgent. While invisible watermarking is well-established for 2D images with\nencoder-decoder frameworks, generalizable and robust solutions for 3D remain\nelusive. The main difficulty arises from the renderer between the 3D encoder\nand 2D decoder, which disrupts direct gradient flow and complicates training.\nExisting 3D methods typically rely on per-scene iterative optimization,\nresulting in time inefficiency and limited generalization. In this work, we\npropose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a\nwell-known yet underexplored representation for watermarking. We identify two\nmajor challenges: (1) ensuring effective training generalized across diverse 3D\nmodels, and (2) reliably extracting watermarks from free-view renderings, even\nunder distortions. Our framework, named GS-Marker, incorporates a 3D encoder to\nembed messages, distortion layers to enhance resilience against various\ndistortions, and a 2D decoder to extract watermarks from renderings. A key\ninnovation is the Adaptive Marker Control mechanism that adaptively perturbs\nthe initially optimized 3DGS, escaping local minima and improving both training\nstability and convergence. Extensive experiments show that GS-Marker\noutperforms per-scene training approaches in terms of decoding accuracy and\nmodel fidelity, while also significantly reducing computation time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18719", "pdf": "https://arxiv.org/pdf/2503.18719", "abs": "https://arxiv.org/abs/2503.18719", "authors": ["Cong Liu", "Liang Hou", "Mingwu Zheng", "Xin Tao", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings", "categories": ["cs.CV"], "comment": null, "summary": "Resolution generalization in image generation tasks enables the production of\nhigher-resolution images with lower training resolution overhead. However, a\nsignificant challenge in resolution generalization, particularly in the widely\nused Diffusion Transformers, lies in the mismatch between the positional\nencodings encountered during testing and those used during training. While\nexisting methods have employed techniques such as interpolation, extrapolation,\nor their combinations, none have fully resolved this issue. In this paper, we\npropose a novel two-dimensional randomized positional encodings (RPE-2D)\nframework that focuses on learning positional order of image patches instead of\nthe specific distances between them, enabling seamless high- and low-resolution\nimage generation without requiring high- and low-resolution image training.\nSpecifically, RPE-2D independently selects positions over a broader range along\nboth the horizontal and vertical axes, ensuring that all position encodings are\ntrained during the inference phase, thus improving resolution generalization.\nAdditionally, we propose a random data augmentation technique to enhance the\nmodeling of position order. To address the issue of image cropping caused by\nthe augmentation, we introduce corresponding micro-conditioning to enable the\nmodel to perceive the specific cropping patterns. On the ImageNet dataset, our\nproposed RPE-2D achieves state-of-the-art resolution generalization\nperformance, outperforming existing competitive methods when trained at a\nresolution of $256 \\times 256$ and inferred at $384 \\times 384$ and $512 \\times\n512$, as well as when scaling from $512 \\times 512$ to $768 \\times 768$ and\n$1024 \\times 1024$. And it also exhibits outstanding capabilities in\nlow-resolution image generation, multi-stage training acceleration and\nmulti-resolution inheritance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18742", "pdf": "https://arxiv.org/pdf/2503.18742", "abs": "https://arxiv.org/abs/2503.18742", "authors": ["Sebastian Tewes", "Yufan Chen", "Omar Moured", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "SFDLA: Source-Free Document Layout Analysis", "categories": ["cs.CV"], "comment": "The benchmark, models, and code will be publicly available at\n  https://github.com/s3setewe/sfdla-DLAdapter", "summary": "Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps://github.com/s3setewe/sfdla-DLAdapter.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18746", "pdf": "https://arxiv.org/pdf/2503.18746", "abs": "https://arxiv.org/abs/2503.18746", "authors": ["Yifei Zhang", "Chang Liu", "Jin Wei", "Xiaomeng Yang", "Yu Zhou", "Can Ma", "Xiangyang Ji"], "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18767", "pdf": "https://arxiv.org/pdf/2503.18767", "abs": "https://arxiv.org/abs/2503.18767", "authors": ["Konstantin Pakulev", "Alexander Vakhitov", "Gonzalo Ferrer"], "title": "Good Keypoints for the Two-View Geometry Estimation Problem", "categories": ["cs.CV"], "comment": "Camera-ready version of the CVPR 2025 paper", "summary": "Local features are essential to many modern downstream applications.\nTherefore, it is of interest to determine the properties of local features that\ncontribute to the downstream performance for a better design of feature\ndetectors and descriptors. In our work, we propose a new theoretical model for\nscoring feature points (keypoints) in the context of the two-view geometry\nestimation problem. The model determines two properties that a good keypoint\nfor solving the homography estimation problem should have: be repeatable and\nhave a small expected measurement error. This result provides key insights into\nwhy maximizing the number of correspondences doesn't always lead to better\nhomography estimation accuracy. We use the developed model to design a method\nthat detects keypoints that benefit the homography estimation introducing the\nBounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes\nfrom strong theoretical foundations, a more accurate keypoint scoring due to\nsubpixel refinement and a cost designed for superior robustness to low saliency\nkeypoints. As a result, BoNeSS-ST outperforms prior self-supervised local\nfeature detectors in both planar homography and epipolar geometry estimation\nproblems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18784", "pdf": "https://arxiv.org/pdf/2503.18784", "abs": "https://arxiv.org/abs/2503.18784", "authors": ["Wenxi Chen", "Raymond A. Yeh", "Shaoshuai Mou", "Yan Gu"], "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is the task of identifying inputs that\ndeviate from the training data distribution. This capability is essential for\nsafely deploying deep computer vision models in open-world environments. In\nthis work, we propose a post-hoc method, Perturbation-Rectified OOD detection\n(PRO), based on the insight that prediction confidence for OOD inputs is more\nsusceptible to reduction under perturbation than in-distribution (IND) inputs.\nBased on the observation, we propose an adversarial score function that\nsearches for the local minimum scores near the original inputs by applying\ngradient descent. This procedure enhances the separability between IND and OOD\nsamples. Importantly, the approach improves OOD detection performance without\ncomplex modifications to the underlying model architectures. We conduct\nextensive experiments using the OpenOOD benchmark~\\cite{yang2022openood}. Our\napproach further pushes the limit of softmax-based OOD detection and is the\nleading post-hoc method for small-scale models. On a CIFAR-10 model with\nadversarial training, PRO effectively detects near-OOD inputs, achieving a\nreduction of more than 10\\% on FPR@95 compared to state-of-the-art methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18794", "pdf": "https://arxiv.org/pdf/2503.18794", "abs": "https://arxiv.org/abs/2503.18794", "authors": ["Yulong Zheng", "Zicheng Jiang", "Shengfeng He", "Yandu Sun", "Junyu Dong", "Huaidong Zhang", "Yong Du"], "title": "NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "This paper is accepted by CVPR 2025", "summary": "Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have noticeably\nadvanced photo-realistic novel view synthesis using images from densely spaced\ncamera viewpoints. However, these methods struggle in few-shot scenarios due to\nlimited supervision. In this paper, we present NexusGS, a 3DGS-based approach\nthat enhances novel view synthesis from sparse-view images by directly\nembedding depth information into point clouds, without relying on complex\nmanual regularizations. Exploiting the inherent epipolar geometry of 3DGS, our\nmethod introduces a novel point cloud densification strategy that initializes\n3DGS with a dense point cloud, reducing randomness in point placement while\npreventing over-smoothing and overfitting. Specifically, NexusGS comprises\nthree key steps: Epipolar Depth Nexus, Flow-Resilient Depth Blending, and\nFlow-Filtered Depth Pruning. These steps leverage optical flow and camera poses\nto compute accurate depth maps, while mitigating the inaccuracies often\nassociated with optical flow. By incorporating epipolar depth priors, NexusGS\nensures reliable dense point cloud coverage and supports stable 3DGS training\nunder sparse-view conditions. Experiments demonstrate that NexusGS\nsignificantly enhances depth accuracy and rendering quality, surpassing\nstate-of-the-art methods by a considerable margin. Furthermore, we validate the\nsuperiority of our generated point clouds by substantially boosting the\nperformance of competing methods. Project page:\nhttps://usmizuki.github.io/NexusGS/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18812", "pdf": "https://arxiv.org/pdf/2503.18812", "abs": "https://arxiv.org/abs/2503.18812", "authors": ["Shrikant Malviya", "Neelanjan Bhowmik", "Stamos Katsigiannis"], "title": "SKDU at De-Factify 4.0: Vision Transformer with Data Augmentation for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "De-Factify 4.0 workshop at the 39th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2025)", "summary": "The aim of this work is to explore the potential of pre-trained\nvision-language models, e.g. Vision Transformers (ViT), enhanced with advanced\ndata augmentation strategies for the detection of AI-generated images. Our\napproach leverages a fine-tuned ViT model trained on the Defactify-4.0 dataset,\nwhich includes images generated by state-of-the-art models such as Stable\nDiffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALL-E 3, and\nMidJourney. We employ perturbation techniques like flipping, rotation, Gaussian\nnoise injection, and JPEG compression during training to improve model\nrobustness and generalisation. The experimental results demonstrate that our\nViT-based pipeline achieves state-of-the-art performance, significantly\noutperforming competing methods on both validation and test datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18853", "pdf": "https://arxiv.org/pdf/2503.18853", "abs": "https://arxiv.org/abs/2503.18853", "authors": ["Xiao Cao", "Beibei Lin", "Bo Wang", "Zhiyong Huang", "Robby T. Tan"], "title": "3DSwapping: Texture Swapping For 3D Object From Single Reference Image", "categories": ["cs.CV"], "comment": null, "summary": "3D texture swapping allows for the customization of 3D object textures,\nenabling efficient and versatile visual transformations in 3D editing. While no\ndedicated method exists, adapted 2D editing and text-driven 3D editing\napproaches can serve this purpose. However, 2D editing requires frame-by-frame\nmanipulation, causing inconsistencies across views, while text-driven 3D\nediting struggles to preserve texture characteristics from reference images. To\ntackle these challenges, we introduce 3DSwapping, a 3D texture swapping method\nthat integrates: 1) progressive generation, 2) view-consistency gradient\nguidance, and 3) prompt-tuned gradient guidance. To ensure view consistency,\nour progressive generation process starts by editing a single reference image\nand gradually propagates the edits to adjacent views. Our view-consistency\ngradient guidance further reinforces consistency by conditioning the generation\nmodel on feature differences between consistent and inconsistent outputs. To\npreserve texture characteristics, we introduce prompt-tuning-based gradient\nguidance, which learns a token that precisely captures the difference between\nthe reference image and the 3D object. This token then guides the editing\nprocess, ensuring more consistent texture preservation across views. Overall,\n3DSwapping integrates these novel strategies to achieve higher-fidelity texture\ntransfer while preserving structural coherence across multiple viewpoints.\nExtensive qualitative and quantitative evaluations confirm that our three novel\ncomponents enable convincing and effective 2D texture swapping for 3D objects.\nCode will be available upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18860", "pdf": "https://arxiv.org/pdf/2503.18860", "abs": "https://arxiv.org/abs/2503.18860", "authors": ["Zunnan Xu", "Zhentao Yu", "Zixiang Zhou", "Jun Zhou", "Xiaoyu Jin", "Fa-Ting Hong", "Xiaozhong Ji", "Junwei Zhu", "Chengfei Cai", "Shiyu Tang", "Qin Lin", "Xiu Li", "Qinglin Lu"], "title": "HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce HunyuanPortrait, a diffusion-based condition control method that\nemploys implicit representations for highly controllable and lifelike portrait\nanimation. Given a single portrait image as an appearance reference and video\nclips as driving templates, HunyuanPortrait can animate the character in the\nreference image by the facial expression and head pose of the driving videos.\nIn our framework, we utilize pre-trained encoders to achieve the decoupling of\nportrait motion information and identity in videos. To do so, implicit\nrepresentation is adopted to encode motion information and is employed as\ncontrol signals in the animation phase. By leveraging the power of stable video\ndiffusion as the main building block, we carefully design adapter layers to\ninject control signals into the denoising unet through attention mechanisms.\nThese bring spatial richness of details and temporal consistency.\nHunyuanPortrait also exhibits strong generalization performance, which can\neffectively disentangle appearance and motion under different image styles. Our\nframework outperforms existing methods, demonstrating superior temporal\nconsistency and controllability. Our project is available at\nhttps://kkakkkka.github.io/HunyuanPortrait.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18880", "pdf": "https://arxiv.org/pdf/2503.18880", "abs": "https://arxiv.org/abs/2503.18880", "authors": ["Hyeonggon Ryu", "Seongyu Kim", "Joon Son Chung", "Arda Senocak"], "title": "Seeing Speech and Sound: Distinguishing and Locating Audios in Visual Scenes", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18883", "pdf": "https://arxiv.org/pdf/2503.18883", "abs": "https://arxiv.org/abs/2503.18883", "authors": ["Savas Ozkan", "Andrea Maracani", "Hyowon Kim", "Sijun Cho", "Eunchung Noh", "Jeongwon Min", "Jung Min Cho", "Mete Ozay"], "title": "Efficient and Accurate Scene Text Recognition with Cascaded-Transformers", "categories": ["cs.CV"], "comment": "Accepted to ACM-MMSys2025", "summary": "In recent years, vision transformers with text decoder have demonstrated\nremarkable performance on Scene Text Recognition (STR) due to their ability to\ncapture long-range dependencies and contextual relationships with high learning\ncapacity. However, the computational and memory demands of these models are\nsignificant, limiting their deployment in resource-constrained applications. To\naddress this challenge, we propose an efficient and accurate STR system.\nSpecifically, we focus on improving the efficiency of encoder models by\nintroducing a cascaded-transformers structure. This structure progressively\nreduces the vision token size during the encoding step, effectively eliminating\nredundant tokens and reducing computational cost. Our experimental results\nconfirm that our STR system achieves comparable performance to state-of-the-art\nbaselines while substantially decreasing computational requirements. In\nparticular, for large-models, the accuracy remains same, 92.77 to 92.68, while\ncomputational complexity is almost halved with our structure.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18903", "pdf": "https://arxiv.org/pdf/2503.18903", "abs": "https://arxiv.org/abs/2503.18903", "authors": ["Moussa Kassem Sbeyti", "Nadja Klein", "Azarm Nowzad", "Fikret Sivrikaya", "Sahin Albayrak"], "title": "Building Blocks for Robust and Effective Semi-Supervised Real-World Object Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to Transactions on Machine Learning Research (TMLR).\n  OpenReview: https://openreview.net/forum?id=vRYt8QLKqK", "summary": "Semi-supervised object detection (SSOD) based on pseudo-labeling\nsignificantly reduces dependence on large labeled datasets by effectively\nleveraging both labeled and unlabeled data. However, real-world applications of\nSSOD often face critical challenges, including class imbalance, label noise,\nand labeling errors. We present an in-depth analysis of SSOD under real-world\nconditions, uncovering causes of suboptimal pseudo-labeling and key trade-offs\nbetween label quality and quantity. Based on our findings, we propose four\nbuilding blocks that can be seamlessly integrated into an SSOD framework. Rare\nClass Collage (RCC): a data augmentation method that enhances the\nrepresentation of rare classes by creating collages of rare objects. Rare Class\nFocus (RCF): a stratified batch sampling strategy that ensures a more balanced\nrepresentation of all classes during training. Ground Truth Label Correction\n(GLC): a label refinement method that identifies and corrects false, missing,\nand noisy ground truth labels by leveraging the consistency of teacher model\npredictions. Pseudo-Label Selection (PLS): a selection method for removing\nlow-quality pseudo-labeled images, guided by a novel metric estimating the\nmissing detection rate while accounting for class rarity. We validate our\nmethods through comprehensive experiments on autonomous driving datasets,\nresulting in up to 6% increase in SSOD performance. Overall, our investigation\nand novel, data-centric, and broadly applicable building blocks enable robust\nand effective SSOD in complex, real-world scenarios. Code is available at\nhttps://mos-ks.github.io/publications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18940", "pdf": "https://arxiv.org/pdf/2503.18940", "abs": "https://arxiv.org/abs/2503.18940", "authors": ["Ye Tian", "Xin Xia", "Yuxi Ren", "Shanchuan Lin", "Xing Wang", "Xuefeng Xiao", "Yunhai Tong", "Ling Yang", "Bin Cui"], "title": "Training-free Diffusion Acceleration with Bottleneck Sampling", "categories": ["cs.CV"], "comment": "Code Repo: https://github.com/tyfeld/Bottleneck-Sampling ,Project\n  Page: https://tyfeld.github.io/BottleneckSampling.github.io/", "summary": "Diffusion models have demonstrated remarkable capabilities in visual content\ngeneration but remain challenging to deploy due to their high computational\ncost during inference. This computational burden primarily arises from the\nquadratic complexity of self-attention with respect to image or video\nresolution. While existing acceleration methods often compromise output quality\nor necessitate costly retraining, we observe that most diffusion models are\npre-trained at lower resolutions, presenting an opportunity to exploit these\nlow-resolution priors for more efficient inference without degrading\nperformance. In this work, we introduce Bottleneck Sampling, a training-free\nframework that leverages low-resolution priors to reduce computational overhead\nwhile preserving output fidelity. Bottleneck Sampling follows a high-low-high\ndenoising workflow: it performs high-resolution denoising in the initial and\nfinal stages while operating at lower resolutions in intermediate steps. To\nmitigate aliasing and blurring artifacts, we further refine the resolution\ntransition points and adaptively shift the denoising timesteps at each stage.\nWe evaluate Bottleneck Sampling on both image and video generation tasks, where\nextensive experiments demonstrate that it accelerates inference by up to\n3$\\times$ for image generation and 2.5$\\times$ for video generation, all while\nmaintaining output quality comparable to the standard full-resolution sampling\nprocess across multiple evaluation metrics. Code is available at:\nhttps://github.com/tyfeld/Bottleneck-Sampling", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18950", "pdf": "https://arxiv.org/pdf/2503.18950", "abs": "https://arxiv.org/abs/2503.18950", "authors": ["Taeksoo Kim", "Hanbyul Joo"], "title": "Target-Aware Video Diffusion Models", "categories": ["cs.CV"], "comment": "The project page is available at https://taeksuu.github.io/tavid/", "summary": "We present a target-aware video diffusion model that generates videos from an\ninput image in which an actor interacts with a specified target while\nperforming a desired action. The target is defined by a segmentation mask and\nthe desired action is described via a text prompt. Unlike existing controllable\nimage-to-video diffusion models that often rely on dense structural or motion\ncues to guide the actor's movements toward the target, our target-aware model\nrequires only a simple mask to indicate the target, leveraging the\ngeneralization capabilities of pretrained models to produce plausible actions.\nThis makes our method particularly effective for human-object interaction (HOI)\nscenarios, where providing precise action guidance is challenging, and further\nenables the use of video diffusion models for high-level action planning in\napplications such as robotics. We build our target-aware model by extending a\nbaseline model to incorporate the target mask as an additional input. To\nenforce target awareness, we introduce a special token that encodes the\ntarget's spatial information within the text prompt. We then fine-tune the\nmodel with our curated dataset using a novel cross-attention loss that aligns\nthe cross-attention maps associated with this token with the input target mask.\nTo further improve performance, we selectively apply this loss to the most\nsemantically relevant transformer blocks and attention regions. Experimental\nresults show that our target-aware model outperforms existing solutions in\ngenerating videos where actors interact accurately with the specified targets.\nWe further demonstrate its efficacy in two downstream applications: video\ncontent creation and zero-shot 3D HOI motion synthesis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17436", "pdf": "https://arxiv.org/pdf/2503.17436", "abs": "https://arxiv.org/abs/2503.17436", "authors": ["Lars Kröger", "Cristian Cioflan", "Victor Kartsch", "Luca Benini"], "title": "On-Device Federated Continual Learning on RISC-V-based Ultra-Low-Power SoC for Intelligent Nano-Drone Swarms", "categories": ["cs.LG", "cs.CV", "cs.MA", "I.2.11; I.2.6; C.5.3; I.4.9"], "comment": "2 pages, 2 tables, 1 figure. Accepted as a poster at the RISC-V\n  Summit Europe 2025", "summary": "RISC-V-based architectures are paving the way for efficient On-Device\nLearning (ODL) in smart edge devices. When applied across multiple nodes, ODL\nenables the creation of intelligent sensor networks that preserve data privacy.\nHowever, developing ODL-capable, battery-operated embedded platforms presents\nsignificant challenges due to constrained computational resources and limited\ndevice lifetime, besides intrinsic learning issues such as catastrophic\nforgetting. We face these challenges by proposing a regularization-based\nOn-Device Federated Continual Learning algorithm tailored for multiple\nnano-drones performing face recognition tasks. We demonstrate our approach on a\nRISC-V-based 10-core ultra-low-power SoC, optimizing the ODL computational\nrequirements. We improve the classification accuracy by 24% over naive\nfine-tuning, requiring 178 ms per local epoch and 10.5 s per global epoch,\ndemonstrating the effectiveness of the architecture for this task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17482", "pdf": "https://arxiv.org/pdf/2503.17482", "abs": "https://arxiv.org/abs/2503.17482", "authors": ["Keyon Vafa", "Sarah Bentley", "Jon Kleinberg", "Sendhil Mullainathan"], "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17491", "pdf": "https://arxiv.org/pdf/2503.17491", "abs": "https://arxiv.org/abs/2503.17491", "authors": ["Emanuele Giacomini", "Luca Di Giammarino", "Lorenzo De Rebotti", "Giorgio Grisetti", "Martin R. Oswald"], "title": "Splat-LOAM: Gaussian Splatting LiDAR Odometry and Mapping", "categories": ["cs.RO", "cs.CV"], "comment": "submitted to ICCV 2025", "summary": "LiDARs provide accurate geometric measurements, making them valuable for\nego-motion estimation and reconstruction tasks. Although its success, managing\nan accurate and lightweight representation of the environment still poses\nchallenges. Both classic and NeRF-based solutions have to trade off accuracy\nover memory and processing times. In this work, we build on recent advancements\nin Gaussian Splatting methods to develop a novel LiDAR odometry and mapping\npipeline that exclusively relies on Gaussian primitives for its scene\nrepresentation. Leveraging spherical projection, we drive the refinement of the\nprimitives uniquely from LiDAR measurements. Experiments show that our approach\nmatches the current registration performance, while achieving SOTA results for\nmapping tasks with minimal GPU requirements. This efficiency makes it a strong\ncandidate for further exploration and potential adoption in real-time robotics\nestimation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17646", "pdf": "https://arxiv.org/pdf/2503.17646", "abs": "https://arxiv.org/abs/2503.17646", "authors": ["Yen Cheng Chang", "Jesse Codling", "Yiwen Dong", "Jiale Zhang", "Jiasi Chen", "Hae Young Noh", "Pei Zhang"], "title": "Leveraging Audio Representations for Vibration-Based Crowd Monitoring in Stadiums", "categories": ["cs.SD", "cs.CV"], "comment": null, "summary": "Crowd monitoring in sports stadiums is important to enhance public safety and\nimprove the audience experience. Existing approaches mainly rely on cameras and\nmicrophones, which can cause significant disturbances and often raise privacy\nconcerns. In this paper, we sense floor vibration, which provides a less\ndisruptive and more non-intrusive way of crowd sensing, to predict crowd\nbehavior. However, since the vibration-based crowd monitoring approach is newly\ndeveloped, one main challenge is the lack of training data due to sports\nstadiums being large public spaces with complex physical activities.\n  In this paper, we present ViLA (Vibration Leverage Audio), a vibration-based\nmethod that reduces the dependency on labeled data by pre-training with\nunlabeled cross-modality data. ViLA is first pre-trained on audio data in an\nunsupervised manner and then fine-tuned with a minimal amount of in-domain\nvibration data. By leveraging publicly available audio datasets, ViLA learns\nthe wave behaviors from audio and then adapts the representation to vibration,\nreducing the reliance on domain-specific vibration data. Our real-world\nexperiments demonstrate that pre-training the vibration model using publicly\navailable audio data (YouTube8M) achieved up to a 5.8x error reduction compared\nto the model without audio pre-training.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17777", "pdf": "https://arxiv.org/pdf/2503.17777", "abs": "https://arxiv.org/abs/2503.17777", "authors": ["Lei Guo", "Wei Chen", "Yuxuan Sun", "Bo Ai", "Nikolaos Pappas", "Tony Quek"], "title": "Hierarchy-Aware and Channel-Adaptive Semantic Communication for Bandwidth-Limited Data Fusion", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by the WCL", "summary": "Obtaining high-resolution hyperspectral images (HR-HSI) is costly and\ndata-intensive, making it necessary to fuse low-resolution hyperspectral images\n(LR-HSI) with high-resolution RGB images (HR-RGB) for practical applications.\nHowever, traditional fusion techniques, which integrate detailed information\ninto the reconstruction, significantly increase bandwidth consumption compared\nto directly transmitting raw data. To overcome these challenges, we propose a\nhierarchy-aware and channel-adaptive semantic communication approach for\nbandwidth-limited data fusion. A hierarchical correlation module is proposed to\npreserve both the overall structural information and the details of the image\nrequired for super-resolution. This module efficiently combines deep semantic\nand shallow features from LR-HSI and HR-RGB. To further reduce bandwidth usage\nwhile preserving reconstruction quality, a channel-adaptive attention mechanism\nbased on Transformer is proposed to dynamically integrate and transmit the deep\nand shallow features, enabling efficient data transmission and high-quality\nHR-HSI reconstruction. Experimental results on the CAVE and Washington DC Mall\ndatasets demonstrate that our method outperforms single-source transmission,\nachieving up to a 2 dB improvement in peak signal-to-noise ratio (PSNR).\nAdditionally, it reduces bandwidth consumption by two-thirds, confirming its\neffectiveness in bandwidth-constrained environments for HR-HSI reconstruction\ntasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17786", "pdf": "https://arxiv.org/pdf/2503.17786", "abs": "https://arxiv.org/abs/2503.17786", "authors": ["Tommaso Di Noto", "Sofyan Jankowski", "Francesco Puccinelli", "Guillaume Marie", "Sebastien Tourbier", "Yasser Aleman-Gomez", "Oscar Esteban", "Ricardo Corredor-Jerez", "Guillaume Saliou", "Patric Hagmann", "Meritxell Bach Cuadra", "Jonas Richiardi"], "title": "Assessing workflow impact and clinical utility of AI-assisted brain aneurysm detection: a multi-reader study", "categories": ["eess.IV", "cs.CV"], "comment": "Paper under review with a Journal in the medical imaging field", "summary": "Despite the plethora of AI-based algorithms developed for anomaly detection\nin radiology, subsequent integration into clinical setting is rarely evaluated.\nIn this work, we assess the applicability and utility of an AI-based model for\nbrain aneurysm detection comparing the performance of two readers with\ndifferent levels of experience (2 and 13 years). We aim to answer the following\nquestions: 1) Do the readers improve their performance when assisted by the AI\nalgorithm? 2) How much does the AI algorithm impact routine clinical workflow?\nWe reuse and enlarge our open-access, Time-Of-Flight Magnetic Resonance\nAngiography dataset (N=460). We use 360 subjects for training/validating our\nalgorithm and 100 as unseen test set for the reading session. Even though our\nmodel reaches state-of-the-art results on the test set (sensitivity=74%, false\npositive rate=1.6), we show that neither the junior nor the senior reader\nsignificantly increase their sensitivity (p=0.59, p=1, respectively). In\naddition, we find that reading time for both readers is significantly higher in\nthe \"AI-assisted\" setting than in the \"Unassisted\" (+15 seconds, on average;\np=3x10^(-4) junior, p=3x10^(-5) senior). The confidence reported by the readers\nis unchanged across the two settings, indicating that the AI assistance does\nnot influence the certainty of the diagnosis. Our findings highlight the\nimportance of clinical validation of AI algorithms in a clinical setting\ninvolving radiologists. This study should serve as a reminder to the community\nto always examine the real-word effectiveness and workflow impact of proposed\nalgorithms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17831", "pdf": "https://arxiv.org/pdf/2503.17831", "abs": "https://arxiv.org/abs/2503.17831", "authors": ["Qingshan Hou", "Meng Wang", "Peng Cao", "Zou Ke", "Xiaoli Liu", "Huazhu Fu", "Osmar R. Zaiane"], "title": "FundusGAN: A Hierarchical Feature-Aware Generative Framework for High-Fidelity Fundus Image Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in ophthalmology foundation models such as RetFound have\ndemonstrated remarkable diagnostic capabilities but require massive datasets\nfor effective pre-training, creating significant barriers for development and\ndeployment. To address this critical challenge, we propose FundusGAN, a novel\nhierarchical feature-aware generative framework specifically designed for\nhigh-fidelity fundus image synthesis. Our approach leverages a Feature Pyramid\nNetwork within its encoder to comprehensively extract multi-scale information,\ncapturing both large anatomical structures and subtle pathological features.\nThe framework incorporates a modified StyleGAN-based generator with dilated\nconvolutions and strategic upsampling adjustments to preserve critical retinal\nstructures while enhancing pathological detail representation. Comprehensive\nevaluations on the DDR, DRIVE, and IDRiD datasets demonstrate that FundusGAN\nconsistently outperforms state-of-the-art methods across multiple metrics\n(SSIM: 0.8863, FID: 54.2, KID: 0.0436 on DDR). Furthermore, disease\nclassification experiments reveal that augmenting training data with\nFundusGAN-generated images significantly improves diagnostic accuracy across\nmultiple CNN architectures (up to 6.49\\% improvement with ResNet50). These\nresults establish FundusGAN as a valuable foundation model component that\neffectively addresses data scarcity challenges in ophthalmological AI research,\nenabling more robust and generalizable diagnostic systems while reducing\ndependency on large-scale clinical data collection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17914", "pdf": "https://arxiv.org/pdf/2503.17914", "abs": "https://arxiv.org/abs/2503.17914", "authors": ["Jianjian Yin", "Tao Chen", "Gensheng Pei", "Yazhou Yao", "Liqiang Nie", "Xiansheng Hua"], "title": "Semi-supervised Semantic Segmentation with Multi-Constraint Consistency Learning", "categories": ["cs.MM", "cs.CV"], "comment": "accepted by IEEE Transactions on Multimedia", "summary": "Consistency regularization has prevailed in semi-supervised semantic\nsegmentation and achieved promising performance. However, existing methods\ntypically concentrate on enhancing the Image-augmentation based Prediction\nconsistency and optimizing the segmentation network as a whole, resulting in\ninsufficient utilization of potential supervisory information. In this paper,\nwe propose a Multi-Constraint Consistency Learning (MCCL) approach to\nfacilitate the staged enhancement of the encoder and decoder. Specifically, we\nfirst design a feature knowledge alignment (FKA) strategy to promote the\nfeature consistency learning of the encoder from image-augmentation. Our FKA\nencourages the encoder to derive consistent features for strongly and weakly\naugmented views from the perspectives of point-to-point alignment and\nprototype-based intra-class compactness. Moreover, we propose a self-adaptive\nintervention (SAI) module to increase the discrepancy of aligned intermediate\nfeature representations, promoting Feature-perturbation based Prediction\nconsistency learning. Self-adaptive feature masking and noise injection are\ndesigned in an instance-specific manner to perturb the features for robust\nlearning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes\ndatasets demonstrate that our proposed MCCL achieves new state-of-the-art\nperformance. The source code and models are made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/MCCL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17987", "pdf": "https://arxiv.org/pdf/2503.17987", "abs": "https://arxiv.org/abs/2503.17987", "authors": ["Chenyu Zhang", "Yiwen Ma", "Lanjun Wang", "Wenhui Li", "Yi Tu", "An-An Liu"], "title": "Metaphor-based Jailbreaking Attacks on Text-to-Image Models", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "13 page3, 4 figures. This paper includes model-generated content that\n  may contain offensive or distressing material", "summary": "To mitigate misuse, text-to-image~(T2I) models commonly incorporate safety\nfilters to prevent the generation of sensitive images. Unfortunately, recent\njailbreaking attack methods use LLMs to generate adversarial prompts that\neffectively bypass safety filters while generating sensitive images, revealing\nthe safety vulnerabilities within the T2I model. However, existing LLM-based\nattack methods lack explicit guidance, relying on substantial queries to\nachieve a successful attack, which limits their practicality in real-world\nscenarios. In this work, we introduce \\textbf{MJA}, a \\textbf{m}etaphor-based\n\\textbf{j}ailbreaking \\textbf{a}ttack method inspired by the Taboo game, aiming\nto balance the attack effectiveness and query efficiency by generating\nmetaphor-based adversarial prompts. Specifically, MJA consists of two modules:\nan LLM-based multi-agent generation module~(MLAG) and an adversarial prompt\noptimization module~(APO). MLAG decomposes the generation of metaphor-based\nadversarial prompts into three subtasks: metaphor retrieval, context matching,\nand adversarial prompt generation. Subsequently, MLAG coordinates three\nLLM-based agents to generate diverse adversarial prompts by exploring various\nmetaphors and contexts. To enhance the attack efficiency, APO first trains a\nsurrogate model to predict the attack results of adversarial prompts and then\ndesigns an acquisition strategy to adaptively identify optimal adversarial\nprompts. Experiments demonstrate that MJA achieves better attack effectiveness\nwhile requiring fewer queries compared to baseline methods. Moreover, our\nadversarial prompts exhibit strong transferability across various open-source\nand commercial T2I models. \\textcolor{red}{This paper includes model-generated\ncontent that may contain offensive or distressing material.}", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18064", "pdf": "https://arxiv.org/pdf/2503.18064", "abs": "https://arxiv.org/abs/2503.18064", "authors": ["Xiaoming Qi", "Jingyang Zhang", "Huazhu Fu", "Guanyu Yang", "Shuo Li", "Yueming Jin"], "title": "Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for FCL", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (\\textbf{FedDAH}). It is to facilitate\ncollaborative learning under the distinct and dynamic task streams across\nclients. To alleviate the catastrophic forgetting, we propose a dynamic\nallocation hypernetwork (DAHyper) where a continually updated hypernetwork is\ndesigned to manage the mapping between task identities and their associated\nmodel parameters, enabling the dynamic allocation of the model across clients.\nFor the biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https://github.com/jinlab-imvr/FedDAH.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18108", "pdf": "https://arxiv.org/pdf/2503.18108", "abs": "https://arxiv.org/abs/2503.18108", "authors": ["Junhao Ge", "Zuhong Liu", "Longteng Fan", "Yifan Jiang", "Jiaqi Su", "Yiming Li", "Zhejun Zhang", "Siheng Chen"], "title": "Unraveling the Effects of Synthetic Data on End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end (E2E) autonomous driving (AD) models require diverse, high-quality\ndata to perform well across various driving scenarios. However, collecting\nlarge-scale real-world data is expensive and time-consuming, making\nhigh-fidelity synthetic data essential for enhancing data diversity and model\nrobustness. Existing driving simulators for synthetic data generation have\nsignificant limitations: game-engine-based simulators struggle to produce\nrealistic sensor data, while NeRF-based and diffusion-based methods face\nefficiency challenges. Additionally, recent simulators designed for closed-loop\nevaluation provide limited interaction with other vehicles, failing to simulate\ncomplex real-world traffic dynamics. To address these issues, we introduce\nSceneCrafter, a realistic, interactive, and efficient AD simulator based on 3D\nGaussian Splatting (3DGS). SceneCrafter not only efficiently generates\nrealistic driving logs across diverse traffic scenarios but also enables robust\nclosed-loop evaluation of end-to-end models. Experimental results demonstrate\nthat SceneCrafter serves as both a reliable evaluation platform and a efficient\ndata generator that significantly improves end-to-end model generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18162", "pdf": "https://arxiv.org/pdf/2503.18162", "abs": "https://arxiv.org/abs/2503.18162", "authors": ["Hui Xue", "Sarah M. Hooper", "Iain Pierce", "Rhodri H. Davies", "John Stairs", "Joseph Naegele", "Adrienne E. Campbell-Washburn", "Charlotte Manisty", "James C. Moon", "Thomas A. Treibel", "Peter Kellman", "Michael S. Hansen"], "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation", "categories": ["physics.med-ph", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18275", "pdf": "https://arxiv.org/pdf/2503.18275", "abs": "https://arxiv.org/abs/2503.18275", "authors": ["Xulang Liu", "Ning Tan"], "title": "GI-SLAM: Gaussian-Inertial SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "10 pages, 2 figures, 5 tables", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful\nrepresentation of geometry and appearance for dense Simultaneous Localization\nand Mapping (SLAM). Through rapid, differentiable rasterization of 3D\nGaussians, many 3DGS SLAM methods achieve near real-time rendering and\naccelerated training. However, these methods largely overlook inertial data,\nwitch is a critical piece of information collected from the inertial\nmeasurement unit (IMU). In this paper, we present GI-SLAM, a novel\ngaussian-inertial SLAM system which consists of an IMU-enhanced camera tracking\nmodule and a realistic 3D Gaussian-based scene representation for mapping. Our\nmethod introduces an IMU loss that seamlessly integrates into the deep learning\nframework underpinning 3D Gaussian Splatting SLAM, effectively enhancing the\naccuracy, robustness and efficiency of camera tracking. Moreover, our SLAM\nsystem supports a wide range of sensor configurations, including monocular,\nstereo, and RGBD cameras, both with and without IMU integration. Our method\nachieves competitive performance compared with existing state-of-the-art\nreal-time methods on the EuRoC and TUM-RGBD datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18462", "pdf": "https://arxiv.org/pdf/2503.18462", "abs": "https://arxiv.org/abs/2503.18462", "authors": ["Tadeusz Dziarmaga", "Marcin Kądziołka", "Artur Kasymov", "Marcin Mazur"], "title": "PALATE: Peculiar Application of the Law of Total Expectation to Enhance the Evaluation of Deep Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep generative models (DGMs) have caused a paradigm shift in the field of\nmachine learning, yielding noteworthy advancements in domains such as image\nsynthesis, natural language processing, and other related areas. However, a\ncomprehensive evaluation of these models that accounts for the trichotomy\nbetween fidelity, diversity, and novelty in generated samples remains a\nformidable challenge. A recently introduced solution that has emerged as a\npromising approach in this regard is the Feature Likelihood Divergence (FLD), a\nmethod that offers a theoretically motivated practical tool, yet also exhibits\nsome computational challenges. In this paper, we propose PALATE, a novel\nenhancement to the evaluation of DGMs that addresses limitations of existing\nmetrics. Our approach is based on a peculiar application of the law of total\nexpectation to random variables representing accessible real data. When\ncombined with the MMD baseline metric and DINOv2 feature extractor, PALATE\noffers a holistic evaluation framework that matches or surpasses\nstate-of-the-art solutions while providing superior computational efficiency\nand scalability to large-scale datasets. Through a series of experiments, we\ndemonstrate the effectiveness of the PALATE enhancement, contributing a\ncomputationally efficient, holistic evaluation approach that advances the field\nof DGMs assessment, especially in detecting sample memorization and evaluating\ngeneralization capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18528", "pdf": "https://arxiv.org/pdf/2503.18528", "abs": "https://arxiv.org/abs/2503.18528", "authors": ["Moein Sorkhei", "Christos Matsoukas", "Johan Fredin Haslum", "Kevin Smith"], "title": "k-NN as a Simple and Effective Estimator of Transferability", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "How well can one expect transfer learning to work in a new setting where the\ndomain is shifted, the task is different, and the architecture changes? Many\ntransfer learning metrics have been proposed to answer this question. But how\naccurate are their predictions in a realistic new setting? We conducted an\nextensive evaluation involving over 42,000 experiments comparing 23\ntransferability metrics across 16 different datasets to assess their ability to\npredict transfer performance. Our findings reveal that none of the existing\nmetrics perform well across the board. However, we find that a simple k-nearest\nneighbor evaluation -- as is commonly used to evaluate feature quality for\nself-supervision -- not only surpasses existing metrics, but also offers better\ncomputational efficiency and ease of implementation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18752", "pdf": "https://arxiv.org/pdf/2503.18752", "abs": "https://arxiv.org/abs/2503.18752", "authors": ["Der-Hau Lee"], "title": "Robust Tube-based Control Strategy for Vision-guided Autonomous Vehicles", "categories": ["eess.SY", "cs.CV", "cs.RO", "cs.SY"], "comment": "13 pages, 14 figures", "summary": "A robust control strategy for autonomous vehicles can improve system\nstability, enhance riding comfort, and prevent driving accidents. This paper\npresents a novel interpolation tube-based constrained iterative linear\nquadratic regulator (itube-CILQR) algorithm for autonomous\ncomputer-vision-based vehicle lane-keeping. The goal of the algorithm is to\nenhance robustness during high-speed cornering on tight turns. The advantages\nof itube-CILQR over the standard tube-approach include reduced system\nconservatism and increased computational speed. Numerical and vision-based\nexperiments were conducted to examine the feasibility of the proposed\nalgorithm. The proposed itube-CILQR algorithm is better suited to vehicle\nlane-keeping than variational CILQR-based methods and model predictive control\n(MPC) approaches using a classical interior-point solver. Specifically, in\nevaluation experiments, itube-CILQR achieved an average runtime of 3.16 ms to\ngenerate a control signal to guide a self-driving vehicle; itube-MPC typically\nrequired a 4.67-times longer computation time to complete the same task.\nMoreover, the influence of conservatism on system behavior was investigated by\nexploring the interpolation variable trajectories derived from the proposed\nitube-CILQR algorithm during lane-keeping maneuvers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18836", "pdf": "https://arxiv.org/pdf/2503.18836", "abs": "https://arxiv.org/abs/2503.18836", "authors": ["Yuxuan Zhang", "Jinkui Hao", "Bo Zhou"], "title": "Dual-domain Multi-path Self-supervised Diffusion Model for Accelerated MRI Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 8 figures, 5 tables", "summary": "Magnetic resonance imaging (MRI) is a vital diagnostic tool, but its\ninherently long acquisition times reduce clinical efficiency and patient\ncomfort. Recent advancements in deep learning, particularly diffusion models,\nhave improved accelerated MRI reconstruction. However, existing diffusion\nmodels' training often relies on fully sampled data, models incur high\ncomputational costs, and often lack uncertainty estimation, limiting their\nclinical applicability. To overcome these challenges, we propose a novel\nframework, called Dual-domain Multi-path Self-supervised Diffusion Model\n(DMSM), that integrates a self-supervised dual-domain diffusion model training\nscheme, a lightweight hybrid attention network for the reconstruction diffusion\nmodel, and a multi-path inference strategy, to enhance reconstruction accuracy,\nefficiency, and explainability. Unlike traditional diffusion-based models, DMSM\neliminates the dependency on training from fully sampled data, making it more\npractical for real-world clinical settings. We evaluated DMSM on two human MRI\ndatasets, demonstrating that it achieves favorable performance over several\nsupervised and self-supervised baselines, particularly in preserving fine\nanatomical structures and suppressing artifacts under high acceleration\nfactors. Additionally, our model generates uncertainty maps that correlate\nreasonably well with reconstruction errors, offering valuable clinically\ninterpretable guidance and potentially enhancing diagnostic confidence.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18840", "pdf": "https://arxiv.org/pdf/2503.18840", "abs": "https://arxiv.org/abs/2503.18840", "authors": ["Meva Himmetoglu", "Ilja Ciernik", "Ender Konukoglu"], "title": "Learning to segment anatomy and lesions from disparately labeled sources in brain MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Segmenting healthy tissue structures alongside lesions in brain Magnetic\nResonance Images (MRI) remains a challenge for today's algorithms due to\nlesion-caused disruption of the anatomy and lack of jointly labeled training\ndatasets, where both healthy tissues and lesions are labeled on the same\nimages. In this paper, we propose a method that is robust to lesion-caused\ndisruptions and can be trained from disparately labeled training sets, i.e.,\nwithout requiring jointly labeled samples, to automatically segment both. In\ncontrast to prior work, we decouple healthy tissue and lesion segmentation in\ntwo paths to leverage multi-sequence acquisitions and merge information with an\nattention mechanism. During inference, an image-specific adaptation reduces\nadverse influences of lesion regions on healthy tissue predictions. During\ntraining, the adaptation is taken into account through meta-learning and\nco-training is used to learn from disparately labeled training images. Our\nmodel shows an improved performance on several anatomical structures and\nlesions on a publicly available brain glioblastoma dataset compared to the\nstate-of-the-art segmentation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
