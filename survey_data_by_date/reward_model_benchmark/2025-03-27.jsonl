{"id": "2503.20745", "pdf": "https://arxiv.org/pdf/2503.20745", "abs": "https://arxiv.org/abs/2503.20745", "authors": ["Yanpeng Sun", "Shan Zhang", "Wei Tang", "Aotian Chen", "Piotr Koniusz", "Kai Zou", "Yuan Xue", "Anton van den Hengel"], "title": "MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams", "categories": ["cs.CV"], "comment": null, "summary": "Diagrams serve as a fundamental form of visual language, representing complex\nconcepts and their inter-relationships through structured symbols, shapes, and\nspatial arrangements. Unlike natural images, their inherently symbolic and\nabstract nature poses significant challenges for Multimodal Large Language\nModels (MLLMs). However, current benchmarks conflate perceptual and reasoning\ntasks, making it difficult to assess whether MLLMs genuinely understand\nmathematical diagrams beyond superficial pattern recognition. To address this\ngap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and\nevaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and\n1.6K carefully curated questions spanning four perception tasks: shape\nclassification, object counting, relationship identification, and object\ngrounding, covering diverse domains including plane geometry, solid geometry,\nand graphical representations. Our evaluation of MLLMs reveals that their\nability to understand diagrams is notably limited, particularly in fine-grained\ngrounding tasks. In response, we construct GeoPeP, a perception-oriented\ndataset of 200K structured geometry image-text pairs explicitly annotated with\ngeometric primitives and precise spatial relationships. Training MLLM on GeoPeP\nleads to significant gains in perceptual accuracy, which in turn substantially\nimproves mathematical reasoning. Our benchmark and dataset establish critical\nstandards for evaluating and advancing multimodal mathematical understanding,\nproviding valuable resources and insights to foster future MLLM research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "mathematical reasoning", "fine-grained"], "score": 6}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference learning", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "helpfulness", "harmlessness", "safety", "accuracy"], "score": 5}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference learning", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "helpfulness", "harmlessness", "safety", "accuracy"], "score": 5}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20715", "pdf": "https://arxiv.org/pdf/2503.20715", "abs": "https://arxiv.org/abs/2503.20715", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL SRW 2025", "summary": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "aspect-based"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20749", "pdf": "https://arxiv.org/pdf/2503.20749", "abs": "https://arxiv.org/abs/2503.20749", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bennet Bei", "Yaochen Xie", "Dakuo Wang", "Jessie Wang", "Qi He"], "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20290", "pdf": "https://arxiv.org/pdf/2503.20290", "abs": "https://arxiv.org/abs/2503.20290", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Yu Tsao", "Junichi Yamagishi", "Yuxuan Wang", "Chao Zhang"], "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "23 pages, 16 figures", "summary": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20428", "pdf": "https://arxiv.org/pdf/2503.20428", "abs": "https://arxiv.org/abs/2503.20428", "authors": ["F. Xavier Gaya-Morey", "Cristina Manresa-Yee", "Célia Martinie", "Jose M. Buades-Rubio"], "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20047", "pdf": "https://arxiv.org/pdf/2503.20047", "abs": "https://arxiv.org/abs/2503.20047", "authors": ["Yu Xin", "Gorkem Can Ates", "Kuang Gong", "Wei Shao"], "title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20306", "pdf": "https://arxiv.org/pdf/2503.20306", "abs": "https://arxiv.org/abs/2503.20306", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Kalyan Sivasailam", "Anandakumar D", "Elakkiya R", "Harsha KG", "Rithanya V", "Harini T", "Afshin Hussain", "Kishore Prasath Venkatesh"], "title": "3D Convolutional Neural Networks for Improved Detection of Intracranial bleeding in CT Imaging", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "12 pages,4 figures", "summary": "Background: Intracranial bleeding (IB) is a life-threatening condition caused\nby traumatic brain injuries, including epidural, subdural, subarachnoid, and\nintraparenchymal hemorrhages. Rapid and accurate detection is crucial to\nprevent severe complications. Traditional imaging can be slow and prone to\nvariability, especially in high-pressure scenarios. Artificial Intelligence\n(AI) provides a solution by quickly analyzing medical images, identifying\nsubtle hemorrhages, and flagging urgent cases. By enhancing diagnostic speed\nand accuracy, AI improves workflows and patient care. This article explores\nAI's role in transforming IB detection in emergency settings.\n  Methods: A U-shaped 3D Convolutional Neural Network (CNN) automates IB\ndetection and classification in volumetric CT scans. Advanced preprocessing,\nincluding CLAHE and intensity normalization, enhances image quality. The\narchitecture preserves spatial and contextual details for precise segmentation.\nA dataset of 2,912 annotated CT scans was used for training and evaluation.\n  Results: The model achieved high performance across major bleed types, with\nprecision, recall, and accuracy exceeding 90 percent in most cases 96 percent\nprecision for epidural hemorrhages and 94 percent accuracy for subarachnoid\nhemorrhages. Its ability to classify and localize hemorrhages highlights its\nclinical reliability.\n  Conclusion: This U-shaped 3D CNN offers a scalable solution for automating IB\ndetection, reducing diagnostic delays, and improving emergency care outcomes.\nFuture work will expand dataset diversity, optimize real-time processing, and\nintegrate multimodal data for enhanced clinical applicability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19936", "pdf": "https://arxiv.org/pdf/2503.19936", "abs": "https://arxiv.org/abs/2503.19936", "authors": ["Kelaiti Xiao", "Liang Yang", "Paerhati Tulajiang", "Hongfei Lin"], "title": "VisualQuest: A Diverse Image Dataset for Evaluating Visual Recognition in LLMs", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces VisualQuest, a novel image dataset designed to assess\nthe ability of large language models (LLMs) to interpret non-traditional,\nstylized imagery. Unlike conventional photographic benchmarks, VisualQuest\nchallenges models with images that incorporate abstract, symbolic, and\nmetaphorical elements, requiring the integration of domain-specific knowledge\nand advanced reasoning. The dataset was meticulously curated through multiple\nstages of filtering, annotation, and standardization to ensure high quality and\ndiversity. Our evaluations using several state-of-the-art multimodal LLMs\nreveal significant performance variations that underscore the importance of\nboth factual background knowledge and inferential capabilities in visual\nrecognition tasks. VisualQuest thus provides a robust and comprehensive\nbenchmark for advancing research in multimodal reasoning and model architecture\ndesign.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20182", "pdf": "https://arxiv.org/pdf/2503.20182", "abs": "https://arxiv.org/abs/2503.20182", "authors": ["Huanhuan Ma", "Haisong Gong", "Xiaoyuan Yi", "Xing Xie", "Dongkuan Xu"], "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Code available via https://github.com/dependentsign/CSI", "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "reliability"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20202", "pdf": "https://arxiv.org/pdf/2503.20202", "abs": "https://arxiv.org/abs/2503.20202", "authors": ["Nan Gao", "Yihua Bao", "Dongdong Weng", "Jiayi Zhao", "Jia Li", "Yan Zhou", "Pengfei Wan", "Di Zhang"], "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO"], "comment": null, "summary": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20000", "pdf": "https://arxiv.org/pdf/2503.20000", "abs": "https://arxiv.org/abs/2503.20000", "authors": ["Jonathan Sauder", "Viktor Domazetoski", "Guilhem Banc-Prandi", "Gabriela Perna", "Anders Meibom", "Devis Tuia"], "title": "The Coralscapes Dataset: Semantic Scene Understanding in Coral Reefs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Coral reefs are declining worldwide due to climate change and local\nstressors. To inform effective conservation or restoration, monitoring at the\nhighest possible spatial and temporal resolution is necessary. Conventional\ncoral reef surveying methods are limited in scalability due to their reliance\non expert labor time, motivating the use of computer vision tools to automate\nthe identification and abundance estimation of live corals from images.\nHowever, the design and evaluation of such tools has been impeded by the lack\nof large high quality datasets. We release the Coralscapes dataset, the first\ngeneral-purpose dense semantic segmentation dataset for coral reefs, covering\n2075 images, 39 benthic classes, and 174k segmentation masks annotated by\nexperts. Coralscapes has a similar scope and the same structure as the widely\nused Cityscapes dataset for urban scene segmentation, allowing benchmarking of\nsemantic segmentation models in a new challenging domain which requires expert\nknowledge to annotate. We benchmark a wide range of semantic segmentation\nmodels, and find that transfer learning from Coralscapes to existing smaller\ndatasets consistently leads to state-of-the-art performance. Coralscapes will\ncatalyze research on efficient, scalable, and standardized coral reef surveying\nmethods based on computer vision, and holds the potential to streamline the\ndevelopment of underwater ecological robotics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20108", "pdf": "https://arxiv.org/pdf/2503.20108", "abs": "https://arxiv.org/abs/2503.20108", "authors": ["Xavier Merino", "Gabriella Pangelinan", "Samuel Langborgh", "Michael C. King", "Kevin W. Bowyer"], "title": "Peepers & Pixels: Human Recognition Accuracy on Low Resolution Faces", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "Automated one-to-many (1:N) face recognition is a powerful investigative tool\ncommonly used by law enforcement agencies. In this context, potential matches\nresulting from automated 1:N recognition are reviewed by human examiners prior\nto possible use as investigative leads. While automated 1:N recognition can\nachieve near-perfect accuracy under ideal imaging conditions, operational\nscenarios may necessitate the use of surveillance imagery, which is often\ndegraded in various quality dimensions. One important quality dimension is\nimage resolution, typically quantified by the number of pixels on the face. The\ncommon metric for this is inter-pupillary distance (IPD), which measures the\nnumber of pixels between the pupils. Low IPD is known to degrade the accuracy\nof automated face recognition. However, the threshold IPD for reliability in\nhuman face recognition remains undefined. This study aims to explore the\nboundaries of human recognition accuracy by systematically testing accuracy\nacross a range of IPD values. We find that at low IPDs (10px, 5px), human\naccuracy is at or below chance levels (50.7%, 35.9%), even as confidence in\ndecision-making remains relatively high (77%, 70.7%). Our findings indicate\nthat, for low IPD images, human recognition ability could be a limiting factor\nto overall system accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "dimension"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20209", "pdf": "https://arxiv.org/pdf/2503.20209", "abs": "https://arxiv.org/abs/2503.20209", "authors": ["Chengyang Hu", "Yuduo Chen", "Lizhuang Ma"], "title": "BEAR: A Video Dataset For Fine-grained Behaviors Recognition Oriented with Action and Environment Factors", "categories": ["cs.CV"], "comment": "Accept by ICME2025", "summary": "Behavior recognition is an important task in video representation learning.\nAn essential aspect pertains to effective feature learning conducive to\nbehavior recognition. Recently, researchers have started to study fine-grained\nbehavior recognition, which provides similar behaviors and encourages the model\nto concern with more details of behaviors with effective features for\ndistinction. However, previous fine-grained behaviors limited themselves to\ncontrolling partial information to be similar, leading to an unfair and not\ncomprehensive evaluation of existing works. In this work, we develop a new\nvideo fine-grained behavior dataset, named BEAR, which provides fine-grained\n(i.e. similar) behaviors that uniquely focus on two primary factors defining\nbehavior: Environment and Action. It includes two fine-grained behavior\nprotocols including Fine-grained Behavior with Similar Environments and\nFine-grained Behavior with Similar Actions as well as multiple sub-protocols as\ndifferent scenarios. Furthermore, with this new dataset, we conduct multiple\nexperiments with different behavior recognition models. Our research primarily\nexplores the impact of input modality, a critical element in studying the\nenvironmental and action-based aspects of behavior recognition. Our\nexperimental results yield intriguing insights that have substantial\nimplications for further research endeavors.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20648", "pdf": "https://arxiv.org/pdf/2503.20648", "abs": "https://arxiv.org/abs/2503.20648", "authors": ["Raj Sanjay Shah", "Lei Xu", "Qianchu Liu", "Jon Burnsky", "Drew Bertagnolli", "Chaitanya Shivade"], "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "rubric"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20235", "pdf": "https://arxiv.org/pdf/2503.20235", "abs": "https://arxiv.org/abs/2503.20235", "authors": ["Ahyun Seo", "Minsu Cho"], "title": "Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Symmetry plays a vital role in understanding structural patterns, aiding\nobject recognition and scene interpretation. This paper focuses on rotation\nsymmetry, where objects remain unchanged when rotated around a central axis,\nrequiring detection of rotation centers and supporting vertices. Traditional\nmethods relied on hand-crafted feature matching, while recent segmentation\nmodels based on convolutional neural networks detect rotation centers but\nstruggle with 3D geometric consistency due to viewpoint distortions. To\novercome this, we propose a model that directly predicts rotation centers and\nvertices in 3D space and projects the results back to 2D while preserving\nstructural integrity. By incorporating a vertex reconstruction stage enforcing\n3D geometric priors -- such as equal side lengths and interior angles -- our\nmodel enhances robustness and accuracy. Experiments on the DENDI dataset show\nsuperior performance in rotation axis detection and validate the impact of 3D\npriors through ablation studies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20786", "pdf": "https://arxiv.org/pdf/2503.20786", "abs": "https://arxiv.org/abs/2503.20786", "authors": ["Sondos Mahmoud Bsharat", "Mukul Ranjan", "Aidar Myrzakhan", "Jiacheng Liu", "Bowei Guo", "Shengkun Tang", "Zhuang Liu", "Yuanzhi Li", "Zhiqiang Shen"], "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU", "summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20201", "pdf": "https://arxiv.org/pdf/2503.20201", "abs": "https://arxiv.org/abs/2503.20201", "authors": ["Salaheddin Alzubi", "Creston Brooks", "Purva Chiniya", "Edoardo Contente", "Chiara von Gerlach", "Lucas Irwin", "Yihan Jiang", "Arda Kaz", "Windsor Nguyen", "Sewoong Oh", "Himanshu Tyagi", "Pramod Viswanath"], "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "27 pages, 8 figures, 4 tables", "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20673", "pdf": "https://arxiv.org/pdf/2503.20673", "abs": "https://arxiv.org/abs/2503.20673", "authors": ["Yinan Sun", "Xiongkuo Min", "Zicheng Zhang", "Yixuan Gao", "Yuqin Cao", "Guangtao Zhai"], "title": "Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20782", "pdf": "https://arxiv.org/pdf/2503.20782", "abs": "https://arxiv.org/abs/2503.20782", "authors": ["Yan-Bo Lin", "Kevin Lin", "Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Chung-Ching Lin", "Xiaofei Wang", "Gedas Bertasius", "Lijuan Wang"], "title": "Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Project page: https://genjib.github.io/project_page/AVED/index.html", "summary": "In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https://genjib.github.io/project_page/AVED/index.html", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20653", "pdf": "https://arxiv.org/pdf/2503.20653", "abs": "https://arxiv.org/abs/2503.20653", "authors": ["Antoine Schieb", "Bilal Hadjadji", "Daniel Tshokola Mweze", "Natalia Fernanda Valderrama", "Valentin Derangère", "Laurent Arnould", "Sylvain Ladoire", "Alain Lalande", "Louis-Oscar Morel", "Nathan Vinçon"], "title": "UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20758", "pdf": "https://arxiv.org/pdf/2503.20758", "abs": "https://arxiv.org/abs/2503.20758", "authors": ["Shakiba Rahimiaghdam", "Hande Alemdar"], "title": "MindfulLIME: A Stable Solution for Explanations of Machine Learning Models with Enhanced Localization Precision -- A Medical Image Case Study", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "Ensuring transparency in machine learning decisions is critically important,\nespecially in sensitive sectors such as healthcare, finance, and justice.\nDespite this, some popular explainable algorithms, such as Local Interpretable\nModel-agnostic Explanations (LIME), often produce unstable explanations due to\nthe random generation of perturbed samples. Random perturbation introduces\nsmall changes or noise to modified instances of the original data, leading to\ninconsistent explanations. Even slight variations in the generated samples\nsignificantly affect the explanations provided by such models, undermining\ntrust and hindering the adoption of interpretable models. To address this\nchallenge, we propose MindfulLIME, a novel algorithm that intelligently\ngenerates purposive samples using a graph-based pruning algorithm and\nuncertainty sampling. MindfulLIME substantially improves the consistency of\nvisual explanations compared to random sampling approaches. Our experimental\nevaluation, conducted on a widely recognized chest X-ray dataset, confirms\nMindfulLIME's stability with a 100% success rate in delivering reliable\nexplanations under identical conditions. Additionally, MindfulLIME improves the\nlocalization precision of visual explanations by reducing the distance between\nthe generated explanations and the actual local annotations compared to LIME.\nWe also performed comprehensive experiments considering various segmentation\nalgorithms and sample numbers, focusing on stability, quality, and efficiency.\nThe results demonstrate the outstanding performance of MindfulLIME across\ndifferent segmentation settings, generating fewer high-quality samples within a\nreasonable processing time. By addressing the stability limitations of LIME in\nimage data, MindfulLIME enhances the trustworthiness and interpretability of\nmachine learning models in specific medical imaging applications, a critical\ndomain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19979", "pdf": "https://arxiv.org/pdf/2503.19979", "abs": "https://arxiv.org/abs/2503.19979", "authors": ["Enora Rice", "Ali Marashian", "Hannah Haynie", "Katharina von der Wense", "Alexis Palmer"], "title": "Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Workshop Language Models for Underserved\n  Communities", "summary": "Cross-lingual transfer learning is an invaluable tool for overcoming data\nscarcity, yet selecting a suitable transfer language remains a challenge. The\nprecise roles of linguistic typology, training data, and model architecture in\ntransfer language choice are not fully understood. We take a holistic approach,\nexamining how both dataset-specific and fine-grained typological features\ninfluence transfer language selection for part-of-speech tagging, considering\ntwo different sources for morphosyntactic features. While previous work\nexamines these dynamics in the context of bilingual biLSTMS, we extend our\nanalysis to a more modern transfer learning pipeline: zero-shot prediction with\npretrained multilingual models. We train a series of transfer language ranking\nsystems and examine how different feature inputs influence ranker performance\nacross architectures. Word overlap, type-token ratio, and genealogical distance\nemerge as top features across all architectures. Our findings reveal that a\ncombination of typological and dataset-dependent features leads to the best\nrankings, and that good performance can be obtained with either feature group\non its own.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19951", "pdf": "https://arxiv.org/pdf/2503.19951", "abs": "https://arxiv.org/abs/2503.19951", "authors": ["Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Changli Tang", "Yixuan Li", "Peihan Li", "Yifan Jiang", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "ACVUBench: Audio-Centric Video Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https://github.com/lark-png/ACVUBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20068", "pdf": "https://arxiv.org/pdf/2503.20068", "abs": "https://arxiv.org/abs/2503.20068", "authors": ["Naitik Jain", "Amogh Joshi", "Mason Earles"], "title": "iNatAg: Multi-Class Classification Models Enabled by a Large-Scale Benchmark Dataset with 4.7M Images of 2,959 Crop and Weed Species", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of crop and weed species is critical for precision\nagriculture and sustainable farming. However, it remains a challenging task due\nto a variety of factors -- a high degree of visual similarity among species,\nenvironmental variability, and a continued lack of large, agriculture-specific\nimage data. We introduce iNatAg, a large-scale image dataset which contains\nover 4.7 million images of 2,959 distinct crop and weed species, with precise\nannotations along the taxonomic hierarchy from binary crop/weed labels to\nspecific species labels. Curated from the broader iNaturalist database, iNatAg\ncontains data from every continent and accurately reflects the variability of\nnatural image captures and environments. Enabled by this data, we train\nbenchmark models built upon the Swin Transformer architecture and evaluate the\nimpact of various modifications such as the incorporation of geospatial data\nand LoRA finetuning. Our best models achieve state-of-the-art performance\nacross all taxonomic classification tasks, achieving 92.38\\% on crop and weed\nclassification. Furthermore, the scale of our dataset enables us to explore\nincorrect misclassifications and unlock new analytic possiblities for plant\nspecies. By combining large-scale species coverage, multi-task labels, and\ngeographic diversity, iNatAg provides a new foundation for building robust,\ngeolocation-aware agricultural classification systems. We release the iNatAg\ndataset publicly through AgML (https://github.com/Project-AgML/AgML), enabling\ndirect access and integration into agricultural machine learning workflows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20302", "pdf": "https://arxiv.org/pdf/2503.20302", "abs": "https://arxiv.org/abs/2503.20302", "authors": ["Sunayana Sitaram", "Adrian de Wynter", "Isobel McCrum", "Qilong Gu", "Si-Qing Chen"], "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "summarization"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20320", "pdf": "https://arxiv.org/pdf/2503.20320", "abs": "https://arxiv.org/abs/2503.20320", "authors": ["Shih-Wen Ke", "Guan-Yu Lai", "Guo-Lin Fang", "Hsi-Yuan Kao"], "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "Vicuna"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20527", "pdf": "https://arxiv.org/pdf/2503.20527", "abs": "https://arxiv.org/abs/2503.20527", "authors": ["Zhicheng Guo", "Sijie Cheng", "Yuchen Niu", "Hao Wang", "Sicheng Zhou", "Wenbing Huang", "Yang Liu"], "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20568", "pdf": "https://arxiv.org/pdf/2503.20568", "abs": "https://arxiv.org/abs/2503.20568", "authors": ["Soumitra Ghosh", "Begona Altuna", "Saeed Farzi", "Pietro Ferrazzi", "Alberto Lavelli", "Giulia Mezzanotte", "Manuela Speranza", "Bernardo Magnini"], "title": "Low-resource Information Extraction with the European Clinical Case Corpus", "categories": ["cs.CL"], "comment": null, "summary": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20639", "pdf": "https://arxiv.org/pdf/2503.20639", "abs": "https://arxiv.org/abs/2503.20639", "authors": ["Jeffery L Painter", "Gregory E Powell", "Andrew Bate"], "title": "PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction", "categories": ["cs.CL", "cs.LG", "J.3; H.3.1; D.2.12"], "comment": null, "summary": "Reliable drug safety reference databases are essential for pharmacovigilance,\nyet existing resources like SIDER are outdated and static. We introduce PVLens,\nan automated system that extracts labeled safety information from FDA\nStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates\nautomation with expert oversight through a web-based review tool. In validation\nagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall\n(0.983) and moderate precision (0.799). By offering a scalable, more accurate\nand continuously updated alternative to SIDER, PVLens enhances real-time\npharamcovigilance with improved accuracy and contemporaneous insights.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20211", "pdf": "https://arxiv.org/pdf/2503.20211", "abs": "https://arxiv.org/abs/2503.20211", "authors": ["Weilong Yan", "Ming Li", "Haipeng Li", "Shuwei Shao", "Robby T. Tan"], "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20230", "pdf": "https://arxiv.org/pdf/2503.20230", "abs": "https://arxiv.org/abs/2503.20230", "authors": ["Ugochukwu Ejike Akpudo", "Yongsheng Gao", "Jun Zhou", "Andrew Lewis"], "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20737", "pdf": "https://arxiv.org/pdf/2503.20737", "abs": "https://arxiv.org/abs/2503.20737", "authors": ["Jeffery L Painter", "François Haguinet", "Gregory E Powell", "Andrew Bate"], "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": null, "summary": "Semantic similarity measures (SSMs) are widely used in biomedical research\nbut remain underutilized in pharmacovigilance. This study evaluates six\nontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety\ndata. Using the Unified Medical Language System (UMLS), we assess each method's\nability to group PTs around medically meaningful centroids. A high-throughput\nframework was developed with a Java API and Python and R interfaces support\nlarge-scale similarity computations. Results show that while path-based methods\nperform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,\nintrinsic information content (IC)-based measures, especially INTRINSIC-LIN and\nSOKAL, consistently yield better clustering accuracy (F1 score of 0.403).\nValidated against expert review and standard MedDRA queries (SMQs), our\nfindings highlight the promise of IC-based SSMs in enhancing pharmacovigilance\nworkflows by improving early signal detection and reducing manual review.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756", "abs": "https://arxiv.org/abs/2503.20756", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20757", "pdf": "https://arxiv.org/pdf/2503.20757", "abs": "https://arxiv.org/abs/2503.20757", "authors": ["Yunhai Hu", "Yilun Zhao", "Chen Zhao", "Arman Cohan"], "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale", "monte carlo tree search", "MCTS"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19950", "pdf": "https://arxiv.org/pdf/2503.19950", "abs": "https://arxiv.org/abs/2503.19950", "authors": ["Han Chen", "Zicong Jiang", "Zining Zhang", "Bingsheng He", "Pingyi Luo", "Mian Lu", "Yuqiang Chen"], "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)", "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20287", "pdf": "https://arxiv.org/pdf/2503.20287", "abs": "https://arxiv.org/abs/2503.20287", "authors": ["Yuhui Wu", "Liyi Chen", "Ruibin Li", "Shihao Wang", "Chenxi Xie", "Lei Zhang"], "title": "InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at InsViE.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20301", "pdf": "https://arxiv.org/pdf/2503.20301", "abs": "https://arxiv.org/abs/2503.20301", "authors": ["Jianyang Zhang", "Qianli Luo", "Guowu Yang", "Wenjing Yang", "Weide Liu", "Guosheng Lin", "Fengmao Lv"], "title": "Attribute-formed Class-specific Concept Space: Endowing Language Bottleneck Model with Better Interpretability and Scalability", "categories": ["cs.CV"], "comment": "This paper has been accepted to CVPR 2025", "summary": "Language Bottleneck Models (LBMs) are proposed to achieve interpretable image\nrecognition by classifying images based on textual concept bottlenecks.\nHowever, current LBMs simply list all concepts together as the bottleneck\nlayer, leading to the spurious cue inference problem and cannot generalized to\nunseen classes. To address these limitations, we propose the Attribute-formed\nLanguage Bottleneck Model (ALBM). ALBM organizes concepts in the\nattribute-formed class-specific space, where concepts are descriptions of\nspecific attributes for specific classes. In this way, ALBM can avoid the\nspurious cue inference problem by classifying solely based on the essential\nconcepts of each class. In addition, the cross-class unified attribute set also\nensures that the concept spaces of different classes have strong correlations,\nas a result, the learned concept classifier can be easily generalized to unseen\nclasses. Moreover, to further improve interpretability, we propose Visual\nAttribute Prompt Learning (VAPL) to extract visual features on fine-grained\nattributes. Furthermore, to avoid labor-intensive concept annotation, we\npropose the Description, Summary, and Supplement (DSS) strategy to\nautomatically generate high-quality concept sets with a complete and precise\nattribute. Extensive experiments on 9 widely used few-shot benchmarks\ndemonstrate the interpretability, transferability, and performance of our\napproach. The code and collected concept sets are available at\nhttps://github.com/tiggers23/ALBM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20314", "pdf": "https://arxiv.org/pdf/2503.20314", "abs": "https://arxiv.org/abs/2503.20314", "authors": ["WanTeam", ":", "Ang Wang", "Baole Ai", "Bin Wen", "Chaojie Mao", "Chen-Wei Xie", "Di Chen", "Feiwu Yu", "Haiming Zhao", "Jianxiao Yang", "Jianyuan Zeng", "Jiayu Wang", "Jingfeng Zhang", "Jingren Zhou", "Jinkai Wang", "Jixuan Chen", "Kai Zhu", "Kang Zhao", "Keyu Yan", "Lianghua Huang", "Mengyang Feng", "Ningyi Zhang", "Pandeng Li", "Pingyu Wu", "Ruihang Chu", "Ruili Feng", "Shiwei Zhang", "Siyang Sun", "Tao Fang", "Tianxing Wang", "Tianyi Gui", "Tingyu Weng", "Tong Shen", "Wei Lin", "Wei Wang", "Wei Wang", "Wenmeng Zhou", "Wente Wang", "Wenting Shen", "Wenyuan Yu", "Xianzhong Shi", "Xiaoming Huang", "Xin Xu", "Yan Kou", "Yangyu Lv", "Yifei Li", "Yijing Liu", "Yiming Wang", "Yingya Zhang", "Yitong Huang", "Yong Li", "You Wu", "Yu Liu", "Yulin Pan", "Yun Zheng", "Yuntao Hong", "Yupeng Shi", "Yutong Feng", "Zeyinzi Jiang", "Zhen Han", "Zhi-Fan Wu", "Ziyu Liu"], "title": "Wan: Open and Advanced Large-Scale Video Generative Models", "categories": ["cs.CV"], "comment": "60 pages, 33 figures", "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20362", "pdf": "https://arxiv.org/pdf/2503.20362", "abs": "https://arxiv.org/abs/2503.20362", "authors": ["Joao Pereira", "Vasco Lopes", "David Semedo", "Joao Neves"], "title": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20523", "pdf": "https://arxiv.org/pdf/2503.20523", "abs": "https://arxiv.org/abs/2503.20523", "authors": ["Lloyd Russell", "Anthony Hu", "Lorenzo Bertoni", "George Fedoseev", "Jamie Shotton", "Elahe Arani", "Gianluca Corrado"], "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Technical Report", "summary": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20722", "pdf": "https://arxiv.org/pdf/2503.20722", "abs": "https://arxiv.org/abs/2503.20722", "authors": ["A. Candito", "A. Dragan", "R. Holbrey", "A. Ribeiro", "R. Donners", "C. Messiou", "N. Tunariu", "D. -M. Koh", "M. D. Blackledge", "The Institute of Cancer Research", "London", "United Kingdom", "The Royal Marsden NHS Foundation Trust", "London", "United Kingdom", "University Hospital Basel", "Basel", "Switzerland"], "title": "A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on Whole-Body Diffusion-Weighted MRI (WB-DWI)", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion\nVolume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized\ncancer imaging biomarkers. However, manual disease delineation for ADC and TDV\nmeasurements is unfeasible in clinical practice, demanding automation. As a\nfirst step, we propose an algorithm to generate fast and reproducible\nprobability maps of the skeleton, adjacent internal organs (liver, spleen,\nurinary bladder, and kidneys), and spinal canal. Methods: We developed an\nautomated deep-learning pipeline based on a 3D patch-based Residual U-Net\narchitecture that localizes and delineates these anatomical structures on\nWB-DWI. The algorithm was trained using \"soft-labels\" (non-binary\nsegmentations) derived from a computationally intensive atlas-based approach.\nFor training and validation, we employed a multi-center WB-DWI dataset\ncomprising 532 scans from patients with Advanced Prostate Cancer (APC) or\nMultiple Myeloma (MM), with testing on 45 patients. Results: Our\nweakly-supervised deep learning model achieved an average dice\nscore/precision/recall of 0.66/0.6/0.73 for skeletal delineations,\n0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with\nsurface distances consistently below 3 mm. Relative median ADC and\nlog-transformed volume differences between automated and manual expert-defined\nfull-body delineations were below 10% and 4%, respectively. The computational\ntime for generating probability maps was 12x faster than the atlas-based\nregistration algorithm (25 s vs. 5 min). An experienced radiologist rated the\nmodel's accuracy \"good\" or \"excellent\" on test datasets. Conclusion: Our model\noffers fast and reproducible probability maps for localizing and delineating\nbody regions on WB-DWI, enabling ADC and TDV quantification, potentially\nsupporting clinicians in disease staging and treatment response assessment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20734", "pdf": "https://arxiv.org/pdf/2503.20734", "abs": "https://arxiv.org/abs/2503.20734", "authors": ["Ziyu Zhou", "Keyan Hu", "Yutian Fang", "Xiaoping Rui"], "title": "SChanger: Change Detection from a Semantic Change and Spatial Consistency Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20771", "pdf": "https://arxiv.org/pdf/2503.20771", "abs": "https://arxiv.org/abs/2503.20771", "authors": ["Masoumeh Sharafi", "Emma Ollivier", "Muhammad Osama Zeeshan", "Soufiane Belharbi", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon Bacon", "Eric~Granger"], "title": "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data", "categories": ["cs.CV"], "comment": null, "summary": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20781", "pdf": "https://arxiv.org/pdf/2503.20781", "abs": "https://arxiv.org/abs/2503.20781", "authors": ["Yulu Pan", "Ce Zhang", "Gedas Bertasius"], "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps://github.com/yulupan00/BASKET.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19976", "pdf": "https://arxiv.org/pdf/2503.19976", "abs": "https://arxiv.org/abs/2503.19976", "authors": ["Navami Kairanda", "Marc Habermann", "Shanthika Naik", "Christian Theobalt", "Vladislav Golyanik"], "title": "Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "15 pages, 12 figures and 3 tables; project page:\n  https://4dqv.mpiinf.mpg.de/ThinShellSfT; CVPR 2025", "summary": "3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular\nRGB videos is a challenging problem, and no solution provides a consistent and\naccurate recovery of fine-grained surface details. To account for the ill-posed\nnature of the setting, existing methods use deformation models with\nstatistical, neural, or physical priors. They also predominantly rely on\nnonadaptive discrete surface representations (e.g. polygonal meshes), perform\nframe-by-frame optimisation leading to error propagation, and suffer from poor\ngradients of the mesh-based differentiable renderers. Consequently, fine\nsurface details such as cloth wrinkles are often not recovered with the desired\naccuracy. In response to these limitations, we propose ThinShell-SfT, a new\nmethod for non-rigid 3D tracking that represents a surface as an implicit and\ncontinuous spatiotemporal neural field. We incorporate continuous thin shell\nphysics prior based on the Kirchhoff-Love model for spatial regularisation,\nwhich starkly contrasts the discretised alternatives of earlier works. Lastly,\nwe leverage 3D Gaussian splatting to differentiably render the surface into\nimage space and optimise the deformations based on analysis-bysynthesis\nprinciples. Our Thin-Shell-SfT outperforms prior works qualitatively and\nquantitatively thanks to our continuous surface formulation in conjunction with\na specially tailored simulation prior and surface-induced 3D Gaussians. See our\nproject page at https://4dqv.mpiinf.mpg.de/ThinShellSfT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20187", "pdf": "https://arxiv.org/pdf/2503.20187", "abs": "https://arxiv.org/abs/2503.20187", "authors": ["Pirzada Suhail", "Amit Sethi"], "title": "Network Inversion for Generating Confidently Classified Counterfeits", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In machine learning, especially with vision classifiers, generating inputs\nthat are confidently classified by the model is essential for understanding its\ndecision boundaries and behavior. However, creating such samples that are\nconfidently classified yet distinct from the training data distribution is a\nchallenge. Traditional methods often modify existing inputs, but they don't\nalways ensure confident classification. In this work, we extend network\ninversion techniques to generate Confidently Classified Counterfeits-synthetic\nsamples that are confidently classified by the model despite being\nsignificantly different from the training data. We achieve this by modifying\nthe generator's conditioning mechanism from soft vector conditioning to one-hot\nvector conditioning and applying Kullback-Leibler divergence (KLD) between the\none-hot vectors and the classifier's output distribution. This encourages the\ngenerator to produce samples that are both plausible and confidently\nclassified. Generating Confidently Classified Counterfeits is crucial for\nensuring the safety and reliability of machine learning systems, particularly\nin safety-critical applications where models must exhibit confidence only on\ndata within the training distribution. By generating such counterfeits, we\nchallenge the assumption that high-confidence predictions are always indicative\nof in-distribution data, providing deeper insights into the model's limitations\nand decision-making process.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20308", "pdf": "https://arxiv.org/pdf/2503.20308", "abs": "https://arxiv.org/abs/2503.20308", "authors": ["Lee Chae-Yeon", "Oh Hyun-Bin", "Han EunGi", "Kim Sung-Bin", "Suekyeong Nam", "Tae-Hyun Oh"], "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20316", "pdf": "https://arxiv.org/pdf/2503.20316", "abs": "https://arxiv.org/abs/2503.20316", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Raghotham Sripadraj", "Kalyan Sivasailam", "Anandakumar D", "Abinaya Ramachandran", "Sudhir MP", "Gunakutti G", "Kishore Prasath Venkatesh"], "title": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning Approach for Automated Diagnosis in Diverse Clinical Settings", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "20 pages , 3 figurea", "summary": "Study Design This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20328", "pdf": "https://arxiv.org/pdf/2503.20328", "abs": "https://arxiv.org/abs/2503.20328", "authors": ["Antoine Bottenmuller", "Florent Magaud", "Arnaud Demortière", "Etienne Decencière", "Petr Dokladal"], "title": "Euclidean Distance to Convex Polyhedra and Application to Class Representation in Spectral Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the aim of estimating the abundance map from observations only, linear\nunmixing approaches are not always suitable to spectral images, especially when\nthe number of bands is too small or when the spectra of the observed data are\ntoo correlated. To address this issue in the general case, we present a novel\napproach which provides an adapted spatial density function based on any\narbitrary linear classifier. A robust mathematical formulation for computing\nthe Euclidean distance to polyhedral sets is presented, along with an efficient\nalgorithm that provides the exact minimum-norm point in a polyhedron. An\nempirical evaluation on the widely-used Samson hyperspectral dataset\ndemonstrates that the proposed method surpasses state-of-the-art approaches in\nreconstructing abundance maps. Furthermore, its application to spectral images\nof a Lithium-ion battery, incompatible with linear unmixing models, validates\nthe method's generality and effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20446", "pdf": "https://arxiv.org/pdf/2503.20446", "abs": "https://arxiv.org/abs/2503.20446", "authors": ["Farzan Moodi", "Fereshteh Khodadadi Shoushtari", "Gelareh Valizadeh", "Dornaz Mazinani", "Hanieh Mobarak Salari", "Hamidreza Saligheh Rad"], "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20571", "pdf": "https://arxiv.org/pdf/2503.20571", "abs": "https://arxiv.org/abs/2503.20571", "authors": ["Vinzenz Uhr", "Ivan Diaz", "Christian Rummel", "Richard McKinley"], "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756", "abs": "https://arxiv.org/abs/2503.20756", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20007", "pdf": "https://arxiv.org/pdf/2503.20007", "abs": "https://arxiv.org/abs/2503.20007", "authors": ["Maksim Borisov", "Zhanibek Kozhirbayev", "Valentin Malykh"], "title": "Low-resource Machine Translation for Code-switched Kazakh-Russian Language Pair", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation for low resource language pairs is a challenging task.\nThis task could become extremely difficult once a speaker uses code switching.\nWe propose a method to build a machine translation model for code-switched\nKazakh-Russian language pair with no labeled data. Our method is basing on\ngeneration of synthetic data. Additionally, we present the first codeswitching\nKazakh-Russian parallel corpus and the evaluation results, which include a\nmodel achieving 16.48 BLEU almost reaching an existing commercial system and\nbeating it by human evaluation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19917", "pdf": "https://arxiv.org/pdf/2503.19917", "abs": "https://arxiv.org/abs/2503.19917", "authors": ["Atsushi Simojo", "Harumi Haraguchi"], "title": "A Study on the Matching Rate of Dance Movements Using 2D Skeleton Detection and 3D Pose Estimation: Why Is SEVENTEEN's Performance So Bita-Zoroi (Perfectly Synchronized)?", "categories": ["cs.CV"], "comment": "14 pages, 11 figures and 20 tables", "summary": "SEVENTEEN is a K-pop group with a large number of members 13 in total and the\nsignificant physical disparity between the tallest and shortest members among\nK-pop groups. However, despite their large numbers and physical differences,\ntheir dance performances exhibit unparalleled unity in the K-pop industry.\nAccording to one theory, their dance synchronization rate is said to be 90% or\neven 97%. However, there is little concrete data to substantiate this\nsynchronization rate. In this study, we analyzed SEVENTEEN's dance performances\nusing videos available on YouTube. We applied 2D skeleton detection and 3D pose\nestimation to evaluate joint angles, body part movements, and jumping and\ncrouching motions to investigate the factors contributing to their performance\nunity. The analysis revealed exceptionally high consistency in the movement\ndirection of body parts, as well as in the ankle and head positions during\njumping movements and the head position during crouching movements. These\nfindings suggested that SEVENTEEN's high synchronization rate can be attributed\nto the consistency of movement direction and the synchronization of ankle and\nhead heights during jumping and crouching movements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20110", "pdf": "https://arxiv.org/pdf/2503.20110", "abs": "https://arxiv.org/abs/2503.20110", "authors": ["Pin-Jie Lin", "Rishab Balasubramanian", "Fengyuan Liu", "Nikhil Kandpal", "Tu Vu"], "title": "Efficient Model Development through Fine-tuning Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 4 figures, 13 tables", "summary": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19947", "pdf": "https://arxiv.org/pdf/2503.19947", "abs": "https://arxiv.org/abs/2503.19947", "authors": ["Paul Koch", "Jörg Krüger", "Ankit Chowdhury", "Oliver Heimann"], "title": "Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20179", "pdf": "https://arxiv.org/pdf/2503.20179", "abs": "https://arxiv.org/abs/2503.20179", "authors": ["Shijia Zhang", "Xiyu Ding", "Kai Ding", "Jacob Zhang", "Kevin Galinsky", "Mengrui Wang", "Ryan P. Mayers", "Zheyu Wang", "Hadi Kharrazi"], "title": "ProtoBERT-LoRA: Parameter-Efficient Prototypical Finetuning for Immunotherapy Study Identification", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "comment": "Submitted to AMIA 2025 Annual Symposium", "summary": "Identifying immune checkpoint inhibitor (ICI) studies in genomic repositories\nlike Gene Expression Omnibus (GEO) is vital for cancer research yet remains\nchallenging due to semantic ambiguity, extreme class imbalance, and limited\nlabeled data in low-resource settings. We present ProtoBERT-LoRA, a hybrid\nframework that combines PubMedBERT with prototypical networks and Low-Rank\nAdaptation (LoRA) for efficient fine-tuning. The model enforces class-separable\nembeddings via episodic prototype training while preserving biomedical domain\nknowledge. Our dataset was divided as: Training (20 positive, 20 negative),\nPrototype Set (10 positive, 10 negative), Validation (20 positive, 200\nnegative), and Test (71 positive, 765 negative). Evaluated on test dataset,\nProtoBERT-LoRA achieved F1-score of 0.624 (precision: 0.481, recall: 0.887),\noutperforming the rule-based system, machine learning baselines and finetuned\nPubMedBERT. Application to 44,287 unlabeled studies reduced manual review\nefforts by 82%. Ablation studies confirmed that combining prototypes with LoRA\nimproved performance by 29% over stand-alone LoRA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20194", "pdf": "https://arxiv.org/pdf/2503.20194", "abs": "https://arxiv.org/abs/2503.20194", "authors": ["Zhouhong Gu", "Xingzhou Chen", "Xiaoran Shi", "Tao Wang", "Suhang Zheng", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "PPO", "policy optimization", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19982", "pdf": "https://arxiv.org/pdf/2503.19982", "abs": "https://arxiv.org/abs/2503.19982", "authors": ["Pei-Kai Huang", "Jun-Xiong Chong", "Cheng-Hsuan Chiang", "Tzu-Hsien Chen", "Tyng-Luh Liu", "Chiou-Ting Hsu"], "title": "SLIP: Spoof-Aware One-Class Face Anti-Spoofing with Language Image Pretraining", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Face anti-spoofing (FAS) plays a pivotal role in ensuring the security and\nreliability of face recognition systems. With advancements in vision-language\npretrained (VLP) models, recent two-class FAS techniques have leveraged the\nadvantages of using VLP guidance, while this potential remains unexplored in\none-class FAS methods. The one-class FAS focuses on learning intrinsic liveness\nfeatures solely from live training images to differentiate between live and\nspoof faces. However, the lack of spoof training data can lead one-class FAS\nmodels to inadvertently incorporate domain information irrelevant to the\nlive/spoof distinction (e.g., facial content), causing performance degradation\nwhen tested with a new application domain. To address this issue, we propose a\nnovel framework called Spoof-aware one-class face anti-spoofing with Language\nImage Pretraining (SLIP). Given that live faces should ideally not be obscured\nby any spoof-attack-related objects (e.g., paper, or masks) and are assumed to\nyield zero spoof cue maps, we first propose an effective language-guided spoof\ncue map estimation to enhance one-class FAS models by simulating whether the\nunderlying faces are covered by attack-related objects and generating\ncorresponding nonzero spoof cue maps. Next, we introduce a novel prompt-driven\nliveness feature disentanglement to alleviate live/spoof-irrelative domain\nvariations by disentangling live/spoof-relevant and domain-dependent\ninformation. Finally, we design an effective augmentation strategy by fusing\nlatent features from live images and spoof prompts to generate spoof-like image\nfeatures and thus diversify latent spoof features to facilitate the learning of\none-class FAS. Our extensive experiments and ablation studies support that SLIP\nconsistently outperforms previous one-class FAS methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20212", "pdf": "https://arxiv.org/pdf/2503.20212", "abs": "https://arxiv.org/abs/2503.20212", "authors": ["Yangyang Meng", "Jinpeng Li", "Guodong Lin", "Yu Pu", "Guanbo Wang", "Hu Du", "Zhiming Shao", "Yukai Huang", "Ke Li", "Wei-Qiang Zhang"], "title": "Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This report introduces Dolphin, a large-scale multilingual automatic speech\nrecognition (ASR) model that extends the Whisper architecture to support a\nwider range of languages. Our approach integrates in-house proprietary and\nopen-source datasets to refine and optimize Dolphin's performance. The model is\nspecifically designed to achieve notable recognition accuracy for 40 Eastern\nlanguages across East Asia, South Asia, Southeast Asia, and the Middle East,\nwhile also supporting 22 Chinese dialects. Experimental evaluations show that\nDolphin significantly outperforms current state-of-the-art open-source models\nacross various languages. To promote reproducibility and community-driven\ninnovation, we are making our trained models and inference source code publicly\navailable.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20011", "pdf": "https://arxiv.org/pdf/2503.20011", "abs": "https://arxiv.org/abs/2503.20011", "authors": ["Luke Chen", "Junyao Wang", "Trier Mortlock", "Pramod Khargonekar", "Mohammad Abdullah Al Faruque"], "title": "Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at CVPR 2025", "summary": "Uncertainty Quantification (UQ) is crucial for ensuring the reliability of\nmachine learning models deployed in real-world autonomous systems. However,\nexisting approaches typically quantify task-level output prediction uncertainty\nwithout considering epistemic uncertainty at the multimodal feature fusion\nlevel, leading to sub-optimal outcomes. Additionally, popular uncertainty\nquantification methods, e.g., Bayesian approximations, remain challenging to\ndeploy in practice due to high computational costs in training and inference.\nIn this paper, we propose HyperDUM, a novel deterministic uncertainty method\n(DUM) that efficiently quantifies feature-level epistemic uncertainty by\nleveraging hyperdimensional computing. Our method captures the channel and\nspatial uncertainties through channel and patch -wise projection and bundling\ntechniques respectively. Multimodal sensor features are then adaptively\nweighted to mitigate uncertainty propagation and improve feature fusion. Our\nevaluations show that HyperDUM on average outperforms the state-of-the-art\n(SOTA) algorithms by up to 2.01%/1.27% in 3D Object Detection and up to 1.29%\nimprovement over baselines in semantic segmentation tasks under various types\nof uncertainties. Notably, HyperDUM requires 2.36x less Floating Point\nOperations and up to 38.30x less parameters than SOTA methods, providing an\nefficient solution for real-world autonomous systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20227", "pdf": "https://arxiv.org/pdf/2503.20227", "abs": "https://arxiv.org/abs/2503.20227", "authors": ["Tianhao Wu", "Yu Wang", "Ngoc Quach"], "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted by the 5th International Conference on\n  Artificial Intelligence and Industrial Technology Applications (AIITA 2025)", "summary": "Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20084", "pdf": "https://arxiv.org/pdf/2503.20084", "abs": "https://arxiv.org/abs/2503.20084", "authors": ["Simiao Ren", "Yao Yao", "Kidus Zewde", "Zisheng Liang", "Tsang", "Ng", "Ning-Yau Cheng", "Xiaoou Zhan", "Qinzhe Liu", "Yifei Chen", "Hengwei Xu"], "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20101", "pdf": "https://arxiv.org/pdf/2503.20101", "abs": "https://arxiv.org/abs/2503.20101", "authors": ["Albert W Reed", "Connor Hashemi", "Dennis Melamed", "Nitesh Menon", "Keigo Hirakawa", "Scott McCloskey"], "title": "EBS-EKF: Accurate and High Frequency Event-based Star Tracking", "categories": ["cs.CV"], "comment": "Accepted into the proceedings of the Conference on Computer Vision\n  and Pattern Recognition (CVPR) for 2025. Link to code and dataset is\n  https://gitlab.kitware.com/nest-public/kw_ebs_star_tracking#", "summary": "Event-based sensors (EBS) are a promising new technology for star tracking\ndue to their low latency and power efficiency, but prior work has thus far been\nevaluated exclusively in simulation with simplified signal models. We propose a\nnovel algorithm for event-based star tracking, grounded in an analysis of the\nEBS circuit and an extended Kalman filter (EKF). We quantitatively evaluate our\nmethod using real night sky data, comparing its results with those from a\nspace-ready active-pixel sensor (APS) star tracker. We demonstrate that our\nmethod is an order-of-magnitude more accurate than existing methods due to\nimproved signal modeling and state estimation, while providing more frequent\nupdates and greater motion tolerance than conventional APS trackers. We provide\nall code and the first dataset of events synchronized with APS solutions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20417", "pdf": "https://arxiv.org/pdf/2503.20417", "abs": "https://arxiv.org/abs/2503.20417", "authors": ["Zhenghan Yu", "Xinyu Hu", "Xiaojun Wan"], "title": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation and Processing", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20172", "pdf": "https://arxiv.org/pdf/2503.20172", "abs": "https://arxiv.org/abs/2503.20172", "authors": ["Mengqing Xue", "Yifei Liu", "Ling Guo", "Shaoli Huang", "Changxing Ding"], "title": "Guiding Human-Object Interactions with Rich Geometry and Relations", "categories": ["cs.CV"], "comment": "CVPR 2025.Project website: https://lalalfhdh.github.io/rog_page/", "summary": "Human-object interaction (HOI) synthesis is crucial for creating immersive\nand realistic experiences for applications such as virtual reality. Existing\nmethods often rely on simplified object representations, such as the object's\ncentroid or the nearest point to a human, to achieve physically plausible\nmotions. However, these approaches may overlook geometric complexity, resulting\nin suboptimal interaction fidelity. To address this limitation, we introduce\nROG, a novel diffusion-based framework that models the spatiotemporal\nrelationships inherent in HOIs with rich geometric detail. For efficient object\nrepresentation, we select boundary-focused and fine-detail key points from the\nobject mesh, ensuring a comprehensive depiction of the object's geometry. This\nrepresentation is used to construct an interactive distance field (IDF),\ncapturing the robust HOI dynamics. Furthermore, we develop a diffusion-based\nrelation model that integrates spatial and temporal attention mechanisms,\nenabling a better understanding of intricate HOI relationships. This relation\nmodel refines the generated motion's IDF, guiding the motion generation process\nto produce relation-aware and semantically aligned movements. Experimental\nevaluations demonstrate that ROG significantly outperforms state-of-the-art\nmethods in the realism and semantic accuracy of synthesized HOIs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20496", "pdf": "https://arxiv.org/pdf/2503.20496", "abs": "https://arxiv.org/abs/2503.20496", "authors": ["Aishik Mandal", "Dana Atzil-Slonim", "Thamar Solorio", "Iryna Gurevych"], "title": "Enhancing Depression Detection via Question-wise Modality Fusion", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, The 10th Workshop on Computational Linguistics\n  and Clinical Psychology", "summary": "Depression is a highly prevalent and disabling condition that incurs\nsubstantial personal and societal costs. Current depression diagnosis involves\ndetermining the depression severity of a person through self-reported\nquestionnaires or interviews conducted by clinicians. This often leads to\ndelayed treatment and involves substantial human resources. Thus, several works\ntry to automate the process using multimodal data. However, they usually\noverlook the following: i) The variable contribution of each modality for each\nquestion in the questionnaire and ii) Using ordinal classification for the\ntask. This results in sub-optimal fusion and training methods. In this work, we\npropose a novel Question-wise Modality Fusion (QuestMF) framework trained with\na novel Imbalanced Ordinal Log-Loss (ImbOLL) function to tackle these issues.\nThe performance of our framework is comparable to the current state-of-the-art\nmodels on the E-DAIC dataset and enhances interpretability by predicting scores\nfor each question. This will help clinicians identify an individual's symptoms,\nallowing them to customise their interventions accordingly. We also make the\ncode for the QuestMF framework publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20533", "pdf": "https://arxiv.org/pdf/2503.20533", "abs": "https://arxiv.org/abs/2503.20533", "authors": ["Yijiong Yu"], "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence", "categories": ["cs.CL"], "comment": "Our code is available in\n  https://github.com/yuyijiong/parallel-decoding-in-one-sequence", "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy, particularly for complex tasks such as mathematical\nreasoning, by employing detailed and comprehensive reasoning processes.\nHowever, generating these lengthy reasoning sequences is computationally\nexpensive and time-consuming. To address this inefficiency, we leverage the\ninherent parallelizability of certain tasks to accelerate the reasoning\nprocess. Specifically, when multiple parallel reasoning branches exist, we\ndecode multiple tokens per step using a specialized attention mask, processing\nthem within a single sequence. Experimental results show that our method\nachieves over 100% speedup in decoding time while basically maintaining\naccuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20207", "pdf": "https://arxiv.org/pdf/2503.20207", "abs": "https://arxiv.org/abs/2503.20207", "authors": ["Peiyuan Ni", "Chee Meng Chew", "Marcelo H. Ang Jr.", "Gregory S. Chirikjian"], "title": "Reasoning and Learning a Perceptual Metric for Self-Training of Reflective Objects in Bin-Picking with a Low-cost Camera", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages, 10 figures", "summary": "Bin-picking of metal objects using low-cost RGB-D cameras often suffers from\nsparse depth information and reflective surface textures, leading to errors and\nthe need for manual labeling. To reduce human intervention, we propose a\ntwo-stage framework consisting of a metric learning stage and a self-training\nstage. Specifically, to automatically process data captured by a low-cost\ncamera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that\noptimizes pose hypotheses under depth, collision, and boundary constraints. To\nfurther refine pose candidates, we adopt a Symmetry-aware Lie-group based\nBayesian Gaussian Mixture Model (SaL-BGMM), integrated with the\nExpectation-Maximization (EM) algorithm, for symmetry-aware filtering.\nAdditionally, we propose a Weighted Ranking Information Noise Contrastive\nEstimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from\nreconstructed data, supporting self-training on untrained or even unseen\nobjects. Experimental results show that our approach outperforms several\nstate-of-the-art methods on both the ROBI dataset and our newly introduced\nSelf-ROBI dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20641", "pdf": "https://arxiv.org/pdf/2503.20641", "abs": "https://arxiv.org/abs/2503.20641", "authors": ["Han Wu", "Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Xiaojin Fu", "Xiongwei Han", "Xing Li", "Hui-Ling Zhen", "Tao Zhong", "Mingxuan Yuan"], "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "categories": ["cs.CL"], "comment": "Work in progress; technical report", "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20252", "pdf": "https://arxiv.org/pdf/2503.20252", "abs": "https://arxiv.org/abs/2503.20252", "authors": ["Yejin Kwon", "Daeun Moon", "Youngje Oh", "Hyunsoo Yoon"], "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20258", "pdf": "https://arxiv.org/pdf/2503.20258", "abs": "https://arxiv.org/abs/2503.20258", "authors": ["Jiaheng Zhou", "Yanfeng Zhou", "Wei Fang", "Yuxing Tang", "Le Lu", "Ge Yang"], "title": "Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis of Medical Ultrasound Videos", "categories": ["cs.CV"], "comment": null, "summary": "Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20268", "pdf": "https://arxiv.org/pdf/2503.20268", "abs": "https://arxiv.org/abs/2503.20268", "authors": ["Ziran Zhang", "Xiaohui Li", "Yihao Liu", "Yujin Wang", "Yueting Chen", "Tianfan Xue", "Shi Guo"], "title": "EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation", "categories": ["cs.CV"], "comment": null, "summary": "Video frame interpolation (VFI) in scenarios with large motion remains\nchallenging due to motion ambiguity between frames. While event cameras can\ncapture high temporal resolution motion information, existing event-based VFI\nmethods struggle with limited training data and complex motion patterns. In\nthis paper, we introduce Event-Guided Video Diffusion Model (EGVD), a novel\nframework that leverages the powerful priors of pre-trained stable video\ndiffusion models alongside the precise temporal information from event cameras.\nOur approach features a Multi-modal Motion Condition Generator (MMCG) that\neffectively integrates RGB frames and event signals to guide the diffusion\nprocess, producing physically realistic intermediate frames. We employ a\nselective fine-tuning strategy that preserves spatial modeling capabilities\nwhile efficiently incorporating event-guided temporal information. We\nincorporate input-output normalization techniques inspired by recent advances\nin diffusion modeling to enhance training stability across varying noise\nlevels. To improve generalization, we construct a comprehensive dataset\ncombining both real and simulated event data across diverse scenarios.\nExtensive experiments on both real and simulated datasets demonstrate that EGVD\nsignificantly outperforms existing methods in handling large motion and\nchallenging lighting conditions, achieving substantial improvements in\nperceptual quality metrics (27.4% better LPIPS on Prophesee and 24.1% on BSRGB)\nwhile maintaining competitive fidelity measures. Code and datasets available\nat: https://github.com/OpenImagingLab/EGVD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20289", "pdf": "https://arxiv.org/pdf/2503.20289", "abs": "https://arxiv.org/abs/2503.20289", "authors": ["Kaifan Sun", "Bingchen Yang", "Peter Wonka", "Jun Xiao", "Haiyong Jiang"], "title": "RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship Triples into the Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "The generation of indoor furniture layouts has significant applications in\naugmented reality, smart homes, and architectural design. Successful furniture\narrangement requires proper physical relationships (e.g., collision avoidance)\nand spacing relationships between furniture and their functional zones to be\nrespected. However, manually defined relationships are almost always incomplete\nand can produce unrealistic layouts. This work instead extracts spacing\nrelationships automatically based on a hierarchical analysis and adopts the\nDelaunay Triangulation to produce important triple relationships. Compared to\npairwise relationship modeling, triple relationships account for interactions\nand space utilization among multiple objects. To this end, we introduce\nRelTriple, a novel approach that enhances furniture distribution by learning\nspacing relationships between objects and regions. We formulate triple\nrelationships as object-to-object (O2O) losses and object-to-region (O2R)\nlosses and integrate them directly into the training process of generative\ndiffusion. Our approach consistently improves over existing state-of-the-art\nmethods in visual results evaluation metrics on unconditional layout\ngeneration, floorplan-conditioned layout generation, and scene rearrangement,\nachieving at least 12% on the introduced spatial relationship metric and\nsuperior spatial coherence and practical usability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20309", "pdf": "https://arxiv.org/pdf/2503.20309", "abs": "https://arxiv.org/abs/2503.20309", "authors": ["Zitian Wang", "Yue Liao", "Kang Rong", "Fengyun Rao", "Yibo Yang", "Si Liu"], "title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20783", "pdf": "https://arxiv.org/pdf/2503.20783", "abs": "https://arxiv.org/abs/2503.20783", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20318", "pdf": "https://arxiv.org/pdf/2503.20318", "abs": "https://arxiv.org/abs/2503.20318", "authors": ["Qian Wang", "Aleksandar Cvejic", "Abdelrahman Eldesokey", "Peter Wonka"], "title": "EditCLIP: Representation Learning for Image Editing", "categories": ["cs.CV"], "comment": "Project page: https://qianwangx.github.io/EditCLIP/", "summary": "We introduce EditCLIP, a novel representation-learning approach for image\nediting. Our method learns a unified representation of edits by jointly\nencoding an input image and its edited counterpart, effectively capturing their\ntransformation. To evaluate its effectiveness, we employ EditCLIP to solve two\ntasks: exemplar-based image editing and automated edit evaluation. In\nexemplar-based image editing, we replace text-based instructions in\nInstructPix2Pix with EditCLIP embeddings computed from a reference exemplar\nimage pair. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods while being more efficient and versatile. For\nautomated evaluation, EditCLIP assesses image edits by measuring the similarity\nbetween the EditCLIP embedding of a given image pair and either a textual\nediting instruction or the EditCLIP embedding of another reference image pair.\nExperiments show that EditCLIP aligns more closely with human judgments than\nexisting CLIP-based metrics, providing a reliable measure of edit quality and\nstructural preservation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20322", "pdf": "https://arxiv.org/pdf/2503.20322", "abs": "https://arxiv.org/abs/2503.20322", "authors": ["Hao Ai", "Kunyi Wang", "Zezhou Wang", "Hao Lu", "Jin Tian", "Yaxin Luo", "Peng Xing", "Jen-Yuan Huang", "Huaxia Li", "Gen luo"], "title": "Dynamic Pyramid Network for Efficient Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in various vision-language (VL) tasks, but their expensive\ncomputations still limit the real-world application. To address this issue,\nrecent efforts aim to compress the visual features to save the computational\ncosts of MLLMs. However, direct visual compression methods, e.g. efficient\nprojectors, inevitably destroy the visual semantics in MLLM, especially in\ndifficult samples. To overcome this shortcoming, we propose a novel dynamic\npyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as\na hierarchical structure where visual features are gradually compressed with\nincreasing depth. In this case, even with a high compression ratio,\nfine-grained visual information can still be perceived in shallow layers. To\nmaximize the benefit of DPN, we further propose an innovative Dynamic Pooling\nExperts (DPE) that can dynamically choose the optimal visual compression rate\naccording to input features. With this design, harder samples will be assigned\nlarger computations, thus preserving the model performance. To validate our\napproach, we conduct extensive experiments on two popular MLLMs and ten\nbenchmarks. Experimental results show that DPN can save up to 56% average FLOPs\non LLaVA while further achieving +0.74% performance gains. Besides, the\ngeneralization ability of DPN is also validated on the existing high-resolution\nMLLM called LLaVA-HR. Our source codes are anonymously released at\nhttps://github.com/aihao2000/DPN-LLaVA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20349", "pdf": "https://arxiv.org/pdf/2503.20349", "abs": "https://arxiv.org/abs/2503.20349", "authors": ["Weiyi You", "Mingyang Zhang", "Leheng Zhang", "Kexuan Shi", "Xingyu Zhou", "Shuhang Gu"], "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20354", "pdf": "https://arxiv.org/pdf/2503.20354", "abs": "https://arxiv.org/abs/2503.20354", "authors": ["Ke Ma", "Jiaqi Tang", "Bin Guo", "Fan Dang", "Sicong Liu", "Zhui Zhu", "Lei Wu", "Cheng Fang", "Ying-Cong Chen", "Zhiwen Yu", "Yunhao Liu"], "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20368", "pdf": "https://arxiv.org/pdf/2503.20368", "abs": "https://arxiv.org/abs/2503.20368", "authors": ["Hongda Liu", "Longguang Wang", "Weijun Guan", "Ye Zhang", "Yulan Guo"], "title": "Pluggable Style Representation Learning for Multi-Style Transfer", "categories": ["cs.CV"], "comment": "18 pages, 13 figures, 2 tables", "summary": "Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20418", "pdf": "https://arxiv.org/pdf/2503.20418", "abs": "https://arxiv.org/abs/2503.20418", "authors": ["Ji Woo Hong", "Tri Ton", "Trung X. Pham", "Gwanhyeong Koo", "Sunjae Yoon", "Chang D. Yoo"], "title": "ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On", "categories": ["cs.CV"], "comment": "CVPR 2025, Project Page: https://jiwoohong93.github.io/ita-mdt/", "summary": "This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion\nTransformer Framework for Image-Based Virtual Try-On (IVTON), designed to\novercome the limitations of previous approaches by leveraging the Masked\nDiffusion Transformer (MDT) for improved handling of both global garment\ncontext and fine-grained details. The IVTON task involves seamlessly\nsuperimposing a garment from one image onto a person in another, creating a\nrealistic depiction of the person wearing the specified garment. Unlike\nconventional diffusion-based virtual try-on models that depend on large\npre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable\ntransformer-based denoising diffusion model with a mask latent modeling scheme,\nachieving competitive results while reducing computational overhead. A key\ncomponent of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA),\na dynamic feature aggregator that combines all of the features from the image\nencoder into a unified feature of the same size, guided by diffusion timestep\nand garment image complexity. This enables adaptive weighting of features,\nallowing the model to emphasize either global information or fine-grained\ndetails based on the requirements of the denoising stage. Additionally, the\nSalient Region Extractor (SRE) module is presented to identify complex region\nof the garment to provide high-resolution local information to the denoising\nmodel as an additional condition alongside the global information of the full\ngarment image. This targeted conditioning strategy enhances detail preservation\nof fine details in highly salient garment regions, optimizing computational\nresources by avoiding unnecessarily processing entire garment image.\nComparative evaluations confirms that ITA-MDT improves efficiency while\nmaintaining strong performance, reaching state-of-the-art results in several\nmetrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20419", "pdf": "https://arxiv.org/pdf/2503.20419", "abs": "https://arxiv.org/abs/2503.20419", "authors": ["Andreas Gilson", "Peter Pietrzyk", "Chiara Paglia", "Annika Killer", "Fabian Keil", "Lukas Meyer", "Dominikus Kittemann", "Patrick Noack", "Oliver Scholz"], "title": "Cherry Yield Forecast: Harvest Prediction for Individual Sweet Cherry Trees", "categories": ["cs.CV"], "comment": null, "summary": "This paper is part of a publication series from the For5G project that has\nthe goal of creating digital twins of sweet cherry trees. At the beginning a\nbrief overview of the revious work in this project is provided. Afterwards the\nfocus shifts to a crucial problem in the fruit farming domain: the difficulty\nof making reliable yield predictions early in the season. Following three Satin\nsweet cherry trees along the year 2023 enabled the collection of accurate\nground truth data about the development of cherries from dormancy until\nharvest. The methodology used to collect this data is presented, along with its\nvaluation and visualization. The predictive power of counting objects at all\nrelevant vegetative stages of the fruit development cycle in cherry trees with\nregards to yield predictions is investigated. It is found that all investigated\nfruit states are suitable for yield predictions based on linear regression.\nConceptionally, there is a trade-off between earliness and external events with\nthe potential to invalidate the prediction. Considering this, two optimal\ntimepoints are suggested that are opening cluster stage before the start of the\nflowering and the early fruit stage right after the second fruit drop. However,\nboth timepoints are challenging to solve with automated procedures based on\nimage data. Counting developing cherries based on images is exceptionally\ndifficult due to the small fruit size and their tendency to be occluded by\nleaves. It was not possible to obtain satisfying results relying on a\nstate-of-the-art fruit-counting method. Counting the elements within a bursting\nbud is also challenging, even when using high resolution cameras. It is\nconcluded that accurate yield prediction for sweet cherry trees is possible\nwhen objects are manually counted and that automated features extraction with\nsimilar accuracy remains an open problem yet to be solved.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20429", "pdf": "https://arxiv.org/pdf/2503.20429", "abs": "https://arxiv.org/abs/2503.20429", "authors": ["Guilherme Fernandes", "Vasco Ramos", "Regev Cohen", "Idan Szpektor", "João Magalhães"], "title": "Latent Beam Diffusion Models for Decoding Image Sequences", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20436", "pdf": "https://arxiv.org/pdf/2503.20436", "abs": "https://arxiv.org/abs/2503.20436", "authors": ["Muxin Pu", "Mei Kuan Lim", "Chun Yong Chong"], "title": "Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign Language Recognition", "categories": ["cs.CV"], "comment": "10 pages, ACM Multimedia", "summary": "Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20483", "pdf": "https://arxiv.org/pdf/2503.20483", "abs": "https://arxiv.org/abs/2503.20483", "authors": ["Yingdong Shi", "Changming Li", "Yifan Wang", "Yongxiang Zhao", "Anqi Pang", "Sibei Yang", "Jingyi Yu", "Kan Ren"], "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025; Project Page:\n  https://foundation-model-research.github.io/difflens", "summary": "Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20504", "pdf": "https://arxiv.org/pdf/2503.20504", "abs": "https://arxiv.org/abs/2503.20504", "authors": ["Zehui Liao", "Shishuai Hu", "Ke Zou", "Huazhu Fu", "Liangli Zhen", "Yong Xia"], "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical Visual Question Answering", "categories": ["cs.CV"], "comment": "11 pages, 2 figures", "summary": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20516", "pdf": "https://arxiv.org/pdf/2503.20516", "abs": "https://arxiv.org/abs/2503.20516", "authors": ["Mahya Nikouei", "Bita Baroutian", "Shahabedin Nabavi", "Fateme Taraghi", "Atefe Aghaei", "Ayoob Sajedi", "Mohsen Ebrahimi Moghaddam"], "title": "Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20537", "pdf": "https://arxiv.org/pdf/2503.20537", "abs": "https://arxiv.org/abs/2503.20537", "authors": ["Ziying Zhang", "Xiang Gao", "Zhixin Wang", "Qiang hu", "Xiaoyun Zhang"], "title": "TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20563", "pdf": "https://arxiv.org/pdf/2503.20563", "abs": "https://arxiv.org/abs/2503.20563", "authors": ["Carlos Gomes", "Benedikt Blumenstiel", "Joao Lucas de Sousa Almeida", "Pedro Henrique de Oliveira", "Paolo Fraccaro", "Francesc Marti Escofet", "Daniela Szwarcman", "Naomi Simumba", "Romeo Kienzler", "Bianca Zadrozny"], "title": "TerraTorch: The Geospatial Foundation Models Toolkit", "categories": ["cs.CV", "cs.LG"], "comment": "IGARSS 2025", "summary": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https://github.com/IBM/terratorch, and can be\ninstalled via pip install terratorch.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20654", "pdf": "https://arxiv.org/pdf/2503.20654", "abs": "https://arxiv.org/abs/2503.20654", "authors": ["Xiangwen Zhang", "Qian Zhang", "Longfei Han", "Qiang Qu", "Xiaoming Chen"], "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20662", "pdf": "https://arxiv.org/pdf/2503.20662", "abs": "https://arxiv.org/abs/2503.20662", "authors": ["Sadaf Khademi", "Mehran Shabanpour", "Reza Taleei", "Anastasia Oikonomou", "Arash Mohammadi"], "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20663", "pdf": "https://arxiv.org/pdf/2503.20663", "abs": "https://arxiv.org/abs/2503.20663", "authors": ["Mingze Sun", "Shiwei Mao", "Keyi Chen", "Yurun Chen", "Shunlin Lu", "Jingbo Wang", "Junting Dong", "Ruqi Huang"], "title": "ARMO: Autoregressive Rigging for Multi-Category Objects", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large-scale generative models have significantly\nimproved the quality and diversity of 3D shape generation. However, most\nexisting methods focus primarily on generating static 3D models, overlooking\nthe potentially dynamic nature of certain shapes, such as humanoids, animals,\nand insects. To address this gap, we focus on rigging, a fundamental task in\nanimation that establishes skeletal structures and skinning for 3D models. In\nthis paper, we introduce OmniRig, the first large-scale rigging dataset,\ncomprising 79,499 meshes with detailed skeleton and skinning information.\nUnlike traditional benchmarks that rely on predefined standard poses (e.g.,\nA-pose, T-pose), our dataset embraces diverse shape categories, styles, and\nposes. Leveraging this rich dataset, we propose ARMO, a novel rigging framework\nthat utilizes an autoregressive model to predict both joint positions and\nconnectivity relationships in a unified manner. By treating the skeletal\nstructure as a complete graph and discretizing it into tokens, we encode the\njoints using an auto-encoder to obtain a latent embedding and an autoregressive\nmodel to predict the tokens. A mesh-conditioned latent diffusion model is used\nto predict the latent embedding for conditional skeleton generation. Our method\naddresses the limitations of regression-based approaches, which often suffer\nfrom error accumulation and suboptimal connectivity estimation. Through\nextensive experiments on the OmniRig dataset, our approach achieves\nstate-of-the-art performance in skeleton prediction, demonstrating improved\ngeneralization across diverse object categories. The code and dataset will be\nmade public for academic use upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20672", "pdf": "https://arxiv.org/pdf/2503.20672", "abs": "https://arxiv.org/abs/2503.20672", "authors": ["Yuyang Peng", "Shishi Xiao", "Keming Wu", "Qisheng Liao", "Bohan Chen", "Kevin Lin", "Danqing Huang", "Ji Li", "Yuhui Yuan"], "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io", "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20685", "pdf": "https://arxiv.org/pdf/2503.20685", "abs": "https://arxiv.org/abs/2503.20685", "authors": ["Yuhao Huang", "Ao Chang", "Haoran Dou", "Xing Tao", "Xinrui Zhou", "Yan Cao", "Ruobing Huang", "Alejandro F Frangi", "Lingyun Bao", "Xin Yang", "Dong Ni"], "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by Medical Image Analysis. 24 pages, 13 figures, 18 tabels", "summary": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20744", "pdf": "https://arxiv.org/pdf/2503.20744", "abs": "https://arxiv.org/abs/2503.20744", "authors": ["Guoqiang Zhang", "Kenta Niwa", "J. P. Lewis", "Cedric Mesnage", "W. Bastiaan Kleijn"], "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20746", "pdf": "https://arxiv.org/pdf/2503.20746", "abs": "https://arxiv.org/abs/2503.20746", "authors": ["Boyuan Chen", "Hanxiao Jiang", "Shaowei Liu", "Saurabh Gupta", "Yunzhu Li", "Hao Zhao", "Shenlong Wang"], "title": "PhysGen3D: Crafting a Miniature Interactive World from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://by-luckk.github.io/PhysGen3D", "summary": "Envisioning physically plausible outcomes from a single image requires a deep\nunderstanding of the world's dynamics. To address this, we introduce PhysGen3D,\na novel framework that transforms a single image into an amodal,\ncamera-centric, interactive 3D scene. By combining advanced image-based\ngeometric and semantic understanding with physics-based simulation, PhysGen3D\ncreates an interactive 3D world from a static image, enabling us to \"imagine\"\nand simulate future scenarios based on user input. At its core, PhysGen3D\nestimates 3D shapes, poses, physical and lighting properties of objects,\nthereby capturing essential physical attributes that drive realistic object\ninteractions. This framework allows users to specify precise initial\nconditions, such as object speed or material properties, for enhanced control\nover generated video outcomes. We evaluate PhysGen3D's performance against\nclosed-source state-of-the-art (SOTA) image-to-video models, including Pika,\nKling, and Gen-3, showing PhysGen3D's capacity to generate videos with\nrealistic physics while offering greater flexibility and fine-grained control.\nOur results show that PhysGen3D achieves a unique balance of photorealism,\nphysical plausibility, and user-driven interactivity, opening new possibilities\nfor generating dynamic, physics-grounded video from an image.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20748", "pdf": "https://arxiv.org/pdf/2503.20748", "abs": "https://arxiv.org/abs/2503.20748", "authors": ["Chen Tang", "Xinzhu Ma", "Encheng Su", "Xiufeng Song", "Xiaohong Liu", "Wei-Hong Li", "Lei Bai", "Wanli Ouyang", "Xiangyu Yue"], "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Traditional spatiotemporal models generally rely on task-specific\narchitectures, which limit their generalizability and scalability across\ndiverse tasks due to domain-specific design requirements. In this paper, we\nintroduce \\textbf{UniSTD}, a unified Transformer-based framework for\nspatiotemporal modeling, which is inspired by advances in recent foundation\nmodels with the two-stage pretraining-then-adaption paradigm. Specifically, our\nwork demonstrates that task-agnostic pretraining on 2D vision and vision-text\ndatasets can build a generalizable model foundation for spatiotemporal\nlearning, followed by specialized joint training on spatiotemporal datasets to\nenhance task-specific adaptability. To improve the learning capabilities across\ndomains, our framework employs a rank-adaptive mixture-of-expert adaptation by\nusing fractional interpolation to relax the discrete variables so that can be\noptimized in the continuous space. Additionally, we introduce a temporal module\nto incorporate temporal dynamics explicitly. We evaluate our approach on a\nlarge-scale dataset covering 10 tasks across 4 disciplines, demonstrating that\na unified spatiotemporal model can achieve scalable, cross-task learning and\nsupport up to 10 tasks simultaneously within one model while reducing training\ncosts in multi-domain applications. Code will be available at\nhttps://github.com/1hunters/UniSTD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20752", "pdf": "https://arxiv.org/pdf/2503.20752", "abs": "https://arxiv.org/abs/2503.20752", "authors": ["Huajie Tan", "Yuheng Ji", "Xiaoshuai Hao", "Minglan Lin", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 22 figures", "summary": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation.cExperimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20776", "pdf": "https://arxiv.org/pdf/2503.20776", "abs": "https://arxiv.org/abs/2503.20776", "authors": ["Shijie Zhou", "Hui Ren", "Yijia Weng", "Shuwang Zhang", "Zhen Wang", "Dejia Xu", "Zhiwen Fan", "Suya You", "Zhangyang Wang", "Leonidas Guibas", "Achuta Kadambi"], "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20784", "pdf": "https://arxiv.org/pdf/2503.20784", "abs": "https://arxiv.org/abs/2503.20784", "authors": ["Jinwei Li", "Huan-ang Gao", "Wenyi Li", "Haohan Chi", "Chenyu Liu", "Chenxi Du", "Yiqian Liu", "Mingju Gao", "Guiyu Zhang", "Zongzheng Zhang", "Li Yi", "Yao Yao", "Jingwei Zhao", "Hongyang Li", "Yikai Wang", "Hao Zhao"], "title": "FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks", "categories": ["cs.CV"], "comment": "Project page:https://fb-4d.c7w.tech/", "summary": "With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20785", "pdf": "https://arxiv.org/pdf/2503.20785", "abs": "https://arxiv.org/abs/2503.20785", "authors": ["Tianqi Liu", "Zihao Huang", "Zhaoxi Chen", "Guangcong Wang", "Shoukang Hu", "Liao Shen", "Huiqiang Sun", "Zhiguo Cao", "Wei Li", "Ziwei Liu"], "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency", "categories": ["cs.CV"], "comment": "Project Page: https://free4d.github.io/ , Code:\n  https://github.com/TQTQliu/Free4D", "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19945", "pdf": "https://arxiv.org/pdf/2503.19945", "abs": "https://arxiv.org/abs/2503.19945", "authors": ["Daniel G. P. Petrini", "Hae Yong Kim"], "title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "8 pages", "summary": "This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20066", "pdf": "https://arxiv.org/pdf/2503.20066", "abs": "https://arxiv.org/abs/2503.20066", "authors": ["Zhirui Dai", "Hojoon Shin", "Yulun Tian", "Ki Myung Brian Lee", "Nikolay Atanasov"], "title": "Learning Scene-Level Signed Directional Distance Function with Ellipsoidal Priors and Neural Residuals", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Dense geometric environment representations are critical for autonomous\nmobile robot navigation and exploration. Recent work shows that implicit\ncontinuous representations of occupancy, signed distance, or radiance learned\nusing neural networks offer advantages in reconstruction fidelity, efficiency,\nand differentiability over explicit discrete representations based on meshes,\npoint clouds, and voxels. In this work, we explore a directional formulation of\nsigned distance, called signed directional distance function (SDDF). Unlike\nsigned distance function (SDF) and similar to neural radiance fields (NeRF),\nSDDF has a position and viewing direction as input. Like SDF and unlike NeRF,\nSDDF directly provides distance to the observed surface along the direction,\nrather than integrating along the view ray, allowing efficient view synthesis.\nTo learn and predict scene-level SDDF efficiently, we develop a differentiable\nhybrid representation that combines explicit ellipsoid priors and implicit\nneural residuals. This approach allows the model to effectively handle large\ndistance discontinuities around obstacle boundaries while preserving the\nability for dense high-fidelity prediction. We show that SDDF is competitive\nwith the state-of-the-art neural implicit scene models in terms of\nreconstruction accuracy and rendering efficiency, while allowing differentiable\nview prediction for robot trajectory optimization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20454", "pdf": "https://arxiv.org/pdf/2503.20454", "abs": "https://arxiv.org/abs/2503.20454", "authors": ["Yangqi Feng", "Shing-Ho J. Lin", "Baoyuan Gao", "Xian Wei"], "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "13 pages, 6 figures", "summary": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20681", "pdf": "https://arxiv.org/pdf/2503.20681", "abs": "https://arxiv.org/abs/2503.20681", "authors": ["Shuaikai Shi", "Qijun Zong"], "title": "Benchmarking Machine Learning Methods for Distributed Acoustic Sensing", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "comment": null, "summary": "Distributed acoustic sensing (DAS) technology represents an innovative\nfiber-optic-based sensing methodology that enables real-time acoustic signal\nmonitoring through the detection of minute perturbations along optical fibers.\nThis sensing approach offers compelling advantages, including extensive\nmeasurement ranges, exceptional spatial resolution, and an expansive dynamic\nmeasurement spectrum.\n  The integration of machine learning (ML) paradigms presents transformative\npotential for DAS technology, encompassing critical domains such as data\naugmentation, sophisticated preprocessing techniques, and advanced acoustic\nevent classification and recognition. By leveraging ML algorithms, DAS systems\ncan transition from traditional data processing methodologies to more automated\nand intelligent analytical frameworks.\n  The computational intelligence afforded by ML-enhanced DAS technologies\nfacilitates unprecedented monitoring capabilities across diverse critical\ninfrastructure sectors. Particularly noteworthy are the technology's\napplications in transportation infrastructure, energy management systems, and\nNatural disaster monitoring frameworks, where the precision of data acquisition\nand the reliability of intelligent decision-making mechanisms are paramount.\n  This research critically examines the comparative performance characteristics\nof classical machine learning methodologies and state-of-the-art deep learning\nmodels in the context of DAS data recognition and interpretation, offering\ncomprehensive insights into the evolving landscape of intelligent sensing\ntechnologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
