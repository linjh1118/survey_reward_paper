{"id": "2503.21679", "pdf": "https://arxiv.org/pdf/2503.21679", "abs": "https://arxiv.org/abs/2503.21679", "authors": ["Yunze Xiao", "Tingyu He", "Lionel Z. Wang", "Yiming Ma", "Xingyu Song", "Xiaohang Xu", "Irene Li", "Ka Chung Ng"], "title": "JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community", "categories": ["cs.CL", "cs.CY"], "comment": "20 pages, 1 figures", "summary": "This paper introduces JiraiBench, the first bilingual benchmark for\nevaluating large language models' effectiveness in detecting self-destructive\ncontent across Chinese and Japanese social media communities. Focusing on the\ntransnational \"Jirai\" (landmine) online subculture that encompasses multiple\nforms of self-destructive behaviors including drug overdose, eating disorders,\nand self-harm, we present a comprehensive evaluation framework incorporating\nboth linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese\nposts and 5,000 Japanese posts with multidimensional annotation along three\nbehavioral categories, achieving substantial inter-annotator agreement.\nExperimental evaluations across four state-of-the-art models reveal significant\nperformance variations based on instructional language, with Japanese prompts\nunexpectedly outperforming Chinese prompts when processing Chinese content.\nThis emergent cross-cultural transfer suggests that cultural proximity can\nsometimes outweigh linguistic similarity in detection tasks. Cross-lingual\ntransfer experiments with fine-tuned models further demonstrate the potential\nfor knowledge transfer between these language systems without explicit target\nlanguage training. These findings highlight the need for culturally-informed\napproaches to multilingual content moderation and provide empirical evidence\nfor the importance of cultural context in developing more effective detection\nsystems for vulnerable online communities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation", "agreement", "inter-annotator agreement"], "score": 6}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21745", "pdf": "https://arxiv.org/pdf/2503.21745", "abs": "https://arxiv.org/abs/2503.21745", "authors": ["Yuhan Zhang", "Mengchen Zhang", "Tong Wu", "Tengfei Wang", "Gordon Wetzstein", "Dahua Lin", "Ziwei Liu"], "title": "3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "correlation", "dimension"], "score": 6}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21380", "pdf": "https://arxiv.org/pdf/2503.21380", "abs": "https://arxiv.org/abs/2503.21380", "authors": ["Haoxiang Sun", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Zheng Liu", "Zhongyuan Wang", "Lei Fang", "Ji-Rong Wen"], "title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "Technical Report on Slow Thinking with LLMs: Evaluation Benchmark", "summary": "In recent years, the rapid development of large reasoning models has resulted\nin the saturation of existing benchmarks for evaluating mathematical reasoning,\nhighlighting the urgent need for more challenging and rigorous evaluation\nframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level\nmathematical benchmark, designed to rigorously test the complex reasoning\ncapabilities of LLMs. OlymMATH features 200 meticulously curated problems, each\nmanually verified and available in parallel English and Chinese versions. The\nproblems are systematically organized into two distinct difficulty tiers: (1)\nAIME-level problems (easy) that establish a baseline for mathematical reasoning\nassessment, and (2) significantly more challenging problems (hard) designed to\npush the boundaries of current state-of-the-art models. In our benchmark, these\nproblems span four core mathematical fields, each including a verifiable\nnumerical solution to enable objective, rule-based evaluation. Empirical\nresults underscore the significant challenge presented by OlymMATH, with\nstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini\ndemonstrating notably limited accuracy on the hard subset. Furthermore, the\nbenchmark facilitates comprehensive bilingual assessment of mathematical\nreasoning abilities-a critical dimension that remains largely unaddressed in\nmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark\nat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "mathematical reasoning", "dimension"], "score": 5}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20953", "pdf": "https://arxiv.org/pdf/2503.20953", "abs": "https://arxiv.org/abs/2503.20953", "authors": ["Julia Ive", "Felix Jozsa", "Nick Jackson", "Paulina Bondaronek", "Ciaran Scott Hill", "Richard Dobson"], "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 0.98 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability", "criteria"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21029", "pdf": "https://arxiv.org/pdf/2503.21029", "abs": "https://arxiv.org/abs/2503.21029", "authors": ["Jungyeul Park", "Yige Chen", "Kyuwon Kim", "KyungTae Lim", "Chulwoo Park"], "title": "Enhancing Korean Dependency Parsing with Morphosyntactic Features", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces UniDive for Korean, an integrated framework that\nbridges Universal Dependencies (UD) and Universal Morphology (UniMorph) to\nenhance the representation and processing of Korean {morphosyntax}. Korean's\nrich inflectional morphology and flexible word order pose challenges for\nexisting frameworks, which often treat morphology and syntax separately,\nleading to inconsistencies in linguistic analysis. UniDive unifies syntactic\nand morphological annotations by preserving syntactic dependencies while\nincorporating UniMorph-derived features, improving consistency in annotation.\nWe construct an integrated dataset and apply it to dependency parsing,\ndemonstrating that enriched morphosyntactic features enhance parsing accuracy,\nparticularly in distinguishing grammatical relations influenced by morphology.\nOur experiments, conducted with both encoder-only and decoder-only models,\nconfirm that explicit morphological information contributes to more accurate\nsyntactic analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21332", "pdf": "https://arxiv.org/pdf/2503.21332", "abs": "https://arxiv.org/abs/2503.21332", "authors": ["Taewon Yun", "Jihwan Oh", "Hyangsuk Min", "Yuho Lee", "Jihwan Bang", "Jason Cai", "Hwanjun Song"], "title": "ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Summarization refinement faces challenges when extending to multi-dimension.\nIn this paper, we introduce ReFeed, a powerful summarization refinement\npipeline that enhances multiple dimensions through reflective reasoning on\nfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based\ndataset optimized for training a lightweight model with reflective reasoning.\nOur experiments reveal how the number of dimensions, feedback exposure, and\nreasoning policy influence refinement performance, highlighting reflective\nreasoning and simultaneously addressing multiple feedback is crucial to\nmitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy\nfeedback and feedback order. Lastly, our finding emphasizes that creating data\nwith a proper goal and guideline constitutes a fundamental pillar of effective\nreasoning. The dataset and model will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization", "multi-dimensional", "dimension"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21190", "pdf": "https://arxiv.org/pdf/2503.21190", "abs": "https://arxiv.org/abs/2503.21190", "authors": ["Erika Mori", "Yue Qiu", "Hirokatsu Kataoka", "Yoshimitsu Aoki"], "title": "Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Social intelligence, the ability to interpret emotions, intentions, and\nbehaviors, is essential for effective communication and adaptive responses. As\nrobots and AI systems become more prevalent in caregiving, healthcare, and\neducation, the demand for AI that can interact naturally with humans grows.\nHowever, creating AI that seamlessly integrates multiple modalities, such as\nvision and speech, remains a challenge. Current video-based methods for social\nintelligence rely on general video recognition or emotion recognition\ntechniques, often overlook the unique elements inherent in human interactions.\nTo address this, we propose the Looped Video Debating (LVD) framework, which\nintegrates Large Language Models (LLMs) with visual information, such as facial\nexpressions and body movements, to enhance the transparency and reliability of\nquestion-answering tasks involving human interaction videos. Our results on the\nSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performance\nwithout fine-tuning. Furthermore, supplementary human annotations on existing\ndatasets provide insights into the model's accuracy, guiding future\nimprovements in AI-driven social intelligence.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21544", "pdf": "https://arxiv.org/pdf/2503.21544", "abs": "https://arxiv.org/abs/2503.21544", "authors": ["Yuwei Yin", "EunJeong Hwang", "Giuseppe Carenini"], "title": "SWI: Speaking with Intent in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "24 pages. Code: https://github.com/YuweiYin/SWI", "summary": "Intent, typically clearly formulated and planned, functions as a cognitive\nframework for reasoning and problem-solving. This paper introduces the concept\nof Speaking with Intent (SWI) in large language models (LLMs), where the\nexplicitly generated intent encapsulates the model's underlying intention and\nprovides high-level planning to guide subsequent analysis and communication. By\nemulating deliberate and purposeful thoughts in the human mind, SWI is\nhypothesized to enhance the reasoning capabilities and generation quality of\nLLMs. Extensive experiments on mathematical reasoning benchmarks consistently\ndemonstrate the superiority of Speaking with Intent over Baseline (i.e.,\ngeneration without explicit intent). Moreover, SWI outperforms answer-trigger\nprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive\nperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).\nAdditionally, the effectiveness and generalizability of SWI are solidified on\nreasoning-intensive question answering (QA) and text summarization benchmarks,\nwhere SWI brings consistent improvement to the Baseline generation. In text\nsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,\nand factual correctness, with fewer hallucinations. Furthermore, human\nevaluations verify the coherence, effectiveness, and interpretability of the\nintent produced by SWI. This proof-of-concept study creates a novel avenue for\nenhancing LLMs' reasoning abilities with cognitive notions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization", "question answering", "mathematical reasoning"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21309", "pdf": "https://arxiv.org/pdf/2503.21309", "abs": "https://arxiv.org/abs/2503.21309", "authors": ["Zixu Li", "Zhiheng Fu", "Yupeng Hu", "Zhiwei Chen", "Haokun Wen", "Liqiang Nie"], "title": "FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Composed Image Retrieval (CIR) facilitates image retrieval through a\nmultimodal query consisting of a reference image and modification text. The\nreference image defines the retrieval context, while the modification text\nspecifies desired alterations. However, existing CIR datasets predominantly\nemploy coarse-grained modification text (CoarseMT), which inadequately captures\nfine-grained retrieval intents. This limitation introduces two key challenges:\n(1) ignoring detailed differences leads to imprecise positive samples, and (2)\ngreater ambiguity arises when retrieving visually similar images. These issues\ndegrade retrieval accuracy, necessitating manual result filtering or repeated\nqueries. To address these limitations, we develop a robust fine-grained CIR\ndata annotation pipeline that minimizes imprecise positive samples and enhances\nCIR systems' ability to discern modification intents accurately. Using this\npipeline, we refine the FashionIQ and CIRR datasets to create two fine-grained\nCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,\nthe first CIR framework explicitly designed to parse the modification text.\nFineCIR effectively captures fine-grained modification semantics and aligns\nthem with ambiguous visual entities, enhancing retrieval precision. Extensive\nexperiments demonstrate that FineCIR consistently outperforms state-of-the-art\nCIR baselines on both fine-grained and traditional CIR benchmark datasets. Our\nFineCIR code and fine-grained CIR datasets are available at\nhttps://github.com/SDU-L/FineCIR.git.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21749", "pdf": "https://arxiv.org/pdf/2503.21749", "abs": "https://arxiv.org/abs/2503.21749", "authors": ["Shitian Zhao", "Qilong Wu", "Xinyue Li", "Bo Zhang", "Ming Li", "Qi Qin", "Dongyang Liu", "Kaipeng Zhang", "Hongsheng Li", "Yu Qiao", "Peng Gao", "Bin Fu", "Zhen Li"], "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis", "categories": ["cs.CV"], "comment": "Project page: https://zhaoshitian.github.io/lexart/", "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21755", "pdf": "https://arxiv.org/pdf/2503.21755", "abs": "https://arxiv.org/abs/2503.21755", "authors": ["Dian Zheng", "Ziqi Huang", "Hongbo Liu", "Kai Zou", "Yinan He", "Fan Zhang", "Yuanhan Zhang", "Jingwen He", "Wei-Shi Zheng", "Yu Qiao", "Ziwei Liu"], "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness", "categories": ["cs.CV"], "comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench", "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21505", "pdf": "https://arxiv.org/pdf/2503.21505", "abs": "https://arxiv.org/abs/2503.21505", "authors": ["Yue Li", "Meng Tian", "Zhenyu Lin", "Jiangtong Zhu", "Dechang Zhu", "Haiqiang Liu", "Zining Wang", "Yueyi Zhang", "Zhiwei Xiong", "Xinhai Zhao"], "title": "Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Existing benchmarks for Vision-Language Model (VLM) on autonomous driving\n(AD) primarily assess interpretability through open-form visual question\nanswering (QA) within coarse-grained tasks, which remain insufficient to assess\ncapabilities in complex driving scenarios. To this end, we introduce\n$\\textbf{VLADBench}$, a challenging and fine-grained dataset featuring\nclose-form QAs that progress from static foundational knowledge and elements to\nadvanced reasoning for dynamic on-road situations. The elaborate\n$\\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,\nGeneral Element Recognition, Traffic Graph Generation, Target Attribute\nComprehension, and Ego Decision-Making and Planning. These domains are further\nbroken down into 11 secondary aspects and 29 tertiary tasks for a granular\nevaluation. A thorough assessment of general and domain-specific (DS) VLMs on\nthis benchmark reveals both their strengths and critical limitations in AD\ncontexts. To further exploit the cognitive and reasoning interactions among the\n5 domains for AD understanding, we start from a small-scale VLM and train the\nDS models on individual domain datasets (collected from 1.4M DS QAs across\npublic sources). The experimental results demonstrate that the proposed\nbenchmark provides a crucial step toward a more comprehensive assessment of\nVLMs in AD, paving the way for the development of more cognitively\nsophisticated and reasoning-capable AD systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21699", "pdf": "https://arxiv.org/pdf/2503.21699", "abs": "https://arxiv.org/abs/2503.21699", "authors": ["Liuyue Xie", "George Z. Wei", "Avik Kuthiala", "Ce Zheng", "Ananya Bal", "Mosam Dabhi", "Liting Wen", "Taru Rustagi", "Ethan Lai", "Sushil Khyalia", "Rohan Choudhury", "Morteza Ziyadi", "Xu Zhang", "Hao Yang", "László A. Jeni"], "title": "MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": null, "summary": "Frontier models have either been language-only or have primarily focused on\nvision and language modalities. Although recent advancements in models with\nvision and audio understanding capabilities have shown substantial progress,\nthe field lacks a standardized evaluation framework for thoroughly assessing\ntheir cross-modality perception performance. We introduce MAVERIX~(Multimodal\nAudio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and\n2,556 questions explicitly designed to evaluate multimodal models through tasks\nthat necessitate close integration of video and audio information. MAVERIX\nuniquely provides models with audiovisual tasks, closely mimicking the\nmultimodal perceptual experiences available to humans during inference and\ndecision-making processes. To our knowledge, MAVERIX is the first benchmark\naimed explicitly at assessing comprehensive audiovisual integration.\nExperiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show\nperformance approaching human levels (around 70% accuracy), while human experts\nreach near-ceiling performance (95.1%). With standardized evaluation protocols,\na rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a\nchallenging testbed for advancing audiovisual multimodal intelligence.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "testbed", "accuracy"], "score": 4}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20835", "pdf": "https://arxiv.org/pdf/2503.20835", "abs": "https://arxiv.org/abs/2503.20835", "authors": ["Qichen Sun", "Yuxing Lu", "Kun Xia", "Li Chen", "He Sun", "Jinzhuo Wang"], "title": "Comprehensive Manuscript Assessment with Text Summarization Using 69707 articles", "categories": ["cs.CL"], "comment": null, "summary": "Rapid and efficient assessment of the future impact of research articles is a\nsignificant concern for both authors and reviewers. The most common standard\nfor measuring the impact of academic papers is the number of citations. In\nrecent years, numerous efforts have been undertaken to predict citation counts\nwithin various citation windows. However, most of these studies focus solely on\na specific academic field or require early citation counts for prediction,\nrendering them impractical for the early-stage evaluation of papers. In this\nwork, we harness Scopus to curate a significantly comprehensive and large-scale\ndataset of information from 69707 scientific articles sourced from 99 journals\nspanning multiple disciplines. We propose a deep learning methodology for the\nimpact-based classification tasks, which leverages semantic features extracted\nfrom the manuscripts and paper metadata. To summarize the semantic features,\nsuch as titles and abstracts, we employ a Transformer-based language model to\nencode semantic features and design a text fusion layer to capture shared\ninformation between titles and abstracts. We specifically focus on the\nfollowing impact-based prediction tasks using information of scientific\nmanuscripts in pre-publication stage: (1) The impact of journals in which the\nmanuscripts will be published. (2) The future impact of manuscripts themselves.\nExtensive experiments on our datasets demonstrate the superiority of our\nproposed model for impact-based prediction tasks. We also demonstrate\npotentials in generating manuscript's feedback and improvement suggestions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "summarization"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248", "abs": "https://arxiv.org/abs/2503.21248", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21295", "pdf": "https://arxiv.org/pdf/2503.21295", "abs": "https://arxiv.org/abs/2503.21295", "authors": ["Shuaijie She", "Junxiao Liu", "Yifeng Liu", "Jiajun Chen", "Xin Huang", "Shujian Huang"], "title": "R-PRM: Reasoning-Driven Process Reward Modeling", "categories": ["cs.CL"], "comment": "The project is available at https://github.com/NJUNLP/R-PRM", "summary": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21169", "pdf": "https://arxiv.org/pdf/2503.21169", "abs": "https://arxiv.org/abs/2503.21169", "authors": ["Jiahao Lyu", "Minghua Zhao", "Jing Hu", "Xuewen Huang", "Yifei Chen", "Shuangli Du"], "title": "VADMamba: Exploring State Space Models for Fast Video Anomaly Detection", "categories": ["cs.CV"], "comment": "Accpeted by ICME 2025", "summary": "Video anomaly detection (VAD) methods are mostly CNN-based or\nTransformer-based, achieving impressive results, but the focus on detection\naccuracy often comes at the expense of inference speed. The emergence of state\nspace models in computer vision, exemplified by the Mamba model, demonstrates\nimproved computational efficiency through selective scans and showcases the\ngreat potential for long-range modeling. Our study pioneers the application of\nMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frame\nprediction and optical flow reconstruction. Specifically, we propose the\nVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)\nlayer and Mamba-based Non-negative Visual State Space (NVSS) block.\nFurthermore, two individual VQ-MaU networks separately predict frames and\nreconstruct corresponding optical flows, further boosting accuracy through a\nclip-level fusion evaluation strategy. Experimental results validate the\nefficacy of the proposed VADMamba across three benchmark datasets,\ndemonstrating superior performance in inference speed compared to previous\nwork. Code is available at https://github.com/jLooo/VADMamba.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21500", "pdf": "https://arxiv.org/pdf/2503.21500", "abs": "https://arxiv.org/abs/2503.21500", "authors": ["Haote Yang", "Xingjian Wei", "Jiang Wu", "Noémi Ligeti-Nagy", "Jiaxing Sun", "Yinfan Wang", "Zijian Győző Yang", "Junyuan Gao", "Jingchao Wang", "Bowen Jiang", "Shasha Wang", "Nanjun Yu", "Zihao Zhang", "Shixin Hong", "Hongwei Liu", "Wei Li", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Gábor Prószéky", "Conghui He"], "title": "OpenHuEval: Evaluating Large Language Model on Hungarian Specifics", "categories": ["cs.CL"], "comment": null, "summary": "We introduce OpenHuEval, the first benchmark for LLMs focusing on the\nHungarian language and specifics. OpenHuEval is constructed from a vast\ncollection of Hungarian-specific materials sourced from multiple origins. In\nthe construction, we incorporated the latest design principles for evaluating\nLLMs, such as using real user queries from the internet, emphasizing the\nassessment of LLMs' generative capabilities, and employing LLM-as-judge to\nenhance the multidimensionality and accuracy of evaluations. Ultimately,\nOpenHuEval encompasses eight Hungarian-specific dimensions, featuring five\ntasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,\nin-depth, and scientifically accurate assessment of LLM performance in the\ncontext of the Hungarian language and its specifics. We evaluated current\nmainstream LLMs, including both traditional LLMs and recently developed Large\nReasoning Models. The results demonstrate the significant necessity for\nevaluation and model optimization tailored to the Hungarian language and\nspecifics. We also established the framework for analyzing the thinking\nprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms\nof these models in non-English languages, with Hungarian serving as a\nrepresentative example. We will release OpenHuEval at\nhttps://github.com/opendatalab/OpenHuEval .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21246", "pdf": "https://arxiv.org/pdf/2503.21246", "abs": "https://arxiv.org/abs/2503.21246", "authors": ["Haoyu Zhao", "Zhongang Qi", "Cong Wang", "Qingping Zheng", "Guansong Lu", "Fei Chen", "Hang Xu", "Zuxuan Wu"], "title": "DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation", "categories": ["cs.CV"], "comment": "11 pages, 10 figures", "summary": "Human image animation has recently gained significant attention due to\nadvancements in generative models. However, existing methods still face two\nmajor challenges: (1) architectural limitations, most models rely on U-Net,\nwhich underperforms compared to the MM-DiT; and (2) the neglect of textual\ninformation, which can enhance controllability. In this work, we introduce\nDynamiCtrl, a novel framework that not only explores different pose-guided\ncontrol structures in MM-DiT, but also reemphasizes the crucial role of text in\nthis task. Specifically, we employ a Shared VAE encoder for both reference\nimages and driving pose videos, eliminating the need for an additional pose\nencoder and simplifying the overall framework. To incorporate pose features\ninto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),\nwhich utilizes adaptive layer normalization to encode sparse pose features. The\nencoded features are directly added to the visual input, preserving the\nspatiotemporal consistency of the backbone while effectively introducing pose\ncontrol into MM-DiT. Furthermore, within the full attention mechanism, we align\ntextual and visual features to enhance controllability. By leveraging text, we\nnot only enable fine-grained control over the generated content, but also, for\nthe first time, achieve simultaneous control over both background and motion.\nExperimental results verify the superiority of DynamiCtrl on benchmark\ndatasets, demonstrating its strong identity preservation, heterogeneous\ncharacter driving, background controllability, and high-quality synthesis. The\nproject page is available at https://gulucaptain.github.io/DynamiCtrl/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21613", "pdf": "https://arxiv.org/pdf/2503.21613", "abs": "https://arxiv.org/abs/2503.21613", "authors": ["Javier Coronado-Blázquez"], "title": "Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach", "categories": ["cs.CL"], "comment": "22 pages, 6 figures", "summary": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "summarization"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21717", "pdf": "https://arxiv.org/pdf/2503.21717", "abs": "https://arxiv.org/abs/2503.21717", "authors": ["Jiefu Ou", "William Gantt Walden", "Kate Sanders", "Zhengping Jiang", "Kaiser Sun", "Jeffrey Cheng", "William Jurayj", "Miriam Wanner", "Shaobo Liang", "Candice Morgan", "Seunghoon Han", "Weiqi Wang", "Chandler May", "Hannah Recknor", "Daniel Khashabi", "Benjamin Van Durme"], "title": "CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?", "categories": ["cs.CL"], "comment": null, "summary": "A core part of scientific peer review involves providing expert critiques\nthat directly assess the scientific claims a paper makes. While it is now\npossible to automatically generate plausible (if generic) reviews, ensuring\nthat these reviews are sound and grounded in the papers' claims remains\nchallenging. To facilitate LLM benchmarking on these challenges, we introduce\nCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and\nreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for\nweakness statements in the reviews and the paper claims that they dispute, as\nwell as fine-grained labels of the validity, objectivity, and type of the\nidentified weaknesses. We benchmark several LLMs on three claim-centric tasks\nsupported by CLAIMCHECK, requiring models to (1) associate weaknesses with the\nclaims they dispute, (2) predict fine-grained labels for weaknesses and rewrite\nthe weaknesses to enhance their specificity, and (3) verify a paper's claims\nwith grounded reasoning. Our experiments reveal that cutting-edge LLMs, while\ncapable of predicting weakness labels in (2), continue to underperform relative\nto human experts on all other tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729", "abs": "https://arxiv.org/abs/2503.21729", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21323", "pdf": "https://arxiv.org/pdf/2503.21323", "abs": "https://arxiv.org/abs/2503.21323", "authors": ["Ling Feng", "Tianyu Xie", "Wei Ma", "Ruijie Fu", "Yingxiao Zhang", "Jun Li", "Bei Zhou"], "title": "DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The modernization of smart farming is a way to improve agricultural\nproduction efficiency, and improve the agricultural production environment.\nAlthough many large models have achieved high accuracy in the task of object\nrecognition and segmentation, they cannot really be put into use in the farming\nindustry due to their own poor interpretability and limitations in\ncomputational volume. In this paper, we built AnYue Shelduck Dateset, which\ncontains a total of 1951 Shelduck datasets, and performed target detection and\nsegmentation annotation with the help of professional annotators. Based on\nAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient and\npowerful module for duck identification based on real shelduckfarms. First of\nall, using the YOLOv8 module designed to divide the mahjong between them,\nPrecision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 on\nthe test set. Again using the DuckSegmentation segmentation model,\nDuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentation\nwas used as the teacher model, and through knowledge distillation, Deeplabv3\nr50 was used as the student model, and the final student model achieved 94.49%\nmIoU on the test set. The method provides a new way of thinking in practical\nsisal duck smart farming.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21760", "pdf": "https://arxiv.org/pdf/2503.21760", "abs": "https://arxiv.org/abs/2503.21760", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization", "question answering"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21457", "pdf": "https://arxiv.org/pdf/2503.21457", "abs": "https://arxiv.org/abs/2503.21457", "authors": ["Xiaoqin Wang", "Xusen Ma", "Xianxu Hou", "Meidan Ding", "Yudong Li", "Junliang Chen", "Wenting Chen", "Xiaoyang Peng", "Linlin Shen"], "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\ncapabilities in various tasks. However, effectively evaluating these MLLMs on\nface perception remains largely unexplored. To address this gap, we introduce\nFaceBench, a dataset featuring hierarchical multi-view and multi-level\nattributes specifically designed to assess the comprehensive face perception\nabilities of MLLMs. Initially, we construct a hierarchical facial attribute\nstructure, which encompasses five views with up to three levels of attributes,\ntotaling over 210 attributes and 700 attribute values. Based on the structure,\nthe proposed FaceBench consists of 49,919 visual question-answering (VQA) pairs\nfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop a\nrobust face perception MLLM baseline, Face-LLaVA, by training with our proposed\nface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVA\nare conducted to test their face perception ability, with results also compared\nagainst human performance. The results reveal that, the existing MLLMs are far\nfrom satisfactory in understanding the fine-grained facial attributes, while\nour Face-LLaVA significantly outperforms existing open-source models with a\nsmall amount of training data and is comparable to commercial ones like GPT-4o\nand Gemini. The dataset will be released at\nhttps://github.com/CVI-SZU/FaceBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21459", "pdf": "https://arxiv.org/pdf/2503.21459", "abs": "https://arxiv.org/abs/2503.21459", "authors": ["Chirag Parikh", "Deepti Rawat", "Rakshitha R. T.", "Tathagata Ghosh", "Ravi Kiran Sarvadevabhatla"], "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025; Project Page: https://roadsocial.github.io/", "summary": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for\ngeneric road event understanding from social media narratives. Unlike existing\ndatasets limited by regional bias, viewpoint bias and expert-driven\nannotations, RoadSocial captures the global complexity of road events with\nvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich social\ndiscourse. Our scalable semi-automatic annotation framework leverages Text LLMs\nand Video LLMs to generate comprehensive question-answer pairs across 12\nchallenging QA tasks, pushing the boundaries of road event understanding.\nRoadSocial is derived from social media videos spanning 14M frames and 414K\nsocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260K\nhigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,\ndriving-specific and general-purpose) on our road event understanding\nbenchmark. We also demonstrate RoadSocial's utility in improving road event\nunderstanding capabilities of general-purpose Video LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21483", "pdf": "https://arxiv.org/pdf/2503.21483", "abs": "https://arxiv.org/abs/2503.21483", "authors": ["Shuming Liu", "Chen Zhao", "Tianqi Xu", "Bernard Ghanem"], "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large video-language models (VLMs) have demonstrated promising progress in\nvarious video understanding tasks. However, their effectiveness in long-form\nvideo analysis is constrained by limited context windows. Traditional\napproaches, such as uniform frame sampling, often inevitably allocate resources\nto irrelevant content, diminishing their effectiveness in real-world scenarios.\nIn this paper, we introduce BOLT, a method to BOost Large VLMs without\nadditional Training through a comprehensive study of frame selection\nstrategies. First, to enable a more realistic evaluation of VLMs in long-form\nvideo understanding, we propose a multi-source retrieval evaluation setting.\nOur findings reveal that uniform sampling performs poorly in noisy contexts,\nunderscoring the importance of selecting the right frames. Second, we explore\nseveral frame selection strategies based on query-frame similarity and analyze\ntheir effectiveness at inference time. Our results show that inverse transform\nsampling yields the most significant performance improvement, increasing\naccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from\n58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21735", "pdf": "https://arxiv.org/pdf/2503.21735", "abs": "https://arxiv.org/abs/2503.21735", "authors": ["Arsham Gholamzadeh Khoee", "Shuai Wang", "Yinan Yu", "Robert Feldt", "Dhasarathy Parthasarathy"], "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Ensuring the reliability and effectiveness of software release decisions is\ncritical, particularly in safety-critical domains like automotive systems.\nPrecise analysis of release validation data, often presented in tabular form,\nplays a pivotal role in this process. However, traditional methods that rely on\nmanual analysis of extensive test datasets and validation metrics are prone to\ndelays and high costs. Large Language Models (LLMs) offer a promising\nalternative but face challenges in analytical reasoning, contextual\nunderstanding, handling out-of-scope queries, and processing structured test\ndata consistently; limitations that hinder their direct application in\nsafety-critical scenarios. This paper introduces GateLens, an LLM-based tool\nfor analyzing tabular data in the automotive domain. GateLens translates\nnatural language queries into Relational Algebra (RA) expressions and then\ngenerates optimized Python code. It outperforms the baseline system on\nbenchmarking datasets, achieving higher F1 scores and handling complex and\nambiguous queries with greater robustness. Ablation studies confirm the\ncritical role of the RA module, with performance dropping sharply when omitted.\nIndustrial evaluations reveal that GateLens reduces analysis time by over 80%\nwhile maintaining high accuracy and reliability. As demonstrated by presented\nresults, GateLens achieved high performance without relying on few-shot\nexamples, showcasing strong generalization across various query types from\ndiverse company roles. Insights from deploying GateLens with a partner\nautomotive company offer practical guidance for integrating AI into critical\nworkflows such as release validation. Results show that by automating test\nresult analysis, GateLens enables faster, more informed, and dependable release\ndecisions, and can thus advance software scalability and reliability in\nautomotive systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21622", "pdf": "https://arxiv.org/pdf/2503.21622", "abs": "https://arxiv.org/abs/2503.21622", "authors": ["Lars Heckler-Kram", "Jan-Hendrik Neudeck", "Ulla Scheler", "Rebecca König", "Carsten Steger"], "title": "The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": "paper under review; dataset first released for the VAND3.0 challenge\n  @ CVPR 2025 https://sites.google.com/view/vand30cvpr2025/challenge", "summary": "In recent years, performance on existing anomaly detection benchmarks like\nMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with\nstate-of-the-art models often competing in the range of less than one\npercentage point. This lack of discriminatory power prevents a meaningful\ncomparison of models and thus hinders progress of the field, especially when\nconsidering the inherent stochastic nature of machine learning results. We\npresent MVTec AD 2, a collection of eight anomaly detection scenarios with more\nthan 8000 high-resolution images. It comprises challenging and highly relevant\nindustrial inspection use cases that have not been considered in previous\ndatasets, including transparent and overlapping objects, dark-field and back\nlight illumination, objects with high variance in the normal data, and\nextremely small defects. We provide comprehensive evaluations of\nstate-of-the-art methods and show that their performance remains below 60%\naverage AU-PRO. Additionally, our dataset provides test scenarios with lighting\ncondition changes to assess the robustness of methods under real-world\ndistribution shifts. We host a publicly accessible evaluation server that holds\nthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).\nAll image data is available at\nhttps://www.mvtec.com/company/research/datasets/mvtec-ad-2.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21721", "pdf": "https://arxiv.org/pdf/2503.21721", "abs": "https://arxiv.org/abs/2503.21721", "authors": ["Jaywon Koo", "Jefferson Hernandez", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance", "categories": ["cs.CV"], "comment": null, "summary": "Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21771", "pdf": "https://arxiv.org/pdf/2503.21771", "abs": "https://arxiv.org/abs/2503.21771", "authors": ["Hongkai Lin", "Dingkang Liang", "Zhenghao Qi", "Xiang Bai"], "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. The code is available at https:\n  //github.com/HongkLin/TIDE", "summary": "Underwater dense prediction, especially depth estimation and semantic\nsegmentation, is crucial for gaining a comprehensive understanding of\nunderwater scenes. Nevertheless, high-quality and large-scale underwater\ndatasets with dense annotations remain scarce because of the complex\nenvironment and the exorbitant data collection costs. This paper proposes a\nunified Text-to-Image and DEnse annotation generation method (TIDE) for\nunderwater scenes. It relies solely on text as input to simultaneously generate\nrealistic underwater images and multiple highly consistent dense annotations.\nSpecifically, we unify the generation of text-to-image and text-to-dense\nannotations within a single model. The Implicit Layout Sharing mechanism (ILS)\nand cross-modal interaction method called Time Adaptive Normalization (TAN) are\nintroduced to jointly optimize the consistency between image and dense\nannotations. We synthesize a large-scale underwater dataset using TIDE to\nvalidate the effectiveness of our method in underwater dense prediction tasks.\nThe results demonstrate that our method effectively improves the performance of\nexisting underwater dense prediction models and mitigates the scarcity of\nunderwater data with dense annotations. We hope our method can offer new\nperspectives on alleviating data scarcity issues in other fields. The code is\navailable at https: //github.com/HongkLin/TIDE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20794", "pdf": "https://arxiv.org/pdf/2503.20794", "abs": "https://arxiv.org/abs/2503.20794", "authors": ["Veysel Kocaman", "Muhammed Santas", "Yigit Gul", "Mehmet Butgul", "David Talby"], "title": "Can Zero-Shot Commercial APIs Deliver Regulatory-Grade Clinical Text DeIdentification?", "categories": ["cs.CL", "cs.CR", "cs.IR", "cs.LG", "H.3, F.2.2, I.2.7"], "comment": "14 pages, accepted at Text2Story Workshop at ECIR 2025", "summary": "We systematically assess the performance of three leading API-based\nde-identification systems - Azure Health Data Services, AWS Comprehend Medical,\nand OpenAI GPT-4o - against our de-identification systems on a ground truth\ndataset of 48 clinical documents annotated by medical experts. Our analysis,\nconducted at both entity-level and token-level, demonstrates that our solution,\nHealthcare NLP, achieves the highest accuracy, with a 96% F1-score in protected\nhealth information (PHI) detection, significantly outperforming Azure (91%),\nAWS (83%), and GPT-4o (79%). Beyond accuracy, Healthcare NLP is also the most\ncost-effective solution, reducing processing costs by over 80% compared to\nAzure and GPT-4o. Its fixed-cost local deployment model avoids the escalating\nper-request fees of cloud-based services, making it a scalable and economical\nchoice. Our results underscore a critical limitation: zero-shot commercial APIs\nfail to meet the accuracy, adaptability, and cost-efficiency required for\nregulatory-grade clinical de-identification. Healthcare NLP's superior\nperformance, customization capabilities, and economic advantages position it as\nthe more viable solution for healthcare organizations seeking compliance and\nscalability in clinical NLP workflows.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21022", "pdf": "https://arxiv.org/pdf/2503.21022", "abs": "https://arxiv.org/abs/2503.21022", "authors": ["W. Riley Casper", "Bobby Orozco"], "title": "Reconstructing Gridded Data from Higher Autocorrelations", "categories": ["cs.CV", "math.GR", "physics.data-an", "20K01, 68T45, 68T10"], "comment": "13 pages, 1 figure", "summary": "The higher-order autocorrelations of integer-valued or rational-valued\ngridded data sets appear naturally in X-ray crystallography, and have\napplications in computer vision systems, correlation tomography, correlation\nspectroscopy, and pattern recognition. In this paper, we consider the problem\nof reconstructing a gridded data set from its higher-order autocorrelations. We\ndescribe an explicit reconstruction algorithm, and prove that the\nautocorrelations up to order 3r + 3 are always sufficient to determine the data\nup to translation, where r is the dimension of the grid. We also provide\nexamples of rational-valued gridded data sets which are not determined by their\nautocorrelations up to order 3r + 2.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "dimension"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20995", "pdf": "https://arxiv.org/pdf/2503.20995", "abs": "https://arxiv.org/abs/2503.20995", "authors": ["Xiaomin Li", "Xupeng Chen", "Jingxuan Fan", "Eric Hanchen Jiang", "Mingye Gao"], "title": "Multi-head Reward Aggregation Guided by Entropy", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with safety guidelines typically\ninvolves reinforcement learning from human feedback (RLHF), relying on\nhuman-generated preference annotations. However, assigning consistent overall\nquality ratings is challenging, prompting recent research to shift towards\ndetailed evaluations based on multiple specific safety criteria. This paper\nuncovers a consistent observation: safety rules characterized by high rating\nentropy are generally less reliable in identifying responses preferred by\nhumans. Leveraging this finding, we introduce ENCORE, a straightforward\nentropy-guided approach that composes multi-head rewards by downweighting rules\nexhibiting high rating entropy. Theoretically, we demonstrate that rules with\nelevated entropy naturally receive minimal weighting in the Bradley-Terry\noptimization framework, justifying our entropy-based penalization. Through\nextensive experiments on RewardBench safety tasks, our method significantly\nsurpasses several competitive baselines, including random weighting, uniform\nweighting, single-head Bradley-Terry models, and LLM-based judging methods. Our\nproposed approach is training-free, broadly applicable to various datasets, and\nmaintains interpretability, offering a practical and effective solution for\nmulti-attribute reward modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "criteria"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21004", "pdf": "https://arxiv.org/pdf/2503.21004", "abs": "https://arxiv.org/abs/2503.21004", "authors": ["Mahmoud Alwakeel", "Emory Buck", "Jonathan G. Martin", "Imran Aslam", "Sudarshan Rajagopal", "Jian Pei", "Mihai V. Podgoreanu", "Christopher J. Lindsell", "An-Kwok Ian Wong"], "title": "Evaluating Large Language Models for Automated Clinical Abstraction in Pulmonary Embolism Registries: Performance Across Model Sizes, Versions, and Parameters", "categories": ["cs.CL"], "comment": null, "summary": "Pulmonary embolism (PE) is a leading cause of cardiovascular mortality, yet\nour understanding of optimal management remains limited due to heterogeneous\nand inaccessible radiology documentation. The PERT Consortium registry\nstandardizes PE management data but depends on resource-intensive manual\nabstraction. Large language models (LLMs) offer a scalable alternative for\nautomating concept extraction from computed tomography PE (CTPE) reports. This\nstudy evaluated the accuracy of LLMs in extracting PE-related concepts compared\nto a human-curated criterion standard. We retrospectively analyzed MIMIC-IV and\nDuke Health CTPE reports using multiple LLaMA models. Larger models (70B)\noutperformed smaller ones (8B), achieving kappa values of 0.98 (PE detection),\n0.65-0.75 (PE location), 0.48-0.51 (right heart strain), and 0.65-0.70 (image\nartifacts). Moderate temperature tuning (0.2-0.5) improved accuracy, while\nexcessive in-context examples reduced performance. A dual-model review\nframework achieved >80-90% precision. LLMs demonstrate strong potential for\nautomating PE registry abstraction, minimizing manual workload while preserving\naccuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["kappa", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21011", "pdf": "https://arxiv.org/pdf/2503.21011", "abs": "https://arxiv.org/abs/2503.21011", "authors": ["Ana Ma", "Derek Powell"], "title": "Can Large Language Models Predict Associations Among Human Attitudes?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21104", "pdf": "https://arxiv.org/pdf/2503.21104", "abs": "https://arxiv.org/abs/2503.21104", "authors": ["Yuyin Chen", "Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Yifei Zhan", "Xianpeng Lang"], "title": "StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Urban scene reconstruction requires modeling both static infrastructure and\ndynamic elements while supporting diverse environmental conditions. We present\n\\textbf{StyledStreets}, a multi-style street simulator that achieves\ninstruction-driven scene editing with guaranteed spatial and temporal\nconsistency. Building on a state-of-the-art Gaussian Splatting framework for\nstreet scenarios enhanced by our proposed pose optimization and multi-view\ntraining, our method enables photorealistic style transfers across seasons,\nweather conditions, and camera setups through three key innovations: First, a\nhybrid embedding scheme disentangles persistent scene geometry from transient\nstyle attributes, allowing realistic environmental edits while preserving\nstructural integrity. Second, uncertainty-aware rendering mitigates supervision\nnoise from diffusion priors, enabling robust training across extreme style\nvariations. Third, a unified parametric model prevents geometric drift through\nregularized updates, maintaining multi-view consistency across seven\nvehicle-mounted cameras.\n  Our framework preserves the original scene's motion patterns and geometric\nrelationships. Qualitative results demonstrate plausible transitions between\ndiverse conditions (snow, sandstorm, night), while quantitative evaluations\nshow state-of-the-art geometric accuracy under style transfers. The approach\nestablishes new capabilities for urban simulation, with applications in\nautonomous vehicle testing and augmented reality systems requiring reliable\nenvironmental consistency. Codes will be publicly available upon publication.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21140", "pdf": "https://arxiv.org/pdf/2503.21140", "abs": "https://arxiv.org/abs/2503.21140", "authors": ["Junjie Chen", "Weilong Chen", "Yifan Zuo", "Yuming Fang"], "title": "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Category-agnostic pose estimation aims to locate keypoints on query images\naccording to a few annotated support images for arbitrary novel classes.\nExisting methods generally extract support features via heatmap pooling, and\nobtain interacted features from support and query via cross-attention. Hence,\nthese works neglect to mine fine-grained and structure-aware (FGSA) features\nfrom both support and query images, which are crucial for pixel-level keypoint\nlocalization. To this end, we propose a novel yet concise framework, which\nrecurrently mines FGSA features from both support and query images.\nSpecifically, we design a FGSA mining module based on deformable attention\nmechanism. On the one hand, we mine fine-grained features by applying\ndeformable attention head over multi-scale feature maps. On the other hand, we\nmine structure-aware features by offsetting the reference points of keypoints\nto their linked keypoints. By means of above module, we recurrently mine FGSA\nfeatures from support and query images, and thus obtain better support features\nand query estimations. In addition, we propose to use mixup keypoints to pad\nvarious classes to a unified keypoint number, which could provide richer\nsupervision than the zero padding used in existing works. We conduct extensive\nexperiments and in-depth studies on large-scale MP-100 dataset, and outperform\nSOTA method dramatically (+3.2\\%PCK@0.05). Code is avaiable at\nhttps://github.com/chenbys/FMMP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21378", "pdf": "https://arxiv.org/pdf/2503.21378", "abs": "https://arxiv.org/abs/2503.21378", "authors": ["Kota Dohi", "Tomoya Nishida", "Harsh Purohit", "Takashi Endo", "Yohei Kawaguchi"], "title": "Retrieving Time-Series Differences Using Natural Language Queries", "categories": ["cs.CL"], "comment": null, "summary": "Effectively searching time-series data is essential for system analysis;\nhowever, traditional methods often require domain expertise to define search\ncriteria. Recent advancements have enabled natural language-based search, but\nthese methods struggle to handle differences between time-series data. To\naddress this limitation, we propose a natural language query-based approach for\nretrieving pairs of time-series data based on differences specified in the\nquery. Specifically, we define six key characteristics of differences,\nconstruct a corresponding dataset, and develop a contrastive learning-based\nmodel to align differences between time-series data with query texts.\nExperimental results demonstrate that our model achieves an overall mAP score\nof 0.994 in retrieving time-series pairs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "criteria"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393", "abs": "https://arxiv.org/abs/2503.21393", "authors": ["Rohitash Chandra", "Aryan Chaudhary", "Yeshwanth Rayavarapu"], "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21208", "pdf": "https://arxiv.org/pdf/2503.21208", "abs": "https://arxiv.org/abs/2503.21208", "authors": ["Wenxuan Qiu", "Chengxin Xie", "Jingui Huang"], "title": "An improved EfficientNetV2 for garbage classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an enhanced waste classification framework based on\nEfficientNetV2 to address challenges in data acquisition cost, generalization,\nand real-time performance. We propose a Channel-Efficient Attention\n(CE-Attention) module that mitigates feature loss during global pooling without\nintroducing dimensional scaling, effectively enhancing critical feature\nextraction. Additionally, a lightweight multi-scale spatial feature extraction\nmodule (SAFM) is developed by integrating depthwise separable convolutions,\nsignificantly reducing model complexity. Comprehensive data augmentation\nstrategies are further employed to improve generalization. Experiments on the\nHuawei Cloud waste classification dataset demonstrate that our method achieves\na classification accuracy of 95.4\\%, surpassing the baseline by 3.2\\% and\noutperforming mainstream models. The results validate the effectiveness of our\napproach in balancing accuracy and efficiency for practical waste\nclassification scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21464", "pdf": "https://arxiv.org/pdf/2503.21464", "abs": "https://arxiv.org/abs/2503.21464", "authors": ["Ryan Marinelli", "Josef Pichlmeier", "Tamas Bisztray"], "title": "Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "In this work, we propose a metric called Number of Thoughts (NofT) to\ndetermine the difficulty of tasks pre-prompting and support Large Language\nModels (LLMs) in production contexts. By setting thresholds based on the number\nof thoughts, this metric can discern the difficulty of prompts and support more\neffective prompt routing. A 2% decrease in latency is achieved when routing\nprompts from the MathInstruct dataset through quantized, distilled versions of\nDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, this\nmetric can be used to detect adversarial prompts used in prompt injection\nattacks with high efficacy. The Number of Thoughts can inform a classifier that\nachieves 95% accuracy in adversarial prompt detection. Our experiments ad\ndatasets used are available on our GitHub page:\nhttps://github.com/rymarinelli/Number_Of_Thoughts/tree/main.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21210", "pdf": "https://arxiv.org/pdf/2503.21210", "abs": "https://arxiv.org/abs/2503.21210", "authors": ["Yueying Gao", "Dongliang Chang", "Bingyao Yu", "Haotian Qin", "Lei Chen", "Kongming Liang", "Zhanyu Ma"], "title": "FakeReasoning: Towards Generalizable Forgery Detection and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and interpretable detection of AI-generated images is essential for\nmitigating risks associated with AI misuse. However, the substantial domain gap\namong generative models makes it challenging to develop a generalizable forgery\ndetection model. Moreover, since every pixel in an AI-generated image is\nsynthesized, traditional saliency-based forgery explanation methods are not\nwell suited for this task. To address these challenges, we propose modeling\nAI-generated image detection and explanation as a Forgery Detection and\nReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provide\naccurate detection through structured and reliable reasoning over forgery\nattributes. To facilitate this task, we introduce the Multi-Modal Forgery\nReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K images\nacross 10 generative models, with 10 types of forgery reasoning annotations,\nenabling comprehensive evaluation of FDR-Task. Additionally, we propose\nFakeReasoning, a forgery detection and reasoning framework with two key\ncomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'\nunderstanding of forgery-related semantics through both cross-modal and\nintra-modal contrastive learning between images and forgery attribute\nreasoning. Second, a Classification Probability Mapper bridges the optimization\ngap between forgery detection and language modeling by mapping the output\nlogits of VLMs to calibrated binary classification probabilities. Experiments\nacross multiple generative models demonstrate that FakeReasoning not only\nachieves robust generalization but also outperforms state-of-the-art methods on\nboth detection and reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21530", "pdf": "https://arxiv.org/pdf/2503.21530", "abs": "https://arxiv.org/abs/2503.21530", "authors": ["Umer Butt", "Stalin Veranasi", "Günter Neumann"], "title": "Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the Information Retrieval (IR) field increasingly recognizes the\nimportance of inclusivity, addressing the needs of low-resource languages\nremains a significant challenge. Transliteration between Urdu and its Romanized\nform, Roman Urdu, remains underexplored despite the widespread use of both\nscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl dataset\nshowed promising results but suffered from poor domain adaptability and limited\nevaluation. We propose a transformer-based approach using the m2m100\nmultilingual translation model, enhanced with masked language modeling (MLM)\npretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverse\nDakshina dataset. To address previous evaluation flaws, we introduce rigorous\ndataset splits and assess performance using BLEU, character-level BLEU, and\nCHRF. Our model achieves strong transliteration performance, with Char-BLEU\nscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. These\nresults outperform both RNN baselines and GPT-4o Mini and demonstrate the\neffectiveness of multilingual transfer learning for low-resource\ntransliteration tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21250", "pdf": "https://arxiv.org/pdf/2503.21250", "abs": "https://arxiv.org/abs/2503.21250", "authors": ["Mohamed Lamine Mekhalfi", "Paul Chippendale", "Francisco Fraile", "Marcos Rico"], "title": "Orange Quality Grading with Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "Orange grading is a crucial step in the fruit industry, as it helps to sort\noranges according to different criteria such as size, quality, ripeness, and\nhealth condition, ensuring safety for human consumption and better price\nallocation and client satisfaction. Automated grading enables faster\nprocessing, precision, and reduced human labor. In this paper, we implement a\ndeep learning-based solution for orange grading via machine vision. Unlike\ntypical grading systems that analyze fruits from a single view, we capture\nmultiview images of each single orange in order to enable a richer\nrepresentation. Afterwards, we compose the acquired images into one collage.\nThis enables the analysis of the whole orange skin. We train a convolutional\nneural network (CNN) on the composed images to grade the oranges into three\nclasses, namely good, bad, and undefined. We also evaluate the performance with\ntwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally that\nmulti-view grading is superior to single view grading.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "criteria"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21259", "pdf": "https://arxiv.org/pdf/2503.21259", "abs": "https://arxiv.org/abs/2503.21259", "authors": ["Wencheng Han", "Dongqian Guo", "Xiao Chen", "Pang Lyu", "Yi Jin", "Jianbing Shen"], "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data", "categories": ["cs.CV"], "comment": null, "summary": "Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https://um-lab.github.io/GSI-MAR/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21338", "pdf": "https://arxiv.org/pdf/2503.21338", "abs": "https://arxiv.org/abs/2503.21338", "authors": ["Yehui Shen", "Lei Zhang", "Qingqiu Li", "Xiongwei Zhao", "Yue Wang", "Huimin Lu", "Xieyuanli Chen"], "title": "UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based on Uncertainty-Guided NeRF Augmentation", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "summary": "Visual place recognition (VPR) is crucial for robots to identify previously\nvisited locations, playing an important role in autonomous navigation in both\nindoor and outdoor environments. However, most existing VPR datasets are\nlimited to single-viewpoint scenarios, leading to reduced recognition accuracy,\nparticularly in multi-directional driving or feature-sparse scenes. Moreover,\nobtaining additional data to mitigate these limitations is often expensive.\nThis paper introduces a novel training paradigm to improve the performance of\nexisting VPR networks by enhancing multi-view diversity within current datasets\nthrough uncertainty estimation and NeRF-based data augmentation. Specifically,\nwe initially train NeRF using the existing VPR dataset. Then, our devised\nself-supervised uncertainty estimation network identifies places with high\nuncertainty. The poses of these uncertain places are input into NeRF to\ngenerate new synthetic observations for further training of VPR networks.\nAdditionally, we propose an improved storage method for efficient organization\nof augmented and original training data. We conducted extensive experiments on\nthree datasets and tested three different VPR backbone networks. The results\ndemonstrate that our proposed training paradigm significantly improves VPR\nperformance by fully utilizing existing data, outperforming other training\napproaches. We further validated the effectiveness of our approach on\nself-recorded indoor and outdoor datasets, consistently demonstrating superior\nresults. Our dataset and code have been released at\n\\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21367", "pdf": "https://arxiv.org/pdf/2503.21367", "abs": "https://arxiv.org/abs/2503.21367", "authors": ["Bořek Reich", "Matej Kunda", "Fedor Zolotarev", "Tuomas Eerola", "Pavel Zemčík", "Tomi Kauppi"], "title": "Multimodal surface defect detection from wooden logs for sawing optimization", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel, good-quality, and less demanding method for detecting\nknots on the surface of wooden logs using multimodal data fusion. Knots are a\nprimary factor affecting the quality of sawn timber, making their detection\nfundamental to any timber grading or cutting optimization system. While X-ray\ncomputed tomography provides accurate knot locations and internal structures,\nit is often too slow or expensive for practical use. An attractive alternative\nis to use fast and cost-effective log surface measurements, such as laser\nscanners or RGB cameras, to detect surface knots and estimate the internal\nstructure of wood. However, due to the small size of knots and noise caused by\nfactors, such as bark and other natural variations, detection accuracy often\nremains low when only one measurement modality is used. In this paper, we\ndemonstrate that by using a data fusion pipeline consisting of separate streams\nfor RGB and point cloud data, combined by a late fusion module, higher knot\ndetection accuracy can be achieved compared to using either modality alone. We\nfurther propose a simple yet efficient sawing angle optimization method that\nutilizes surface knot detections and cross-correlation to minimize the amount\nof unwanted arris knots, demonstrating its benefits over randomized sawing\nangles.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21438", "pdf": "https://arxiv.org/pdf/2503.21438", "abs": "https://arxiv.org/abs/2503.21438", "authors": ["Anis Ur Rahman", "Einari Heinaro", "Mete Ahishali", "Samuli Junttila"], "title": "Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery", "categories": ["cs.CV"], "comment": "11 pages, 4 figures, 4 tables", "summary": "Mapping standing dead trees is critical for assessing forest health,\nmonitoring biodiversity, and mitigating wildfire risks, for which aerial\nimagery has proven useful. However, dense canopy structures, spectral overlaps\nbetween living and dead vegetation, and over-segmentation errors limit the\nreliability of existing methods. This study introduces a hybrid postprocessing\nframework that refines deep learning-based tree segmentation by integrating\nwatershed algorithms with adaptive filtering, enhancing boundary delineation,\nand reducing false positives in complex forest environments. Tested on\nhigh-resolution aerial imagery from boreal forests, the framework improved\ninstance-level segmentation accuracy by 41.5% and reduced positional errors by\n57%, demonstrating robust performance in densely vegetated regions. By\nbalancing detection accuracy and over-segmentation artifacts, the method\nenabled the precise identification of individual dead trees, which is critical\nfor ecological monitoring. The framework's computational efficiency supports\nscalable applications, such as wall-to-wall tree mortality mapping over large\ngeographic regions using aerial or satellite imagery. These capabilities\ndirectly benefit wildfire risk assessment (identifying fuel accumulations),\ncarbon stock estimation (tracking emissions from decaying biomass), and\nprecision forestry (targeting salvage loggings). By bridging advanced remote\nsensing techniques with practical forest management needs, this work advances\ntools for large-scale ecological conservation and climate resilience planning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21477", "pdf": "https://arxiv.org/pdf/2503.21477", "abs": "https://arxiv.org/abs/2503.21477", "authors": ["Wenyi Xiong", "Jian Chen", "Ziheng Qi"], "title": "Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction Method", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE TIM for possible publication", "summary": "Trajectory prediction, as a critical component of autonomous driving systems,\nhas attracted the attention of many researchers. Existing prediction algorithms\nfocus on extracting more detailed scene features or selecting more reasonable\ntrajectory destinations. However, in the face of dynamic and evolving future\nmovements of the target vehicle, these algorithms cannot provide a fine-grained\nand continuous description of future behaviors and lane constraints, which\ndegrades the prediction accuracy. To address this challenge, we present BLNet,\na novel dualstream architecture that synergistically integrates behavioral\nintention recognition and lane constraint modeling through parallel attention\nmechanisms. The framework generates fine-grained behavior state queries\n(capturing spatial-temporal movement patterns) and lane queries (encoding lane\ntopology constraints), supervised by two auxiliary losses, respectively.\nSubsequently, a two-stage decoder first produces trajectory proposals, then\nperforms point-level refinement by jointly incorporating both the continuity of\npassed lanes and future motion features. Extensive experiments on two large\ndatasets, nuScenes and Argoverse, show that our network exhibits significant\nperformance gains over existing direct regression and goal-based algorithms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21525", "pdf": "https://arxiv.org/pdf/2503.21525", "abs": "https://arxiv.org/abs/2503.21525", "authors": ["Yuxi Hu", "Jun Zhang", "Zhe Zhang", "Rafael Weilharter", "Yuchen Rao", "Kuangyi Chen", "Runze Yuan", "Friedrich Fraundorfer"], "title": "ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D point\nclouds from a series of overlapping images. Recent learning-based MVS\nframeworks overlook the geometric information embedded in features and\ncorrelations, leading to weak cost matching. In this paper, we propose\nICG-MVSNet, which explicitly integrates intra-view and cross-view relationships\nfor depth estimation. Specifically, we develop an intra-view feature fusion\nmodule that leverages the feature coordinate correlations within a single image\nto enhance robust cost matching. Additionally, we introduce a lightweight\ncross-view aggregation module that efficiently utilizes the contextual\ninformation from volume correlations to guide regularization. Our method is\nevaluated on the DTU dataset and Tanks and Temples benchmark, consistently\nachieving competitive performance against state-of-the-art works, while\nrequiring lower computational resources.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21695", "pdf": "https://arxiv.org/pdf/2503.21695", "abs": "https://arxiv.org/abs/2503.21695", "authors": ["Jiahe Qian", "Yaoyu Fang", "Jinkui Hao", "Bo Zhou"], "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 tables, 2 figures", "summary": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21767", "pdf": "https://arxiv.org/pdf/2503.21767", "abs": "https://arxiv.org/abs/2503.21767", "authors": ["Hairong Yin", "Huangying Zhan", "Yi Xu", "Raymond A. Yeh"], "title": "Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary querying in 3D Gaussian Splatting aims to identify\nsemantically relevant regions within a 3D Gaussian representation based on a\ngiven text query. Prior work, such as LangSplat, addressed this task by\nretrieving these regions in the form of segmentation masks on 2D renderings.\nMore recently, OpenGaussian introduced point-level querying, which directly\nselects a subset of 3D Gaussians. In this work, we propose a point-level\nquerying method that builds upon LangSplat's framework. Our approach improves\nthe framework in two key ways: (a) we leverage masklets from the Segment\nAnything Model 2 (SAM2) to establish semantic consistent ground-truth for\ndistilling the language Gaussians; (b) we introduces a novel two-step querying\napproach that first retrieves the distilled ground-truth and subsequently uses\nthe ground-truth to query the individual Gaussians. Experimental evaluations on\nthree benchmark datasets demonstrate that the proposed method achieves better\nperformance compared to state-of-the-art approaches. For instance, our method\nachieves an mIoU improvement of +20.42 on the 3D-OVS dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21776", "pdf": "https://arxiv.org/pdf/2503.21776", "abs": "https://arxiv.org/abs/2503.21776", "authors": ["Kaituo Feng", "Kaixiong Gong", "Bohao Li", "Zonghao Guo", "Yibing Wang", "Tianshuo Peng", "Benyou Wang", "Xiangyu Yue"], "title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "categories": ["cs.CV"], "comment": "Project page: https://github.com/tulerfeng/Video-R1", "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21778", "pdf": "https://arxiv.org/pdf/2503.21778", "abs": "https://arxiv.org/abs/2503.21778", "authors": ["Ziren Gong", "Fabio Tosi", "Youmin Zhang", "Stefano Mattoccia", "Matteo Poggi"], "title": "HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM", "categories": ["cs.CV"], "comment": "ICRA 2025. Project Page: https://zorangong.github.io/HS-SLAM/", "summary": "NeRF-based SLAM has recently achieved promising results in tracking and\nreconstruction. However, existing methods face challenges in providing\nsufficient scene representation, capturing structural information, and\nmaintaining global consistency in scenes emerging significant movement or being\nforgotten. To this end, we present HS-SLAM to tackle these problems. To enhance\nscene representation capacity, we propose a hybrid encoding network that\ncombines the complementary strengths of hash-grid, tri-planes, and one-blob,\nimproving the completeness and smoothness of reconstruction. Additionally, we\nintroduce structural supervision by sampling patches of non-local pixels rather\nthan individual rays to better capture the scene structure. To ensure global\nconsistency, we implement an active global bundle adjustment (BA) to eliminate\ncamera drifts and mitigate accumulative errors. Experimental results\ndemonstrate that HS-SLAM outperforms the baselines in tracking and\nreconstruction accuracy while maintaining the efficiency required for robotics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21054", "pdf": "https://arxiv.org/pdf/2503.21054", "abs": "https://arxiv.org/abs/2503.21054", "authors": ["Yiqing Shen", "Chenjia Li", "Bohan Liu", "Cheng-Yi Li", "Tito Porras", "Mathias Unberath"], "title": "Operating Room Workflow Analysis via Reasoning Segmentation over Digital Twins", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Analyzing operating room (OR) workflows to derive quantitative insights into\nOR efficiency is important for hospitals to maximize patient care and financial\nsustainability. Prior work on OR-level workflow analysis has relied on\nend-to-end deep neural networks. While these approaches work well in\nconstrained settings, they are limited to the conditions specified at\ndevelopment time and do not offer the flexibility necessary to accommodate the\nOR workflow analysis needs of various OR scenarios (e.g., large academic center\nvs. rural provider) without data collection, annotation, and retraining.\nReasoning segmentation (RS) based on foundation models offers this flexibility\nby enabling automated analysis of OR workflows from OR video feeds given only\nan implicit text query related to the objects of interest. Due to the reliance\non large language model (LLM) fine-tuning, current RS approaches struggle with\nreasoning about semantic/spatial relationships and show limited generalization\nto OR video due to variations in visual characteristics and domain-specific\nterminology. To address these limitations, we first propose a novel digital\ntwin (DT) representation that preserves both semantic and spatial relationships\nbetween the various OR components. Then, building on this foundation, we\npropose ORDiRS (Operating Room Digital twin representation for Reasoning\nSegmentation), an LLM-tuning-free RS framework that reformulates RS into a\n\"reason-retrieval-synthesize\" paradigm. Finally, we present ORDiRS-Agent, an\nLLM-based agent that decomposes OR workflow analysis queries into manageable RS\nsub-queries and generates responses by combining detailed textual explanations\nwith supporting visual evidence from RS. Experimental results on both an\nin-house and a public OR dataset demonstrate that our ORDiRS achieves a cIoU\nimprovement of 6.12%-9.74% compared to the existing state-of-the-arts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21425", "pdf": "https://arxiv.org/pdf/2503.21425", "abs": "https://arxiv.org/abs/2503.21425", "authors": ["Yongxu Wang", "Xu Cao", "Weiyun Yi", "Zhaoxin Fan"], "title": "STAMICS: Splat, Track And Map with Integrated Consistency and Semantics for Dense RGB-D SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,\nenabling systems to autonomously navigate and understand complex environments.\nCurrent SLAM approaches predominantly rely on geometric cues for mapping and\nlocalization, but they often fail to ensure semantic consistency, particularly\nin dynamic or densely populated scenes. To address this limitation, we\nintroduce STAMICS, a novel method that integrates semantic information with 3D\nGaussian representations to enhance both localization and mapping accuracy.\nSTAMICS consists of three key components: a 3D Gaussian-based scene\nrepresentation for high-fidelity reconstruction, a graph-based clustering\ntechnique that enforces temporal semantic consistency, and an open-vocabulary\nsystem that allows for the classification of unseen objects. Extensive\nexperiments show that STAMICS significantly improves camera pose estimation and\nmap quality, outperforming state-of-the-art methods while reducing\nreconstruction errors. Code will be public available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21634", "pdf": "https://arxiv.org/pdf/2503.21634", "abs": "https://arxiv.org/abs/2503.21634", "authors": ["Yassir Lairgi"], "title": "When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The accurate determination of the beginning of each Hijri month is essential\nfor religious, cultural, and administrative purposes. Manazel (The code and\ndatasets are available at https://github.com/lairgiyassir/manazel) addresses\nthis challenge in Morocco by leveraging 13 years of crescent visibility data to\nrefine the ODEH criterion, a widely used standard for lunar crescent visibility\nprediction. The study integrates two key features, the Arc of Vision (ARCV) and\nthe total width of the crescent (W), to enhance the accuracy of lunar\nvisibility assessments. A machine learning approach utilizing the Logistic\nRegression algorithm is employed to classify crescent visibility conditions,\nachieving a predictive accuracy of 98.83%. This data-driven methodology offers\na robust and reliable framework for determining the start of the Hijri month,\ncomparing different data classification tools, and improving the consistency of\nlunar calendar calculations in Morocco. The findings demonstrate the\neffectiveness of machine learning in astronomical applications and highlight\nthe potential for further enhancements in the modeling of crescent visibility.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21696", "pdf": "https://arxiv.org/pdf/2503.21696", "abs": "https://arxiv.org/abs/2503.21696", "authors": ["Wenqi Zhang", "Mengna Wang", "Gangao Liu", "Xu Huixin", "Yiwei Jiang", "Yongliang Shen", "Guiyang Hou", "Zhe Zheng", "Hang Zhang", "Xin Li", "Weiming Lu", "Peng Li", "Yueting Zhuang"], "title": "Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks", "categories": ["cs.CL", "cs.CV"], "comment": "Code: https://github.com/zwq2018/embodied_reasoner Dataset:\n  https://huggingface.co/datasets/zwq2018/embodied_reasoner", "summary": "Recent advances in deep thinking models have demonstrated remarkable\nreasoning capabilities on mathematical and coding tasks. However, their\neffectiveness in embodied domains which require continuous interaction with\nenvironments through image action interleaved trajectories remains largely\n-unexplored. We present Embodied Reasoner, a model that extends o1 style\nreasoning to interactive embodied search tasks. Unlike mathematical reasoning\nthat relies primarily on logical deduction, embodied scenarios demand spatial\nunderstanding, temporal reasoning, and ongoing self-reflection based on\ninteraction history. To address these challenges, we synthesize 9.3k coherent\nObservation-Thought-Action trajectories containing 64k interactive images and\n90k diverse thinking processes (analysis, spatial reasoning, reflection,\nplanning, and verification). We develop a three-stage training pipeline that\nprogressively enhances the model's capabilities through imitation learning,\nself-exploration via rejection sampling, and self-correction through reflection\ntuning. The evaluation shows that our model significantly outperforms those\nadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and\nClaude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer\nrepeated searches and logical inconsistencies, with particular advantages in\ncomplex long-horizon tasks. Real-world environments also show our superiority\nwhile exhibiting fewer repeated searches and logical inconsistency cases.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20853", "pdf": "https://arxiv.org/pdf/2503.20853", "abs": "https://arxiv.org/abs/2503.20853", "authors": ["Alexander Swerdlow", "Mihir Prabhudesai", "Siddharth Gandhi", "Deepak Pathak", "Katerina Fragkiadaki"], "title": "Unified Multimodal Discrete Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Website: https://unidisc.github.io", "summary": "Multimodal generative models that can understand and generate across multiple\nmodalities are dominated by autoregressive (AR) approaches, which process\ntokens sequentially from left to right, or top to bottom. These models jointly\nhandle images, text, video, and audio for various tasks such as image\ncaptioning, question answering, and image generation. In this work, we explore\ndiscrete diffusion models as a unified generative formulation in the joint text\nand image domain, building upon their recent success in text generation.\nDiscrete diffusion models offer several advantages over AR models, including\nimproved control over quality versus diversity of generated samples, the\nability to perform joint multimodal inpainting (across both text and image\ndomains), and greater controllability in generation through guidance.\nLeveraging these benefits, we present the first Unified Multimodal Discrete\nDiffusion (UniDisc) model which is capable of jointly understanding and\ngenerating text and images for a variety of downstream tasks. We compare\nUniDisc to multimodal AR models, performing a scaling analysis and\ndemonstrating that UniDisc outperforms them in terms of both performance and\ninference-time compute, enhanced controllability, editability, inpainting, and\nflexible trade-off between inference time and generation quality. Code and\nadditional visualizations are available at https://unidisc.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20919", "pdf": "https://arxiv.org/pdf/2503.20919", "abs": "https://arxiv.org/abs/2503.20919", "authors": ["Yupei Li", "Qiyang Sun", "Sunil Munthumoduku Krishna Murthy", "Emran Alturki", "Björn W. Schuller"], "title": "GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Affective Computing (AC) is essential for advancing Artificial General\nIntelligence (AGI), with emotion recognition serving as a key component.\nHowever, human emotions are inherently dynamic, influenced not only by an\nindividual's expressions but also by interactions with others, and\nsingle-modality approaches often fail to capture their full dynamics.\nMultimodal Emotion Recognition (MER) leverages multiple signals but\ntraditionally relies on utterance-level analysis, overlooking the dynamic\nnature of emotions in conversations. Emotion Recognition in Conversation (ERC)\naddresses this limitation, yet existing methods struggle to align multimodal\nfeatures and explain why emotions evolve within dialogues. To bridge this gap,\nwe propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly\nconsiders voice and transcripts of both the speaker and their conversational\npartner(s) to identify the most influential sentences driving emotional shifts.\nBy integrating Contrastive Language-Audio Pretraining (CLAP) for improved\ncross-modal alignment and employing a gating mechanism to emphasise emotionally\nimpactful utterances, GatedxLSTM enhances both interpretability and\nperformance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion\npredictions by modelling contextual dependencies. Experiments on the IEMOCAP\ndataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA)\nperformance among open-source methods in four-class emotion classification.\nThese results validate its effectiveness for ERC applications and provide an\ninterpretability analysis from a psychological perspective.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20897", "pdf": "https://arxiv.org/pdf/2503.20897", "abs": "https://arxiv.org/abs/2503.20897", "authors": ["Venuri Amarasinghe", "Asini Jayakody", "Isun Randila", "Kalinga Bandara", "Chamuditha Jayanga Galappaththige", "Ranga Rodrigo"], "title": "Feature Modulation for Semi-Supervised Domain Generalization without Domain Labels", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised domain generalization (SSDG) leverages a small fraction of\nlabeled data alongside unlabeled data to enhance model generalization. Most of\nthe existing SSDG methods rely on pseudo-labeling (PL) for unlabeled data,\noften assuming access to domain labels-a privilege not always available.\nHowever, domain shifts introduce domain noise, leading to inconsistent PLs that\ndegrade model performance. Methods derived from FixMatch suffer particularly\nfrom lower PL accuracy, reducing the effectiveness of unlabeled data. To\naddress this, we tackle the more challenging domain-label agnostic SSDG, where\ndomain labels for unlabeled data are not available during training. First, we\npropose a feature modulation strategy that enhances class-discriminative\nfeatures while suppressing domain-specific information. This modulation shifts\nfeatures toward Similar Average Representations-a modified version of class\nprototypes-that are robust across domains, encouraging the classifier to\ndistinguish between closely related classes and feature extractor to form\ntightly clustered, domain-invariant representations. Second, to mitigate domain\nnoise and improve pseudo-label accuracy, we introduce a loss-scaling function\nthat dynamically lowers the fixed confidence threshold for pseudo-labels,\noptimizing the use of unlabeled data. With these key innovations, our approach\nachieves significant improvements on four major domain generalization\nbenchmarks-even without domain labels. We will make the code available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20936", "pdf": "https://arxiv.org/pdf/2503.20936", "abs": "https://arxiv.org/abs/2503.20936", "authors": ["Daniel Etaat", "Dvij Kalaria", "Nima Rahmanian", "Shankar Sastry"], "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Physical agility is a necessary skill in competitive table tennis, but by no\nmeans sufficient. Champions excel in this fast-paced and highly dynamic\nenvironment by anticipating their opponent's intent - buying themselves the\nnecessary time to react. In this work, we take one step towards designing such\nan anticipatory agent. Previous works have developed systems capable of\nreal-time table tennis gameplay, though they often do not leverage\nanticipation. Among the works that forecast opponent actions, their approaches\nare limited by dataset size and variety. Our paper contributes (1) a scalable\nsystem for reconstructing monocular video of table tennis matches in 3D and (2)\nan uncertainty-aware controller that anticipates opponent actions. We\ndemonstrate in simulation that our policy improves the ball return rate against\nhigh-speed hits from 49.9% to 59.0% as compared to a baseline non-anticipatory\npolicy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20967", "pdf": "https://arxiv.org/pdf/2503.20967", "abs": "https://arxiv.org/abs/2503.20967", "authors": ["David Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Akshay Chaudhari", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Lopes Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H. Miller", "Amir Borhani", "Hatice Savas", "Eric Hart", "Drew Torigian", "Jayaram K. Udupa", "Elizabeth Krupinski", "Ulas Bagci"], "title": "Eyes Tell the Truth: GazeVal Highlights Shortcomings of Generative AI in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "The demand for high-quality synthetic data for model training and\naugmentation has never been greater in medical imaging. However, current\nevaluations predominantly rely on computational metrics that fail to align with\nhuman expert recognition. This leads to synthetic images that may appear\nrealistic numerically but lack clinical authenticity, posing significant\nchallenges in ensuring the reliability and effectiveness of AI-driven medical\ntools. To address this gap, we introduce GazeVal, a practical framework that\nsynergizes expert eye-tracking data with direct radiological evaluations to\nassess the quality of synthetic medical images. GazeVal leverages gaze patterns\nof radiologists as they provide a deeper understanding of how experts perceive\nand interact with synthetic data in different tasks (i.e., diagnostic or Turing\ntests). Experiments with sixteen radiologists revealed that 96.6% of the\ngenerated images (by the most recent state-of-the-art AI algorithm) were\nidentified as fake, demonstrating the limitations of generative AI in producing\nclinically accurate images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20981", "pdf": "https://arxiv.org/pdf/2503.20981", "abs": "https://arxiv.org/abs/2503.20981", "authors": ["Xiaoran Xu", "Zhaoqian Xue", "Chi Zhang", "Jhonatan Medri", "Junjie Xiong", "Jiayan Zhou", "Jin Jin", "Yongfeng Zhang", "Siyuan Ma", "Lingyao Li"], "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Investigating the public experience of urgent care facilities is essential\nfor promoting community healthcare development. Traditional survey methods\noften fall short due to limited scope, time, and spatial coverage.\nCrowdsourcing through online reviews or social media offers a valuable approach\nto gaining such insights. With recent advancements in large language models\n(LLMs), extracting nuanced perceptions from reviews has become feasible. This\nstudy collects Google Maps reviews across the DMV and Florida areas and\nconducts prompt engineering with the GPT model to analyze the aspect-based\nsentiment of urgent care. We first analyze the geospatial patterns of various\naspects, including interpersonal factors, operational efficiency, technical\nquality, finances, and facilities. Next, we determine Census Block\nGroup(CBG)-level characteristics underpinning differences in public perception,\nincluding population density, median income, GINI Index, rent-to-income ratio,\nhousehold below poverty rate, no insurance rate, and unemployment rate. Our\nresults show that interpersonal factors and operational efficiency emerge as\nthe strongest determinants of patient satisfaction in urgent care, while\ntechnical quality, finances, and facilities show no significant independent\neffects when adjusted for in multivariate models. Among socioeconomic and\ndemographic factors, only population density demonstrates a significant but\nmodest association with patient ratings, while the remaining factors exhibit no\nsignificant correlations. Overall, this study highlights the potential of\ncrowdsourcing to uncover the key factors that matter to residents and provide\nvaluable insights for stakeholders to improve public satisfaction with urgent\ncare.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["aspect-based"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21003", "pdf": "https://arxiv.org/pdf/2503.21003", "abs": "https://arxiv.org/abs/2503.21003", "authors": ["Tai D. Nguyen", "Aref Azizpour", "Matthew C. Stamm"], "title": "Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images", "categories": ["cs.CV"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "The emergence of advanced AI-based tools to generate realistic images poses\nsignificant challenges for forensic detection and source attribution,\nespecially as new generative techniques appear rapidly. Traditional methods\noften fail to generalize to unseen generators due to reliance on features\nspecific to known sources during training. To address this problem, we propose\na novel approach that explicitly models forensic microstructures - subtle,\npixel-level patterns unique to the image creation process. Using only real\nimages in a self-supervised manner, we learn a set of diverse predictive\nfilters to extract residuals that capture different aspects of these\nmicrostructures. By jointly modeling these residuals across multiple scales, we\nobtain a compact model whose parameters constitute a unique forensic\nself-description for each image. This self-description enables us to perform\nzero-shot detection of synthetic images, open-set source attribution of images,\nand clustering based on source without prior knowledge. Extensive experiments\ndemonstrate that our method achieves superior accuracy and adaptability\ncompared to competing techniques, advancing the state of the art in synthetic\nmedia forensics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20988", "pdf": "https://arxiv.org/pdf/2503.20988", "abs": "https://arxiv.org/abs/2503.20988", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "categories": ["cs.CL", "cs.GR"], "comment": null, "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21056", "pdf": "https://arxiv.org/pdf/2503.21056", "abs": "https://arxiv.org/abs/2503.21056", "authors": ["Yiqing Shen", "Bohan Liu", "Chenjia Li", "Lalithkumar Seenivasan", "Mathias Unberath"], "title": "Online Reasoning Video Segmentation with Just-in-Time Digital Twins", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Reasoning segmentation (RS) aims to identify and segment objects of interest\nbased on implicit text queries. As such, RS is a catalyst for embodied AI\nagents, enabling them to interpret high-level commands without requiring\nexplicit step-by-step guidance. However, current RS approaches rely heavily on\nthe visual perception capabilities of multimodal large language models (LLMs),\nleading to several major limitations. First, they struggle with queries that\nrequire multiple steps of reasoning or those that involve complex\nspatial/temporal relationships. Second, they necessitate LLM fine-tuning, which\nmay require frequent updates to maintain compatibility with contemporary LLMs\nand may increase risks of catastrophic forgetting during fine-tuning. Finally,\nbeing primarily designed for static images or offline video processing, they\nscale poorly to online video data. To address these limitations, we propose an\nagent framework that disentangles perception and reasoning for online video RS\nwithout LLM fine-tuning. Our innovation is the introduction of a just-in-time\ndigital twin concept, where -- given an implicit query -- a LLM plans the\nconstruction of a low-level scene representation from high-level video using\nspecialist vision models. We refer to this approach to creating a digital twin\nas \"just-in-time\" because the LLM planner will anticipate the need for specific\ninformation and only request this limited subset instead of always evaluating\nevery specialist model. The LLM then performs reasoning on this digital twin\nrepresentation to identify target objects. To evaluate our approach, we\nintroduce a new comprehensive video reasoning segmentation benchmark comprising\n200 videos with 895 implicit text queries. The benchmark spans three reasoning\ncategories (semantic, spatial, and temporal) with three different reasoning\nchain complexity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21069", "pdf": "https://arxiv.org/pdf/2503.21069", "abs": "https://arxiv.org/abs/2503.21069", "authors": ["Fan Qi", "Yu Duan", "Changsheng Xu"], "title": "Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-guided diffusion models have revolutionized\nconditional image generation, yet they struggle to synthesize complex scenes\nwith multiple objects due to imprecise spatial grounding and limited\nscalability. We address these challenges through two key modules: 1)\nJanus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridges\ntext understanding and layout generation via a compact 1B-parameter\narchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integrating\nLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRA\nis capable of preserving the base model's parameters and ensuring plug-and-play\nadaptability, minimizing architectural intrusion while enabling efficient\nfine-tuning. To support a comprehensive evaluation, we create DescripBox and\nDescripBox-1024, benchmarks that span diverse scenes and resolutions. The\nproposed method achieves state-of-the-art performance on COCO and LVIS\nbenchmarks while maintaining parameter efficiency, demonstrating superior\nlayout fidelity and scalability for open-world synthesis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073", "abs": "https://arxiv.org/abs/2503.21073", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Viégas", "Martin Wattenberg"], "title": "Shared Global and Local Geometry of Language Model Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn this work, we find that the token embeddings of language models exhibit\ncommon geometric structure. First, we find ``global'' similarities: token\nembeddings often share similar relative orientations. Next, we characterize\nlocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by\ndefining a simple measure for the intrinsic dimension of each token embedding.\nOur intrinsic dimension measure demonstrates that token embeddings lie on a\nlower dimensional manifold. We qualitatively show that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Both characterizations allow us to\nfind similarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we empirically demonstrate that steering vectors from\none language model can be transferred to another, despite the two models having\ndifferent dimensions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21072", "pdf": "https://arxiv.org/pdf/2503.21072", "abs": "https://arxiv.org/abs/2503.21072", "authors": ["Judy X Yang", "Jing Wang", "Zhuanfeng", "Li", "Chenhong Sui Zekun Long", "Jun Zhou"], "title": "HSLiNets: Evaluating Band Ordering Strategies in Hyperspectral and LiDAR Fusion", "categories": ["cs.CV"], "comment": "2 figures, 5 pages", "summary": "The integration of hyperspectral imaging (HSI) and Light Detection and\nRanging (LiDAR) data provides complementary spectral and spatial information\nfor remote sensing applications. While previous studies have explored the role\nof band selection and grouping in HSI classification, little attention has been\ngiven to how the spectral sequence or band order affects classification\noutcomes when fused with LiDAR. In this work, we systematically investigate the\ninfluence of band order on HSI-LiDAR fusion performance. Through extensive\nexperiments, we demonstrate that band order significantly impacts\nclassification accuracy, revealing a previously overlooked factor in\nfusion-based models. Motivated by this observation, we propose a novel fusion\narchitecture that not only integrates HSI and LiDAR data but also learns from\nmultiple band order configurations. The proposed method enhances feature\nrepresentation by adaptively fusing different spectral sequences, leading to\nimproved classification accuracy. Experimental results on the Houston 2013 and\nTrento datasets show that our approach outperforms state-of-the-art fusion\nmodels. Data and code are available at https://github.com/Judyxyang/HSLiNets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21080", "pdf": "https://arxiv.org/pdf/2503.21080", "abs": "https://arxiv.org/abs/2503.21080", "authors": ["Yuhan Liu", "Yunbo Long"], "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues", "categories": ["cs.CL"], "comment": null, "summary": "While large language model (LLM)-based chatbots have been applied for\neffective engagement in credit dialogues, their capacity for dynamic emotional\nexpression remains limited. Current agents primarily rely on passive empathy\nrather than affective reasoning. For instance, when faced with persistent\nclient negativity, the agent should employ strategic emotional adaptation by\nexpressing measured anger to discourage counterproductive behavior and guide\nthe conversation toward resolution. This context-aware emotional modulation is\nessential for imitating the nuanced decision-making of human negotiators. This\npaper introduces an EQ-negotiator that combines emotion sensing from\npre-trained language models (PLMs) with emotional reasoning based on Game\nTheory and Hidden Markov Models. It takes into account both the current and\nhistorical emotions of the client to better manage and address negative\nemotions during interactions. By fine-tuning pre-trained language models (PLMs)\non public emotion datasets and validating them on the credit dialogue datasets,\nour approach enables LLM-based agents to effectively capture shifts in client\nemotions and dynamically adjust their response tone based on our emotion\ndecision policies in real-world financial negotiations. This EQ-negotiator can\nalso help credit agencies foster positive client relationships, enhancing\nsatisfaction in credit services.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21074", "pdf": "https://arxiv.org/pdf/2503.21074", "abs": "https://arxiv.org/abs/2503.21074", "authors": ["Ooha Lakkadi Reddy"], "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21\n  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this\n  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at\n  Duke Kunshan University; accepted for presentation at the 2025 Computer\n  Applications and Quantitative Methods in Archaeology Conference, Athens", "summary": "This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21127", "pdf": "https://arxiv.org/pdf/2503.21127", "abs": "https://arxiv.org/abs/2503.21127", "authors": ["Ziyi Zhou", "Xiaoming Zhang", "Shenghan Tan", "Litian Zhang", "Chaozhuo Li"], "title": "Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The proliferation of fake news on social media platforms has exerted a\nsubstantial influence on society, leading to discernible impacts and\ndeleterious consequences. Conventional deep learning methodologies employing\nsmall language models (SLMs) suffer from the necessity for extensive supervised\ntraining and the challenge of adapting to rapidly evolving circumstances. Large\nlanguage models (LLMs), despite their robust zero-shot capabilities, have\nfallen short in effectively identifying fake news due to a lack of pertinent\ndemonstrations and the dynamic nature of knowledge. In this paper, a novel\nframework Multi-Round Collaboration Detection (MRCD) is proposed to address\nthese aforementioned limitations. The MRCD framework is capable of enjoying the\nmerits from both LLMs and SLMs by integrating their generalization abilities\nand specialized functionalities, respectively. Our approach features a\ntwo-stage retrieval module that selects relevant and up-to-date demonstrations\nand knowledge, enhancing in-context learning for better detection of emerging\nnews events. We further design a multi-round learning framework to ensure more\nreliable detection results. Our framework MRCD achieves SOTA results on two\nreal-world datasets Pheme and Twitter16, with accuracy improvements of 7.4\\%\nand 12.8\\% compared to using only SLMs, which effectively addresses the\nlimitations of current models and improves the detection of emergent fake news.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21122", "pdf": "https://arxiv.org/pdf/2503.21122", "abs": "https://arxiv.org/abs/2503.21122", "authors": ["Teng Huang", "Han Ding", "Wenxin Sun", "Cui Zhao", "Ge Wang", "Fei Wang", "Kun Zhao", "Zhi Wang", "Wei Xi"], "title": "One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation", "categories": ["cs.CV"], "comment": "IEEE INFOCOM 2025", "summary": "Wireless sensing systems, particularly those using mmWave technology, offer\ndistinct advantages over traditional vision-based approaches, such as enhanced\nprivacy and effectiveness in poor lighting conditions. These systems,\nleveraging FMCW signals, have shown success in human-centric applications like\nlocalization, gesture recognition, and so on. However, comprehensive mmWave\ndatasets for diverse applications are scarce, often constrained by\npre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistent\nannotation formats. To overcome these limitations, we propose mmGen, a novel\nand generalized framework tailored for full-scene mmWave signal generation. By\nconstructing physical signal transmission models, mmGen synthesizes\nhuman-reflected and environment-reflected mmWave signals from the constructed\n3D meshes. Additionally, we incorporate methods to account for material\nproperties, antenna gains, and multipath reflections, enhancing the realism of\nthe synthesized signals. We conduct extensive experiments using a prototype\nsystem with commercial mmWave devices and Kinect sensors. The results show that\nthe average similarity of Range-Angle and micro-Doppler signatures between the\nsynthesized and real-captured signals across three different environments\nexceeds 0.91 and 0.89, respectively, demonstrating the effectiveness and\npractical applicability of mmGen.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21227", "pdf": "https://arxiv.org/pdf/2503.21227", "abs": "https://arxiv.org/abs/2503.21227", "authors": ["Hengyuan Zhao", "Ziqin Wang", "Qixin Sun", "Kaiyou Song", "Yilin Li", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Although applying Mixture of Experts to large language models for learning\nnew tasks is widely regarded as an effective strategy for continuous learning,\nthere still remain two major challenges: (1) As the number of tasks grows,\nsimple parameter expansion strategies can lead to excessively large models. (2)\nModifying the parameters of the existing router results in the erosion of\npreviously acquired knowledge. In this paper, we present an innovative\nframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)\narchitecture without any replay data. Specifically, we have developed a method\ncalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts to\nassess whether additional knowledge is required for a specific layer. This\napproach enables the model to adaptively expand its network parameters based on\ntask distribution, thereby significantly improving the efficiency of parameter\nexpansion. Additionally, we introduce a hierarchical routing algorithm called\nProbabilistic Task Locator (PTL), where high-level routing captures inter-task\ninformation and low-level routing focuses on intra-task details, ensuring that\nnew task experts do not interfere with existing ones. Our experiments shows\nthat our efficient architecture has substantially improved model performance on\nthe Coin benchmark while maintaining a reasonable parameter count.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21124", "pdf": "https://arxiv.org/pdf/2503.21124", "abs": "https://arxiv.org/abs/2503.21124", "authors": ["Shuaiyu Zhang", "Xun Lin", "Rongxiang Zhang", "Yu Bai", "Yong Xu", "Tao Tan", "Xunbin Zheng", "Zitong Yu"], "title": "AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "The integration of pathologic images and genomic data for survival analysis\nhas gained increasing attention with advances in multimodal learning. However,\ncurrent methods often ignore biological characteristics, such as heterogeneity\nand sparsity, both within and across modalities, ultimately limiting their\nadaptability to clinical practice. To address these challenges, we propose\nAdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for\nefficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is\nspecifically adapted to the uniqueness of medical data, enabling accurate\npredictions with minimal resource consumption, even under challenging scenarios\nwith missing modalities. Initially, AdaMHF employs an experts expansion and\nresidual structure to activate specialized experts for extracting heterogeneous\nand sparse features. Extracted tokens undergo refinement via selection and\naggregation, reducing the weight of non-dominant features while preserving\ncomprehensive information. Subsequently, the encoded features are\nhierarchically fused, allowing multi-grained interactions across modalities to\nbe captured. Furthermore, we introduce a survival prediction benchmark designed\nto resolve scenarios with missing modalities, mirroring real-world clinical\nconditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF\nsurpasses current state-of-the-art (SOTA) methods, showcasing exceptional\nperformance in both complete and incomplete modality settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21263", "pdf": "https://arxiv.org/pdf/2503.21263", "abs": "https://arxiv.org/abs/2503.21263", "authors": ["Wenxuan Lu", "Jiangyang He", "Zhanqiu Zhang", "Yiwen Guo", "Tianning Zang"], "title": "Cultivating Game Sense for Yourself: Making VLMs Gaming Experts", "categories": ["cs.CL"], "comment": null, "summary": "Developing agents capable of fluid gameplay in first/third-person games\nwithout API access remains a critical challenge in Artificial General\nIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) as\ndirect controllers, frequently pausing the game to analyze screens and plan\naction through language reasoning. However, this inefficient paradigm\nfundamentally restricts agents to basic and non-fluent interactions: relying on\nisolated VLM reasoning for each action makes it impossible to handle tasks\nrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,\nACT combat). To handle this, we propose a paradigm shift in gameplay agent\ndesign: instead of directly controlling gameplay, VLM develops specialized\nexecution modules tailored for tasks like shooting and combat. These modules\nhandle real-time game interactions, elevating VLM to a high-level developer.\nBuilding upon this paradigm, we introduce GameSense, a gameplay agent framework\nwhere VLM develops task-specific game sense modules by observing task execution\nand leveraging vision tools and neural network training pipelines. These\nmodules encapsulate action-feedback logic, ranging from direct action rules to\nneural network-based decisions. Experiments demonstrate that our framework is\nthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, and\nFlappy Bird, setting a new benchmark for game-playing agents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21144", "pdf": "https://arxiv.org/pdf/2503.21144", "abs": "https://arxiv.org/abs/2503.21144", "authors": ["Jinwei Qi", "Chaonan Ji", "Sheng Xu", "Peng Zhang", "Bang Zhang", "Liefeng Bo"], "title": "ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model", "categories": ["cs.CV"], "comment": "Project Page: https://humanaigc.github.io/chat-anyone/", "summary": "Real-time interactive video-chat portraits have been increasingly recognized\nas the future trend, particularly due to the remarkable progress made in text\nand voice chat technologies. However, existing methods primarily focus on\nreal-time generation of head movements, but struggle to produce synchronized\nbody motions that match these head actions. Additionally, achieving\nfine-grained control over the speaking style and nuances of facial expressions\nremains a challenge. To address these limitations, we introduce a novel\nframework for stylized real-time portrait video generation, enabling expressive\nand flexible video chat that extends from talking head to upper-body\ninteraction. Our approach consists of the following two stages. The first stage\ninvolves efficient hierarchical motion diffusion models, that take both\nexplicit and implicit motion representations into account based on audio\ninputs, which can generate a diverse range of facial expressions with stylistic\ncontrol and synchronization between head and body movements. The second stage\naims to generate portrait video featuring upper-body movements, including hand\ngestures. We inject explicit hand control signals into the generator to produce\nmore detailed hand movements, and further perform face refinement to enhance\nthe overall realism and expressiveness of the portrait video. Additionally, our\napproach supports efficient and continuous generation of upper-body portrait\nvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supporting\ninteractive video-chat in real-time. Experimental results demonstrate the\ncapability of our approach to produce portrait videos with rich expressiveness\nand natural upper-body movements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21349", "pdf": "https://arxiv.org/pdf/2503.21349", "abs": "https://arxiv.org/abs/2503.21349", "authors": ["Noah Losch", "Lucas Plagwitz", "Antonius Büscher", "Julian Varghese"], "title": "Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7; J.3"], "comment": "4 pages, 2 tables,", "summary": "We investigate the effectiveness of fine-tuning large language models (LLMs)\non small medical datasets for text classification and named entity recognition\ntasks. Using a German cardiology report dataset and the i2b2 Smoking Challenge\ndataset, we demonstrate that fine-tuning small LLMs locally on limited training\ndata can improve performance achieving comparable results to larger models. Our\nexperiments show that fine-tuning improves performance on both tasks, with\nnotable gains observed with as few as 200-300 training examples. Overall, the\nstudy highlights the potential of task-specific fine-tuning of LLMs for\nautomating clinical workflows and efficiently extracting structured data from\nunstructured medical text.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21360", "pdf": "https://arxiv.org/pdf/2503.21360", "abs": "https://arxiv.org/abs/2503.21360", "authors": ["Manuela Sanguinetti", "Alessandra Perniciano", "Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Maurizio Atzori"], "title": "From User Preferences to Optimization Constraints Using Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This work explores using Large Language Models (LLMs) to translate user\npreferences into energy optimization constraints for home appliances. We\ndescribe a task where natural language user utterances are converted into\nformal constraints for smart appliances, within the broader context of a\nrenewable energy community (REC) and in the Italian scenario. We evaluate the\neffectiveness of various LLMs currently available for Italian in translating\nthese preferences resorting to classical zero-shot, one-shot, and few-shot\nlearning settings, using a pilot dataset of Italian user requests paired with\ncorresponding formal constraint representation. Our contributions include\nestablishing a baseline performance for this task, publicly releasing the\ndataset and code for further research, and providing insights on observed best\npractices and limitations of LLMs in this particular domain", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21164", "pdf": "https://arxiv.org/pdf/2503.21164", "abs": "https://arxiv.org/abs/2503.21164", "authors": ["Samra Irshad", "Seungkyu Lee", "Nassir Navab", "Hong Joo Lee", "Seong Tae Kim"], "title": "Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 9 figures", "summary": "The presence of adversarial examples in the physical world poses significant\nchallenges to the deployment of Deep Neural Networks in safety-critical\napplications such as autonomous driving. Most existing methods for crafting\nphysical-world adversarial examples are ad-hoc, relying on temporary\nmodifications like shadows, laser beams, or stickers that are tailored to\nspecific scenarios. In this paper, we introduce a new class of physical-world\nadversarial examples, AdvWT, which draws inspiration from the naturally\noccurring phenomenon of `wear and tear', an inherent property of physical\nobjects. Unlike manually crafted perturbations, `wear and tear' emerges\norganically over time due to environmental degradation, as seen in the gradual\ndeterioration of outdoor signboards. To achieve this, AdvWT follows a two-step\napproach. First, a GAN-based, unsupervised image-to-image translation network\nis employed to model these naturally occurring damages, particularly in the\ncontext of outdoor signboards. The translation network encodes the\ncharacteristics of damaged signs into a latent `damage style code'. In the\nsecond step, we introduce adversarial perturbations into the style code,\nstrategically optimizing its transformation process. This manipulation subtly\nalters the damage style representation, guiding the network to generate\nadversarial images where the appearance of damages remains perceptually\nrealistic, while simultaneously ensuring their effectiveness in misleading\nneural networks. Through comprehensive experiments on two traffic sign\ndatasets, we show that AdvWT effectively misleads DNNs in both digital and\nphysical domains. AdvWT achieves an effective attack success rate, greater\nrobustness, and a more natural appearance compared to existing physical-world\nadversarial examples. Additionally, integrating AdvWT into training enhances a\nmodel's generalizability to real-world damaged signs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21172", "pdf": "https://arxiv.org/pdf/2503.21172", "abs": "https://arxiv.org/abs/2503.21172", "authors": ["Jingye Chen", "Yuzhong Zhao", "Yupan Huang", "Lei Cui", "Li Dong", "Tengchao Lv", "Qifeng Chen", "Furu Wei"], "title": "Model as a Game: On Numerical and Spatial Consistency for Generative Games", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Recent advances in generative models have significantly impacted game\ngeneration. However, despite producing high-quality graphics and adequately\nreceiving player input, existing models often fail to maintain fundamental game\nproperties such as numerical and spatial consistency. Numerical consistency\nensures gameplay mechanics correctly reflect score changes and other\nquantitative elements, while spatial consistency prevents jarring scene\ntransitions, providing seamless player experiences. In this paper, we revisit\nthe paradigm of generative games to explore what truly constitutes a Model as a\nGame (MaaG) with a well-developed mechanism. We begin with an empirical study\non ``Traveler'', a 2D game created by an LLM featuring minimalist rules yet\nchallenging generative models in maintaining consistency. Based on the DiT\narchitecture, we design two specialized modules: (1) a numerical module that\nintegrates a LogicNet to determine event triggers, with calculations processed\nexternally as conditions for image generation; and (2) a spatial module that\nmaintains a map of explored areas, retrieving location-specific information\nduring generation and linking new observations to ensure continuity.\nExperiments across three games demonstrate that our integrated modules\nsignificantly enhance performance on consistency metrics compared to baselines,\nwhile incurring minimal time overhead during inference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21383", "pdf": "https://arxiv.org/pdf/2503.21383", "abs": "https://arxiv.org/abs/2503.21383", "authors": ["Chengxing Jia", "Ziniu Li", "Pengyuan Wang", "Yi-Chen Li", "Zhenyu Hou", "Yuxiao Dong", "Yang Yu"], "title": "Controlling Large Language Model with Latent Actions", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adapting Large Language Models (LLMs) to downstream tasks using Reinforcement\nLearning (RL) has proven to be an effective approach. However, LLMs do not\ninherently define the structure of an agent for RL training, particularly in\nterms of defining the action space. This paper studies learning a compact\nlatent action space to enhance the controllability and exploration of RL for\nLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),\na framework that integrates a latent action space into pre-trained LLMs. We\napply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,\ncompared to RL with token-level actions, CoLA's latent action enables greater\nsemantic diversity in text generation. For enhancing downstream tasks, we show\nthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassing\nthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte Carlo\nTree Search variant. Furthermore, CoLA with RL consistently improves\nperformance on agent-based tasks without degrading the pre-trained LLM's\ncapabilities, unlike the baseline. Finally, CoLA reduces computation time by\nhalf in tasks involving enhanced thinking prompts for LLMs by RL. These results\nhighlight CoLA's potential to advance RL-based adaptation of LLMs for\ndownstream applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21460", "pdf": "https://arxiv.org/pdf/2503.21460", "abs": "https://arxiv.org/abs/2503.21460", "authors": ["Junyu Luo", "Weizhi Zhang", "Ye Yuan", "Yusheng Zhao", "Junwei Yang", "Yiyang Gu", "Bohan Wu", "Binqi Chen", "Ziyue Qiao", "Qingqing Long", "Rongcheng Tu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yifan Wang", "Meng Xiao", "Chenwu Liu", "Jingyang Yuan", "Shichang Zhang", "Yiqiao Jin", "Fan Zhang", "Xian Wu", "Hanqing Zhao", "Dacheng Tao", "Philip S. Yu", "Ming Zhang"], "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges", "categories": ["cs.CL"], "comment": "329 papers surveyed, resources are at\n  https://github.com/luo-junyu/Awesome-Agent-Papers", "summary": "The era of intelligent agents is upon us, driven by revolutionary\nadvancements in large language models. Large Language Model (LLM) agents, with\ngoal-driven behaviors and dynamic adaptation capabilities, potentially\nrepresent a critical pathway toward artificial general intelligence. This\nsurvey systematically deconstructs LLM agent systems through a\nmethodology-centered taxonomy, linking architectural foundations, collaboration\nmechanisms, and evolutionary pathways. We unify fragmented research threads by\nrevealing fundamental connections between agent design principles and their\nemergent behaviors in complex environments. Our work provides a unified\narchitectural perspective, examining how agents are constructed, how they\ncollaborate, and how they evolve over time, while also addressing evaluation\nmethodologies, tool applications, practical challenges, and diverse application\ndomains. By surveying the latest developments in this rapidly evolving field,\nwe offer researchers a structured taxonomy for understanding LLM agents and\nidentify promising directions for future research. The collection is available\nat https://github.com/luo-junyu/Awesome-Agent-Papers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21480", "pdf": "https://arxiv.org/pdf/2503.21480", "abs": "https://arxiv.org/abs/2503.21480", "authors": ["John Murzaku", "Owen Rambow"], "title": "OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs", "categories": ["cs.CL"], "comment": "Submitted to COLM 2025. Preprint", "summary": "The use of omni-LLMs (large language models that accept any modality as\ninput), particularly for multimodal cognitive state tasks involving speech, is\nunderstudied. We present OmniVox, the first systematic evaluation of four\nomni-LLMs on the zero-shot emotion recognition task. We evaluate on two widely\nused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shot\nomni-LLMs outperform or are competitive with fine-tuned audio models. Alongside\nour audio-only evaluation, we also evaluate omni-LLMs on text only and text and\naudio. We present acoustic prompting, an audio-specific prompting strategy for\nomni-LLMs which focuses on acoustic feature analysis, conversation context\nanalysis, and step-by-step reasoning. We compare our acoustic prompting to\nminimal prompting and full chain-of-thought prompting techniques. We perform a\ncontext window analysis on IEMOCAP and MELD, and find that using context helps,\nespecially on IEMOCAP. We conclude with an error analysis on the generated\nacoustic reasoning outputs from the omni-LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21219", "pdf": "https://arxiv.org/pdf/2503.21219", "abs": "https://arxiv.org/abs/2503.21219", "authors": ["Sibo Wu", "Congrong Xu", "Binbin Huang", "Andreas Geiger", "Anpei Chen"], "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, 3D reconstruction and generation have demonstrated impressive novel\nview synthesis results, achieving high fidelity and efficiency. However, a\nnotable conditioning gap can be observed between these two fields, e.g.,\nscalable 3D scene reconstruction often requires densely captured views, whereas\n3D generation typically relies on a single or no input view, which\nsignificantly limits their applications. We found that the source of this\nphenomenon lies in the misalignment between 3D constraints and generative\npriors. To address this problem, we propose a reconstruction-driven video\ndiffusion model that learns to condition video frames on artifact-prone RGB-D\nrenderings. Moreover, we propose a cyclical fusion pipeline that iteratively\nadds restoration frames from the generative model to the training set, enabling\nprogressive expansion and addressing the viewpoint saturation limitations seen\nin previous reconstruction and generation pipelines. Our evaluation, including\nview synthesis from sparse view and masked input, validates the effectiveness\nof our approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21254", "pdf": "https://arxiv.org/pdf/2503.21254", "abs": "https://arxiv.org/abs/2503.21254", "authors": ["Zhaokai Wang", "Chenxi Bao", "Le Zhuo", "Jingrui Han", "Yang Yue", "Yihong Tang", "Victor Shea-Jay Huang", "Yue Liao"], "title": "Vision-to-Music Generation: A Survey", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Vision-to-music Generation, including video-to-music and image-to-music\ntasks, is a significant branch of multimodal artificial intelligence\ndemonstrating vast application prospects in fields such as film scoring, short\nvideo creation, and dance music synthesis. However, compared to the rapid\ndevelopment of modalities like text and images, research in vision-to-music is\nstill in its preliminary stage due to its complex internal structure and the\ndifficulty of modeling dynamic relationships with video. Existing surveys focus\non general music generation without comprehensive discussion on\nvision-to-music. In this paper, we systematically review the research progress\nin the field of vision-to-music generation. We first analyze the technical\ncharacteristics and core challenges for three input types: general videos,\nhuman movement videos, and images, as well as two output types of symbolic\nmusic and audio music. We then summarize the existing methodologies on\nvision-to-music generation from the architecture perspective. A detailed review\nof common datasets and evaluation metrics is provided. Finally, we discuss\ncurrent challenges and promising directions for future research. We hope our\nsurvey can inspire further innovation in vision-to-music generation and the\nbroader field of multimodal generation in academic research and industrial\napplications. To follow latest works and foster further innovation in this\nfield, we are continuously maintaining a GitHub repository at\nhttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21258", "pdf": "https://arxiv.org/pdf/2503.21258", "abs": "https://arxiv.org/abs/2503.21258", "authors": ["Jizhou Han", "Chenhao Ding", "Yuhang He", "Songlin Dong", "Qiang Wang", "Xinyuan Gao", "Yihong Gong"], "title": "Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Few-shot class-incremental Learning (FSCIL) enables models to learn new\nclasses from limited data while retaining performance on previously learned\nclasses. Traditional FSCIL methods often require fine-tuning parameters with\nlimited new class data and suffer from a separation between learning new\nclasses and utilizing old knowledge. Inspired by the analogical learning\nmechanisms of the human brain, we propose a novel analogical generative method.\nOur approach includes the Brain-Inspired Analogical Generator (BiAG), which\nderives new class weights from existing classes without parameter fine-tuning\nduring incremental stages. BiAG consists of three components: Weight\nSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module\n(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory\nfor semantic conversion, WSA supplements new class weights, and WPAA computes\nanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,\nand CIFAR-100 datasets demonstrate that our method achieves higher final and\naverage accuracy compared to SOTA methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21670", "pdf": "https://arxiv.org/pdf/2503.21670", "abs": "https://arxiv.org/abs/2503.21670", "authors": ["Rajvee Sheth", "Himanshu Beniwal", "Mayank Singh"], "title": "COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of digital communication has driven the widespread use of\ncode-mixing, particularly Hindi-English, in multilingual communities. Existing\ndatasets often focus on romanized text, have limited scope, or rely on\nsynthetic data, which fails to capture realworld language nuances. Human\nannotations are crucial for assessing the naturalness and acceptability of\ncode-mixed text. To address these challenges, We introduce COMI-LINGUA, the\nlargest manually annotated dataset for code-mixed text, comprising 100,970\ninstances evaluated by three expert annotators in both Devanagari and Roman\nscripts. The dataset supports five fundamental NLP tasks: Language\nIdentification, Matrix Language Identification, Part-of-Speech Tagging, Named\nEntity Recognition, and Translation. We evaluate LLMs on these tasks using\nCOMILINGUA, revealing limitations in current multilingual modeling strategies\nand emphasizing the need for improved code-mixed text processing capabilities.\nCOMI-LINGUA is publically availabe at:\nhttps://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21262", "pdf": "https://arxiv.org/pdf/2503.21262", "abs": "https://arxiv.org/abs/2503.21262", "authors": ["Yunusa Haruna", "Adamu Lawan"], "title": "vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Capturing long-range dependencies efficiently is essential for visual\nrecognition tasks, yet existing methods face limitations. Convolutional neural\nnetworks (CNNs) struggle with restricted receptive fields, while Vision\nTransformers (ViTs) achieve global context and long-range modeling at a high\ncomputational cost. State-space models (SSMs) offer an alternative, but their\napplication in vision remains underexplored. This work introduces vGamba, a\nhybrid vision backbone that integrates SSMs with attention mechanisms to\nenhance efficiency and expressiveness. At its core, the Gamba bottleneck block\nthat includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,\nalongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated Fusion\nModule for effective feature representation. The interplay of these components\nensures that vGamba leverages the low computational demands of SSMs while\nmaintaining the accuracy of attention mechanisms for modeling long-range\ndependencies in vision tasks. Additionally, the Fusion module enables seamless\ninteraction between these components. Extensive experiments on classification,\ndetection, and segmentation tasks demonstrate that vGamba achieves a superior\ntrade-off between accuracy and computational efficiency, outperforming several\nexisting models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21268", "pdf": "https://arxiv.org/pdf/2503.21268", "abs": "https://arxiv.org/abs/2503.21268", "authors": ["Ming Yan", "Xincheng Lin", "Yuhua Luo", "Shuqi Fan", "Yudi Dai", "Qixin Zhong", "Lincai Zhong", "Yuexin Ma", "Lan Xu", "Chenglu Wen", "Siqi Shen", "Cheng Wang"], "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate", "categories": ["cs.CV"], "comment": "CVPR2025, project in \\href{this\n  link}{http://www.lidarhumanmotion.net/climbingcap/}", "summary": "Human Motion Recovery (HMR) research mainly focuses on ground-based motions\nsuch as running. The study on capturing climbing motion, an off-ground motion,\nis sparse. This is partly due to the limited availability of climbing motion\ndatasets, especially large-scale and challenging 3D labeled datasets. To\naddress the insufficiency of climbing motion datasets, we collect AscendMotion,\na large-scale well-annotated, and challenging climbing motion dataset. It\nconsists of 412k RGB, LiDAR frames, and IMU measurements, including the\nchallenging climbing motions of 22 skilled climbing coaches across 12 different\nrock walls. Capturing the climbing motions is challenging as it requires\nprecise recovery of not only the complex pose but also the global position of\nclimbers. Although multiple global HMR methods have been proposed, they cannot\nfaithfully capture climbing motions. To address the limitations of HMR methods\nfor climbing, we propose ClimbingCap, a motion recovery method that\nreconstructs continuous 3D human climbing motion in a global coordinate system.\nOne key insight is to use the RGB and LiDAR modalities to separately\nreconstruct motions in camera coordinates and global coordinates and to\noptimize them jointly. We demonstrate the quality of the AscendMotion dataset\nand present promising results from ClimbingCap. The AscendMotion dataset and\nsource code release publicly at \\href{this\nlink}{http://www.lidarhumanmotion.net/climbingcap/}", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21269", "pdf": "https://arxiv.org/pdf/2503.21269", "abs": "https://arxiv.org/abs/2503.21269", "authors": ["Zhaoyi Yan", "Kangjun Liu", "Qixiang Ye"], "title": "Delving Deep into Semantic Relation Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge distillation has become a cornerstone technique in deep learning,\nfacilitating the transfer of knowledge from complex models to lightweight\ncounterparts. Traditional distillation approaches focus on transferring\nknowledge at the instance level, but fail to capture nuanced semantic\nrelationships within the data. In response, this paper introduces a novel\nmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), which\nreimagines knowledge distillation through a semantics-relation lens among each\nsample. By leveraging semantic components, \\ie, superpixels, SeRKD enables a\nmore comprehensive and context-aware transfer of knowledge, which skillfully\nintegrates superpixel-based semantic extraction with relation-based knowledge\ndistillation for a sophisticated model compression and distillation.\nParticularly, the proposed method is naturally relevant in the domain of Vision\nTransformers (ViTs), where visual tokens serve as fundamental units of\nrepresentation. Experimental evaluations on benchmark datasets demonstrate the\nsuperiority of SeRKD over existing methods, underscoring its efficacy in\nenhancing model performance and generalization capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21277", "pdf": "https://arxiv.org/pdf/2503.21277", "abs": "https://arxiv.org/abs/2503.21277", "authors": ["Hiroya Makino", "Takahiro Yamaguchi", "Hiroyuki Sakai"], "title": "Zero-Shot Visual Concept Blending Without Text Guidance", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel, zero-shot image generation technique called \"Visual\nConcept Blending\" that provides fine-grained control over which features from\nmultiple reference images are transferred to a source image. If only a single\nreference image is available, it is difficult to isolate which specific\nelements should be transferred. However, using multiple reference images, the\nproposed approach distinguishes between common and unique features by\nselectively incorporating them into a generated output. By operating within a\npartially disentangled Contrastive Language-Image Pre-training (CLIP) embedding\nspace (from IP-Adapter), our method enables the flexible transfer of texture,\nshape, motion, style, and more abstract conceptual transformations without\nrequiring additional training or text prompts. We demonstrate its effectiveness\nacross a diverse range of tasks, including style transfer, form metamorphosis,\nand conceptual transformations, showing how subtle or abstract attributes\n(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlessly\ncombined into a new image. In a user study, participants accurately recognized\nwhich features were intended to be transferred. Its simplicity, flexibility,\nand high-level control make Visual Concept Blending valuable for creative\nfields such as art, design, and content creation, where combining specific\nvisual qualities from multiple inspirations is crucial.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21714", "pdf": "https://arxiv.org/pdf/2503.21714", "abs": "https://arxiv.org/abs/2503.21714", "authors": ["Pietro Tropeano", "Maria Maistro", "Tuukka Ruotsalo", "Christina Lioma"], "title": "As easy as PIE: understanding when pruning causes language models to disagree", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Findings)", "summary": "Language Model (LM) pruning compresses the model by removing weights, nodes,\nor other parts of its architecture. Typically, pruning focuses on the resulting\nefficiency gains at the cost of effectiveness. However, when looking at how\nindividual data points are affected by pruning, it turns out that a particular\nsubset of data points always bears most of the brunt (in terms of reduced\naccuracy) when pruning, but this effect goes unnoticed when reporting the mean\naccuracy of all data points. These data points are called PIEs and have been\nstudied in image processing, but not in NLP. In a study of various NLP\ndatasets, pruning methods, and levels of compression, we find that PIEs impact\ninference quality considerably, regardless of class frequency, and that BERT is\nmore prone to this than BiLSTM. We also find that PIEs contain a high amount of\ndata points that have the largest influence on how well the model generalises\nto unseen data. This means that when pruning, with seemingly moderate loss to\naccuracy across all data points, we in fact hurt tremendously those data points\nthat matter the most. We trace what makes PIEs both hard and impactful to\ninference to their overall longer and more semantically complex text. These\nfindings are novel and contribute to understanding how LMs are affected by\npruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21313", "pdf": "https://arxiv.org/pdf/2503.21313", "abs": "https://arxiv.org/abs/2503.21313", "authors": ["Zerui Chen", "Rolandos Alexandros Potamias", "Shizhe Chen", "Cordelia Schmid"], "title": "HORT: Monocular Hand-held Objects Reconstruction with Transformers", "categories": ["cs.CV"], "comment": "Project Page: https://zerchen.github.io/projects/hort.html", "summary": "Reconstructing hand-held objects in 3D from monocular images remains a\nsignificant challenge in computer vision. Most existing approaches rely on\nimplicit 3D representations, which produce overly smooth reconstructions and\nare time-consuming to generate explicit 3D shapes. While more recent methods\ndirectly reconstruct point clouds with diffusion models, the multi-step\ndenoising makes high-resolution reconstruction inefficient. To address these\nlimitations, we propose a transformer-based model to efficiently reconstruct\ndense 3D point clouds of hand-held objects. Our method follows a coarse-to-fine\nstrategy, first generating a sparse point cloud from the image and\nprogressively refining it into a dense representation using pixel-aligned image\nfeatures. To enhance reconstruction accuracy, we integrate image features with\n3D hand geometry to jointly predict the object point cloud and its pose\nrelative to the hand. Our model is trained end-to-end for optimal performance.\nExperimental results on both synthetic and real datasets demonstrate that our\nmethod achieves state-of-the-art accuracy with much faster inference speed,\nwhile generalizing well to in-the-wild images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20807", "pdf": "https://arxiv.org/pdf/2503.20807", "abs": "https://arxiv.org/abs/2503.20807", "authors": ["Pin-Yu Chen", "Han Shen", "Payel Das", "Tianyi Chen"], "title": "Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "The first two authors contribute equally to this work and are listed\n  in alphabetical order", "summary": "Fine-tuning Large Language Models (LLMs) on some task-specific datasets has\nbeen a primary use of LLMs. However, it has been empirically observed that this\napproach to enhancing capability inevitably compromises safety, a phenomenon\nalso known as the safety-capability trade-off in LLM fine-tuning. This paper\npresents a theoretical framework for understanding the interplay between safety\nand capability in two primary safety-aware LLM fine-tuning strategies,\nproviding new insights into the effects of data similarity, context overlap,\nand alignment loss landscape. Our theoretical results characterize the\nfundamental limits of the safety-capability trade-off in LLM fine-tuning, which\nare also validated by numerical experiments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21377", "pdf": "https://arxiv.org/pdf/2503.21377", "abs": "https://arxiv.org/abs/2503.21377", "authors": ["Hamadi Chihaoui", "Paolo Favaro"], "title": "Unsupervised Real-World Denoising: Sparsity is All You Need", "categories": ["cs.CV"], "comment": null, "summary": "Supervised training for real-world denoising presents challenges due to the\ndifficulty of collecting large datasets of paired noisy and clean images.\nRecent methods have attempted to address this by utilizing unpaired datasets of\nclean and noisy images. Some approaches leverage such unpaired data to train\ndenoisers in a supervised manner by generating synthetic clean-noisy pairs.\nHowever, these methods often fall short due to the distribution gap between\nsynthetic and real noisy images. To mitigate this issue, we propose a solution\nbased on input sparsification, specifically using random input masking. Our\nmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiser\nto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,\ninput sparsification reduces the gap between synthetic and real noisy images.\nOn the other hand, an inpainter trained in a supervised manner can still\naccurately reconstruct sparse inputs by predicting missing clean pixels using\nthe remaining unmasked pixels. Our approach begins with a synthetic Gaussian\nnoise sampler and iteratively refines it using a noise dataset derived from the\ndenoiser's predictions. The noise dataset is created by subtracting predicted\npseudo-clean images from real noisy images at each iteration. The core\nintuition is that improving the denoiser results in a more accurate noise\ndataset and, consequently, a better noise sampler. We validate our method\nthrough extensive experiments on real-world noisy image datasets, demonstrating\ncompetitive performance compared to existing unsupervised denoising methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20846", "pdf": "https://arxiv.org/pdf/2503.20846", "abs": "https://arxiv.org/abs/2503.20846", "authors": ["Viktor Schlegel", "Anil A Bharath", "Zilong Zhao", "Kevin Yee"], "title": "Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead", "categories": ["cs.CR", "cs.CL", "cs.CV"], "comment": "23 pages + references + Appendix. Preprint", "summary": "Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20992", "pdf": "https://arxiv.org/pdf/2503.20992", "abs": "https://arxiv.org/abs/2503.20992", "authors": ["Michael Brown", "Sofia Martinez", "Priya Singh"], "title": "ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer", "categories": ["cs.GR", "cs.CL"], "comment": null, "summary": "Text-driven speech style transfer aims to mold the intonation, pace, and\ntimbre of a spoken utterance to match stylistic cues from text descriptions.\nWhile existing methods leverage large-scale neural architectures or pre-trained\nlanguage models, the computational costs often remain high. In this paper, we\npresent \\emph{ReverBERT}, an efficient framework for text-driven speech style\ntransfer that draws inspiration from a state space model (SSM) paradigm,\nloosely motivated by the image-based method of Wang and\nLiu~\\cite{wang2024stylemamba}. Unlike image domain techniques, our method\noperates in the speech space and integrates a discrete Fourier transform of\nlatent speech features to enable smooth and continuous style modulation. We\nalso propose a novel \\emph{Transformer-based SSM} layer for bridging textual\nstyle descriptors with acoustic attributes, dramatically reducing inference\ntime while preserving high-quality speech characteristics. Extensive\nexperiments on benchmark speech corpora demonstrate that \\emph{ReverBERT}\nsignificantly outperforms baselines in terms of naturalness, expressiveness,\nand computational efficiency. We release our model and code publicly to foster\nfurther research in text-driven speech style transfer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21449", "pdf": "https://arxiv.org/pdf/2503.21449", "abs": "https://arxiv.org/abs/2503.21449", "authors": ["Lucas Nunes", "Rodrigo Marcuzzi", "Jens Behley", "Cyrill Stachniss"], "title": "Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Semantic scene understanding is crucial for robotics and computer vision\napplications. In autonomous driving, 3D semantic segmentation plays an\nimportant role for enabling safe navigation. Despite significant advances in\nthe field, the complexity of collecting and annotating 3D data is a bottleneck\nin this developments. To overcome that data annotation limitation, synthetic\nsimulated data has been used to generate annotated data on demand. There is\nstill however a domain gap between real and simulated data. More recently,\ndiffusion models have been in the spotlight, enabling close-to-real data\nsynthesis. Those generative models have been recently applied to the 3D data\ndomain for generating scene-scale data with semantic annotations. Still, those\nmethods either rely on image projection or decoupled models trained with\ndifferent resolutions in a coarse-to-fine manner. Such intermediary\nrepresentations impact the generated data quality due to errors added in those\ntransformations. In this work, we propose a novel approach able to generate 3D\nsemantic scene-scale data without relying on any projection or decoupled\ntrained multi-resolution models, achieving more realistic semantic scene data\ngeneration compared to previous state-of-the-art methods. Besides improving 3D\nsemantic scene-scale data synthesis, we thoroughly evaluate the use of the\nsynthetic scene samples as labeled data to train a semantic segmentation\nnetwork. In our experiments, we show that using the synthetic annotated data\ngenerated by our method as training data together with the real semantic\nsegmentation labels, leads to an improvement in the semantic segmentation model\nperformance. Our results show the potential of generated scene-scale point\nclouds to generate more training data to extend existing datasets, reducing the\ndata annotation effort. Our code is available at\nhttps://github.com/PRBonn/3DiSS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21074", "pdf": "https://arxiv.org/pdf/2503.21074", "abs": "https://arxiv.org/abs/2503.21074", "authors": ["Ooha Lakkadi Reddy"], "title": "Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "106 pages total (main text: 42, 48 w/refs, 100 w/appendices). 21\n  figures, 4 tables in main; 106 figs, 8 tables total. Code and data at this\n  URL: https://github.com/oohalakkadi/ivc2tyc. Submitted as undergrad thesis at\n  Duke Kunshan University; accepted for presentation at the 2025 Computer\n  Applications and Quantitative Methods in Archaeology Conference, Athens", "summary": "This thesis employs a hybrid CNN-Transformer architecture, in conjunction\nwith a detailed anthropological framework, to investigate potential historical\nconnections between the visual morphology of the Indus Valley script and\npictographic systems of the Tibetan-Yi Corridor. Through an ensemble\nmethodology of three target scripts across 15 independently trained models, we\ndemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold\nhigher visual similarity to the Indus script (61.7%-63.5%) than to the Bronze\nAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.\nAdditionally and contrarily to our current understanding of the networks of the\nIndus Valley Civilization, the Indus script unexpectedly maps closer to\nTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than to\nthe aforementioned contemporaneous West Asian signaries, both of which recorded\nmean cosine similarities of 0.104 and 0.080 despite their close geographic\nproximity and evident trade relations. Across various dimensionality reduction\npractices and clustering methodologies, the Indus script consistently clusters\nclosest to Tibetan-Yi Corridor scripts. Our computational results align with\nqualitative observations of specific pictorial parallels in numeral systems,\ngender markers, and key iconographic elements; this is further supported by\narchaeological evidence of sustained contact networks along the ancient\nShu-Shendu road in tandem with the Indus Valley Civilization's decline,\nproviding a plausible transmission pathway. While alternative explanations\ncannot be ruled out, the specificity and consistency of observed similarities\nchallenge conventional narratives of isolated script development and suggest\nmore complex ancient cultural transmission networks between South and East Asia\nthan previously recognized.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21114", "pdf": "https://arxiv.org/pdf/2503.21114", "abs": "https://arxiv.org/abs/2503.21114", "authors": ["Jamshid Sourati", "Grace Shao"], "title": "Measuring and Analyzing Subjective Uncertainty in Scientific Communications", "categories": ["cs.DL", "cs.CL"], "comment": "Coming with Appendix and supplementary material", "summary": "Uncertainty of scientific findings are typically reported through statistical\nmetrics such as $p$-values, confidence intervals, etc. The magnitude of this\nobjective uncertainty is reflected in the language used by the authors to\nreport their findings primarily through expressions carrying\nuncertainty-inducing terms or phrases. This language uncertainty is a\nsubjective concept and is highly dependent on the writing style of the authors.\nThere is evidence that such subjective uncertainty influences the impact of\nscience on public audience. In this work, we turned our focus to scientists\nthemselves, and measured/analyzed the subjective uncertainty and its impact\nwithin scientific communities across different disciplines. We showed that the\nlevel of this type of uncertainty varies significantly across different fields,\nyears of publication and geographical locations. We also studied the\ncorrelation between subjective uncertainty and several bibliographical metrics,\nsuch as number/gender of authors, centrality of the field's community, citation\ncount, etc. The underlying patterns identified in this work are useful in\nidentification and documentation of linguistic norms in scientific\ncommunication in different communities/societies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21465", "pdf": "https://arxiv.org/pdf/2503.21465", "abs": "https://arxiv.org/abs/2503.21465", "authors": ["Deependra Singh", "Saksham Agarwal", "Subhankar Mishra"], "title": "Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T10, 68T45, 92C55", "I.2.10; I.5.4; J.3"], "comment": "17 pages, 3 figures, 7 tables. Conference paper presented at the\n  International Health Informatics Conference (IHIC 2023)", "summary": "Our research is motivated by the urgent global issue of a large population\naffected by retinal diseases, which are evenly distributed but underserved by\nspecialized medical expertise, particularly in non-urban areas. Our primary\nobjective is to bridge this healthcare gap by developing a comprehensive\ndiagnostic system capable of accurately predicting retinal diseases solely from\nfundus images. However, we faced significant challenges due to limited, diverse\ndatasets and imbalanced class distributions. To overcome these issues, we have\ndevised innovative strategies. Our research introduces novel approaches,\nutilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),\nTransformer encoders, and ensemble architectures sequentially and in parallel\nto classify retinal fundus images into 20 disease labels. Our overarching goal\nis to assess these advanced models' potential in practical applications, with a\nstrong focus on enhancing retinal disease diagnosis accuracy across a broader\nspectrum of conditions. Importantly, our efforts have surpassed baseline model\nresults, with the C-Tran ensemble model emerging as the leader, achieving a\nremarkable model score of 0.9166, surpassing the baseline score of 0.9.\nAdditionally, experiments with the IEViT model showcased equally promising\noutcomes with improved computational efficiency. We've also demonstrated the\neffectiveness of dynamic patch extraction and the integration of domain\nknowledge in computer vision tasks. In summary, our research strives to\ncontribute significantly to retinal disease diagnosis, addressing the critical\nneed for accessible healthcare solutions in underserved regions while aiming\nfor comprehensive and accurate disease prediction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21683", "pdf": "https://arxiv.org/pdf/2503.21683", "abs": "https://arxiv.org/abs/2503.21683", "authors": ["Hui Wang"], "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21541", "pdf": "https://arxiv.org/pdf/2503.21541", "abs": "https://arxiv.org/abs/2503.21541", "authors": ["Achint Soni", "Meet Soni", "Sirisha Rambhatla"], "title": "LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing aims to modify specific regions of an image\naccording to natural language instructions while maintaining the general\nstructure and the background fidelity. Existing methods utilize masks derived\nfrom cross-attention maps generated from diffusion models to identify the\ntarget regions for modification. However, since cross-attention mechanisms\nfocus on semantic relevance, they struggle to maintain the image integrity. As\na result, these methods often lack spatial consistency, leading to editing\nartifacts and distortions. In this work, we address these limitations and\nintroduce LOCATEdit, which enhances cross-attention maps through a graph-based\napproach utilizing self-attention-derived patch relationships to maintain\nsmooth, coherent attention across image regions, ensuring that alterations are\nlimited to the designated items while retaining the surrounding structure.\n\\method consistently and substantially outperforms existing baselines on\nPIE-Bench, demonstrating its state-of-the-art performance and effectiveness on\nvarious editing tasks. Code can be found on\nhttps://github.com/LOCATEdit/LOCATEdit/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21566", "pdf": "https://arxiv.org/pdf/2503.21566", "abs": "https://arxiv.org/abs/2503.21566", "authors": ["Tongchao Luo", "Mingquan Qiu", "Zhenyu Wu", "Zebo Zhao", "Dingyou Zhang"], "title": "Bearing fault diagnosis based on multi-scale spectral images and convolutional neural network", "categories": ["cs.CV"], "comment": "12pages, 10 figures and 8 tables", "summary": "To address the challenges of low diagnostic accuracy in traditional bearing\nfault diagnosis methods, this paper proposes a novel fault diagnosis approach\nbased on multi-scale spectrum feature images and deep learning. Firstly, the\nvibration signal are preprocessed through mean removal and then converted to\nmulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novel\nfeature called multi-scale spectral image (MSSI) is constructed by multi-length\nspectrum paving scheme. Finally, a deep learning framework, convolutional\nneural network (CNN), is formulated to diagnose the bearing faults. Two\nexperimental cases are utilized to verify the effectiveness of the proposed\nmethod. Experimental results demonstrate that the proposed method significantly\nimproves the accuracy of fault diagnosis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21581", "pdf": "https://arxiv.org/pdf/2503.21581", "abs": "https://arxiv.org/abs/2503.21581", "authors": ["Liuyue Xie", "Jiancong Guo", "Ozan Cakmakci", "Andre Araujo", "Laszlo A. Jeni", "Zhiheng Jia"], "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21595", "pdf": "https://arxiv.org/pdf/2503.21595", "abs": "https://arxiv.org/abs/2503.21595", "authors": ["Jincheng Yan", "Yun Wang", "Xiaoyan Luo", "Yu-Wing Tai"], "title": "FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) plays a critical role in applications like\nsecurity surveillance and criminal investigations by matching individuals\nacross large image galleries captured by non-overlapping cameras. Traditional\nReID methods rely on unimodal inputs, typically images, but face limitations\ndue to challenges like occlusions, lighting changes, and pose variations. While\nadvancements in image-based and text-based ReID systems have been made, the\nintegration of both modalities has remained under-explored. This paper presents\nFusionSegReID, a multimodal model that combines both image and text inputs for\nenhanced ReID performance. By leveraging the complementary strengths of these\nmodalities, our model improves matching accuracy and robustness, particularly\nin complex, real-world scenarios where one modality may struggle. Our\nexperiments show significant improvements in Top-1 accuracy and mean Average\nPrecision (mAP) for ReID, as well as better segmentation results in challenging\nscenarios like occlusion and low-quality images. Ablation studies further\nconfirm that multimodal fusion and segmentation modules contribute to enhanced\nre-identification and mask accuracy. The results show that FusionSegReID\noutperforms traditional unimodal models, offering a more robust and flexible\nsolution for real-world person ReID tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21690", "pdf": "https://arxiv.org/pdf/2503.21690", "abs": "https://arxiv.org/abs/2503.21690", "authors": ["Nikin~Matharaarachchi", "Muhammad~Fermi Pasha", "Sonya~Coleman", "Kah PengWong"], "title": "CMED: A Child Micro-Expression Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions are short bursts of emotion that are difficult to hide.\nTheir detection in children is an important cue to assist psychotherapists in\nconducting better therapy. However, existing research on the detection of\nmicro-expressions has focused on adults, whose expressions differ in their\ncharacteristics from those of children. The lack of research is a direct\nconsequence of the lack of a child-based micro-expressions dataset as it is\nmuch more challenging to capture children's facial expressions due to the lack\nof predictability and controllability. This study compiles a dataset of\nspontaneous child micro-expression videos, the first of its kind, to the best\nof the authors knowledge. The dataset is captured in the wild using video\nconferencing software. This dataset enables us to then explore key features and\ndifferences between adult and child micro-expressions. This study also\nestablishes a baseline for the automated spotting and recognition of\nmicro-expressions in children using three approaches comprising of hand-created\nand learning-based approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21732", "pdf": "https://arxiv.org/pdf/2503.21732", "abs": "https://arxiv.org/abs/2503.21732", "authors": ["Xianglong He", "Zi-Xin Zou", "Chia-Hao Chen", "Yuan-Chen Guo", "Ding Liang", "Chun Yuan", "Wanli Ouyang", "Yan-Pei Cao", "Yangguang Li"], "title": "SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling", "categories": ["cs.CV"], "comment": "Project page: https://xianglonghe.github.io/TripoSF", "summary": "Creating high-fidelity 3D meshes with arbitrary topology, including open\nsurfaces and complex interiors, remains a significant challenge. Existing\nimplicit field methods often require costly and detail-degrading watertight\nconversion, while other approaches struggle with high resolutions. This paper\nintroduces SparseFlex, a novel sparse-structured isosurface representation that\nenables differentiable mesh reconstruction at resolutions up to $1024^3$\ndirectly from rendering losses. SparseFlex combines the accuracy of Flexicubes\nwith a sparse voxel structure, focusing computation on surface-adjacent regions\nand efficiently handling open surfaces. Crucially, we introduce a frustum-aware\nsectional voxel training strategy that activates only relevant voxels during\nrendering, dramatically reducing memory consumption and enabling\nhigh-resolution training. This also allows, for the first time, the\nreconstruction of mesh interiors using only rendering supervision. Building\nupon this, we demonstrate a complete shape modeling pipeline by training a\nvariational autoencoder (VAE) and a rectified flow transformer for high-quality\n3D shape generation. Our experiments show state-of-the-art reconstruction\naccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in\nF-score compared to previous methods, and demonstrate the generation of\nhigh-resolution, detailed 3D shapes with arbitrary topology. By enabling\nhigh-resolution, differentiable mesh reconstruction and generation with\nrendering losses, SparseFlex significantly advances the state-of-the-art in 3D\nshape representation and modeling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21747", "pdf": "https://arxiv.org/pdf/2503.21747", "abs": "https://arxiv.org/abs/2503.21747", "authors": ["Aniket Didolkar", "Andrii Zadaianchuk", "Rabiul Awal", "Maximilian Seitzer", "Efstratios Gavves", "Aishwarya Agrawal"], "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at CVPR 2025", "summary": "Object-centric representation learning aims to decompose visual scenes into\nfixed-size vectors called \"slots\" or \"object files\", where each slot captures a\ndistinct object. Current state-of-the-art object-centric models have shown\nremarkable success in object discovery in diverse domains, including complex\nreal-world scenes. However, these models suffer from a key limitation: they\nlack controllability. Specifically, current object-centric models learn\nrepresentations based on their preconceived understanding of objects, without\nallowing user input to guide which objects are represented. Introducing\ncontrollability into object-centric models could unlock a range of useful\ncapabilities, such as the ability to extract instance-specific representations\nfrom a scene. In this work, we propose a novel approach for user-directed\ncontrol over slot representations by conditioning slots on language\ndescriptions. The proposed ConTRoLlable Object-centric representation learning\napproach, which we term CTRL-O, achieves targeted object-language binding in\ncomplex real-world scenes without requiring mask supervision. Next, we apply\nthese controllable slot representations on two downstream vision language\ntasks: text-to-image generation and visual question answering. The proposed\napproach enables instance-specific text-to-image generation and also achieves\nstrong performance on visual question answering.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21772", "pdf": "https://arxiv.org/pdf/2503.21772", "abs": "https://arxiv.org/abs/2503.21772", "authors": ["Zilin Xiao", "Pavel Suma", "Ayush Sachdeva", "Hao-Jen Wang", "Giorgos Kordopatis-Zilos", "Giorgos Tolias", "Vicente Ordonez"], "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21777", "pdf": "https://arxiv.org/pdf/2503.21777", "abs": "https://arxiv.org/abs/2503.21777", "authors": ["Jiahao Xie", "Alessio Tonioni", "Nathalie Rauschmayr", "Federico Tombari", "Bernt Schiele"], "title": "Test-Time Visual In-Context Tuning", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/Jiahao000/VICT", "summary": "Visual in-context learning (VICL), as a new paradigm in computer vision,\nallows the model to rapidly adapt to various tasks with only a handful of\nprompts and examples. While effective, the existing VICL paradigm exhibits poor\ngeneralizability under distribution shifts. In this work, we propose test-time\nVisual In-Context Tuning (VICT), a method that can adapt VICL models on the fly\nwith a single test sample. Specifically, we flip the role between the task\nprompts and the test sample and use a cycle consistency loss to reconstruct the\noriginal task prompt output. Our key insight is that a model should be aware of\na new test distribution if it can successfully recover the original task\nprompts. Extensive experiments on six representative vision tasks ranging from\nhigh-level visual understanding to low-level image processing, with 15 common\ncorruptions, demonstrate that our VICT can improve the generalizability of VICL\nto unseen new domains. In addition, we show the potential of applying VICT for\nunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21779", "pdf": "https://arxiv.org/pdf/2503.21779", "abs": "https://arxiv.org/abs/2503.21779", "authors": ["Weihao Yu", "Yuanhao Cai", "Ruyi Zha", "Zhiwen Fan", "Chenxin Li", "Yixuan Yuan"], "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction", "categories": ["cs.CV"], "comment": "Project Page: https://x2-gaussian.github.io/", "summary": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for\ncapturing dynamic anatomical changes but faces inherent limitations from\nconventional phase-binning workflows. Current methods discretize temporal\nresolution into fixed phases with respiratory gating devices, introducing\nmotion misalignment and restricting clinical practicality. In this paper, We\npropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT\nreconstruction by integrating dynamic radiative Gaussian splatting with\nself-supervised respiratory motion learning. Our approach models anatomical\ndynamics through a spatiotemporal encoder-decoder architecture that predicts\ntime-varying Gaussian deformations, eliminating phase discretization. To remove\ndependency on external gating devices, we introduce a physiology-driven\nperiodic consistency loss that learns patient-specific breathing cycles\ndirectly from projections via differentiable optimization. Extensive\nexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR\ngain over traditional methods and 2.25 dB improvement against prior Gaussian\nsplatting techniques. By unifying continuous motion modeling with hardware-free\nperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for\ndynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21780", "pdf": "https://arxiv.org/pdf/2503.21780", "abs": "https://arxiv.org/abs/2503.21780", "authors": ["Reza Qorbani", "Gianluca Villani", "Theodoros Panagiotakopoulos", "Marc Botet Colomer", "Linus Härenstam-Nielsen", "Mattia Segu", "Pier Luigi Dovesi", "Jussi Karlgren", "Daniel Cremers", "Federico Tombari", "Matteo Poggi"], "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://thegoodailab.org/semla Code:\n  https://github.com/rezaqorbani/SemLA", "summary": "Open-vocabulary semantic segmentation models associate vision and text to\nlabel pixels from an undefined set of classes using textual queries, providing\nversatile performance on novel datasets. However, large shifts between training\nand test domains degrade their performance, requiring fine-tuning for effective\nreal-world applications. We introduce Semantic Library Adaptation (SemLA), a\nnovel framework for training-free, test-time domain adaptation. SemLA leverages\na library of LoRA-based adapters indexed with CLIP embeddings, dynamically\nmerging the most relevant adapters based on proximity to the target domain in\nthe embedding space. This approach constructs an ad-hoc model tailored to each\nspecific input without additional training. Our method scales efficiently,\nenhances explainability by tracking adapter contributions, and inherently\nprotects data privacy, making it ideal for sensitive applications.\nComprehensive experiments on a 20-domain benchmark built over 10 standard\ndatasets demonstrate SemLA's superior adaptability and performance across\ndiverse settings, establishing a new standard in domain adaptation for\nopen-vocabulary semantic segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20808", "pdf": "https://arxiv.org/pdf/2503.20808", "abs": "https://arxiv.org/abs/2503.20808", "authors": ["Xiaoming Qi", "Jingyang Zhang", "Huazhu Fu", "Guanyu Yang", "Shuo Li", "Yueming Jin"], "title": "Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for Federated Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated continual learning (FCL) offers an emerging pattern to facilitate\nthe applicability of federated learning (FL) in real-world scenarios, where\ntasks evolve dynamically and asynchronously across clients, especially in\nmedical scenario. Existing server-side FCL methods in nature domain construct a\ncontinually learnable server model by client aggregation on all-involved tasks.\nHowever, they are challenged by: (1) Catastrophic forgetting for previously\nlearned tasks, leading to error accumulation in server model, making it\ndifficult to sustain comprehensive knowledge across all tasks. (2) Biased\noptimization due to asynchronous tasks handled across different clients,\nleading to the collision of optimization targets of different clients at the\nsame time steps. In this work, we take the first step to propose a novel\nserver-side FCL pattern in medical domain, Dynamic Allocation Hypernetwork with\nadaptive model recalibration (FedDAH). It is to facilitate collaborative\nlearning under the distinct and dynamic task streams across clients. To\nalleviate the catastrophic forgetting, we propose a dynamic allocation\nhypernetwork (DAHyper) where a continually updated hypernetwork is designed to\nmanage the mapping between task identities and their associated model\nparameters, enabling the dynamic allocation of the model across clients. For\nthe biased optimization, we introduce a novel adaptive model recalibration\n(AMR) to incorporate the candidate changes of historical models into current\nserver updates, and assign weights to identical tasks across different time\nsteps based on the similarity for continual optimization. Extensive experiments\non the AMOS dataset demonstrate the superiority of our FedDAH to other FCL\nmethods on sites with different task streams. The code is\navailable:https://github.com/jinlab-imvr/FedDAH.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.20846", "pdf": "https://arxiv.org/pdf/2503.20846", "abs": "https://arxiv.org/abs/2503.20846", "authors": ["Viktor Schlegel", "Anil A Bharath", "Zilong Zhao", "Kevin Yee"], "title": "Generating Synthetic Data with Formal Privacy Guarantees: State of the Art and the Road Ahead", "categories": ["cs.CR", "cs.CL", "cs.CV"], "comment": "23 pages + references + Appendix. Preprint", "summary": "Privacy-preserving synthetic data offers a promising solution to harness\nsegregated data in high-stakes domains where information is compartmentalized\nfor regulatory, privacy, or institutional reasons. This survey provides a\ncomprehensive framework for understanding the landscape of privacy-preserving\nsynthetic data, presenting the theoretical foundations of generative models and\ndifferential privacy followed by a review of state-of-the-art methods across\ntabular data, images, and text. Our synthesis of evaluation approaches\nhighlights the fundamental trade-off between utility for down-stream tasks and\nprivacy guarantees, while identifying critical research gaps: the lack of\nrealistic benchmarks representing specialized domains and insufficient\nempirical evaluations required to contextualise formal guarantees.\n  Through empirical analysis of four leading methods on five real-world\ndatasets from specialized domains, we demonstrate significant performance\ndegradation under realistic privacy constraints ($\\epsilon \\leq 4$), revealing\na substantial gap between results reported on general domain benchmarks and\nperformance on domain-specific data. %Our findings highlight key challenges\nincluding unaccounted privacy leakage, insufficient empirical verification of\nformal guarantees, and a critical deficit of realistic benchmarks. These\nchallenges underscore the need for robust evaluation frameworks, standardized\nbenchmarks for specialized domains, and improved techniques to address the\nunique requirements of privacy-sensitive fields such that this technology can\ndeliver on its considerable potential.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21442", "pdf": "https://arxiv.org/pdf/2503.21442", "abs": "https://arxiv.org/abs/2503.21442", "authors": ["Qiyu Dai", "Xingyu Ni", "Qianfan Shen", "Wenzheng Chen", "Baoquan Chen", "Mengyu Chu"], "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We consider the problem of adding dynamic rain effects to in-the-wild scenes\nin a physically-correct manner. Recent advances in scene modeling have made\nsignificant progress, with NeRF and 3DGS techniques emerging as powerful tools\nfor reconstructing complex scenes. However, while effective for novel view\nsynthesis, these methods typically struggle with challenging scene editing\ntasks, such as physics-based rain simulation. In contrast, traditional\nphysics-based simulations can generate realistic rain effects, such as\nraindrops and splashes, but they often rely on skilled artists to carefully set\nup high-fidelity scenes. This process lacks flexibility and scalability,\nlimiting its applicability to broader, open-world environments. In this work,\nwe introduce RainyGS, a novel approach that leverages the strengths of both\nphysics-based modeling and 3DGS to generate photorealistic, dynamic rain\neffects in open-world scenes with physical accuracy. At the core of our method\nis the integration of physically-based raindrop and shallow water simulation\ntechniques within the fast 3DGS rendering framework, enabling realistic and\nefficient simulations of raindrop behavior, splashes, and reflections. Our\nmethod supports synthesizing rain effects at over 30 fps, offering users\nflexible control over rain intensity -- from light drizzles to heavy downpours.\nWe demonstrate that RainyGS performs effectively for both real-world outdoor\nscenes and large-scale driving scenarios, delivering more photorealistic and\nphysically-accurate rain effects compared to state-of-the-art methods. Project\npage can be found at https://pku-vcl-geometry.github.io/RainyGS/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21501", "pdf": "https://arxiv.org/pdf/2503.21501", "abs": "https://arxiv.org/abs/2503.21501", "authors": ["Brett Levac", "Ajil Jalal", "Kannan Ramchandran", "Jonathan I. Tamir"], "title": "Double Blind Imaging with Generative Modeling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Blind inverse problems in imaging arise from uncertainties in the system used\nto collect (noisy) measurements of images. Recovering clean images from these\nmeasurements typically requires identifying the imaging system, either\nimplicitly or explicitly. A common solution leverages generative models as\npriors for both the images and the imaging system parameters (e.g., a class of\npoint spread functions). To learn these priors in a straightforward manner\nrequires access to a dataset of clean images as well as samples of the imaging\nsystem. We propose an AmbientGAN-based generative technique to identify the\ndistribution of parameters in unknown imaging systems, using only unpaired\nclean images and corrupted measurements. This learned distribution can then be\nused in model-based recovery algorithms to solve blind inverse problems such as\nblind deconvolution. We successfully demonstrate our technique for learning\nGaussian blur and motion blur priors from noisy measurements and show their\nutility in solving blind deconvolution with diffusion posterior sampling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21510", "pdf": "https://arxiv.org/pdf/2503.21510", "abs": "https://arxiv.org/abs/2503.21510", "authors": ["Samuel Bilson", "Anna Pustogvar"], "title": "Uncertainty-aware Bayesian machine learning modelling of land cover classification", "categories": ["cs.LG", "cs.CV"], "comment": "31 pages, 10 figures", "summary": "Land cover classification involves the production of land cover maps, which\ndetermine the type of land through remote sensing imagery. Over recent years,\nsuch classification is being performed by machine learning classification\nmodels, which can give highly accurate predictions on land cover per pixel\nusing large quantities of input training data. However, such models do not\ncurrently take account of input measurement uncertainty, which is vital for\ntraceability in metrology. In this work we propose a Bayesian classification\nframework using generative modelling to take account of input measurement\nuncertainty. We take the specific case of Bayesian quadratic discriminant\nanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in\n2020 and 2021. We benchmark the performance of the model against more popular\nclassification models used in land cover maps such as random forests and neural\nnetworks. We find that such Bayesian models are more trustworthy, in the sense\nthat they are more interpretable, explicitly model the input measurement\nuncertainty, and maintain predictive performance of class probability outputs\nacross datasets of different years and sizes, whilst also being computationally\nefficient.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21555", "pdf": "https://arxiv.org/pdf/2503.21555", "abs": "https://arxiv.org/abs/2503.21555", "authors": ["Hyunjun Lee", "Hyunsoo Lee", "Sookwan Han"], "title": "SyncSDE: A Probabilistic Framework for Diffusion Synchronization", "categories": ["cs.LG", "cs.CV", "cs.GR"], "comment": "Accepted to CVPR2025", "summary": "There have been many attempts to leverage multiple diffusion models for\ncollaborative generation, extending beyond the original domain. A prominent\napproach involves synchronizing multiple diffusion trajectories by mixing the\nestimated scores to artificially correlate the generation processes. However,\nexisting methods rely on naive heuristics, such as averaging, without\nconsidering task specificity. These approaches do not clarify why such methods\nwork and often fail when a heuristic suitable for one task is blindly applied\nto others. In this paper, we present a probabilistic framework for analyzing\nwhy diffusion synchronization works and reveal where heuristics should be\nfocused - modeling correlations between multiple trajectories and adapting them\nto each specific task. We further identify optimal correlation models per task,\nachieving better results than previous approaches that apply a single heuristic\nacross all tasks without justification.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
{"id": "2503.21668", "pdf": "https://arxiv.org/pdf/2503.21668", "abs": "https://arxiv.org/abs/2503.21668", "authors": ["Danaja Rutar", "Alva Markelius", "Konstantinos Voudouris", "José Hernández-Orallo", "Lucy Cheke"], "title": "Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "One of the core components of our world models is 'intuitive physics' - an\nunderstanding of objects, space, and causality. This capability enables us to\npredict events, plan action and navigate environments, all of which rely on a\ncomposite sense of objecthood. Despite its importance, there is no single,\nunified account of objecthood, though multiple theoretical frameworks provide\ninsights. In the first part of this paper, we present a comprehensive overview\nof the main theoretical frameworks in objecthood research - Gestalt psychology,\nenactive cognition, and developmental psychology - and identify the core\ncapabilities each framework attributes to object understanding, as well as what\nfunctional roles they play in shaping world models in biological agents. Given\nthe foundational role of objecthood in world modelling, understanding\nobjecthood is also essential in AI. In the second part of the paper, we\nevaluate how current AI paradigms approach and test objecthood capabilities\ncompared to those in cognitive science. We define an AI paradigm as a\ncombination of how objecthood is conceptualised, the methods used for studying\nobjecthood, the data utilised, and the evaluation techniques. We find that,\nwhilst benchmarks can detect that AI systems model isolated aspects of\nobjecthood, the benchmarks cannot detect when AI systems lack functional\nintegration across these capabilities, not solving the objecthood challenge\nfully. Finally, we explore novel evaluation approaches that align with the\nintegrated vision of objecthood outlined in this paper. These methods are\npromising candidates for advancing from isolated object capabilities toward\ngeneral-purpose AI with genuine object understanding in real-world contexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-30.jsonl"}
