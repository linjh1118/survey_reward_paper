{"id": "2503.21956", "pdf": "https://arxiv.org/pdf/2503.21956", "abs": "https://arxiv.org/abs/2503.21956", "authors": ["Taqwa I. Alhadidi", "Asmaa Alazmi", "Shadi Jaradat", "Ahmed Jaber", "Huthaifa Ashqar", "Mohammed Elhenawy"], "title": "Enhancing Pavement Crack Classification with Bidirectional Cascaded Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Pavement distress, such as cracks and potholes, is a significant issue\naffecting road safety and maintenance. In this study, we present the\nimplementation and evaluation of Bidirectional Cascaded Neural Networks (BCNNs)\nfor the classification of pavement crack images following image augmentation.\nWe classified pavement cracks into three main categories: linear cracks,\npotholes, and fatigue cracks on an enhanced dataset utilizing U-Net 50 for\nimage augmentation. The augmented dataset comprised 599 images. Our proposed\nBCNN model was designed to leverage both forward and backward information\nflows, with detection accuracy enhanced by its cascaded structure wherein each\nlayer progressively refines the output of the preceding one. Our model achieved\nan overall accuracy of 87%, with precision, recall, and F1-score measures\nindicating high effectiveness across the categories. For fatigue cracks, the\nmodel recorded a precision of 0.87, recall of 0.83, and F1-score of 0.85 on 205\nimages. Linear cracks were detected with a precision of 0.81, recall of 0.89,\nand F1-score of 0.85 on 205 images, and potholes with a precision of 0.96,\nrecall of 0.90, and F1-score of 0.93 on 189 images. The macro and weighted\naverage of precision, recall, and F1-score were identical at 0.88, confirming\nthe BCNN's excellent performance in classifying complex pavement crack\npatterns. This research demonstrates the potential of BCNNs to significantly\nenhance the accuracy and reliability of pavement distress classification,\nresulting in more effective and efficient pavement maintenance and management\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22351", "pdf": "https://arxiv.org/pdf/2503.22351", "abs": "https://arxiv.org/abs/2503.22351", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "title": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images", "categories": ["cs.CV"], "comment": "Please visit our project page this\n  https://kaist-viclab.github.io/One-Look-is-Enough_site", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches and results in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrates into which our PRO can be well harmonized, making their DE\ncapabilities still effective for the grid input of high-resolution images with\nlittle depth discontinuities at the grid boundaries. Our PRO runs fast at\ninference time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21910", "pdf": "https://arxiv.org/pdf/2503.21910", "abs": "https://arxiv.org/abs/2503.21910", "authors": ["Karima Kadaoui", "Hanin Atwany", "Hamdan Al-Ali", "Abdelrahman Mohamed", "Ali Mekky", "Sergei Tilga", "Natalia Fedorova", "Ekaterina Artemova", "Hanan Aldarmaki", "Yova Kementchedjhieva"], "title": "JEEM: Vision-Language Understanding in Four Arabic Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "question answering"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21839", "pdf": "https://arxiv.org/pdf/2503.21839", "abs": "https://arxiv.org/abs/2503.21839", "authors": ["Haolong Yan", "Kaijun Tan", "Yeqing Shen", "Xin Huang", "Zheng Ge", "Xiangyu Zhang", "Si Li", "Daxin Jiang"], "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps://github.com/stepfun-ai/M-DocSum-Bench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "summarization", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22115", "pdf": "https://arxiv.org/pdf/2503.22115", "abs": "https://arxiv.org/abs/2503.22115", "authors": ["Yazhou Zhang", "Qimeng Liu", "Qiuchi Li", "Peng Zhang", "Jing Qin"], "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22353", "pdf": "https://arxiv.org/pdf/2503.22353", "abs": "https://arxiv.org/abs/2503.22353", "authors": ["Yubo Li", "Yidi Miao", "Xueying Ding", "Ramayya Krishnan", "Rema Padman"], "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nperformance across multiple interaction rounds. This paper introduces a\ncomprehensive framework for evaluating and improving LLM response consistency,\nmaking three key contributions. First, we propose a novel Position-Weighted\nConsistency (PWC) score that captures both the importance of early-stage\nstability and recovery patterns in multi-turn interactions. Second, we present\na carefully curated benchmark dataset spanning diverse domains and difficulty\nlevels, specifically designed to evaluate LLM consistency under various\nchallenging follow-up scenarios. Third, we introduce Confidence-Aware Response\nGeneration (CARG), a framework that significantly improves response stability\nby incorporating model confidence signals into the generation process.\nEmpirical results demonstrate that CARG significantly improves response\nstability without sacrificing accuracy, underscoring its potential for reliable\nLLM deployment in critical applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22050", "pdf": "https://arxiv.org/pdf/2503.22050", "abs": "https://arxiv.org/abs/2503.22050", "authors": ["Tai An", "Weiqiang Huang", "Da Xu", "Qingyuan He", "Jiacheng Hu", "Yujia Lou"], "title": "A Deep Learning Framework for Boundary-Aware Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "As a fundamental task in computer vision, semantic segmentation is widely\napplied in fields such as autonomous driving, remote sensing image analysis,\nand medical image processing. In recent years, Transformer-based segmentation\nmethods have demonstrated strong performance in global feature modeling.\nHowever, they still struggle with blurred target boundaries and insufficient\nrecognition of small targets. To address these issues, this study proposes a\nMask2Former-based semantic segmentation algorithm incorporating a boundary\nenhancement feature bridging module (BEFBM). The goal is to improve target\nboundary accuracy and segmentation consistency. Built upon the Mask2Former\nframework, this method constructs a boundary-aware feature map and introduces a\nfeature bridging mechanism. This enables effective cross-scale feature fusion,\nenhancing the model's ability to focus on target boundaries. Experiments on the\nCityscapes dataset demonstrate that, compared to mainstream segmentation\nmethods, the proposed approach achieves significant improvements in metrics\nsuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retention\nin complex scenes. Visual analysis further confirms the model's advantages in\nfine-grained regions. Future research will focus on optimizing computational\nefficiency and exploring its potential in other high-precision segmentation\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22093", "pdf": "https://arxiv.org/pdf/2503.22093", "abs": "https://arxiv.org/abs/2503.22093", "authors": ["Ximing Wen", "Mallika Mainali", "Anik Sen"], "title": "How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "2 pages, accepted by ToM@AAAI25", "summary": "Vision Language Models (VLMs) have demonstrated strong reasoning capabilities\nin Visual Question Answering (VQA) tasks; However, their ability to perform\nTheory of Mind (ToM) tasks such as accurately inferring human intentions,\nbeliefs, and other mental states remains underexplored. In this work, we\npropose an open-ended question framework to comprehensively evaluate VLMs'\nperformance across diverse categories of ToM tasks. We curated and annotated a\nbenchmark dataset composed of 30 images. We then assessed the performance of\nfour VLMs of varying sizes on this dataset. Our experimental results show that\nthe GPT-4 model outperformed all others, with only one smaller model,\nGPT-4o-mini, achieving comparable performance. Additionally, we observed that\nVLMs often struggle to accurately infer intentions in complex scenarios such as\nbullying or cheating. Moreover, our findings also reveal that smaller models\ncan sometimes infer correct intentions despite relying on incorrect visual\ncues.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "question answering"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22121", "pdf": "https://arxiv.org/pdf/2503.22121", "abs": "https://arxiv.org/abs/2503.22121", "authors": ["Tharun Anand", "Siva Sankar", "Pravin Nair"], "title": "Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations", "categories": ["cs.CV"], "comment": null, "summary": "With rapid advancements in generative modeling, deepfake techniques are\nincreasingly narrowing the gap between real and synthetic videos, raising\nserious privacy and security concerns. Beyond traditional face swapping and\nreenactment, an emerging trend in recent state-of-the-art deepfake generation\nmethods involves localized edits such as subtle manipulations of specific\nfacial features like raising eyebrows, altering eye shapes, or modifying mouth\nexpressions. These fine-grained manipulations pose a significant challenge for\nexisting detection models, which struggle to capture such localized variations.\nTo the best of our knowledge, this work presents the first detection approach\nexplicitly designed to generalize to localized edits in deepfake videos by\nleveraging spatiotemporal representations guided by facial action units. Our\nmethod leverages a cross-attention-based fusion of representations learned from\npretext tasks like random masking and action unit detection, to create an\nembedding that effectively encodes subtle, localized changes. Comprehensive\nevaluations across multiple deepfake generation methods demonstrate that our\napproach, despite being trained solely on the traditional FF+ dataset, sets a\nnew benchmark in detecting recent deepfake-generated videos with fine-grained\nlocal edits, achieving a $20\\%$ improvement in accuracy over current\nstate-of-the-art detection methods. Additionally, our method delivers\ncompetitive performance on standard datasets, highlighting its robustness and\ngeneralization across diverse types of local and global forgeries.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22152", "pdf": "https://arxiv.org/pdf/2503.22152", "abs": "https://arxiv.org/abs/2503.22152", "authors": ["Yuxuan Li", "Vijay Veerabadran", "Michael L. Iuzzolino", "Brett D. Roads", "Asli Celikyilmaz", "Karl Ridgeway"], "title": "EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce EgoToM, a new video question-answering benchmark that extends\nTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToM\nmodel, we generate multi-choice video QA instances for the Ego4D dataset to\nbenchmark the ability to predict a camera wearer's goals, beliefs, and next\nactions. We study the performance of both humans and state of the art\nmultimodal large language models (MLLMs) on these three interconnected\ninference problems. Our evaluation shows that MLLMs achieve close to\nhuman-level accuracy on inferring goals from egocentric videos. However, MLLMs\n(including the largest ones we tested with over 100B parameters) fall short of\nhuman performance when inferring the camera wearers' in-the-moment belief\nstates and future actions that are most consistent with the unseen video\nfuture. We believe that our results will shape the future design of an\nimportant class of egocentric digital assistants which are equipped with a\nreasonable model of the user's internal mental states.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22262", "pdf": "https://arxiv.org/pdf/2503.22262", "abs": "https://arxiv.org/abs/2503.22262", "authors": ["Songsong Yu", "Yuxin Chen", "Zhongang Qi", "Zeke Xie", "Yifan Wang", "Lijun Wang", "Ying Shan", "Huchuan Lu"], "title": "Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Project webpage:\n  https://mono2stereo-bench.github.io/", "summary": "With the rapid proliferation of 3D devices and the shortage of 3D content,\nstereo conversion is attracting increasing attention. Recent works introduce\npretrained Diffusion Models (DMs) into this task. However, due to the scarcity\nof large-scale training data and comprehensive benchmarks, the optimal\nmethodologies for employing DMs in stereo conversion and the accurate\nevaluation of stereo effects remain largely unexplored. In this work, we\nintroduce the Mono2Stereo dataset, providing high-quality training data and\nbenchmark to support in-depth exploration of stereo conversion. With this\ndataset, we conduct an empirical study that yields two primary findings. 1) The\ndifferences between the left and right views are subtle, yet existing metrics\nconsider overall pixels, failing to concentrate on regions critical to stereo\neffects. 2) Mainstream methods adopt either one-stage left-to-right generation\nor warp-and-inpaint pipeline, facing challenges of degraded stereo effect and\nimage distortion respectively. Based on these findings, we introduce a new\nevaluation metric, Stereo Intersection-over-Union, which prioritizes disparity\nand achieves a high correlation with human judgments on stereo effect.\nMoreover, we propose a strong baseline model, harmonizing the stereo effect and\nimage quality simultaneously, and notably surpassing current mainstream\nmethods. Our code and data will be open-sourced to promote further research in\nstereo conversion. Our models are available at mono2stereo-bench.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "correlation"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22375", "pdf": "https://arxiv.org/pdf/2503.22375", "abs": "https://arxiv.org/abs/2503.22375", "authors": ["Christian Steinhauser", "Philipp Reis", "Hubert Padusinski", "Jacob Langner", "Eric Sax"], "title": "Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to IEEE IV 2025, Under Review", "summary": "Precise perception of the environment is essential in highly automated\ndriving systems, which rely on machine learning tasks such as object detection\nand segmentation. Compression of sensor data is commonly used for data\nhandling, while virtualization is used for hardware-in-the-loop validation.\nBoth methods can alter sensor data and degrade model performance. This\nnecessitates a systematic approach to quantifying image validity. This paper\npresents a four-step framework to evaluate the impact of image modifications on\nmachine learning tasks. First, a dataset with modified images is prepared to\nensure one-to-one matching image pairs, enabling measurement of deviations\nresulting from compression and virtualization. Second, image deviations are\nquantified by comparing the effects of compression and virtualization against\noriginal camera-based sensor data. Third, the performance of state-of-the-art\nobject detection models is analyzed to determine how altered input data affects\nperception tasks, including bounding box accuracy and reliability. Finally, a\ncorrelation analysis is performed to identify relationships between image\nquality and model performance. As a result, the LPIPS metric achieves the\nhighest correlation between image deviation and machine learning performance\nacross all evaluated machine learning tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22420", "pdf": "https://arxiv.org/pdf/2503.22420", "abs": "https://arxiv.org/abs/2503.22420", "authors": ["Jiangyong Huang", "Baoxiong Jia", "Yan Wang", "Ziyu Zhu", "Xiongkun Linghu", "Qing Li", "Song-Chun Zhu", "Siyuan Huang"], "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://beacon-3d.github.io", "summary": "Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VL\nmodels, creating a \"mist\" that obscures rigorous insights into model\ncapabilities and 3D-VL tasks. This mist persists due to three key limitations.\nFirst, flawed test data, like ambiguous referential text in the grounding task,\ncan yield incorrect and unreliable test results. Second, oversimplified metrics\nsuch as simply averaging accuracy per question answering (QA) pair, cannot\nreveal true model capability due to their vulnerability to language variations.\nThird, existing benchmarks isolate the grounding and QA tasks, disregarding the\nunderlying coherence that QA should be based on solid grounding capabilities.\nTo unveil the \"mist\", we propose Beacon3D, a benchmark for 3D-VL grounding and\nQA tasks, delivering a perspective shift in the evaluation of 3D-VL\nunderstanding. Beacon3D features (i) high-quality test data with precise and\nnatural language, (ii) object-centric evaluation with multiple tests per object\nto ensure robustness, and (iii) a novel chain-of-analysis paradigm to address\nlanguage robustness and model performance coherence across grounding and QA.\nOur evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)\nobject-centric evaluation elicits true model performance and particularly weak\ngeneralization in QA; (ii) grounding-QA coherence remains fragile in current\n3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VL\nmodels, though as a prevalent practice, hinders grounding capabilities and has\nyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysis\ncould benefit the 3D-VL community towards faithful developments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22526", "pdf": "https://arxiv.org/pdf/2503.22526", "abs": "https://arxiv.org/abs/2503.22526", "authors": ["Martin Kišš", "Michal Hradiš", "Martina Dvořáková", "Václav Jiroušek", "Filip Kersch"], "title": "AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 2 tables, 6 figures; Submitted to ICDAR25", "summary": "We introduce the AnnoPage Dataset, a novel collection of 7550 pages from\nhistorical documents, primarily in Czech and German, spanning from 1485 to the\npresent, focusing on the late 19th and early 20th centuries. The dataset is\ndesigned to support research in document layout analysis and object detection.\nEach page is annotated with axis-aligned bounding boxes (AABB) representing\nelements of 25 categories of non-textual elements, such as images, maps,\ndecorative elements, or charts, following the Czech Methodology of image\ndocument processing. The annotations were created by expert librarians to\nensure accuracy and consistency. The dataset also incorporates pages from\nmultiple, mainly historical, document datasets to enhance variability and\nmaintain continuity. The dataset is divided into development and test subsets,\nwith the test set carefully selected to maintain the category distribution. We\nprovide baseline results using YOLO and DETR object detectors, offering a\nreference point for future research. The AnnoPage Dataset is publicly available\non Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truth\nannotations in YOLO format.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21818", "pdf": "https://arxiv.org/pdf/2503.21818", "abs": "https://arxiv.org/abs/2503.21818", "authors": ["Tianqi Tu", "Hui Wang", "Jiangbo Pei", "Xiaojuan Yu", "Aidong Men", "Suxia Wang", "Qingchao Chen", "Ying Tan", "Feng Yu", "Minghui Zhao"], "title": "Deep Learning-Based Quantitative Assessment of Renal Chronicity Indices in Lupus Nephritis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Renal chronicity indices (CI) have been identified as strong\npredictors of long-term outcomes in lupus nephritis (LN) patients. However,\nassessment by pathologists is hindered by challenges such as substantial time\nrequirements, high interobserver variation, and susceptibility to fatigue. This\nstudy aims to develop an effective deep learning (DL) pipeline that automates\nthe assessment of CI and provides valuable prognostic insights from a\ndisease-specific perspective. Methods: We curated a dataset comprising 282\nslides obtained from 141 patients across two independent cohorts with a\ncomplete 10-years follow-up. Our DL pipeline was developed on 60 slides (22,410\npatch images) from 30 patients in the training cohort and evaluated on both an\ninternal testing set (148 slides, 77,605 patch images) and an external testing\nset (74 slides, 27,522 patch images). Results: The study included two cohorts\nwith slight demographic differences, particularly in age and hemoglobin levels.\nThe DL pipeline showed high segmentation performance across tissue compartments\nand histopathologic lesions, outperforming state-of-the-art methods. The DL\npipeline also demonstrated a strong correlation with pathologists in assessing\nCI, significantly improving interobserver agreement. Additionally, the DL\npipeline enhanced prognostic accuracy, particularly in outcome prediction, when\ncombined with clinical parameters and pathologist-assessed CIs Conclusions: The\nDL pipeline demonstrated accuracy and efficiency in assessing CI in LN, showing\npromise in improving interobserver agreement among pathologists. It also\nexhibited significant value in prognostic analysis and enhancing outcome\nprediction in LN patients, offering a valuable tool for clinical\ndecision-making.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "agreement", "accuracy"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22052", "pdf": "https://arxiv.org/pdf/2503.22052", "abs": "https://arxiv.org/abs/2503.22052", "authors": ["Jan Hurtado", "Joao P. Maia", "Cesar A. Sierra-Franco", "Alberto Raposo"], "title": "Improving the generalization of deep learning models in the segmentation of mammography images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Mammography stands as the main screening method for detecting breast cancer\nearly, enhancing treatment success rates. The segmentation of landmark\nstructures in mammography images can aid the medical assessment in the\nevaluation of cancer risk and the image acquisition adequacy. We introduce a\nseries of data-centric strategies aimed at enriching the training data for deep\nlearning-based segmentation of landmark structures. Our approach involves\naugmenting the training samples through annotation-guided image intensity\nmanipulation and style transfer to achieve better generalization than standard\ntraining procedures. These augmentations are applied in a balanced manner to\nensure the model learns to process a diverse range of images generated by\ndifferent vendor equipments while retaining its efficacy on the original data.\nWe present extensive numerical and visual results that demonstrate the superior\ngeneralization capabilities of our methods when compared to the standard\ntraining. For this evaluation, we consider a large dataset that includes\nmammography images generated by different vendor equipments. Further, we\npresent complementary results that show both the strengths and limitations of\nour methods across various scenarios. The accuracy and robustness demonstrated\nin the experiments suggest that our method is well-suited for integration into\nclinical practice.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813", "abs": "https://arxiv.org/abs/2503.21813", "authors": ["Zhangcheng Qiang"], "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems", "categories": ["cs.CL", "cs.IR"], "comment": "10 pages, 4 figures, 3 tables, 2 prompt templates", "summary": "Hallucinations are inevitable in downstream tasks using large language models\n(LLMs). While addressing hallucinations becomes a substantial challenge for\nLLM-based ontology matching (OM) systems, we introduce a new benchmark dataset\ncalled OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching)\ndatasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing\nhallucinations of different LLMs performing OM tasks. These OM-specific\nhallucinations are carefully classified into two primary categories and six\nsub-categories. We showcase the usefulness of the dataset in constructing the\nLLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21819", "pdf": "https://arxiv.org/pdf/2503.21819", "abs": "https://arxiv.org/abs/2503.21819", "authors": ["Xuying Li", "Zhuo Li", "Yuji Kosuga", "Victor Bian"], "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human values and safety\nconstraints is challenging, especially when objectives like helpfulness,\ntruthfulness, and avoidance of harm conflict. Reinforcement Learning from Human\nFeedback (RLHF) has achieved notable success in steering models, but is complex\nand can be unstable. Recent approaches such as Direct Preference Optimization\n(DPO) simplify preference-based fine-tuning but may introduce bias or trade-off\ncertain objectives~\\cite{dpo}. In this work, we propose a Group Relative Policy\nOptimization (GRPO) framework with a multi-label reward regression model to\nachieve safe and aligned language generation. The GRPO algorithm optimizes a\npolicy by comparing groups of sampled responses, eliminating the need for a\nseparate value critic and improving training efficiency~\\cite{grpo}. We train a\nreward model to predict multiple alignment scores (e.g., safety, helpfulness,\netc.), which are combined into a single reward signal. We provide a theoretical\nderivation for using this learned multi-aspect reward within GRPO and discuss\nits advantages and limitations. Empirically, our approach improves all the\nsafety and quality metrics evaluated in language generation tasks on model\nscales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of\nobjectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO\nachieves alignment with significantly lower computational cost and explicit\nmulti-objective handling. \\textbf{We will open-source all trained models at\nhttps://huggingface.co/hydroxai.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "PPO", "reinforcement learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 8}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "truthfulness", "safety"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21827", "pdf": "https://arxiv.org/pdf/2503.21827", "abs": "https://arxiv.org/abs/2503.21827", "authors": ["Mark Phil Pacot", "Jayno Juventud", "Gleen Dalaorao"], "title": "Hybrid Multi-Stage Learning Framework for Edge Detection: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection remains a fundamental yet challenging task in computer vision,\nespecially under varying illumination, noise, and complex scene conditions.\nThis paper introduces a Hybrid Multi-Stage Learning Framework that integrates\nConvolutional Neural Network (CNN) feature extraction with a Support Vector\nMachine (SVM) classifier to improve edge localization and structural accuracy.\nUnlike conventional end-to-end deep learning models, our approach decouples\nfeature representation and classification stages, enhancing robustness and\ninterpretability. Extensive experiments conducted on benchmark datasets such as\nBSDS500 and NYUDv2 demonstrate that the proposed framework outperforms\ntraditional edge detectors and even recent learning-based methods in terms of\nOptimal Dataset Scale (ODS) and Optimal Image Scale (OIS), while maintaining\ncompetitive Average Precision (AP). Both qualitative and quantitative results\nhighlight enhanced performance on edge continuity, noise suppression, and\nperceptual clarity achieved by our method. This work not only bridges classical\nand deep learning paradigms but also sets a new direction for scalable,\ninterpretable, and high-quality edge detection solutions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21848", "pdf": "https://arxiv.org/pdf/2503.21848", "abs": "https://arxiv.org/abs/2503.21848", "authors": ["Jonathan Attard", "Dylan Seychell"], "title": "Comparative Analysis of Image, Video, and Audio Classifiers for Automated News Video Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint for paper in CAI 2025, 7 pages, 5 tables, 3 tables", "summary": "News videos require efficient content organisation and retrieval systems, but\ntheir unstructured nature poses significant challenges for automated\nprocessing. This paper presents a comprehensive comparative analysis of image,\nvideo, and audio classifiers for automated news video segmentation. This work\npresents the development and evaluation of multiple deep learning approaches,\nincluding ResNet, ViViT, AST, and multimodal architectures, to classify five\ndistinct segment types: advertisements, stories, studio scenes, transitions,\nand visualisations. Using a custom-annotated dataset of 41 news videos\ncomprising 1,832 scene clips, our experiments demonstrate that image-based\nclassifiers achieve superior performance (84.34\\% accuracy) compared to more\ncomplex temporal models. Notably, the ResNet architecture outperformed\nstate-of-the-art video classifiers while requiring significantly fewer\ncomputational resources. Binary classification models achieved high accuracy\nfor transitions (94.23\\%) and advertisements (92.74\\%). These findings advance\nthe understanding of effective architectures for news video segmentation and\nprovide practical insights for implementing automated content organisation\nsystems in media applications. These include media archiving, personalised\ncontent delivery, and intelligent video search.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21889", "pdf": "https://arxiv.org/pdf/2503.21889", "abs": "https://arxiv.org/abs/2503.21889", "authors": ["Patrice Bechard", "Chao Wang", "Amirhossein Abaskohi", "Juan Rodriguez", "Christopher Pal", "David Vazquez", "Spandana Gella", "Sai Rajeswar", "Perouz Taslakian"], "title": "StarFlow: Generating Structured Workflow Outputs From Sketch Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Workflows are a fundamental component of automation in enterprise platforms,\nenabling the orchestration of tasks, data processing, and system integrations.\nDespite being widely used, building workflows can be complex, often requiring\nmanual configuration through low-code platforms or visual programming tools. To\nsimplify this process, we explore the use of generative foundation models,\nparticularly vision-language models (VLMs), to automatically generate\nstructured workflows from visual inputs. Translating hand-drawn sketches or\ncomputer-generated diagrams into executable workflows is challenging due to the\nambiguity of free-form drawings, variations in diagram styles, and the\ndifficulty of inferring execution logic from visual elements. To address this,\nwe introduce StarFlow, a framework for generating structured workflow outputs\nfrom sketches using vision-language models. We curate a diverse dataset of\nworkflow diagrams -- including synthetic, manually annotated, and real-world\nsamples -- to enable robust training and evaluation. We finetune and benchmark\nmultiple vision-language models, conducting a series of ablation studies to\nanalyze the strengths and limitations of our approach. Our results show that\nfinetuning significantly enhances structured workflow generation, outperforming\nlarge vision-language models on this task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22040", "pdf": "https://arxiv.org/pdf/2503.22040", "abs": "https://arxiv.org/abs/2503.22040", "authors": ["Hao Lin", "Yongjun Zhang"], "title": "The Risks of Using Large Language Models for Text Annotation in Social Science Research", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Generative artificial intelligence (GenAI) or large language models (LLMs)\nhave the potential to revolutionize computational social science, particularly\nin automated textual analysis. In this paper, we conduct a systematic\nevaluation of the promises and risks of using LLMs for diverse coding tasks,\nwith social movement studies serving as a case example. We propose a framework\nfor social scientists to incorporate LLMs into text annotation, either as the\nprimary coding decision-maker or as a coding assistant. This framework provides\ntools for researchers to develop the optimal prompt, and to examine and report\nthe validity and reliability of LLMs as a methodological tool. Additionally, we\ndiscuss the associated epistemic risks related to validity, reliability,\nreplicability, and transparency. We conclude with several practical guidelines\nfor using LLMs in text annotation tasks, and how we can better communicate the\nepistemic risks in research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "reliability"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21904", "pdf": "https://arxiv.org/pdf/2503.21904", "abs": "https://arxiv.org/abs/2503.21904", "authors": ["Zhiwei Yang", "Chen Gao", "Jing Liu", "Peng Wu", "Guansong Pang", "Mike Zheng Shou"], "title": "AssistPDA: An Online Video Surveillance Assistant for Video Anomaly Prediction, Detection, and Analysis", "categories": ["cs.CV"], "comment": "13 pages", "summary": "The rapid advancements in large language models (LLMs) have spurred growing\ninterest in LLM-based video anomaly detection (VAD). However, existing\napproaches predominantly focus on video-level anomaly question answering or\noffline detection, ignoring the real-time nature essential for practical VAD\napplications. To bridge this gap and facilitate the practical deployment of\nLLM-based VAD, we introduce AssistPDA, the first online video anomaly\nsurveillance assistant that unifies video anomaly prediction, detection, and\nanalysis (VAPDA) within a single framework. AssistPDA enables real-time\ninference on streaming videos while supporting interactive user engagement.\nNotably, we introduce a novel event-level anomaly prediction task, enabling\nproactive anomaly forecasting before anomalies fully unfold. To enhance the\nability to model intricate spatiotemporal relationships in anomaly events, we\npropose a Spatio-Temporal Relation Distillation (STRD) module. STRD transfers\nthe long-term spatiotemporal modeling capabilities of vision-language models\n(VLMs) from offline settings to real-time scenarios. Thus it equips AssistPDA\nwith a robust understanding of complex temporal dependencies and long-sequence\nmemory. Additionally, we construct VAPDA-127K, the first large-scale benchmark\ndesigned for VLM-based online VAPDA. Extensive experiments demonstrate that\nAssistPDA outperforms existing offline VLM-based approaches, setting a new\nstate-of-the-art for real-time VAPDA. Our dataset and code will be open-sourced\nto facilitate further research in the community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22388", "pdf": "https://arxiv.org/pdf/2503.22388", "abs": "https://arxiv.org/abs/2503.22388", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "LLMs are transforming software development, yet current code generation and\ncode repair benchmarks mainly assess syntactic and functional correctness in\nsimple, single-error cases. LLMs' capabilities to autonomously find and fix\nruntime logical errors in complex data science code remain largely unexplored.\nTo address this gap, we introduce DSDBench: the Data Science Debugging\nBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hop\nerror tracing and multi-bug detection in data science code debugging. DSDBench\nadapts datasets from existing data science task benchmarks, such as DABench and\nMatPlotBench, featuring realistic data science debugging tasks with\nautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes\n1,117 annotated samples with 741 cause-effect error pairs and runtime error\nmessages. Evaluations of state-of-the-art LLMs on DSDBench show significant\nperformance gaps, highlighting challenges in debugging logical runtime errors\nin data science code. DSDBench offers a crucial resource to evaluate and\nimprove LLMs' debugging and reasoning capabilities, enabling more reliable\nAI-assisted data science in the future.DSDBench is publicly available at\nhttps://github.com/KevinCL16/DSDBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22026", "pdf": "https://arxiv.org/pdf/2503.22026", "abs": "https://arxiv.org/abs/2503.22026", "authors": ["SaiKiran Tedla", "Junyong Lee", "Beixuan Yang", "Mahmoud Afifi", "Michael Brown"], "title": "Multispectral Demosaicing via Dual Cameras", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Multispectral (MS) images capture detailed scene information across a wide\nrange of spectral bands, making them invaluable for applications requiring rich\nspectral data. Integrating MS imaging into multi camera devices, such as\nsmartphones, has the potential to enhance both spectral applications and RGB\nimage quality. A critical step in processing MS data is demosaicing, which\nreconstructs color information from the mosaic MS images captured by the\ncamera. This paper proposes a method for MS image demosaicing specifically\ndesigned for dual-camera setups where both RGB and MS cameras capture the same\nscene. Our approach leverages co-captured RGB images, which typically have\nhigher spatial fidelity, to guide the demosaicing of lower-fidelity MS images.\nWe introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB\nand MS mosaiced images with ground-truth demosaiced outputs - that enables\ntraining and evaluation of our method. Experimental results demonstrate that\nour method achieves state-of-the-art accuracy compared to existing techniques.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22060", "pdf": "https://arxiv.org/pdf/2503.22060", "abs": "https://arxiv.org/abs/2503.22060", "authors": ["Ukcheol Shin", "Jinsun Park"], "title": "Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges", "categories": ["cs.CV", "cs.RO"], "comment": "MS^2 dataset:\n  https://sites.google.com/view/multi-spectral-stereo-dataset, Source code:\n  https://github.com/UkcheolShin/SupDepth4Thermal", "summary": "Achieving robust and accurate spatial perception under adverse weather and\nlighting conditions is crucial for the high-level autonomy of self-driving\nvehicles and robots. However, existing perception algorithms relying on the\nvisible spectrum are highly affected by weather and lighting conditions. A\nlong-wave infrared camera (i.e., thermal imaging camera) can be a potential\nsolution to achieve high-level robustness. However, the absence of large-scale\ndatasets and standardized benchmarks remains a significant bottleneck to\nprogress in active research for robust visual perception from thermal images.\nTo this end, this manuscript provides a large-scale Multi-Spectral Stereo\n(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,\nstereo LiDAR data, and GNSS/IMU information along with semi-dense depth ground\ntruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairs\ncaptured across diverse locations (e.g., urban city, residential area, campus,\nand high-way road) at different times (e.g., morning, daytime, and nighttime)\nand under various weather conditions (e.g., clear-sky, cloudy, and rainy).\nSecondly, we conduct a thorough evaluation of monocular and stereo depth\nestimation networks across RGB, NIR, and thermal modalities to establish\nstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, and\nrainy). Lastly, we provide in-depth analyses and discuss the challenges\nrevealed by the benchmark results, such as the performance variability for each\nmodality under adverse conditions, domain shift between different sensor\nmodalities, and potential research direction for thermal perception. Our\ndataset and source code are publicly available at\nhttps://sites.google.com/view/multi-spectral-stereo-dataset and\nhttps://github.com/UkcheolShin/SupDepth4Thermal.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22069", "pdf": "https://arxiv.org/pdf/2503.22069", "abs": "https://arxiv.org/abs/2503.22069", "authors": ["Ekansh Chauhan", "Anila Sharma", "Amit Sharma", "Vikas Nishadham", "Asha Ghughtyal", "Ankur Kumar", "Gurudutt Gupta", "Anurag Mehta", "C. V. Jawahar", "P. K. Vinod"], "title": "Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast cancer, the most common malignancy among women, requires precise\ndetection and classification for effective treatment. Immunohistochemistry\n(IHC) biomarkers like HER2, ER, and PR are critical for identifying breast\ncancer subtypes. However, traditional IHC classification relies on\npathologists' expertise, making it labor-intensive and subject to significant\ninter-observer variability. To address these challenges, this study introduces\nthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHC\nslides (HER2, ER, and PR) aimed at automating receptor status classification.\nThe primary focus is on developing predictive models for HER2 3-way\nclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deep\nlearning models revealed that an end-to-end ConvNeXt network utilizing\nlow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,\nand 83.56%, respectively, for 3-way classification, outperforming patch-based\nmethods by over 5.35% in F1 score. This study highlights the potential of\nsimple yet effective deep learning techniques to significantly improve accuracy\nand reproducibility in breast cancer classification, supporting their\nintegration into clinical workflows for better patient outcomes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22087", "pdf": "https://arxiv.org/pdf/2503.22087", "abs": "https://arxiv.org/abs/2503.22087", "authors": ["Seokha Moon", "Janghyun Baek", "Giseop Kim", "Jinkyu Kim", "Sunwook Choi"], "title": "Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy prediction has emerged as a key perception task for autonomous\ndriving, as it reconstructs 3D environments to provide a comprehensive scene\nunderstanding. Recent studies focus on integrating spatiotemporal information\nobtained from past observations to improve prediction accuracy, using a\nmulti-frame fusion approach that processes multiple past frames together.\nHowever, these methods struggle with a trade-off between efficiency and\naccuracy, which significantly limits their practicality. To mitigate this\ntrade-off, we propose StreamOcc, a novel framework that aggregates\nspatio-temporal information in a stream-based manner. StreamOcc consists of two\nkey components: (i) Stream-based Voxel Aggregation, which effectively\naccumulates past observations while minimizing computational costs, and (ii)\nQuery-guided Aggregation, which recurrently aggregates instance-level features\nof dynamic objects into corresponding voxel features, refining fine-grained\ndetails of dynamic objects. Experiments on the Occ3D-nuScenes dataset show that\nStreamOcc achieves state-of-the-art performance in real-time settings, while\nreducing memory usage by more than 50% compared to previous methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22125", "pdf": "https://arxiv.org/pdf/2503.22125", "abs": "https://arxiv.org/abs/2503.22125", "authors": ["Ivan Beleacov"], "title": "Semantic segmentation for building houses from wooden cubes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, 2 tables", "summary": "Automated construction is one of the most promising areas that can improve\nefficiency, reduce costs and minimize errors in the process of building\nconstruction. In this paper, a comparative analysis of three neural network\nmodels for semantic segmentation, U-Net(light), LinkNet and PSPNet, is\nperformed. Two specialized datasets with images of houses built from wooden\ncubes were created for the experiments. The first dataset contains 4 classes\n(background, foundation, walls, roof ) and is designed for basic model\nevaluation, while the second dataset includes 44 classes where each cube is\nlabeled as a separate object. The models were trained with the same\nhyperparameters and their accuracy was evaluated using MeanIoU and F1 Score\nmetrics. According to the results obtained, U-Net(light) showed the best\nperformance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and\n25% respectively on the second dataset. The poor results on the second dataset\nare due to the limited amount of data, the complexity of the partitioning and\nthe imbalance of classes, making it difficult to accurately select individual\ncubes. In addition, overtraining was observed in all experiments, manifested by\nhigh accuracy on the training dataset and its significant decrease on the\nvalidation dataset. The present work is the basis for the development of\nalgorithms for automatic generation of staged building plans, which can be\nfurther scaled to design complete buildings. Future research is planned to\nextend the datasets and apply methods to combat overfitting (L1/L2\nregularization, Early Stopping). The next stage of work will be the development\nof algorithms for automatic generation of a step-by-step plan for building\nhouses from cubes using manipulators. Index Terms-Deep Learning, Computer\nvision, CNN, Semantic segmentation, Construction materials.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22251", "pdf": "https://arxiv.org/pdf/2503.22251", "abs": "https://arxiv.org/abs/2503.22251", "authors": ["Guneet Mutreja", "Ksenia Bittner"], "title": "Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach", "categories": ["cs.CV"], "comment": null, "summary": "Accurate classification of building roof types from aerial imagery is crucial\nfor various remote sensing applications, including urban planning, disaster\nmanagement, and infrastructure monitoring. However, this task is often hindered\nby the limited availability of labeled data for supervised learning approaches.\nTo address this challenge, this paper investigates the effectiveness of self\nsupervised learning with EfficientNet architectures, known for their\ncomputational efficiency, for building roof type classification. We propose a\nnovel framework that incorporates a Convolutional Block Attention Module (CBAM)\nto enhance the feature extraction capabilities of EfficientNet. Furthermore, we\nexplore the benefits of pretraining on a domain-specific dataset, the Aerial\nImage Dataset (AID), compared to ImageNet pretraining. Our experimental results\ndemonstrate the superiority of our approach. Employing Simple Framework for\nContrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3\nand CBAM achieves a 95.5% accuracy on our validation set, matching the\nperformance of state-of-the-art transformer-based models while utilizing\nsignificantly fewer parameters. We also provide a comprehensive evaluation on\ntwo challenging test sets, demonstrating the generalization capability of our\nmethod. Notably, our findings highlight the effectiveness of domain-specific\npretraining, consistently leading to higher accuracy compared to models\npretrained on the generic ImageNet dataset. Our work establishes EfficientNet\nbased self-supervised learning as a computationally efficient and highly\neffective approach for building roof type classification, particularly\nbeneficial in scenarios with limited labeled data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22309", "pdf": "https://arxiv.org/pdf/2503.22309", "abs": "https://arxiv.org/abs/2503.22309", "authors": ["Zakaria Laskar", "Tomas Vojir", "Matej Grcic", "Iaroslav Melekhov", "Shankar Gangisettye", "Juho Kannala", "Jiri Matas", "Giorgos Tolias", "C. V. Jawahar"], "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Before deployment in the real-world deep neural networks require thorough\nevaluation of how they handle both knowns, inputs represented in the training\ndata, and unknowns (anomalies). This is especially important for scene\nunderstanding tasks with safety critical applications, such as in autonomous\ndriving. Existing datasets allow evaluation of only knowns or unknowns - but\nnot both, which is required to establish \"in the wild\" suitability of deep\nneural network models. To bridge this gap, we propose a novel anomaly\nsegmentation dataset, ISSU, that features a diverse set of anomaly inputs from\ncluttered real-world environments. The dataset is twice larger than existing\nanomaly segmentation datasets, and provides a training, validation and test set\nfor controlled in-domain evaluation. The test set consists of a static and\ntemporal part, with the latter comprised of videos. The dataset provides\nannotations for both closed-set (knowns) and anomalies, enabling closed-set and\nopen-set evaluation. The dataset covers diverse conditions, such as domain and\ncross-sensor shift, illumination variation and allows ablation of anomaly\ndetection methods with respect to these variations. Evaluation results of\ncurrent state-of-the-art methods confirm the need for improvements especially\nin domain-generalization, small and large object segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22374", "pdf": "https://arxiv.org/pdf/2503.22374", "abs": "https://arxiv.org/abs/2503.22374", "authors": ["Giulio Federico", "Giuseppe Amato", "Fabio Carrara", "Claudio Gennaro", "Marco Di Benedetto"], "title": "ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the nature of human sketches is challenging because of the wide\nvariation in how they are created. Recognizing complex structural patterns\nimproves both the accuracy in recognizing sketches and the fidelity of the\ngenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithm\ndesigned to address these challenges through a multi-scale context extraction\napproach. The model captures intricate details at multiple scales and combines\nthem using an ensemble-like mechanism, where the extracted features work\ncollaboratively to enhance the recognition and generation of key details\ncrucial for classification and generation tasks.\n  The effectiveness of ViSketch-GPT is validated through extensive experiments\non the QuickDraw dataset. Our model establishes a new benchmark, significantly\noutperforming existing methods in both classification and generation tasks,\nwith substantial improvements in accuracy and the fidelity of generated\nsketches.\n  The proposed algorithm offers a robust framework for understanding complex\nstructures by extracting features that collaborate to recognize intricate\ndetails, enhancing the understanding of structures like sketches and making it\na versatile tool for various applications in computer vision and machine\nlearning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22394", "pdf": "https://arxiv.org/pdf/2503.22394", "abs": "https://arxiv.org/abs/2503.22394", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Qiqi Yao", "Haijun Hu", "Jiankun Wang", "Xi Zhang an Hongliang Ren"], "title": "Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate tissue point tracking in endoscopic videos is critical for\nrobotic-assisted surgical navigation and scene understanding, but remains\nchallenging due to complex deformations, instrument occlusion, and the scarcity\nof dense trajectory annotations. Existing methods struggle with long-term\ntracking under these conditions due to limited feature utilization and\nannotation dependence. We present Endo-TTAP, a novel framework addressing these\nchallenges through: (1) A Multi-Facet Guided Attention (MFGA) module that\nsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicit\nmotion patterns to jointly predict point positions with uncertainty and\nocclusion awareness; (2) A two-stage curriculum learning strategy employing an\nAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybrid\nsupervision. Stage I utilizes synthetic data with optical flow ground truth for\nuncertainty-occlusion regularization, while Stage II combines unsupervised flow\nconsistency and semi-supervised learning with refined pseudo-labels from\noff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasets\nand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-art\nperformance in tissue point tracking, particularly in scenarios characterized\nby complex endoscopic conditions. The source code and dataset will be available\nat https://anonymous.4open.science/r/Endo-TTAP-36E5.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22176", "pdf": "https://arxiv.org/pdf/2503.22176", "abs": "https://arxiv.org/abs/2503.22176", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Kalyan Sivasailam", "Anandakumar D", "Keerthana R", "Mounigasri M", "Abilaasha G", "Kishore Prasath Venkatesh"], "title": "A Multi-Site Study on AI-Driven Pathology Detection and Osteoarthritis Grading from Knee X-Ray", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "15 pages, 2 figures", "summary": "Introduction: Bone health disorders like osteoarthritis and osteoporosis pose\nmajor global health challenges, often leading to delayed diagnoses due to\nlimited diagnostic tools. This study presents an AI-powered system that\nanalyzes knee X-rays to detect key pathologies, including joint space\nnarrowing, sclerosis, osteophytes, tibial spikes, alignment issues, and soft\ntissue anomalies. It also grades osteoarthritis severity, enabling timely,\npersonalized treatment.\n  Study Design: The research used 1.3 million knee X-rays from a multi-site\nIndian clinical trial across government, private, and SME hospitals. The\ndataset ensured diversity in demographics, imaging equipment, and clinical\nsettings. Rigorous annotation and preprocessing yielded high-quality training\ndatasets for pathology-specific models like ResNet15 for joint space narrowing\nand DenseNet for osteoarthritis grading.\n  Performance: The AI system achieved strong diagnostic accuracy across diverse\nimaging environments. Pathology-specific models excelled in precision, recall,\nand NPV, validated using Mean Squared Error (MSE), Intersection over Union\n(IoU), and Dice coefficient. Subgroup analyses across age, gender, and\nmanufacturer variations confirmed generalizability for real-world applications.\n  Conclusion: This scalable, cost-effective solution for bone health\ndiagnostics demonstrated robust performance in a multi-site trial. It holds\npromise for widespread adoption, especially in resource-limited healthcare\nsettings, transforming bone health management and enabling proactive patient\ncare.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21800", "pdf": "https://arxiv.org/pdf/2503.21800", "abs": "https://arxiv.org/abs/2503.21800", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "ELM: Ensemble of Language Models for Predicting Tumor Group from Pathology Reports", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Population-based cancer registries (PBCRs) face a significant bottleneck in\nmanually extracting data from unstructured pathology reports, a process crucial\nfor tasks like tumor group assignment, which can consume 900 person-hours for\napproximately 100,000 reports. To address this, we introduce ELM (Ensemble of\nLanguage Models), a novel ensemble-based approach leveraging both small\nlanguage models (SLMs) and large language models (LLMs). ELM utilizes six\nfine-tuned SLMs, where three SLMs use the top part of the pathology report and\nthree SLMs use the bottom part. This is done to maximize report coverage. ELM\nrequires five-out-of-six agreement for a tumor group classification.\nDisagreements are arbitrated by an LLM with a carefully curated prompt. Our\nevaluation across nineteen tumor groups demonstrates ELM achieves an average\nprecision and recall of 0.94, outperforming single-model and\nensemble-without-LLM approaches. Deployed at the British Columbia Cancer\nRegistry, ELM demonstrates how LLMs can be successfully applied in a PBCR\nsetting to achieve state-of-the-art results and significantly enhance\noperational efficiencies, saving hundreds of person-hours annually.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21888", "pdf": "https://arxiv.org/pdf/2503.21888", "abs": "https://arxiv.org/abs/2503.21888", "authors": ["Zeyad Alghamdi", "Tharindu Kumarage", "Garima Agrawal", "Mansooreh Karami", "Ibrahim Almuteb", "Huan Liu"], "title": "RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21834", "pdf": "https://arxiv.org/pdf/2503.21834", "abs": "https://arxiv.org/abs/2503.21834", "authors": ["Haomin Yu", "Tianyi Li", "Kristian Torp", "Christian S. Jensen"], "title": "A Multi-Modal Knowledge-Enhanced Framework for Vessel Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Accurate vessel trajectory prediction facilitates improved navigational\nsafety, routing, and environmental protection. However, existing prediction\nmethods are challenged by the irregular sampling time intervals of the vessel\ntracking data from the global AIS system and the complexity of vessel movement.\nThese aspects render model learning and generalization difficult. To address\nthese challenges and improve vessel trajectory prediction, we propose the\nmulti-modal knowledge-enhanced framework (MAKER) for vessel trajectory\nprediction. To contend better with the irregular sampling time intervals, MAKER\nfeatures a Large language model-guided Knowledge Transfer (LKT) module that\nleverages pre-trained language models to transfer trajectory-specific\ncontextual knowledge effectively. To enhance the ability to learn complex\ntrajectory patterns, MAKER incorporates a Knowledge-based Self-paced Learning\n(KSL) module. This module employs kinematic knowledge to progressively\nintegrate complex patterns during training, allowing for adaptive learning and\nenhanced generalization. Experimental results on two vessel trajectory datasets\nshow that MAKER can improve the prediction accuracy of state-of-the-art methods\nby 12.08%-17.86%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21927", "pdf": "https://arxiv.org/pdf/2503.21927", "abs": "https://arxiv.org/abs/2503.21927", "authors": ["Sahan Hewage Wewelwala", "T. G. D. K. Sumanathilaka"], "title": "Hybrid Emotion Recognition: Enhancing Customer Interactions Through Acoustic and Textual Analysis", "categories": ["cs.CL"], "comment": "5 pages, 1 figure, 2 tables", "summary": "This research presents a hybrid emotion recognition system integrating\nadvanced Deep Learning, Natural Language Processing (NLP), and Large Language\nModels (LLMs) to analyze audio and textual data for enhancing customer\ninteractions in contact centers. By combining acoustic features with textual\nsentiment analysis, the system achieves nuanced emotion detection, addressing\nthe limitations of traditional approaches in understanding complex emotional\nstates. Leveraging LSTM and CNN models for audio analysis and DistilBERT for\ntextual evaluation, the methodology accommodates linguistic and cultural\nvariations while ensuring real-time processing. Rigorous testing on diverse\ndatasets demonstrates the system's robustness and accuracy, highlighting its\npotential to transform customer service by enabling personalized, empathetic\ninteractions and improving operational efficiency. This research establishes a\nfoundation for more intelligent and human-centric digital communication,\nredefining customer service standards.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21836", "pdf": "https://arxiv.org/pdf/2503.21836", "abs": "https://arxiv.org/abs/2503.21836", "authors": ["Ran Wei", "ZhiXiong Lan", "Qing Yan", "Ning Song", "Ming Lv", "LongQing Ye"], "title": "iMedImage Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "Background: Chromosome karyotype analysis is crucial for diagnosing\nhereditary diseases, yet detecting structural abnormalities remains\nchallenging. While AI has shown promise in medical imaging, its effectiveness\nvaries across modalities. Leveraging advances in Foundation Models that\nintegrate multimodal medical imaging for robust feature extraction and accurate\ndiagnosis, we developed iMedImage, an end-to-end model for general medical\nimage recognition, demonstrating strong performance across multiple imaging\ntasks, including chromosome abnormality detection. Materials and Methods: We\nconstructed a comprehensive medical image dataset encompassing multiple\nmodalities from common medical domains, including chromosome, cell, pathology,\nultrasound, X-ray, CT, and MRI images. Based on this dataset, we developed the\niMedImage model, which incorporates the following key features: (1) a unified\nrepresentation method for diverse modality inputs and medical imaging tasks;\n(2) multi-level (case-level, image-level, patch-level) image recognition\ncapabilities enhanced by Chain of Thought (CoT) embedding and Mixture of\nExperts (MoE) strategies. Results: The test set comprised data from 12\ninstitutions across six regions in China, covering three mainstream scanning\ndevices, and included naturally distributed, unscreened abnormal cases. On this\ndiverse dataset, the model achieved a fully automated chromosome analysis\nworkflow, including segmentation, karyotyping, and abnormality detection,\nreaching a sensitivity of 92.75% and a specificity of 91.5%. Conclusion: We\npropose iMedImage, an end-to-end foundation model for medical image analysis,\ndemonstrating its superior performance across various medical imaging tasks.\niMedImage provides clinicians with a precise imaging analysis tool and\ncontributes to improving diagnostic accuracy and disease screening.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21934", "pdf": "https://arxiv.org/pdf/2503.21934", "abs": "https://arxiv.org/abs/2503.21934", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Lyuben Baltadzhiev", "Maria Drencheva", "Kristian Minchev", "Mislav Balunović", "Nikola Jovanović", "Martin Vechev"], "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad", "categories": ["cs.CL"], "comment": null, "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena\nindicate that state-of-the-art reasoning models achieve impressive performance\non mathematical competitions like AIME, with the leading model, o3-mini,\nachieving scores comparable to top human competitors. However, these benchmarks\nevaluate models solely based on final numerical answers, neglecting rigorous\nreasoning and proof generation which are essential for real-world mathematical\ntasks. To address this, we introduce the first comprehensive evaluation of\nfull-solution reasoning for challenging mathematical problems. Using expert\nhuman annotators, we evaluated several state-of-the-art reasoning models on the\nsix problems from the 2025 USAMO within hours of their release. Our results\nreveal that all tested models struggled significantly, achieving less than 5%\non average. Through detailed analysis of reasoning traces, we identify the most\ncommon failure modes and find several unwanted artifacts arising from the\noptimization strategies employed during model training. Overall, our results\nsuggest that current LLMs are inadequate for rigorous mathematical reasoning\ntasks, highlighting the need for substantial improvements in reasoning and\nproof generation capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21851", "pdf": "https://arxiv.org/pdf/2503.21851", "abs": "https://arxiv.org/abs/2503.21851", "authors": ["Alessandro Conti", "Massimiliano Mancini", "Enrico Fini", "Yiming Wang", "Paolo Rota", "Elisa Ricci"], "title": "On Large Multimodal Models as Open-World Image Classifiers", "categories": ["cs.CV"], "comment": "23 pages, 13 figures, code is available at\n  https://github.com/altndrr/lmms-owc", "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22048", "pdf": "https://arxiv.org/pdf/2503.22048", "abs": "https://arxiv.org/abs/2503.22048", "authors": ["Chung-En Sun", "Ge Yan", "Tsui-Wei Weng"], "title": "ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) augmented with\nchain-of-thought (CoT) reasoning demonstrate impressive problem-solving\nabilities. However, in this work, we identify a recurring issue where these\nmodels occasionally generate overly short reasoning, leading to degraded\nperformance on even simple mathematical problems. Specifically, we investigate\nhow reasoning length is embedded in the hidden representations of reasoning\nmodels and its impact on accuracy. Our analysis reveals that reasoning length\nis governed by a linear direction in the representation space, allowing us to\ninduce overly short reasoning by steering the model along this direction.\nBuilding on this insight, we introduce ThinkEdit, a simple yet effective\nweight-editing approach to mitigate the issue of overly short reasoning. We\nfirst identify a small subset of attention heads (approximately 2%) that\npredominantly drive short reasoning behavior. We then edit the output\nprojection weights of these heads to suppress the short reasoning direction.\nWith changes to only 0.1% of the model's parameters, ThinkEdit effectively\nreduces overly short reasoning and yields notable accuracy gains for short\nreasoning outputs (+5.44%), along with an overall improvement across multiple\nmath benchmarks (+2.43%). Our findings provide new mechanistic insights into\nhow reasoning length is controlled within LLMs and highlight the potential of\nfine-grained model interventions to improve reasoning quality. Our code is\navailable at https://github.com/Trustworthy-ML-Lab/ThinkEdit", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22092", "pdf": "https://arxiv.org/pdf/2503.22092", "abs": "https://arxiv.org/abs/2503.22092", "authors": ["Dina Albassam", "Adam Cross", "Chengxiang Zhai"], "title": "Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes", "categories": ["cs.CL"], "comment": "19 pages, 3 figures, 5 tables", "summary": "Electronic Health Records (EHRs) often lack explicit links between\nmedications and diagnoses, making clinical decision-making and research more\ndifficult. Even when links exist, diagnosis lists may be incomplete, especially\nduring early patient visits. Discharge summaries tend to provide more complete\ninformation, which can help infer accurate diagnoses, especially with the help\nof large language models (LLMs). This study investigates whether LLMs can\npredict implicitly mentioned diagnoses from clinical notes and link them to\ncorresponding medications. We address two research questions: (1) Does majority\nvoting across diverse LLM configurations outperform the best single\nconfiguration in diagnosis prediction? (2) How sensitive is majority voting\naccuracy to LLM hyperparameters such as temperature, top-p, and summary length?\nTo evaluate, we created a new dataset of 240 expert-annotated\nmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran\n18 prompting configurations across short and long summary lengths, generating\n8568 test cases. Results show that majority voting achieved 75 percent\naccuracy, outperforming the best single configuration at 66 percent. No single\nhyperparameter setting dominated, but combining deterministic, balanced, and\nexploratory strategies improved performance. Shorter summaries generally led to\nhigher accuracy.In conclusion, ensemble-style majority voting with diverse LLM\nconfigurations improves diagnosis prediction in EHRs and offers a promising\nmethod to link medications and diagnoses in clinical texts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22277", "pdf": "https://arxiv.org/pdf/2503.22277", "abs": "https://arxiv.org/abs/2503.22277", "authors": ["Fabian Schmidt", "Karin Hammerfald", "Henrik Haaland Jahren", "Vladimir Vlassov"], "title": "CFiCS: Graph-Based Classification of Common Factors and Microcounseling Skills", "categories": ["cs.CL"], "comment": "10 pages, 3 figures, 2 tables", "summary": "Common factors and microcounseling skills are critical to the effectiveness\nof psychotherapy. Understanding and measuring these elements provides valuable\ninsights into therapeutic processes and outcomes. However, automatic\nidentification of these change principles from textual data remains challenging\ndue to the nuanced and context-dependent nature of therapeutic dialogue. This\npaper introduces CFiCS, a hierarchical classification framework integrating\ngraph machine learning with pretrained contextual embeddings. We represent\ncommon factors, intervention concepts, and microcounseling skills as a\nheterogeneous graph, where textual information from ClinicalBERT enriches each\nnode. This structure captures both the hierarchical relationships (e.g.,\nskill-level nodes linking to broad factors) and the semantic properties of\ntherapeutic concepts. By leveraging graph neural networks, CFiCS learns\ninductive node embeddings that generalize to unseen text samples lacking\nexplicit connections. Our results demonstrate that integrating ClinicalBERT\nnode features and graph structure significantly improves classification\nperformance, especially in fine-grained skill prediction. CFiCS achieves\nsubstantial gains in both micro and macro F1 scores across all tasks compared\nto baselines, including random forests, BERT-based multi-task models, and\ngraph-based methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22280", "pdf": "https://arxiv.org/pdf/2503.22280", "abs": "https://arxiv.org/abs/2503.22280", "authors": ["Rrubaa Panchendrarajan", "Rubén Míguez", "Arkaitz Zubiaga"], "title": "MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters", "categories": ["cs.CL"], "comment": null, "summary": "In the context of fact-checking, claims are often repeated across various\nplatforms and in different languages, which can benefit from a process that\nreduces this redundancy. While retrieving previously fact-checked claims has\nbeen investigated as a solution, the growing number of unverified claims and\nexpanding size of fact-checked databases calls for alternative, more efficient\nsolutions. A promising solution is to group claims that discuss the same\nunderlying facts into clusters to improve claim retrieval and validation.\nHowever, research on claim clustering is hindered by the lack of suitable\ndatasets. To bridge this gap, we introduce \\textit{MultiClaimNet}, a collection\nof three multilingual claim cluster datasets containing claims in 86 languages\nacross diverse topics. Claim clusters are formed automatically from\nclaim-matching pairs with limited manual intervention. We leverage two existing\nclaim-matching datasets to form the smaller datasets within\n\\textit{MultiClaimNet}. To build the larger dataset, we propose and validate an\napproach involving retrieval of approximate nearest neighbors to form candidate\nclaim pairs and an automated annotation of claim similarity using large\nlanguage models. This larger dataset contains 85.3K fact-checked claims written\nin 78 languages. We further conduct extensive experiments using various\nclustering techniques and sentence embedding models to establish baseline\nperformance. Our datasets and findings provide a strong foundation for scalable\nclaim clustering, contributing to efficient fact-checking pipelines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22303", "pdf": "https://arxiv.org/pdf/2503.22303", "abs": "https://arxiv.org/abs/2503.22303", "authors": ["Magdalena Kaiser", "Gerhard Weikum"], "title": "Preference-based Learning with Retrieval Augmented Generation for Conversational Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "WWW 2025 Short Paper, 5 pages", "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22338", "pdf": "https://arxiv.org/pdf/2503.22338", "abs": "https://arxiv.org/abs/2503.22338", "authors": ["Shrikant Malviya", "Pablo Arnau-González", "Miguel Arevalillo-Herráez", "Stamos Katsigiannis"], "title": "SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection", "categories": ["cs.CL"], "comment": "De-Factify 4.0 Workshop at the 39th AAAI Conference on Artificial\n  Intelligence (AAAI 2025)", "summary": "The rapid advancement of large language models (LLMs) has introduced new\nchallenges in distinguishing human-written text from AI-generated content. In\nthis work, we explored a pipelined approach for AI-generated text detection\nthat includes a feature extraction step (i.e. prompt-based rewriting features\ninspired by RAIDAR and content-based features derived from the NELA toolkit)\nfollowed by a classification module. Comprehensive experiments were conducted\non the Defactify4.0 dataset, evaluating two tasks: binary classification to\ndifferentiate human-written and AI-generated text, and multi-class\nclassification to identify the specific generative model used to generate the\ninput text. Our findings reveal that NELA features significantly outperform\nRAIDAR features in both tasks, demonstrating their ability to capture nuanced\nlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELA\nfeatures provided minimal improvement, highlighting the redundancy introduced\nby less discriminative features. Among the classifiers tested, XGBoost emerged\nas the most effective, leveraging the rich feature sets to achieve high\naccuracy and generalisation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22019", "pdf": "https://arxiv.org/pdf/2503.22019", "abs": "https://arxiv.org/abs/2503.22019", "authors": ["Earl Ranario", "Lars Lundqvist", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification", "categories": ["cs.CV"], "comment": null, "summary": "Semantically consistent cross-domain image translation facilitates the\ngeneration of training data by transferring labels across different domains,\nmaking it particularly useful for plant trait identification in agriculture.\nHowever, existing generative models struggle to maintain object-level accuracy\nwhen translating images between domains, especially when domain gaps are\nsignificant. In this work, we introduce AGILE (Attention-Guided Image and Label\nTranslation for Efficient Cross-Domain Plant Trait Identification), a\ndiffusion-based framework that leverages optimized text embeddings and\nattention guidance to semantically constrain image translation. AGILE utilizes\npretrained diffusion models and publicly available agricultural datasets to\nimprove the fidelity of translated images while preserving critical object\nsemantics. Our approach optimizes text embeddings to strengthen the\ncorrespondence between source and target images and guides attention maps\nduring the denoising process to control object placement. We evaluate AGILE on\ncross-domain plant datasets and demonstrate its effectiveness in generating\nsemantically accurate translated images. Quantitative experiments show that\nAGILE enhances object detection performance in the target domain while\nmaintaining realism and consistency. Compared to prior image translation\nmethods, AGILE achieves superior semantic alignment, particularly in\nchallenging cases where objects vary significantly or domain gaps are\nsubstantial.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22411", "pdf": "https://arxiv.org/pdf/2503.22411", "abs": "https://arxiv.org/abs/2503.22411", "authors": ["Petter Törnberg", "Juliana Chueri"], "title": "Elite Political Discourse has Become More Toxic in Western Countries", "categories": ["cs.CL"], "comment": null, "summary": "Toxic and uncivil politics is widely seen as a growing threat to democratic\nvalues and governance, yet our understanding of the drivers and evolution of\npolitical incivility remains limited. Leveraging a novel dataset of nearly 18\nmillion Twitter messages from parliamentarians in 17 countries over five years,\nthis paper systematically investigates whether politics internationally is\nbecoming more uncivil, and what are the determinants of political incivility.\nOur analysis reveals a marked increase in toxic discourse among political\nelites, and that it is associated to radical-right parties and parties in\nopposition. Toxicity diminished markedly during the early phase of the COVID-19\npandemic and, surprisingly, during election campaigns. Furthermore, our results\nindicate that posts relating to ``culture war'' topics, such as migration and\nLGBTQ+ rights, are substantially more toxic than debates focused on welfare or\neconomic issues. These findings underscore a troubling shift in international\ndemocracies toward an erosion of constructive democratic dialogue.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22079", "pdf": "https://arxiv.org/pdf/2503.22079", "abs": "https://arxiv.org/abs/2503.22079", "authors": ["Kunshan Yang", "Wenwei Luo", "Yuguo Hu", "Jiafu Yan", "Mengmeng Jing", "Lin Zuo"], "title": "A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible Objects Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Flexible objects recognition remains a significant challenge due to its\ninherently diverse shapes and sizes, translucent attributes, and subtle\ninter-class differences. Graph-based models, such as graph convolution networks\nand graph vision models, are promising in flexible objects recognition due to\ntheir ability of capturing variable relations within the flexible objects.\nThese methods, however, often focus on global visual relationships or fail to\nalign semantic and visual information. To alleviate these limitations, we\npropose a semantic-enhanced heterogeneous graph learning method. First, an\nadaptive scanning module is employed to extract discriminative semantic\ncontext, facilitating the matching of flexible objects with varying shapes and\nsizes while aligning semantic and visual nodes to enhance cross-modal feature\ncorrelation. Second, a heterogeneous graph generation module aggregates global\nvisual and local semantic node features, improving the recognition of flexible\nobjects. Additionally, We introduce the FSCW, a large-scale flexible dataset\ncurated from existing sources. We validate our method through extensive\nexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks\n(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22458", "pdf": "https://arxiv.org/pdf/2503.22458", "abs": "https://arxiv.org/abs/2503.22458", "authors": ["Shengyue Guan", "Haoyi Xiong", "Jindong Wang", "Jiang Bian", "Bin Zhu", "Jian-guang Lou"], "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This survey examines evaluation methods for large language model (LLM)-based\nagents in multi-turn conversational settings. Using a PRISMA-inspired\nframework, we systematically reviewed nearly 250 scholarly sources, capturing\nthe state of the art from various venues of publication, and establishing a\nsolid foundation for our analysis. Our study offers a structured approach by\ndeveloping two interrelated taxonomy systems: one that defines \\emph{what to\nevaluate} and another that explains \\emph{how to evaluate}. The first taxonomy\nidentifies key components of LLM-based agents for multi-turn conversations and\ntheir evaluation dimensions, including task completion, response quality, user\nexperience, memory and context retention, as well as planning and tool\nintegration. These components ensure that the performance of conversational\nagents is assessed in a holistic and meaningful manner. The second taxonomy\nsystem focuses on the evaluation methodologies. It categorizes approaches into\nannotation-based evaluations, automated metrics, hybrid strategies that combine\nhuman assessments with quantitative measures, and self-judging methods\nutilizing LLMs. This framework not only captures traditional metrics derived\nfrom language understanding, such as BLEU and ROUGE scores, but also\nincorporates advanced techniques that reflect the dynamic, interactive nature\nof multi-turn dialogues.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22473", "pdf": "https://arxiv.org/pdf/2503.22473", "abs": "https://arxiv.org/abs/2503.22473", "authors": ["Hanchao Liu", "Rongjun Li", "Weimin Xiong", "Ziyu Zhou", "Wei Peng"], "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 Industry Track", "summary": "Workflows play a crucial role in enhancing enterprise efficiency by\norchestrating complex processes with multiple tools or components. However,\nhand-crafted workflow construction requires expert knowledge, presenting\nsignificant technical barriers. Recent advancements in Large Language Models\n(LLMs) have improved the generation of workflows from natural language\ninstructions (aka NL2Workflow), yet existing single LLM agent-based methods\nface performance degradation on complex tasks due to the need for specialized\nknowledge and the strain of task-switching. To tackle these challenges, we\npropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,\norchestrator, and filler agent, each with distinct roles that collaboratively\nenhance the conversion process. As there are currently no publicly available\nNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which\nincludes 3,695 real-world business samples for training and evaluation.\nExperimental results show that our approach significantly increases the success\nrate of workflow construction, providing a novel and effective solution for\nenterprise NL2Workflow services.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22547", "pdf": "https://arxiv.org/pdf/2503.22547", "abs": "https://arxiv.org/abs/2503.22547", "authors": ["Zhuo-Yang Song", "Zeyu Li", "Qing-Hong Cao", "Ming-xing Luo", "Hua Xing Zhu"], "title": "Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation", "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 9 figures, 2 tables", "summary": "The geometric evolution of token representations in large language models\n(LLMs) presents a fundamental paradox: while human language inherently\norganizes semantic information in low-dimensional spaces ($\\sim 10^1$\ndimensions), modern LLMs employ high-dimensional embeddings ($\\sim 10^3$\ndimensions) processed through Transformer architectures. To resolve this\nparadox, this work bridges this conceptual gap by developing a geometric\nframework that tracks token dynamics across Transformers layers. Through\nlayer-wise analysis of intrinsic dimensions across multiple architectures, we\nreveal an expansion-contraction pattern where tokens diffuse to a \"working\nspace\" and then progressively project onto lower-dimensional submanifolds. Our\nfinding implies a negative correlation between the working space dimension and\nparameter-sensitive performance of the LLMs, and indicates that effective\nmodels tend to compress tokens into approximately 10-dimensional submanifolds,\nclosely resembling human semantic spaces. This work not only advances LLM\ninterpretability by reframing Transformers layers as projectors that mediate\nbetween high-dimensional computation and low-dimensional semantics, but also\nprovides practical tools for model diagnostics that do not rely on\ntask-specific evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "dimension"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22120", "pdf": "https://arxiv.org/pdf/2503.22120", "abs": "https://arxiv.org/abs/2503.22120", "authors": ["Protyay Dey", "Rejoy Chakraborty", "Abhilasha S. Jadhav", "Kapil Rana", "Gaurav Sharma", "Puneet Goyal"], "title": "Camera Model Identification with SPAIR-Swin and Entropy based Non-Homogeneous Patches", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 5 figures", "summary": "Source camera model identification (SCMI) plays a pivotal role in image\nforensics with applications including authenticity verification and copyright\nprotection. For identifying the camera model used to capture a given image, we\npropose SPAIR-Swin, a novel model combining a modified spatial attention\nmechanism and inverted residual block (SPAIR) with a Swin Transformer.\nSPAIR-Swin effectively captures both global and local features, enabling robust\nidentification of artifacts such as noise patterns that are particularly\neffective for SCMI. Additionally, unlike conventional methods focusing on\nhomogeneous patches, we propose a patch selection strategy for SCMI that\nemphasizes high-entropy regions rich in patterns and textures. Extensive\nevaluations on four benchmark SCMI datasets demonstrate that SPAIR-Swin\noutperforms existing methods, achieving patch-level accuracies of 99.45%,\n98.39%, 99.45%, and 97.46% and image-level accuracies of 99.87%, 99.32%, 100%,\nand 98.61% on the Dresden, Vision, Forchheim, and Socrates datasets,\nrespectively. Our findings highlight that high-entropy patches, which contain\nhigh-frequency information such as edge sharpness, noise, and compression\nartifacts, are more favorable in improving SCMI accuracy. Code will be made\navailable upon request.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22585", "pdf": "https://arxiv.org/pdf/2503.22585", "abs": "https://arxiv.org/abs/2503.22585", "authors": ["Kevin Cohen", "Laura Manrique-Gómez", "Rubén Manrique"], "title": "Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": null, "summary": "This study explores the use of large language models (LLMs) to enhance\ndatasets and improve irony detection in 19th-century Latin American newspapers.\nTwo strategies were employed to evaluate the efficacy of BERT and GPT-4o models\nin capturing the subtle nuances nature of irony, through both multi-class and\nbinary classification tasks. First, we implemented dataset enhancements focused\non enriching emotional and contextual cues; however, these showed limited\nimpact on historical language analysis. The second strategy, a semi-automated\nannotation process, effectively addressed class imbalance and augmented the\ndataset with high-quality annotations. Despite the challenges posed by the\ncomplexity of irony, this work contributes to the advancement of sentiment\nanalysis through two key contributions: introducing a new historical Spanish\ndataset tagged for sentiment analysis and irony detection, and proposing a\nsemi-automated annotation methodology where human expertise is crucial for\nrefining LLMs results, enriched by incorporating historical and cultural\ncontexts as core features.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22678", "pdf": "https://arxiv.org/pdf/2503.22678", "abs": "https://arxiv.org/abs/2503.22678", "authors": ["Mohammad Almansoori", "Komal Kumar", "Hisham Cholakkal"], "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions", "categories": ["cs.CL"], "comment": "14 page, 4 figures, 61 references", "summary": "In this work, we introduce MedAgentSim, an open-source simulated clinical\nenvironment with doctor, patient, and measurement agents designed to evaluate\nand enhance LLM performance in dynamic diagnostic settings. Unlike prior\napproaches, our framework requires doctor agents to actively engage with\npatients through multi-turn conversations, requesting relevant medical\nexaminations (e.g., temperature, blood pressure, ECG) and imaging results\n(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic\nprocess. Additionally, we incorporate self improvement mechanisms that allow\nmodels to iteratively refine their diagnostic strategies. We enhance LLM\nperformance in our simulated setting by integrating multi-agent discussions,\nchain-of-thought reasoning, and experience-based knowledge retrieval,\nfacilitating progressive learning as doctor agents interact with more patients.\nWe also introduce an evaluation benchmark for assessing the LLM's ability to\nengage in dynamic, context-aware diagnostic interactions. While MedAgentSim is\nfully automated, it also supports a user-controlled mode, enabling human\ninteraction with either the doctor or patient agent. Comprehensive evaluations\nin various simulated diagnostic scenarios demonstrate the effectiveness of our\napproach. Our code, simulation tool, and benchmark are available at\n\\href{https://medagentsim.netlify.app/}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22154", "pdf": "https://arxiv.org/pdf/2503.22154", "abs": "https://arxiv.org/abs/2503.22154", "authors": ["Jae-Young Yim", "Dongwook Kim", "Jae-Young Sim"], "title": "Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "We should collect large amount of data to train deep neural networks for\nvarious applications. Recently, the dataset distillation for images and texts\nhas been attracting a lot of attention, that reduces the original dataset to a\nsynthetic dataset while preserving essential task-relevant information.\nHowever, 3D point clouds distillation is almost unexplored due to the\nchallenges of unordered structures of points. In this paper, we propose a novel\ndistribution matching-based dataset distillation method for 3D point clouds\nthat jointly optimizes the geometric structures of synthetic dataset as well as\nthe orientations of synthetic models. To ensure the consistent feature\nalignment between different 3D point cloud models, we devise a permutation\ninvariant distribution matching loss with the sorted feature vectors. We also\nemploy learnable rotation angles to transform each syntheic model according to\nthe optimal orientation best representing the original feature distribution.\nExtensive experimental results on widely used four benchmark datasets,\nincluding ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate that\nthe proposed method consistently outperforms the existing methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21810", "pdf": "https://arxiv.org/pdf/2503.21810", "abs": "https://arxiv.org/abs/2503.21810", "authors": ["Zhenyu Wu", "Jiaoyan Chen", "Norman W. Paton"], "title": "Taxonomy Inference for Tabular Data Using Large Language Models", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Taxonomy inference for tabular data is a critical task of schema inference,\naiming at discovering entity types (i.e., concepts) of the tables and building\ntheir hierarchy. It can play an important role in data management, data\nexploration, ontology learning, and many data-centric applications. Existing\nschema inference systems focus more on XML, JSON or RDF data, and often rely on\nlexical formats and structures of the data for calculating similarities, with\nlimited exploitation of the semantics of the text across a table. Motivated by\nrecent works on taxonomy completion and construction using Large Language\nModels (LLMs), this paper presents two LLM-based methods for taxonomy inference\nfor tables: (i) EmTT which embeds columns by fine-tuning with contrastive\nlearning encoder-alone LLMs like BERT and utilises clustering for hierarchy\nconstruction, and (ii) GeTT which generates table entity types and their\nhierarchy by iterative prompting using a decoder-alone LLM like GPT-4.\nExtensive evaluation on three real-world datasets with six metrics covering\ndifferent aspects of the output taxonomies has demonstrated that EmTT and GeTT\ncan both produce taxonomies with strong consistency relative to the Ground\nTruth.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22175", "pdf": "https://arxiv.org/pdf/2503.22175", "abs": "https://arxiv.org/abs/2503.22175", "authors": ["Ruiqi Liu", "Boyu Diao", "Libo Huang", "Hangda Liu", "Chuanguang Yang", "Zhulin An", "Yongjun Xu"], "title": "Efficient Continual Learning through Frequency Decomposition and Integration", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning (CL) aims to learn new tasks while retaining past\nknowledge, addressing the challenge of forgetting during task adaptation.\nRehearsal-based methods, which replay previous samples, effectively mitigate\nforgetting. However, research on enhancing the efficiency of these methods,\nespecially in resource-constrained environments, remains limited, hindering\ntheir application in real-world systems with dynamic data streams. The human\nperceptual system processes visual scenes through complementary frequency\nchannels: low-frequency signals capture holistic cues, while high-frequency\ncomponents convey structural details vital for fine-grained discrimination.\nInspired by this, we propose the Frequency Decomposition and Integration\nNetwork (FDINet), a novel framework that decomposes and integrates information\nacross frequencies. FDINet designs two lightweight networks to independently\nprocess low- and high-frequency components of images. When integrated with\nrehearsal-based methods, this frequency-aware design effectively enhances\ncross-task generalization through low-frequency information, preserves\nclass-specific details using high-frequency information, and facilitates\nefficient training due to its lightweight architecture. Experiments demonstrate\nthat FDINet reduces backbone parameters by 78%, improves accuracy by up to\n7.49% over state-of-the-art (SOTA) methods, and decreases peak memory usage by\nup to 80%. Additionally, on edge devices, FDINet accelerates training by up to\n5$\\times$.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22180", "pdf": "https://arxiv.org/pdf/2503.22180", "abs": "https://arxiv.org/abs/2503.22180", "authors": ["Juwei Guan", "Xiaolin Fang", "Donghyun Kim", "Haotian Gong", "Tongxin Zhu", "Zhen Ling", "Ming Yang"], "title": "Knowledge Rectification for Camouflaged Object Detection: Unlocking Insights from Low-Quality Data", "categories": ["cs.CV"], "comment": null, "summary": "Low-quality data often suffer from insufficient image details, introducing an\nextra implicit aspect of camouflage that complicates camouflaged object\ndetection (COD). Existing COD methods focus primarily on high-quality data,\noverlooking the challenges posed by low-quality data, which leads to\nsignificant performance degradation. Therefore, we propose KRNet, the first\nframework explicitly designed for COD on low-quality data. KRNet presents a\nLeader-Follower framework where the Leader extracts dual gold-standard\ndistributions: conditional and hybrid, from high-quality data to drive the\nFollower in rectifying knowledge learned from low-quality data. The framework\nfurther benefits from a cross-consistency strategy that improves the\nrectification of these distributions and a time-dependent conditional encoder\nthat enriches the distribution diversity. Extensive experiments on benchmark\ndatasets demonstrate that KRNet outperforms state-of-the-art COD methods and\nsuper-resolution-assisted COD approaches, proving its effectiveness in tackling\nthe challenges of low-quality data in COD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22674", "pdf": "https://arxiv.org/pdf/2503.22674", "abs": "https://arxiv.org/abs/2503.22674", "authors": ["Belinda Z. Li", "Been Kim", "Zi Wang"], "title": "QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Code and dataset are available at\n  \\url{https://github.com/google-deepmind/questbench}", "summary": "Recently, a large amount of work has focused on improving large language\nmodels' (LLMs') performance on reasoning benchmarks such as math and logic.\nHowever, past work has largely assumed that tasks are well-defined. In the real\nworld, queries to LLMs are often underspecified, only solvable through\nacquiring missing information. We formalize this as a constraint satisfaction\nproblem (CSP) with missing variable assignments. Using a special case of this\nformalism where only one necessary variable assignment is missing, we can\nrigorously evaluate an LLM's ability to identify the minimal necessary question\nto ask and quantify axes of difficulty levels for each problem. We present\nQuestBench, a set of underspecified reasoning tasks solvable by asking at most\none question, which includes: (1) Logic-Q: Logical reasoning tasks with one\nmissing proposition, (2) Planning-Q: PDDL planning problems with initial states\nthat are partially-observed, (3) GSM-Q: Human-annotated grade school math\nproblems with one missing variable assignment, and (4) GSME-Q: a version of\nGSM-Q where word problems are translated into equations by human annotators.\nThe LLM is tasked with selecting the correct clarification question(s) from a\nlist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, their\naccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates that\nthe ability to solve well-specified reasoning problems may not be sufficient\nfor success on our benchmark: models have difficulty identifying the right\nquestion to ask, even when they can solve the fully specified version of the\nproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, even\nwhen explicitly presented with the option to predict ``not sure.'' This\nhighlights the need for deeper investigation into models' information\nacquisition capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22209", "pdf": "https://arxiv.org/pdf/2503.22209", "abs": "https://arxiv.org/abs/2503.22209", "authors": ["Wonhyeok Choi", "Kyumin Hwang", "Minwoo Choi", "Kiljoon Han", "Wonjoon Choi", "Mingyu Shin", "Sunghoon Im"], "title": "Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at AAAI 2025", "summary": "Self-supervised monocular depth estimation (SSMDE) has gained attention in\nthe field of deep learning as it estimates depth without requiring ground truth\ndepth maps. This approach typically uses a photometric consistency loss between\na synthesized image, generated from the estimated depth, and the original\nimage, thereby reducing the need for extensive dataset acquisition. However,\nthe conventional photometric consistency loss relies on the Lambertian\nassumption, which often leads to significant errors when dealing with\nreflective surfaces that deviate from this model. To address this limitation,\nwe propose a novel framework that incorporates intrinsic image decomposition\ninto SSMDE. Our method synergistically trains for both monocular depth\nestimation and intrinsic image decomposition. The accurate depth estimation\nfacilitates multi-image consistency for intrinsic image decomposition by\naligning different view coordinate systems, while the decomposition process\nidentifies reflective areas and excludes corrupted gradients from the depth\ntraining process. Furthermore, our framework introduces a pseudo-depth\ngeneration and knowledge distillation technique to further enhance the\nperformance of the student model across both reflective and non-reflective\nsurfaces. Comprehensive evaluations on multiple datasets show that our approach\nsignificantly outperforms existing SSMDE baselines in depth prediction,\nespecially on reflective surfaces.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22225", "pdf": "https://arxiv.org/pdf/2503.22225", "abs": "https://arxiv.org/abs/2503.22225", "authors": ["Haijie Yang", "Zhenyu Zhang", "Hao Tang", "Jianjun Qian", "Jian Yang"], "title": "Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance", "categories": ["cs.CV"], "comment": "https://anonymous-hub1127.github.io/FYM.github.io/", "summary": "Pre-trained conditional diffusion models have demonstrated remarkable\npotential in image editing. However, they often face challenges with temporal\nconsistency, particularly in the talking head domain, where continuous changes\nin facial expressions intensify the level of difficulty. These issues stem from\nthe independent editing of individual images and the inherent loss of temporal\ncontinuity during the editing process. In this paper, we introduce Follow Your\nMotion (FYM), a generic framework for maintaining temporal consistency in\nportrait editing. Specifically, given portrait images rendered by a pre-trained\n3D Gaussian Splatting model, we first develop a diffusion model that\nintuitively and inherently learns motion trajectory changes at different scales\nand pixel coordinates, from the first frame to each subsequent frame. This\napproach ensures that temporally inconsistent edited avatars inherit the motion\ninformation from the rendered avatars. Secondly, to maintain fine-grained\nexpression temporal consistency in talking head editing, we propose a dynamic\nre-weighted attention mechanism. This mechanism assigns higher weight\ncoefficients to landmark points in space and dynamically updates these weights\nbased on landmark loss, achieving more consistent and refined facial\nexpressions. Extensive experiments demonstrate that our method outperforms\nexisting approaches in terms of temporal consistency and can be used to\noptimize and compensate for temporally inconsistent outputs in a range of\napplications, such as text-driven editing, relighting, and various other\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22231", "pdf": "https://arxiv.org/pdf/2503.22231", "abs": "https://arxiv.org/abs/2503.22231", "authors": ["Yishen Ji", "Ziyue Zhu", "Zhenxin Zhu", "Kaixin Xiong", "Ming Lu", "Zhiqi Li", "Lijun Zhou", "Haiyang Sun", "Bing Wang", "Tong Lu"], "title": "CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in driving video generation has shown significant potential\nfor enhancing self-driving systems by providing scalable and controllable\ntraining data. Although pretrained state-of-the-art generation models, guided\nby 2D layout conditions (e.g., HD maps and bounding boxes), can produce\nphotorealistic driving videos, achieving controllable multi-view videos with\nhigh 3D consistency remains a major challenge. To tackle this, we introduce a\nnovel spatial adaptive generation framework, CoGen, which leverages advances in\n3D generation to improve performance in two key aspects: (i) To ensure 3D\nconsistency, we first generate high-quality, controllable 3D conditions that\ncapture the geometry of driving scenes. By replacing coarse 2D conditions with\nthese fine-grained 3D representations, our approach significantly enhances the\nspatial consistency of the generated videos. (ii) Additionally, we introduce a\nconsistency adapter module to strengthen the robustness of the model to\nmulti-condition control. The results demonstrate that this method excels in\npreserving geometric fidelity and visual realism, offering a reliable video\ngeneration solution for autonomous driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22265", "pdf": "https://arxiv.org/pdf/2503.22265", "abs": "https://arxiv.org/abs/2503.22265", "authors": ["Haomin Zhang", "Chang Liu", "Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 5 figures", "summary": "Currently, high-quality, synchronized audio is synthesized using various\nmulti-modal joint learning frameworks, leveraging video and optional text\ninputs. In the video-to-audio benchmarks, video-to-audio quality, semantic\nalignment, and audio-visual synchronization are effectively achieved. However,\nin real-world scenarios, speech and audio often coexist in videos\nsimultaneously, and the end-to-end generation of synchronous speech and audio\ngiven video and text conditions are not well studied. Therefore, we propose an\nend-to-end multi-modal generation framework that simultaneously produces speech\nand audio based on video and text conditions. Furthermore, the advantages of\nvideo-to-audio (V2A) models for generating speech from videos remain unclear.\nThe proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a\ntext-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF)\nmodule. In the evaluation, the proposed end-to-end framework achieves\nstate-of-the-art performance on the video-audio benchmark, video-speech\nbenchmark, and text-speech benchmark. In detail, our framework achieves\ncomparable results in the comparison with state-of-the-art models for the\nvideo-audio and text-speech benchmarks, and surpassing state-of-the-art models\nin the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM\n78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to\n7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing\nsettings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22346", "pdf": "https://arxiv.org/pdf/2503.22346", "abs": "https://arxiv.org/abs/2503.22346", "authors": ["Ruifeng Luo", "Zhengjie Liu", "Tianxiao Cheng", "Jie Wang", "Tongjie Wang", "Xingguang Wei", "Haomin Wang", "YanPeng Li", "Fu Chai", "Fei Cheng", "Shenglong Ye", "Wenhai Wang", "Yanting Zhang", "Yu Qiao", "Hongjie Zhang", "Xianzhong Zhao"], "title": "ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting", "categories": ["cs.CV"], "comment": null, "summary": "Recognizing symbols in architectural CAD drawings is critical for various\nadvanced engineering applications. In this paper, we propose a novel CAD data\nannotation engine that leverages intrinsic attributes from systematically\narchived CAD drawings to automatically generate high-quality annotations, thus\nsignificantly reducing manual labeling efforts. Utilizing this engine, we\nconstruct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks\nfrom 5538 highly standardized drawings, making it over 26 times larger than the\nlargest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversity\nand broader categories, offering line-grained annotations. Furthermore, we\npresent a new baseline model for panoptic symbol spotting, termed Dual-Pathway\nSymbol Spotter (DPSS). It incorporates an adaptive fusion module to enhance\nprimitive features with complementary image features, achieving\nstate-of-the-art performance and enhanced robustness. Extensive experiments\nvalidate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K and\nits potential to drive innovation in architectural design and construction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22352", "pdf": "https://arxiv.org/pdf/2503.22352", "abs": "https://arxiv.org/abs/2503.22352", "authors": ["Barış Batuhan Topal", "Umut Özyurt", "Zafer Doğan Budak", "Ramazan Gokberk Cinbis"], "title": "Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-image generative models, particularly latent\ndiffusion models (LDMs), have demonstrated remarkable capabilities in\nsynthesizing high-quality images from textual prompts. However, achieving\nidentity personalization-ensuring that a model consistently generates\nsubject-specific outputs from limited reference images-remains a fundamental\nchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),\na novel framework that leverages meta-learning to encode domain-specific priors\ninto LoRA-based identity personalization. Our method introduces a structured\nthree-layer LoRA architecture that separates identity-agnostic knowledge from\nidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers are\nmeta-trained across multiple subjects, learning a shared manifold that captures\ngeneral identity-related features. In the second stage, only the LoRA-Mid and\nLoRA-Up layers are optimized to specialize on a given subject, significantly\nreducing adaptation time while improving identity fidelity. To evaluate our\napproach, we introduce Meta-PHD, a new benchmark dataset for identity\npersonalization, and compare Meta-LoRA against state-of-the-art methods. Our\nresults demonstrate that Meta-LoRA achieves superior identity retention,\ncomputational efficiency, and adaptability across diverse identity conditions.\nThe code, model weights, and dataset will be released publicly upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22363", "pdf": "https://arxiv.org/pdf/2503.22363", "abs": "https://arxiv.org/abs/2503.22363", "authors": ["Nandakishor M", "Vrinda Govind V", "Anuradha Puthalath", "Anzy L", "Swathi P S", "Aswathi R", "Devaprabha A R", "Varsha Raj", "Midhuna Krishnan K", "Akhila Anilkumar T V", "Yamuna P V"], "title": "ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Force estimation in human-object interactions is crucial for various fields\nlike ergonomics, physical therapy, and sports science. Traditional methods\ndepend on specialized equipment such as force plates and sensors, which makes\naccurate assessments both expensive and restricted to laboratory settings. In\nthis paper, we introduce ForcePose, a novel deep learning framework that\nestimates applied forces by combining human pose estimation with object\ndetection. Our approach leverages MediaPipe for skeletal tracking and SSD\nMobileNet for object recognition to create a unified representation of\nhuman-object interaction. We've developed a specialized neural network that\nprocesses both spatial and temporal features to predict force magnitude and\ndirection without needing any physical sensors. After training on our dataset\nof 850 annotated videos with corresponding force measurements, our model\nachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees in\nforce direction. When compared to existing computer vision approaches, our\nmethod performs 27.5% better while still offering real-time performance on\nstandard computing hardware. ForcePose opens up new possibilities for force\nanalysis in diverse real-world scenarios where traditional measurement tools\nare impractical or intrusive. This paper discusses our methodology, the dataset\ncreation process, evaluation metrics, and potential applications across\nrehabilitation, ergonomics assessment, and athletic performance analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22397", "pdf": "https://arxiv.org/pdf/2503.22397", "abs": "https://arxiv.org/abs/2503.22397", "authors": ["Vida Adeli", "Soroush Mehraban", "Majid Mirmehdi", "Alan Whone", "Benjamin Filtjens", "Amirhossein Dadashzadeh", "Alfonso Fasano", "Andrea Iaboni Babak Taati"], "title": "GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain", "categories": ["cs.CV"], "comment": null, "summary": "Gait analysis is crucial for the diagnosis and monitoring of movement\ndisorders like Parkinson's Disease. While computer vision models have shown\npotential for objectively evaluating parkinsonian gait, their effectiveness is\nlimited by scarce clinical datasets and the challenge of collecting large and\nwell-labelled data, impacting model accuracy and risk of bias. To address these\ngaps, we propose GAITGen, a novel framework that generates realistic gait\nsequences conditioned on specified pathology severity levels. GAITGen employs a\nConditional Residual Vector Quantized Variational Autoencoder to learn\ndisentangled representations of motion dynamics and pathology-specific factors,\ncoupled with Mask and Residual Transformers for conditioned sequence\ngeneration. GAITGen generates realistic, diverse gait sequences across severity\nlevels, enriching datasets and enabling large-scale model training in\nparkinsonian gait analysis. Experiments on our new PD-GaM (real) dataset\ndemonstrate that GAITGen outperforms adapted state-of-the-art models in both\nreconstruction fidelity and generation quality, accurately capturing critical\npathology-specific gait features. A clinical user study confirms the realism\nand clinical relevance of our generated sequences. Moreover, incorporating\nGAITGen-generated data into downstream tasks improves parkinsonian gait\nseverity estimation, highlighting its potential for advancing clinical gait\nanalysis.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22436", "pdf": "https://arxiv.org/pdf/2503.22436", "abs": "https://arxiv.org/abs/2503.22436", "authors": ["Fuhao Li", "Huan Jin", "Bin Gao", "Liaoyuan Fan", "Lihui Jiang", "Long Zeng"], "title": "NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view 3D visual grounding is critical for autonomous driving vehicles to\ninterpret natural languages and localize target objects in complex\nenvironments. However, existing datasets and methods suffer from coarse-grained\nlanguage instructions, and inadequate integration of 3D geometric reasoning\nwith linguistic comprehension. To this end, we introduce NuGrounding, the first\nlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.\nWe present a Hierarchy of Grounding (HoG) method to construct NuGrounding to\ngenerate hierarchical multi-level instructions, ensuring comprehensive coverage\nof human instruction patterns. To tackle this challenging dataset, we propose a\nnovel paradigm that seamlessly combines instruction comprehension abilities of\nmulti-modal LLMs (MLLMs) with precise localization abilities of specialist\ndetection models. Our approach introduces two decoupled task tokens and a\ncontext query to aggregate 3D geometric information and semantic instructions,\nfollowed by a fusion decoder to refine spatial-semantic feature fusion for\nprecise localization. Extensive experiments demonstrate that our method\nsignificantly outperforms the baselines adapted from representative 3D scene\nunderstanding methods by a significant margin and achieves 0.59 in precision\nand 0.64 in recall, with improvements of 50.8% and 54.7%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22462", "pdf": "https://arxiv.org/pdf/2503.22462", "abs": "https://arxiv.org/abs/2503.22462", "authors": ["Krispin Wandel", "Hesheng Wang"], "title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Poster:\n  https://cvpr.thecvf.com/virtual/2025/poster/32799", "summary": "Semantic correspondence made tremendous progress through the recent\nadvancements of large vision models (LVM). While these LVMs have been shown to\nreliably capture local semantics, the same can currently not be said for\ncapturing global geometric relationships between semantic object regions. This\nproblem leads to unreliable performance for semantic correspondence between\nimages with extreme view variation. In this work, we aim to leverage monocular\ndepth estimates to capture these geometric relationships for more robust and\ndata-efficient semantic correspondence. First, we introduce a simple but\neffective method to build 3D object-class representations from monocular depth\nestimates and LVM features using a sparsely annotated image correspondence\ndataset. Second, we formulate an alignment energy that can be minimized using\ngradient descent to obtain an alignment between the 3D object-class\nrepresentation and the object-class instance in the input RGB-image. Our method\nachieves state-of-the-art matching accuracy in multiple categories on the\nchallenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10\npoints on three categories and overall by 3.3 points from 85.6% to 88.9%.\nAdditional resources and code are available at https://dub.sh/semalign3d.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21829", "pdf": "https://arxiv.org/pdf/2503.21829", "abs": "https://arxiv.org/abs/2503.21829", "authors": ["Ivan Diaz", "Florin Scherer", "Yanik Berli", "Roland Wiest", "Helly Hammer", "Robert Hoepner", "Alejandro Leon Betancourt", "Piotr Radojewski", "Richard McKinley"], "title": "Learning from spatially inhomogenous data: resolution-adaptive convolutions for multiple sclerosis lesion segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In the setting of clinical imaging, differences in between vendors, hospitals\nand sequences can yield highly inhomogeneous imaging data. In MRI in\nparticular, voxel dimension, slice spacing and acquisition plane can vary\nsubstantially. For clinical applications, therefore, algorithms must be trained\nto handle data with various voxel resolutions. The usual strategy to deal with\nheterogeneity of resolution is harmonization: resampling imaging data to a\ncommon (usually isovoxel) resolution. This can lead to loss of fidelity arising\nfrom interpolation artifacts out-of-plane and downsampling in-plane. We present\nin this paper a network architecture designed to be able to learn directly from\nspatially heterogeneous data, without resampling: a segmentation network based\non the e3nn framework that leverages a spherical harmonic, rather than\nvoxel-grid, parameterization of convolutional kernels, with a fixed physical\nradius. Networks based on these kernels can be resampled to their input voxel\ndimensions. We trained and tested our network on a publicly available dataset\nassembled from three centres, and on an in-house dataset of Multiple Sclerosis\ncases with a high degree of spatial inhomogeneity. We compared our approach to\na standard U-Net with two strategies for handling inhomogeneous data: training\ndirectly on the data without resampling, and resampling to a common resolution\nof 1mm isovoxels. We show that our network is able to learn from various\ncombinations of voxel sizes and outperforms classical U-Nets on 2D testing\ncases and most 3D testing cases. This shows an ability to generalize well when\ntested on image resolutions not seen during training. Our code can be found at:\nhttp://github.com/SCAN-NRAD/e3nn\\_U-Net.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dimension"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22140", "pdf": "https://arxiv.org/pdf/2503.22140", "abs": "https://arxiv.org/abs/2503.22140", "authors": ["Chang Cai", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Image Recovery", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": null, "summary": "Message passing algorithms have been tailored for compressive imaging\napplications by plugging in different types of off-the-shelf image denoisers.\nThese off-the-shelf denoisers mostly rely on some generic or hand-crafted\npriors for denoising. Due to their insufficient accuracy in capturing the true\nimage prior, these methods often fail to produce satisfactory results,\nespecially in largely underdetermined scenarios. On the other hand, score-based\ngenerative modeling offers a promising way to accurately characterize the\nsophisticated image distribution. In this paper, by exploiting the close\nrelation between score-based modeling and empirical Bayes-optimal denoising, we\ndevise a message passing framework that integrates a score-based minimum mean\nsquared error (MMSE) denoiser for compressive image recovery. This framework is\nfirmly rooted in Bayesian formalism, in which state evolution (SE) equations\naccurately predict its asymptotic performance. Experiments on the FFHQ dataset\ndemonstrate that our method strikes a significantly better\nperformance-complexity tradeoff than conventional message passing, regularized\nlinear regression, and score-based posterior sampling baselines. Remarkably,\nour method typically requires less than 20 neural function evaluations (NFEs)\nto converge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22143", "pdf": "https://arxiv.org/pdf/2503.22143", "abs": "https://arxiv.org/abs/2503.22143", "authors": ["Sungyu Jeong", "Won Joon Choi", "Junung Choi", "Anik Biswas", "Byungsub Kim"], "title": "A Self-Supervised Learning of a Foundation Model for Analog Layout Design Automation", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.LG"], "comment": "8 pages, 11 figures", "summary": "We propose a UNet-based foundation model and its self-supervised learning\nmethod to address two key challenges: 1) lack of qualified annotated analog\nlayout data, and 2) excessive variety in analog layout design tasks. For\nself-supervised learning, we propose random patch sampling and random masking\ntechniques automatically to obtain enough training data from a small\nunannotated layout dataset. The obtained data are greatly augmented, less\nbiased, equally sized, and contain enough information for excessive varieties\nof qualified layout patterns. By pre-training with the obtained data, the\nproposed foundation model can learn implicit general knowledge on layout\npatterns so that it can be fine-tuned for various downstream layout tasks with\nsmall task-specific datasets. Fine-tuning provides an efficient and\nconsolidated methodology for diverse downstream tasks, reducing the enormous\nhuman effort to develop a model per task separately. In experiments, the\nfoundation model was pre-trained using 324,000 samples obtained from 6\nsilicon-proved manually designed analog circuits, then it was fine-tuned for\nthe five example downstream tasks: generating contacts, vias, dummy fingers,\nN-wells, and metal routings. The fine-tuned models successfully performed these\ntasks for more than one thousand unseen layout inputs, generating DRC/LVS-clean\nlayouts for 96.6% of samples. Compared with training the model from scratch for\nthe metal routing task, fine-tuning required only 1/8 of the data to achieve\nthe same dice score of 0.95. With the same data, fine-tuning achieved a 90%\nlower validation loss and a 40% higher benchmark score than training from\nscratch.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22200", "pdf": "https://arxiv.org/pdf/2503.22200", "abs": "https://arxiv.org/abs/2503.22200", "authors": ["Haomin Zhang", "Sizhe Shan", "Haoyu Wang", "Zihao Chen", "Xiulong Liu", "Chaofan Ding", "Xinhan Di"], "title": "Enhance Generation Quality of Flow Matching V2A Model via Multi-Step CoT-Like Guidance and Combined Preference Optimization", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "10 pages, 4 figures", "summary": "Creating high-quality sound effects from videos and text prompts requires\nprecise alignment between visual and audio domains, both semantically and\ntemporally, along with step-by-step guidance for professional audio generation.\nHowever, current state-of-the-art video-guided audio generation models often\nfall short of producing high-quality audio for both general and specialized use\ncases. To address this challenge, we introduce a multi-stage, multi-modal,\nend-to-end generative framework with Chain-of-Thought-like (CoT-like) guidance\nlearning, termed Chain-of-Perform (CoP). First, we employ a transformer-based\nnetwork architecture designed to achieve CoP guidance, enabling the generation\nof both general and professional audio. Second, we implement a multi-stage\ntraining framework that follows step-by-step guidance to ensure the generation\nof high-quality sound effects. Third, we develop a CoP multi-modal dataset,\nguided by video, to support step-by-step sound effects generation. Evaluation\nresults highlight the advantages of the proposed multi-stage CoP generative\nframework compared to the state-of-the-art models on a variety of datasets,\nwith FAD 0.79 to 0.74 (+6.33%), CLIP 16.12 to 17.70 (+9.80%) on VGGSound,\nSI-SDR 1.98dB to 3.35dB (+69.19%), MOS 2.94 to 3.49(+18.71%) on PianoYT-2h, and\nSI-SDR 2.22dB to 3.21dB (+44.59%), MOS 3.07 to 3.42 (+11.40%) on Piano-10h.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22208", "pdf": "https://arxiv.org/pdf/2503.22208", "abs": "https://arxiv.org/abs/2503.22208", "authors": ["Yunming Liang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from Videos", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "11 pages, 6 figures", "summary": "Currently, high-quality, synchronized audio is synthesized from video and\noptional text inputs using various multi-modal joint learning frameworks.\nHowever, the precise alignment between the visual and generated audio domains\nremains far from satisfactory. One key factor is the lack of sufficient\ntemporal and semantic alignment annotations in open-source video-audio and\ntext-audio benchmarks. Therefore, we propose a framework for audio generation\nfrom videos, leveraging the internal chain-of-thought (CoT) of a multi-modal\nlarge language model (MLLM) to enable step-by-step reasoning without requiring\nadditional annotations. Additionally, a corresponding multi-modal reasoning\ndataset is constructed to facilitate the learning of initial reasoning in audio\ngeneration. In the experiments, we demonstrate the effectiveness of the\nproposed framework in reducing misalignment (voice-over) in generated audio and\nachieving competitive performance compared to various state-of-the-art models.\nThe evaluation results show that the proposed method outperforms\nstate-of-the-art approaches across multiple metrics. Specifically, the F DP\naSST indicator is reduced by up to 10.07%, the F DP AN N s indicator by up to\n11.62%, and the F DV GG indicator by up to 38.61%. Furthermore, the IS\nindicator improves by up to 4.95%, the IB-score indicator increases by up to\n6.39%, and the DeSync indicator is reduced by up to 0.89%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22236", "pdf": "https://arxiv.org/pdf/2503.22236", "abs": "https://arxiv.org/abs/2503.22236", "authors": ["Chongjie Ye", "Yushuang Wu", "Ziteng Lu", "Jiahao Chang", "Xiaoyang Guo", "Jiaqing Zhou", "Hao Zhao", "Xiaoguang Han"], "title": "Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging", "categories": ["cs.GR", "cs.CV"], "comment": "https://stable-x.github.io/Hi3DGen/static", "summary": "With the growing demand for high-fidelity 3D models from 2D images, existing\nmethods still face significant challenges in accurately reproducing\nfine-grained geometric details due to limitations in domain gaps and inherent\nambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel\nframework for generating high-fidelity 3D geometry from images via normal\nbridging. Hi3DGen consists of three key components: (1) an image-to-normal\nestimator that decouples the low-high frequency image pattern with noise\ninjection and dual-stream training to achieve generalizable, stable, and sharp\nestimation; (2) a normal-to-geometry learning approach that uses\nnormal-regularized latent diffusion learning to enhance 3D geometry generation\nfidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality\ndataset to support training. Extensive experiments demonstrate the\neffectiveness and superiority of our framework in generating rich geometric\ndetails, outperforming state-of-the-art methods in terms of fidelity. Our work\nprovides a new direction for high-fidelity 3D geometry generation from images\nby leveraging normal maps as an intermediate representation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22589", "pdf": "https://arxiv.org/pdf/2503.22589", "abs": "https://arxiv.org/abs/2503.22589", "authors": ["Adam Breuer", "Bryce J. Dietrich", "Michael H. Crespin", "Matthew Butler", "J. A. Pyrse", "Kosuke Imai"], "title": "Using AI to Summarize US Presidential Campaign TV Advertisement Videos, 1952-2012", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 7 tables, 4 figures, and linked datasets", "summary": "This paper introduces the largest and most comprehensive dataset of US\npresidential campaign television advertisements, available in digital format.\nThe dataset also includes machine-searchable transcripts and high-quality\nsummaries designed to facilitate a variety of academic research. To date, there\nhas been great interest in collecting and analyzing US presidential campaign\nadvertisements, but the need for manual procurement and annotation led many to\nrely on smaller subsets. We design a large-scale parallelized, AI-based\nanalysis pipeline that automates the laborious process of preparing,\ntranscribing, and summarizing videos. We then apply this methodology to the\n9,707 presidential ads from the Julian P. Kanter Political Commercial Archive.\nWe conduct extensive human evaluations to show that these transcripts and\nsummaries match the quality of manually generated alternatives. We illustrate\nthe value of this data by including an application that tracks the genesis and\nevolution of current focal issue areas over seven decades of presidential\nelections. Our analysis pipeline and codebase also show how to use LLM-based\ntools to obtain high-quality summaries for other video datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21806", "pdf": "https://arxiv.org/pdf/2503.21806", "abs": "https://arxiv.org/abs/2503.21806", "authors": ["Heqing Zou", "Fengmao Lv", "Desheng Zheng", "Eng Siong Chng", "Deepu Rajan"], "title": "Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICME 2025", "summary": "Multilingual speech emotion recognition aims to estimate a speaker's\nemotional state using a contactless method across different languages. However,\nvariability in voice characteristics and linguistic diversity poses significant\nchallenges for zero-shot speech emotion recognition, especially with\nmultilingual datasets. In this paper, we propose leveraging contrastive\nlearning to refine multilingual speech features and extend large language\nmodels for zero-shot multilingual speech emotion estimation. Specifically, we\nemploy a novel two-stage training framework to align speech signals with\nlinguistic features in the emotional space, capturing both emotion-aware and\nlanguage-agnostic speech representations. To advance research in this field, we\nintroduce a large-scale synthetic multilingual speech emotion dataset, M5SER.\nOur experiments demonstrate the effectiveness of the proposed method in both\nspeech emotion recognition and zero-shot multilingual speech emotion\nrecognition, including previously unseen datasets and languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21817", "pdf": "https://arxiv.org/pdf/2503.21817", "abs": "https://arxiv.org/abs/2503.21817", "authors": ["Weili Zeng", "Ziyuan Huang", "Kaixiang Ji", "Yichao Yan"], "title": "Skip-Vision: A Comprehensive Framework for Accelerating Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21833", "pdf": "https://arxiv.org/pdf/2503.21833", "abs": "https://arxiv.org/abs/2503.21833", "authors": ["Alan Yang", "Yulin Chen", "Sean Lee", "Venus Montes"], "title": "Refining Time Series Anomaly Detectors using Large Language Models", "categories": ["cs.CL"], "comment": "Main content: 4 pages, 1 figure, 1 table", "summary": "Time series anomaly detection (TSAD) is of widespread interest across many\nindustries, including finance, healthcare, and manufacturing. Despite the\ndevelopment of numerous automatic methods for detecting anomalies, human\noversight remains necessary to review and act upon detected anomalies, as well\nas verify their accuracy. We study the use of multimodal large language models\n(LLMs) to partially automate this process. We find that LLMs can effectively\nidentify false alarms by integrating visual inspection of time series plots\nwith text descriptions of the data-generating process. By leveraging the\ncapabilities of LLMs, we aim to reduce the reliance on human effort required to\nmaintain a TSAD system", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21838", "pdf": "https://arxiv.org/pdf/2503.21838", "abs": "https://arxiv.org/abs/2503.21838", "authors": ["Jiancheng Zhao", "Xingda Yu", "Zhen Yang"], "title": "MSPLoRA: A Multi-Scale Pyramid Low-Rank Adaptation for Efficient Model Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) has become an essential approach for\nadapting large-scale pre-trained models while reducing computational costs.\nAmong PEFT methods, LoRA significantly reduces trainable parameters by\ndecomposing weight updates into low-rank matrices. However, traditional LoRA\napplies a fixed rank across all layers, failing to account for the varying\ncomplexity of hierarchical information, which leads to inefficient adaptation\nand redundancy. To address this, we propose MSPLoRA (Multi-Scale Pyramid LoRA),\nwhich introduces Global Shared LoRA, Mid-Level Shared LoRA, and Layer-Specific\nLoRA to capture global patterns, mid-level features, and fine-grained\ninformation, respectively. This hierarchical structure reduces inter-layer\nredundancy while maintaining strong adaptation capability. Experiments on\nvarious NLP tasks demonstrate that MSPLoRA achieves more efficient adaptation\nand better performance while significantly reducing the number of trainable\nparameters. Furthermore, additional analyses based on Singular Value\nDecomposition validate its information decoupling ability, highlighting MSPLoRA\nas a scalable and effective optimization strategy for parameter-efficient\nfine-tuning in large language models. Our code is available at\nhttps://github.com/Oblivioniss/MSPLoRA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21824", "pdf": "https://arxiv.org/pdf/2503.21824", "abs": "https://arxiv.org/abs/2503.21824", "authors": ["Haitong Liu", "Kuofeng Gao", "Yang Bai", "Jinmin Li", "Jinxiao Shan", "Tao Dai", "Shu-Tao Xia"], "title": "Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations", "categories": ["cs.CV", "cs.CR"], "comment": "Accepted by CVPR 2025", "summary": "Recently, video-based large language models (video-based LLMs) have achieved\nimpressive performance across various video comprehension tasks. However, this\nrapid advancement raises significant privacy and security concerns,\nparticularly regarding the unauthorized use of personal video data in automated\nannotation by video-based LLMs. These unauthorized annotated video-text pairs\ncan then be used to improve the performance of downstream tasks, such as\ntext-to-video generation. To safeguard personal videos from unauthorized use,\nwe propose two series of protective video watermarks with imperceptible\nadversarial perturbations, named Ramblings and Mutes. Concretely, Ramblings aim\nto mislead video-based LLMs into generating inaccurate captions for the videos,\nthereby degrading the quality of video annotations through inconsistencies\nbetween video content and captions. Mutes, on the other hand, are designed to\nprompt video-based LLMs to produce exceptionally brief captions, lacking\ndescriptive detail. Extensive experiments demonstrate that our video\nwatermarking methods effectively protect video data by significantly reducing\nvideo annotation performance across various video-based LLMs, showcasing both\nstealthiness and robustness in protecting personal video content. Our code is\navailable at https://github.com/ttthhl/Protecting_Your_Video_Content.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21830", "pdf": "https://arxiv.org/pdf/2503.21830", "abs": "https://arxiv.org/abs/2503.21830", "authors": ["Maximilian Plattner", "Arturs Berzins", "Johannes Brandstetter"], "title": "Shape Generation via Weight Space Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Foundation models for 3D shape generation have recently shown a remarkable\ncapacity to encode rich geometric priors across both global and local\ndimensions. However, leveraging these priors for downstream tasks can be\nchallenging as real-world data are often scarce or noisy, and traditional\nfine-tuning can lead to catastrophic forgetting. In this work, we treat the\nweight space of a large 3D shape-generative model as a data modality that can\nbe explored directly. We hypothesize that submanifolds within this\nhigh-dimensional weight space can modulate topological properties or\nfine-grained part features separately, demonstrating early-stage evidence via\ntwo experiments. First, we observe a sharp phase transition in global\nconnectivity when interpolating in conditioning space, suggesting that small\nchanges in weight space can drastically alter topology. Second, we show that\nlow-dimensional reparameterizations yield controlled local geometry changes\neven with very limited data. These results highlight the potential of weight\nspace learning to unlock new approaches for 3D shape generation and specialized\nfine-tuning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21911", "pdf": "https://arxiv.org/pdf/2503.21911", "abs": "https://arxiv.org/abs/2503.21911", "authors": ["Sayed Muddashir Hossain", "Simon Ostermann", "Patrick Gebhard", "Cord Benecke", "Josef van Genabith", "Philipp Müller"], "title": "AutoPsyC: Automatic Recognition of Psychodynamic Conflicts from Semi-structured Interviews with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Psychodynamic conflicts are persistent, often unconscious themes that shape a\nperson's behaviour and experiences. Accurate diagnosis of psychodynamic\nconflicts is crucial for effective patient treatment and is commonly done via\nlong, manually scored semi-structured interviews. Existing automated solutions\nfor psychiatric diagnosis tend to focus on the recognition of broad disorder\ncategories such as depression, and it is unclear to what extent psychodynamic\nconflicts which even the patient themselves may not have conscious access to\ncould be automatically recognised from conversation. In this paper, we propose\nAutoPsyC, the first method for recognising the presence and significance of\npsychodynamic conflicts from full-length Operationalized Psychodynamic\nDiagnostics (OPD) interviews using Large Language Models (LLMs). Our approach\ncombines recent advances in parameter-efficient fine-tuning and\nRetrieval-Augmented Generation (RAG) with a summarisation strategy to\neffectively process entire 90 minute long conversations. In evaluations on a\ndataset of 141 diagnostic interviews we show that AutoPsyC consistently\noutperforms all baselines and ablation conditions on the recognition of four\nhighly relevant psychodynamic conflicts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21841", "pdf": "https://arxiv.org/pdf/2503.21841", "abs": "https://arxiv.org/abs/2503.21841", "authors": ["Jingtao Li", "Yingyi Liu", "Xinyu Wang", "Yunning Peng", "Chen Sun", "Shaoyu Wang", "Zhendong Sun", "Tian Ke", "Xiao Jiang", "Tangwei Lu", "Anran Zhao", "Yanfei Zhong"], "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Advanced interpretation of hyperspectral remote sensing images benefits many\nprecise Earth observation tasks. Recently, visual foundation models have\npromoted the remote sensing interpretation but concentrating on RGB and\nmultispectral images. Due to the varied hyperspectral channels,existing\nfoundation models would face image-by-image tuning situation, imposing great\npressure on hardware and time resources. In this paper, we propose a\ntuning-free hyperspectral foundation model called HyperFree, by adapting the\nexisting visual prompt engineering. To process varied channel numbers, we\ndesign a learned weight dictionary covering full-spectrum from $0.4 \\sim 2.5 \\,\n\\mu\\text{m}$, supporting to build the embedding layer dynamically. To make the\nprompt design more tractable, HyperFree can generate multiple semantic-aware\nmasks for one prompt by treating feature distance as semantic-similarity. After\npre-training HyperFree on constructed large-scale high-resolution hyperspectral\nimages, HyperFree (1 prompt) has shown comparable results with specialized\nmodels (5 shots) on 5 tasks and 11 datasets.Code and dataset are accessible at\nhttps://rsidea.whu.edu.cn/hyperfree.htm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21961", "pdf": "https://arxiv.org/pdf/2503.21961", "abs": "https://arxiv.org/abs/2503.21961", "authors": ["Xianzhi Li", "Ethan Callanan", "Xiaodan Zhu", "Mathieu Sibue", "Antony Papadimitriou", "Mahmoud Mahfouz", "Zhiqiang Ma", "Xiaomo Liu"], "title": "Entropy-Aware Branching for Improved Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) are effectively aligned through extensive\npre-training and fine-tuning, they still struggle with varying levels of\nuncertainty during token generation. In our investigation of mathematical\nreasoning, we observe that errors are more likely to arise at tokens exhibiting\nhigh entropy and variance of entropy in the model's output distribution. Based\non the observation, we propose a novel approach that dynamically branches the\ngeneration process on demand instead of defaulting to the single most probable\ntoken. By exploring in parallel multiple branches stemming from high\nprobability tokens of critical decision points, the model can discover diverse\nreasoning paths that might otherwise be missed. We further harness external\nfeedback from larger models to rank and select the most coherent and accurate\nreasoning branch. Our experimental results on mathematical word problems and\ncalculation questions show that this branching strategy boosts the reasoning\ncapabilities of small LLMs up to 4.6% compared to conventional argmax decoding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22006", "pdf": "https://arxiv.org/pdf/2503.22006", "abs": "https://arxiv.org/abs/2503.22006", "authors": ["Marc Brinner", "Tarek Al Mustafa", "Sina Zarrieß"], "title": "Enhancing Domain-Specific Encoder Models with LLM-Generated Data: How to Leverage Ontologies, and How to Do Without Them", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We investigate the use of LLM-generated data for continual pretraining of\nencoder models in specialized domains with limited training data, using the\nscientific domain of invasion biology as a case study. To this end, we leverage\ndomain-specific ontologies by enriching them with LLM-generated data and\npretraining the encoder model as an ontology-informed embedding model for\nconcept definitions. To evaluate the effectiveness of this method, we compile a\nbenchmark specifically designed for assessing model performance in invasion\nbiology. After demonstrating substantial improvements over standard LLM\npretraining, we investigate the feasibility of applying the proposed approach\nto domains without comprehensive ontologies by substituting ontological\nconcepts with concepts automatically extracted from a small corpus of\nscientific abstracts and establishing relationships between concepts through\ndistributional statistics. Our results demonstrate that this automated approach\nachieves comparable performance using only a small set of scientific abstracts,\nresulting in a fully automated pipeline for enhancing domain-specific\nunderstanding of small encoder models that is especially suited for application\nin low-resource settings and achieves performance comparable to masked language\nmodeling pretraining on much larger datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21854", "pdf": "https://arxiv.org/pdf/2503.21854", "abs": "https://arxiv.org/abs/2503.21854", "authors": ["Hongyi Zeng", "Wenxuan Liu", "Tianhua Xia", "Jinhui Chen", "Ziyun Li", "Sai Qian Zhang"], "title": "Foveated Instance Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation is essential for augmented reality and virtual reality\n(AR/VR) as it enables precise object recognition and interaction, enhancing the\nintegration of virtual and real-world elements for an immersive experience.\nHowever, the high computational overhead of segmentation limits its application\non resource-constrained AR/VR devices, causing large processing latency and\ndegrading user experience. In contrast to conventional scenarios, AR/VR users\ntypically focus on only a few regions within their field of view before\nshifting perspective, allowing segmentation to be concentrated on gaze-specific\nareas. This insight drives the need for efficient segmentation methods that\nprioritize processing instance of interest, reducing computational load and\nenhancing real-time performance. In this paper, we present a foveated instance\nsegmentation (FovealSeg) framework that leverages real-time user gaze data to\nperform instance segmentation exclusively on instance of interest, resulting in\nsubstantial computational savings. Evaluation results show that FSNet achieves\nan IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline.\nThe code is available at https://github.com/SAI-", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22036", "pdf": "https://arxiv.org/pdf/2503.22036", "abs": "https://arxiv.org/abs/2503.22036", "authors": ["Oliver Kramer"], "title": "Cognitive Prompts Using Guilford's Structure of Intellect Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong language generation\ncapabilities but often struggle with structured reasoning, leading to\ninconsistent or suboptimal problem-solving. To mitigate this limitation,\nGuilford's Structure of Intellect (SOI) model - a foundational framework from\nintelligence theory - is leveraged as the basis for cognitive prompt\nengineering. The SOI model categorizes cognitive operations such as pattern\nrecognition, memory retrieval, and evaluation, offering a systematic approach\nto enhancing LLM reasoning and decision-making. This position paper presents a\nnovel cognitive prompting approach for enforcing SOI-inspired reasoning for\nimproving clarity, coherence, and adaptability in model responses.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21893", "pdf": "https://arxiv.org/pdf/2503.21893", "abs": "https://arxiv.org/abs/2503.21893", "authors": ["Taufiq Ahmed", "Abhishek Kumar", "Constantino Álvarez Casado", "Anlan Zhang", "Tuomo Hänninen", "Lauri Loven", "Miguel Bordallo López", "Sasu Tarkoma"], "title": "Exponentially Weighted Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection Model Training in Unmanned Aerial Vehicles Surveillance Scenarios", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 2 figures, 9 tables, 6 formulas, conference paper", "summary": "Object detection models often struggle with class imbalance, where rare\ncategories appear significantly less frequently than common ones. Existing\nsampling-based rebalancing strategies, such as Repeat Factor Sampling (RFS) and\nInstance-Aware Repeat Factor Sampling (IRFS), mitigate this issue by adjusting\nsample frequencies based on image and instance counts. However, these methods\nare based on linear adjustments, which limit their effectiveness in long-tailed\ndistributions. This work introduces Exponentially Weighted Instance-Aware\nRepeat Factor Sampling (E-IRFS), an extension of IRFS that applies exponential\nscaling to better differentiate between rare and frequent classes. E-IRFS\nadjusts sampling probabilities using an exponential function applied to the\ngeometric mean of image and instance frequencies, ensuring a more adaptive\nrebalancing strategy. We evaluate E-IRFS on a dataset derived from the\nFireman-UAV-RGBT Dataset and four additional public datasets, using YOLOv11\nobject detection models to identify fire, smoke, people and lakes in emergency\nscenarios. The results show that E-IRFS improves detection performance by 22\\%\nover the baseline and outperforms RFS and IRFS, particularly for rare\ncategories. The analysis also highlights that E-IRFS has a stronger effect on\nlightweight models with limited capacity, as these models rely more on data\nsampling strategies to address class imbalance. The findings demonstrate that\nE-IRFS improves rare object detection in resource-constrained environments,\nmaking it a suitable solution for real-time applications such as UAV-based\nemergency monitoring.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "sampling strategies"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22074", "pdf": "https://arxiv.org/pdf/2503.22074", "abs": "https://arxiv.org/abs/2503.22074", "authors": ["Chuan-Wei Kuo", "Siyu Chen", "Chenqi Yan", "Yu Yang Fredrik Liu"], "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21932", "pdf": "https://arxiv.org/pdf/2503.21932", "abs": "https://arxiv.org/abs/2503.21932", "authors": ["Seyed Hamidreza Nabaei", "Zeyang Zheng", "Dong Chen", "Arsalan Heydarian"], "title": "Multimodal Data Integration for Sustainable Indoor Gardening: Tracking Anyplant with Time Series Foundation Model", "categories": ["cs.CV", "cs.CE", "cs.LG"], "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)", "summary": "Indoor gardening within sustainable buildings offers a transformative\nsolution to urban food security and environmental sustainability. By 2030,\nurban farming, including Controlled Environment Agriculture (CEA) and vertical\nfarming, is expected to grow at a compound annual growth rate (CAGR) of 13.2%\nfrom 2024 to 2030, according to market reports. This growth is fueled by\nadvancements in Internet of Things (IoT) technologies, sustainable innovations\nsuch as smart growing systems, and the rising interest in green interior\ndesign. This paper presents a novel framework that integrates computer vision,\nmachine learning (ML), and environmental sensing for the automated monitoring\nof plant health and growth. Unlike previous approaches, this framework combines\nRGB imagery, plant phenotyping data, and environmental factors such as\ntemperature and humidity, to predict plant water stress in a controlled growth\nenvironment. The system utilizes high-resolution cameras to extract phenotypic\nfeatures, such as RGB, plant area, height, and width while employing the\nLag-Llama time series model to analyze and predict water stress. Experimental\nresults demonstrate that integrating RGB, size ratios, and environmental data\nsignificantly enhances predictive accuracy, with the Fine-tuned model achieving\nthe lowest errors (MSE = 0.420777, MAE = 0.595428) and reduced uncertainty.\nThese findings highlight the potential of multimodal data and intelligent\nsystems to automate plant care, optimize resource consumption, and align indoor\ngardening with sustainable building management practices, paving the way for\nresilient, green urban spaces.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22144", "pdf": "https://arxiv.org/pdf/2503.22144", "abs": "https://arxiv.org/abs/2503.22144", "authors": ["Papa Abdou Karim Karou Diallo", "Amal Zouaq"], "title": "FRASE: Structured Representations for Generalizable SPARQL Query Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Translating natural language questions into SPARQL queries enables Knowledge\nBase querying for factual and up-to-date responses. However, existing datasets\nfor this task are predominantly template-based, leading models to learn\nsuperficial mappings between question and query templates rather than\ndeveloping true generalization capabilities. As a result, models struggle when\nencountering naturally phrased, template-free questions. This paper introduces\nFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages Frame\nSemantic Role Labeling (FSRL) to address this limitation. We also present\nLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question is\nenriched using FRASE through frame detection and the mapping of frame-elements\nto their argument. We evaluate the impact of this approach through extensive\nexperiments on recent large language models (LLMs) under different fine-tuning\nconfigurations. Our results demonstrate that integrating frame-based structured\nrepresentations consistently improves SPARQL generation performance,\nparticularly in challenging generalization scenarios when test questions\nfeature unseen templates (unknown template splits) and when they are all\nnaturally phrased (reformulated questions).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21970", "pdf": "https://arxiv.org/pdf/2503.21970", "abs": "https://arxiv.org/abs/2503.21970", "authors": ["Yujie Chen", "Haotong Qin", "Zhang Zhang", "Michelo Magno", "Luca Benini", "Yawei Li"], "title": "Q-MambaIR: Accurate Quantized Mamba for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "State-Space Models (SSMs) have attracted considerable attention in Image\nRestoration (IR) due to their ability to scale linearly sequence length while\neffectively capturing long-distance dependencies. However, deploying SSMs to\nedge devices is challenging due to the constraints in memory, computing\ncapacity, and power consumption, underscoring the need for efficient\ncompression strategies. While low-bit quantization is an efficient model\ncompression strategy for reducing size and accelerating IR tasks, SSM suffers\nsubstantial performance drops at ultra-low bit-widths (2-4 bits), primarily due\nto outliers that exacerbate quantization error. To address this challenge, we\npropose Q-MambaIR, an accurate, efficient, and flexible Quantized Mamba for IR\ntasks. Specifically, we introduce a Statistical Dynamic-balancing Learnable\nScalar (DLS) to dynamically adjust the quantization mapping range, thereby\nmitigating the peak truncation loss caused by extreme values. Furthermore, we\ndesign a Range-floating Flexible Allocator (RFA) with an adaptive threshold to\nflexibly round values. This approach preserves high-frequency details and\nmaintains the SSM's feature extraction capability. Notably, RFA also enables\npre-deployment weight quantization, striking a balance between computational\nefficiency and model accuracy. Extensive experiments on IR tasks demonstrate\nthat Q-MambaIR consistently outperforms existing quantized SSMs, achieving much\nhigher state-of-the-art (SOTA) accuracy results with only a negligible increase\nin training computation and storage saving.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21979", "pdf": "https://arxiv.org/pdf/2503.21979", "abs": "https://arxiv.org/abs/2503.21979", "authors": ["Size Wu", "Wenwei Zhang", "Lumin Xu", "Sheng Jin", "Zhonghua Wu", "Qingyi Tao", "Wentao Liu", "Wei Li", "Chen Change Loy"], "title": "Harmonizing Visual Representations for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Unifying visual understanding and generation within a single multimodal\nframework remains a significant challenge, as the two inherently heterogeneous\ntasks require representations at different levels of granularity. Current\napproaches that utilize vector quantization (VQ) or variational autoencoders\n(VAE) for unified visual representation prioritize intrinsic imagery features\nover semantics, compromising understanding performance. In this work, we take\ninspiration from masked image modelling (MIM) that learns rich semantics via a\nmask-and-reconstruct pre-training and its successful extension to masked\nautoregressive (MAR) image generation. A preliminary study on the MAR encoder's\nrepresentation reveals exceptional linear probing accuracy and precise feature\nresponse to visual concepts, which indicates MAR's potential for visual\nunderstanding tasks beyond its original generation role. Based on these\ninsights, we present \\emph{Harmon}, a unified autoregressive framework that\nharmonizes understanding and generation tasks with a shared MAR encoder.\nThrough a three-stage training procedure that progressively optimizes\nunderstanding and generation capabilities, Harmon achieves state-of-the-art\nimage generation results on the GenEval, MJHQ30K and WISE benchmarks while\nmatching the performance of methods with dedicated semantic encoders (e.g.,\nJanus) on image understanding benchmarks. Our code and models will be available\nat https://github.com/wusize/Harmon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21999", "pdf": "https://arxiv.org/pdf/2503.21999", "abs": "https://arxiv.org/abs/2503.21999", "authors": ["Tony Tran", "Bin Hu"], "title": "FACETS: Efficient Once-for-all Object Detection via Constrained Iterative Search", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 6 figures", "summary": "Neural Architecture Search (NAS) for deep learning object detection\nframeworks typically involves multiple modules, each performing distinct tasks.\nThese modules contribute to a vast search space, resulting in searches that can\ntake several GPU hours or even days, depending on the complexity of the search\nspace. This makes joint optimization both challenging and computationally\nexpensive. Furthermore, satisfying target device constraints across modules\nadds additional complexity to the optimization process. To address these\nchallenges, we propose \\textbf{FACETS}, e\\textbf{\\underline{F}}ficient\nOnce-for-\\textbf{\\underline{A}}ll Object Detection via\n\\textbf{\\underline{C}}onstrained\nit\\textbf{\\underline{E}}ra\\textbf{\\underline{T}}ive\\textbf{\\underline{S}}earch,\na novel unified iterative NAS method that refines the architecture of all\nmodules in a cyclical manner. FACETS leverages feedback from previous\niterations, alternating between fixing one module's architecture and optimizing\nthe others. This approach reduces the overall search space while preserving\ninterdependencies among modules and incorporates constraints based on the\ntarget device's computational budget. In a controlled comparison against\nprogressive and single-module search strategies, FACETS achieves architectures\nwith up to $4.75\\%$ higher accuracy twice as fast as progressive search\nstrategies in earlier stages, while still being able to achieve a global\noptimum. Moreover, FACETS demonstrates the ability to iteratively refine the\nsearch space, producing better performing architectures over time. The refined\nsearch space yields candidates with a mean accuracy up to $27\\%$ higher than\nglobal search and $5\\%$ higher than progressive search methods via random\nsampling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22362", "pdf": "https://arxiv.org/pdf/2503.22362", "abs": "https://arxiv.org/abs/2503.22362", "authors": ["Yuan He", "Bailan He", "Zifeng Ding", "Alisia Lupidi", "Yuqicheng Zhu", "Shuo Chen", "Caiqi Zhang", "Jiaoyan Chen", "Yunpu Ma", "Volker Tresp", "Ian Horrocks"], "title": "Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding and mitigating hallucinations in Large Language Models (LLMs)\nis crucial for ensuring reliable content generation. While previous research\nhas primarily focused on \"when\" LLMs hallucinate, our work explains \"why\" and\ndirectly links model behaviour to the pre-training data that forms their prior\nknowledge. Specifically, we demonstrate that an asymmetry exists in the\nrecognition of logically equivalent facts, which can be attributed to frequency\ndiscrepancies of entities appearing as subjects versus objects. Given that most\npre-training datasets are inaccessible, we leverage the fully open-source OLMo\nseries by indexing its Dolma dataset to estimate entity frequencies. Using\nrelational facts (represented as triples) from Wikidata5M, we construct probing\ndatasets to isolate this effect. Our experiments reveal that facts with a\nhigh-frequency subject and a low-frequency object are better recognised than\ntheir inverse, despite their logical equivalence. The pattern reverses in\nlow-to-high frequency settings, and no statistically significant asymmetry\nemerges when both entities are high-frequency. These findings highlight the\ninfluential role of pre-training data in shaping model predictions and provide\ninsights for inferring the characteristics of pre-training data in closed or\npartially closed LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22395", "pdf": "https://arxiv.org/pdf/2503.22395", "abs": "https://arxiv.org/abs/2503.22395", "authors": ["Tereza Vrabcová", "Marek Kadlčík", "Petr Sojka", "Michal Štefánik", "Michal Spiegel"], "title": "Negation: A Pink Elephant in the Large Language Models' Room?", "categories": ["cs.CL"], "comment": null, "summary": "Negations are key to determining sentence meaning, making them essential for\nlogical reasoning. Despite their importance, negations pose a substantial\nchallenge for large language models (LLMs) and remain underexplored.\n  We construct two multilingual natural language inference (NLI) datasets with\n\\textit{paired} examples differing in negation. We investigate how model size\nand language impact its ability to handle negation correctly by evaluating\npopular LLMs.\n  Contrary to previous work, we show that increasing the model size\nconsistently improves the models' ability to handle negations. Furthermore, we\nfind that both the models' reasoning accuracy and robustness to negation are\nlanguage-dependent and that the length and explicitness of the premise have a\ngreater impact on robustness than language.\n  Our datasets can facilitate further research and improvements of language\nmodel reasoning in multilingual settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22426", "pdf": "https://arxiv.org/pdf/2503.22426", "abs": "https://arxiv.org/abs/2503.22426", "authors": ["Yuto Nishida", "Makoto Morishita", "Hiroyuki Deguchi", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Long-Tail Crisis in Nearest Neighbor Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Findings", "summary": "The $k$-nearest-neighbor language model ($k$NN-LM), one of the\nretrieval-augmented language models, improves the perplexity for given text by\ndirectly accessing a large datastore built from any text data during inference.\nA widely held hypothesis for the success of $k$NN-LM is that its explicit\nmemory, i.e., the datastore, enhances predictions for long-tail phenomena.\nHowever, prior works have primarily shown its ability to retrieve long-tail\ncontexts, leaving the model's performance remain underexplored in estimating\nthe probabilities of long-tail target tokens during inference. In this paper,\nwe investigate the behavior of $k$NN-LM on low-frequency tokens, examining\nprediction probability, retrieval accuracy, token distribution in the\ndatastore, and approximation error of the product quantization. Our\nexperimental results reveal that $k$NN-LM does not improve prediction\nperformance for low-frequency tokens but mainly benefits high-frequency tokens\nregardless of long-tail contexts in the datastore.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22582", "pdf": "https://arxiv.org/pdf/2503.22582", "abs": "https://arxiv.org/abs/2503.22582", "authors": ["Sarubi Thillainathan", "Songchen Yuan", "En-Shiun Annie Lee", "Sanath Jayasena", "Surangika Ranathunga"], "title": "Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning multilingual sequence-to-sequence large language models (msLLMs)\nhas shown promise in developing neural machine translation (NMT) systems for\nlow-resource languages (LRLs). However, conventional single-stage fine-tuning\nmethods struggle in extremely low-resource NMT settings, where training data is\nvery limited. This paper contributes to artificial intelligence by proposing\ntwo approaches for adapting msLLMs in these challenging scenarios: (1)\ncontinual pre-training (CPT), where the msLLM is further trained with\ndomain-specific monolingual data to compensate for the under-representation of\nLRLs, and (2) intermediate task transfer learning (ITTL), a method that\nfine-tunes the msLLM with both in-domain and out-of-domain parallel data to\nenhance its translation capabilities across various domains and tasks. As an\napplication in engineering, these methods are implemented in NMT systems for\nSinhala, Tamil, and English (six language pairs) in domain-specific, extremely\nlow-resource settings (datasets containing fewer than 100,000 samples). Our\nexperiments reveal that these approaches enhance translation performance by an\naverage of +1.47 bilingual evaluation understudy (BLEU) score compared to the\nstandard single-stage fine-tuning baseline across all translation directions.\nAdditionally, a multi-model ensemble further improves performance by an\nadditional BLEU score.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22168", "pdf": "https://arxiv.org/pdf/2503.22168", "abs": "https://arxiv.org/abs/2503.22168", "authors": ["Woojung Han", "Yeonkyung Lee", "Chanyoung Kim", "Kwanghyun Park", "Seong Jae Hwang"], "title": "Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Diffusion-based text-to-image (T2I) models have recently excelled in\nhigh-quality image generation, particularly in a training-free manner, enabling\ncost-effective adaptability and generalization across diverse tasks. However,\nwhile the existing methods have been continuously focusing on several\nchallenges, such as \"missing objects\" and \"mismatched attributes,\" another\ncritical issue of \"mislocated objects\" remains where generated spatial\npositions fail to align with text prompts. Surprisingly, ensuring such\nseemingly basic functionality remains challenging in popular T2I models due to\nthe inherent difficulty of imposing explicit spatial guidance via text forms.\nTo address this, we propose STORM (Spatial Transport Optimization by\nRepositioning Attention Map), a novel training-free approach for spatially\ncoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),\nrooted in optimal transport theory, to dynamically adjust object attention maps\nfor precise spatial adherence, supported by a Spatial Transport (ST) Cost\nfunction that enhances spatial understanding. Our analysis shows that\nintegrating spatial awareness is most effective in the early denoising stages,\nwhile later phases refine details. Extensive experiments demonstrate that STORM\nsurpasses existing methods, effectively mitigating mislocated objects while\nimproving missing and mismatched attributes, setting a new benchmark for\nspatial alignment in T2I synthesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21902", "pdf": "https://arxiv.org/pdf/2503.21902", "abs": "https://arxiv.org/abs/2503.21902", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "Oliver Karras", "Sören Auer"], "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track", "summary": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22171", "pdf": "https://arxiv.org/pdf/2503.22171", "abs": "https://arxiv.org/abs/2503.22171", "authors": ["Min Cao", "ZiYin Zeng", "YuXin Lu", "Mang Ye", "Dong Yi", "Jinqiao Wang"], "title": "An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval", "categories": ["cs.CV"], "comment": "20 pages,13 figures", "summary": "Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research.\nMainstream research paradigm necessitates real-world person images with manual\ntextual annotations for training models, posing privacy-sensitive and\nlabor-intensive issues. Several pioneering efforts explore synthetic data for\nTBPR but still rely on real data, keeping the aforementioned issues and also\nresulting in diversity-deficient issue in synthetic datasets, thus impacting\nTBPR performance. Moreover, these works tend to explore synthetic data for TBPR\nthrough limited perspectives, leading to exploration-restricted issue. In this\npaper, we conduct an empirical study to explore the potential of synthetic data\nfor TBPR, highlighting three key aspects. (1) We propose an inter-class image\ngeneration pipeline, in which an automatic prompt construction strategy is\nintroduced to guide generative Artificial Intelligence (AI) models in\ngenerating various inter-class images without reliance on original data. (2) We\ndevelop an intra-class image augmentation pipeline, in which the generative AI\nmodels are applied to further edit the images for obtaining various intra-class\nimages. (3) Building upon the proposed pipelines and an automatic text\ngeneration pipeline, we explore the effectiveness of synthetic data in diverse\nscenarios through extensive experiments. Additionally, we experimentally\ninvestigate various noise-robust learning strategies to mitigate the inherent\nnoise in synthetic data. We will release the code, along with the synthetic\nlarge-scale dataset generated by our pipelines, which are expected to advance\npractical TBPR research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22172", "pdf": "https://arxiv.org/pdf/2503.22172", "abs": "https://arxiv.org/abs/2503.22172", "authors": ["Minho Park", "Sunghyun Park", "Jungsoo Lee", "Hyojin Park", "Kyuwoong Hwang", "Fatih Porikli", "Jaegul Choo", "Sungha Choi"], "title": "Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of data scarcity in semantic segmentation\nby generating datasets through text-to-image (T2I) generation models, reducing\nimage acquisition and labeling costs. Segmentation dataset generation faces two\nkey challenges: 1) aligning generated samples with the target domain and 2)\nproducing informative samples beyond the training data. Fine-tuning T2I models\ncan help generate samples aligned with the target domain. However, it often\noverfits and memorizes training data, limiting their ability to generate\ndiverse and well-aligned samples. To overcome these issues, we propose\nConcept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively\nidentifies and updates only the weights associated with necessary concepts\n(e.g., style or viewpoint) for domain alignment while preserving the pretrained\nknowledge of the T2I model to produce informative samples. We demonstrate its\neffectiveness in generating datasets for urban-scene segmentation,\noutperforming baseline and state-of-the-art methods in in-domain (few-shot and\nfully-supervised) settings, as well as in domain generalization tasks,\nespecially under challenging conditions such as adverse weather and varying\nillumination, further highlighting its superiority.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22038", "pdf": "https://arxiv.org/pdf/2503.22038", "abs": "https://arxiv.org/abs/2503.22038", "authors": ["Ngoc Tuong Vy Nguyen", "Felix D Childress", "Yunting Yin"], "title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection", "categories": ["cs.MA", "cs.CL"], "comment": "Accepted to the 13th International Symposium on Digital Forensics and\n  Security (ISDFS 2025)", "summary": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly\nrefine their methods, making phishing emails harder to detect. Traditional\ndetection methods, including rule-based systems and supervised machine learning\nmodels, either rely on predefined patterns like blacklists, which can be\nbypassed with slight modifications, or require large datasets for training and\nstill can generate false positives and false negatives. In this work, we\npropose a multi-agent large language model (LLM) prompting technique that\nsimulates debates among agents to detect whether the content presented on an\nemail is phishing. Our approach uses two LLM agents to present arguments for or\nagainst the classification task, with a judge agent adjudicating the final\nverdict based on the quality of reasoning provided. This debate mechanism\nenables the models to critically analyze contextual cue and deceptive patterns\nin text, which leads to improved classification accuracy. The proposed\nframework is evaluated on multiple phishing email datasets and demonstrate that\nmixed-agent configurations consistently outperform homogeneous configurations.\nResults also show that the debate structure itself is sufficient to yield\naccurate decisions without extra prompting strategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22174", "pdf": "https://arxiv.org/pdf/2503.22174", "abs": "https://arxiv.org/abs/2503.22174", "authors": ["Jialun Pei", "Zhangjun Zhou", "Diandian Guo", "Zhixi Li", "Jing Qin", "Bo Du", "Pheng-Ann Heng"], "title": "Synergistic Bleeding Region and Point Detection in Surgical Videos", "categories": ["cs.CV"], "comment": null, "summary": "Intraoperative bleeding in laparoscopic surgery causes rapid obscuration of\nthe operative field to hinder the surgical process. Intelligent detection of\nbleeding regions can quantify the blood loss to assist decision-making, while\nlocating the bleeding point helps surgeons quickly identify the source of\nbleeding and achieve hemostasis in time. In this study, we first construct a\nreal-world surgical bleeding detection dataset, named SurgBlood, comprising\n5,330 frames from 95 surgical video clips with bleeding region and point\nannotations. Accordingly, we develop a dual-task synergistic online detector\ncalled BlooDet, designed to perform simultaneous detection of bleeding regions\nand points in surgical videos. Our framework embraces a dual-branch\nbidirectional guidance design based on Segment Anything Model 2 (SAM 2). The\nmask branch detects bleeding regions through adaptive edge and point prompt\nembeddings, while the point branch leverages mask memory to induce bleeding\npoint memory modeling and captures the direction of bleed point movement\nthrough inter-frame optical flow. By interactive guidance and prompts, the two\nbranches explore potential spatial-temporal relationships while leveraging\nmemory modeling from previous frames to infer the current bleeding condition.\nExtensive experiments demonstrate that our approach outperforms other\ncounterparts on SurgBlood in both bleeding region and point detection tasks,\ne.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% for\nbleeding point detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22122", "pdf": "https://arxiv.org/pdf/2503.22122", "abs": "https://arxiv.org/abs/2503.22122", "authors": ["Puzhen Yuan", "Angyuan Ma", "Yunchao Yao", "Huaxiu Yao", "Masayoshi Tomizuka", "Mingyu Ding"], "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22179", "pdf": "https://arxiv.org/pdf/2503.22179", "abs": "https://arxiv.org/abs/2503.22179", "authors": ["Dailan He", "Xiahong Wang", "Shulun Wang", "Guanglu Song", "Bingqi Ma", "Hao Shao", "Yu Liu", "Hongsheng Li"], "title": "High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Face swapping aims to seamlessly transfer a source facial identity onto a\ntarget while preserving target attributes such as pose and expression.\nDiffusion models, known for their superior generative capabilities, have\nrecently shown promise in advancing face-swapping quality. This paper addresses\ntwo key challenges in diffusion-based face swapping: the prioritized\npreservation of identity over target attributes and the inherent conflict\nbetween identity and attribute conditioning. To tackle these issues, we\nintroduce an identity-constrained attribute-tuning framework for face swapping\nthat first ensures identity preservation and then fine-tunes for attribute\nalignment, achieved through a decoupled condition injection. We further enhance\nfidelity by incorporating identity and adversarial losses in a post-training\nrefinement stage. Our proposed identity-constrained diffusion-based\nface-swapping model outperforms existing methods in both qualitative and\nquantitative evaluations, demonstrating superior identity similarity and\nattribute consistency, achieving a new state-of-the-art performance in\nhigh-fidelity face swapping.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22193", "pdf": "https://arxiv.org/pdf/2503.22193", "abs": "https://arxiv.org/abs/2503.22193", "authors": ["Yang Liu", "Feixiang Liu", "Jiale Du", "Xinbo Gao", "Jungong Han"], "title": "Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks and supervised learning have achieved\nremarkable success in various fields but are limited by the need for large\nannotated datasets. Few-shot learning (FSL) addresses this limitation by\nenabling models to generalize from only a few labeled examples. Transductive\nfew-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled\ndata, though it faces challenges like the hubness problem. To overcome these\nlimitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC)\nMethod, which addresses the key challenges in few-shot learning through three\ninnovative contributions. First, we introduce a decentralized covariance matrix\nto mitigate the hubness problem, ensuring a more uniform distribution of\nembeddings. Second, our method combines local alignment and global uniformity\nthrough adaptive weighting and nonlinear transformation, balancing intra-class\nclustering with inter-class separation. Third, we employ a Variational Sinkhorn\nFew-Shot Classifier to optimize the distances between samples and class\nprototypes, enhancing classification accuracy and robustness. These combined\ninnovations allow the UMMEC method to achieve superior performance with minimal\nlabeled data. Our UMMEC method significantly improves classification\nperformance with minimal labeled data, advancing the state-of-the-art in TFSL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22610", "pdf": "https://arxiv.org/pdf/2503.22610", "abs": "https://arxiv.org/abs/2503.22610", "authors": ["Antonia Karamolegkou", "Malvina Nikandrou", "Georgios Pantazopoulos", "Danae Sanchez Villegas", "Phillip Rust", "Ruchira Dhar", "Daniel Hershcovich", "Anders Søgaard"], "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper explores the effectiveness of Multimodal Large Language models\n(MLLMs) as assistive technologies for visually impaired individuals. We conduct\na user survey to identify adoption patterns and key challenges users face with\nsuch technologies. Despite a high adoption rate of these models, our findings\nhighlight concerns related to contextual understanding, cultural sensitivity,\nand complex scene understanding, particularly for individuals who may rely\nsolely on them for visual interpretation. Informed by these results, we collate\nfive user-centred tasks with image and video inputs, including a novel task on\nOptical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals\nthat further advancements are necessary to overcome limitations related to\ncultural context, multilingual support, Braille reading comprehension,\nassistive object recognition, and hallucinations. This work provides critical\ninsights into the future direction of multimodal AI for accessibility,\nunderscoring the need for more inclusive, robust, and trustworthy visual\nassistance technologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22201", "pdf": "https://arxiv.org/pdf/2503.22201", "abs": "https://arxiv.org/abs/2503.22201", "authors": ["Jaewoo Jeong", "Seohee Lee", "Daehee Park", "Giwon Lee", "Kuk-Jin Yoon"], "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Pedestrian trajectory forecasting is crucial in various applications such as\nautonomous driving and mobile robot navigation. In such applications,\ncamera-based perception enables the extraction of additional modalities (human\npose, text) to enhance prediction accuracy. Indeed, we find that textual\ndescriptions play a crucial role in integrating additional modalities into a\nunified understanding. However, online extraction of text requires the use of\nVLM, which may not be feasible for resource-constrained systems. To address\nthis challenge, we propose a multi-modal knowledge distillation framework: a\nstudent model with limited modality is distilled from a teacher model trained\nwith full range of modalities. The comprehensive knowledge of a teacher model\ntrained with trajectory, human pose, and text is distilled into a student model\nusing only trajectory or human pose as a sole supplement. In doing so, we\nseparately distill the core locomotion insights from intra-agent multi-modality\nand inter-agent interaction. Our generalizable framework is validated with two\nstate-of-the-art models across three datasets on both ego-view (JRDB, SIT) and\nBEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated text\ncaptions. Distilled student models show consistent improvement in all\nprediction metrics for both full and instantaneous observations, improving up\nto ~13%. The code is available at https://github.com/Jaewoo97/KDTF.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22218", "pdf": "https://arxiv.org/pdf/2503.22218", "abs": "https://arxiv.org/abs/2503.22218", "authors": ["Wenjie Liu", "Zhongliang Liu", "Xiaoyan Yang", "Man Sha", "Yang Li"], "title": "ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 14 figures", "summary": "3D scene stylization approaches based on Neural Radiance Fields (NeRF)\nachieve promising results by optimizing with Nearest Neighbor Feature Matching\n(NNFM) loss. However, NNFM loss does not consider global style information. In\naddition, the implicit representation of NeRF limits their fine-grained control\nover the resulting scenes. In this paper, we introduce ABC-GS, a novel\nframework based on 3D Gaussian Splatting to achieve high-quality 3D style\ntransfer. To this end, a controllable matching stage is designed to achieve\nprecise alignment between scene content and style features through segmentation\nmasks. Moreover, a style transfer loss function based on feature alignment is\nproposed to ensure that the outcomes of style transfer accurately reflect the\nglobal style of the reference image. Furthermore, the original geometric\ninformation of the scene is preserved with the depth loss and Gaussian\nregularization terms. Extensive experiments show that our ABC-GS provides\ncontrollability of style transfer and achieves stylization results that are\nmore faithfully aligned with the global style of the chosen artistic reference.\nOur homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22237", "pdf": "https://arxiv.org/pdf/2503.22237", "abs": "https://arxiv.org/abs/2503.22237", "authors": ["Kunliang Liu", "Jianming Wang", "Rize Jin", "Wonjun Hwang", "Tae-Sun Chung"], "title": "SCHNet: SAM Marries CLIP for Human Parsing", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) and\nContrastive Language-Image Pre-training Model (CLIP) has shown promising\nperformance for segmentation and detection tasks. However, although SAM excels\nin fine-grained segmentation, it faces major challenges when applying it to\nsemantic-aware segmentation. While CLIP exhibits a strong semantic\nunderstanding capability via aligning the global features of language and\nvision, it has deficiencies in fine-grained segmentation tasks. Human parsing\nrequires to segment human bodies into constituent parts and involves both\naccurate fine-grained segmentation and high semantic understanding of each\npart. Based on traits of SAM and CLIP, we formulate high efficient modules to\neffectively integrate features of them to benefit human parsing. We propose a\nSemantic-Refinement Module to integrate semantic features of CLIP with SAM\nfeatures to benefit parsing. Moreover, we formulate a high efficient\nFine-tuning Module to adjust the pretrained SAM for human parsing that needs\nhigh semantic information and simultaneously demands spatial details, which\nsignificantly reduces the training time compared with full-time training and\nachieves notable performance. Extensive experiments demonstrate the\neffectiveness of our method on LIP, PPP, and CIHP databases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22268", "pdf": "https://arxiv.org/pdf/2503.22268", "abs": "https://arxiv.org/abs/2503.22268", "authors": ["Nan Huang", "Wenzhao Zheng", "Chenfeng Xu", "Kurt Keutzer", "Shanghang Zhang", "Angjoo Kanazawa", "Qianqian Wang"], "title": "Segment Any Motion in Videos", "categories": ["cs.CV"], "comment": "CVPR 2025. Website: https://motion-seg.github.io/", "summary": "Moving object segmentation is a crucial task for achieving a high-level\nunderstanding of visual scenes and has numerous downstream applications. Humans\ncan effortlessly segment moving objects in videos. Previous work has largely\nrelied on optical flow to provide motion cues; however, this approach often\nresults in imperfect predictions due to challenges such as partial motion,\ncomplex deformations, motion blur and background distractions. We propose a\nnovel approach for moving object segmentation that combines long-range\ntrajectory motion cues with DINO-based semantic features and leverages SAM2 for\npixel-level mask densification through an iterative prompting strategy. Our\nmodel employs Spatio-Temporal Trajectory Attention and Motion-Semantic\nDecoupled Embedding to prioritize motion while integrating semantic support.\nExtensive testing on diverse datasets demonstrates state-of-the-art\nperformance, excelling in challenging scenarios and fine-grained segmentation\nof multiple objects. Our code is available at https://motion-seg.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22281", "pdf": "https://arxiv.org/pdf/2503.22281", "abs": "https://arxiv.org/abs/2503.22281", "authors": ["Xuan Loc Pham", "Mathias Prokop", "Bram van Ginneken", "Alessa Hering"], "title": "Divide to Conquer: A Field Decomposition Approach for Multi-Organ Whole-Body CT Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Image registration is an essential technique for the analysis of Computed\nTomography (CT) images in clinical practice. However, existing methodologies\nare predominantly tailored to a specific organ of interest and often exhibit\nlower performance on other organs, thus limiting their generalizability and\napplicability. Multi-organ registration addresses these limitations, but the\nsimultaneous alignment of multiple organs with diverse shapes, sizes and\nlocations requires a highly complex deformation field with a multi-layer\ncomposition of individual deformations. This study introduces a novel field\ndecomposition approach to address the high complexity of deformations in\nmulti-organ whole-body CT image registration. The proposed method is trained\nand evaluated on a longitudinal dataset of 691 patients, each with two CT\nimages obtained at distinct time points. These scans fully encompass the\nthoracic, abdominal, and pelvic regions. Two baseline registration methods are\nselected for this study: one based on optimization techniques and another based\non deep learning. Experimental results demonstrate that the proposed approach\noutperforms baseline methods in handling complex deformations in multi-organ\nwhole-body CT image registration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22291", "pdf": "https://arxiv.org/pdf/2503.22291", "abs": "https://arxiv.org/abs/2503.22291", "authors": ["Bin Zhang", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "As object detectors are increasingly deployed as black-box cloud services or\npre-trained models with restricted access to the original training data, the\nchallenge of zero-shot object-level out-of-distribution (OOD) detection arises.\nThis task becomes crucial in ensuring the reliability of detectors in\nopen-world settings. While existing methods have demonstrated success in\nimage-level OOD detection using pre-trained vision-language models like CLIP,\ndirectly applying such models to object-level OOD detection presents challenges\ndue to the loss of contextual information and reliance on image-level\nalignment. To tackle these challenges, we introduce a new method that leverages\nvisual prompts and text-augmented in-distribution (ID) space construction to\nadapt CLIP for zero-shot object-level OOD detection. Our method preserves\ncritical contextual information and improves the ability to differentiate\nbetween ID and OOD objects, achieving competitive performance across different\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22324", "pdf": "https://arxiv.org/pdf/2503.22324", "abs": "https://arxiv.org/abs/2503.22324", "authors": ["Chenyang Xu", "XingGuo Deng", "Rui Zhong"], "title": "AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The 3D Gaussian Splatting (3D-GS) is a novel method for scene representation\nand view synthesis. Although Scaffold-GS achieves higher quality real-time\nrendering compared to the original 3D-GS, its fine-grained rendering of the\nscene is extremely dependent on adequate viewing angles. The spectral bias of\nneural network learning results in Scaffold-GS's poor ability to perceive and\nlearn high-frequency information in the scene. In this work, we propose\nenhancing the manifold complexity of input features and using network-based\nfeature map loss to improve the image reconstruction quality of 3D-GS models.\nWe introduce AH-GS, which enables 3D Gaussians in structurally complex regions\nto obtain higher-frequency encodings, allowing the model to more effectively\nlearn the high-frequency information of the scene. Additionally, we incorporate\nhigh-frequency reinforce loss to further enhance the model's ability to capture\ndetailed frequency information. Our result demonstrates that our model\nsignificantly improves rendering fidelity, and in specific scenarios (e.g.,\nMipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS in\njust 15K iterations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22359", "pdf": "https://arxiv.org/pdf/2503.22359", "abs": "https://arxiv.org/abs/2503.22359", "authors": ["Jiahao Xia", "Min Xu", "Wenjian Huang", "Jianguo Zhang", "Haimin Zhang", "Chunxia Xiao"], "title": "Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment", "categories": ["cs.CV"], "comment": "24 Pages, 9 Figures", "summary": "Despite the similar structures of human faces, existing face alignment\nmethods cannot learn unified knowledge from multiple datasets with different\nlandmark annotations. The limited training samples in a single dataset commonly\nresult in fragile robustness in this field. To mitigate knowledge discrepancies\namong different datasets and train a task-agnostic unified face alignment\n(TUFA) framework, this paper presents a strategy to unify knowledge from\nmultiple datasets. Specifically, we calculate a mean face shape for each\ndataset. To explicitly align these mean shapes on an interpretable plane based\non their semantics, each shape is then incorporated with a group of semantic\nalignment embeddings. The 2D coordinates of these aligned shapes can be viewed\nas the anchors of the plane. By encoding them into structure prompts and\nfurther regressing the corresponding facial landmarks using image features, a\nmapping from the plane to the target faces is finally established, which\nunifies the learning target of different datasets. Consequently, multiple\ndatasets can be utilized to boost the generalization ability of the model. The\nsuccessful mitigation of discrepancies also enhances the efficiency of\nknowledge transferring to a novel dataset, significantly boosts the performance\nof few-shot face alignment. Additionally, the interpretable plane endows TUFA\nwith a task-agnostic characteristic, enabling it to locate landmarks unseen\nduring training in a zero-shot manner. Extensive experiments are carried on\nseven benchmarks and the results demonstrate an impressive improvement in face\nalignment brought by knowledge discrepancies mitigation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22398", "pdf": "https://arxiv.org/pdf/2503.22398", "abs": "https://arxiv.org/abs/2503.22398", "authors": ["David Fischinger", "Martin Boyer"], "title": "DF-Net: The Digital Forensics Network for Image Forgery Detection", "categories": ["cs.CV"], "comment": "Published in 2023 at the 25th Irish Machine Vision and Image\n  Processing Conference (IMVIP),\n  https://iprcs.github.io/pdf/IMVIP2023_Proceeding.pdf", "summary": "The orchestrated manipulation of public opinion, particularly through\nmanipulated images, often spread via online social networks (OSN), has become a\nserious threat to society. In this paper we introduce the Digital Forensics Net\n(DF-Net), a deep neural network for pixel-wise image forgery detection. The\nreleased model outperforms several state-of-the-art methods on four established\nbenchmark datasets. Most notably, DF-Net's detection is robust against lossy\nimage operations (e.g resizing, compression) as they are automatically\nperformed by social networks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22417", "pdf": "https://arxiv.org/pdf/2503.22417", "abs": "https://arxiv.org/abs/2503.22417", "authors": ["David Fischinger", "Martin Boyer"], "title": "DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection", "categories": ["cs.CV"], "comment": "Published at the 25th Irish Machine Vision and Image Processing\n  Conference (IMVIP) --- Proceedings:\n  https://iprcs.github.io/pdf/IMVIP2023_Proceeding.pdf --- Dataset download:\n  https://zenodo.org/records/7326540/files/DF2023_train.zip\n  https://zenodo.org/records/7326540/files/DF2023_val.zip Kaggle:\n  https://www.kaggle.com/datasets/davidfischinger/df2023-digital-forensics-2023-dataset/data", "summary": "The deliberate manipulation of public opinion, especially through altered\nimages, which are frequently disseminated through online social networks, poses\na significant danger to society. To fight this issue on a technical level we\nsupport the research community by releasing the Digital Forensics 2023 (DF2023)\ntraining and validation dataset, comprising one million images from four major\nforgery categories: splicing, copy-move, enhancement and removal. This dataset\nenables an objective comparison of network architectures and can significantly\nreduce the time and effort of researchers preparing datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22430", "pdf": "https://arxiv.org/pdf/2503.22430", "abs": "https://arxiv.org/abs/2503.22430", "authors": ["Sergio Izquierdo", "Mohamed Sayed", "Michael Firman", "Guillermo Garcia-Hernando", "Daniyar Turmukhambetov", "Javier Civera", "Oisin Mac Aodha", "Gabriel Brostow", "Jamie Watson"], "title": "MVSAnywhere: Zero-Shot Multi-View Stereo", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Computing accurate depth from multiple views is a fundamental and\nlongstanding challenge in computer vision. However, most existing approaches do\nnot generalize well across different domains and scene types (e.g. indoor vs.\noutdoor). Training a general-purpose multi-view stereo model is challenging and\nraises several questions, e.g. how to best make use of transformer-based\narchitectures, how to incorporate additional metadata when there is a variable\nnumber of input views, and how to estimate the range of valid depths which can\nvary considerably across different scenes and is typically not known a priori?\nTo address these issues, we introduce MVSA, a novel and versatile Multi-View\nStereo architecture that aims to work Anywhere by generalizing across diverse\ndomains and depth ranges. MVSA combines monocular and multi-view cues with an\nadaptive cost volume to deal with scale-related issues. We demonstrate\nstate-of-the-art zero-shot depth estimation on the Robust Multi-View Depth\nBenchmark, surpassing existing multi-view stereo and monocular baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22437", "pdf": "https://arxiv.org/pdf/2503.22437", "abs": "https://arxiv.org/abs/2503.22437", "authors": ["Xu Wang", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Complete reconstruction of surgical scenes is crucial for robot-assisted\nsurgery (RAS). Deep depth estimation is promising but existing works struggle\nwith depth discontinuities, resulting in noisy predictions at object boundaries\nand do not achieve complete reconstruction omitting occluded surfaces. To\naddress these issues we propose EndoLRMGS, that combines Large Reconstruction\nModelling (LRM) and Gaussian Splatting (GS), for complete surgical scene\nreconstruction. GS reconstructs deformable tissues and LRM generates 3D models\nfor surgical tools while position and scale are subsequently optimized by\nintroducing orthogonal perspective joint projection optimization (OPjPO) to\nenhance accuracy. In experiments on four surgical videos from three public\ndatasets, our method improves the Intersection-over-union (IoU) of tool 3D\nmodels in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR of\nthe tools projection from 3.82% to 11.07%. Tissue rendering quality also\nimproves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to\n29.21% across all test videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22513", "pdf": "https://arxiv.org/pdf/2503.22513", "abs": "https://arxiv.org/abs/2503.22513", "authors": ["Martin Kišš", "Michal Hradiš"], "title": "Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, 7 tables, 6 figures; Submitted to ICDAR25", "summary": "Self-supervised learning has emerged as a powerful approach for leveraging\nlarge-scale unlabeled data to improve model performance in various domains. In\nthis paper, we explore masked self-supervised pre-training for text recognition\ntransformers. Specifically, we propose two modifications to the pre-training\nphase: progressively increasing the masking probability, and modifying the loss\nfunction to incorporate both masked and non-masked patches. We conduct\nextensive experiments using a dataset of 50M unlabeled text lines for\npre-training and four differently sized annotated datasets for fine-tuning.\nFurthermore, we compare our pre-trained models against those trained with\ntransfer learning, demonstrating the effectiveness of the self-supervised\npre-training. In particular, pre-training consistently improves the character\nerror rate of models, in some cases up to 30 % relatively. It is also on par\nwith transfer learning but without relying on extra annotated text lines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22537", "pdf": "https://arxiv.org/pdf/2503.22537", "abs": "https://arxiv.org/abs/2503.22537", "authors": ["Remy Sabathier", "Niloy J. Mitra", "David Novotny"], "title": "LIM: Large Interpolator Model for Dynamic Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing dynamic assets from video data is central to many in computer\nvision and graphics tasks. Existing 4D reconstruction approaches are limited by\ncategory-specific models or slow optimization-based methods. Inspired by the\nrecent Large Reconstruction Model (LRM), we present the Large Interpolation\nModel (LIM), a transformer-based feed-forward solution, guided by a novel\ncausal consistency loss, for interpolating implicit 3D representations across\ntime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM produces\na deformed shape at any continuous time $t\\in[t_0,t_1]$, delivering\nhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicit\nmesh tracking across time, producing a consistently uv-textured mesh sequence\nready for integration into existing production pipelines. We also use LIM, in\nconjunction with a diffusion-based multiview generator, to produce dynamic 4D\nreconstructions from monocular videos. We evaluate LIM on various dynamic\ndatasets, benchmarking against image-space interpolation methods (e.g., FiLM)\nand direct triplane linear interpolation, and demonstrate clear advantages. In\nsummary, LIM is the first feed-forward model capable of high-speed tracked 4D\nasset reconstruction across diverse categories.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22622", "pdf": "https://arxiv.org/pdf/2503.22622", "abs": "https://arxiv.org/abs/2503.22622", "authors": ["Jangho Park", "Taesung Kwon", "Jong Chul Ye"], "title": "Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model", "categories": ["cs.CV"], "comment": "project page: https://zero4dvid.github.io/", "summary": "Recently, multi-view or 4D video generation has emerged as a significant\nresearch topic. Nonetheless, recent approaches to 4D generation still struggle\nwith fundamental limitations, as they primarily rely on harnessing multiple\nvideo diffusion models with additional training or compute-intensive training\nof a full 4D diffusion model with limited real-world 4D data and large\ncomputational costs. To address these challenges, here we propose the first\ntraining-free 4D video generation method that leverages the off-the-shelf video\ndiffusion models to generate multi-view videos from a single input video. Our\napproach consists of two key steps: (1) By designating the edge frames in the\nspatio-temporal sampling grid as key frames, we first synthesize them using a\nvideo diffusion model, leveraging a depth-based warping technique for guidance.\nThis approach ensures structural consistency across the generated frames,\npreserving spatial and temporal coherence. (2) We then interpolate the\nremaining frames using a video diffusion model, constructing a fully populated\nand temporally coherent sampling grid while preserving spatial and temporal\nconsistency. Through this approach, we extend a single video into a multi-view\nvideo along novel camera trajectories while maintaining spatio-temporal\nconsistency. Our method is training-free and fully utilizes an off-the-shelf\nvideo diffusion model, offering a practical and effective solution for\nmulti-view video generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22668", "pdf": "https://arxiv.org/pdf/2503.22668", "abs": "https://arxiv.org/abs/2503.22668", "authors": ["Sindhu B Hegde", "K R Prajwal", "Taein Kwon", "Andrew Zisserman"], "title": "Understanding Co-speech Gestures in-the-wild", "categories": ["cs.CV"], "comment": "Main paper - 11 pages, 4 figures, Supplementary - 5 pages, 4 figures", "summary": "Co-speech gestures play a vital role in non-verbal communication. In this\npaper, we introduce a new framework for co-speech gesture understanding in the\nwild. Specifically, we propose three new tasks and benchmarks to evaluate a\nmodel's capability to comprehend gesture-text-speech associations: (i)\ngesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker\ndetection using gestures. We present a new approach that learns a tri-modal\nspeech-text-video-gesture representation to solve these tasks. By leveraging a\ncombination of global phrase contrastive loss and local gesture-word coupling\nloss, we demonstrate that a strong gesture representation can be learned in a\nweakly supervised manner from videos in the wild. Our learned representations\noutperform previous methods, including large vision-language models (VLMs),\nacross all three tasks. Further analysis reveals that speech and text\nmodalities capture distinct gesture-related signals, underscoring the\nadvantages of learning a shared tri-modal embedding space. The dataset, model,\nand code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22676", "pdf": "https://arxiv.org/pdf/2503.22676", "abs": "https://arxiv.org/abs/2503.22676", "authors": ["Boyang", "Yu", "Yanlin Jin", "Ashok Veeraraghavan", "Akshat Dave", "Guha Balakrishnan"], "title": "TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We present TranSplat, a 3D scene rendering algorithm that enables realistic\ncross-scene object transfer (from a source to a target scene) based on the\nGaussian Splatting framework. Our approach addresses two critical challenges:\n(1) precise 3D object extraction from the source scene, and (2) faithful\nrelighting of the transferred object in the target scene without explicit\nmaterial property estimation. TranSplat fits a splatting model to the source\nscene, using 2D object masks to drive fine-grained 3D segmentation. Following\nuser-guided insertion of the object into the target scene, along with automatic\nrefinement of position and orientation, TranSplat derives per-Gaussian radiance\ntransfer functions via spherical harmonic analysis to adapt the object's\nappearance to match the target scene's lighting environment. This relighting\nstrategy does not require explicitly estimating physical scene properties such\nas BRDFs. Evaluated on several synthetic and real-world scenes and objects,\nTranSplat yields excellent 3D object extractions and relighting performance\ncompared to recent baseline methods and visually convincing cross-scene object\ntransfers. We conclude by discussing the limitations of the approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22677", "pdf": "https://arxiv.org/pdf/2503.22677", "abs": "https://arxiv.org/abs/2503.22677", "authors": ["Ruining Li", "Chuanxia Zheng", "Christian Rupprecht", "Andrea Vedaldi"], "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://ruiningli.com/dso", "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21860", "pdf": "https://arxiv.org/pdf/2503.21860", "abs": "https://arxiv.org/abs/2503.21860", "authors": ["Kailin Li", "Puhao Li", "Tengyu Liu", "Yuyang Li", "Siyuan Huang"], "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22015", "pdf": "https://arxiv.org/pdf/2503.22015", "abs": "https://arxiv.org/abs/2503.22015", "authors": ["Ali Zafari", "Xi Chen", "Shirin Jalali"], "title": "DeCompress: Denoising via Neural Compression", "categories": ["eess.IV", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Learning-based denoising algorithms achieve state-of-the-art performance\nacross various denoising tasks. However, training such models relies on access\nto large training datasets consisting of clean and noisy image pairs. On the\nother hand, in many imaging applications, such as microscopy, collecting ground\ntruth images is often infeasible. To address this challenge, researchers have\nrecently developed algorithms that can be trained without requiring access to\nground truth data. However, training such models remains computationally\nchallenging and still requires access to large noisy training samples. In this\nwork, inspired by compression-based denoising and recent advances in neural\ncompression, we propose a new compression-based denoising algorithm, which we\nname DeCompress, that i) does not require access to ground truth images, ii)\ndoes not require access to large training dataset - only a single noisy image\nis sufficient, iii) is robust to overfitting, and iv) achieves superior\nperformance compared with zero-shot or unsupervised learning-based denoisers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22122", "pdf": "https://arxiv.org/pdf/2503.22122", "abs": "https://arxiv.org/abs/2503.22122", "authors": ["Puzhen Yuan", "Angyuan Ma", "Yunchao Yao", "Huaxiu Yao", "Masayoshi Tomizuka", "Mingyu Ding"], "title": "REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nrobotic planning, particularly for long-horizon tasks that require a holistic\nunderstanding of the environment for task decomposition. Existing methods\ntypically rely on prior environmental knowledge or carefully designed\ntask-specific prompts, making them struggle with dynamic scene changes or\nunexpected task conditions, e.g., a robot attempting to put a carrot in the\nmicrowave but finds the door was closed. Such challenges underscore two\ncritical issues: adaptability and efficiency. To address them, in this work, we\npropose an adaptive multi-agent planning framework, termed REMAC, that enables\nefficient, scene-agnostic multi-robot long-horizon task planning and execution\nthrough continuous reflection and self-evolution. REMAC incorporates two key\nmodules: a self-reflection module performing pre-condition and post-condition\nchecks in the loop to evaluate progress and refine plans, and a self-evolvement\nmodule dynamically adapting plans based on scene-specific reasoning. It offers\nseveral appealing benefits: 1) Robots can initially explore and reason about\nthe environment without complex prompt design. 2) Robots can keep reflecting on\npotential planning errors and adapting the plan based on task-specific\ninsights. 3) After iterations, a robot can call another one to coordinate tasks\nin parallel, maximizing the task execution efficiency. To validate REMAC's\neffectiveness, we build a multi-agent environment for long-horizon robot\nmanipulation and navigation based on RoboCasa, featuring 4 task categories with\n27 task styles and 50+ different objects. Based on it, we further benchmark\nstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, and\nGrok3, demonstrating REMAC's superiority by boosting average success rates by\n40% and execution efficiency by 52.7% over the single robot baseline.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22263", "pdf": "https://arxiv.org/pdf/2503.22263", "abs": "https://arxiv.org/abs/2503.22263", "authors": ["Dongping Liao", "Xitong Gao", "Yabo Xu", "Chengzhong Xu"], "title": "FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning", "categories": ["cs.LG", "cs.CV"], "comment": "https://github.com/0-ml/flip", "summary": "The increasing emphasis on privacy and data security has driven the adoption\nof federated learning, a decentralized approach to train machine learning\nmodels without sharing raw data. Prompt learning, which fine-tunes prompt\nembeddings of pretrained models, offers significant advantages in federated\nsettings by reducing computational costs and communication overheads while\nleveraging the strong performance and generalization capabilities of\nvision-language models such as CLIP. This paper addresses the intersection of\nfederated learning and prompt learning, particularly for vision-language\nmodels. In this work, we introduce a comprehensive framework, named FLIP, to\nevaluate federated prompt learning algorithms. FLIP assesses the performance of\n8 state-of-the-art federated prompt learning methods across 4 federated\nlearning protocols and 12 open datasets, considering 6 distinct evaluation\nscenarios. Our findings demonstrate that prompt learning maintains strong\ngeneralization performance in both in-distribution and out-of-distribution\nsettings with minimal resource consumption. This work highlights the\neffectiveness of federated prompt learning in environments characterized by\ndata scarcity, unseen classes, and cross-domain distributional shifts. We\nopen-source the code for all implemented algorithms in FLIP to facilitate\nfurther research in this domain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22271", "pdf": "https://arxiv.org/pdf/2503.22271", "abs": "https://arxiv.org/abs/2503.22271", "authors": ["Omini Rathore", "Richard Paul", "Abigail Morrison", "Hanno Scharr", "Elisabeth Pfaehler"], "title": "Efficient Epistemic Uncertainty Estimation in Cerebrovascular Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Brain vessel segmentation of MR scans is a critical step in the diagnosis of\ncerebrovascular diseases. Due to the fine vessel structure, manual vessel\nsegmentation is time consuming. Therefore, automatic deep learning (DL) based\nsegmentation techniques are intensively investigated. As conventional DL models\nyield a high complexity and lack an indication of decision reliability, they\nare often considered as not trustworthy. This work aims to increase trust in DL\nbased models by incorporating epistemic uncertainty quantification into\ncerebrovascular segmentation models for the first time. By implementing an\nefficient ensemble model combining the advantages of Bayesian Approximation and\nDeep Ensembles, we aim to overcome the high computational costs of conventional\nprobabilistic networks. Areas of high model uncertainty and erroneous\npredictions are aligned which demonstrates the effectiveness and reliability of\nthe approach. We perform extensive experiments applying the ensemble model on\nout-of-distribution (OOD) data. We demonstrate that for OOD-images, the\nestimated uncertainty increases. Additionally, omitting highly uncertain areas\nimproves the segmentation quality, both for in- and out-of-distribution data.\nThe ensemble model explains its limitations in a reliable manner and can\nmaintain trustworthiness also for OOD data and could be considered in clinical\napplications", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22563", "pdf": "https://arxiv.org/pdf/2503.22563", "abs": "https://arxiv.org/abs/2503.22563", "authors": ["Pasquale Cascarano", "Lorenzo Stacchio", "Andrea Sebastiani", "Alessandro Benfenati", "Ulugbek S. Kamilov", "Gustavo Marfia"], "title": "RELD: Regularization by Latent Diffusion Models for Image Restoration", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, Diffusion Models have become the new state-of-the-art in\ndeep generative modeling, ending the long-time dominance of Generative\nAdversarial Networks. Inspired by the Regularization by Denoising principle, we\nintroduce an approach that integrates a Latent Diffusion Model, trained for the\ndenoising task, into a variational framework using Half-Quadratic Splitting,\nexploiting its regularization properties. This approach, under appropriate\nconditions that can be easily met in various imaging applications, allows for\nreduced computational cost while achieving high-quality results. The proposed\nstrategy, called Regularization by Latent Denoising (RELD), is then tested on a\ndataset of natural images, for image denoising, deblurring, and\nsuper-resolution tasks. The numerical experiments show that RELD is competitive\nwith other state-of-the-art methods, particularly achieving remarkable results\nwhen evaluated using perceptual quality metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22588", "pdf": "https://arxiv.org/pdf/2503.22588", "abs": "https://arxiv.org/abs/2503.22588", "authors": ["Heiko Renz", "Maximilian Krämer", "Frank Hoffmann", "Torsten Bertram"], "title": "Next-Best-Trajectory Planning of Robot Manipulators for Effective Observation and Exploration", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted for publication at the IEEE International Conference on\n  Robotics and Automation (ICRA), 2025", "summary": "Visual observation of objects is essential for many robotic applications,\nsuch as object reconstruction and manipulation, navigation, and scene\nunderstanding. Machine learning algorithms constitute the state-of-the-art in\nmany fields but require vast data sets, which are costly and time-intensive to\ncollect. Automated strategies for observation and exploration are crucial to\nenhance the efficiency of data gathering. Therefore, a novel strategy utilizing\nthe Next-Best-Trajectory principle is developed for a robot manipulator\noperating in dynamic environments. Local trajectories are generated to maximize\nthe information gained from observations along the path while avoiding\ncollisions. We employ a voxel map for environment modeling and utilize\nraycasting from perspectives around a point of interest to estimate the\ninformation gain. A global ergodic trajectory planner provides an optional\nreference trajectory to the local planner, improving exploration and helping to\navoid local minima. To enhance computational efficiency, raycasting for\nestimating the information gain in the environment is executed in parallel on\nthe graphics processing unit. Benchmark results confirm the efficiency of the\nparallelization, while real-world experiments demonstrate the strategy's\neffectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22592", "pdf": "https://arxiv.org/pdf/2503.22592", "abs": "https://arxiv.org/abs/2503.22592", "authors": ["Thomas Boucher", "Nicholas Tetlow", "Annie Fung", "Amy Dewar", "Pietro Arina", "Sven Kerneis", "John Whittle", "Evangelos B. Mazomenos"], "title": "KEVS: Enhancing Segmentation of Visceral Adipose Tissue in Pre-Cystectomy CT with Gaussian Kernel Density Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Preprint for submission to IPCAI special edition of IJCARS 2025,\n  version prior to any peer review", "summary": "Purpose: The distribution of visceral adipose tissue (VAT) in cystectomy\npatients is indicative of the incidence of post-operative complications.\nExisting VAT segmentation methods for computed tomography (CT) employing\nintensity thresholding have limitations relating to inter-observer variability.\nMoreover, the difficulty in creating ground-truth masks limits the development\nof deep learning (DL) models for this task. This paper introduces a novel\nmethod for VAT prediction in pre-cystectomy CT, which is fully automated and\ndoes not require ground-truth VAT masks for training, overcoming aforementioned\nlimitations. Methods: We introduce the Kernel density Enhanced VAT Segmentator\n( KEVS), combining a DL semantic segmentation model, for multi-body feature\nprediction, with Gaussian kernel density estimation analysis of predicted\nsubcutaneous adipose tissue to achieve accurate scan-specific predictions of\nVAT in the abdominal cavity. Uniquely for a DL pipeline, KEVS does not require\nground-truth VAT masks. Results: We verify the ability of KEVS to accurately\nsegment abdominal organs in unseen CT data and compare KEVS VAT segmentation\npredictions to existing state-of-the-art (SOTA) approaches in a dataset of 20\npre-cystectomy CT scans, collected from University College London Hospital\n(UCLH-Cyst), with expert ground-truth annotations. KEVS presents a 4.80% and\n6.02% improvement in Dice Coefficient over the second best DL and\nthresholding-based VAT segmentation techniques respectively when evaluated on\nUCLH-Cyst. Conclusion: This research introduces KEVS; an automated, SOTA method\nfor the prediction of VAT in pre-cystectomy CT which eliminates inter-observer\nvariability and is trained entirely on open-source CT datasets which do not\ncontain ground-truth VAT masks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22658", "pdf": "https://arxiv.org/pdf/2503.22658", "abs": "https://arxiv.org/abs/2503.22658", "authors": ["Frank J. Brooks", "Rucha Deshpande"], "title": "Evaluation of Machine-generated Biomedical Images via A Tally-based Similarity Measure", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages. Manuscript under review at IEEE. Data available at\n  https://doi.org/10.13012/B2IDB-2642688_V1", "summary": "Super-resolution, in-painting, whole-image generation, unpaired\nstyle-transfer, and network-constrained image reconstruction each include an\naspect of machine-learned image synthesis where the actual ground truth is not\nknown at time of use. It is generally difficult to quantitatively and\nauthoritatively evaluate the quality of synthetic images; however, in\nmission-critical biomedical scenarios robust evaluation is paramount. In this\nwork, all practical image-to-image comparisons really are relative\nqualifications, not absolute difference quantifications; and, therefore,\nmeaningful evaluation of generated image quality can be accomplished using the\nTversky Index, which is a well-established measure for assessing perceptual\nsimilarity. This evaluation procedure is developed and then demonstrated using\nmultiple image data sets, both real and simulated. The main result is that when\nthe subjectivity and intrinsic deficiencies of any feature-encoding choice are\nput upfront, Tversky's method leads to intuitive results, whereas traditional\nmethods based on summarizing distances in deep feature spaces do not.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
