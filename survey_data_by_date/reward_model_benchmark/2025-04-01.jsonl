{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "fine-grained", "dimension"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23131", "pdf": "https://arxiv.org/pdf/2503.23131", "abs": "https://arxiv.org/abs/2503.23131", "authors": ["Alexander Vogel", "Omar Moured", "Yufan Chen", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning", "categories": ["cs.CV"], "comment": "All models and code will be publicly available at\n  https://github.com/moured/RefChartQA", "summary": "Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps://github.com/moured/RefChartQA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768", "abs": "https://arxiv.org/abs/2503.23768", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance. (ii)\nFew-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits\nin improving font recognition accuracy across different VLMs. (iii) Attention\nanalysis sheds light on the inherent limitations of VLMs in capturing semantic\nfeatures.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23106", "pdf": "https://arxiv.org/pdf/2503.23106", "abs": "https://arxiv.org/abs/2503.23106", "authors": ["Chao Tao", "Dandan Zhong", "Weiliang Mu", "Zhuofei Du", "Haiyang Wu"], "title": "A large-scale image-text dataset benchmark for farmland segmentation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The traditional deep learning paradigm that solely relies on labeled data has\nlimitations in representing the spatial relationships between farmland elements\nand the surrounding environment.It struggles to effectively model the dynamic\ntemporal evolution and spatial heterogeneity of farmland. Language,as a\nstructured knowledge carrier,can explicitly express the spatiotemporal\ncharacteristics of farmland, such as its shape, distribution,and surrounding\nenvironmental information.Therefore,a language-driven learning paradigm can\neffectively alleviate the challenges posed by the spatiotemporal heterogeneity\nof farmland.However,in the field of remote sensing imagery of farmland,there is\ncurrently no comprehensive benchmark dataset to support this research\ndirection.To fill this gap,we introduced language based descriptions of\nfarmland and developed FarmSeg-VL dataset,the first fine-grained image-text\ndataset designed for spatiotemporal farmland segmentation.Firstly, this article\nproposed a semi-automatic annotation method that can accurately assign caption\nto each image, ensuring high data quality and semantic richness while improving\nthe efficiency of dataset construction.Secondly,the FarmSeg-VL exhibits\nsignificant spatiotemporal characteristics.In terms of the temporal\ndimension,it covers all four seasons.In terms of the spatial dimension,it\ncovers eight typical agricultural regions across China.In addition, in terms of\ncaptions,FarmSeg-VL covers rich spatiotemporal characteristics of\nfarmland,including its inherent properties,phenological characteristics,\nspatial distribution,topographic and geomorphic features,and the distribution\nof surrounding environments.Finally,we present a performance analysis of VLMs\nand the deep learning models that rely solely on labels trained on the\nFarmSeg-VL,demonstrating its potential as a standard benchmark for farmland\nsegmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "fine-grained", "dimension"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23768", "pdf": "https://arxiv.org/pdf/2503.23768", "abs": "https://arxiv.org/abs/2503.23768", "authors": ["Zhecheng Li", "Guoxian Song", "Yujun Cai", "Zhen Xiong", "Junsong Yuan", "Yiwei Wang"], "title": "Texture or Semantics? Vision-Language Models Get Lost in Font Recognition", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguistic\ncapabilities, achieving impressive performance in various tasks such as image\nrecognition and object localization. However, their effectiveness in\nfine-grained tasks remains an open question. In everyday scenarios, individuals\nencountering design materials, such as magazines, typography tutorials,\nresearch papers, or branding content, may wish to identify aesthetically\npleasing fonts used in the text. Given their multimodal capabilities and free\naccessibility, many VLMs are often considered potential tools for font\nrecognition. This raises a fundamental question: Do VLMs truly possess the\ncapability to recognize fonts? To investigate this, we introduce the Font\nRecognition Benchmark (FRB), a compact and well-structured dataset comprising\n15 commonly used fonts. FRB includes two versions: (i) an easy version, where\n10 sentences are rendered in different fonts, and (ii) a hard version, where\neach text sample consists of the names of the 15 fonts themselves, introducing\na stroop effect that challenges model perception. Through extensive evaluation\nof various VLMs on font recognition tasks, we arrive at the following key\nfindings: (i) Current VLMs exhibit limited font recognition capabilities, with\nmany state-of-the-art models failing to achieve satisfactory performance. (ii)\nFew-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefits\nin improving font recognition accuracy across different VLMs. (iii) Attention\nanalysis sheds light on the inherent limitations of VLMs in capturing semantic\nfeatures.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22954", "pdf": "https://arxiv.org/pdf/2503.22954", "abs": "https://arxiv.org/abs/2503.22954", "authors": ["Xinyu Yao", "Aditya Sannabhadti", "Holly Wiberg", "Karmel S. Shehadeh", "Rema Padman"], "title": "Can LLMs Support Medical Knowledge Imputation? An Evaluation-Based Perspective", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "10 pages, 3 figures, AMIA", "summary": "Medical knowledge graphs (KGs) are essential for clinical decision support\nand biomedical research, yet they often exhibit incompleteness due to knowledge\ngaps and structural limitations in medical coding systems. This issue is\nparticularly evident in treatment mapping, where coding systems such as ICD,\nMondo, and ATC lack comprehensive coverage, resulting in missing or\ninconsistent associations between diseases and their potential treatments. To\naddress this issue, we have explored the use of Large Language Models (LLMs)\nfor imputing missing treatment relationships. Although LLMs offer promising\ncapabilities in knowledge augmentation, their application in medical knowledge\nimputation presents significant risks, including factual inaccuracies,\nhallucinated associations, and instability between and within LLMs. In this\nstudy, we systematically evaluate LLM-driven treatment mapping, assessing its\nreliability through benchmark comparisons. Our findings highlight critical\nlimitations, including inconsistencies with established clinical guidelines and\npotential risks to patient safety. This study serves as a cautionary guide for\nresearchers and practitioners, underscoring the importance of critical\nevaluation and hybrid approaches when leveraging LLMs to enhance treatment\nmappings on medical knowledge graphs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety", "reliability"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22976", "pdf": "https://arxiv.org/pdf/2503.22976", "abs": "https://arxiv.org/abs/2503.22976", "authors": ["Jiahui Zhang", "Yurui Chen", "Yanpeng Zhou", "Yueming Xu", "Ze Huang", "Jilin Mei", "Junhui Chen", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "categories": ["cs.CV"], "comment": "Project page: https://fudan-zvg.github.io/spar", "summary": "Recent advances in LVLMs have improved vision-language understanding, but\nthey still struggle with spatial perception, limiting their ability to reason\nabout complex 3D scenes. Unlike previous approaches that incorporate 3D\nrepresentations into models to improve spatial understanding, we aim to unlock\nthe potential of VLMs by leveraging spatially relevant image data. To this end,\nwe introduce a novel 2D spatial data generation and annotation pipeline built\nupon scene data with 3D ground-truth. This pipeline enables the creation of a\ndiverse set of spatial tasks, ranging from basic perception tasks to more\ncomplex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a\nlarge-scale dataset generated from thousands of scenes across multiple public\ndatasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a\nmore comprehensive evaluation of spatial capabilities compared to existing\nspatial benchmarks, supporting both single-view and multi-view inputs. Training\non both SPAR-7M and large-scale 2D datasets enables our models to achieve\nstate-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on\n3D task-specific datasets yields competitive results, underscoring the\neffectiveness of our dataset in enhancing spatial reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23062", "pdf": "https://arxiv.org/pdf/2503.23062", "abs": "https://arxiv.org/abs/2503.23062", "authors": ["Sagi Eppel", "Mor Bismut", "Alona Faktor"], "title": "Shape and Texture Recognition in Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Shape and texture recognition is fundamental to visual perception. The\nability to identify shapes regardless of orientation, texture, or context, and\nto recognize textures independently of their associated objects, is essential\nfor general visual understanding of the world. We introduce the Large Shape &\nTextures dataset (LAS&T), a giant collection of diverse shapes and textures\nautomatically extracted from real-world images. This dataset is used to\nevaluate how effectively leading Large Vision-Language Models (LVLMs)\nunderstand shapes, textures, and materials in both 2D and 3D scenes. For shape\nrecognition, we test models' ability to match identical shapes that differ in\norientation, texture, color, or environment. Our results show that LVLMs' shape\nidentification capabilities remain significantly below human performance.\nSingle alterations (orientation, texture) cause minor decreases in matching\naccuracy, while multiple changes precipitate dramatic drops. LVLMs appear to\nrely predominantly on high-level and semantic features and struggle with\nabstract shapes lacking clear class associations. For texture and material\nrecognition, we evaluate models' ability to identify identical textures and\nmaterials across different objects and environments. Interestingly, leading\nLVLMs approach human-level performance in recognizing materials in 3D scenes,\nyet substantially underperform humans when identifying simpler 2D textures. The\nLAS&T dataset and benchmark, the largest and most diverse resource for shape\nand texture evaluation, is freely available with generation and testing\nscripts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23330", "pdf": "https://arxiv.org/pdf/2503.23330", "abs": "https://arxiv.org/abs/2503.23330", "authors": ["Hongxiang Jiang", "Jihao Yin", "Qixiong Wang", "Jiaqi Feng", "Guo Chen"], "title": "EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nimpressive results in various visual tasks. However, in remote sensing (RS),\nhigh resolution and small proportion of objects pose challenges to existing\nMLLMs, which struggle with object-centric tasks, particularly in precise\nlocalization and fine-grained attribute description for each object. These RS\nMLLMs have not yet surpassed classical visual perception models, as they only\nprovide coarse image understanding, leading to limited gains in real-world\nscenarios. To address this gap, we establish EagleVision, an MLLM tailored for\nremote sensing that excels in object detection and attribute comprehension.\nEquipped with the Attribute Disentangle module, EagleVision learns\ndisentanglement vision tokens to express distinct attributes. To support\nobject-level visual-language alignment, we construct EVAttrs-95K, the first\nlarge-scale object attribute understanding dataset in RS for instruction\ntuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves\nstate-of-the-art performance on both fine-grained object detection and object\nattribute understanding tasks, highlighting the mutual promotion between\ndetection and understanding capabilities in MLLMs. The code, model, data, and\ndemo will be available at https://github.com/XiangTodayEatsWhat/EagleVision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24115", "pdf": "https://arxiv.org/pdf/2503.24115", "abs": "https://arxiv.org/abs/2503.24115", "authors": ["Zhiming Ma", "Peidong Wang", "Minhua Huang", "Jingpeng Wang", "Kai Wu", "Xiangzhao Lv", "Yachun Pang", "Yin Yang", "Wenjie Tang", "Yuchen Kang"], "title": "TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "The detection of telecom fraud faces significant challenges due to the lack\nof high-quality multimodal training data that integrates audio signals with\nreasoning-oriented textual analysis. To address this gap, we present\nTeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset\nspecifically designed for automated telecom fraud analysis. Our dataset is\nconstructed through three strategies: (1) Privacy-preserved text-truth sample\ngeneration using automatically speech recognition (ASR)-transcribed call\nrecordings (with anonymized original audio), ensuring real-world consistency\nthrough text-to-speech (TTS) model regeneration; (2) Semantic enhancement via\nlarge language model (LLM)-based self-instruction sampling on authentic ASR\noutputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that\nsimulates emerging fraud tactics through predefined communication scenarios and\nfraud typologies. The generated dataset contains 28,511 rigorously processed\nspeech-text pairs, complete with detailed annotations for fraud reasoning. The\ndataset is divided into three tasks: scenario classification, fraud detection,\nfraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a\nstandardized evaluation benchmark comprising proportionally sampled instances\nfrom the dataset, to facilitate systematic testing of model performance on\ntelecom fraud detection tasks. We also contribute a production-optimized\nsupervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while\nopen-sourcing the data processing framework to enable community-driven dataset\nexpansion. This work establishes a foundational framework for multimodal\nanti-fraud research while addressing critical challenges in data privacy and\nscenario diversity. The project will be released at\nhttps://github.com/JimmyMa99/TeleAntiFraud.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23519", "pdf": "https://arxiv.org/pdf/2503.23519", "abs": "https://arxiv.org/abs/2503.23519", "authors": ["Haruya Ishikawa", "Yoshimitsu Aoki"], "title": "BoundMatch: Boundary detection applied to semi-supervised segmentation for urban-driving scenes", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Semi-supervised semantic segmentation (SS-SS) aims to mitigate the heavy\nannotation burden of dense pixel labeling by leveraging abundant unlabeled\nimages alongside a small labeled set. While current teacher-student consistency\nregularization methods achieve strong results, they often overlook a critical\nchallenge: the precise delineation of object boundaries. In this paper, we\npropose BoundMatch, a novel multi-task SS-SS framework that explicitly\nintegrates semantic boundary detection into the consistency regularization\npipeline. Our core mechanism, Boundary Consistency Regularized Multi-Task\nLearning (BCRM), enforces prediction agreement between teacher and student\nmodels on both segmentation masks and detailed semantic boundaries. To further\nenhance performance and sharpen contours, BoundMatch incorporates two\nlightweight fusion modules: Boundary-Semantic Fusion (BSF) injects learned\nboundary cues into the segmentation decoder, while Spatial Gradient Fusion\n(SGF) refines boundary predictions using mask gradients, leading to\nhigher-quality boundary pseudo-labels. This framework is built upon SAMTH, a\nstrong teacher-student baseline featuring a Harmonious Batch Normalization\n(HBN) update strategy for improved stability. Extensive experiments on diverse\ndatasets including Cityscapes, BDD100K, SYNTHIA, ADE20K, and Pascal VOC show\nthat BoundMatch achieves competitive performance against state-of-the-art\nmethods while significantly improving boundary-specific evaluation metrics. We\nalso demonstrate its effectiveness in realistic large-scale unlabeled data\nscenarios and on lightweight architectures designed for mobile deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "agreement", "consistency"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23587", "pdf": "https://arxiv.org/pdf/2503.23587", "abs": "https://arxiv.org/abs/2503.23587", "authors": ["Martin Malenický", "Martin Cífka", "Médéric Fourmy", "Louis Montaut", "Justin Carpentier", "Josef Sivic", "Vladimir Petrik"], "title": "PhysPose: Refining 6D Object Poses with Physical Constraints", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://data.ciirc.cvut.cz/public/projects/2025PhysPose", "summary": "Accurate 6D object pose estimation from images is a key problem in\nobject-centric scene understanding, enabling applications in robotics,\naugmented reality, and scene reconstruction. Despite recent advances, existing\nmethods often produce physically inconsistent pose estimates, hindering their\ndeployment in real-world scenarios. We introduce PhysPose, a novel approach\nthat integrates physical reasoning into pose estimation through a\npostprocessing optimization enforcing non-penetration and gravitational\nconstraints. By leveraging scene geometry, PhysPose refines pose estimates to\nensure physical plausibility. Our approach achieves state-of-the-art accuracy\non the YCB-Video dataset from the BOP benchmark and improves over the\nstate-of-the-art pose estimation methods on the HOPE-Video dataset.\nFurthermore, we demonstrate its impact in robotics by significantly improving\nsuccess rates in a challenging pick-and-place task, highlighting the importance\nof physical consistency in real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23339", "pdf": "https://arxiv.org/pdf/2503.23339", "abs": "https://arxiv.org/abs/2503.23339", "authors": ["Neil Mallinar", "A. Ali Heydari", "Xin Liu", "Anthony Z. Faranesh", "Brent Winslow", "Nova Hammerquist", "Benjamin Graef", "Cathy Speed", "Mark Malhotra", "Shwetak Patel", "Javier L. Prieto", "Daniel McDuff", "Ahmed A. Metwally"], "title": "A Scalable Framework for Evaluating Health Language Models", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for analyzing\ncomplex datasets. Recent studies demonstrate their potential to generate\nuseful, personalized responses when provided with patient-specific health\ninformation that encompasses lifestyle, biomarkers, and context. As LLM-driven\nhealth applications are increasingly adopted, rigorous and efficient one-sided\nevaluation methodologies are crucial to ensure response quality across multiple\ndimensions, including accuracy, personalization and safety. Current evaluation\npractices for open-ended text responses heavily rely on human experts. This\napproach introduces human factors and is often cost-prohibitive,\nlabor-intensive, and hinders scalability, especially in complex domains like\nhealthcare where response assessment necessitates domain expertise and\nconsiders multifaceted patient data. In this work, we introduce Adaptive\nPrecise Boolean rubrics: an evaluation framework that streamlines human and\nautomated evaluation of open-ended questions by identifying gaps in model\nresponses using a minimal set of targeted rubrics questions. Our approach is\nbased on recent work in more general evaluation settings that contrasts a\nsmaller set of complex evaluation targets with a larger set of more precise,\ngranular targets answerable with simple boolean responses. We validate this\napproach in metabolic health, a domain encompassing diabetes, cardiovascular\ndisease, and obesity. Our results demonstrate that Adaptive Precise Boolean\nrubrics yield higher inter-rater agreement among expert and non-expert human\nevaluators, and in automated assessments, compared to traditional Likert\nscales, while requiring approximately half the evaluation time of Likert-based\nmethods. This enhanced efficiency, particularly in automated evaluation and\nnon-expert contributions, paves the way for more extensive and cost-effective\nevaluation of LLMs in health.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "agreement", "accuracy"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23660", "pdf": "https://arxiv.org/pdf/2503.23660", "abs": "https://arxiv.org/abs/2503.23660", "authors": ["Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Current movie dubbing technology can generate the desired voice from a given\nspeech prompt, ensuring good synchronization between speech and visuals while\naccurately conveying the intended emotions. However, in movie dubbing, key\naspects such as adapting to different dubbing styles, handling dialogue,\nnarration, and monologue effectively, and understanding subtle details like the\nage and gender of speakers, have not been well studied. To address this\nchallenge, we propose a framework of multi-modal large language model. First,\nit utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visual\ninputs to understand dubbing styles and fine-grained attributes. Second, it\ngenerates high-quality dubbing through large speech generation models, guided\nby multimodal conditions. Additionally, we have developed a movie dubbing\ndataset with CoT annotations. The evaluation results demonstrate a performance\nimprovement over state-of-the-art methods across multiple datasets. In\nparticular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from\n82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animation\ndataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for\ndubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 and\nWER decreases from 52.69% to 23.20% for initial reasoning setting on proposed\nCoT-Movie-Dubbing dataset in the comparison with the state-of-the art models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "dialogue", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23730", "pdf": "https://arxiv.org/pdf/2503.23730", "abs": "https://arxiv.org/abs/2503.23730", "authors": ["Yoonshik Kim", "Jaeyoon Jung"], "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches", "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering", "criteria"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23730", "pdf": "https://arxiv.org/pdf/2503.23730", "abs": "https://arxiv.org/abs/2503.23730", "authors": ["Yoonshik Kim", "Jaeyoon Jung"], "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches", "summary": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering", "criteria"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23911", "pdf": "https://arxiv.org/pdf/2503.23911", "abs": "https://arxiv.org/abs/2503.23911", "authors": ["Ruisheng Han", "Kanglei Zhou", "Amir Atapour-Abarghouei", "Xiaohui Liang", "Hubert P. H. Shum"], "title": "FineCausal: A Causal-Based Framework for Interpretable Fine-Grained Action Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Action quality assessment (AQA) is critical for evaluating athletic\nperformance, informing training strategies, and ensuring safety in competitive\nsports. However, existing deep learning approaches often operate as black boxes\nand are vulnerable to spurious correlations, limiting both their reliability\nand interpretability. In this paper, we introduce FineCausal, a novel\ncausal-based framework that achieves state-of-the-art performance on the\nFineDiving-HM dataset. Our approach leverages a Graph Attention Network-based\ncausal intervention module to disentangle human-centric foreground cues from\nbackground confounders, and incorporates a temporal causal attention module to\ncapture fine-grained temporal dependencies across action stages. This\ndual-module strategy enables FineCausal to generate detailed spatio-temporal\nrepresentations that not only achieve state-of-the-art scoring performance but\nalso provide transparent, interpretable feedback on which features drive the\nassessment. Despite its strong performance, FineCausal requires extensive\nexpert knowledge to define causal structures and depends on high-quality\nannotations, challenges that we discuss and address as future research\ndirections. Code is available at https://github.com/Harrison21/FineCausal.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "reliability", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23312", "pdf": "https://arxiv.org/pdf/2503.23312", "abs": "https://arxiv.org/abs/2503.23312", "authors": ["Hyunsik Jeon", "Satoshi Koide", "Yu Wang", "Zhankui He", "Julian McAuley"], "title": "LaViC: Adapting Large Vision-Language Models to Visually-Aware Conversational Recommendation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Conversational recommender systems engage users in dialogues to refine their\nneeds and provide more personalized suggestions. Although textual information\nsuffices for many domains, visually driven categories such as fashion or home\ndecor potentially require detailed visual information related to color, style,\nor design. To address this challenge, we propose LaViC (Large Vision-Language\nConversational Recommendation Framework), a novel approach that integrates\ncompact image representations into dialogue-based recommendation systems. LaViC\nleverages a large vision-language model in a two-stage process: (1) visual\nknowledge self-distillation, which condenses product images from hundreds of\ntokens into a small set of visual tokens in a self-distillation manner,\nsignificantly reducing computational overhead, and (2) recommendation prompt\ntuning, which enables the model to incorporate both dialogue context and\ndistilled visual tokens, providing a unified mechanism for capturing textual\nand visual features. To support rigorous evaluation of visually-aware\nconversational recommendation, we construct a new dataset by aligning Reddit\nconversations with Amazon product listings across multiple visually oriented\ncategories (e.g., fashion, beauty, and home). This dataset covers realistic\nuser queries and product appearances in domains where visual details are\ncrucial. Extensive experiments demonstrate that LaViC significantly outperforms\ntext-only conversational recommendation methods and open-source vision-language\nbaselines. Moreover, LaViC achieves competitive or superior accuracy compared\nto prominent proprietary baselines (e.g., GPT-3.5-turbo, GPT-4o-mini, and\nGPT-4o), demonstrating the necessity of explicitly using visual data for\ncapturing product attributes and showing the effectiveness of our\nvision-language integration. Our code and dataset are available at\nhttps://github.com/jeon185/LaViC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22698", "pdf": "https://arxiv.org/pdf/2503.22698", "abs": "https://arxiv.org/abs/2503.22698", "authors": ["Basab Jha", "Firoj Paudel"], "title": "Fragile Mastery: Are Domain-Specific Trade-Offs Undermining On-Device Language Models?", "categories": ["cs.CL"], "comment": "14 Pages, 5 figures", "summary": "The application of on-device language models (ODLMs) on resource-constrained\nedge devices is a multi-dimensional problem that strikes a fine balance between\ncomputational effectiveness, memory, power usage, and linguistic capacity\nacross heterogeneous tasks. This holistic study conducts a thorough\ninvestigation of the trade-offs between domain-specific optimization and\ncross-domain robustness, culminating in the proposal of the Generalized Edge\nModel (GEM), a new architecture that aims to balance specialization and\ngeneralization in a harmonious manner. With a rigorous experimental approach\ntesting 47 well-chosen benchmarks in eight domains--healthcare, law, finance,\nSTEM, commonsense, conversational AI, multilingual, and domain-adaptive\ntasks--we show that conventional optimization techniques decrease target task\nperplexity by 18-25% but result in a precipitous decline in general-task\nperformance with F1 scores decreasing by 12-29%, as reported by Liu et al. GEM\nemploys a Sparse Cross-Attention Router (SCAR) to dynamically allocate\ncomputation to a variable number of computing resources with a cross-domain F1\naccuracy of 0.89 on less than 100ms latency across Raspberry Pi 4, Pixel 6,\niPhone 13, and bespoke custom neural processing units (NPUs). Compared to GPT-4\nLite, GEM enhances the general-task level by 7% with respect and parity in\ndomain-specific performance. We propose three new measurement tools--Domain\nSpecialization Index (DSI), Generalization Gap (GG), and Cross-Domain Transfer\nRatio (CDTR)--which show strong correlation between model compression intensity\nand brittleness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22881", "pdf": "https://arxiv.org/pdf/2503.22881", "abs": "https://arxiv.org/abs/2503.22881", "authors": ["Lauren Shrack", "Timm Haucke", "Antoine Salaün", "Arjun Subramonian", "Sara Beery"], "title": "Pairwise Matching of Intermediate Representations for Fine-grained Explainability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22884", "pdf": "https://arxiv.org/pdf/2503.22884", "abs": "https://arxiv.org/abs/2503.22884", "authors": ["Yi-Ting Shen", "Sungmin Eum", "Doheon Lee", "Rohit Shete", "Chiao-Yi Wang", "Heesung Kwon", "Shuvra S. Bhattacharyya"], "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22965", "pdf": "https://arxiv.org/pdf/2503.22965", "abs": "https://arxiv.org/abs/2503.22965", "authors": ["Henri Mueller", "Yechan Kim", "Trevor Gee", "Mahla Nejati"], "title": "Pallet Detection And Localisation From Synthetic Data", "categories": ["cs.CV"], "comment": "10 pages, 9 images, 4 tables, submitted and accepted to ACRA 2024\n  (https://www.araa.asn.au/conference/acra-2024/)", "summary": "The global warehousing industry is experiencing rapid growth, with the market\nsize projected to grow at an annual rate of 8.1% from 2024 to 2030 [Grand View\nResearch, 2021]. This expansion has led to a surge in demand for efficient\npallet detection and localisation systems. While automation can significantly\nstreamline warehouse operations, the development of such systems often requires\nextensive manual data annotation, with an average of 35 seconds per image, for\na typical computer vision project. This paper presents a novel approach to\nenhance pallet detection and localisation using purely synthetic data and\ngeometric features derived from their side faces. By implementing a domain\nrandomisation engine in Unity, the need for time-consuming manual annotation is\neliminated while achieving high-performance results. The proposed method\ndemonstrates a pallet detection performance of 0.995 mAP50 for single pallets\non a real-world dataset. Additionally, an average position accuracy of less\nthan 4.2 cm and an average rotation accuracy of 8.2{\\deg} were achieved for\npallets within a 5-meter range, with the pallet positioned head-on.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23243", "pdf": "https://arxiv.org/pdf/2503.23243", "abs": "https://arxiv.org/abs/2503.23243", "authors": ["Megan A. Brown", "Shubham Atreja", "Libby Hemphill", "Patrick Y. Wu"], "title": "Evaluating how LLM annotations represent diverse views on contentious topics", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Researchers have proposed the use of generative large language models (LLMs)\nto label data for both research and applied settings. This literature\nemphasizes the improved performance of LLMs relative to other natural language\nmodels, noting that LLMs typically outperform other models on standard metrics\nsuch as accuracy, precision, recall, and F1 score. However, previous literature\nhas also highlighted the bias embedded in language models, particularly around\ncontentious topics such as potentially toxic content. This bias could result in\nlabels applied by LLMs that disproportionately align with majority groups over\na more diverse set of viewpoints. In this paper, we evaluate how LLMs represent\ndiverse viewpoints on these contentious tasks. Across four annotation tasks on\nfour datasets, we show that LLMs do not show substantial disagreement with\nannotators on the basis of demographics. Instead, the model, prompt, and\ndisagreement between human annotators on the labeling task are far more\npredictive of LLM agreement. Our findings suggest that when using LLMs to\nannotate data, under-representing the views of particular groups is not a\nsubstantial concern. We conclude with a discussion of the implications for\nresearchers and practitioners.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23035", "pdf": "https://arxiv.org/pdf/2503.23035", "abs": "https://arxiv.org/abs/2503.23035", "authors": ["Yuxiang Bao", "Huijie Liu", "Xun Gao", "Huan Fu", "Guoliang Kang"], "title": "FreeInv: Free Lunch for Improving DDIM Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Naive DDIM inversion process usually suffers from a trajectory deviation\nissue, i.e., the latent trajectory during reconstruction deviates from the one\nduring inversion. To alleviate this issue, previous methods either learn to\nmitigate the deviation or design cumbersome compensation strategy to reduce the\nmismatch error, exhibiting substantial time and computation cost. In this work,\nwe present a nearly free-lunch method (named FreeInv) to address the issue more\neffectively and efficiently. In FreeInv, we randomly transform the latent\nrepresentation and keep the transformation the same between the corresponding\ninversion and reconstruction time-step. It is motivated from a statistical\nperspective that an ensemble of DDIM inversion processes for multiple\ntrajectories yields a smaller trajectory mismatch error on expectation.\nMoreover, through theoretical analysis and empirical study, we show that\nFreeInv performs an efficient ensemble of multiple trajectories. FreeInv can be\nfreely integrated into existing inversion-based image and video editing\ntechniques. Especially for inverting video sequences, it brings more\nsignificant fidelity and efficiency improvements. Comprehensive quantitative\nand qualitative evaluation on PIE benchmark and DAVIS dataset shows that\nFreeInv remarkably outperforms conventional DDIM inversion, and is competitive\namong previous state-of-the-art inversion methods, with superior computation\nefficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23439", "pdf": "https://arxiv.org/pdf/2503.23439", "abs": "https://arxiv.org/abs/2503.23439", "authors": ["Hyunjong Ok", "Suho Yoo", "Jaeho Lee"], "title": "Speculative End-Turn Detector for Efficient Speech Chatbot Assistant", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint", "summary": "Spoken dialogue systems powered by large language models have demonstrated\nremarkable abilities in understanding human speech and generating appropriate\nspoken responses. However, these systems struggle with end-turn detection (ETD)\n-- the ability to distinguish between user turn completion and hesitation. This\nlimitation often leads to premature or delayed responses, disrupting the flow\nof spoken conversations. In this paper, we introduce the ETD Dataset, the first\npublic dataset for end-turn detection. The ETD dataset consists of both\nsynthetic speech data generated with text-to-speech models and real-world\nspeech data collected from web sources. We also propose SpeculativeETD, a novel\ncollaborative inference framework that balances efficiency and accuracy to\nimprove real-time ETD in resource-constrained environments. Our approach\njointly employs a lightweight GRU-based model, which rapidly detects the\nnon-speaking units in real-time on local devices, and a high-performance\nWav2vec-based model running on the server to make a more challenging\nclassification of distinguishing turn ends from mere pauses. Experiments\ndemonstrate that the proposed SpeculativeETD significantly improves ETD\naccuracy while keeping the required computations low. Datasets and code will be\navailable after the review.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "dialogue"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23512", "pdf": "https://arxiv.org/pdf/2503.23512", "abs": "https://arxiv.org/abs/2503.23512", "authors": ["Qiang Yi", "Yangfan He", "Jianhui Wang", "Xinyuan Song", "Shiyao Qian", "Miao Zhang", "Li Sun", "Tianyu Shi"], "title": "SCORE: Story Coherence and Retrieval Enhancement for AI Narratives", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at generating creative narratives but\nstruggle with long-term coherence and emotional consistency in complex stories.\nTo address this, we propose SCORE (Story Coherence and Retrieval Enhancement),\na framework integrating three components: 1) Dynamic State Tracking (monitoring\nobjects/characters via symbolic logic), 2) Context-Aware Summarization\n(hierarchical episode summaries for temporal progression), and 3) Hybrid\nRetrieval (combining TF-IDF keyword relevance with cosine similarity-based\nsemantic embeddings). The system employs a temporally-aligned\nRetrieval-Augmented Generation (RAG) pipeline to validate contextual\nconsistency. Evaluations show SCORE achieves 23.6% higher coherence (NCI-2.0\nbenchmark), 89.7% emotional consistency (EASM metric), and 41.8% fewer\nhallucinations versus baseline GPT models. Its modular design supports\nincremental knowledge graph construction for persistent story memory and\nmulti-LLM backend compatibility, offering an explainable solution for\nindustrial-scale narrative systems requiring long-term consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "summarization"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23514", "pdf": "https://arxiv.org/pdf/2503.23514", "abs": "https://arxiv.org/abs/2503.23514", "authors": ["Siqi Fan", "Xiusheng Huang", "Yiqun Yao", "Xuezhi Fang", "Kang Liu", "Peng Han", "Shuo Shang", "Aixin Sun", "Yequan Wang"], "title": "If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can carry out human-like dialogue, but unlike\nhumans, they are stateless due to the superposition property. However, during\nmulti-turn, multi-agent interactions, LLMs begin to exhibit consistent,\ncharacter-like behaviors, hinting at a form of emergent lifelong learning.\nDespite this, existing benchmarks often fail to capture these dynamics,\nprimarily focusing on static, open-ended evaluations. To address this gap, we\nintroduce LIFESTATE-BENCH, a benchmark designed to assess lifelong learning in\nLLMs. It features two episodic datasets: Hamlet and a synthetic script\ncollection, rich in narrative structure and character interactions. Our fact\nchecking evaluation probes models' self-awareness, episodic memory retrieval,\nand relationship tracking, across both parametric and non-parametric\napproaches. Experiments on models like Llama3.1-8B, GPT-4-turbo, and DeepSeek\nR1, we demonstrate that nonparametric methods significantly outperform\nparametric ones in managing stateful learning. However, all models exhibit\nchallenges with catastrophic forgetting as interactions extend, highlighting\nthe need for further advancements in lifelong learning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dialogue"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23214", "pdf": "https://arxiv.org/pdf/2503.23214", "abs": "https://arxiv.org/abs/2503.23214", "authors": ["Vincent Gbouna Zakka", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Action Recognition in Real-World Ambient Assisted Living Environment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing ageing population and their preference to maintain independence\nby living in their own homes require proactive strategies to ensure safety and\nsupport. Ambient Assisted Living (AAL) technologies have emerged to facilitate\nageing in place by offering continuous monitoring and assistance within the\nhome. Within AAL technologies, action recognition plays a crucial role in\ninterpreting human activities and detecting incidents like falls, mobility\ndecline, or unusual behaviours that may signal worsening health conditions.\nHowever, action recognition in practical AAL applications presents challenges,\nincluding occlusions, noisy data, and the need for real-time performance. While\nadvancements have been made in accuracy, robustness to noise, and computation\nefficiency, achieving a balance among them all remains a challenge. To address\nthis challenge, this paper introduces the Robust and Efficient Temporal\nConvolution network (RE-TCN), which comprises three main elements: Adaptive\nTemporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data\naugmentation techniques. These elements aim to enhance the model's accuracy,\nrobustness against noise and occlusion, and computational efficiency within\nreal-world AAL contexts. RE-TCN outperforms existing models in terms of\naccuracy, noise and occlusion robustness, and has been validated on four\nbenchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28.\nThe code is publicly available at: https://github.com/Gbouna/RE-TCN", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23257", "pdf": "https://arxiv.org/pdf/2503.23257", "abs": "https://arxiv.org/abs/2503.23257", "authors": ["Mohammadmahdi Honarmand", "Onur Cezmi Mutlu", "Parnian Azizian", "Saimourya Surabhi", "Dennis P. Wall"], "title": "FIESTA: Fisher Information-based Efficient Selective Test-time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust facial expression recognition in unconstrained, \"in-the-wild\"\nenvironments remains challenging due to significant domain shifts between\ntraining and testing distributions. Test-time adaptation (TTA) offers a\npromising solution by adapting pre-trained models during inference without\nrequiring labeled test data. However, existing TTA approaches typically rely on\nmanually selecting which parameters to update, potentially leading to\nsuboptimal adaptation and high computational costs. This paper introduces a\nnovel Fisher-driven selective adaptation framework that dynamically identifies\nand updates only the most critical model parameters based on their importance\nas quantified by Fisher information. By integrating this principled parameter\nselection approach with temporal consistency constraints, our method enables\nefficient and effective adaptation specifically tailored for video-based facial\nexpression recognition. Experiments on the challenging AffWild2 benchmark\ndemonstrate that our approach significantly outperforms existing TTA methods,\nachieving a 7.7% improvement in F1 score over the base model while adapting\nonly 22,000 parameters-more than 20 times fewer than comparable methods. Our\nablation studies further reveal that parameter importance can be effectively\nestimated from minimal data, with sampling just 1-3 frames sufficient for\nsubstantial performance gains. The proposed approach not only enhances\nrecognition accuracy but also dramatically reduces computational overhead,\nmaking test-time adaptation more practical for real-world affective computing\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23307", "pdf": "https://arxiv.org/pdf/2503.23307", "abs": "https://arxiv.org/abs/2503.23307", "authors": ["Cong Wei", "Bo Sun", "Haoyu Ma", "Ji Hou", "Felix Juefei-Xu", "Zecheng He", "Xiaoliang Dai", "Luxin Zhang", "Kunpeng Li", "Tingbo Hou", "Animesh Sinha", "Peter Vajda", "Wenhu Chen"], "title": "MoCha: Towards Movie-Grade Talking Character Synthesis", "categories": ["cs.CV"], "comment": "https://congwei1230.github.io/MoCha/", "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23829", "pdf": "https://arxiv.org/pdf/2503.23829", "abs": "https://arxiv.org/abs/2503.23829", "authors": ["Yi Su", "Dian Yu", "Linfeng Song", "Juntao Li", "Haitao Mi", "Zhaopeng Tu", "Min Zhang", "Dong Yu"], "title": "Expanding RL with Verifiable Rewards Across Diverse Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23377", "pdf": "https://arxiv.org/pdf/2503.23377", "abs": "https://arxiv.org/abs/2503.23377", "authors": ["Kai Liu", "Wei Li", "Lai Chen", "Shengqiong Wu", "Yanhao Zheng", "Jiayi Ji", "Fan Zhou", "Rongxin Jiang", "Jiebo Luo", "Hao Fei", "Tat-Seng Chua"], "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Work in progress. Homepage: https://javisdit.github.io/", "summary": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24198", "pdf": "https://arxiv.org/pdf/2503.24198", "abs": "https://arxiv.org/abs/2503.24198", "authors": ["Jingxian Xu", "Mengyu Zhou", "Weichang Liu", "Hanbing Liu", "Shi Han", "Dongmei Zhang"], "title": "TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in problem-solving\nby incorporating reasoning processes. However, this enhanced reasoning\ncapability results in an increased number of output tokens during inference,\nleading to higher computational costs. To address this challenge, we propose\nTwT (Thinking without Tokens), a method that reduces inference-time costs\nthrough habitual reasoning distillation with multi-teachers' guidance, while\nmaintaining high performance. Our approach introduces a Habitual Reasoning\nDistillation method, which internalizes explicit reasoning into the model's\nhabitual behavior through a Teacher-Guided compression strategy inspired by\nhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling\n(DCRS), a technique that generates a high-quality and diverse distillation\ndataset using multiple teacher models, making our method suitable for\nunsupervised scenarios. Experimental results demonstrate that TwT effectively\nreduces inference costs while preserving superior performance, achieving up to\na 13.6% improvement in accuracy with fewer output tokens compared to other\ndistillation methods, offering a highly practical solution for efficient LLM\ndeployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "criteria"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24245", "pdf": "https://arxiv.org/pdf/2503.24245", "abs": "https://arxiv.org/abs/2503.24245", "authors": ["Dun Yuan", "Hao Zhou", "Di Wu", "Xue Liu", "Hao Chen", "Yan Xin", "Jianzhong", "Zhang"], "title": "Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "This work has been accepted to ICC 2025 IEEE International Conference\n  on Communications. copyright 2025 IEEE", "summary": "Large language models (LLMs) have made significant progress in\ngeneral-purpose natural language processing tasks. However, LLMs are still\nfacing challenges when applied to domain-specific areas like\ntelecommunications, which demands specialized expertise and adaptability to\nevolving standards. This paper presents a novel framework that combines\nknowledge graph (KG) and retrieval-augmented generation (RAG) techniques to\nenhance LLM performance in the telecom domain. The framework leverages a KG to\ncapture structured, domain-specific information about network protocols,\nstandards, and other telecom-related entities, comprehensively representing\ntheir relationships. By integrating KG with RAG, LLMs can dynamically access\nand utilize the most relevant and up-to-date knowledge during response\ngeneration. This hybrid approach bridges the gap between structured knowledge\nrepresentation and the generative capabilities of LLMs, significantly enhancing\naccuracy, adaptability, and domain-specific comprehension. Our results\ndemonstrate the effectiveness of the KG-RAG framework in addressing complex\ntechnical queries with precision. The proposed KG-RAG model attained an\naccuracy of 88% for question answering tasks on a frequently used\ntelecom-specific dataset, compared to 82% for the RAG-only and 48% for the\nLLM-only approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24310", "pdf": "https://arxiv.org/pdf/2503.24310", "abs": "https://arxiv.org/abs/2503.24310", "authors": ["Alok Abhishek", "Lisa Erickson", "Tushar Bandopadhyay"], "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models", "categories": ["cs.CL", "cs.AI", "68T01 (Primary), 68T50 (Secondary)", "I.2.0; I.2.7"], "comment": "32 pages, 33 figures, preprint version", "summary": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "factuality"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23450", "pdf": "https://arxiv.org/pdf/2503.23450", "abs": "https://arxiv.org/abs/2503.23450", "authors": ["Bohao Xing", "Kaishen Yuan", "Zitong Yu", "Xin Liu", "Heikki Kälviäinen"], "title": "AU-TTT: Vision Test-Time Training model for Facial Action Unit Detection", "categories": ["cs.CV"], "comment": null, "summary": "Facial Action Units (AUs) detection is a cornerstone of objective facial\nexpression analysis and a critical focus in affective computing. Despite its\nimportance, AU detection faces significant challenges, such as the high cost of\nAU annotation and the limited availability of datasets. These constraints often\nlead to overfitting in existing methods, resulting in substantial performance\ndegradation when applied across diverse datasets. Addressing these issues is\nessential for improving the reliability and generalizability of AU detection\nmethods. Moreover, many current approaches leverage Transformers for their\neffectiveness in long-context modeling, but they are hindered by the quadratic\ncomplexity of self-attention. Recently, Test-Time Training (TTT) layers have\nemerged as a promising solution for long-sequence modeling. Additionally, TTT\napplies self-supervised learning for iterative updates during both training and\ninference, offering a potential pathway to mitigate the generalization\nchallenges inherent in AU detection tasks. In this paper, we propose a novel\nvision backbone tailored for AU detection, incorporating bidirectional TTT\nblocks, named AU-TTT. Our approach introduces TTT Linear to the AU detection\ntask and optimizes image scanning mechanisms for enhanced performance.\nAdditionally, we design an AU-specific Region of Interest (RoI) scanning\nmechanism to capture fine-grained facial features critical for AU detection.\nExperimental results demonstrate that our method achieves competitive\nperformance in both within-domain and cross-domain scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23145", "pdf": "https://arxiv.org/pdf/2503.23145", "abs": "https://arxiv.org/abs/2503.23145", "authors": ["Anjiang Wei", "Tarun Suresh", "Jiannan Cao", "Naveen Kannan", "Yuheng Wu", "Kai Yan", "Thiago S. F. X. Teixeira", "Ke Wang", "Alex Aiken"], "title": "CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Inductive program synthesis, or programming by example, requires synthesizing\nfunctions from input-output examples that generalize to unseen inputs. While\nlarge language model agents have shown promise in programming tasks guided by\nnatural language, their ability to perform inductive program synthesis is\nunderexplored. Existing evaluation protocols rely on static sets of examples\nand held-out tests, offering no feedback when synthesized functions are\nincorrect and failing to reflect real-world scenarios such as reverse\nengineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge,\na new evaluation framework where agents interact with a hidden target function\nby querying it with new inputs, synthesizing candidate functions, and\niteratively refining their solutions using a differential testing oracle. This\ninteractive setting encourages agents to perform function calls and\nself-correction based on feedback. We construct the first large-scale benchmark\nfor general-purpose inductive program synthesis, featuring 1114 functions.\nAmong 18 models evaluated, o3-mini performs best with a success rate of 52.7%,\nhighlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on\ncurated synthesis traces yields up to a 31% relative performance gain. CodeARC\nprovides a more realistic and challenging testbed for evaluating LLM-based\nprogram synthesis and inductive reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "self-correction"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "testbed"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24260", "pdf": "https://arxiv.org/pdf/2503.24260", "abs": "https://arxiv.org/abs/2503.24260", "authors": ["Zhengren Wang", "Rui Ling", "Chufan Wang", "Yongan Yu", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "title": "MaintainCoder: Maintainable Code Generation Under Dynamic Requirements", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Modern code generation has made significant strides in functional correctness\nand execution efficiency. However, these systems often overlook a critical\ndimension in real-world software development: maintainability. To handle\ndynamic requirements with minimal rework, we propose MaintainCoder as a\npioneering solution. It integrates Waterfall model, design patterns, and\nmulti-agent collaboration to systematically enhance cohesion, reduce coupling,\nand improve adaptability. We also introduce MaintainBench, a benchmark\ncomprising requirement changes and corresponding dynamic metrics on\nmaintainance effort. Experiments demonstrate that existing code generation\nmethods struggle to meet maintainability standards when requirements evolve. In\ncontrast, MaintainCoder improves maintainability metrics by 14-30% with even\nhigher correctness, i.e. pass@k. Our work not only provides the foundation of\nmaintainable code generation, but also highlights the need for more holistic\ncode quality research. Resources:\nhttps://github.com/IAAR-Shanghai/MaintainCoder.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "code generation", "dimension"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24370", "pdf": "https://arxiv.org/pdf/2503.24370", "abs": "https://arxiv.org/abs/2503.24370", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Prateek Mittal"], "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23747", "pdf": "https://arxiv.org/pdf/2503.23747", "abs": "https://arxiv.org/abs/2503.23747", "authors": ["Jingyi Zhou", "Peng Ye", "Haoyu Zhang", "Jiakang Yuan", "Rao Qiang", "Liu YangChenXu", "Wu Cailin", "Feng Xu", "Tao Chen"], "title": "Consistency-aware Self-Training for Iterative-based Stereo Matching", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Iterative-based methods have become mainstream in stereo matching due to\ntheir high performance. However, these methods heavily rely on labeled data and\nface challenges with unlabeled real-world data. To this end, we propose a\nconsistency-aware self-training framework for iterative-based stereo matching\nfor the first time, leveraging real-world unlabeled data in a teacher-student\nmanner. We first observe that regions with larger errors tend to exhibit more\npronounced oscillation characteristics during model prediction.Based on this,\nwe introduce a novel consistency-aware soft filtering module to evaluate the\nreliability of teacher-predicted pseudo-labels, which consists of a\nmulti-resolution prediction consistency filter and an iterative prediction\nconsistency filter to assess the prediction fluctuations of multiple\nresolutions and iterative optimization respectively. Further, we introduce a\nconsistency-aware soft-weighted loss to adjust the weight of pseudo-labels\naccordingly, relieving the error accumulation and performance degradation\nproblem due to incorrect pseudo-labels. Extensive experiments demonstrate that\nour method can improve the performance of various iterative-based stereo\nmatching approaches in various scenarios. In particular, our method can achieve\nfurther enhancements over the current SOTA methods on several benchmark\ndatasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "reliability"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23771", "pdf": "https://arxiv.org/pdf/2503.23771", "abs": "https://arxiv.org/abs/2503.23771", "authors": ["Fengxiang Wang", "Hongzhen Wang", "Mingshuo Chen", "Di Wang", "Yulin Wang", "Zonghao Guo", "Qiang Ma", "Long Lan", "Wenjing Yang", "Jing Zhang", "Zhiyuan Liu", "Maosong Sun"], "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?", "categories": ["cs.CV"], "comment": "It has been accepted by CVPR2025", "summary": "The astonishing breakthrough of multimodal large language models (MLLMs) has\nnecessitated new benchmarks to quantitatively assess their capabilities, reveal\ntheir limitations, and indicate future research directions. However, this is\nchallenging in the context of remote sensing (RS), since the imagery features\nultra-high resolution that incorporates extremely complex semantic\nrelationships. Existing benchmarks usually adopt notably smaller image sizes\nthan real-world RS scenarios, suffer from limited annotation quality, and\nconsider insufficient dimensions of evaluation. To address these issues, we\npresent XLRS-Bench: a comprehensive benchmark for evaluating the perception and\nreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.\nXLRS-Bench boasts the largest average image size (8500$\\times$8500) observed\nthus far, with all evaluation samples meticulously annotated manually, assisted\nby a novel semi-automatic captioner on ultra-high-resolution RS images. On top\nof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds of\nperceptual capabilities and 6 kinds of reasoning capabilities, with a primary\nemphasis on advanced cognitive processes that facilitate real-world\ndecision-making and the capture of spatiotemporal changes. The results of both\ngeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts are\nneeded for real-world RS applications. We have open-sourced XLRS-Bench to\nsupport further research in developing more powerful MLLMs for remote sensing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23775", "pdf": "https://arxiv.org/pdf/2503.23775", "abs": "https://arxiv.org/abs/2503.23775", "authors": ["Lucas Heublein", "Nisha L. Raichur", "Tobias Feigl", "Tobias Brieger", "Fin Heuer", "Lennart Asbach", "Alexander Rügamer", "Felix Ott"], "title": "Evaluation of (Un-)Supervised Machine Learning Methods for GNSS Interference Classification with Real-World Data Discrepancies", "categories": ["cs.CV", "cs.LG", "68-00, 68T01, 68T30", "E.0; E.2; H.4; I.5"], "comment": "34 pages, 25 figures", "summary": "The accuracy and reliability of vehicle localization on roads are crucial for\napplications such as self-driving cars, toll systems, and digital tachographs.\nTo achieve accurate positioning, vehicles typically use global navigation\nsatellite system (GNSS) receivers to validate their absolute positions.\nHowever, GNSS-based positioning can be compromised by interference signals,\nnecessitating the identification, classification, determination of purpose, and\nlocalization of such interference to mitigate or eliminate it. Recent\napproaches based on machine learning (ML) have shown superior performance in\nmonitoring interference. However, their feasibility in real-world applications\nand environments has yet to be assessed. Effective implementation of ML\ntechniques requires training datasets that incorporate realistic interference\nsignals, including real-world noise and potential multipath effects that may\noccur between transmitter, receiver, and satellite in the operational area.\nAdditionally, these datasets require reference labels. Creating such datasets\nis often challenging due to legal restrictions, as causing interference to GNSS\nsources is strictly prohibited. Consequently, the performance of ML-based\nmethods in practical applications remains unclear. To address this gap, we\ndescribe a series of large-scale measurement campaigns conducted in real-world\nsettings at two highway locations in Germany and the Seetal Alps in Austria,\nand in large-scale controlled indoor environments. We evaluate the latest\nsupervised ML-based methods to report on their performance in real-world\nsettings and present the applicability of pseudo-labeling for unsupervised\nlearning. We demonstrate the challenges of combining datasets due to data\ndiscrepancies and evaluate outlier detection, domain adaptation, and data\naugmentation techniques to present the models' capabilities to adapt to changes\nin the datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23907", "pdf": "https://arxiv.org/pdf/2503.23907", "abs": "https://arxiv.org/abs/2503.23907", "authors": ["Zhichao Liao", "Xiaokun Liu", "Wenyu Qin", "Qingyu Li", "Qiulin Wang", "Pengfei Wan", "Di Zhang", "Long Zeng", "Pingfa Feng"], "title": "HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image Aesthetic Assessment (IAA) is a long-standing and challenging research\ntask. However, its subset, Human Image Aesthetic Assessment (HIAA), has been\nscarcely explored, even though HIAA is widely used in social media, AI\nworkflows, and related domains. To bridge this research gap, our work pioneers\na holistic implementation framework tailored for HIAA. Specifically, we\nintroduce HumanBeauty, the first dataset purpose-built for HIAA, which\ncomprises 108k high-quality human images with manual annotations. To achieve\ncomprehensive and fine-grained HIAA, 50K human images are manually collected\nthrough a rigorous curation process and annotated leveraging our trailblazing\n12-dimensional aesthetic standard, while the remaining 58K with overall\naesthetic labels are systematically filtered from public datasets. Based on the\nHumanBeauty database, we propose HumanAesExpert, a powerful Vision Language\nModel for aesthetic evaluation of human images. We innovatively design an\nExpert head to incorporate human knowledge of aesthetic sub-dimensions while\njointly utilizing the Language Modeling (LM) and Regression head. This approach\nempowers our model to achieve superior proficiency in both overall and\nfine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregates\nscores from all three heads, to effectively balance the capabilities of each\nhead, thereby realizing improved assessment precision. Extensive experiments\ndemonstrate that our HumanAesExpert models deliver significantly better\nperformance in HIAA than other state-of-the-art models. Our datasets, models,\nand codes are publicly released to advance the HIAA community. Project webpage:\nhttps://humanaesexpert.github.io/HumanAesExpert/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23951", "pdf": "https://arxiv.org/pdf/2503.23951", "abs": "https://arxiv.org/abs/2503.23951", "authors": ["Fangda Chen", "Shanshan Zhao", "Chuanfu Xu", "Long Lan"], "title": "JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://fdchen24.github.io/JointTuner-Website", "summary": "Recent text-to-video advancements have enabled coherent video synthesis from\nprompts and expanded to fine-grained control over appearance and motion.\nHowever, existing methods either suffer from concept interference due to\nfeature domain mismatch caused by naive decoupled optimizations or exhibit\nappearance contamination induced by spatial feature leakage resulting from the\nentanglement of motion and appearance in reference video reconstructions. In\nthis paper, we propose JointTuner, a novel adaptive joint training framework,\nto alleviate these issues. Specifically, we develop Adaptive LoRA, which\nincorporates a context-aware gating mechanism, and integrate the gated LoRA\ncomponents into the spatial and temporal Transformers within the diffusion\nmodel. These components enable simultaneous optimization of appearance and\nmotion, eliminating concept interference. In addition, we introduce the\nAppearance-independent Temporal Loss, which decouples motion patterns from\nintrinsic appearance in reference video reconstructions through an\nappearance-agnostic noise prediction task. The key innovation lies in adding\nframe-wise offset noise to the ground-truth Gaussian noise, perturbing its\ndistribution, thereby disrupting spatial attributes associated with frames\nwhile preserving temporal coherence. Furthermore, we construct a benchmark\ncomprising 90 appearance-motion customized combinations and 10 multi-type\nautomatic metrics across four dimensions, facilitating a more comprehensive\nevaluation for this customization task. Extensive experiments demonstrate the\nsuperior performance of our method compared to current advanced approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23963", "pdf": "https://arxiv.org/pdf/2503.23963", "abs": "https://arxiv.org/abs/2503.23963", "authors": ["Miao Fan", "Shanshan Yu", "Shengtong Xu", "Kun Jiang", "Haoyi Xiong", "Xiangzeng Liu"], "title": "A Benchmark for Vision-Centric HD Mapping by V2I Systems", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IEEE IV'25", "summary": "Autonomous driving faces safety challenges due to a lack of global\nperspective and the semantic information of vectorized high-definition (HD)\nmaps. Information from roadside cameras can greatly expand the map perception\nrange through vehicle-to-infrastructure (V2I) communications. However, there is\nstill no dataset from the real world available for the study on map\nvectorization onboard under the scenario of vehicle-infrastructure cooperation.\nTo prosper the research on online HD mapping for Vehicle-Infrastructure\nCooperative Autonomous Driving (VICAD), we release a real-world dataset, which\ncontains collaborative camera frames from both vehicles and roadside\ninfrastructures, and provides human annotations of HD map elements. We also\npresent an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centric\nV2I systems to construct vectorized maps. To reduce computation costs and\nfurther deploy V2I-HD on autonomous vehicles, we introduce a directionally\ndecoupled self-attention mechanism to V2I-HD. Extensive experiments show that\nV2I-HD has superior performance in real-time inference speed, as tested by our\nreal-world dataset. Abundant qualitative results also demonstrate stable and\nrobust map construction quality with low cost in complex and various driving\nscenes. As a benchmark, both source codes and the dataset have been released at\nOneDrive for the purpose of further study.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24129", "pdf": "https://arxiv.org/pdf/2503.24129", "abs": "https://arxiv.org/abs/2503.24129", "authors": ["Dominik Schnaus", "Nikita Araslanov", "Daniel Cremers"], "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, Project page:\n  https://dominik-schnaus.github.io/itsamatch/", "summary": "The platonic representation hypothesis suggests that vision and language\nembeddings become more homogeneous as model and dataset sizes increase. In\nparticular, pairwise distances within each modality become more similar. This\nsuggests that as foundation models mature, it may become possible to match\nvision and language embeddings in a fully unsupervised fashion, i.e. without\nparallel data. We present the first feasibility study, and investigate\nconformity of existing vision and language foundation models in the context of\nunsupervised, or \"blind\", matching. First, we formulate unsupervised matching\nas a quadratic assignment problem and introduce a novel heuristic that\noutperforms previous solvers. We also develop a technique to find optimal\nmatching problems, for which a non-trivial match is very likely. Second, we\nconduct an extensive study deploying a range of vision and language models on\nfour datasets. Our analysis reveals that for many problem instances, vision and\nlanguage representations can be indeed matched without supervision. This\nfinding opens up the exciting possibility of embedding semantic knowledge into\nother modalities virtually annotation-free. As a proof of concept, we showcase\nan unsupervised classifier, which achieves non-trivial classification accuracy\nwithout any image-text annotation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24180", "pdf": "https://arxiv.org/pdf/2503.24180", "abs": "https://arxiv.org/abs/2503.24180", "authors": ["Ziming Cheng", "Zhiyuan Huang", "Junting Pan", "Zhaohui Hou", "Mingjie Zhan"], "title": "Navi-plus: Managing Ambiguous GUI Navigation Tasks with Follow-up", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Graphical user interfaces (GUI) automation agents are emerging as powerful\ntools, enabling humans to accomplish increasingly complex tasks on smart\ndevices. However, users often inadvertently omit key information when conveying\ntasks, which hinders agent performance in the current agent paradigm that does\nnot support immediate user intervention. To address this issue, we introduce a\n$\\textbf{Self-Correction GUI Navigation}$ task that incorporates interactive\ninformation completion capabilities within GUI agents. We developed the\n$\\textbf{Navi-plus}$ dataset with GUI follow-up question-answer pairs,\nalongside a $\\textbf{Dual-Stream Trajectory Evaluation}$ method to benchmark\nthis new capability. Our results show that agents equipped with the ability to\nask GUI follow-up questions can fully recover their performance when faced with\nambiguous user tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24267", "pdf": "https://arxiv.org/pdf/2503.24267", "abs": "https://arxiv.org/abs/2503.24267", "authors": ["Yixuan Li", "Yu Tian", "Yipo Huang", "Wei Lu", "Shiqi Wang", "Weisi Lin", "Anderson Rocha"], "title": "FakeScope: Large Multimodal Expert Model for Transparent AI-Generated Image Forensics", "categories": ["cs.CV"], "comment": null, "summary": "The rapid and unrestrained advancement of generative artificial intelligence\n(AI) presents a double-edged sword: while enabling unprecedented creativity, it\nalso facilitates the generation of highly convincing deceptive content,\nundermining societal trust. As image generation techniques become increasingly\nsophisticated, detecting synthetic images is no longer just a binary task: it\nnecessitates interpretable, context-aware methodologies that enhance\ntrustworthiness and transparency. However, existing detection models primarily\nfocus on classification, offering limited explanatory insights into image\nauthenticity. In this work, we propose FakeScope, an expert multimodal model\n(LMM) tailored for AI-generated image forensics, which not only identifies\nAI-synthetic images with high accuracy but also provides rich, interpretable,\nand query-driven forensic insights. We first construct FakeChain dataset that\ncontains linguistic authenticity reasoning based on visual trace evidence,\ndeveloped through a novel human-machine collaborative framework. Building upon\nit, we further present FakeInstruct, the largest multimodal instruction tuning\ndataset containing 2 million visual instructions tailored to enhance forensic\nawareness in LMMs. FakeScope achieves state-of-the-art performance in both\nclosed-ended and open-ended forensic scenarios. It can distinguish synthetic\nimages with high accuracy while offering coherent and insightful explanations,\nfree-form discussions on fine-grained forgery attributes, and actionable\nenhancement strategies. Notably, despite being trained exclusively on\nqualitative hard labels, FakeScope demonstrates remarkable zero-shot\nquantitative capability on detection, enabled by our proposed token-based\nprobability estimation strategy. Furthermore, FakeScope exhibits strong\ngeneralization and in-the-wild ability, ensuring its applicability in\nreal-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24345", "pdf": "https://arxiv.org/pdf/2503.24345", "abs": "https://arxiv.org/abs/2503.24345", "authors": ["Fang Yan", "Jianfeng Wu", "Jiawen Li", "Wei Wang", "Jiaxuan Lu", "Wen Chen", "Zizhao Gao", "Jianan Li", "Hong Yan", "Jiabo Ma", "Minda Chen", "Yang Lu", "Qing Chen", "Yizhi Wang", "Xitong Ling", "Xuenian Wang", "Zihan Wang", "Qiang Huang", "Shengyi Hua", "Mianxin Liu", "Lei Ma", "Tian Shen", "Xiaofan Zhang", "Yonghong He", "Hao Chen", "Shaoting Zhang", "Zhe Wang"], "title": "PathOrchestra: A Comprehensive Foundation Model for Computational Pathology with Over 100 Diverse Clinical-Grade Tasks", "categories": ["cs.CV"], "comment": null, "summary": "The complexity and variability inherent in high-resolution pathological\nimages present significant challenges in computational pathology. While\npathology foundation models leveraging AI have catalyzed transformative\nadvancements, their development demands large-scale datasets, considerable\nstorage capacity, and substantial computational resources. Furthermore,\nensuring their clinical applicability and generalizability requires rigorous\nvalidation across a broad spectrum of clinical tasks. Here, we present\nPathOrchestra, a versatile pathology foundation model trained via\nself-supervised learning on a dataset comprising 300K pathological slides from\n20 tissue and organ types across multiple centers. The model was rigorously\nevaluated on 112 clinical tasks using a combination of 61 private and 51 public\ndatasets. These tasks encompass digital slide preprocessing, pan-cancer\nclassification, lesion identification, multi-cancer subtype classification,\nbiomarker assessment, gene expression prediction, and the generation of\nstructured reports. PathOrchestra demonstrated exceptional performance across\n27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks,\nincluding pan-cancer classification across various organs, lymphoma subtype\ndiagnosis, and bladder cancer screening. Notably, it is the first model to\ngenerate structured reports for high-incidence colorectal cancer and\ndiagnostically complex lymphoma-areas that are infrequently addressed by\nfoundational models but hold immense clinical potential. Overall, PathOrchestra\nexemplifies the feasibility and efficacy of a large-scale, self-supervised\npathology foundation model, validated across a broad range of clinical-grade\ntasks. Its high accuracy and reduced reliance on extensive data annotation\nunderline its potential for clinical integration, offering a pathway toward\nmore efficient and high-quality medical services.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24389", "pdf": "https://arxiv.org/pdf/2503.24389", "abs": "https://arxiv.org/abs/2503.24389", "authors": ["Chenyang Li", "Wenxuan Liu", "Guoqiang Gong", "Xiaobo Ding", "Xian Zhong"], "title": "SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Underwater object detection is critical for oceanic research and industrial\nsafety inspections. However, the complex optical environment and the limited\nresources of underwater equipment pose significant challenges to achieving high\naccuracy and low power consumption. To address these issues, we propose Spiking\nUnderwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging the\nlightweight and energy-efficient properties of SNNs, SU-YOLO incorporates a\nnovel spike-based underwater image denoising method based solely on integer\naddition, which enhances the quality of feature maps with minimal computational\noverhead. In addition, we introduce Separated Batch Normalization (SeBN), a\ntechnique that normalizes feature maps independently across multiple time steps\nand is optimized for integration with residual structures to capture the\ntemporal dynamics of SNNs more effectively. The redesigned spiking residual\nblocks integrate the Cross Stage Partial Network (CSPNet) with the YOLO\narchitecture to mitigate spike degradation and enhance the model's feature\nextraction capabilities. Experimental results on URPC2019 underwater dataset\ndemonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and an\nenergy consumption of 2.98 mJ, surpassing mainstream SNN models in both\ndetection accuracy and computational efficiency. These results underscore the\npotential of SNNs for engineering applications. The code is available in\nhttps://github.com/lwxfight/snn-underwater.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22746", "pdf": "https://arxiv.org/pdf/2503.22746", "abs": "https://arxiv.org/abs/2503.22746", "authors": ["Kyung Ho Lim", "Ujin Kang", "Xiang Li", "Jin Sung Kim", "Young-Chul Jung", "Sangjoon Park", "Byung-Hoon Kim"], "title": "Susceptibility of Large Language Models to User-Driven Factors in Medical Queries", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in healthcare, but their\nreliability is heavily influenced by user-driven factors such as question\nphrasing and the completeness of clinical information. In this study, we\nexamined how misinformation framing, source authority, model persona, and\nomission of key clinical details affect the diagnostic accuracy and reliability\nof LLM outputs. We conducted two experiments: one introducing misleading\nexternal opinions with varying assertiveness (perturbation test), and another\nremoving specific categories of patient information (ablation test). Using\npublic datasets (MedQA and Medbullets), we evaluated proprietary models\n(GPT-4o, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Gemini 1.5 Flash)\nand open-source models (LLaMA 3 8B, LLaMA 3 Med42 8B, DeepSeek R1 8B). All\nmodels were vulnerable to user-driven misinformation, with proprietary models\nespecially affected by definitive and authoritative language. Assertive tone\nhad the greatest negative impact on accuracy. In the ablation test, omitting\nphysical exam findings and lab results caused the most significant performance\ndrop. Although proprietary models had higher baseline accuracy, their\nperformance declined sharply under misinformation. These results highlight the\nneed for well-structured prompts and complete clinical context. Users should\navoid authoritative framing of misinformation and provide full clinical\ndetails, especially for complex cases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22856", "pdf": "https://arxiv.org/pdf/2503.22856", "abs": "https://arxiv.org/abs/2503.22856", "authors": ["Shanshan Bai", "Anna Kruspe", "Xiaoxiang Zhu"], "title": "Generating Synthetic Oracle Datasets to Analyze Noise Impact: A Study on Building Function Classification Using Tweets", "categories": ["cs.CL"], "comment": null, "summary": "Tweets provides valuable semantic context for earth observation tasks and\nserves as a complementary modality to remote sensing imagery. In building\nfunction classification (BFC), tweets are often collected using geographic\nheuristics and labeled via external databases, an inherently weakly supervised\nprocess that introduces both label noise and sentence level feature noise\n(e.g., irrelevant or uninformative tweets). While label noise has been widely\nstudied, the impact of sentence level feature noise remains underexplored,\nlargely due to the lack of clean benchmark datasets for controlled analysis. In\nthis work, we propose a method for generating a synthetic oracle dataset using\nLLM, designed to contain only tweets that are both correctly labeled and\nsemantically relevant to their associated buildings. This oracle dataset\nenables systematic investigation of noise impacts that are otherwise difficult\nto isolate in real-world data. To assess its utility, we compare model\nperformance using Naive Bayes and mBERT classifiers under three configurations:\nreal vs. synthetic training data, and cross-domain generalization. Results show\nthat noise in real tweets significantly degrades the contextual learning\ncapacity of mBERT, reducing its performance to that of a simple keyword-based\nmodel. In contrast, the clean synthetic dataset allows mBERT to learn\neffectively, outperforming Naive Bayes Bayes by a large margin. These findings\nhighlight that addressing feature noise is more critical than model complexity\nin this task. Our synthetic dataset offers a novel experimental environment for\nfuture noise injection studies and is publicly available on GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22877", "pdf": "https://arxiv.org/pdf/2503.22877", "abs": "https://arxiv.org/abs/2503.22877", "authors": ["Bruno Coelho", "Shujaat Mirza", "Yuyuan Cui", "Christina Pöpper", "Damon McCoy"], "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22869", "pdf": "https://arxiv.org/pdf/2503.22869", "abs": "https://arxiv.org/abs/2503.22869", "authors": ["Alexey Gavryushin", "Florian Redhardt", "Gaia Di Lorenzo", "Luc Van Gool", "Marc Pollefeys", "Kaichun Mo", "Xi Wang"], "title": "SIGHT: Single-Image Conditioned Generation of Hand Trajectories for Hand-Object Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel task of generating realistic and diverse 3D hand\ntrajectories given a single image of an object, which could be involved in a\nhand-object interaction scene or pictured by itself. When humans grasp an\nobject, appropriate trajectories naturally form in our minds to use it for\nspecific tasks. Hand-object interaction trajectory priors can greatly benefit\napplications in robotics, embodied AI, augmented reality and related fields.\nHowever, synthesizing realistic and appropriate hand trajectories given a\nsingle object or hand-object interaction image is a highly ambiguous task,\nrequiring to correctly identify the object of interest and possibly even the\ncorrect interaction among many possible alternatives. To tackle this\nchallenging problem, we propose the SIGHT-Fusion system, consisting of a\ncurated pipeline for extracting visual features of hand-object interaction\ndetails from egocentric videos involving object manipulation, and a\ndiffusion-based conditional motion generation model processing the extracted\nfeatures. We train our method given video data with corresponding hand\ntrajectory annotations, without supervision in the form of action labels. For\nthe evaluation, we establish benchmarks utilizing the first-person FPHAB and\nHOI4D datasets, testing our method against various baselines and using multiple\nmetrics. We also introduce task simulators for executing the generated hand\ntrajectories and reporting task success rates as an additional metric.\nExperiments show that our method generates more appropriate and realistic hand\ntrajectories than baselines and presents promising generalization capability on\nunseen objects. The accuracy of the generated hand trajectories is confirmed in\na physics simulation setting, showcasing the authenticity of the created\nsequences and their applicability in downstream uses.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22973", "pdf": "https://arxiv.org/pdf/2503.22973", "abs": "https://arxiv.org/abs/2503.22973", "authors": ["Vivek Iyer", "Ricardo Rei", "Pinzhen Chen", "Alexandra Birch"], "title": "XL-Instruct: Synthetic Data for Cross-Lingual Open-Ended Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Cross-lingual open-ended generation -- i.e. generating responses in a desired\nlanguage different from that of the user's query -- is an important yet\nunderstudied problem. We introduce XL-AlpacaEval, a new benchmark for\nevaluating cross-lingual generation capabilities in Large Language Models\n(LLMs), and propose XL-Instruct, a high-quality synthetic data generation\nmethod. Fine-tuning with just 8K XL-Instruct-generated instructions\nsignificantly improves model performance, increasing the win rate against\nGPT-4o-Mini from 7.4% to 21.5%, and improving on several fine-grained quality\nmetrics. Additionally, models fine-tuned on XL-Instruct exhibit strong\nzero-shot transfer to both English-only and multilingual generation tasks.\nGiven its consistent gains across the board, we strongly recommend\nincorporating XL-Instruct in the post-training pipeline of future multilingual\nLLMs. To facilitate further research, we will publicly and freely release the\nXL-Instruct and XL-AlpacaEval datasets, which constitute two of the few\ncross-lingual resources currently available in the literature.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22985", "pdf": "https://arxiv.org/pdf/2503.22985", "abs": "https://arxiv.org/abs/2503.22985", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Zezhong Wang", "Bin Liang", "Binyang Li", "Kam-Fai Wong"], "title": "FReM: A Flexible Reasoning Mechanism for Balancing Quick and Slow Thinking in Long-Context Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Long-context question-answering (LCQA) systems have greatly benefited from\nthe powerful reasoning capabilities of large language models (LLMs), which can\nbe categorized into slow and quick reasoning modes. However, both modes have\ntheir limitations. Slow thinking generally leans to explore every possible\nreasoning path, which leads to heavy overthinking and wastes time. Quick\nthinking usually relies on pattern matching rather than truly understanding the\nquery logic, which misses proper understanding. To address these issues, we\npropose FReM: Flexible Reasoning Mechanism, a method that adjusts reasoning\ndepth according to the complexity of each question. Specifically, FReM\nleverages synthetic reference QA examples to provide an explicit chain of\nthought, enabling efficient handling of simple queries while allowing deeper\nreasoning for more complex ones. By doing so, FReM helps quick-thinking models\nmove beyond superficial pattern matching and narrows the reasoning space for\nslow-thinking models to avoid unnecessary exploration. Experiments on seven QA\ndatasets show that FReM improves reasoning accuracy and scalability,\nparticularly for complex multihop questions, indicating its potential to\nadvance LCQA methodologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22906", "pdf": "https://arxiv.org/pdf/2503.22906", "abs": "https://arxiv.org/abs/2503.22906", "authors": ["Heng Yu", "Juze Zhang", "Changan Chen", "Tiange Xiang", "Yusu Fang", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "SocialGen: Modeling Multi-Human Social Interaction with Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Human interactions in everyday life are inherently social, involving\nengagements with diverse individuals across various contexts. Modeling these\nsocial interactions is fundamental to a wide range of real-world applications.\nIn this paper, we introduce SocialGen, the first unified motion-language model\ncapable of modeling interaction behaviors among varying numbers of individuals,\nto address this crucial yet challenging problem. Unlike prior methods that are\nlimited to two-person interactions, we propose a novel social motion\nrepresentation that supports tokenizing the motions of an arbitrary number of\nindividuals and aligning them with the language space. This alignment enables\nthe model to leverage rich, pretrained linguistic knowledge to better\nunderstand and reason about human social behaviors. To tackle the challenges of\ndata scarcity, we curate a comprehensive multi-human interaction dataset,\nSocialX, enriched with textual annotations. Leveraging this dataset, we\nestablish the first comprehensive benchmark for multi-human interaction tasks.\nOur method achieves state-of-the-art performance across motion-language tasks,\nsetting a new standard for multi-human interaction modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22912", "pdf": "https://arxiv.org/pdf/2503.22912", "abs": "https://arxiv.org/abs/2503.22912", "authors": ["Xin Liang", "Yogesh S Rawat"], "title": "DIFFER: Disentangling Identity Features via Semantic Cues for Clothes-Changing Person Re-ID", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "Clothes-changing person re-identification (CC-ReID) aims to recognize\nindividuals under different clothing scenarios. Current CC-ReID approaches\neither concentrate on modeling body shape using additional modalities including\nsilhouette, pose, and body mesh, potentially causing the model to overlook\nother critical biometric traits such as gender, age, and style, or they\nincorporate supervision through additional labels that the model tries to\ndisregard or emphasize, such as clothing or personal attributes. However, these\nannotations are discrete in nature and do not capture comprehensive\ndescriptions.\n  In this work, we propose DIFFER: Disentangle Identity Features From Entangled\nRepresentations, a novel adversarial learning method that leverages textual\ndescriptions to disentangle identity features. Recognizing that image features\ninherently mix inseparable information, DIFFER introduces NBDetach, a mechanism\ndesigned for feature disentanglement by leveraging the separable nature of text\ndescriptions as supervision. It partitions the feature space into distinct\nsubspaces and, through gradient reversal layers, effectively separates\nidentity-related features from non-biometric features. We evaluate DIFFER on 4\ndifferent benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to\ndemonstrate its effectiveness and provide state-of-the-art performance across\nall the benchmarks. DIFFER consistently outperforms the baseline method, with\nimprovements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on\nCelebReID-Light, and 1% on CCVID. Our code can be found here.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23029", "pdf": "https://arxiv.org/pdf/2503.23029", "abs": "https://arxiv.org/abs/2503.23029", "authors": ["Yichun Feng", "Jiawei Wang", "Ruikun He", "Lu Zhou", "Yixue Li"], "title": "A Retrieval-Augmented Knowledge Mining Method with Deep Thinking LLMs for Biomedical Research and Clinical Support", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge graphs and large language models (LLMs) are key tools for\nbiomedical knowledge integration and reasoning, facilitating structured\norganization of scientific articles and discovery of complex semantic\nrelationships. However, current methods face challenges: knowledge graph\nconstruction is limited by complex terminology, data heterogeneity, and rapid\nknowledge evolution, while LLMs show limitations in retrieval and reasoning,\nmaking it difficult to uncover cross-document associations and reasoning\npathways. To address these issues, we propose a pipeline that uses LLMs to\nconstruct a biomedical knowledge graph (BioStrataKG) from large-scale articles\nand builds a cross-document question-answering dataset (BioCDQA) to evaluate\nlatent knowledge retrieval and multi-hop reasoning. We then introduce\nIntegrated and Progressive Retrieval-Augmented Reasoning (IP-RAR) to enhance\nretrieval accuracy and knowledge reasoning. IP-RAR maximizes information recall\nthrough Integrated Reasoning-based Retrieval and refines knowledge via\nProgressive Reasoning-based Generation, using self-reflection to achieve deep\nthinking and precise contextual understanding. Experiments show that IP-RAR\nimproves document retrieval F1 score by 20\\% and answer generation accuracy by\n25\\% over existing methods. This framework helps doctors efficiently integrate\ntreatment evidence for personalized medication plans and enables researchers to\nanalyze advancements and research gaps, accelerating scientific discovery and\ndecision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23078", "pdf": "https://arxiv.org/pdf/2503.23078", "abs": "https://arxiv.org/abs/2503.23078", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Yiming Du", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Binyang Li", "Kam-Fai Wong"], "title": "EventWeave: A Dynamic Framework for Capturing Core and Supporting Events in Dialogue Systems", "categories": ["cs.CL"], "comment": null, "summary": "Existing large language models (LLMs) have shown remarkable progress in\ndialogue systems. However, many approaches still overlook the fundamental role\nof events throughout multi-turn interactions, leading to \\textbf{incomplete\ncontext tracking}. Without tracking these events, dialogue systems often lose\ncoherence and miss subtle shifts in user intent, causing disjointed responses.\nTo bridge this gap, we present \\textbf{EventWeave}, an event-centric framework\nthat identifies and updates both core and supporting events as the conversation\nunfolds. Specifically, we organize these events into a dynamic event graph,\nwhich represents the interplay between \\textbf{core events} that shape the\nprimary idea and \\textbf{supporting events} that provide critical context\nduring the whole dialogue. By leveraging this dynamic graph, EventWeave helps\nmodels focus on the most relevant events when generating responses, thus\navoiding repeated visits of the entire dialogue history. Experimental results\non two benchmark datasets show that EventWeave improves response quality and\nevent relevance without fine-tuning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dialogue"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23213", "pdf": "https://arxiv.org/pdf/2503.23213", "abs": "https://arxiv.org/abs/2503.23213", "authors": ["Diana Bolanos", "Mohammadmehdi Ataei", "Daniele Grandi", "Kosa Goucher-Lambert"], "title": "RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Product recalls provide valuable insights into potential risks and hazards\nwithin the engineering design process, yet their full potential remains\nunderutilized. In this study, we curate data from the United States Consumer\nProduct Safety Commission (CPSC) recalls database to develop a multimodal\ndataset, RECALL-MM, that informs data-driven risk assessment using historical\ninformation, and augment it using generative methods. Patterns in the dataset\nhighlight specific areas where improved safety measures could have significant\nimpact. We extend our analysis by demonstrating interactive clustering maps\nthat embed all recalls into a shared latent space based on recall descriptions\nand product names. Leveraging these data-driven tools, we explore three case\nstudies to demonstrate the dataset's utility in identifying product risks and\nguiding safer design decisions. The first two case studies illustrate how\ndesigners can visualize patterns across recalled products and situate new\nproduct ideas within the broader recall landscape to proactively anticipate\nhazards. In the third case study, we extend our approach by employing a large\nlanguage model (LLM) to predict potential hazards based solely on product\nimages. This demonstrates the model's ability to leverage visual context to\nidentify risk factors, revealing strong alignment with historical recall data\nacross many hazard categories. However, the analysis also highlights areas\nwhere hazard prediction remains challenging, underscoring the importance of\nrisk awareness throughout the design process. Collectively, this work aims to\nbridge the gap between historical recall data and future product safety,\npresenting a scalable, data-driven approach to safer engineering design.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23295", "pdf": "https://arxiv.org/pdf/2503.23295", "abs": "https://arxiv.org/abs/2503.23295", "authors": ["Mikhail Krasitskii", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Grigori Sidorov", "Alexander Gelbukh"], "title": "Advancing Sentiment Analysis in Tamil-English Code-Mixed Texts: Challenges and Transformer-Based Solutions", "categories": ["cs.CL"], "comment": null, "summary": "The sentiment analysis task in Tamil-English code-mixed texts has been\nexplored using advanced transformer-based models. Challenges from grammatical\ninconsistencies, orthographic variations, and phonetic ambiguities have been\naddressed. The limitations of existing datasets and annotation gaps have been\nexamined, emphasizing the need for larger and more diverse corpora. Transformer\narchitectures, including XLM-RoBERTa, mT5, IndicBERT, and RemBERT, have been\nevaluated in low-resource, code-mixed environments. Performance metrics have\nbeen analyzed, highlighting the effectiveness of specific models in handling\nmultilingual sentiment classification. The findings suggest that further\nadvancements in data augmentation, phonetic normalization, and hybrid modeling\napproaches are required to enhance accuracy. Future research directions for\nimproving sentiment analysis in code-mixed texts have been proposed.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23064", "pdf": "https://arxiv.org/pdf/2503.23064", "abs": "https://arxiv.org/abs/2503.23064", "authors": ["Yufan Ren", "Konstantinos Tertikas", "Shalini Maiti", "Junlin Han", "Tong Zhang", "Sabine Süsstrunk", "Filippos Kokkinos"], "title": "VGRP-Bench: Visual Grid Reasoning Puzzle Benchmark for Large Vision-Language Models", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Large Vision-Language Models (LVLMs) struggle with puzzles, which require\nprecise perception, rule comprehension, and logical reasoning. Assessing and\nenhancing their performance in this domain is crucial, as it reflects their\nability to engage in structured reasoning - an essential skill for real-world\nproblem-solving. However, existing benchmarks primarily evaluate pre-trained\nmodels without additional training or fine-tuning, often lack a dedicated focus\non reasoning, and fail to establish a systematic evaluation framework. To\naddress these limitations, we introduce VGRP-Bench, a Visual Grid Reasoning\nPuzzle Benchmark featuring 20 diverse puzzles. VGRP-Bench spans multiple\ndifficulty levels, and includes extensive experiments not only on existing chat\nLVLMs (e.g., GPT-4o), but also on reasoning LVLMs (e.g., Gemini-Thinking). Our\nresults reveal that even the state-of-the-art LVLMs struggle with these\npuzzles, highlighting fundamental limitations in their puzzle-solving\ncapabilities. Most importantly, through systematic experiments, we identify and\nanalyze key factors influencing LVLMs' puzzle-solving performance, including\nthe number of clues, grid size, and rule complexity. Furthermore, we explore\ntwo Supervised Fine-Tuning (SFT) strategies that can be used in post-training:\nSFT on solutions (S-SFT) and SFT on synthetic reasoning processes (R-SFT).\nWhile both methods significantly improve performance on trained puzzles, they\nexhibit limited generalization to unseen ones. We will release VGRP-Bench to\nfacilitate further research on LVLMs for complex, real-world problem-solving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23415", "pdf": "https://arxiv.org/pdf/2503.23415", "abs": "https://arxiv.org/abs/2503.23415", "authors": ["Alexander Murphy", "Mohd Sanad Zaki Rizvi", "Aden Haussmann", "Ping Nie", "Guifu Liu", "Aryo Pradipta Gema", "Pasquale Minervini"], "title": "An Analysis of Decoding Methods for LLM-based Agents for Faithful Multi-Hop Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) frequently produce factually inaccurate outputs\n- a phenomenon known as hallucination - which limits their accuracy in\nknowledge-intensive NLP tasks. Retrieval-augmented generation and agentic\nframeworks such as Reasoning and Acting (ReAct) can address this issue by\ngiving the model access to external knowledge. However, LLMs often fail to\nremain faithful to retrieved information. Mitigating this is critical,\nespecially if LLMs are required to reason about the retrieved information.\nRecent research has explored training-free decoding strategies to improve the\nfaithfulness of model generations. We present a systematic analysis of how the\ncombination of the ReAct framework and decoding strategies (i.e., DeCoRe, DoLa,\nand CAD) can influence the faithfulness of LLM-generated answers. Our results\nshow that combining an agentic framework for knowledge retrieval with decoding\nmethods that enhance faithfulness can increase accuracy on the downstream\nMulti-Hop Question Answering tasks. For example, we observe an F1 increase from\n19.5 to 32.6 on HotpotQA when using ReAct and DoLa.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23109", "pdf": "https://arxiv.org/pdf/2503.23109", "abs": "https://arxiv.org/abs/2503.23109", "authors": ["Xiaolu Liu", "Ruizi Yang", "Song Wang", "Wentong Li", "Junbo Chen", "Jianke Zhu"], "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction", "categories": ["cs.CV"], "comment": "17 pages, 10 figures", "summary": "Reliable high-definition (HD) map construction is crucial for the driving\nsafety of autonomous vehicles. Although recent studies demonstrate improved\nperformance, their generalization capability across unfamiliar driving scenes\nremains unexplored. To tackle this issue, we propose UIGenMap, an\nuncertainty-instructed structure injection approach for generalizable HD map\nvectorization, which concerns the uncertainty resampling in statistical\ndistribution and employs explicit instance features to reduce excessive\nreliance on training data. Specifically, we introduce the perspective-view (PV)\ndetection branch to obtain explicit structural features, in which the\nuncertainty-aware decoder is designed to dynamically sample probability\ndistributions considering the difference in scenes. With probabilistic\nembedding and selection, UI2DPrompt is proposed to construct PV-learnable\nprompts. These PV prompts are integrated into the map decoder by designed\nhybrid injection to compensate for neglected instance structures. To ensure\nreal-time inference, a lightweight Mimic Query Distillation is designed to\nlearn from PV prompts, which can serve as an efficient alternative to the flow\nof PV branches. Extensive experiments on challenging geographically disjoint\n(geo-based) data splits demonstrate that our UIGenMap achieves superior\nperformance, with +5.7 mAP improvement on the nuScenes dataset. Source code\nwill be available at https://github.com/xiaolul2/UIGenMap.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23130", "pdf": "https://arxiv.org/pdf/2503.23130", "abs": "https://arxiv.org/abs/2503.23130", "authors": ["Boyi Ma", "Yanguang Zhao", "Jie Wang", "Guankun Wang", "Kun Yuan", "Tong Chen", "Long Bai", "Hongliang Ren"], "title": "Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Technical Report", "summary": "DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates\noutstanding performance in general scene understanding, question-answering\n(QA), and text generation tasks, owing to its efficient training paradigm and\nstrong reasoning capabilities. In this study, we investigate the dialogue\ncapabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks\nsuch as Single Phrase QA, Visual QA, and Detailed Description. The Single\nPhrase QA tasks further include sub-tasks such as surgical instrument\nrecognition, action understanding, and spatial position analysis. We conduct\nextensive evaluations using publicly available datasets, including EndoVis18\nand CholecT50, along with their corresponding dialogue data. Our comprehensive\nevaluation results indicate that, when provided with specific prompts,\nDeepSeek-V3 performs well in surgical instrument and tissue recognition tasks\nHowever, DeepSeek-V3 exhibits significant limitations in spatial position\nanalysis and struggles to understand surgical actions accurately. Additionally,\nour findings reveal that, under general prompts, DeepSeek-V3 lacks the ability\nto effectively analyze global surgical concepts and fails to provide detailed\ninsights into surgical scenarios. Based on our observations, we argue that the\nDeepSeek-V3 is not ready for vision-language tasks in surgical contexts without\nfine-tuning on surgery-specific datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23673", "pdf": "https://arxiv.org/pdf/2503.23673", "abs": "https://arxiv.org/abs/2503.23673", "authors": ["Zhengyi Zhao", "Shubo Zhang", "Bin Liang", "Binyang Li", "Kam-Fai Wong"], "title": "WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation", "categories": ["cs.CL"], "comment": null, "summary": "In Biomedical Natural Language Processing (BioNLP) tasks, such as Relation\nExtraction, Named Entity Recognition, and Text Classification, the scarcity of\nhigh-quality data remains a significant challenge. This limitation poisons\nlarge language models to correctly understand relationships between biological\nentities, such as molecules and diseases, or drug interactions, and further\nresults in potential misinterpretation of biomedical documents. To address this\nissue, current approaches generally adopt the Synthetic Data Augmentation\nmethod which involves similarity computation followed by word replacement, but\ncounterfactual data are usually generated. As a result, these methods disrupt\nmeaningful word sets or produce sentences with meanings that deviate\nsubstantially from the original context, rendering them ineffective in\nimproving model performance. To this end, this paper proposes a\nbiomedical-dedicated rationale-based synthetic data augmentation method. Beyond\nthe naive lexicon similarity, specific bio-relation similarity is measured to\nhold the augmented instance having a strong correlation with bio-relation\ninstead of simply increasing the diversity of augmented data. Moreover, a\nmulti-agents-involved reflection mechanism helps the model iteratively\ndistinguish different usage of similar entities to escape falling into the\nmis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark,\nwhich includes 9 common datasets spanning four major BioNLP tasks. Our\nexperimental results demonstrate consistent performance improvements across all\ntasks, highlighting the effectiveness of our approach in addressing the\nchallenges associated with data scarcity and enhancing the overall performance\nof biomedical NLP models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23687", "pdf": "https://arxiv.org/pdf/2503.23687", "abs": "https://arxiv.org/abs/2503.23687", "authors": ["Sharad Duwal"], "title": "MKA: Leveraging Cross-Lingual Consensus for Model Abstention", "categories": ["cs.CL", "cs.LG"], "comment": "To appear in Building Trust Workshop at ICLR 2025", "summary": "Reliability of LLMs is questionable even as they get better at more tasks. A\nwider adoption of LLMs is contingent on whether they are usably factual. And if\nthey are not, on whether they can properly calibrate their confidence in their\nresponses. This work focuses on utilizing the multilingual knowledge of an LLM\nto inform its decision to abstain or answer when prompted. We develop a\nmultilingual pipeline to calibrate the model's confidence and let it abstain\nwhen uncertain. We run several multilingual models through the pipeline to\nprofile them across different languages. We find that the performance of the\npipeline varies by model and language, but that in general they benefit from\nit. This is evidenced by the accuracy improvement of $71.2\\%$ for Bengali over\na baseline performance without the pipeline. Even a high-resource language like\nEnglish sees a $15.5\\%$ improvement. These results hint at possible further\nimprovements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23688", "pdf": "https://arxiv.org/pdf/2503.23688", "abs": "https://arxiv.org/abs/2503.23688", "authors": ["William Guey", "Pierrick Bougault", "Vitor D. de Moura", "Wei Zhang", "Jose O. Gomes"], "title": "Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions", "categories": ["cs.CL", "cs.HC"], "comment": "Preliminary version,20 pages, 10 figures, 1 table", "summary": "This study systematically analyzes geopolitical bias across 11 prominent\nLarge Language Models (LLMs) by examining their responses to seven critical\ntopics in U.S.-China relations. Utilizing a bilingual (English and Chinese) and\ndual-framing (affirmative and reverse) methodology, we generated 19,712 prompts\ndesigned to detect ideological leanings in model outputs. Responses were\nquantitatively assessed on a normalized scale from -2 (strongly Pro-China) to\n+2 (strongly Pro-U.S.) and categorized according to stance, neutrality, and\nrefusal rates. The findings demonstrate significant and consistent ideological\nalignments correlated with the LLMs' geographic origins; U.S.-based models\npredominantly favored Pro-U.S. stances, while Chinese-origin models exhibited\npronounced Pro-China biases. Notably, language and prompt framing substantially\ninfluenced model responses, with several LLMs exhibiting stance reversals based\non prompt polarity or linguistic context. Additionally, we introduced\ncomprehensive metrics to evaluate response consistency across languages and\nframing conditions, identifying variability and vulnerabilities in model\nbehaviors. These results offer practical insights that can guide organizations\nand individuals in selecting LLMs best aligned with their operational\npriorities and geopolitical considerations, underscoring the importance of\ncareful model evaluation in politically sensitive applications. Furthermore,\nthe research highlights specific prompt structures and linguistic variations\nthat can strategically trigger distinct responses from models, revealing\nmethods for effectively navigating and influencing LLM outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23266", "pdf": "https://arxiv.org/pdf/2503.23266", "abs": "https://arxiv.org/abs/2503.23266", "authors": ["Shihao Cheng", "Jinlu Zhang", "Yue Liu", "Zhigang Tu"], "title": "OwlSight: A Robust Illumination Adaptation Framework for Dark Video Human Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition in low-light environments is crucial for various\nreal-world applications. However, the existing approaches overlook the full\nutilization of brightness information throughout the training phase, leading to\nsuboptimal performance. To address this limitation, we propose OwlSight, a\nbiomimetic-inspired framework with whole-stage illumination enhancement to\ninteract with action classification for accurate dark video human action\nrecognition. Specifically, OwlSight incorporates a Time-Consistency Module\n(TCM) to capture shallow spatiotemporal features meanwhile maintaining temporal\ncoherence, which are then processed by a Luminance Adaptation Module (LAM) to\ndynamically adjust the brightness based on the input luminance distribution.\nFurthermore, a Reflect Augmentation Module (RAM) is presented to maximize\nillumination utilization and simultaneously enhance action recognition via two\ninteractive paths. Additionally, we build Dark-101, a large-scale dataset\ncomprising 18,310 dark videos across 101 action categories, significantly\nsurpassing existing datasets (e.g., ARID1.5 and Dark-48) in scale and\ndiversity. Extensive experiments demonstrate that the proposed OwlSight\nachieves state-of-the-art performance across four low-light action recognition\nbenchmarks. Notably, it outperforms previous best approaches by 5.36% on\nARID1.5 and 1.72% on Dark-101, highlighting its effectiveness in challenging\ndark environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23283", "pdf": "https://arxiv.org/pdf/2503.23283", "abs": "https://arxiv.org/abs/2503.23283", "authors": ["Lu Yu", "Haoyu Han", "Zhe Tao", "Hantao Yao", "Changsheng Xu"], "title": "Language Guided Concept Bottleneck Models for Interpretable Continual Learning", "categories": ["cs.CV"], "comment": "CVPR 2025; Project Page: https://github.com/FisherCats/CLG-CBM", "summary": "Continual learning (CL) aims to enable learning systems to acquire new\nknowledge constantly without forgetting previously learned information. CL\nfaces the challenge of mitigating catastrophic forgetting while maintaining\ninterpretability across tasks. Most existing CL methods focus primarily on\npreserving learned knowledge to improve model performance. However, as new\ninformation is introduced, the interpretability of the learning process becomes\ncrucial for understanding the evolving decision-making process, yet it is\nrarely explored. In this paper, we introduce a novel framework that integrates\nlanguage-guided Concept Bottleneck Models (CBMs) to address both challenges.\nOur approach leverages the Concept Bottleneck Layer, aligning semantic\nconsistency with CLIP models to learn human-understandable concepts that can\ngeneralize across tasks. By focusing on interpretable concepts, our method not\nonly enhances the models ability to retain knowledge over time but also\nprovides transparent decision-making insights. We demonstrate the effectiveness\nof our approach by achieving superior performance on several datasets,\noutperforming state-of-the-art methods with an improvement of up to 3.06% in\nfinal average accuracy on ImageNet-subset. Additionally, we offer concept\nvisualizations for model predictions, further advancing the understanding of\ninterpretable continual learning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23779", "pdf": "https://arxiv.org/pdf/2503.23779", "abs": "https://arxiv.org/abs/2503.23779", "authors": ["Ine Gevers", "Victor De Marez", "Luna De Bruyne", "Walter Daelemans"], "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23297", "pdf": "https://arxiv.org/pdf/2503.23297", "abs": "https://arxiv.org/abs/2503.23297", "authors": ["Zhenyang Liu", "Yikai Wang", "Sixiao Zheng", "Tongying Pan", "Longfei Liang", "Yanwei Fu", "Xiangyang Xue"], "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary 3D visual grounding and reasoning aim to localize objects in\na scene based on implicit language descriptions, even when they are occluded.\nThis ability is crucial for tasks such as vision-language navigation and\nautonomous robotics. However, current methods struggle because they rely\nheavily on fine-tuning with 3D annotations and mask proposals, which limits\ntheir ability to handle diverse semantics and common knowledge required for\neffective reasoning. In this work, we propose ReasonGrounder, an LVLM-guided\nframework that uses hierarchical 3D feature Gaussian fields for adaptive\ngrouping based on physical scale, enabling open-vocabulary 3D grounding and\nreasoning. ReasonGrounder interprets implicit instructions using large\nvision-language models (LVLM) and localizes occluded objects through 3D\nGaussian splatting. By incorporating 2D segmentation masks from the SAM and\nmulti-view CLIP embeddings, ReasonGrounder selects Gaussian groups based on\nobject scale, enabling accurate localization through both explicit and implicit\nlanguage understanding, even in novel, occluded views. We also contribute\nReasoningGD, a new dataset containing over 10K scenes and 2 million annotations\nfor evaluating open-vocabulary 3D grounding and amodal perception under\nocclusion. Experiments show that ReasonGrounder significantly improves 3D\ngrounding accuracy in real-world scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23798", "pdf": "https://arxiv.org/pdf/2503.23798", "abs": "https://arxiv.org/abs/2503.23798", "authors": ["Xuan Luo", "Weizhi Wang", "Xifeng Yan"], "title": "Adaptive Layer-skipping in Pre-trained LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23899", "pdf": "https://arxiv.org/pdf/2503.23899", "abs": "https://arxiv.org/abs/2503.23899", "authors": ["Diana Galvan-Sosa", "Gabrielle Gaudeau", "Pride Kavumba", "Yunmeng Li", "Hongyi gu", "Zheng Yuan", "Keisuke Sakaguchi", "Paula Buttery"], "title": "Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset", "categories": ["cs.CL", "I.2.7"], "comment": "9 main pages (21 appendix pages), 7 figures, submitted to ACL 2025", "summary": "The performance and usability of Large-Language Models (LLMs) are driving\ntheir use in explanation generation tasks. However, despite their widespread\nadoption, LLM explanations have been found to be unreliable, making it\ndifficult for users to distinguish good from bad explanations. To address this\nissue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of\n26k explanations, written and later quality-annotated using the rubric by both\nhumans and six open- and closed-source LLMs. The CUBE dataset focuses on two\nreasoning and two language tasks, providing the necessary diversity for us to\neffectively test our proposed rubric. Using Rubrik, we find that explanations\nare influenced by both task and perceived difficulty. Low quality stems\nprimarily from a lack of conciseness in LLM-generated explanations, rather than\ncohesion and word choice. The full dataset, rubric, and code will be made\navailable upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "rubric"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24027", "pdf": "https://arxiv.org/pdf/2503.24027", "abs": "https://arxiv.org/abs/2503.24027", "authors": ["Florian Carichon", "Romain Rampa", "Golnoosh Farnadi"], "title": "Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes", "categories": ["cs.CL"], "comment": null, "summary": "Novelty modeling and detection is a core topic in Natural Language Processing\n(NLP), central to numerous tasks such as recommender systems and automatic\nsummarization. It involves identifying pieces of text that deviate in some way\nfrom previously known information. However, novelty is also a crucial\ndeterminant of the unique perception of relevance and quality of an experience,\nas it rests upon each individual's understanding of the world. Social factors,\nparticularly cultural background, profoundly influence perceptions of novelty\nand innovation. Cultural novelty arises from differences in salience and\nnovelty as shaped by the distance between distinct communities. While cultural\ndiversity has garnered increasing attention in artificial intelligence (AI),\nthe lack of robust metrics for quantifying cultural novelty hinders a deeper\nunderstanding of these divergences. This gap limits quantifying and\nunderstanding cultural differences within computational frameworks. To address\nthis, we propose an interdisciplinary framework that integrates knowledge from\nsociology and management. Central to our approach is GlobalFusion, a novel\ndataset comprising 500 dishes and approximately 100,000 cooking recipes\ncapturing cultural adaptation from over 150 countries. By introducing a set of\nJensen-Shannon Divergence metrics for novelty, we leverage this dataset to\nanalyze textual divergences when recipes from one community are modified by\nanother with a different cultural background. The results reveal significant\ncorrelations between our cultural novelty metrics and established cultural\nmeasures based on linguistic, religious, and geographical distances. Our\nfindings highlight the potential of our framework to advance the understanding\nand measurement of cultural diversity in AI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23359", "pdf": "https://arxiv.org/pdf/2503.23359", "abs": "https://arxiv.org/abs/2503.23359", "authors": ["Linfeng Tang", "Yeda Wang", "Meiqi Gong", "Zizhuo Li", "Yuxin Deng", "Xunpeng Yi", "Chunyu Li", "Han Xu", "Hao Zhang", "Jiayi Ma"], "title": "VideoFusion: A Spatio-Temporal Collaborative Network for Mutli-modal Video Fusion and Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Compared to images, videos better align with real-world acquisition scenarios\nand possess valuable temporal cues. However, existing multi-sensor fusion\nresearch predominantly integrates complementary context from multiple images\nrather than videos. This primarily stems from two factors: 1) the scarcity of\nlarge-scale multi-sensor video datasets, limiting research in video fusion, and\n2) the inherent difficulty of jointly modeling spatial and temporal\ndependencies in a unified framework. This paper proactively compensates for the\ndilemmas. First, we construct M3SVD, a benchmark dataset with $220$ temporally\nsynchronized and spatially registered infrared-visible video pairs comprising\n153,797 frames, filling the data gap for the video fusion community. Secondly,\nwe propose VideoFusion, a multi-modal video fusion model that fully exploits\ncross-modal complementarity and temporal dynamics to generate spatio-temporally\ncoherent videos from (potentially degraded) multi-modal inputs. Specifically,\n1) a differential reinforcement module is developed for cross-modal information\ninteraction and enhancement, 2) a complete modality-guided fusion strategy is\nemployed to adaptively integrate multi-modal features, and 3) a bi-temporal\nco-attention mechanism is devised to dynamically aggregate forward-backward\ntemporal contexts to reinforce cross-frame feature representations. Extensive\nexperiments reveal that VideoFusion outperforms existing image-oriented fusion\nparadigms in sequential scenarios, effectively mitigating temporal\ninconsistency and interference.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23365", "pdf": "https://arxiv.org/pdf/2503.23365", "abs": "https://arxiv.org/abs/2503.23365", "authors": ["Zhangcun Yan", "Jianqing Li", "Peng Hang", "Jian Sun"], "title": "OnSiteVRU: A High-Resolution Trajectory Dataset for High-Density Vulnerable Road Users", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "With the acceleration of urbanization and the growth of transportation\ndemands, the safety of vulnerable road users (VRUs, such as pedestrians and\ncyclists) in mixed traffic flows has become increasingly prominent,\nnecessitating high-precision and diverse trajectory data to support the\ndevelopment and optimization of autonomous driving systems. However, existing\ndatasets fall short in capturing the diversity and dynamics of VRU behaviors,\nmaking it difficult to meet the research demands of complex traffic\nenvironments. To address this gap, this study developed the OnSiteVRU datasets,\nwhich cover a variety of scenarios, including intersections, road segments, and\nurban villages. These datasets provide trajectory data for motor vehicles,\nelectric bicycles, and human-powered bicycles, totaling approximately 17,429\ntrajectories with a precision of 0.04 seconds. The datasets integrate both\naerial-view natural driving data and onboard real-time dynamic detection data,\nalong with environmental information such as traffic signals, obstacles, and\nreal-time maps, enabling a comprehensive reconstruction of interaction events.\nThe results demonstrate that VRU\\_Data outperforms traditional datasets in\nterms of VRU density and scene coverage, offering a more comprehensive\nrepresentation of VRU behavioral characteristics. This provides critical\nsupport for traffic flow modeling, trajectory prediction, and autonomous\ndriving virtual testing. The dataset is publicly available for download at:\n  https://www.kaggle.com/datasets/zcyan2/mixed-traffic-trajectory-dataset-in-from-shanghai.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23388", "pdf": "https://arxiv.org/pdf/2503.23388", "abs": "https://arxiv.org/abs/2503.23388", "authors": ["Fanding Huang", "Jingyan Jiang", "Qinting Jiang", "Hebei Li", "Faisal Nadeem Khan", "Zhi Wang"], "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to CVPR 2025", "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24307", "pdf": "https://arxiv.org/pdf/2503.24307", "abs": "https://arxiv.org/abs/2503.24307", "authors": ["Arshia Kermani", "Veronica Perez-Rosas", "Vangelis Metsis"], "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22692", "pdf": "https://arxiv.org/pdf/2503.22692", "abs": "https://arxiv.org/abs/2503.22692", "authors": ["Shokoufeh Mirzaei", "Jesse Arzate", "Yukti Vijay"], "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "comment": "14 pages, 4 Figures, 4 Tables, Under review by Journal of Aerospace\n  Information Systems", "summary": "Transcription of aviation communications has several applications, from\nassisting air traffic controllers in identifying the accuracy of read-back\nerrors to search and rescue operations. Recent advances in artificial\nintelligence have provided unprecedented opportunities for improving aviation\ncommunication transcription tasks. OpenAI's Whisper is one of the leading\nautomatic speech recognition models. However, fine-tuning Whisper for aviation\ncommunication transcription is not computationally efficient. Thus, this paper\naims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation\nto fine-tune a more computationally efficient version of Whisper,\ndistil-Whisper. To perform the fine-tuning, we used the Air Traffic Control\nCorpus dataset from the Linguistic Data Consortium, which contains\napproximately 70 hours of controller and pilot transmissions near three major\nairports in the US. The objective was to reduce the word error rate to enhance\naccuracy in the transcription of aviation communication. First, starting with\nan initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we\nperformed a grid search. We applied a 5-fold cross-validation to find the best\ncombination of distil-Whisper hyperparameters. Then, we fine-tuned the model\nfor LoRA hyperparameters, achieving an impressive average word error rate of\n3.86% across five folds. This result highlights the model's potential for use\nin the cockpit.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23452", "pdf": "https://arxiv.org/pdf/2503.23452", "abs": "https://arxiv.org/abs/2503.23452", "authors": ["Yuhang Yang", "Ke Fan", "Shangkun Sun", "Hongxiang Li", "Ailing Zeng", "FeiLin Han", "Wei Zhai", "Wei Liu", "Yang Cao", "Zheng-Jun Zha"], "title": "VideoGen-Eval: Agent-based System for Video Generation Evaluation", "categories": ["cs.CV"], "comment": "project:https://github.com/AILab-CVC/VideoGen-Eval", "summary": "The rapid advancement of video generation has rendered existing evaluation\nsystems inadequate for assessing state-of-the-art models, primarily due to\nsimple prompts that cannot showcase the model's capabilities, fixed evaluation\noperators struggling with Out-of-Distribution (OOD) cases, and misalignment\nbetween computed metrics and human preferences. To bridge the gap, we propose\nVideoGen-Eval, an agent evaluation system that integrates LLM-based content\nstructuring, MLLM-based content judgment, and patch tools designed for\ntemporal-dense dimensions, to achieve a dynamic, flexible, and expandable video\ngeneration evaluation. Additionally, we introduce a video generation benchmark\nto evaluate existing cutting-edge models and verify the effectiveness of our\nevaluation system. It comprises 700 structured, content-rich prompts (both T2V\nand I2V) and over 12,000 videos generated by 20+ models, among them, 8\ncutting-edge models are selected as quantitative evaluation for the agent and\nhuman. Extensive experiments validate that our proposed agent-based evaluation\nsystem demonstrates strong alignment with human preferences and reliably\ncompletes the evaluation, as well as the diversity and richness of the\nbenchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23453", "pdf": "https://arxiv.org/pdf/2503.23453", "abs": "https://arxiv.org/abs/2503.23453", "authors": ["Maofu Liu", "Jiahui Liu", "Xiaokang Zhang"], "title": "Semantic-Spatial Feature Fusion with Dynamic Graph Refinement for Remote Sensing Image Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image captioning aims to generate semantically accurate\ndescriptions that are closely linked to the visual features of remote sensing\nimages. Existing approaches typically emphasize fine-grained extraction of\nvisual features and capturing global information. However, they often overlook\nthe complementary role of textual information in enhancing visual semantics and\nface challenges in precisely locating objects that are most relevant to the\nimage context. To address these challenges, this paper presents a\nsemantic-spatial feature fusion with dynamic graph refinement (SFDR) method,\nwhich integrates the semantic-spatial feature fusion (SSFF) and dynamic graph\nfeature refinement (DGFR) modules. The SSFF module utilizes a multi-level\nfeature representation strategy by leveraging pre-trained CLIP features, grid\nfeatures, and ROI features to integrate rich semantic and spatial information.\nIn the DGFR module, a graph attention network captures the relationships\nbetween feature nodes, while a dynamic weighting mechanism prioritizes objects\nthat are most relevant to the current scene and suppresses less significant\nones. Therefore, the proposed SFDR method significantly enhances the quality of\nthe generated descriptions. Experimental results on three benchmark datasets\ndemonstrate the effectiveness of the proposed method. The source code will be\navailable at https://github.com/zxk688}{https://github.com/zxk688.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22708", "pdf": "https://arxiv.org/pdf/2503.22708", "abs": "https://arxiv.org/abs/2503.22708", "authors": ["Peter Jansen", "Oyvind Tafjord", "Marissa Radensky", "Pao Siangliulue", "Tom Hope", "Bhavana Dalvi Mishra", "Bodhisattwa Prasad Majumder", "Daniel S. Weld", "Peter Clark"], "title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation", "categories": ["cs.AI", "cs.CL"], "comment": "98 Pages (13 pages: main paper body; 85 pages: appendix)", "summary": "Despite the surge of interest in autonomous scientific discovery (ASD) of\nsoftware artifacts (e.g., improved ML algorithms), current ASD systems face two\nkey limitations: (1) they largely explore variants of existing codebases or\nsimilarly constrained design spaces, and (2) they produce large volumes of\nresearch artifacts (such as automatically generated papers and code) that are\ntypically evaluated using conference-style paper review with limited evaluation\nof code. In this work we introduce CodeScientist, a novel ASD system that\nframes ideation and experiment construction as a form of genetic search jointly\nover combinations of research articles and codeblocks defining common actions\nin a domain (like prompting a language model). We use this paradigm to conduct\nhundreds of automated experiments on machine-generated ideas broadly in the\ndomain of agents and virtual environments, with the system returning 19\ndiscoveries, 6 of which were judged as being both at least minimally sound and\nincrementally novel after a multi-faceted evaluation beyond that typically\nconducted in prior work, including external (conference-style) review, code\nreview, and replication attempts. Moreover, the discoveries span new tasks,\nagents, metrics, and data, suggesting a qualitative shift from benchmark\noptimization to broader discoveries.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23456", "pdf": "https://arxiv.org/pdf/2503.23456", "abs": "https://arxiv.org/abs/2503.23456", "authors": ["Maofu Liu", "Xin Jiang", "Xiaokang Zhang"], "title": "CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer for Referring Remote Sensing Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task,\naiming to segment specific target objects in remote sensing (RS) images based\non a given language expression. Existing RRSIS methods typically employ\ncoarse-grained unidirectional alignment approaches to obtain multimodal\nfeatures, and they often overlook the critical role of language features as\ncontextual information during the decoding process. Consequently, these methods\nexhibit weak object-level correspondence between visual and language features,\nleading to incomplete or erroneous predicted masks, especially when handling\ncomplex expressions and intricate RS image scenes. To address these challenges,\nwe propose a fine-grained cross-modal alignment and decoding Transformer,\nCADFormer, for RRSIS. Specifically, we design a semantic mutual guidance\nalignment module (SMGAM) to achieve both vision-to-language and\nlanguage-to-vision alignment, enabling comprehensive integration of visual and\ntextual features for fine-grained cross-modal alignment. Furthermore, a\ntextual-enhanced cross-modal decoder (TCMD) is introduced to incorporate\nlanguage features during decoding, using refined textual information as context\nto enhance the relationship between cross-modal features. To thoroughly\nevaluate the performance of CADFormer, especially for inconspicuous targets in\ncomplex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which\nincludes larger high-resolution RS image patches and semantically richer\nlanguage expressions. Extensive experiments on the RRSIS-HR dataset and the\npopular RRSIS-D dataset demonstrate the effectiveness and superiority of\nCADFormer. Datasets and source codes will be available at\nhttps://github.com/zxk688.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23459", "pdf": "https://arxiv.org/pdf/2503.23459", "abs": "https://arxiv.org/abs/2503.23459", "authors": ["Chenglong Lu", "Shen Liang", "Xuewei Wang", "Wei Wang"], "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach", "categories": ["cs.CV"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025", "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically\nwith the number of tokens, calling for effective token pruning policies. Most\nexisting policies are handcrafted, lacking adaptivity to varying inputs.\nMoreover, they fail to consider the sequential nature of token pruning across\nmultiple layers. In this work, for the first time (as far as we know), we\nexploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy.\nFormulating token pruning as a sequential decision-making problem, we model it\nas a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO)\nwhere each agent makes an individualized pruning decision for a single token.\nWe also develop reward functions that enable simultaneous collaboration and\ncompetition of these agents to balance efficiency and accuracy. On the\nwell-known ImageNet-1k dataset, our method improves the inference speed by up\nto 44% while incurring only a negligible accuracy drop of 0.4%. The source code\nis available at https://github.com/daashuai/rl4evit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23461", "pdf": "https://arxiv.org/pdf/2503.23461", "abs": "https://arxiv.org/abs/2503.23461", "authors": ["Nikai Du", "Zhennan Chen", "Zhizhou Chen", "Shan Gao", "Xi Chen", "Zhengkai Jiang", "Jian Yang", "Ying Tai"], "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23472", "pdf": "https://arxiv.org/pdf/2503.23472", "abs": "https://arxiv.org/abs/2503.23472", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Efficient Dynamic Attention 3D Convolution for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including insufficient utilization of joint spatial-spectral\ninformation, gradient vanishing with increasing depth, and overfitting. To\nenhance feature extraction efficiency while skipping redundant information,\nthis paper proposes a dynamic attention convolution design based on an improved\n3D-DenseNet model. The design employs multiple parallel convolutional kernels\ninstead of a single kernel and assigns dynamic attention weights to these\nparallel convolutions. This dynamic attention mechanism achieves adaptive\nfeature response based on spatial characteristics in the spatial dimension of\nhyperspectral images, focusing more on key spatial structures. In the spectral\ndimension, it enables dynamic discrimination of different bands, alleviating\ninformation redundancy and computational complexity caused by high spectral\ndimensionality. The DAC module enhances model representation capability by\nattention-based aggregation of multiple convolutional kernels without\nincreasing network depth or width. The proposed method demonstrates superior\nperformance in both inference speed and accuracy, outperforming mainstream\nhyperspectral image classification methods on the IN, UP, and KSC datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22879", "pdf": "https://arxiv.org/pdf/2503.22879", "abs": "https://arxiv.org/abs/2503.22879", "authors": ["Hung-Yueh Chiang", "Chi-Chih Chang", "Natalia Frumkin", "Kai-Chiang Wu", "Mohamed S. Abdelfattah", "Diana Marculescu"], "title": "Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF"], "comment": null, "summary": "State Space Models (SSMs) are emerging as a compelling alternative to\nTransformers because of their consistent memory usage and high performance.\nDespite this, scaling up SSMs on cloud services or limited-resource devices is\nchallenging due to their storage requirements and computational power. To\novercome this, quantizing SSMs with low bit-width data formats can reduce model\nsize and benefit from hardware acceleration. As SSMs are prone to\nquantization-induced errors, recent efforts have focused on optimizing a\nparticular model or bit-width for efficiency without sacrificing performance.\nHowever, distinct bit-width configurations are essential for different\nscenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for\nenhancing generation speed in short prompt applications for a single user. To\nthis end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both\nMamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment\non various platforms. Based on the channel order preserving and activation\npersistence of SSMs, we propose an offline approach to quantize inputs of a\nlinear recurrence in 8-bit by sorting and clustering for input $x$, combined\nwith a per-state-group quantization for input-dependent parameters $B$ and $C$.\nTo ensure compute-invariance in the SSM output, we rearrange weights offline\naccording to the clustering sequence. The experiments show that Quamba2-8B\noutperforms several state-of-the-art SSM quantization methods and delivers\n1.3$\\times$ and 3$\\times$ speed-ups in the pre-filling and generation stages,\nrespectively, while offering 4$\\times$ memory reduction with only a $1.6\\%$\naverage accuracy drop. The evaluation on MMLU shows the generalizability and\nrobustness of our framework. The code and quantized models will be released at:\nhttps://github.com/enyac-group/Quamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23502", "pdf": "https://arxiv.org/pdf/2503.23502", "abs": "https://arxiv.org/abs/2503.23502", "authors": ["Jannik Endres", "Oliver Hahn", "Charles Corbière", "Simone Schaub-Meyer", "Stefan Roth", "Alexandre Alahi"], "title": "Boosting Omnidirectional Stereo Matching with a Pre-trained Depth Foundation Model", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project page: https://vita-epfl.github.io/DFI-OmniStereo-website/", "summary": "Omnidirectional depth perception is essential for mobile robotics\napplications that require scene understanding across a full 360{\\deg} field of\nview. Camera-based setups offer a cost-effective option by using stereo depth\nestimation to generate dense, high-resolution depth maps without relying on\nexpensive active sensing. However, existing omnidirectional stereo matching\napproaches achieve only limited depth accuracy across diverse environments,\ndepth ranges, and lighting conditions, due to the scarcity of real-world data.\nWe present DFI-OmniStereo, a novel omnidirectional stereo matching method that\nleverages a large-scale pre-trained foundation model for relative monocular\ndepth estimation within an iterative optimization-based stereo matching\narchitecture. We introduce a dedicated two-stage training strategy to utilize\nthe relative monocular depth features for our omnidirectional stereo matching\nbefore scale-invariant fine-tuning. DFI-OmniStereo achieves state-of-the-art\nresults on the real-world Helvipad dataset, reducing disparity MAE by\napproximately 16% compared to the previous best omnidirectional stereo method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22989", "pdf": "https://arxiv.org/pdf/2503.22989", "abs": "https://arxiv.org/abs/2503.22989", "authors": ["Gabriel Recchia", "Chatrik Singh Mangat", "Issac Li", "Gayatri Krishnakumar"], "title": "FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research", "categories": ["cs.AI", "cs.CL", "I.2"], "comment": "43 pages, 3 figures. for associated repository, see\n  https://github.com/modulo-research/findtheflaws", "summary": "As AI models tackle increasingly complex problems, ensuring reliable human\noversight becomes more challenging due to the difficulty of verifying\nsolutions. Approaches to scaling AI supervision include debate, in which two\nagents engage in structured dialogue to help a judge evaluate claims; critique,\nin which models identify potential flaws in proposed solutions; and\nprover-verifier games, in which a capable 'prover' model generates solutions\nthat must be verifiable by a less capable 'verifier'. Evaluations of the\nscalability of these and similar approaches to difficult problems benefit from\ndatasets that include (1) long-form expert-verified correct solutions and (2)\nlong-form flawed solutions with annotations highlighting specific errors, but\nfew are available.\n  To address this gap, we present FindTheFlaws, a group of five diverse\ndatasets spanning medicine, mathematics, science, coding, and the Lojban\nlanguage. Each dataset contains questions and long-form solutions with expert\nannotations validating their correctness or identifying specific error(s) in\nthe reasoning. We evaluate frontier models' critiquing capabilities and observe\na range of performance that can be leveraged for scalable oversight\nexperiments: models performing more poorly on particular datasets can serve as\njudges/verifiers for more capable models. Additionally, for some task/dataset\ncombinations, expert baselines exceed even top model performance, making them\nmore beneficial for scalable oversight experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23130", "pdf": "https://arxiv.org/pdf/2503.23130", "abs": "https://arxiv.org/abs/2503.23130", "authors": ["Boyi Ma", "Yanguang Zhao", "Jie Wang", "Guankun Wang", "Kun Yuan", "Tong Chen", "Long Bai", "Hongliang Ren"], "title": "Can DeepSeek-V3 Reason Like a Surgeon? An Empirical Evaluation for Vision-Language Understanding in Robotic-Assisted Surgery", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": "Technical Report", "summary": "DeepSeek-V3, a recently emerging Large Language Model (LLM), demonstrates\noutstanding performance in general scene understanding, question-answering\n(QA), and text generation tasks, owing to its efficient training paradigm and\nstrong reasoning capabilities. In this study, we investigate the dialogue\ncapabilities of DeepSeek-V3 in robotic surgery scenarios, focusing on tasks\nsuch as Single Phrase QA, Visual QA, and Detailed Description. The Single\nPhrase QA tasks further include sub-tasks such as surgical instrument\nrecognition, action understanding, and spatial position analysis. We conduct\nextensive evaluations using publicly available datasets, including EndoVis18\nand CholecT50, along with their corresponding dialogue data. Our comprehensive\nevaluation results indicate that, when provided with specific prompts,\nDeepSeek-V3 performs well in surgical instrument and tissue recognition tasks\nHowever, DeepSeek-V3 exhibits significant limitations in spatial position\nanalysis and struggles to understand surgical actions accurately. Additionally,\nour findings reveal that, under general prompts, DeepSeek-V3 lacks the ability\nto effectively analyze global surgical concepts and fails to provide detailed\ninsights into surgical scenarios. Based on our observations, we argue that the\nDeepSeek-V3 is not ready for vision-language tasks in surgical contexts without\nfine-tuning on surgery-specific datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23239", "pdf": "https://arxiv.org/pdf/2503.23239", "abs": "https://arxiv.org/abs/2503.23239", "authors": ["Reza Esfandiarpoor", "George Zerveas", "Ruochen Zhang", "Macton Mgonzo", "Carsten Eickhoff", "Stephen H. Bach"], "title": "Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Code: https://github.com/BatsResearch/sycl", "summary": "Recent advancements in large language models (LLMs) have allowed the\naugmentation of information retrieval (IR) pipelines with synthetic data in\nvarious ways. Yet, the main training paradigm remains: contrastive learning\nwith binary relevance labels and the InfoNCE loss, where one positive document\nis compared against one or more negatives. This objective treats all documents\nthat are not explicitly annotated as relevant on an equally negative footing,\nregardless of their actual degree of relevance, thus (a) missing subtle nuances\nthat are useful for ranking and (b) being susceptible to annotation noise. To\novercome this limitation, in this work we forgo real training documents and\nannotations altogether and use open-source LLMs to directly generate synthetic\ndocuments that answer real user queries according to several different levels\nof relevance. This fully synthetic ranking context of graduated relevance,\ntogether with an appropriate list-wise loss (Wasserstein distance), enables us\nto train dense retrievers in a way that better captures the ranking task.\nExperiments on various IR datasets show that our proposed approach outperforms\nconventional training with InfoNCE by a large margin. Without using any real\ndocuments for training, our dense retriever significantly outperforms the same\nretriever trained through self-supervision. More importantly, it matches the\nperformance of the same retriever trained on real, labeled training documents\nof the same dataset, while being more robust to distribution shift and clearly\noutperforming it when evaluated zero-shot on the BEIR dataset collection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23424", "pdf": "https://arxiv.org/pdf/2503.23424", "abs": "https://arxiv.org/abs/2503.23424", "authors": ["Gil Gekker", "Meirav Segal", "Dan Lahav", "Omer Nevo"], "title": "What Makes an Evaluation Useful? Common Pitfalls and Best Practices", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Following the rapid increase in Artificial Intelligence (AI) capabilities in\nrecent years, the AI community has voiced concerns regarding possible safety\nrisks. To support decision-making on the safe use and development of AI\nsystems, there is a growing need for high-quality evaluations of dangerous\nmodel capabilities. While several attempts to provide such evaluations have\nbeen made, a clear definition of what constitutes a \"good evaluation\" has yet\nto be agreed upon. In this practitioners' perspective paper, we present a set\nof best practices for safety evaluations, drawing on prior work in model\nevaluation and illustrated through cybersecurity examples. We first discuss the\nsteps of the initial thought process, which connects threat modeling to\nevaluation design. Then, we provide the characteristics and parameters that\nmake an evaluation useful. Finally, we address additional considerations as we\nmove from building specific evaluations to building a full and comprehensive\nevaluation suite.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23448", "pdf": "https://arxiv.org/pdf/2503.23448", "abs": "https://arxiv.org/abs/2503.23448", "authors": ["Max Hort", "Linas Vidziunas", "Leon Moonen"], "title": "Semantic-Preserving Transformations as Mutation Operators: A Study on Their Effectiveness in Defect Detection", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in Mutation 2025 at the 18th IEEE\n  International Conference on Software Testing, Verification and Validation\n  (ICST 2025)", "summary": "Recent advances in defect detection use language models. Existing works\nenhanced the training data to improve the models' robustness when applied to\nsemantically identical code (i.e., predictions should be the same). However,\nthe use of semantically identical code has not been considered for improving\nthe tools during their application - a concept closely related to metamorphic\ntesting.\n  The goal of our study is to determine whether we can use semantic-preserving\ntransformations, analogue to mutation operators, to improve the performance of\ndefect detection tools in the testing stage. We first collect existing\npublications which implemented semantic-preserving transformations and share\ntheir implementation, such that we can reuse them. We empirically study the\neffectiveness of three different ensemble strategies for enhancing defect\ndetection tools. We apply the collected transformations on the Devign dataset,\nconsidering vulnerabilities as a type of defect, and two fine-tuned large\nlanguage models for defect detection (VulBERTa, PLBART). We found 28\npublications with 94 different transformations.\n  We choose to implement 39 transformations from four of the publications, but\na manual check revealed that 23 out 39 transformations change code semantics.\nUsing the 16 remaining, correct transformations and three ensemble strategies,\nwe were not able to increase the accuracy of the defect detection models. Our\nresults show that reusing shared semantic-preserving transformation is\ndifficult, sometimes even causing wrongful changes to the semantics.\n  Keywords: defect detection, language model, semantic-preserving\ntransformation, ensemble", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23466", "pdf": "https://arxiv.org/pdf/2503.23466", "abs": "https://arxiv.org/abs/2503.23466", "authors": ["Max Hort", "Leon Moonen"], "title": "Codehacks: A Dataset of Adversarial Tests for Competitive Programming Problems Obtained from Codeforces", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication at the 18th IEEE International Conference on\n  Software Testing, Verification and Validation (ICST 2025)", "summary": "Software is used in critical applications in our day-to-day life and it is\nimportant to ensure its correctness. One popular approach to assess correctness\nis to evaluate software on tests. If a test fails, it indicates a fault in the\nsoftware under test; if all tests pass correctly, one may assume that the\nsoftware is correct. However, the reliability of these results depends on the\ntest suite considered, and there is a risk of false negatives (i.e. software\nthat passes all available tests but contains bugs because some cases are not\ntested). Therefore, it is important to consider error-inducing test cases when\nevaluating software.\n  To support data-driven creation of such a test-suite, which is especially of\ninterest for testing software synthesized from large language models, we curate\na dataset (Codehacks) of programming problems together with corresponding\nerror-inducing test cases (i.e., \"hacks\"). This dataset is collected from the\nwild, in particular, from the Codeforces online judge platform. The dataset\ncomprises 288,617 hacks for 5,578 programming problems, each with a natural\nlanguage description, as well as the source code for 2,196 submitted solutions\nto these problems that can be broken with their corresponding hacks.\n  Keywords: competitive programming, language model, dataset", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23667", "pdf": "https://arxiv.org/pdf/2503.23667", "abs": "https://arxiv.org/abs/2503.23667", "authors": ["Kotaro Inoue"], "title": "Context-Independent OCR with Multimodal LLMs: Effects of Image Resolution and Visual Complexity", "categories": ["cs.CV"], "comment": null, "summary": "Due to their high versatility in tasks such as image captioning, document\nanalysis, and automated content generation, multimodal Large Language Models\n(LLMs) have attracted significant attention across various industrial fields.\nIn particular, they have been shown to surpass specialized models in Optical\nCharacter Recognition (OCR). Nevertheless, their performance under different\nimage conditions remains insufficiently investigated, and individual character\nrecognition is not guaranteed due to their reliance on contextual cues. In this\nwork, we examine a context-independent OCR task using single-character images\nwith diverse visual complexities to determine the conditions for accurate\nrecognition. Our findings reveal that multimodal LLMs can match conventional\nOCR methods at about 300 ppi, yet their performance deteriorates significantly\nbelow 150 ppi. Additionally, we observe a very weak correlation between visual\ncomplexity and misrecognitions, whereas a conventional OCR-specific model\nexhibits no correlation. These results suggest that image resolution and visual\ncomplexity may play an important role in the reliable application of multimodal\nLLMs to OCR tasks that require precise character-level accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23702", "pdf": "https://arxiv.org/pdf/2503.23702", "abs": "https://arxiv.org/abs/2503.23702", "authors": ["Shufan Xi", "Zexian Liu", "Junlin Chang", "Hongyu Wu", "Xiaogang Wang", "Aimin Hao"], "title": "3D Dental Model Segmentation with Geometrical Boundary Preserving", "categories": ["cs.CV"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2025", "summary": "3D intraoral scan mesh is widely used in digital dentistry diagnosis,\nsegmenting 3D intraoral scan mesh is a critical preliminary task. Numerous\napproaches have been devised for precise tooth segmentation. Currently, the\ndeep learning-based methods are capable of the high accuracy segmentation of\ncrown. However, the segmentation accuracy at the junction between the crown and\nthe gum is still below average. Existing down-sampling methods are unable to\neffectively preserve the geometric details at the junction. To address these\nproblems, we propose CrossTooth, a boundary-preserving segmentation method that\ncombines 3D mesh selective downsampling to retain more vertices at the\ntooth-gingiva area, along with cross-modal discriminative boundary features\nextracted from multi-view rendered images, enhancing the geometric\nrepresentation of the segmentation network. Using a point network as a backbone\nand incorporating image complementary features, CrossTooth significantly\nimproves segmentation accuracy, as demonstrated by experiments on a public\nintraoral scan dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23715", "pdf": "https://arxiv.org/pdf/2503.23715", "abs": "https://arxiv.org/abs/2503.23715", "authors": ["Kun Liu", "Qi Liu", "Xinchen Liu", "Jie Li", "Yongdong Zhang", "Jiebo Luo", "Xiaodong He", "Wu Liu"], "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text-to-video (T2V) generation has made tremendous progress in generating\ncomplicated scenes based on texts. However, human-object interaction (HOI)\noften cannot be precisely generated by current T2V models due to the lack of\nlarge-scale videos with accurate captions for HOI. To address this issue, we\nintroduce HOIGen-1M, the first largescale dataset for HOI Generation,\nconsisting of over one million high-quality videos collected from diverse\nsources. In particular, to guarantee the high quality of videos, we first\ndesign an efficient framework to automatically curate HOI videos using the\npowerful multimodal large language models (MLLMs), and then the videos are\nfurther cleaned by human annotators. Moreover, to obtain accurate textual\ncaptions for HOI videos, we design a novel video description method based on a\nMixture-of-Multimodal-Experts (MoME) strategy that not only generates\nexpressive captions but also eliminates the hallucination by individual MLLM.\nFurthermore, due to the lack of an evaluation framework for generated HOI\nvideos, we propose two new metrics to assess the quality of generated videos in\na coarse-to-fine manner. Extensive experiments reveal that current T2V models\nstruggle to generate high-quality HOI videos and confirm that our HOIGen-1M\ndataset is instrumental for improving HOI video generation. Project webpage is\navailable at https://liuqi-creat.github.io/HOIGen.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24358", "pdf": "https://arxiv.org/pdf/2503.24358", "abs": "https://arxiv.org/abs/2503.24358", "authors": ["Hao Wang", "Ligong Han", "Kai Xu", "Akash Srivastava"], "title": "SQuat: Subspace-orthogonal KV Cache Quantization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23764", "pdf": "https://arxiv.org/pdf/2503.23764", "abs": "https://arxiv.org/abs/2503.23764", "authors": ["Md Mahfuz Al Hasan", "Mahdi Zaman", "Abdul Jawad", "Alberto Santamaria-Pang", "Ho Hin Lee", "Ivan Tarapov", "Kyle See", "Md Shah Imran", "Antika Roy", "Yaser Pourmohammadi Fallah", "Navid Asadizanjani", "Reza Forghani"], "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limi- tations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual rep- resentation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architec- ture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency de- tails while replacing heavy upsampling layers\nwith efficient wavelet-based summarization and reconstruction. This\nsignificantly reduces the number of parameters, which is critical for\nreal-world deployment where compu- tational resources and training times are\nconstrained. Furthermore, the model is generic and easily adaptable to diverse\napplications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrate\nperformance on par with state-of-the-art methods while offering substantially\nlower computational complexity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23862", "pdf": "https://arxiv.org/pdf/2503.23862", "abs": "https://arxiv.org/abs/2503.23862", "authors": ["SeonYeong Lee", "EonSeung Seong", "DongEon Lee", "SiYeoul Lee", "Yubin Cho", "Chunsu Park", "Seonho Kim", "MinKyoung Seo", "YoungSin Ko", "MinWoo Kim"], "title": "Learned Image Compression and Restoration for Digital Pathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital pathology images play a crucial role in medical diagnostics, but\ntheir ultra-high resolution and large file sizes pose significant challenges\nfor storage, transmission, and real-time visualization. To address these\nissues, we propose CLERIC, a novel deep learning-based image compression\nframework designed specifically for whole slide images (WSIs). CLERIC\nintegrates a learnable lifting scheme and advanced convolutional techniques to\nenhance compression efficiency while preserving critical pathological details.\nOur framework employs a lifting-scheme transform in the analysis stage to\ndecompose images into low- and high-frequency components, enabling more\nstructured latent representations. These components are processed through\nparallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent\nResidual Blocks (R2B) to improve feature extraction and spatial adaptability.\nThe synthesis stage applies an inverse lifting transform for effective image\nreconstruction, ensuring high-fidelity restoration of fine-grained tissue\nstructures. We evaluate CLERIC on a digital pathology image dataset and compare\nits performance against state-of-the-art learned image compression (LIC)\nmodels. Experimental results demonstrate that CLERIC achieves superior\nrate-distortion (RD) performance, significantly reducing storage requirements\nwhile maintaining high diagnostic image quality. Our study highlights the\npotential of deep learning-based compression in digital pathology, facilitating\nefficient data management and long-term storage while ensuring seamless\nintegration into clinical workflows and AI-assisted diagnostic systems. Code\nand models are available at: https://github.com/pnu-amilab/CLERIC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23925", "pdf": "https://arxiv.org/pdf/2503.23925", "abs": "https://arxiv.org/abs/2503.23925", "authors": ["Zizhuo Li", "Yifan Lu", "Linfeng Tang", "Shihua Zhang", "Jiayi Ma"], "title": "CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching", "categories": ["cs.CV"], "comment": null, "summary": "This prospective study proposes CoMatch, a novel semi-dense image matcher\nwith dynamic covisibility awareness and bilateral subpixel accuracy. Firstly,\nobserving that modeling context interaction over the entire coarse feature map\nelicits highly redundant computation due to the neighboring representation\nsimilarity of tokens, a covisibility-guided token condenser is introduced to\nadaptively aggregate tokens in light of their covisibility scores that are\ndynamically estimated, thereby ensuring computational efficiency while\nimproving the representational capacity of aggregated tokens simultaneously.\nSecondly, considering that feature interaction with massive non-covisible areas\nis distracting, which may degrade feature distinctiveness, a\ncovisibility-assisted attention mechanism is deployed to selectively suppress\nirrelevant message broadcast from non-covisible reduced tokens, resulting in\nrobust and compact attention to relevant rather than all ones. Thirdly, we find\nthat at the fine-level stage, current methods adjust only the target view's\nkeypoints to subpixel level, while those in the source view remain restricted\nat the coarse level and thus not informative enough, detrimental to keypoint\nlocation-sensitive usages. A simple yet potent fine correlation module is\ndeveloped to refine the matching candidates in both source and target views to\nsubpixel level, attaining attractive performance improvement. Thorough\nexperimentation across an array of public benchmarks affirms CoMatch's\npromising accuracy, efficiency, and generalizability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23930", "pdf": "https://arxiv.org/pdf/2503.23930", "abs": "https://arxiv.org/abs/2503.23930", "authors": ["Jiankai Tang", "Jiacheng Liu", "Renling Tong", "Kai Zhu", "Zhe Li", "Xin Yi", "Junliang Xing", "Yuanchun Shi", "Yuntao Wang"], "title": "Exploring Reliable PPG Authentication on Smartwatches in Daily Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Photoplethysmography (PPG) Sensors, widely deployed in smartwatches, offer a\nsimple and non-invasive authentication approach for daily use. However, PPG\nauthentication faces reliability issues due to motion artifacts from physical\nactivity and physiological variability over time. To address these challenges,\nwe propose MTL-RAPID, an efficient and reliable PPG authentication model, that\nemploys a multitask joint training strategy, simultaneously assessing signal\nquality and verifying user identity. The joint optimization of these two tasks\nin MTL-RAPID results in a structure that outperforms models trained on\nindividual tasks separately, achieving stronger performance with fewer\nparameters. In our comprehensive user studies regarding motion artifacts (N =\n30), time variations (N = 32), and user preferences (N = 16), MTL-RAPID\nachieves a best AUC of 99.2\\% and an EER of 3.5\\%, outperforming existing\nbaselines. We opensource our PPG authentication dataset along with the\nMTL-RAPID model to facilitate future research on GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23959", "pdf": "https://arxiv.org/pdf/2503.23959", "abs": "https://arxiv.org/abs/2503.23959", "authors": ["Bizhe Bai", "Jianjian Cao", "Yadan Luo", "Tao Che"], "title": "Local Information Matters: Inference Acceleration For Grounded Conversation Generation Models Through Adaptive Local-Aware Token Pruning", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "Grounded Conversation Generation (GCG) is an emerging vision-language task\nthat requires models to generate natural language responses seamlessly\nintertwined with corresponding object segmentation masks. Recent models, such\nas GLaMM and OMG-LLaVA, achieve pixel-level grounding but incur significant\ncomputational costs due to processing a large number of visual tokens. Existing\ntoken pruning methods, like FastV and PyramidDrop, fail to preserve the local\nvisual features critical for accurate grounding, leading to substantial\nperformance drops in GCG tasks. To address this, we propose Adaptive\nLocal-Aware Token Pruning (ALTP), a simple yet effective framework that\naccelerates GCG models by prioritizing local object information. ALTP\nintroduces two key components: (1) Detail Density Capture (DDC), which uses\nsuperpixel segmentation to retain tokens in object-centric regions, preserving\nfine-grained details, and (2) Dynamic Density Formation (DDF), which\ndynamically allocates tokens based on information density, ensuring higher\nretention in semantically rich areas. Extensive experiments on the GranDf\ndataset demonstrate that ALTP significantly outperforms existing token pruning\nmethods, such as FastV and PyramidDrop, on both GLaMM and OMG-LLaVA models.\nNotably, when applied to GLaMM, ALTP achieves a 90% reduction in visual tokens\nwith a 4.9% improvement in AP50 and a 5.0% improvement in Recall compared to\nPyramidDrop. Similarly, on OMG-LLaVA, ALTP improves AP by 2.1% and mIOU by 3.0%\nat a 90% token reduction compared with PDrop.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23980", "pdf": "https://arxiv.org/pdf/2503.23980", "abs": "https://arxiv.org/abs/2503.23980", "authors": ["Yanbo Wang", "Yongtao Chen", "Chuan Cao", "Tianchen Deng", "Wentao Zhao", "Jingchuan Wang", "Weidong Chen"], "title": "SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR\npoint clouds with cross-scene adaptability and 4D consistency. Unlike recent\napproaches that rely on camera distillation, SALT operates directly on raw\nLiDAR data, automatically generating pre-segmentation results. To achieve this,\nwe propose a novel zero-shot learning paradigm, termed data alignment, which\ntransforms LiDAR data into pseudo-images by aligning with the training\ndistribution of vision foundation models. Additionally, we design a\n4D-consistent prompting strategy and 4D non-maximum suppression module to\nenhance SAM2, ensuring high-quality, temporally consistent presegmentation.\nSALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and\nachieves nearly 40-50% of human annotator performance on our newly collected\nlow-resolution LiDAR data and on combined data from three LiDAR types,\nsignificantly boosting annotation efficiency. We anticipate that SALT's\nopen-sourcing will catalyze substantial expansion of current LiDAR datasets and\nlay the groundwork for the future development of LiDAR foundation models. Code\nis available at https://github.com/Cavendish518/SALT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23993", "pdf": "https://arxiv.org/pdf/2503.23993", "abs": "https://arxiv.org/abs/2503.23993", "authors": ["Ming Yuan", "Sichao Wang", "Chuang Zhang", "Lei He", "Qing Xu", "Jianqiang Wang"], "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The depth completion task is a critical problem in autonomous driving,\ninvolving the generation of dense depth maps from sparse depth maps and RGB\nimages. Most existing methods employ a spatial propagation network to\niteratively refine the depth map after obtaining an initial dense depth. In\nthis paper, we propose DenseFormer, a novel method that integrates the\ndiffusion model into the depth completion task. By incorporating the denoising\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\nprogressively refining an initial random depth distribution through multiple\niterations. We propose a feature extraction module that leverages a feature\npyramid structure, along with multi-layer deformable attention, to effectively\nextract and integrate features from sparse depth maps and RGB images, which\nserve as the guiding condition for the diffusion process. Additionally, this\npaper presents a depth refinement module that applies multi-step iterative\nrefinement across various ranges to the dense depth results generated by the\ndiffusion process. The module utilizes image features enriched with multi-scale\ninformation and sparse depth input to further enhance the accuracy of the\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\ndemonstrate that DenseFormer outperforms classical depth completion methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24008", "pdf": "https://arxiv.org/pdf/2503.24008", "abs": "https://arxiv.org/abs/2503.24008", "authors": ["Qi Wu", "Quanlong Zheng", "Yanhao Zhang", "Junlin Xie", "Jinguo Luo", "Kuo Wang", "Peng Liu", "Qingsong Xie", "Ru Zhen", "Haonan Lu", "Zhenyu Yang"], "title": "H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of multimodal models, the demand for assessing\nvideo understanding capabilities has been steadily increasing. However,\nexisting benchmarks for evaluating video understanding exhibit significant\nlimitations in coverage, task diversity, and scene adaptability. These\nshortcomings hinder the accurate assessment of models' comprehensive video\nunderstanding capabilities. To tackle this challenge, we propose a hierarchical\nand holistic video understanding (H2VU) benchmark designed to evaluate both\ngeneral video and online streaming video comprehension. This benchmark\ncontributes three key features:\n  Extended video duration: Spanning videos from brief 3-second clips to\ncomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in\ncurrent benchmarks. Comprehensive assessment tasks: Beyond traditional\nperceptual and reasoning tasks, we have introduced modules for\ncountercommonsense comprehension and trajectory state tracking. These additions\ntest the models' deep understanding capabilities beyond mere prior knowledge.\nEnriched video data: To keep pace with the rapid evolution of current AI\nagents, we have expanded first-person streaming video datasets. This expansion\nallows for the exploration of multimodal models' performance in understanding\nstreaming videos from a first-person perspective. Extensive results from H2VU\nreveal that existing multimodal large language models (MLLMs) possess\nsubstantial potential for improvement in our newly proposed evaluation tasks.\nWe expect that H2VU will facilitate advancements in video understanding\nresearch by offering a comprehensive and in-depth analysis of MLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24014", "pdf": "https://arxiv.org/pdf/2503.24014", "abs": "https://arxiv.org/abs/2503.24014", "authors": ["Minh David Thao Chan", "Ruoyu Zhao", "Yukuan Jia", "Ruiqing Mao", "Sheng Zhou"], "title": "Optimization of Layer Skipping and Frequency Scaling for Convolutional Neural Networks under Latency Constraint", "categories": ["cs.CV"], "comment": "12 pages, 6 figures, Accepted in Proc. Eur. Conf. Comput. Vis. (ECCV)\n  Workshops. Milan, Italy: Springer, September 2024", "summary": "The energy consumption of Convolutional Neural Networks (CNNs) is a critical\nfactor in deploying deep learning models on resource-limited equipment such as\nmobile devices and autonomous vehicles. We propose an approach involving\nProportional Layer Skipping (PLS) and Frequency Scaling (FS). Layer skipping\nreduces computational complexity by selectively bypassing network layers,\nwhereas frequency scaling adjusts the frequency of the processor to optimize\nenergy use under latency constraints. Experiments of PLS and FS on ResNet-152\nwith the CIFAR-10 dataset demonstrated significant reductions in computational\ndemands and energy consumption with minimal accuracy loss. This study offers\npractical solutions for improving real-time processing in resource-limited\nsettings and provides insights into balancing computational efficiency and\nmodel performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24026", "pdf": "https://arxiv.org/pdf/2503.24026", "abs": "https://arxiv.org/abs/2503.24026", "authors": ["Boyuan Wang", "Xiaofeng Wang", "Chaojun Ni", "Guosheng Zhao", "Zhiqin Yang", "Zheng Zhu", "Muyang Zhang", "Yukun Zhou", "Xinze Chen", "Guan Huang", "Lihong Liu", "Xingang Wang"], "title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation", "categories": ["cs.CV"], "comment": "Project Page: https://humandreamer.github.io", "summary": "Human-motion video generation has been a challenging task, primarily due to\nthe difficulty inherent in learning human body movements. While some approaches\nhave attempted to drive human-centric video generation explicitly through pose\ncontrol, these methods typically rely on poses derived from existing videos,\nthereby lacking flexibility. To address this, we propose HumanDreamer, a\ndecoupled human video generation framework that first generates diverse poses\nfrom text prompts and then leverages these poses to generate human-motion\nvideos. Specifically, we propose MotionVid, the largest dataset for\nhuman-motion pose generation. Based on the dataset, we present MotionDiT, which\nis trained to generate structured human-motion poses from text prompts.\nBesides, a novel LAMA loss is introduced, which together contribute to a\nsignificant improvement in FID by 62.4%, along with respective enhancements in\nR-precision for top1, top2, and top3 by 41.8%, 26.3%, and 18.3%, thereby\nadvancing both the Text-to-Pose control accuracy and FID metrics. Our\nexperiments across various Pose-to-Video baselines demonstrate that the poses\ngenerated by our method can produce diverse and high-quality human-motion\nvideos. Furthermore, our model can facilitate other downstream tasks, such as\npose sequence prediction and 2D-3D motion lifting.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24272", "pdf": "https://arxiv.org/pdf/2503.24272", "abs": "https://arxiv.org/abs/2503.24272", "authors": ["Yizhou Huang", "Yihua Cheng", "Kezhi Wang"], "title": "Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Understanding human motion is crucial for accurate pedestrian trajectory\nprediction. Conventional methods typically rely on supervised learning, where\nground-truth labels are directly optimized against predicted trajectories. This\namplifies the limitations caused by long-tailed data distributions, making it\ndifficult for the model to capture abnormal behaviors. In this work, we propose\na self-supervised pedestrian trajectory prediction framework that explicitly\nmodels position, velocity, and acceleration. We leverage velocity and\nacceleration information to enhance position prediction through feature\ninjection and a self-supervised motion consistency mechanism. Our model\nhierarchically injects velocity features into the position stream. Acceleration\nfeatures are injected into the velocity stream. This enables the model to\npredict position, velocity, and acceleration jointly. From the predicted\nposition, we compute corresponding pseudo velocity and acceleration, allowing\nthe model to learn from data-generated pseudo labels and thus achieve\nself-supervised learning. We further design a motion consistency evaluation\nstrategy grounded in physical principles; it selects the most reasonable\npredicted motion trend by comparing it with historical dynamics and uses this\ntrend to guide and constrain trajectory generation. We conduct experiments on\nthe ETH-UCY and Stanford Drone datasets, demonstrating that our method achieves\nstate-of-the-art performance on both datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24282", "pdf": "https://arxiv.org/pdf/2503.24282", "abs": "https://arxiv.org/abs/2503.24282", "authors": ["Jian Wang", "Xin Lan", "Jizhe Zhou", "Yuxin Tian", "Jiancheng Lv"], "title": "Style Quantization for Data-Efficient GAN Training", "categories": ["cs.CV"], "comment": null, "summary": "Under limited data setting, GANs often struggle to navigate and effectively\nexploit the input latent space. Consequently, images generated from adjacent\nvariables in a sparse input latent space may exhibit significant discrepancies\nin realism, leading to suboptimal consistency regularization (CR) outcomes. To\naddress this, we propose \\textit{SQ-GAN}, a novel approach that enhances CR by\nintroducing a style space quantization scheme. This method transforms the\nsparse, continuous input latent space into a compact, structured discrete proxy\nspace, allowing each element to correspond to a specific real data point,\nthereby improving CR performance. Instead of direct quantization, we first map\nthe input latent variables into a less entangled ``style'' space and apply\nquantization using a learnable codebook. This enables each quantized code to\ncontrol distinct factors of variation. Additionally, we optimize the optimal\ntransport distance to align the codebook codes with features extracted from the\ntraining data by a foundation model, embedding external knowledge into the\ncodebook and establishing a semantically rich vocabulary that properly\ndescribes the training dataset. Extensive experiments demonstrate significant\nimprovements in both discriminator robustness and generation quality with our\nmethod.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24306", "pdf": "https://arxiv.org/pdf/2503.24306", "abs": "https://arxiv.org/abs/2503.24306", "authors": ["Adam Schmidt", "Mert Asim Karaoglu", "Soham Sinha", "Mingang Jang", "Ho-Gun Ha", "Kyungmin Jung", "Kyeongmo Gu", "Ihsan Ullah", "Hyunki Lee", "Jonáš Šerých", "Michal Neoral", "Jiří Matas", "Rulin Zhou", "Wenlong He", "An Wang", "Hongliang Ren", "Bruno Silva", "Sandro Queirós", "Estêvão Lima", "João L. Vilaça", "Shunsuke Kikuchi", "Atsushi Kouno", "Hiroki Matsuzaki", "Tongtong Li", "Yulu Chen", "Ling Li", "Xiang Ma", "Xiaojian Li", "Mona Sheikh Zeinoddin", "Xu Wang", "Zafer Tandogdu", "Greg Shaw", "Evangelos Mazomenos", "Danail Stoyanov", "Yuxin Chen", "Zijian Wu", "Alexander Ladikos", "Simon DiMaio", "Septimiu E. Salcudean", "Omid Mohareri"], "title": "Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge", "categories": ["cs.CV"], "comment": null, "summary": "Understanding tissue motion in surgery is crucial to enable applications in\ndownstream tasks such as segmentation, 3D reconstruction, virtual tissue\nlandmarking, autonomous probe-based scanning, and subtask autonomy. Labeled\ndata are essential to enabling algorithms in these downstream tasks since they\nallow us to quantify and train algorithms. This paper introduces a point\ntracking challenge to address this, wherein participants can submit their\nalgorithms for quantification. The submitted algorithms are evaluated using a\ndataset named surgical tattoos in infrared (STIR), with the challenge aptly\nnamed the STIR Challenge 2024. The STIR Challenge 2024 comprises two\nquantitative components: accuracy and efficiency. The accuracy component tests\nthe accuracy of algorithms on in vivo and ex vivo sequences. The efficiency\ncomponent tests the latency of algorithm inference. The challenge was conducted\nas a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with\n4 teams submitting before and 4 submitting after challenge day. This paper\ndetails the STIR Challenge 2024, which serves to move the field towards more\naccurate and efficient algorithms for spatial understanding in surgery. In this\npaper we summarize the design, submissions, and results from the challenge. The\nchallenge dataset is available here: https://zenodo.org/records/14803158 , and\nthe code for baseline models and metric calculation is available here:\nhttps://github.com/athaddius/STIRMetrics", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24326", "pdf": "https://arxiv.org/pdf/2503.24326", "abs": "https://arxiv.org/abs/2503.24326", "authors": ["Rupert Polley", "Sai Vignesh Abishek Deenadayalan", "J. Marius Zöllner"], "title": "Self-Supervised Pretraining for Aerial Road Extraction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep neural networks for aerial image segmentation require large amounts of\nlabeled data, but high-quality aerial datasets with precise annotations are\nscarce and costly to produce. To address this limitation, we propose a\nself-supervised pretraining method that improves segmentation performance while\nreducing reliance on labeled data. Our approach uses inpainting-based\npretraining, where the model learns to reconstruct missing regions in aerial\nimages, capturing their inherent structure before being fine-tuned for road\nextraction. This method improves generalization, enhances robustness to domain\nshifts, and is invariant to model architecture and dataset choice. Experiments\nshow that our pretraining significantly boosts segmentation accuracy,\nespecially in low-data regimes, making it a scalable solution for aerial image\nanalysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24357", "pdf": "https://arxiv.org/pdf/2503.24357", "abs": "https://arxiv.org/abs/2503.24357", "authors": ["Shuaizheng Liu", "Jianqi Ma", "Lingchen Sun", "Xiangtao Kong", "Lei Zhang"], "title": "InstructRestore: Region-Customized Image Restoration with Human Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Despite the significant progress in diffusion prior-based image restoration,\nmost existing methods apply uniform processing to the entire image, lacking the\ncapability to perform region-customized image restoration according to user\ninstructions. In this work, we propose a new framework, namely InstructRestore,\nto perform region-adjustable image restoration following human instructions. To\nachieve this, we first develop a data generation engine to produce training\ntriplets, each consisting of a high-quality image, the target region\ndescription, and the corresponding region mask. With this engine and careful\ndata screening, we construct a comprehensive dataset comprising 536,945\ntriplets to support the training and evaluation of this task. We then examine\nhow to integrate the low-quality image features under the ControlNet\narchitecture to adjust the degree of image details enhancement. Consequently,\nwe develop a ControlNet-like model to identify the target region and allocate\ndifferent integration scales to the target and surrounding regions, enabling\nregion-customized image restoration that aligns with user instructions.\nExperimental results demonstrate that our proposed InstructRestore approach\nenables effective human-instructed image restoration, such as images with bokeh\neffects and user-instructed local enhancement. Our work advances the\ninvestigation of interactive image restoration and enhancement techniques.\nData, code, and models will be found at\nhttps://github.com/shuaizhengliu/InstructRestore.git.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24374", "pdf": "https://arxiv.org/pdf/2503.24374", "abs": "https://arxiv.org/abs/2503.24374", "authors": ["Maxim V. Shugaev", "Vincent Chen", "Maxim Karrenbach", "Kyle Ashley", "Bridget Kennedy", "Naresh P. Cuntoor"], "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "This work addresses the problem of novel view synthesis in diverse scenes\nfrom small collections of RGB images. We propose ERUPT (Efficient Rendering\nwith Unposed Patch Transformer) a state-of-the-art scene reconstruction model\ncapable of efficient scene rendering using unposed imagery. We introduce\npatch-based querying, in contrast to existing pixel-based queries, to reduce\nthe compute required to render a target view. This makes our model highly\nefficient both during training and at inference, capable of rendering at 600\nfps on commercial hardware. Notably, our model is designed to use a learned\nlatent camera pose which allows for training using unposed targets in datasets\nwith sparse or inaccurate ground truth camera pose. We show that our approach\ncan generalize on large real-world data and introduce a new benchmark dataset\n(MSVS-1M) for latent view synthesis using street-view imagery collected from\nMapillary. In contrast to NeRF and Gaussian Splatting, which require dense\nimagery and precise metadata, ERUPT can render novel views of arbitrary scenes\nwith as few as five unposed input images. ERUPT achieves better rendered image\nquality than current state-of-the-art methods for unposed image synthesis\ntasks, reduces labeled data requirements by ~95\\% and decreases computational\nrequirements by an order of magnitude, providing efficient novel view synthesis\nfor diverse real-world scenes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24381", "pdf": "https://arxiv.org/pdf/2503.24381", "abs": "https://arxiv.org/abs/2503.24381", "authors": ["Yuping Wang", "Xiangyu Huang", "Xiaokang Sun", "Mingxuan Yan", "Shuo Xing", "Zhengzhong Tu", "Jiachen Li"], "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "comment": "14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;\n  Code: https://github.com/tasl-lab/UniOcc", "summary": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22713", "pdf": "https://arxiv.org/pdf/2503.22713", "abs": "https://arxiv.org/abs/2503.22713", "authors": ["Nooshin Bahador", "Milad Lankarany"], "title": "Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept Study", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "19 pages, 8 figures", "summary": "Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22715", "pdf": "https://arxiv.org/pdf/2503.22715", "abs": "https://arxiv.org/abs/2503.22715", "authors": ["Jiahao Qin", "Feng Liu", "Lu Zong"], "title": "Hierarchical Adaptive Expert for Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.CV", "cs.MM"], "comment": "11 pages, 3 figures", "summary": "Multimodal sentiment analysis has emerged as a critical tool for\nunderstanding human emotions across diverse communication channels. While\nexisting methods have made significant strides, they often struggle to\neffectively differentiate and integrate modality-shared and modality-specific\ninformation, limiting the performance of multimodal learning. To address this\nchallenge, we propose the Hierarchical Adaptive Expert for Multimodal Sentiment\nAnalysis (HAEMSA), a novel framework that synergistically combines evolutionary\noptimization, cross-modal knowledge transfer, and multi-task learning. HAEMSA\nemploys a hierarchical structure of adaptive experts to capture both global and\nlocal modality representations, enabling more nuanced sentiment analysis. Our\napproach leverages evolutionary algorithms to dynamically optimize network\narchitectures and modality combinations, adapting to both partial and full\nmodality scenarios. Extensive experiments demonstrate HAEMSA's superior\nperformance across multiple benchmark datasets. On CMU-MOSEI, HAEMSA achieves a\n2.6% increase in 7-class accuracy and a 0.059 decrease in MAE compared to the\nprevious best method. For CMU-MOSI, we observe a 6.3% improvement in 7-class\naccuracy and a 0.058 reduction in MAE. On IEMOCAP, HAEMSA outperforms the\nstate-of-the-art by 2.84% in weighted-F1 score for emotion recognition. These\nresults underscore HAEMSA's effectiveness in capturing complex multimodal\ninteractions and generalizing across different emotional contexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23042", "pdf": "https://arxiv.org/pdf/2503.23042", "abs": "https://arxiv.org/abs/2503.23042", "authors": ["M Rita Verdelho", "Alexandre Bernardino", "Catarina Barata"], "title": "MIL vs. Aggregation: Evaluating Patient-Level Survival Prediction Strategies Using Graph-Based Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Oncologists often rely on a multitude of data, including whole-slide images\n(WSIs), to guide therapeutic decisions, aiming for the best patient outcome.\nHowever, predicting the prognosis of cancer patients can be a challenging task\ndue to tumor heterogeneity and intra-patient variability, and the complexity of\nanalyzing WSIs. These images are extremely large, containing billions of\npixels, making direct processing computationally expensive and requiring\nspecialized methods to extract relevant information. Additionally, multiple\nWSIs from the same patient may capture different tumor regions, some being more\ninformative than others. This raises a fundamental question: Should we use all\nWSIs to characterize the patient, or should we identify the most representative\nslide for prognosis? Our work seeks to answer this question by performing a\ncomparison of various strategies for predicting survival at the WSI and patient\nlevel. The former treats each WSI as an independent sample, mimicking the\nstrategy adopted in other works, while the latter comprises methods to either\naggregate the predictions of the several WSIs or automatically identify the\nmost relevant slide using multiple-instance learning (MIL). Additionally, we\nevaluate different Graph Neural Networks architectures under these strategies.\nWe conduct our experiments using the MMIST-ccRCC dataset, which comprises\npatients with clear cell renal cell carcinoma (ccRCC). Our results show that\nMIL-based selection improves accuracy, suggesting that choosing the most\nrepresentative slide benefits survival prediction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23284", "pdf": "https://arxiv.org/pdf/2503.23284", "abs": "https://arxiv.org/abs/2503.23284", "authors": ["Feng-Lin Liu", "Hongbo Fu", "Xintao Wang", "Weicai Ye", "Pengfei Wan", "Di Zhang", "Lin Gao"], "title": "SketchVideo: Sketch-based Video Generation and Editing", "categories": ["cs.GR", "cs.CV"], "comment": "CVPR 2025", "summary": "Video generation and editing conditioned on text prompts or images have\nundergone significant advancements. However, challenges remain in accurately\ncontrolling global layout and geometry details solely by texts, and supporting\nmotion control and local modification through images. In this paper, we aim to\nachieve sketch-based spatial and motion control for video generation and\nsupport fine-grained editing of real or synthetic videos. Based on the DiT\nvideo generation model, we propose a memory-efficient control structure with\nsketch control blocks that predict residual features of skipped DiT blocks.\nSketches are drawn on one or two keyframes (at arbitrary time points) for easy\ninteraction. To propagate such temporally sparse sketch conditions across all\nframes, we propose an inter-frame attention mechanism to analyze the\nrelationship between the keyframes and each video frame. For sketch-based video\nediting, we design an additional video insertion module that maintains\nconsistency between the newly edited content and the original video's spatial\nfeature and dynamic motion. During inference, we use latent fusion for the\naccurate preservation of unedited regions. Extensive experiments demonstrate\nthat our SketchVideo achieves superior performance in controllable video\ngeneration and editing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23598", "pdf": "https://arxiv.org/pdf/2503.23598", "abs": "https://arxiv.org/abs/2503.23598", "authors": ["Kalliopi Basioti", "Pritish Sahu", "Qingze Tony Liu", "Zihao Xu", "Hao Wang", "Vladimir Pavlovic"], "title": "GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs", "categories": ["cs.AI", "cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Raven's Progressive Matrices (RPMs) is an established benchmark to examine\nthe ability to perform high-level abstract visual reasoning (AVR). Despite the\ncurrent success of algorithms that solve this task, humans can generalize\nbeyond a given puzzle and create new puzzles given a set of rules, whereas\nmachines remain locked in solving a fixed puzzle from a curated choice list. We\npropose Generative Visual Puzzles (GenVP), a framework to model the entire RPM\ngeneration process, a substantially more challenging task. Our model's\ncapability spans from generating multiple solutions for one specific problem\nprompt to creating complete new puzzles out of the desired set of rules.\nExperiments on five different datasets indicate that GenVP achieves\nstate-of-the-art (SOTA) performance both in puzzle-solving accuracy and\nout-of-distribution (OOD) generalization in 22 OOD scenarios. Compared to SOTA\ngenerative approaches, which struggle to solve RPMs when the feasible solution\nspace increases, GenVP efficiently generalizes to these challenging setups.\nMoreover, our model demonstrates the ability to produce a wide range of\ncomplete RPMs given a set of abstract rules by effectively capturing the\nrelationships between abstract rules and visual object properties.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23898", "pdf": "https://arxiv.org/pdf/2503.23898", "abs": "https://arxiv.org/abs/2503.23898", "authors": ["Rihui Zhang", "Haiming Zhu", "Jingtong Zhao", "Lei Zhang", "Fang-Fang Yin", "Chunhao Wang", "Zhenyu Yang"], "title": "An Explainable Neural Radiomic Sequence Model with Spatiotemporal Continuity for Quantifying 4DCT-based Pulmonary Ventilation", "categories": ["physics.med-ph", "cs.CV"], "comment": "43 pages, 13 figures", "summary": "Accurate evaluation of regional lung ventilation is essential for the\nmanagement and treatment of lung cancer patients, supporting assessments of\npulmonary function, optimization of therapeutic strategies, and monitoring of\ntreatment response. Currently, ventilation scintigraphy using nuclear medicine\ntechniques is widely employed in clinical practice; however, it is often\ntime-consuming, costly, and entails additional radiation exposure. In this\nstudy, we propose an explainable neural radiomic sequence model to identify\nregions of compromised pulmonary ventilation based on four-dimensional computed\ntomography (4DCT). A cohort of 45 lung cancer patients from the VAMPIRE dataset\nwas analyzed. For each patient, lung volumes were segmented from 4DCT, and\nvoxel-wise radiomic features (56-dimensional) were extracted across the\nrespiratory cycle to capture local intensity and texture dynamics, forming\ntemporal radiomic sequences. Ground truth ventilation defects were delineated\nvoxel-wise using Galligas-PET and DTPA-SPECT. To identify compromised regions,\nwe developed a temporal saliency-enhanced explainable long short-term memory\n(LSTM) network trained on the radiomic sequences. Temporal saliency maps were\ngenerated to highlight key features contributing to the model's predictions.\nThe proposed model demonstrated robust performance, achieving average (range)\nDice similarity coefficients of 0.78 (0.74-0.79) for 25 PET cases and 0.78\n(0.74-0.82) for 20 SPECT cases. The temporal saliency map explained three key\nradiomic sequences in ventilation quantification: during lung exhalation,\ncompromised pulmonary function region typically exhibits (1) an increasing\ntrend of intensity and (2) a decreasing trend of homogeneity, in contrast to\nhealthy lung tissue.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24138", "pdf": "https://arxiv.org/pdf/2503.24138", "abs": "https://arxiv.org/abs/2503.24138", "authors": ["Uxue Delaquintana-Aramendi", "Leire Benito-del-Valle", "Aitor Alvarez-Gila", "Javier Pascau", "Luisa F Sánchez-Peralta", "Artzai Picón", "J Blas Pagador", "Cristina L Saratxaga"], "title": "AI-Assisted Colonoscopy: Polyp Detection and Segmentation using Foundation Models", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "In colonoscopy, 80% of the missed polyps could be detected with the help of\nDeep Learning models. In the search for algorithms capable of addressing this\nchallenge, foundation models emerge as promising candidates. Their zero-shot or\nfew-shot learning capabilities, facilitate generalization to new data or tasks\nwithout extensive fine-tuning. A concept that is particularly advantageous in\nthe medical imaging domain, where large annotated datasets for traditional\ntraining are scarce. In this context, a comprehensive evaluation of foundation\nmodels for polyp segmentation was conducted, assessing both detection and\ndelimitation. For the study, three different colonoscopy datasets have been\nemployed to compare the performance of five different foundation models,\nDINOv2, YOLO-World, GroundingDINO, SAM and MedSAM, against two benchmark\nnetworks, YOLOv8 and Mask R-CNN. Results show that the success of foundation\nmodels in polyp characterization is highly dependent on domain specialization.\nFor optimal performance in medical applications, domain-specific models are\nessential, and generic models require fine-tuning to achieve effective results.\nThrough this specialization, foundation models demonstrated superior\nperformance compared to state-of-the-art detection and segmentation models,\nwith some models even excelling in zero-shot evaluation; outperforming\nfine-tuned models on unseen data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22727", "pdf": "https://arxiv.org/pdf/2503.22727", "abs": "https://arxiv.org/abs/2503.22727", "authors": ["Alejandro Lozano", "Min Woo Sun", "James Burgess", "Jeffrey J. Nirschl", "Christopher Polzak", "Yuhui Zhang", "Liangyu Chen", "Jeffrey Gu", "Ivan Lopez", "Josiah Aklilu", "Anita Rau", "Austin Wolfgang Katzer", "Collin Chiu", "Orr Zohar", "Xiaohan Wang", "Alfred Seunghoon Song", "Chiang Chia-Chun", "Robert Tibshirani", "Serena Yeung-Levy"], "title": "A Large-Scale Vision-Language Dataset Derived from Open Scientific Literature to Advance Biomedical Generalist AI", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the excitement behind biomedical artificial intelligence (AI), access\nto high-quality, diverse, and large-scale data - the foundation for modern AI\nsystems - is still a bottleneck to unlocking its full potential. To address\nthis gap, we introduce Biomedica, an open-source dataset derived from the\nPubMed Central Open Access subset, containing over 6 million scientific\narticles and 24 million image-text pairs, along with 27 metadata fields\n(including expert human annotations). To overcome the challenges of accessing\nour large-scale dataset, we provide scalable streaming and search APIs through\na web server, facilitating seamless integration with AI systems. We demonstrate\nthe utility of the Biomedica dataset by building embedding models, chat-style\nmodels, and retrieval-augmented chat agents. Notably, all our AI models surpass\nprevious open systems in their respective categories, underscoring the critical\nrole of diverse, high-quality, and large-scale biomedical data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22828", "pdf": "https://arxiv.org/pdf/2503.22828", "abs": "https://arxiv.org/abs/2503.22828", "authors": ["Alexander Gurung", "Mirella Lapata"], "title": "Learning to Reason for Long-Form Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22880", "pdf": "https://arxiv.org/pdf/2503.22880", "abs": "https://arxiv.org/abs/2503.22880", "authors": ["Matias Valdenegro-Toro", "Deepan Chakravarthi Padmanabhan", "Deepak Singh", "Bilal Wehbe", "Yvan Petillot"], "title": "The Marine Debris Forward-Looking Sonar Datasets", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 12 figures, Oceans Brest 2025 camera readyu", "summary": "Sonar sensing is fundamental for underwater robotics, but limited by\ncapabilities of AI systems, which need large training datasets. Public data in\nsonar modalities is lacking. This paper presents the Marine Debris\nForward-Looking Sonar datasets, with three different settings (watertank,\nturntable, flooded quarry) increasing dataset diversity and multiple computer\nvision tasks: object classification, object detection, semantic segmentation,\npatch matching, and unsupervised learning. We provide full dataset description,\nbasic analysis and initial results for some tasks. We expect the research\ncommunity will benefit from this dataset, which is publicly available at\nhttps://doi.org/10.5281/zenodo.15101686", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22948", "pdf": "https://arxiv.org/pdf/2503.22948", "abs": "https://arxiv.org/abs/2503.22948", "authors": ["Tianyang Xu", "Xiaoze Liu", "Feijie Wu", "Xiaoqian Wang", "Jing Gao"], "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing by\nlearning from massive datasets, yet this rapid progress has also drawn legal\nscrutiny, as the ability to unintentionally generate copyrighted content has\nalready prompted several prominent lawsuits. In this work, we introduce SUV\n(Selective Unlearning for Verbatim data), a selective unlearning framework\ndesigned to prevent LLM from memorizing copyrighted content while preserving\nits overall utility. In detail, the proposed method constructs a dataset that\ncaptures instances of copyrighted infringement cases by the targeted LLM. With\nthe dataset, we unlearn the content from the LLM by means of Direct Preference\nOptimization (DPO), which replaces the verbatim copyrighted content with\nplausible and coherent alternatives. Since DPO may hinder the LLM's performance\nin other unrelated tasks, we integrate gradient projection and Fisher\ninformation regularization to mitigate the degradation. We validate our\napproach using a large-scale dataset of 500 famous books (predominantly\ncopyrighted works) and demonstrate that SUV significantly reduces verbatim\nmemorization with negligible impact on the performance on unrelated tasks.\nExtensive experiments on both our dataset and public benchmarks confirm the\nscalability and efficacy of our approach, offering a promising solution for\nmitigating copyright risks in real-world LLM applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22996", "pdf": "https://arxiv.org/pdf/2503.22996", "abs": "https://arxiv.org/abs/2503.22996", "authors": ["Giang Do", "Hung Le", "Truyen Tran"], "title": "Sparse Mixture of Experts as Unified Competitive Learning", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Sparse Mixture of Experts (SMoE) improves the efficiency of large language\nmodel training by directing input tokens to a subset of experts. Despite its\nsuccess in generation tasks, its generalization ability remains an open\nquestion. In this paper, we demonstrate that current SMoEs, which fall into two\ncategories: (1) Token Choice ;and (2) Expert Choice, struggle with tasks such\nas the Massive Text Embedding Benchmark (MTEB). By analyzing their mechanism\nthrough the lens of competitive learning, our study finds that the Token Choice\napproach may overly focus on irrelevant experts, while the Expert Choice\napproach risks discarding important tokens, potentially affecting performance.\nMotivated by this analysis, we propose Unified Competitive Learning SMoE\n(USMoE), a novel and efficient framework designed to improve the performance of\nexisting SMoEs in both scenarios: with and without training. Extensive\nexperiments across various tasks show that USMoE achieves up to a 10%\nimprovement over traditional approaches or reduces computational inference\ncosts by 14% while maintaining strong performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22909", "pdf": "https://arxiv.org/pdf/2503.22909", "abs": "https://arxiv.org/abs/2503.22909", "authors": ["Anas Berka", "Mohamed El Hajji", "Raphael Canals", "Youssef Es-saady", "Adel Hafiane"], "title": "Enhancing DeepLabV3+ to Fuse Aerial and Satellite Images for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial and satellite imagery are inherently complementary remote sensing\nsources, offering high-resolution detail alongside expansive spatial coverage.\nHowever, the use of these sources for land cover segmentation introduces\nseveral challenges, prompting the development of a variety of segmentation\nmethods. Among these approaches, the DeepLabV3+ architecture is considered as a\npromising approach in the field of single-source image segmentation. However,\ndespite its reliable results for segmentation, there is still a need to\nincrease its robustness and improve its performance. This is particularly\ncrucial for multimodal image segmentation, where the fusion of diverse types of\ninformation is essential.\n  An interesting approach involves enhancing this architectural framework\nthrough the integration of novel components and the modification of certain\ninternal processes.\n  In this paper, we enhance the DeepLabV3+ architecture by introducing a new\ntransposed conventional layers block for upsampling a second entry to fuse it\nwith high level features. This block is designed to amplify and integrate\ninformation from satellite images, thereby enriching the segmentation process\nthrough fusion with aerial images.\n  For experiments, we used the LandCover.ai (Land Cover from Aerial Imagery)\ndataset for aerial images, alongside the corresponding dataset sourced from\nSentinel 2 data.\n  Through the fusion of both sources, the mean Intersection over Union (mIoU)\nachieved a total mIoU of 84.91% without data augmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23007", "pdf": "https://arxiv.org/pdf/2503.23007", "abs": "https://arxiv.org/abs/2503.23007", "authors": ["Giang Do", "Hung Le", "Truyen Tran"], "title": "S2MoE: Robust Sparse Mixture of Experts via Stochastic Learning", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Sparse Mixture of Experts (SMoE) enables efficient training of large language\nmodels by routing input tokens to a select number of experts. However, training\nSMoE remains challenging due to the issue of representation collapse. Recent\nstudies have focused on improving the router to mitigate this problem, but\nexisting approaches face two key limitations: (1) expert embeddings are\nsignificantly smaller than the model's dimension, contributing to\nrepresentation collapse, and (2) routing each input to the Top-K experts can\ncause them to learn overly similar features. In this work, we propose a novel\napproach called Robust Sparse Mixture of Experts via Stochastic Learning\n(S2MoE), which is a mixture of experts designed to learn from both\ndeterministic and non-deterministic inputs via Learning under Uncertainty.\nExtensive experiments across various tasks demonstrate that S2MoE achieves\nperformance comparable to other routing methods while reducing computational\ninference costs by 28%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23053", "pdf": "https://arxiv.org/pdf/2503.23053", "abs": "https://arxiv.org/abs/2503.23053", "authors": ["Hongjia Liu", "Jinlong Li"], "title": "A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities in solving\ncomplex tasks. Recent work has explored decomposing such tasks into subtasks\nwith independent contexts. However, some contextually related subtasks may\nencounter information loss during execution, leading to redundant operations or\nexecution failures. To address this issue, we propose a training-free framework\nwith an interaction mechanism, which enables a subtask to query specific\ninformation or trigger certain actions in completed subtasks by sending\nrequests. To implement interaction, we introduce a subtask trajectory memory to\nenable resumption of completed subtasks upon receiving interaction requests.\nAdditionally, we propose a new action during execution, which generates a\nconcise and precise description of execution process and outcomes of a subtask,\nto assist subsequent subtasks in determining interaction targets and requests.\nWe evaluate our framework on interactive decision-making task WebShop and\nmulti-hop question answering HotpotQA, with GPT-3.5 and GPT-4, and comparison\nresults show that our framework outperforms the state-of-the-art training-free\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23077", "pdf": "https://arxiv.org/pdf/2503.23077", "abs": "https://arxiv.org/abs/2503.23077", "authors": ["Yue Liu", "Jiaying Wu", "Yufei He", "Hongcheng Gao", "Hongyu Chen", "Baolong Bi", "Jiaheng Zhang", "Zhiqi Huang", "Bryan Hooi"], "title": "Efficient Inference for Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) significantly improve the reasoning ability of\nLarge Language Models (LLMs) by learning to reason, exhibiting promising\nperformance in complex task-solving. However, their deliberative reasoning\nprocess leads to inefficiencies in token usage, memory consumption, and\ninference time. Thus, this survey provides a review of efficient inference\nmethods designed specifically for LRMs, focusing on mitigating token\ninefficiency while preserving the reasoning quality. First, we introduce a\ntaxonomy to group the recent methods into two main categories: (a) explicit\ncompact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit\nreasoning structure, and (b) implicit latent CoT, which encodes reasoning steps\nwithin hidden representations instead of explicit tokens. Meanwhile, we discuss\ntheir strengths and weaknesses. Then, we conduct empirical analyses on existing\nmethods from performance and efficiency aspects. Besides, we present open\nchallenges in this field, including human-centric controllable reasoning,\ntrade-off between interpretability and efficiency of reasoning, ensuring safety\nof efficient reasoning, and broader applications of efficient reasoning. In\naddition, we highlight key insights for enhancing LRMs' inference efficiency\nvia techniques such as model merging, new architectures, and agent routers. We\nhope this work serves as a valuable guide, helping researchers overcome\nchallenges in this vibrant\nfield\\footnote{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22936", "pdf": "https://arxiv.org/pdf/2503.22936", "abs": "https://arxiv.org/abs/2503.22936", "authors": ["Pei-Kai Huanga", "Jun-Xiong Chong", "Ming-Tsung Hsu", "Fang-Yu Hsu", "Chiou-Ting Hsu"], "title": "Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing (FAS) heavily relies on identifying live/spoof\ndiscriminative features to counter face presentation attacks. Recently, we\nproposed LDCformer to successfully incorporate the Learnable Descriptive\nConvolution (LDC) into ViT, to model long-range dependency of locally\ndescriptive features for FAS. In this paper, we propose three novel training\nstrategies to effectively enhance the training of LDCformer to largely boost\nits feature characterization capability. The first strategy, dual-attention\nsupervision, is developed to learn fine-grained liveness features guided by\nregional live/spoof attentions. The second strategy, self-challenging\nsupervision, is designed to enhance the discriminability of the features by\ngenerating challenging training data. In addition, we propose a third training\nstrategy, transitional triplet mining strategy, through narrowing the\ncross-domain gap while maintaining the transitional relationship between live\nand spoof features, to enlarge the domain-generalization capability of\nLDCformer. Extensive experiments show that LDCformer under joint supervision of\nthe three novel training strategies outperforms previous methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22952", "pdf": "https://arxiv.org/pdf/2503.22952", "abs": "https://arxiv.org/abs/2503.22952", "authors": ["Yuxuan Wang", "Yueqian Wang", "Bo Chen", "Tong Wu", "Dongyan Zhao", "Zilong Zheng"], "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts", "categories": ["cs.CV"], "comment": "To appear at CVPR 2025", "summary": "The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has\npropelled the development of Omni language models, designed to process and\nproactively respond to continuous streams of multi-modal data. Despite their\npotential, evaluating their real-world interactive capabilities in streaming\nvideo contexts remains a formidable challenge. In this work, we introduce\nOmniMMI, a comprehensive multi-modal interaction benchmark tailored for\nOmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and\n2,290 questions, addressing two critical yet underexplored challenges in\nexisting video benchmarks: streaming video understanding and proactive\nreasoning, across six distinct subtasks. Moreover, we propose a novel\nframework, Multi-modal Multiplexing Modeling (M4), designed to enable an\ninference-efficient streaming model that can see, listen while generating.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22963", "pdf": "https://arxiv.org/pdf/2503.22963", "abs": "https://arxiv.org/abs/2503.22963", "authors": ["Peiyu Chen", "Fuling Lin", "Weipeng Guan", "Peng Lu"], "title": "SuperEIO: Self-Supervised Event Feature Learning for Event Inertial Odometry", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras asynchronously output low-latency event streams, promising for\nstate estimation in high-speed motion and challenging lighting conditions. As\nopposed to frame-based cameras, the motion-dependent nature of event cameras\npresents persistent challenges in achieving robust event feature detection and\nmatching. In recent years, learning-based approaches have demonstrated superior\nrobustness over traditional handcrafted methods in feature detection and\nmatching, particularly under aggressive motion and HDR scenarios. In this\npaper, we propose SuperEIO, a novel framework that leverages the learning-based\nevent-only detection and IMU measurements to achieve event-inertial odometry.\nOur event-only feature detection employs a convolutional neural network under\ncontinuous event streams. Moreover, our system adopts the graph neural network\nto achieve event descriptor matching for loop closure. The proposed system\nutilizes TensorRT to accelerate the inference speed of deep networks, which\nensures low-latency processing and robust real-time operation on\nresource-limited platforms. Besides, we evaluate our method extensively on\nmultiple public datasets, demonstrating its superior accuracy and robustness\ncompared to other state-of-the-art event-based methods. We have also\nopen-sourced our pipeline to facilitate research in the field:\nhttps://github.com/arclab-hku/SuperEIO.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23088", "pdf": "https://arxiv.org/pdf/2503.23088", "abs": "https://arxiv.org/abs/2503.23088", "authors": ["Himanshu Beniwal", "Reddybathuni Venkat", "Rohit Kumar", "Birudugadda Srivibhav", "Daksh Jain", "Pavan Doddi", "Eshwar Dhande", "Adithya Ananth", "Kuldeep", "Heer Kubadia", "Pratham Sharda", "Mayank Singh"], "title": "UNITYAI-GUARD: Pioneering Toxicity Detection Across Low-Resource Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This work introduces UnityAI-Guard, a framework for binary toxicity\nclassification targeting low-resource Indian languages. While existing systems\npredominantly cater to high-resource languages, UnityAI-Guard addresses this\ncritical gap by developing state-of-the-art models for identifying toxic\ncontent across diverse Brahmic/Indic scripts. Our approach achieves an\nimpressive average F1-score of 84.23% across seven languages, leveraging a\ndataset of 888k training instances and 35k manually verified test instances. By\nadvancing multilingual content moderation for linguistically diverse regions,\nUnityAI-Guard also provides public API access to foster broader adoption and\napplication.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23095", "pdf": "https://arxiv.org/pdf/2503.23095", "abs": "https://arxiv.org/abs/2503.23095", "authors": ["Yuelyu Ji", "Rui Meng", "Zhuochun Li", "Daqing He"], "title": "Memory-Aware and Uncertainty-Guided Retrieval for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Multi-hop question answering (QA) requires models to retrieve and reason over\nmultiple pieces of evidence. While Retrieval-Augmented Generation (RAG) has\nmade progress in this area, existing methods often suffer from two key\nlimitations: (1) fixed or overly frequent retrieval steps, and (2) ineffective\nuse of previously retrieved knowledge.\n  We propose MIND (Memory-Informed and INteractive Dynamic RAG), a framework\nthat addresses these challenges through: (i) prompt-based entity extraction to\nidentify reasoning-relevant elements, (ii) dynamic retrieval triggering based\non token-level entropy and attention signals, and (iii) memory-aware filtering,\nwhich stores high-confidence facts across reasoning steps to enable consistent\nmulti-hop generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23163", "pdf": "https://arxiv.org/pdf/2503.23163", "abs": "https://arxiv.org/abs/2503.23163", "authors": ["Yuxin Lu", "Yu-Ying Chuang", "R. Harald Baayen"], "title": "The realization of tones in spontaneous spoken Taiwan Mandarin: a corpus-based survey and theory-driven computational modeling", "categories": ["cs.CL"], "comment": null, "summary": "A growing body of literature has demonstrated that semantics can co-determine\nfine phonetic detail. However, the complex interplay between phonetic\nrealization and semantics remains understudied, particularly in pitch\nrealization. The current study investigates the tonal realization of Mandarin\ndisyllabic words with all 20 possible combinations of two tones, as found in a\ncorpus of Taiwan Mandarin spontaneous speech. We made use of Generalized\nAdditive Mixed Models (GAMs) to model f0 contours as a function of a series of\npredictors, including gender, tonal context, tone pattern, speech rate, word\nposition, bigram probability, speaker and word. In the GAM analysis, word and\nsense emerged as crucial predictors of f0 contours, with effect sizes that\nexceed those of tone pattern. For each word token in our dataset, we then\nobtained a contextualized embedding by applying the GPT-2 large language model\nto the context of that token in the corpus. We show that the pitch contours of\nword tokens can be predicted to a considerable extent from these contextualized\nembeddings, which approximate token-specific meanings in contexts of use. The\nresults of our corpus study show that meaning in context and phonetic\nrealization are far more entangled than standard linguistic theory predicts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23204", "pdf": "https://arxiv.org/pdf/2503.23204", "abs": "https://arxiv.org/abs/2503.23204", "authors": ["Aden Haussmann"], "title": "The Challenge of Achieving Attributability in Multilingual Table-to-Text Generation with Question-Answer Blueprints", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multilingual Natural Language Generation (NLG) is challenging due to the lack\nof training data for low-resource languages. However, some low-resource\nlanguages have up to tens of millions of speakers globally, making it important\nto improve NLG tools for them. Table-to-Text NLG is an excellent measure of\nmodels' reasoning abilities but is very challenging in the multilingual\nsetting. System outputs are often not attributable, or faithful, to the data in\nthe source table. Intermediate planning techniques like Question-Answer (QA)\nblueprints have been shown to improve attributability on summarisation tasks.\nThis work explores whether QA blueprints make multilingual Table-to-Text\noutputs more attributable to the input tables. This paper extends the\nchallenging multilingual Table-to-Text dataset, TaTA, which includes African\nlanguages, with QA blueprints. Sequence-to-sequence language models are then\nfinetuned on this dataset, with and without blueprints. Results show that QA\nblueprints improve performance for models finetuned and evaluated only on\nEnglish examples, but do not demonstrate gains in the multilingual setting.\nThis is due to inaccuracies in machine translating the blueprints from English\ninto target languages when generating the training data, and models failing to\nrely closely on the blueprints they generate. An in-depth analysis is conducted\non why this is challenging.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22986", "pdf": "https://arxiv.org/pdf/2503.22986", "abs": "https://arxiv.org/abs/2503.22986", "authors": ["Yunsong Wang", "Tianxin Huang", "Hanlin Chen", "Gim Hee Lee"], "title": "FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recently, the integration of the efficient feed-forward scheme into 3D\nGaussian Splatting (3DGS) has been actively explored. However, most existing\nmethods focus on sparse view reconstruction of small regions and cannot produce\neligible whole-scene reconstruction results in terms of either quality or\nefficiency. In this paper, we propose FreeSplat++, which focuses on extending\nthe generalizable 3DGS to become an alternative approach to large-scale indoor\nwhole-scene reconstruction, which has the potential of significantly\naccelerating the reconstruction speed and improving the geometric accuracy. To\nfacilitate whole-scene reconstruction, we initially propose the Low-cost\nCross-View Aggregation framework to efficiently process extremely long input\nsequences. Subsequently, we introduce a carefully designed pixel-wise triplet\nfusion method to incrementally aggregate the overlapping 3D Gaussian primitives\nfrom multiple views, adaptively reducing their redundancy. Furthermore, we\npropose a weighted floater removal strategy that can effectively reduce\nfloaters, which serves as an explicit depth fusion approach that is crucial in\nwhole-scene reconstruction. After the feed-forward reconstruction of 3DGS\nprimitives, we investigate a depth-regularized per-scene fine-tuning process.\nLeveraging the dense, multi-view consistent depth maps obtained during the\nfeed-forward prediction phase for an extra constraint, we refine the entire\nscene's 3DGS primitive to enhance rendering quality while preserving geometric\naccuracy. Extensive experiments confirm that our FreeSplat++ significantly\noutperforms existing generalizable 3DGS methods, especially in whole-scene\nreconstructions. Compared to conventional per-scene optimized 3DGS approaches,\nour method with depth-regularized per-scene fine-tuning demonstrates\nsubstantial improvements in reconstruction accuracy and a notable reduction in\ntraining time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23012", "pdf": "https://arxiv.org/pdf/2503.23012", "abs": "https://arxiv.org/abs/2503.23012", "authors": ["Xinlei Shao", "Hongruixuan Chen", "Fan Zhao", "Kirsty Magson", "Jundong Chen", "Peiran Li", "Jiaqi Wang", "Jun Sasaki"], "title": "Multi-label classification for multi-temporal, multi-spatial coral reef condition monitoring using vision foundation model with adapter learning", "categories": ["cs.CV"], "comment": null, "summary": "Coral reef ecosystems provide essential ecosystem services, but face\nsignificant threats from climate change and human activities. Although advances\nin deep learning have enabled automatic classification of coral reef\nconditions, conventional deep models struggle to achieve high performance when\nprocessing complex underwater ecological images. Vision foundation models,\nknown for their high accuracy and cross-domain generalizability, offer\npromising solutions. However, fine-tuning these models requires substantial\ncomputational resources and results in high carbon emissions. To address these\nchallenges, adapter learning methods such as Low-Rank Adaptation (LoRA) have\nemerged as a solution. This study introduces an approach integrating the DINOv2\nvision foundation model with the LoRA fine-tuning method. The approach\nleverages multi-temporal field images collected through underwater surveys at\n15 dive sites at Koh Tao, Thailand, with all images labeled according to\nuniversal standards used in citizen science-based conservation programs. The\nexperimental results demonstrate that the DINOv2-LoRA model achieved superior\naccuracy, with a match ratio of 64.77%, compared to 60.34% achieved by the best\nconventional model. Furthermore, incorporating LoRA reduced the trainable\nparameters from 1,100M to 5.91M. Transfer learning experiments conducted under\ndifferent temporal and spatial settings highlight the exceptional\ngeneralizability of DINOv2-LoRA across different seasons and sites. This study\nis the first to explore the efficient adaptation of foundation models for\nmulti-label classification of coral reef conditions under multi-temporal and\nmulti-spatial settings. The proposed method advances the classification of\ncoral reef conditions and provides a tool for monitoring, conserving, and\nmanaging coral reef ecosystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23021", "pdf": "https://arxiv.org/pdf/2503.23021", "abs": "https://arxiv.org/abs/2503.23021", "authors": ["Sol Erika Boman", "Nita Mulliqi", "Anders Blilie", "Xiaoyi Ji", "Kelvin Szolnoky", "Einar Gudlaugsson", "Emiel A. M. Janssen", "Svein R. Kjosavik", "José Asenjo", "Marcello Gambacorta", "Paolo Libretti", "Marcin Braun", "Radzislaw Kordek", "Roman Łowicki", "Kristina Hotakainen", "Päivi Väre", "Bodil Ginnerup Pedersen", "Karina Dalsgaard Sørensen", "Benedicte Parm Ulhøi", "Lars Egevad", "Kimmo Kartasalo"], "title": "The impact of tissue detection on diagnostic artificial intelligence algorithms in digital pathology", "categories": ["cs.CV"], "comment": "25 pages, 2 tables, 3 figures, 1 supplementary figure", "summary": "Tissue detection is a crucial first step in most digital pathology\napplications. Details of the segmentation algorithm are rarely reported, and\nthere is a lack of studies investigating the downstream effects of a poor\nsegmentation algorithm. Disregarding tissue detection quality could create a\nbottleneck for downstream performance and jeopardize patient safety if\ndiagnostically relevant parts of the specimen are excluded from analysis in\nclinical applications.\n  This study aims to determine whether performance of downstream tasks is\nsensitive to the tissue detection method, and to compare performance of\nclassical and AI-based tissue detection. To this end, we trained an AI model\nfor Gleason grading of prostate cancer in whole slide images (WSIs) using two\ndifferent tissue detection algorithms: thresholding (classical) and UNet++\n(AI). A total of 33,823 WSIs scanned on five digital pathology scanners were\nused to train the tissue detection AI model. The downstream Gleason grading\nalgorithm was trained and tested using 70,524 WSIs from 13 clinical sites\nscanned on 13 different scanners.\n  There was a decrease from 116 (0.43%) to 22 (0.08%) fully undetected tissue\nsamples when switching from thresholding-based tissue detection to AI-based,\nsuggesting an AI model may be more reliable than a classical model for avoiding\ntotal failures on slides with unusual appearance. On the slides where tissue\ncould be detected by both algorithms, no significant difference in overall\nGleason grading performance was observed. However, tissue detection dependent\nclinically significant variations in AI grading were observed in 3.5% of\nmalignant slides, highlighting the importance of robust tissue detection for\noptimal clinical performance of diagnostic AI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23022", "pdf": "https://arxiv.org/pdf/2503.23022", "abs": "https://arxiv.org/abs/2503.23022", "authors": ["Xianglong He", "Junyi Chen", "Di Huang", "Zexiang Liu", "Xiaoshui Huang", "Wanli Ouyang", "Chun Yuan", "Yangguang Li"], "title": "MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs", "categories": ["cs.CV"], "comment": null, "summary": "In the domain of 3D content creation, achieving optimal mesh topology through\nAI models has long been a pursuit for 3D artists. Previous methods, such as\nMeshGPT, have explored the generation of ready-to-use 3D objects via mesh\nauto-regressive techniques. While these methods produce visually impressive\nresults, their reliance on token-by-token predictions in the auto-regressive\nprocess leads to several significant limitations. These include extremely slow\ngeneration speeds and an uncontrollable number of mesh faces. In this paper, we\nintroduce MeshCraft, a novel framework for efficient and controllable mesh\ngeneration, which leverages continuous spatial diffusion to generate discrete\ntriangle faces. Specifically, MeshCraft consists of two core components: 1) a\ntransformer-based VAE that encodes raw meshes into continuous face-level tokens\nand decodes them back to the original meshes, and 2) a flow-based diffusion\ntransformer conditioned on the number of faces, enabling the generation of\nhigh-quality 3D meshes with a predefined number of faces. By utilizing the\ndiffusion model for the simultaneous generation of the entire mesh topology,\nMeshCraft achieves high-fidelity mesh generation at significantly faster speeds\ncompared to auto-regressive methods. Specifically, MeshCraft can generate an\n800-face mesh in just 3.2 seconds (35$\\times$ faster than existing baselines).\nExtensive experiments demonstrate that MeshCraft outperforms state-of-the-art\ntechniques in both qualitative and quantitative evaluations on ShapeNet dataset\nand demonstrates superior performance on Objaverse dataset. Moreover, it\nintegrates seamlessly with existing conditional guidance strategies, showcasing\nits potential to relieve artists from the time-consuming manual work involved\nin mesh creation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23024", "pdf": "https://arxiv.org/pdf/2503.23024", "abs": "https://arxiv.org/abs/2503.23024", "authors": ["Zhihao Yuan", "Yibo Peng", "Jinke Ren", "Yinghong Liao", "Yatong Han", "Chun-Mei Feng", "Hengshuang Zhao", "Guanbin Li", "Shuguang Cui", "Zhen Li"], "title": "Empowering Large Language Models with 3D Situation Awareness", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Driven by the great success of Large Language Models (LLMs) in the 2D image\ndomain, their applications in 3D scene understanding has emerged as a new\ntrend. A key difference between 3D and 2D is that the situation of an\negocentric observer in 3D scenes can change, resulting in different\ndescriptions (e.g., ''left\" or ''right\"). However, current LLM-based methods\noverlook the egocentric perspective and simply use datasets from a global\nviewpoint. To address this issue, we propose a novel approach to automatically\ngenerate a situation-aware dataset by leveraging the scanning trajectory during\ndata collection and utilizing Vision-Language Models (VLMs) to produce\nhigh-quality captions and question-answer pairs. Furthermore, we introduce a\nsituation grounding module to explicitly predict the position and orientation\nof observer's viewpoint, thereby enabling LLMs to ground situation description\nin 3D scenes. We evaluate our approach on several benchmarks, demonstrating\nthat our method effectively enhances the 3D situational awareness of LLMs while\nsignificantly expanding existing datasets and reducing manual effort.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23281", "pdf": "https://arxiv.org/pdf/2503.23281", "abs": "https://arxiv.org/abs/2503.23281", "authors": ["Hieu Nghiem", "Tuan-Dung Le", "Suhao Chen", "Thanh Thieu", "Andrew Gin", "Ellie Phuong Nguyen", "Dursun Delen", "Johnson Thomas", "Jivan Lamichhane", "Zhuqi Miao"], "title": "Extracting Patient History from Clinical Text: A Comparative Study of Clinical Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Extracting medical history entities (MHEs) related to a patient's chief\ncomplaint (CC), history of present illness (HPI), and past, family, and social\nhistory (PFSH) helps structure free-text clinical notes into standardized EHRs,\nstreamlining downstream tasks like continuity of care, medical coding, and\nquality metrics. Fine-tuned clinical large language models (cLLMs) can assist\nin this process while ensuring the protection of sensitive data via on-premises\ndeployment. This study evaluates the performance of cLLMs in recognizing\nCC/HPI/PFSH-related MHEs and examines how note characteristics impact model\naccuracy. We annotated 1,449 MHEs across 61 outpatient-related clinical notes\nfrom the MTSamples repository. To recognize these entities, we fine-tuned seven\nstate-of-the-art cLLMs. Additionally, we assessed the models' performance when\nenhanced by integrating, problems, tests, treatments, and other basic medical\nentities (BMEs). We compared the performance of these models against GPT-4o in\na zero-shot setting. To further understand the textual characteristics\naffecting model accuracy, we conducted an error analysis focused on note\nlength, entity length, and segmentation. The cLLMs showed potential in reducing\nthe time required for extracting MHEs by over 20%. However, detecting many\ntypes of MHEs remained challenging due to their polysemous nature and the\nfrequent involvement of non-medical vocabulary. Fine-tuned GatorTron and\nGatorTronS, two of the most extensively trained cLLMs, demonstrated the highest\nperformance. Integrating pre-identified BME information improved model\nperformance for certain entities. Regarding the impact of textual\ncharacteristics on model performance, we found that longer entities were harder\nto identify, note length did not correlate with a higher error rate, and\nwell-organized segments with headings are beneficial for the extraction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23294", "pdf": "https://arxiv.org/pdf/2503.23294", "abs": "https://arxiv.org/abs/2503.23294", "authors": ["Wei Tao", "Bin Zhang", "Xiaoyang Qu", "Jiguang Wan", "Jianzong Wang"], "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference", "categories": ["cs.CL"], "comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)", "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23044", "pdf": "https://arxiv.org/pdf/2503.23044", "abs": "https://arxiv.org/abs/2503.23044", "authors": ["Yuanyuan Gao", "Hao Li", "Jiaqi Chen", "Zhengyu Zou", "Zhihang Zhong", "Dingwen Zhang", "Xiao Sun", "Junwei Han"], "title": "CityGS-X: A Scalable Architecture for Efficient and Geometrically Accurate Large-Scale Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://lifuguan.github.io/CityGS-X/", "summary": "Despite its significant achievements in large-scale scene reconstruction, 3D\nGaussian Splatting still faces substantial challenges, including slow\nprocessing, high computational costs, and limited geometric accuracy. These\ncore issues arise from its inherently unstructured design and the absence of\nefficient parallelization. To overcome these challenges simultaneously, we\nintroduce CityGS-X, a scalable architecture built on a novel parallelized\nhybrid hierarchical 3D representation (PH^2-3D). As an early attempt, CityGS-X\nabandons the cumbersome merge-and-partition process and instead adopts a\nnewly-designed batch-level multi-task rendering process. This architecture\nenables efficient multi-GPU rendering through dynamic Level-of-Detail voxel\nallocations, significantly improving scalability and performance. Through\nextensive experiments, CityGS-X consistently outperforms existing methods in\nterms of faster training times, larger rendering capacities, and more accurate\ngeometric details in large-scale scenes. Notably, CityGS-X can train and render\na scene with 5,000+ images in just 5 hours using only 4 * 4090 GPUs, a task\nthat would make other alternative methods encounter Out-Of-Memory (OOM) issues\nand fail completely. This implies that CityGS-X is far beyond the capacity of\nother existing methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23361", "pdf": "https://arxiv.org/pdf/2503.23361", "abs": "https://arxiv.org/abs/2503.23361", "authors": ["Linxin Song", "Xuwei Ding", "Jieyu Zhang", "Taiwei Shi", "Ryotaro Shimizu", "Rahul Gupta", "Yang Liu", "Jian Kang", "Jieyu Zhao"], "title": "Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) possess impressive linguistic capabilities but\noften fail to faithfully retain factual knowledge, leading to hallucinations\nand unreliable outputs. Understanding LLMs' knowledge deficiencies by\nexhaustively evaluating against full-scale knowledge bases is computationally\nprohibitive, especially for closed-weight models. We propose stochastic error\nascent (SEA), a scalable and efficient framework for discovering knowledge\ndeficiencies (errors) in closed-weight LLMs under a strict query budget. Rather\nthan naively probing all knowledge candidates, SEA formulates error discovery\nas a stochastic optimization process: it iteratively retrieves new high-error\ncandidates by leveraging the semantic similarity to previously observed\nfailures. To further enhance search efficiency and coverage, SEA employs\nhierarchical retrieval across document and paragraph levels, and constructs a\nrelation directed acyclic graph to model error propagation and identify\nsystematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors\nthan Automated Capability Discovery and 26.7% more than AutoBencher, while\nreducing the cost-per-error by 599x and 9x, respectively. Human evaluation\nconfirms the high quality of generated questions, while ablation and\nconvergence analyses validate the contribution of each component in SEA.\nFurther analysis on the discovered errors reveals correlated failure patterns\nacross LLM families and recurring deficits, highlighting the need for better\ndata coverage and targeted fine-tuning in future LLM development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23362", "pdf": "https://arxiv.org/pdf/2503.23362", "abs": "https://arxiv.org/abs/2503.23362", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong", "Xi-He Qiu", "Chun-Ming Xia", "Fei Dai"], "title": "Mixture of Routers", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages,4 figures", "summary": "Supervised fine-tuning (SFT) is a milestone in aligning large language models\nwith human instructions and adapting them to downstream tasks. In particular,\nLow-Rank Adaptation (LoRA) has gained widespread attention due to its parameter\nefficiency. However, its impact on improving the performance of large models\nremains limited. Recent studies suggest that combining LoRA with\nMixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE\nadapts to the diversity and complexity of datasets by dynamically selecting the\nmost suitable experts, thereby improving task accuracy and efficiency. Despite\nimpressive results, recent studies reveal issues in the MoE routing mechanism,\nsuch as incorrect assignments and imbalanced expert allocation. Inspired by the\nprinciples of Redundancy and Fault Tolerance Theory. We innovatively integrate\nthe concept of Mixture of Experts into the routing mechanism and propose an\nefficient fine-tuning method called Mixture of Routers (MoR). It employs\nmultiple sub-routers for joint selection and uses a learnable main router to\ndetermine the weights of the sub-routers. The results show that MoR outperforms\nbaseline models on most tasks, achieving an average performance improvement of\n1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method\nsuitable for a wide range of applications. Our code is available here:\nhttps://anonymous.4open.science/r/MoR-DFC6.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23094", "pdf": "https://arxiv.org/pdf/2503.23094", "abs": "https://arxiv.org/abs/2503.23094", "authors": ["Andrea Boscolo Camiletto", "Jian Wang", "Eduardo Alvarado", "Rishabh Dabral", "Thabo Beeler", "Marc Habermann", "Christian Theobalt"], "title": "FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Egocentric motion capture with a head-mounted body-facing stereo camera is\ncrucial for VR and AR applications but presents significant challenges such as\nheavy occlusions and limited annotated real-world data. Existing methods rely\non synthetic pretraining and struggle to generate smooth and accurate\npredictions in real-world settings, particularly for lower limbs. Our work\naddresses these limitations by introducing a lightweight VR-based data\ncollection setup with on-board, real-time 6D pose tracking. Using this setup,\nwe collected the most extensive real-world dataset for ego-facing ego-mounted\ncameras to date in size and motion variability. Effectively integrating this\nmultimodal input -- device pose and camera feeds -- is challenging due to the\ndiffering characteristics of each data source. To address this, we propose\nFRAME, a simple yet effective architecture that combines device pose and camera\nfeeds for state-of-the-art body pose prediction through geometrically sound\nmultimodal integration and can run at 300 FPS on modern hardware. Lastly, we\nshowcase a novel training strategy to enhance the model's generalization\ncapabilities. Our approach exploits the problem's geometric properties,\nyielding high-quality motion capture free from common artifacts in prior works.\nQualitative and quantitative evaluations, along with extensive comparisons,\ndemonstrate the effectiveness of our method. Data, code, and CAD designs will\nbe available at https://vcai.mpi-inf.mpg.de/projects/FRAME/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23383", "pdf": "https://arxiv.org/pdf/2503.23383", "abs": "https://arxiv.org/abs/2503.23383", "authors": ["Xuefeng Li", "Haoyang Zou", "Pengfei Liu"], "title": "ToRL: Scaling Tool-Integrated RL", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for\ntraining large language models (LLMs) to autonomously use computational tools\nvia reinforcement learning. Unlike supervised fine-tuning, ToRL allows models\nto explore and discover optimal strategies for tool use. Experiments with\nQwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\%\naccuracy on AIME~24, surpassing reinforcement learning without tool integration\nby 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%.\nFurther analysis reveals emergent behaviors such as strategic tool invocation,\nself-regulation of ineffective code, and dynamic adaptation between\ncomputational and analytical reasoning, all arising purely through\nreward-driven learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23121", "pdf": "https://arxiv.org/pdf/2503.23121", "abs": "https://arxiv.org/abs/2503.23121", "authors": ["Guohong Huang", "Ling-An Zeng", "Zexin Zheng", "Shengbo Gu", "Wei-Shi Zheng"], "title": "Efficient Explicit Joint-level Interaction Modeling with Mamba for Text-guided HOI Generation", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "We propose a novel approach for generating text-guided human-object\ninteractions (HOIs) that achieves explicit joint-level interaction modeling in\na computationally efficient manner. Previous methods represent the entire human\nbody as a single token, making it difficult to capture fine-grained joint-level\ninteractions and resulting in unrealistic HOIs. However, treating each\nindividual joint as a token would yield over twenty times more tokens,\nincreasing computational overhead. To address these challenges, we introduce an\nEfficient Explicit Joint-level Interaction Model (EJIM). EJIM features a\nDual-branch HOI Mamba that separately and efficiently models spatiotemporal HOI\ninformation, as well as a Dual-branch Condition Injector for integrating text\nsemantics and object geometry into human and object motions. Furthermore, we\ndesign a Dynamic Interaction Block and a progressive masking mechanism to\niteratively filter out irrelevant joints, ensuring accurate and nuanced\ninteraction modeling. Extensive quantitative and qualitative evaluations on\npublic datasets demonstrate that EJIM surpasses previous works by a large\nmargin while using only 5\\% of the inference time. Code is available\n\\href{https://github.com/Huanggh531/EJIM}{here}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23125", "pdf": "https://arxiv.org/pdf/2503.23125", "abs": "https://arxiv.org/abs/2503.23125", "authors": ["Shuhao Fu", "Andrew Jun Lee", "Anna Wang", "Ida Momennejad", "Trevor Bihl", "Hongjing Lu", "Taylor W. Webb"], "title": "Evaluating Compositional Scene Understanding in Multimodal Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The visual world is fundamentally compositional. Visual scenes are defined by\nthe composition of objects and their relations. Hence, it is essential for\ncomputer vision systems to reflect and exploit this compositionality to achieve\nrobust and generalizable scene understanding. While major strides have been\nmade toward the development of general-purpose, multimodal generative models,\nincluding both text-to-image models and multimodal vision-language models, it\nremains unclear whether these systems are capable of accurately generating and\ninterpreting scenes involving the composition of multiple objects and\nrelations. In this work, we present an evaluation of the compositional visual\nprocessing capabilities in the current generation of text-to-image (DALL-E 3)\nand multimodal vision-language models (GPT-4V, GPT-4o, Claude Sonnet 3.5,\nQWEN2-VL-72B, and InternVL2.5-38B), and compare the performance of these\nsystems to human participants. The results suggest that these systems display\nsome ability to solve compositional and relational tasks, showing notable\nimprovements over the previous generation of multimodal models, but with\nperformance nevertheless well below the level of human participants,\nparticularly for more complex scenes involving many ($>5$) objects and multiple\nrelations. These results highlight the need for further progress toward\ncompositional understanding of visual scenes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23483", "pdf": "https://arxiv.org/pdf/2503.23483", "abs": "https://arxiv.org/abs/2503.23483", "authors": ["Katrina Brown", "Reid McIlroy"], "title": "Order Independence With Finetuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published as a Bi-Align workshop paper at ICLR 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on many NLP\ntasks, yet often exhibit order dependence: simply reordering semantically\nidentical tokens (e.g., answer choices in multiple-choice questions) can lead\nto inconsistent predictions. Recent work proposes Set-Based Prompting (SBP) as\na way to remove order information from designated token subsets, thereby\nmitigating positional biases. However, applying SBP on base models induces an\nout-of-distribution input format, which can degrade in-distribution\nperformance. We introduce a fine-tuning strategy that integrates SBP into the\ntraining process, \"pulling\" these set-formatted prompts closer to the model's\ntraining manifold. We show that SBP can be incorporated into a model via\nfine-tuning. Our experiments on in-distribution (MMLU) and out-of-distribution\n(CSQA, ARC Challenge) multiple-choice tasks show that SBP fine-tuning\nsignificantly improves accuracy and robustness to answer-order permutations,\nall while preserving broader language modeling capabilities. We discuss the\nbroader implications of order-invariant modeling and outline future directions\nfor building fairer, more consistent LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23137", "pdf": "https://arxiv.org/pdf/2503.23137", "abs": "https://arxiv.org/abs/2503.23137", "authors": ["Tuo Liang", "Zhe Hu", "Jing Li", "Hao Zhang", "Yiren Lu", "Yunlai Zhou", "Yiran Qiao", "Disheng Liu", "Jeirui Peng", "Jing Ma", "Yu Yin"], "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding humor-particularly when it involves complex, contradictory\nnarratives that require comparative reasoning-remains a significant challenge\nfor large vision-language models (VLMs). This limitation hinders AI's ability\nto engage in human-like reasoning and cultural expression. In this paper, we\ninvestigate this challenge through an in-depth analysis of comics that\njuxtapose panels to create humor through contradictions. We introduce the\nYesBut (V2), a novel benchmark with 1,262 comic images from diverse\nmultilingual and multicultural contexts, featuring comprehensive annotations\nthat capture various aspects of narrative understanding. Using this benchmark,\nwe systematically evaluate a wide range of VLMs through four complementary\ntasks spanning from surface content comprehension to deep narrative reasoning,\nwith particular emphasis on comparative reasoning between contradictory\nelements. Our extensive experiments reveal that even the most advanced models\nsignificantly underperform compared to humans, with common failures in visual\nperception, key element identification, comparative analysis and\nhallucinations. We further investigate text-based training strategies and\nsocial knowledge augmentation methods to enhance model performance. Our\nfindings not only highlight critical weaknesses in VLMs' understanding of\ncultural and creative expressions but also provide pathways toward developing\ncontext-aware models capable of deeper narrative understanding though\ncomparative reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23523", "pdf": "https://arxiv.org/pdf/2503.23523", "abs": "https://arxiv.org/abs/2503.23523", "authors": ["Haochen Liu", "Song Wang", "Chen Chen", "Jundong Li"], "title": "Question-Aware Knowledge Graph Prompting for Enhancing Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with tasks requiring external\nknowledge, such as knowledge-intensive Multiple Choice Question Answering\n(MCQA). Integrating Knowledge Graphs (KGs) can enhance reasoning; however,\nexisting methods typically demand costly fine-tuning or retrieve noisy KG\ninformation. Recent approaches leverage Graph Neural Networks (GNNs) to\ngenerate KG-based input embedding prefixes as soft prompts for LLMs but fail to\naccount for question relevance, resulting in noisy prompts. Moreover, in MCQA\ntasks, the absence of relevant KG knowledge for certain answer options remains\na significant challenge. To address these issues, we propose Question-Aware\nKnowledge Graph Prompting (QAP), which incorporates question embeddings into\nGNN aggregation to dynamically assess KG relevance. QAP employs global\nattention to capture inter-option relationships, enriching soft prompts with\ninferred knowledge. Experimental results demonstrate that QAP outperforms\nstate-of-the-art methods across multiple datasets, highlighting its\neffectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23178", "pdf": "https://arxiv.org/pdf/2503.23178", "abs": "https://arxiv.org/abs/2503.23178", "authors": ["Pengyu Chen", "Teng Fei", "Yunyan Du", "Jiawei Yi", "Yi Li", "John A. Kupfer"], "title": "Intelligent Bear Prevention System Based on Computer Vision: An Approach to Reduce Human-Bear Conflicts in the Tibetan Plateau Area, China", "categories": ["cs.CV"], "comment": null, "summary": "Conflicts between humans and bears on the Tibetan Plateau present substantial\nthreats to local communities and hinder wildlife preservation initiatives. This\nresearch introduces a novel strategy that incorporates computer vision\nalongside Internet of Things (IoT) technologies to alleviate these issues.\nTailored specifically for the harsh environment of the Tibetan Plateau, the\napproach utilizes the K210 development board paired with the YOLO object\ndetection framework along with a tailored bear-deterrent mechanism, offering\nminimal energy usage and real-time efficiency in bear identification and\ndeterrence. The model's performance was evaluated experimentally, achieving a\nmean Average Precision (mAP) of 91.4%, demonstrating excellent precision and\ndependability. By integrating energy-efficient components, the proposed system\neffectively surpasses the challenges of remote and off-grid environments,\nensuring uninterrupted operation in secluded locations. This study provides a\nviable, eco-friendly, and expandable solution to mitigate human-bear conflicts,\nthereby improving human safety and promoting bear conservation in isolated\nareas like Yushu, China.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23542", "pdf": "https://arxiv.org/pdf/2503.23542", "abs": "https://arxiv.org/abs/2503.23542", "authors": ["Xabier de Zuazo", "Eva Navas", "Ibon Saratxaga", "Inma Hernáez Rioja"], "title": "Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages", "categories": ["cs.CL", "68T50 (Primary), 62H30", "I.2.7; I.2.6; J.5.2"], "comment": "26 pages, 6 figures, includes supplementary materials. Will be\n  submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing", "summary": "Automatic speech recognition systems have undoubtedly advanced with the\nintegration of multilingual and multitask models such as Whisper, which have\nshown a promising ability to understand and process speech across a wide range\nof languages. Despite their robustness, these models often fall short in\nhandling the linguistic distinctions of minority languages. This study\naddresses this gap by integrating traditional and novel language models with\nfine-tuned Whisper models to raise their performance in less commonly studied\nlanguages. Through rigorous fine-tuning and evaluation across multiple\ndatasets, we demonstrate substantial improvements in word error rate,\nparticularly in low-resource scenarios. Our approach not only does take\nadvantage of the extensive data Whisper was pre-trained on, but also\ncomplements its linguistic adaptability by incorporating language models. We\nobtained improvements up to 51\\% for in-distribution datasets and up to 34\\%\nfor out-of-distribution sentences using statistical language models, while\nlarge language models provided moderate but consistently robust improvement\nacross diverse linguistic contexts. The findings reveal that, while the\nintegration reliably benefits all model sizes, the extent of improvement\nvaries, highlighting the importance of optimized language model parameters.\nFinally, we emphasize the importance of selecting appropriate evaluation\nparameters when reporting the results using transformer-based ASR models. In\nsummary, this research clears the way for more inclusive ASR technologies that\nperform better across languages by enriching their linguistic knowledge. For\nfurther implementation details of this study, the technical documentation and\nsource code are available at http://www.github.com/hitz-zentroa/whisper-lm.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23185", "pdf": "https://arxiv.org/pdf/2503.23185", "abs": "https://arxiv.org/abs/2503.23185", "authors": ["Shota Hirose", "Kazuki Kotoyori", "Kasidis Arunruangsirilert", "Fangzheng Lin", "Heming Sun", "Jiro Katto"], "title": "Real-time Video Prediction With Fast Video Interpolation Model and Prediction Training", "categories": ["cs.CV"], "comment": "ICIP 2024", "summary": "Transmission latency significantly affects users' quality of experience in\nreal-time interaction and actuation. As latency is principally inevitable,\nvideo prediction can be utilized to mitigate the latency and ultimately enable\nzero-latency transmission. However, most of the existing video prediction\nmethods are computationally expensive and impractical for real-time\napplications. In this work, we therefore propose real-time video prediction\ntowards the zero-latency interaction over networks, called IFRVP (Intermediate\nFeature Refinement Video Prediction). Firstly, we propose three training\nmethods for video prediction that extend frame interpolation models, where we\nutilize a simple convolution-only frame interpolation network based on IFRNet.\nSecondly, we introduce ELAN-based residual blocks into the prediction models to\nimprove both inference speed and accuracy. Our evaluations show that our\nproposed models perform efficiently and achieve the best trade-off between\nprediction accuracy and computational speed among the existing video prediction\nmethods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23566", "pdf": "https://arxiv.org/pdf/2503.23566", "abs": "https://arxiv.org/abs/2503.23566", "authors": ["Haein Kong", "Seonghyeon Moon"], "title": "When LLM Therapists Become Salespeople: Evaluating Large Language Models for Ethical Motivational Interviewing", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been actively applied in the mental health\nfield. Recent research shows the promise of LLMs in applying psychotherapy,\nespecially motivational interviewing (MI). However, there is a lack of studies\ninvestigating how language models understand MI ethics. Given the risks that\nmalicious actors can use language models to apply MI for unethical purposes, it\nis important to evaluate their capability of differentiating ethical and\nunethical MI practices. Thus, this study investigates the ethical awareness of\nLLMs in MI with multiple experiments. Our findings show that LLMs have a\nmoderate to strong level of knowledge in MI. However, their ethical standards\nare not aligned with the MI spirit, as they generated unethical responses and\nperformed poorly in detecting unethical responses. We proposed a Chain-of-Ethic\nprompt to mitigate those risks and improve safety. Finally, our proposed\nstrategy effectively improved ethical MI response generation and detection\nperformance. These findings highlight the need for safety evaluations and\nguidelines for building ethical LLM-powered psychotherapy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23671", "pdf": "https://arxiv.org/pdf/2503.23671", "abs": "https://arxiv.org/abs/2503.23671", "authors": ["Tongke Ni", "Yang Fan", "Junru Zhou", "Xiangping Wu", "Qingcai Chen"], "title": "CrossFormer: Cross-Segment Semantic Fusion for Document Segmentation", "categories": ["cs.CL"], "comment": "10 pages, 4 figures", "summary": "Text semantic segmentation involves partitioning a document into multiple\nparagraphs with continuous semantics based on the subject matter, contextual\ninformation, and document structure. Traditional approaches have typically\nrelied on preprocessing documents into segments to address input length\nconstraints, resulting in the loss of critical semantic information across\nsegments. To address this, we present CrossFormer, a transformer-based model\nfeaturing a novel cross-segment fusion module that dynamically models latent\nsemantic dependencies across document segments, substantially elevating\nsegmentation accuracy. Additionally, CrossFormer can replace rule-based chunk\nmethods within the Retrieval-Augmented Generation (RAG) system, producing more\nsemantically coherent chunks that enhance its efficacy. Comprehensive\nevaluations confirm CrossFormer's state-of-the-art performance on public text\nsemantic segmentation datasets, alongside considerable gains on RAG benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23226", "pdf": "https://arxiv.org/pdf/2503.23226", "abs": "https://arxiv.org/abs/2503.23226", "authors": ["Kushal Agrawal", "Romi Banerjee"], "title": "Synthetic Art Generation and DeepFake Detection A Study on Jamini Roy Inspired Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 7 figures, 6 tables", "summary": "The intersection of generative AI and art is a fascinating area that brings\nboth exciting opportunities and significant challenges, especially when it\ncomes to identifying synthetic artworks. This study takes a unique approach by\nexamining diffusion-based generative models in the context of Indian art,\nspecifically focusing on the distinctive style of Jamini Roy. To explore this,\nwe fine-tuned Stable Diffusion 3 and used techniques like ControlNet and\nIPAdapter to generate realistic images. This allowed us to create a new dataset\nthat includes both real and AI-generated artworks, which is essential for a\ndetailed analysis of what these models can produce. We employed various\nqualitative and quantitative methods, such as Fourier domain assessments and\nautocorrelation metrics, to uncover subtle differences between synthetic images\nand authentic pieces. A key takeaway from recent research is that existing\nmethods for detecting deepfakes face considerable challenges, especially when\nthe deepfakes are of high quality and tailored to specific cultural contexts.\nThis highlights a critical gap in current detection technologies, particularly\nin light of the challenges identified above, where high-quality and culturally\nspecific deepfakes are difficult to detect. This work not only sheds light on\nthe increasing complexity of generative models but also sets a crucial\nfoundation for future research aimed at effective detection of synthetic art.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23234", "pdf": "https://arxiv.org/pdf/2503.23234", "abs": "https://arxiv.org/abs/2503.23234", "authors": ["Alessio Borgi", "Luca Maiano", "Irene Amerini"], "title": "Z-SASLM: Zero-Shot Style-Aligned SLI Blending Latent Manipulation", "categories": ["cs.CV"], "comment": "Accepted to the CVPR 2025 Workshop AI for Creative Visual Content\n  Generation Editing and Understanding", "summary": "We introduce Z-SASLM, a Zero-Shot Style-Aligned SLI (Spherical Linear\nInterpolation) Blending Latent Manipulation pipeline that overcomes the\nlimitations of current multi-style blending methods. Conventional approaches\nrely on linear blending, assuming a flat latent space leading to suboptimal\nresults when integrating multiple reference styles. In contrast, our framework\nleverages the non-linear geometry of the latent space by using SLI Blending to\ncombine weighted style representations. By interpolating along the geodesic on\nthe hypersphere, Z-SASLM preserves the intrinsic structure of the latent space,\nensuring high-fidelity and coherent blending of diverse styles - all without\nthe need for fine-tuning. We further propose a new metric, Weighted Multi-Style\nDINO ViT-B/8, designed to quantitatively evaluate the consistency of the\nblended styles. While our primary focus is on the theoretical and practical\nadvantages of SLI Blending for style manipulation, we also demonstrate its\neffectiveness in a multi-modal content fusion setting through comprehensive\nexperimental studies. Experimental results show that Z-SASLM achieves enhanced\nand robust style alignment. The implementation code can be found at:\nhttps://github.com/alessioborgi/Z-SASLM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23740", "pdf": "https://arxiv.org/pdf/2503.23740", "abs": "https://arxiv.org/abs/2503.23740", "authors": ["Lu Fan", "Jiashu Pu", "Rongsheng Zhang", "Xiao-Ming Wu"], "title": "LANID: LLM-assisted New Intent Discovery", "categories": ["cs.CL", "cs.AI"], "comment": "Published in LREC-COLING 2024", "summary": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23275", "pdf": "https://arxiv.org/pdf/2503.23275", "abs": "https://arxiv.org/abs/2503.23275", "authors": ["Deeksha Arun", "Kagan Ozturk", "Kevin W. Bowyer", "Patrick Flynn"], "title": "Improved Ear Verification with Vision Transformers and Overlapping Patches", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ear recognition has emerged as a promising biometric modality due to the\nrelative stability in appearance during adulthood. Although Vision Transformers\n(ViTs) have been widely used in image recognition tasks, their efficiency in\near recognition has been hampered by a lack of attention to overlapping\npatches, which is crucial for capturing intricate ear features. In this study,\nwe evaluate ViT-Tiny (ViT-T), ViT-Small (ViT-S), ViT-Base (ViT-B) and ViT-Large\n(ViT-L) configurations on a diverse set of datasets (OPIB, AWE, WPUT, and\nEarVN1.0), using an overlapping patch selection strategy. Results demonstrate\nthe critical importance of overlapping patches, yielding superior performance\nin 44 of 48 experiments in a structured study. Moreover, upon comparing the\nresults of the overlapping patches with the non-overlapping configurations, the\nincrease is significant, reaching up to 10% for the EarVN1.0 dataset. In terms\nof model performance, the ViT-T model consistently outperformed the ViT-S,\nViT-B, and ViT-L models on the AWE, WPUT, and EarVN1.0 datasets. The highest\nscores were achieved in a configuration with a patch size of 28x28 and a stride\nof 14 pixels. This patch-stride configuration represents 25% of the normalized\nimage area (112x112 pixels) for the patch size and 12.5% of the row or column\nsize for the stride. This study confirms that transformer architectures with\noverlapping patch selection can serve as an efficient and high-performing\noption for ear-based biometric recognition tasks in verification scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23300", "pdf": "https://arxiv.org/pdf/2503.23300", "abs": "https://arxiv.org/abs/2503.23300", "authors": ["Wenqi Jia", "Bolin Lai", "Miao Liu", "Danfei Xu", "James M. Rehg"], "title": "Learning Predictive Visuomotor Coordination", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and predicting human visuomotor coordination is crucial for\napplications in robotics, human-computer interaction, and assistive\ntechnologies. This work introduces a forecasting-based task for visuomotor\nmodeling, where the goal is to predict head pose, gaze, and upper-body motion\nfrom egocentric visual and kinematic observations. We propose a\n\\textit{Visuomotor Coordination Representation} (VCR) that learns structured\ntemporal dependencies across these multimodal signals. We extend a\ndiffusion-based motion modeling framework that integrates egocentric vision and\nkinematic sequences, enabling temporally coherent and accurate visuomotor\npredictions. Our approach is evaluated on the large-scale EgoExo4D dataset,\ndemonstrating strong generalization across diverse real-world activities. Our\nresults highlight the importance of multimodal integration in understanding\nvisuomotor coordination, contributing to research in visuomotor learning and\nhuman behavior modeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23811", "pdf": "https://arxiv.org/pdf/2503.23811", "abs": "https://arxiv.org/abs/2503.23811", "authors": ["Chris Brogly", "Connor McElroy"], "title": "Did ChatGPT or Copilot use alter the style of internet news headlines? A time series regression analysis", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The release of advanced Large Language Models (LLMs) such as ChatGPT and\nCopilot is changing the way text is created and may influence the content that\nwe find on the web. This study investigated whether the release of these two\npopular LLMs coincided with a change in writing style in headlines and links on\nworldwide news websites. 175 NLP features were obtained for each text in a\ndataset of 451 million headlines/links. An interrupted time series analysis was\napplied for each of the 175 NLP features to evaluate whether there were any\nstatistically significant sustained changes after the release dates of ChatGPT\nand/or Copilot. There were a total of 44 features that did not appear to have\nany significant sustained change after the release of ChatGPT/Copilot. A total\nof 91 other features did show significant change with ChatGPT and/or Copilot\nalthough significance with earlier control LLM release dates (GPT-1/2/3,\nGopher) removed them from consideration. This initial analysis suggests these\nlanguage models may have had a limited impact on the style of individual news\nheadlines/links, with respect to only some NLP measures.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23848", "pdf": "https://arxiv.org/pdf/2503.23848", "abs": "https://arxiv.org/abs/2503.23848", "authors": ["Minghan Wang", "Ye Bai", "Yuxia Wang", "Thuy-Trang Vu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development", "categories": ["cs.CL"], "comment": null, "summary": "High-quality speech dialogue datasets are crucial for Speech-LLM development,\nyet existing acquisition methods face significant limitations. Human recordings\nincur high costs and privacy concerns, while synthetic approaches often lack\nconversational authenticity. To address these challenges, we introduce\n\\textsc{SpeechDialogueFactory}, a production-ready framework for generating\nnatural speech dialogues efficiently. Our solution employs a comprehensive\npipeline including metadata generation, dialogue scripting,\nparalinguistic-enriched utterance simulation, and natural speech synthesis with\nvoice cloning. Additionally, the system provides an interactive UI for detailed\nsample inspection and a high-throughput batch synthesis mode. Evaluations show\nthat dialogues generated by our system achieve a quality comparable to human\nrecordings while significantly reducing production costs. We release our work\nas an open-source toolkit, alongside example datasets available in English and\nChinese, empowering researchers and developers in Speech-LLM research and\ndevelopment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895", "abs": "https://arxiv.org/abs/2503.23895", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23332", "pdf": "https://arxiv.org/pdf/2503.23332", "abs": "https://arxiv.org/abs/2503.23332", "authors": ["Wenhao Luo", "Zhangyi Shen", "Ye Yao", "Feng Ding", "Guopu Zhu", "Weizhi Meng"], "title": "TraceMark-LDM: Authenticatable Watermarking for Latent Diffusion Models via Binary-Guided Rearrangement", "categories": ["cs.CV"], "comment": "14 pages, 6 figures,", "summary": "Image generation algorithms are increasingly integral to diverse aspects of\nhuman society, driven by their practical applications. However, insufficient\noversight in artificial Intelligence generated content (AIGC) can facilitate\nthe spread of malicious content and increase the risk of copyright\ninfringement. Among the diverse range of image generation models, the Latent\nDiffusion Model (LDM) is currently the most widely used, dominating the\nmajority of the Text-to-Image model market. Currently, most attribution methods\nfor LDMs rely on directly embedding watermarks into the generated images or\ntheir intermediate noise, a practice that compromises both the quality and the\nrobustness of the generated content. To address these limitations, we introduce\nTraceMark-LDM, an novel algorithm that integrates watermarking to attribute\ngenerated images while guaranteeing non-destructive performance. Unlike current\nmethods, TraceMark-LDM leverages watermarks as guidance to rearrange random\nvariables sampled from a Gaussian distribution. To mitigate potential\ndeviations caused by inversion errors, the small absolute elements are grouped\nand rearranged. Additionally, we fine-tune the LDM encoder to enhance the\nrobustness of the watermark. Experimental results show that images synthesized\nusing TraceMark-LDM exhibit superior quality and attribution accuracy compared\nto state-of-the-art (SOTA) techniques. Notably, TraceMark-LDM demonstrates\nexceptional robustness against various common attack methods, consistently\noutperforming SOTA methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23337", "pdf": "https://arxiv.org/pdf/2503.23337", "abs": "https://arxiv.org/abs/2503.23337", "authors": ["Jingui Ma", "Yang Hu", "Luyang Tang", "Jiayu Yang", "Yongqi Zhai", "Ronggang Wang"], "title": "Enhancing 3D Gaussian Splatting Compression via Spatial Condition-based Prediction", "categories": ["cs.CV", "cs.MM"], "comment": "The paper has been accepted by ICME2025 in March,2025", "summary": "Recently, 3D Gaussian Spatting (3DGS) has gained widespread attention in\nNovel View Synthesis (NVS) due to the remarkable real-time rendering\nperformance. However, the substantial cost of storage and transmission of\nvanilla 3DGS hinders its further application (hundreds of megabytes or even\ngigabytes for a single scene). Motivated by the achievements of prediction in\nvideo compression, we introduce the prediction technique into the anchor-based\nGaussian representation to effectively reduce the bit rate. Specifically, we\npropose a spatial condition-based prediction module to utilize the\ngrid-captured scene information for prediction, with a residual compensation\nstrategy designed to learn the missing fine-grained information. Besides, to\nfurther compress the residual, we propose an instance-aware hyper prior,\ndeveloping a structure-aware and instance-aware entropy model. Extensive\nexperiments demonstrate the effectiveness of our prediction-based compression\nframework and each technical component. Even compared with SOTA compression\nmethod, our framework still achieves a bit rate savings of 24.42 percent. Code\nis to be released!", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23344", "pdf": "https://arxiv.org/pdf/2503.23344", "abs": "https://arxiv.org/abs/2503.23344", "authors": ["Ragav Sachdeva", "Andrew Zisserman"], "title": "From Panels to Prose: Generating Literary Narratives from Comics", "categories": ["cs.CV"], "comment": null, "summary": "Comics have long been a popular form of storytelling, offering visually\nengaging narratives that captivate audiences worldwide. However, the visual\nnature of comics presents a significant barrier for visually impaired readers,\nlimiting their access to these engaging stories. In this work, we provide a\npragmatic solution to this accessibility challenge by developing an automated\nsystem that generates text-based literary narratives from manga comics. Our\napproach aims to create an evocative and immersive prose that not only conveys\nthe original narrative but also captures the depth and complexity of\ncharacters, their interactions, and the vivid settings in which they reside.\n  To this end we make the following contributions: (1) We present a unified\nmodel, Magiv3, that excels at various functional tasks pertaining to comic\nunderstanding, such as localising panels, characters, texts, and speech-bubble\ntails, performing OCR, grounding characters etc. (2) We release human-annotated\ncaptions for over 3300 Japanese comic panels, along with character grounding\nannotations, and benchmark large vision-language models in their ability to\nunderstand comic images. (3) Finally, we demonstrate how integrating large\nvision-language models with Magiv3, can generate seamless literary narratives\nthat allows visually impaired audiences to engage with the depth and richness\nof comic storytelling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23990", "pdf": "https://arxiv.org/pdf/2503.23990", "abs": "https://arxiv.org/abs/2503.23990", "authors": ["Yumeng Fu", "Junjie Wu", "Zhongjie Wang", "Meishan Zhang", "Yulin Wu", "Bingquan Liu"], "title": "BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal emotion recognition in conversation (MERC), the task of\nidentifying the emotion label for each utterance in a conversation, is vital\nfor developing empathetic machines. Current MLLM-based MERC studies focus\nmainly on capturing the speaker's textual or vocal characteristics, but ignore\nthe significance of video-derived behavior information. Different from text and\naudio inputs, learning videos with rich facial expression, body language and\nposture, provides emotion trigger signals to the models for more accurate\nemotion predictions. In this paper, we propose a novel behavior-aware\nMLLM-based framework (BeMERC) to incorporate speaker's behaviors, including\nsubtle facial micro-expression, body language and posture, into a vanilla\nMLLM-based MERC model, thereby facilitating the modeling of emotional dynamics\nduring a conversation. Furthermore, BeMERC adopts a two-stage instruction\ntuning strategy to extend the model to the conversations scenario for\nend-to-end training of a MERC predictor. Experiments demonstrate that BeMERC\nachieves superior performance than the state-of-the-art methods on two\nbenchmark datasets, and also provides a detailed discussion on the significance\nof video-derived behavior information in MERC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23353", "pdf": "https://arxiv.org/pdf/2503.23353", "abs": "https://arxiv.org/abs/2503.23353", "authors": ["Xiangyang Luo", "Junhao Cheng", "Yifan Xie", "Xin Zhang", "Tao Feng", "Zhou Liu", "Fei Ma", "Fei Yu"], "title": "Object Isolated Attention for Consistent Story Visualization", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "Open-ended story visualization is a challenging task that involves generating\ncoherent image sequences from a given storyline. One of the main difficulties\nis maintaining character consistency while creating natural and contextually\nfitting scenes--an area where many existing methods struggle. In this paper, we\npropose an enhanced Transformer module that uses separate self attention and\ncross attention mechanisms, leveraging prior knowledge from pre-trained\ndiffusion models to ensure logical scene creation. The isolated self attention\nmechanism improves character consistency by refining attention maps to reduce\nfocus on irrelevant areas and highlight key features of the same character.\nMeanwhile, the isolated cross attention mechanism independently processes each\ncharacter's features, avoiding feature fusion and further strengthening\nconsistency. Notably, our method is training-free, allowing the continuous\ngeneration of new characters and storylines without re-tuning. Both qualitative\nand quantitative evaluations show that our approach outperforms current\nmethods, demonstrating its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24006", "pdf": "https://arxiv.org/pdf/2503.24006", "abs": "https://arxiv.org/abs/2503.24006", "authors": ["Safa Alsaidi", "Marc Vincent", "Olivia Boyer", "Nicolas Garcelon", "Miguel Couceiro", "Adrien Coulet"], "title": "Comparing representations of long clinical texts for the task of patient note-identification", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we address the challenge of patient-note identification, which\ninvolves accurately matching an anonymized clinical note to its corresponding\npatient, represented by a set of related notes. This task has broad\napplications, including duplicate records detection and patient similarity\nanalysis, which require robust patient-level representations. We explore\nvarious embedding methods, including Hierarchical Attention Networks (HAN),\nthree-level Hierarchical Transformer Networks (HTN), LongFormer, and advanced\nBERT-based models, focusing on their ability to process mediumto-long clinical\ntexts effectively. Additionally, we evaluate different pooling strategies\n(mean, max, and mean_max) for aggregating wordlevel embeddings into\npatient-level representations and we examine the impact of sliding windows on\nmodel performance. Our results indicate that BERT-based embeddings outperform\ntraditional and hierarchical models, particularly in processing lengthy\nclinical notes and capturing nuanced patient representations. Among the pooling\nstrategies, mean_max pooling consistently yields the best results, highlighting\nits ability to capture critical features from clinical notes. Furthermore, the\nreproduction of our results on both MIMIC dataset and Necker hospital data\nwarehouse illustrates the generalizability of these approaches to real-world\napplications, emphasizing the importance of both embedding methods and\naggregation strategies in optimizing patient-note identification and enhancing\npatient-level modeling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24013", "pdf": "https://arxiv.org/pdf/2503.24013", "abs": "https://arxiv.org/abs/2503.24013", "authors": ["Gergely Flamich", "David Vilar", "Jan-Thorsten Peter", "Markus Freitag"], "title": "You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation", "categories": ["cs.CL"], "comment": null, "summary": "The goal of translation, be it by human or by machine, is, given some text in\na source language, to produce text in a target language that simultaneously 1)\npreserves the meaning of the source text and 2) achieves natural expression in\nthe target language. However, researchers in the machine translation community\nusually assess translations using a single score intended to capture semantic\naccuracy and the naturalness of the output simultaneously. In this paper, we\nbuild on recent advances in information theory to mathematically prove and\nempirically demonstrate that such single-score summaries do not and cannot give\nthe complete picture of a system's true performance. Concretely, we prove that\na tradeoff exists between accuracy and naturalness and demonstrate it by\nevaluating the submissions to the WMT24 shared task. Our findings help explain\nwell-known empirical phenomena, such as the observation that optimizing\ntranslation systems for a specific accuracy metric (like BLEU) initially\nimproves the system's naturalness, while ``overfitting'' the system to the\nmetric can significantly degrade its naturalness. Thus, we advocate for a\nchange in how translations are evaluated: rather than comparing systems using a\nsingle number, they should be compared on an accuracy-naturalness plane.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23368", "pdf": "https://arxiv.org/pdf/2503.23368", "abs": "https://arxiv.org/abs/2503.23368", "authors": ["Xindi Yang", "Baolu Li", "Yiming Zhang", "Zhenfei Yin", "Lei Bai", "Liqian Ma", "Zhiyong Wang", "Jianfei Cai", "Tien-Tsin Wong", "Huchuan Lu", "Xu Jia"], "title": "Towards Physically Plausible Video Generation via VLM Planning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 11 figures", "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics. In\nthe first stage, we employ a Vision Language Model (VLM) as a coarse-grained\nmotion planner, integrating chain-of-thought and physics-aware reasoning to\npredict a rough motion trajectories/changes that approximate real-world\nphysical dynamics while ensuring the inter-frame consistency. In the second\nstage, we use the predicted motion trajectories/changes to guide the video\ngeneration of a VDM. As the predicted motion trajectories/changes are rough,\nnoise is added during inference to provide freedom to the VDM in generating\nmotion with more fine details. Extensive experimental results demonstrate that\nour framework can produce physically plausible motion, and comparative\nevaluations highlight the notable superiority of our approach over existing\nmethods. More video results are available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23370", "pdf": "https://arxiv.org/pdf/2503.23370", "abs": "https://arxiv.org/abs/2503.23370", "authors": ["Chenxing Sun", "Jing Bai"], "title": "Map Feature Perception Metric for Map Generation Quality Assessment and Loss Optimization", "categories": ["cs.CV"], "comment": null, "summary": "In intelligent cartographic generation tasks empowered by generative models,\nthe authenticity of synthesized maps constitutes a critical determinant.\nConcurrently, the selection of appropriate evaluation metrics to quantify map\nauthenticity emerges as a pivotal research challenge. Current methodologies\npredominantly adopt computer vision-based image assessment metrics to compute\ndiscrepancies between generated and reference maps. However, conventional\nvisual similarity metrics-including L1, L2, SSIM, and FID-primarily operate at\npixel-level comparisons, inadequately capturing cartographic global features\nand spatial correlations, consequently inducing semantic-structural artifacts\nin generated outputs. This study introduces a novel Map Feature Perception\nMetric designed to evaluate global characteristics and spatial congruence\nbetween synthesized and target maps. Diverging from pixel-wise metrics, our\napproach extracts elemental-level deep features that comprehensively encode\ncartographic structural integrity and topological relationships. Experimental\nvalidation demonstrates MFP's superior capability in evaluating cartographic\nsemantic features, with classification-enhanced implementations outperforming\nconventional loss functions across diverse generative frameworks. When employed\nas optimization objectives, our metric achieves performance gains ranging from\n2% to 50% across multiple benchmarks compared to traditional L1, L2, and SSIM\nbaselines. This investigation concludes that explicit consideration of\ncartographic global attributes and spatial coherence substantially enhances\ngenerative model optimization, thereby significantly improving the geographical\nplausibility of synthesized maps.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23379", "pdf": "https://arxiv.org/pdf/2503.23379", "abs": "https://arxiv.org/abs/2503.23379", "authors": ["Haiduo Huang", "Yadong Zhang", "Pengju Ren"], "title": "KernelDNA: Dynamic Kernel Sharing via Decoupled Naive Adapters", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dynamic convolution enhances model capacity by adaptively combining multiple\nkernels, yet faces critical trade-offs: prior works either (1) incur\nsignificant parameter overhead by scaling kernel numbers linearly, (2)\ncompromise inference speed through complex kernel interactions, or (3) struggle\nto jointly optimize dynamic attention and static kernels. We also observe that\npre-trained Convolutional Neural Networks (CNNs) exhibit inter-layer redundancy\nakin to that in Large Language Models (LLMs). Specifically, dense convolutional\nlayers can be efficiently replaced by derived ``child\" layers generated from a\nshared ``parent\" convolutional kernel through an adapter.\n  To address these limitations and implement the weight-sharing mechanism, we\npropose a lightweight convolution kernel plug-in, named KernelDNA. It decouples\nkernel adaptation into input-dependent dynamic routing and pre-trained static\nmodulation, ensuring both parameter efficiency and hardware-friendly inference.\nUnlike existing dynamic convolutions that expand parameters via multi-kernel\nensembles, our method leverages cross-layer weight sharing and adapter-based\nmodulation, enabling dynamic kernel specialization without altering the\nstandard convolution structure. This design preserves the native computational\nefficiency of standard convolutions while enhancing representation power\nthrough input-adaptive kernel adjustments. Experiments on image classification\nand dense prediction tasks demonstrate that KernelDNA achieves state-of-the-art\naccuracy-efficiency balance among dynamic convolution variants. Our codes are\navailable at https://github.com/haiduo/KernelDNA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24206", "pdf": "https://arxiv.org/pdf/2503.24206", "abs": "https://arxiv.org/abs/2503.24206", "authors": ["Abdul Sittar", "Luka Golob", "Mateja Smiljanic"], "title": "Synthetic News Generation for Fake News Classification", "categories": ["cs.CL"], "comment": "13 pages, 8 figures", "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23381", "pdf": "https://arxiv.org/pdf/2503.23381", "abs": "https://arxiv.org/abs/2503.23381", "authors": ["Jiexin Wang", "Wenwen Qiang", "Zhao Yang", "Bing Su"], "title": "Enhancing Human Motion Prediction via Multi-range Decoupling Decoding with Gating-adjusting Aggregation", "categories": ["cs.CV"], "comment": null, "summary": "Expressive representation of pose sequences is crucial for accurate motion\nmodeling in human motion prediction (HMP). While recent deep learning-based\nmethods have shown promise in learning motion representations, these methods\ntend to overlook the varying relevance and dependencies between historical\ninformation and future moments, with a stronger correlation for short-term\npredictions and weaker for distant future predictions. This limits the learning\nof motion representation and then hampers prediction performance. In this\npaper, we propose a novel approach called multi-range decoupling decoding with\ngating-adjusting aggregation ($MD2GA$), which leverages the temporal\ncorrelations to refine motion representation learning. This approach employs a\ntwo-stage strategy for HMP. In the first stage, a multi-range decoupling\ndecoding adeptly adjusts feature learning by decoding the shared features into\ndistinct future lengths, where different decoders offer diverse insights into\nmotion patterns. In the second stage, a gating-adjusting aggregation\ndynamically combines the diverse insights guided by input motion data.\nExtensive experiments demonstrate that the proposed method can be easily\nintegrated into other motion prediction methods and enhance their prediction\nperformance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23398", "pdf": "https://arxiv.org/pdf/2503.23398", "abs": "https://arxiv.org/abs/2503.23398", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Zeynep Akata"], "title": "A Large Scale Analysis of Gender Biases in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "With the increasing use of image generation technology, understanding its\nsocial biases, including gender bias, is essential. This paper presents the\nfirst large-scale study on gender bias in text-to-image (T2I) models, focusing\non everyday situations. While previous research has examined biases in\noccupations, we extend this analysis to gender associations in daily\nactivities, objects, and contexts. We create a dataset of 3,217 gender-neutral\nprompts and generate 200 images per prompt from five leading T2I models. We\nautomatically detect the perceived gender of people in the generated images and\nfilter out images with no person or multiple people of different genders,\nleaving 2,293,295 images. To enable a broad analysis of gender bias in T2I\nmodels, we group prompts into semantically similar concepts and calculate the\nproportion of male- and female-gendered images for each prompt. Our analysis\nshows that T2I models reinforce traditional gender roles, reflect common gender\nstereotypes in household roles, and underrepresent women in financial related\nactivities. Women are predominantly portrayed in care- and human-centered\nscenarios, and men in technical or physical labor scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24293", "pdf": "https://arxiv.org/pdf/2503.24293", "abs": "https://arxiv.org/abs/2503.24293", "authors": ["Hayley Ross", "Kathryn Davidson", "Najoung Kim"], "title": "Is analogy enough to draw novel adjective-noun inferences?", "categories": ["cs.CL"], "comment": "8 pages (16 pages with appendix). Submitted to SCiL 2025", "summary": "Recent work (Ross et al., 2025, 2024) has argued that the ability of humans\nand LLMs respectively to generalize to novel adjective-noun combinations shows\nthat they each have access to a compositional mechanism to determine the\nphrase's meaning and derive inferences. We study whether these inferences can\ninstead be derived by analogy to known inferences, without need for\ncomposition. We investigate this by (1) building a model of analogical\nreasoning using similarity over lexical items, and (2) asking human\nparticipants to reason by analogy. While we find that this strategy works well\nfor a large proportion of the dataset of Ross et al. (2025), there are novel\ncombinations for which both humans and LLMs derive convergent inferences but\nwhich are not well handled by analogy. We thus conclude that the mechanism\nhumans and LLMs use to generalize in these cases cannot be fully reduced to\nanalogy, and likely involves composition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23407", "pdf": "https://arxiv.org/pdf/2503.23407", "abs": "https://arxiv.org/abs/2503.23407", "authors": ["Wei Zeng", "Xuebin Chang", "Jianghao Su", "Xiang Gu", "Jian Sun", "Zongben Xu"], "title": "GMapLatent: Geometric Mapping in Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain generative models based on encoder-decoder AI architectures have\nattracted much attention in generating realistic images, where domain alignment\nis crucial for generation accuracy. Domain alignment methods usually deal\ndirectly with the initial distribution; however, mismatched or mixed clusters\ncan lead to mode collapse and mixture problems in the decoder, compromising\nmodel generalization capabilities. In this work, we innovate a cross-domain\nalignment and generation model that introduces a canonical latent space\nrepresentation based on geometric mapping to align the cross-domain latent\nspaces in a rigorous and precise manner, thus avoiding mode collapse and\nmixture in the encoder-decoder generation architectures. We name this model\nGMapLatent. The core of the method is to seamlessly align latent spaces with\nstrict cluster correspondence constraints using the canonical parameterizations\nof cluster-decorated latent spaces. We first (1) transform the latent space to\na canonical parameter domain by composing barycenter translation, optimal\ntransport merging and constrained harmonic mapping, and then (2) compute\ngeometric registration with cluster constraints over the canonical parameter\ndomains. This process realizes a bijective (one-to-one and onto) mapping\nbetween newly transformed latent spaces and generates a precise alignment of\ncluster pairs. Cross-domain generation is then achieved through the aligned\nlatent spaces embedded in the encoder-decoder pipeline. Experiments on\ngray-scale and color images validate the efficiency, efficacy and applicability\nof GMapLatent, and demonstrate that the proposed model has superior performance\nover existing models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23422", "pdf": "https://arxiv.org/pdf/2503.23422", "abs": "https://arxiv.org/abs/2503.23422", "authors": ["Xin Zuo", "Jiaran Jiang", "Jifeng Shen", "Wankou Yang"], "title": "Improving underwater semantic segmentation with underwater image quality attention and muti-scale aggregation attention", "categories": ["cs.CV"], "comment": "Accepted by Pattern Analysis and Applications", "summary": "Underwater image understanding is crucial for both submarine navigation and\nseabed exploration. However, the low illumination in underwater environments\ndegrades the imaging quality, which in turn seriously deteriorates the\nperformance of underwater semantic segmentation, particularly for outlining the\nobject region boundaries. To tackle this issue, we present UnderWater SegFormer\n(UWSegFormer), a transformer-based framework for semantic segmentation of\nlow-quality underwater images. Firstly, we propose the Underwater Image Quality\nAttention (UIQA) module. This module enhances the representation of highquality\nsemantic information in underwater image feature channels through a channel\nself-attention mechanism. In order to address the issue of loss of imaging\ndetails due to the underwater environment, the Multi-scale Aggregation\nAttention(MAA) module is proposed. This module aggregates sets of semantic\nfeatures at different scales by extracting discriminative information from\nhigh-level features,thus compensating for the semantic loss of detail in\nunderwater objects. Finally, during training, we introduce Edge Learning Loss\n(ELL) in order to enhance the model's learning of underwater object edges and\nimprove the model's prediction accuracy. Experiments conducted on the SUIM and\nDUT-USEG (DUT) datasets have demonstrated that the proposed method has\nadvantages in terms of segmentation completeness, boundary clarity, and\nsubjective perceptual details when compared to SOTA methods. In addition, the\nproposed method achieves the highest mIoU of 82.12 and 71.41 on the SUIM and\nDUT datasets, respectively. Code will be available at\nhttps://github.com/SAWRJJ/UWSegFormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24364", "pdf": "https://arxiv.org/pdf/2503.24364", "abs": "https://arxiv.org/abs/2503.24364", "authors": ["Łukasz Borchmann", "Marek Wydmuch"], "title": "Query and Conquer: Execution-Guided SQL Generation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel approach for generating complex outputs that significantly\nimproves accuracy in text-to-SQL tasks. Our method leverages execution results\nto select the most semantically consistent query from multiple candidates,\nenabling smaller, cost-effective models to surpass computationally intensive\nreasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference\ncost by as much as 30 times. It integrates effortlessly with existing models,\noffering a practical and scalable pathway to state-of-the-art SQL generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24377", "pdf": "https://arxiv.org/pdf/2503.24377", "abs": "https://arxiv.org/abs/2503.24377", "authors": ["Rui Wang", "Hongru Wang", "Boyang Xue", "Jianhui Pang", "Shudong Liu", "Yi Chen", "Jiahao Qiu", "Derek Fai Wong", "Heng Ji", "Kam-Fai Wong"], "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23451", "pdf": "https://arxiv.org/pdf/2503.23451", "abs": "https://arxiv.org/abs/2503.23451", "authors": ["Aimira Baitieva", "Yacine Bouaouni", "Alexandre Briot", "Dick Ameln", "Souhaiel Khalfaoui", "Samet Akcay"], "title": "Beyond Academic Benchmarks: Critical Analysis and Best Practices for Visual Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is essential for automating visual inspection in\nmanufacturing. This field of computer vision is rapidly evolving, with\nincreasing attention towards real-world applications. Meanwhile, popular\ndatasets are typically produced in controlled lab environments with\nartificially created defects, unable to capture the diversity of real\nproduction conditions. New methods often fail in production settings, showing\nsignificant performance degradation or requiring impractical computational\nresources. This disconnect between academic results and industrial viability\nthreatens to misdirect visual anomaly detection research. This paper makes\nthree key contributions: (1) we demonstrate the importance of real-world\ndatasets and establish benchmarks using actual production data, (2) we provide\na fair comparison of existing SOTA methods across diverse tasks by utilizing\nmetrics that are valuable for practical applications, and (3) we present a\ncomprehensive analysis of recent advancements in this field by discussing\nimportant challenges and new perspectives for bridging the academia-industry\ngap. The code is publicly available at\nhttps://github.com/abc-125/viad-benchmark", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22705", "pdf": "https://arxiv.org/pdf/2503.22705", "abs": "https://arxiv.org/abs/2503.22705", "authors": ["Georgios P. Georgiou"], "title": "Enhancing nonnative speech perception and production through an AI-powered application", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "While research on using Artificial Intelligence (AI) through various\napplications to enhance foreign language pronunciation is expanding, it has\nprimarily focused on aspects such as comprehensibility and intelligibility,\nlargely neglecting the improvement of individual speech sounds in both\nperception and production. This study seeks to address this gap by examining\nthe impact of training with an AI-powered mobile application on nonnative sound\nperception and production. Participants completed a pretest assessing their\nability to discriminate the second language English heed-hid contrast and\nproduce these vowels in sentence contexts. The intervention involved training\nwith the Speakometer mobile application, which incorporated recording tasks\nfeaturing the English vowels, along with pronunciation feedback and practice.\nThe posttest mirrored the pretest to measure changes in performance. The\nresults revealed significant improvements in both discrimination accuracy and\nproduction of the target contrast following the intervention. However,\nparticipants did not achieve native-like competence. These findings highlight\nthe effectiveness of AI-powered applications in facilitating speech acquisition\nand support their potential use for personalized, interactive pronunciation\ntraining beyond the classroom.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23455", "pdf": "https://arxiv.org/pdf/2503.23455", "abs": "https://arxiv.org/abs/2503.23455", "authors": ["Junzhu Mao", "Yang Shen", "Jinyang Guo", "Yazhou Yao", "Xiansheng Hua"], "title": "Efficient Token Compression for Vision Transformer with Spatial Information Preserved", "categories": ["cs.CV", "cs.MM"], "comment": "accepted by IEEE Transactions on Multimedia", "summary": "Token compression is essential for reducing the computational and memory\nrequirements of transformer models, enabling their deployment in\nresource-constrained environments. In this work, we propose an efficient and\nhardware-compatible token compression method called Prune and Merge. Our\napproach integrates token pruning and merging operations within transformer\nmodels to achieve layer-wise token compression. By introducing trainable merge\nand reconstruct matrices and utilizing shortcut connections, we efficiently\nmerge tokens while preserving important information and enabling the\nrestoration of pruned tokens. Additionally, we introduce a novel\ngradient-weighted attention scoring mechanism that computes token importance\nscores during the training phase, eliminating the need for separate\ncomputations during inference and enhancing compression efficiency. We also\nleverage gradient information to capture the global impact of tokens and\nautomatically identify optimal compression structures. Extensive experiments on\nthe ImageNet-1k and ADE20K datasets validate the effectiveness of our approach,\nachieving significant speed-ups with minimal accuracy degradation compared to\nstate-of-the-art methods. For instance, on DeiT-Small, we achieve a\n1.64$\\times$ speed-up with only a 0.2\\% drop in accuracy on ImageNet-1k.\nMoreover, by compressing segmenter models and comparing with existing methods,\nwe demonstrate the superior performance of our approach in terms of efficiency\nand effectiveness. Code and models have been made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/prune_and_merge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22735", "pdf": "https://arxiv.org/pdf/2503.22735", "abs": "https://arxiv.org/abs/2503.22735", "authors": ["Andrew Rothwell", "Joss Moorkens", "Tomas Svoboda"], "title": "Training in translation tools and technologies: Findings of the EMT survey 2023", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "This article reports on the third iteration of a survey of computerized tools\nand technologies taught as part of postgraduate translation training\nprogrammes. While the survey was carried out under the aegis of the EMT\nNetwork, more than half of responses are from outside that network. The results\nshow the responsiveness of programmes to innovations in translation technology,\nwith increased compulsory inclusion of machine translation, post-editing, and\nquality evaluation, and a rapid response to the release of generative tools.\nThe flexibility required during the Covid-19 pandemic has also led to some\nlasting changes to programmes. While the range of tools being taught has\ncontinued to expand, programmes seem to be consolidating their core offering\naround cloud-based software with cost-free academic access. There has also been\nan increase in the embedding of professional contexts and workflows associated\nwith translation technology. Generic file management and data security skills\nhave increased in perceived importance, and legal and ethical issues related to\ntranslation data have also become more prominent. In terms of course delivery\nthe shift away from conventional labs identified in EMT2017 has accelerated\nmarkedly, no doubt partly driven by the pandemic, accompanied by a dramatic\nexpansion in the use of students' personal devices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23463", "pdf": "https://arxiv.org/pdf/2503.23463", "abs": "https://arxiv.org/abs/2503.23463", "authors": ["Xingcheng Zhou", "Xuyuan Han", "Feng Yang", "Yunpu Ma", "Alois C. Knoll"], "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model", "categories": ["cs.CV"], "comment": null, "summary": "We present OpenDriveVLA, a Vision-Language Action (VLA) model designed for\nend-to-end autonomous driving. OpenDriveVLA builds upon open-source pre-trained\nlarge Vision-Language Models (VLMs) to generate reliable driving actions,\nconditioned on 3D environmental perception, ego vehicle states, and driver\ncommands. To bridge the modality gap between driving visual representations and\nlanguage embeddings, we propose a hierarchical vision-language alignment\nprocess, projecting both 2D and 3D structured visual tokens into a unified\nsemantic space. Besides, OpenDriveVLA models the dynamic relationships between\nthe ego vehicle, surrounding agents, and static road elements through an\nautoregressive agent-env-ego interaction process, ensuring both spatially and\nbehaviorally informed trajectory planning. Extensive experiments on the\nnuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art\nresults across open-loop trajectory planning and driving-related\nquestion-answering tasks. Qualitative analyses further illustrate\nOpenDriveVLA's superior capability to follow high-level driving commands and\nrobustly generate trajectories under challenging scenarios, highlighting its\npotential for next-generation end-to-end autonomous driving. We will release\nour code to facilitate further research in this domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23468", "pdf": "https://arxiv.org/pdf/2503.23468", "abs": "https://arxiv.org/abs/2503.23468", "authors": ["Eytan Kats", "Kai Geißler", "Jochen G. Hirsch", "Stefan Heldman", "Mattias P. Heinrich"], "title": "Internal Organ Localization Using Depth Images", "categories": ["cs.CV"], "comment": "Accepted for German Conference on Medical Image Computing 2025 (BVM\n  2025)", "summary": "Automated patient positioning is a crucial step in streamlining MRI workflows\nand enhancing patient throughput. RGB-D camera-based systems offer a promising\napproach to automate this process by leveraging depth information to estimate\ninternal organ positions. This paper investigates the feasibility of a\nlearning-based framework to infer approximate internal organ positions from the\nbody surface. Our approach utilizes a large-scale dataset of MRI scans to train\na deep learning model capable of accurately predicting organ positions and\nshapes from depth images alone. We demonstrate the effectiveness of our method\nin localization of multiple internal organs, including bones and soft tissues.\nOur findings suggest that RGB-D camera-based systems integrated into MRI\nworkflows have the potential to streamline scanning procedures and improve\npatient experience by enabling accurate and automated patient positioning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22832", "pdf": "https://arxiv.org/pdf/2503.22832", "abs": "https://arxiv.org/abs/2503.22832", "authors": ["Simeng Sun", "Cheng-Ping Hsieh", "Faisal Ladhak", "Erik Arakelyan", "Santiago Akle Serano", "Boris Ginsburg"], "title": "L0-Reasoning Bench: Evaluating Procedural Correctness in Language Models via Simple Program Execution", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Complex reasoning tasks often rely on the ability to consistently and\naccurately apply simple rules across incremental steps, a foundational\ncapability which we term \"level-0\" reasoning. To systematically evaluate this\ncapability, we introduce L0-Bench, a language model benchmark for testing\nprocedural correctness -- the ability to generate correct reasoning processes,\ncomplementing existing benchmarks that primarily focus on outcome correctness.\nGiven synthetic Python functions with simple operations, L0-Bench grades models\non their ability to generate step-by-step, error-free execution traces. The\nsynthetic nature of L0-Bench enables systematic and scalable generation of test\nprograms along various axes (e.g., number of trace steps). We evaluate a\ndiverse array of recent closed-source and open-weight models on a baseline test\nset. All models exhibit degradation as the number of target trace steps\nincreases, while larger models and reasoning-enhanced models better maintain\ncorrectness over multiple steps. Additionally, we use L0-Bench to explore\ntest-time scaling along three dimensions: input context length, number of\nsolutions for majority voting, and inference steps. Our results suggest\nsubstantial room to improve \"level-0\" reasoning and potential directions to\nbuild more reliable reasoning systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22968", "pdf": "https://arxiv.org/pdf/2503.22968", "abs": "https://arxiv.org/abs/2503.22968", "authors": ["Hanwool Lee", "Soo Yong Kim", "Dasol Choi", "SangWon Baek", "Seunghyeok Hong", "Ilgyun Jeong", "Inseon Hwang", "Naeun Lee", "Guijin Son"], "title": "HRET: A Self-Evolving LLM Evaluation Toolkit for Korean", "categories": ["cs.CE", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in Korean large language models (LLMs) have spurred\nnumerous benchmarks and evaluation methodologies, yet the lack of a\nstandardized evaluation framework has led to inconsistent results and limited\ncomparability. To address this, we introduce HRET Haerae Evaluation Toolkit, an\nopen-source, self-evolving evaluation framework tailored specifically for\nKorean LLMs. HRET unifies diverse evaluation methods, including logit-based\nscoring, exact-match, language-inconsistency penalization, and LLM-as-a-Judge\nassessments. Its modular, registry-based architecture integrates major\nbenchmarks (HAE-RAE Bench, KMMLU, KUDGE, HRM8K) and multiple inference backends\n(vLLM, HuggingFace, OpenAI-compatible endpoints). With automated pipelines for\ncontinuous evolution, HRET provides a robust foundation for reproducible, fair,\nand transparent Korean NLP research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23507", "pdf": "https://arxiv.org/pdf/2503.23507", "abs": "https://arxiv.org/abs/2503.23507", "authors": ["Siladittya Manna", "Suresh Das", "Sayantari Ghosh", "Saumik Bhattacharya"], "title": "Federated Self-Supervised Learning for One-Shot Cross-Modal and Cross-Imaging Technique Segmentation", "categories": ["cs.CV", "cs.LG", "eess.IV", "physics.med-ph"], "comment": null, "summary": "Decentralized federated learning enables learning of data representations\nfrom multiple sources without compromising the privacy of the clients. In\napplications like medical image segmentation, where obtaining a large annotated\ndataset from a single source is a distressing problem, federated\nself-supervised learning can provide some solace. In this work, we push the\nlimits further by exploring a federated self-supervised one-shot segmentation\ntask representing a more data-scarce scenario. We adopt a pre-existing\nself-supervised few-shot segmentation framework CoWPro and adapt it to the\nfederated learning scenario. To the best of our knowledge, this work is the\nfirst to attempt a self-supervised few-shot segmentation task in the federated\nlearning domain. Moreover, we consider the clients to be constituted of data\nfrom different modalities and imaging techniques like MR or CT, which makes the\nproblem even harder. Additionally, we reinforce and improve the baseline CoWPro\nmethod using a fused dice loss which shows considerable improvement in\nperformance over the baseline CoWPro. Finally, we evaluate this novel framework\non a completely unseen held-out part of the local client dataset. We observe\nthat the proposed framework can achieve performance at par or better than the\nFedAvg version of the CoWPro framework on the held-out validation dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23508", "pdf": "https://arxiv.org/pdf/2503.23508", "abs": "https://arxiv.org/abs/2503.23508", "authors": ["Yuming Chen", "Jiangyan Feng", "Haodong Zhang", "Lijun Gong", "Feng Zhu", "Rui Zhao", "Qibin Hou", "Ming-Ming Cheng", "Yibing Song"], "title": "Re-Aligning Language to Visual Objects with an Agentic Workflow", "categories": ["cs.CV"], "comment": "33 pages, 20 figures, 17 tables, ICLR 2025", "summary": "Language-based object detection (LOD) aims to align visual objects with\nlanguage expressions. A large amount of paired data is utilized to improve LOD\nmodel generalizations. During the training process, recent studies leverage\nvision-language models (VLMs) to automatically generate human-like expressions\nfor visual objects, facilitating training data scaling up. In this process, we\nobserve that VLM hallucinations bring inaccurate object descriptions (e.g.,\nobject name, color, and shape) to deteriorate VL alignment quality. To reduce\nVLM hallucinations, we propose an agentic workflow controlled by an LLM to\nre-align language to visual objects via adaptively adjusting image and text\nprompts. We name this workflow Real-LOD, which includes planning, tool use, and\nreflection steps. Given an image with detected objects and VLM raw language\nexpressions, Real-LOD reasons its state automatically and arranges action based\non our neural symbolic designs (i.e., planning). The action will adaptively\nadjust the image and text prompts and send them to VLMs for object\nre-description (i.e., tool use). Then, we use another LLM to analyze these\nrefined expressions for feedback (i.e., reflection). These steps are conducted\nin a cyclic form to gradually improve language descriptions for re-aligning to\nvisual objects. We construct a dataset that contains a tiny amount of 0.18M\nimages with re-aligned language expression and train a prevalent LOD model to\nsurpass existing LOD methods by around 50% on the standard benchmarks. Our\nReal-LOD workflow, with automatic VL refinement, reveals a potential to\npreserve data quality along with scaling up data quantity, which further\nimproves LOD performance from a data-alignment perspective.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23509", "pdf": "https://arxiv.org/pdf/2503.23509", "abs": "https://arxiv.org/abs/2503.23509", "authors": ["Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025", "categories": ["cs.CV"], "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment target objects\nthroughout a video based on a text description. This task has attracted\nincreasing attention in the field of computer vision due to its promising\napplications in video editing and human-agent interaction. Recently, ReferDINO\nhas demonstrated promising performance in this task by adapting object-level\nvision-language knowledge from pretrained foundational image models. In this\nreport, we further enhance its capabilities by incorporating the advantages of\nSAM2 in mask quality and object consistency. In addition, to effectively\nbalance performance between single-object and multi-object scenarios, we\nintroduce a conditional mask fusion strategy that adaptively fuses the masks\nfrom ReferDINO and SAM2. Our solution, termed ReferDINO-Plus, achieves 60.43\n\\(\\mathcal{J}\\&\\mathcal{F}\\) on MeViS test set, securing 2nd place in the MeViS\nPVUW challenge at CVPR 2025. The code is available at:\nhttps://github.com/iSEE-Laboratory/ReferDINO-Plus.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23529", "pdf": "https://arxiv.org/pdf/2503.23529", "abs": "https://arxiv.org/abs/2503.23529", "authors": ["Shuhei Tarashima", "Xinqi Shu", "Norio Tagawa"], "title": "ViLAaD: Enhancing \"Attracting and Dispersing'' Source-Free Domain Adaptation with Vision-and-Language Model", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto a target dataset from a different domain without access to the source data.\nConventional SFDA methods are limited by the information encoded in the\npre-trained source model and the unlabeled target data. Recently, approaches\nleveraging auxiliary resources have emerged, yet remain in their early stages,\noffering ample opportunities for research. In this work, we propose a novel\nmethod that incorporates auxiliary information by extending an existing SFDA\nframework using Vision-and-Language (ViL) models. Specifically, we build upon\nAttracting and Dispersing (AaD), a widely adopted SFDA technique, and\ngeneralize its core principle to naturally integrate ViL models as a powerful\ninitialization for target adaptation. Our approach, called ViL-enhanced AaD\n(ViLAaD), preserves the simplicity and flexibility of the AaD framework, while\nleveraging ViL models to significantly boost adaptation performance. We\nvalidate our method through experiments using various ViL models, demonstrating\nthat ViLAaD consistently outperforms both AaD and zero-shot classification by\nViL models, especially when both the source model and ViL model provide strong\ninitializations. Moreover, the flexibility of ViLAaD allows it to be seamlessly\nincorporated into an alternating optimization framework with ViL prompt tuning\nand extended with additional objectives for target model adaptation. Extensive\nexperiments on four SFDA benchmarks show that this enhanced version, ViLAaD++,\nachieves state-of-the-art performance across multiple SFDA scenarios, including\nClosed-set SFDA, Partial-set SFDA, and Open-set SFDA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23137", "pdf": "https://arxiv.org/pdf/2503.23137", "abs": "https://arxiv.org/abs/2503.23137", "authors": ["Tuo Liang", "Zhe Hu", "Jing Li", "Hao Zhang", "Yiren Lu", "Yunlai Zhou", "Yiran Qiao", "Disheng Liu", "Jeirui Peng", "Jing Ma", "Yu Yin"], "title": "When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Understanding humor-particularly when it involves complex, contradictory\nnarratives that require comparative reasoning-remains a significant challenge\nfor large vision-language models (VLMs). This limitation hinders AI's ability\nto engage in human-like reasoning and cultural expression. In this paper, we\ninvestigate this challenge through an in-depth analysis of comics that\njuxtapose panels to create humor through contradictions. We introduce the\nYesBut (V2), a novel benchmark with 1,262 comic images from diverse\nmultilingual and multicultural contexts, featuring comprehensive annotations\nthat capture various aspects of narrative understanding. Using this benchmark,\nwe systematically evaluate a wide range of VLMs through four complementary\ntasks spanning from surface content comprehension to deep narrative reasoning,\nwith particular emphasis on comparative reasoning between contradictory\nelements. Our extensive experiments reveal that even the most advanced models\nsignificantly underperform compared to humans, with common failures in visual\nperception, key element identification, comparative analysis and\nhallucinations. We further investigate text-based training strategies and\nsocial knowledge augmentation methods to enhance model performance. Our\nfindings not only highlight critical weaknesses in VLMs' understanding of\ncultural and creative expressions but also provide pathways toward developing\ncontext-aware models capable of deeper narrative understanding though\ncomparative reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23606", "pdf": "https://arxiv.org/pdf/2503.23606", "abs": "https://arxiv.org/abs/2503.23606", "authors": ["Wei Xu", "Charles James Wagner", "Junjie Luo", "Qi Guo"], "title": "Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project page: https://blurry-edges.qiguo.org/", "summary": "Extracting depth information from photon-limited, defocused images is\nchallenging because depth from defocus (DfD) relies on accurate estimation of\ndefocus blur, which is fundamentally sensitive to image noise. We present a\nnovel approach to robustly measure object depths from photon-limited images\nalong the defocused boundaries. It is based on a new image patch\nrepresentation, Blurry-Edges, that explicitly stores and visualizes a rich set\nof low-level patch information, including boundaries, color, and smoothness. We\ndevelop a deep neural network architecture that predicts the Blurry-Edges\nrepresentation from a pair of differently defocused images, from which depth\ncan be calculated using a closed-form DfD relation we derive. The experimental\nresults on synthetic and real data show that our method achieves the highest\ndepth estimation accuracy on photon-limited images compared to a broad range of\nstate-of-the-art DfD methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23618", "pdf": "https://arxiv.org/pdf/2503.23618", "abs": "https://arxiv.org/abs/2503.23618", "authors": ["Amar Kumar", "Anita Kriz", "Barak Pertzov", "Tal Arbel"], "title": "Leveraging Vision-Language Foundation Models to Reveal Hidden Image-Attribute Relationships in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language foundation models (VLMs) have shown impressive performance in\nguiding image generation through text, with emerging applications in medical\nimaging. In this work, we are the first to investigate the question: 'Can\nfine-tuned foundation models help identify critical, and possibly unknown, data\nproperties?' By evaluating our proposed method on a chest x-ray dataset, we\nshow that these models can generate high-resolution, precisely edited images\ncompared to methods that rely on Structural Causal Models (SCMs) according to\nnumerous metrics. For the first time, we demonstrate that fine-tuned VLMs can\nreveal hidden data relationships that were previously obscured due to available\nmetadata granularity and model capacity limitations. Our experiments\ndemonstrate both the potential of these models to reveal underlying dataset\nproperties while also exposing the limitations of fine-tuned VLMs for accurate\nimage editing and susceptibility to biases and spurious correlations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23664", "pdf": "https://arxiv.org/pdf/2503.23664", "abs": "https://arxiv.org/abs/2503.23664", "authors": ["Masahiko Tsuji", "Hitoshi Niigaki", "Ryuichi Tanida"], "title": "LiM-Loc: Visual Localization with Dense and Accurate 3D Reference Maps Directly Corresponding 2D Keypoints to 3D LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Visual localization is to estimate the 6-DOF camera pose of a query image in\na 3D reference map. We extract keypoints from the reference image and generate\na 3D reference map with 3D reconstruction of the keypoints in advance. We\nemphasize that the more keypoints in the 3D reference map and the smaller the\nerror of the 3D positions of the keypoints, the higher the accuracy of the\ncamera pose estimation. However, previous image-only methods require a huge\nnumber of images, and it is difficult to 3D-reconstruct keypoints without error\ndue to inevitable mismatches and failures in feature matching. As a result, the\n3D reference map is sparse and inaccurate. In contrast, accurate 3D reference\nmaps can be generated by combining images and 3D sensors. Recently, 3D-LiDAR\nhas been widely used around the world. LiDAR, which measures a large space with\nhigh density, has become inexpensive. In addition, accurately calibrated\ncameras are also widely used, so images that record the external parameters of\nthe camera without errors can be easily obtained. In this paper, we propose a\nmethod to directly assign 3D LiDAR point clouds to keypoints to generate dense\nand accurate 3D reference maps. The proposed method avoids feature matching and\nachieves accurate 3D reconstruction for almost all keypoints. To estimate\ncamera pose over a wide area, we use the wide-area LiDAR point cloud to remove\npoints that are not visible to the camera and reduce 2D-3D correspondence\nerrors. Using indoor and outdoor datasets, we apply the proposed method to\nseveral state-of-the-art local features and confirm that it improves the\naccuracy of camera pose estimation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23684", "pdf": "https://arxiv.org/pdf/2503.23684", "abs": "https://arxiv.org/abs/2503.23684", "authors": ["Haitao Tian", "Junyang Li", "Chenxing Wang", "Helong Jiang"], "title": "Detail-aware multi-view stereo network for depth estimation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view stereo methods have achieved great success for depth estimation\nbased on the coarse-to-fine depth learning frameworks, however, the existing\nmethods perform poorly in recovering the depth of object boundaries and detail\nregions. To address these issues, we propose a detail-aware multi-view stereo\nnetwork (DA-MVSNet) with a coarse-to-fine framework. The geometric depth clues\nhidden in the coarse stage are utilized to maintain the geometric structural\nrelationships between object surfaces and enhance the expressive capability of\nimage features. In addition, an image synthesis loss is employed to constrain\nthe gradient flow for detailed regions and further strengthen the supervision\nof object boundaries and texture-rich areas. Finally, we propose an adaptive\ndepth interval adjustment strategy to improve the accuracy of object\nreconstruction. Extensive experiments on the DTU and Tanks & Temples datasets\ndemonstrate that our method achieves competitive results. The code is available\nat https://github.com/wsmtht520-/DAMVSNet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23709", "pdf": "https://arxiv.org/pdf/2503.23709", "abs": "https://arxiv.org/abs/2503.23709", "authors": ["Xulong Shi", "Caiyi Sun", "Zhi Qi", "Liu Hao", "Xiaodong Yang"], "title": "Expanding-and-Shrinking Binary Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "While binary neural networks (BNNs) offer significant benefits in terms of\nspeed, memory and energy, they encounter substantial accuracy degradation in\nchallenging tasks compared to their real-valued counterparts. Due to the\nbinarization of weights and activations, the possible values of each entry in\nthe feature maps generated by BNNs are strongly constrained. To tackle this\nlimitation, we propose the expanding-and-shrinking operation, which enhances\nbinary feature maps with negligible increase of computation complexity, thereby\nstrengthening the representation capacity. Extensive experiments conducted on\nmultiple benchmarks reveal that our approach generalizes well across diverse\napplications ranging from image classification, object detection to generative\ndiffusion model, while also achieving remarkable improvement over various\nleading binarization algorithms based on different architectures including both\nCNNs and Transformers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23712", "pdf": "https://arxiv.org/pdf/2503.23712", "abs": "https://arxiv.org/abs/2503.23712", "authors": ["Jie Cheng", "Hao Zheng", "Meiguang Zheng", "Lei Wang", "Hao Wu", "Jian Zhang"], "title": "ElimPCL: Eliminating Noise Accumulation with Progressive Curriculum Labeling for Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": "ICME 2025 camera-ready", "summary": "Source-Free Domain Adaptation (SFDA) aims to train a target model without\nsource data, and the key is to generate pseudo-labels using a pre-trained\nsource model. However, we observe that the source model often produces highly\nuncertain pseudo-labels for hard samples, particularly those heavily affected\nby domain shifts, leading to these noisy pseudo-labels being introduced even\nbefore adaptation and further reinforced through parameter updates.\nAdditionally, they continuously influence neighbor samples through propagation\nin the feature space.To eliminate the issue of noise accumulation, we propose a\nnovel Progressive Curriculum Labeling (ElimPCL) method, which iteratively\nfilters trustworthy pseudo-labeled samples based on prototype consistency to\nexclude high-noise samples from training. Furthermore, a Dual MixUP technique\nis designed in the feature space to enhance the separability of hard samples,\nthereby mitigating the interference of noisy samples on their\nneighbors.Extensive experiments validate the effectiveness of ElimPCL,\nachieving up to a 3.4% improvement on challenging tasks compared to\nstate-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24290", "pdf": "https://arxiv.org/pdf/2503.24290", "abs": "https://arxiv.org/abs/2503.24290", "authors": ["Jingcheng Hu", "Yinmin Zhang", "Qi Han", "Daxin Jiang", "Xiangyu Zhang", "Heung-Yeung Shum"], "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23725", "pdf": "https://arxiv.org/pdf/2503.23725", "abs": "https://arxiv.org/abs/2503.23725", "authors": ["Hongwei Ren", "Xiaopeng Lin", "Hongxiang Huang", "Yue Zhou", "Bojun Cheng"], "title": "Exploring Temporal Dynamics in Event-based Eye Tracker", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Event-based Vision Workshop", "summary": "Eye-tracking is a vital technology for human-computer interaction, especially\nin wearable devices such as AR, VR, and XR. The realization of high-speed and\nhigh-precision eye-tracking using frame-based image sensors is constrained by\ntheir limited temporal resolution, which impairs the accurate capture of rapid\nocular dynamics, such as saccades and blinks. Event cameras, inspired by\nbiological vision systems, are capable of perceiving eye movements with\nextremely low power consumption and ultra-high temporal resolution. This makes\nthem a promising solution for achieving high-speed, high-precision tracking\nwith rich temporal dynamics. In this paper, we propose TDTracker, an effective\neye-tracking framework that captures rapid eye movements by thoroughly modeling\ntemporal dynamics from both implicit and explicit perspectives. TDTracker\nutilizes 3D convolutional neural networks to capture implicit short-term\ntemporal dynamics and employs a cascaded structure consisting of a\nFrequency-aware Module, GRU, and Mamba to extract explicit long-term temporal\ndynamics. Ultimately, a prediction heatmap is used for eye coordinate\nregression. Experimental results demonstrate that TDTracker achieves\nstate-of-the-art (SOTA) performance on the synthetic SEET dataset and secured\nThird place in the CVPR event-based eye-tracking challenge 2025. Our code is\navailable at https://github.com/rhwxmx/TDTracker.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23731", "pdf": "https://arxiv.org/pdf/2503.23731", "abs": "https://arxiv.org/abs/2503.23731", "authors": ["Yinq-Rong Chern", "Yuhao Lee", "Hsiao-Ching Lin", "Guan-Ting Chen", "Ying-Hsien Chen", "Fu-Sung Lin", "Chih-Yao Chuang", "Jenn-Jier James Lien", "Chih-Hsien Huang"], "title": "Investigation of intelligent barbell squat coaching system based on computer vision and machine learning", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Purpose: Research has revealed that strength training can reduce the\nincidence of chronic diseases and physical deterioration at any age. Therefore,\nhaving a movement diagnostic system is crucial for training alone. Hence, this\nstudy developed an artificial intelligence and computer vision-based barbell\nsquat coaching system with a real-time mode that immediately diagnoses the\nissue and provides feedback after each squat. In addition, a replay mode allows\nusers to examine their previous squats and check their comments. Initially,\nfour primary characteristics of the barbell squat were identified: body joint\nangles, dorsiflexion, the ratio of knee-to-hip movement, and barbell stability.\nMethods: We collect 8,151 squats from 77 participants, categorizing them as\ngood squats and six issues. Then, we trained the diagnosis models with three\nmachine-learning architectures. Furthermore, this research applied the SHapley\nAdditive exPlanations (SHAP) method to enhance the accuracy of issue prediction\nand reduce the computation time by feature selection. Results: The F1 score of\nthe six issues reached 86.86%, 69.01%, 77.42%, 90.74%, 95.83%, and 100%. Each\nsquat diagnosis took less than 0.5 seconds. Finally, this study examined the\nefficacy of the proposed system with two groups of participants trained with\nand without the system. Subsequently, participants trained with the system\nexhibited substantial improvements in their squat technique, as assessed both\nby the system itself and by a professional weightlifting coach. Conclusion:\nThis is a comprehensive study that integrates artificial intelligence, computer\nvision and multivariable processing technologies, aimed at building a\nreal-time, user-friendly barbell squat feedback and training system.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23736", "pdf": "https://arxiv.org/pdf/2503.23736", "abs": "https://arxiv.org/abs/2503.23736", "authors": ["Lingyu Liu", "Yaxiong Wang", "Li Zhu", "Zhedong Zheng"], "title": "Every Painting Awakened: A Training-free Framework for Painting-to-Animation Generation", "categories": ["cs.CV", "cs.MM"], "comment": "The project is available at:\n  https://painting-animation.github.io/animation/", "summary": "We introduce a training-free framework specifically designed to bring\nreal-world static paintings to life through image-to-video (I2V) synthesis,\naddressing the persistent challenge of aligning these motions with textual\nguidance while preserving fidelity to the original artworks. Existing I2V\nmethods, primarily trained on natural video datasets, often struggle to\ngenerate dynamic outputs from static paintings. It remains challenging to\ngenerate motion while maintaining visual consistency with real-world paintings.\nThis results in two distinct failure modes: either static outputs due to\nlimited text-based motion interpretation or distorted dynamics caused by\ninadequate alignment with real-world artistic styles. We leverage the advanced\ntext-image alignment capabilities of pre-trained image models to guide the\nanimation process. Our approach introduces synthetic proxy images through two\nkey innovations: (1) Dual-path score distillation: We employ a dual-path\narchitecture to distill motion priors from both real and synthetic data,\npreserving static details from the original painting while learning dynamic\ncharacteristics from synthetic frames. (2) Hybrid latent fusion: We integrate\nhybrid features extracted from real paintings and synthetic proxy images via\nspherical linear interpolation in the latent space, ensuring smooth transitions\nand enhancing temporal consistency. Experimental evaluations confirm that our\napproach significantly improves semantic alignment with text prompts while\nfaithfully preserving the unique characteristics and integrity of the original\npaintings. Crucially, by achieving enhanced dynamic effects without requiring\nany model training or learnable parameters, our framework enables plug-and-play\nintegration with existing I2V methods, making it an ideal solution for\nanimating real-world paintings. More animated examples can be found on our\nproject website.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23746", "pdf": "https://arxiv.org/pdf/2503.23746", "abs": "https://arxiv.org/abs/2503.23746", "authors": ["Dizhan Xue", "Jing Cui", "Shengsheng Qian", "Chuanrui Hu", "Changsheng Xu"], "title": "Short-video Propagation Influence Rating: A New Real-world Dataset and A New Large Graph Model", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM", "cs.SI"], "comment": null, "summary": "Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23765", "pdf": "https://arxiv.org/pdf/2503.23765", "abs": "https://arxiv.org/abs/2503.23765", "authors": ["Yun Li", "Yiming Zhang", "Tao Lin", "XiangRui Liu", "Wenxiao Cai", "Zheng Liu", "Bo Zhao"], "title": "STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution\nfor Embodied AI and Autonomous Driving has become a prevailing trend. While\nMLLMs have been extensively studied for visual semantic understanding tasks,\ntheir ability to perform precise and quantitative spatial-temporal\nunderstanding in real-world applications remains largely unexamined, leading to\nuncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we\nintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal\nunderstanding through challenging tasks such as estimating and predicting the\nappearance, pose, displacement, and motion of objects. Our benchmark\nencompasses a wide range of robot and vehicle operations across desktop,\nindoor, and outdoor scenarios. The extensive experiments reveals that the\nstate-of-the-art MLLMs still struggle in real-world spatial-temporal\nunderstanding, especially in tasks requiring precise distance estimation and\nmotion analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23786", "pdf": "https://arxiv.org/pdf/2503.23786", "abs": "https://arxiv.org/abs/2503.23786", "authors": ["Haoran Shen", "Peixian Zhuang", "Jiahao Kou", "Yuxin Zeng", "Haoying Xu", "Jiangyun Li"], "title": "MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Models (SAMs), as vision foundation models, have\ndemonstrated remarkable performance across various image analysis tasks.\nDespite their strong generalization capabilities, SAMs encounter challenges in\nfine-grained detail segmentation for high-resolution class-independent\nsegmentation (HRCS), due to the limitations in the direct processing of\nhigh-resolution inputs and low-resolution mask predictions, and the reliance on\naccurate manual prompts. To address these limitations, we propose MGD-SAM2\nwhich integrates SAM2 with multi-view feature interaction between a global\nimage and local patches to achieve precise segmentation. MGD-SAM2 incorporates\nthe pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter\n(MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the\nHierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement\nModule (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2\nencoder for enhanced extraction of local details and global semantics in HRCS\nimages. Then, MCEM and HMIM are proposed to further exploit local texture and\nglobal context by aggregating multi-view features within and across\nmulti-scales. Finally, DRM is designed to generate gradually restored\nhigh-resolution mask predictions, compensating for the loss of fine-grained\ndetails resulting from directly upsampling the low-resolution prediction maps.\nExperimental results demonstrate the superior performance and strong\ngeneralization of our model on multiple high-resolution and normal-resolution\ndatasets. Code will be available at https://github.com/sevenshr/MGD-SAM2.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23793", "pdf": "https://arxiv.org/pdf/2503.23793", "abs": "https://arxiv.org/abs/2503.23793", "authors": ["Zhongnan Cai", "Yingying Wang", "Yunlong Lin", "Hui Zheng", "Ge Meng", "Zixu Lin", "Jiaxin Xie", "Junbin Lu", "Yue Huang", "Xinghao Ding"], "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables", "categories": ["cs.CV"], "comment": "12 pages, 6 figures", "summary": "Recently, deep learning-based pan-sharpening algorithms have achieved notable\nadvancements over traditional methods. However, many deep learning-based\napproaches incur substantial computational overhead during inference,\nespecially with high-resolution images. This excessive computational demand\nlimits the applicability of these methods in real-world scenarios, particularly\nin the absence of dedicated computing devices such as GPUs and TPUs. To address\nthese challenges, we propose Pan-LUT, a novel learnable look-up table (LUT)\nframework for pan-sharpening that strikes a balance between performance and\ncomputational efficiency for high-resolution remote sensing images. To finely\ncontrol the spectral transformation, we devise the PAN-guided look-up table\n(PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained\nspatial details and adaptively learn local contexts, we introduce the spatial\ndetails look-up table (SDLUT) and adaptive aggregation look-up table (AALUT).\nOur proposed method contains fewer than 300K parameters and processes a 8K\nresolution image in under 1 ms using a single NVIDIA GeForce RTX 2080 Ti GPU,\ndemonstrating significantly faster performance compared to other methods.\nExperiments reveal that Pan-LUT efficiently processes large remote sensing\nimages in a lightweight manner, bridging the gap to real-world applications.\nFurthermore, our model surpasses SOTA methods in full-resolution scenes under\nreal-world conditions, highlighting its effectiveness and efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23796", "pdf": "https://arxiv.org/pdf/2503.23796", "abs": "https://arxiv.org/abs/2503.23796", "authors": ["Bosung Kim", "Kyuhwan Lee", "Isu Jeong", "Jungmin Cheon", "Yeojin Lee", "Seulki Lee"], "title": "On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices", "categories": ["cs.CV"], "comment": null, "summary": "We present On-device Sora, the first model training-free solution for\ndiffusion-based on-device text-to-video generation that operates efficiently on\nsmartphone-grade devices. To address the challenges of diffusion-based\ntext-to-video generation on computation- and memory-limited mobile devices, the\nproposed On-device Sora applies three novel techniques to pre-trained video\ngenerative models. First, Linear Proportional Leap (LPL) reduces the excessive\ndenoising steps required in video diffusion through an efficient leap-based\napproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive\ntoken-processing computation in attention layers by merging consecutive tokens\nalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading\n(CI-DL) dynamically partitions large models into smaller blocks and loads them\ninto memory for concurrent model inference, effectively addressing the\nchallenges of limited device memory. We implement On-device Sora on the iPhone\n15 Pro, and the experimental evaluations show that it is capable of generating\nhigh-quality videos on the device, comparable to those produced by high-end\nGPUs. These results show that On-device Sora enables efficient and high-quality\nvideo generation on resource-constrained mobile devices. We envision the\nproposed On-device Sora as a significant first step toward democratizing\nstate-of-the-art generative technologies, enabling video generation on\ncommodity mobile and embedded devices without resource-intensive re-training\nfor model optimization (compression). The code implementation is available at a\nGitHub repository(https://github.com/eai-lab/On-device-Sora).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23806", "pdf": "https://arxiv.org/pdf/2503.23806", "abs": "https://arxiv.org/abs/2503.23806", "authors": ["Xiaoqing Guo", "Wuyang Li", "Yixuan Yuan"], "title": "Bridge the Gap Between Visual and Linguistic Comprehension for Generalized Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Generalized zero-shot semantic segmentation (GZS3) aims to achieve the\nhuman-level capability of segmenting not only seen classes but also novel class\nregions unseen in the training data through introducing the bridge of semantic\nrepresentations, e.g., word vector. While effective, the way of utilizing one\nsemantic representation to associate the corresponding class and to enable the\nknowledge transfer from seen to unseen classes is insufficient as well as\nincompatible with human cognition. Inspired by the observation that humans\noften use some `part' and `state' information to comprehend the seen objects\nand imagine unseen classes, we decouple each class into detailed descriptions,\nincluding object parts and states. Based on the decoupling formulation, we\npropose a Decoupled Vision-Language Matching (DeVLMatch) framework, composed of\nspatial-part (SPMatch) and channel-state (CSMatch) matching modules, for GZS3.\nIn SPMatch, we comprehend objects with spatial part information from both\nvisual and linguistic perspectives and perform graph matching to bridge the\ngap. In CSMatch, states of objects from the linguistic perspective are matched\nto compatible channel information from the visual perspective. By decoupling\nand matching objects across visual and linguistic comprehension, we can\nexplicitly introspect the relationship between seen and unseen classes in\nfine-grained object part and state levels, thereby facilitating the knowledge\ntransfer from seen to unseen classes in visual space. The proposed DeVLMatch\nframework surpasses the previous GZS3 methods on standard benchmarks, including\nPASCAL VOC, COCO-Stuff, and CATARACTS, demonstrating its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23881", "pdf": "https://arxiv.org/pdf/2503.23881", "abs": "https://arxiv.org/abs/2503.23881", "authors": ["Tianyi Gong", "Boyan Li", "Yifei Zhong", "Fangxin Wang"], "title": "ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "The increasing demand for augmented and virtual reality applications has\nhighlighted the importance of crafting immersive 3D scenes from a simple\nsingle-view image. However, due to the partial priors provided by single-view\ninput, existing methods are often limited to reconstruct low-consistency 3D\nscenes with narrow fields of view from single-view input. These limitations\nmake them less capable of generalizing to reconstruct immersive scenes. To\naddress this problem, we propose ExScene, a two-stage pipeline to reconstruct\nan immersive 3D scene from any given single-view image. ExScene designs a novel\nmultimodal diffusion model to generate a high-fidelity and globally consistent\npanoramic image. We then develop a panoramic depth estimation approach to\ncalculate geometric information from panorama, and we combine geometric\ninformation with high-fidelity panoramic image to train an initial 3D Gaussian\nSplatting (3DGS) model. Following this, we introduce a GS refinement technique\nwith 2D stable video diffusion priors. We add camera trajectory consistency and\ncolor-geometric priors into the denoising process of diffusion to improve color\nand spatial consistency across image sequences. These refined sequences are\nthen used to fine-tune the initial 3DGS model, leading to better reconstruction\nquality. Experimental results demonstrate that our ExScene achieves consistent\nand immersive scene reconstruction using only single-view input, significantly\nsurpassing state-of-the-art baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23882", "pdf": "https://arxiv.org/pdf/2503.23882", "abs": "https://arxiv.org/abs/2503.23882", "authors": ["Halil İbrahim Öztürk", "Muhammet Esat Kalfaoğlu", "Ozsel Kilinc"], "title": "GLane3D : Detecting Lanes with Graph of 3D Keypoints", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Accurate and efficient lane detection in 3D space is essential for autonomous\ndriving systems, where robust generalization is the foremost requirement for 3D\nlane detection algorithms. Considering the extensive variation in lane\nstructures worldwide, achieving high generalization capacity is particularly\nchallenging, as algorithms must accurately identify a wide variety of lane\npatterns worldwide. Traditional top-down approaches rely heavily on learning\nlane characteristics from training datasets, often struggling with lanes\nexhibiting previously unseen attributes. To address this generalization\nlimitation, we propose a method that detects keypoints of lanes and\nsubsequently predicts sequential connections between them to construct complete\n3D lanes. Each key point is essential for maintaining lane continuity, and we\npredict multiple proposals per keypoint by allowing adjacent grids to predict\nthe same keypoint using an offset mechanism. PointNMS is employed to eliminate\noverlapping proposal keypoints, reducing redundancy in the estimated BEV graph\nand minimizing computational overhead from connection estimations. Our model\nsurpasses previous state-of-the-art methods on both the Apollo and OpenLane\ndatasets, demonstrating superior F1 scores and a strong generalization capacity\nwhen models trained on OpenLane are evaluated on the Apollo dataset, compared\nto prior approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23888", "pdf": "https://arxiv.org/pdf/2503.23888", "abs": "https://arxiv.org/abs/2503.23888", "authors": ["Xin Zhang", "Siting Huang", "Xiangyang Luo", "Yifan Xie", "Weijiang Yu", "Heng Chang", "Fei Ma", "Fei Yu"], "title": "MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 5 figures,IEEE International Conference on Multimedia & Expo\n  2025", "summary": "Face editing modifies the appearance of face, which plays a key role in\ncustomization and enhancement of personal images. Although much work have\nachieved remarkable success in text-driven face editing, they still face\nsignificant challenges as none of them simultaneously fulfill the\ncharacteristics of diversity, controllability and flexibility. To address this\nchallenge, we propose MuseFace, a text-driven face editing framework, which\nrelies solely on text prompt to enable face editing. Specifically, MuseFace\nintegrates a Text-to-Mask diffusion model and a semantic-aware face editing\nmodel, capable of directly generating fine-grained semantic masks from text and\nperforming face editing. The Text-to-Mask diffusion model provides\n\\textit{diversity} and \\textit{flexibility} to the framework, while the\nsemantic-aware face editing model ensures \\textit{controllability} of the\nframework. Our framework can create fine-grained semantic masks, making precise\nface editing possible, and significantly enhancing the controllability and\nflexibility of face editing models. Extensive experiments demonstrate that\nMuseFace achieves superior high-fidelity performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23897", "pdf": "https://arxiv.org/pdf/2503.23897", "abs": "https://arxiv.org/abs/2503.23897", "authors": ["Yufei Wang", "Lanqing Guo", "Zhihao Li", "Jiaxing Huang", "Pichao Wang", "Bihan Wen", "Jian Wang"], "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23905", "pdf": "https://arxiv.org/pdf/2503.23905", "abs": "https://arxiv.org/abs/2503.23905", "authors": ["Qihan Huang", "Long Chan", "Jinlong Liu", "Wanggui He", "Hao Jiang", "Mingli Song", "Jingyuan Chen", "Chang Yao", "Jie Song"], "title": "Boosting MLLM Reasoning with Text-Debiased Hint-GRPO", "categories": ["cs.CV"], "comment": null, "summary": "MLLM reasoning has drawn widespread research for its excellent\nproblem-solving capability. Current reasoning methods fall into two types: PRM,\nwhich supervises the intermediate reasoning steps, and ORM, which supervises\nthe final results. Recently, DeepSeek-R1 has challenged the traditional view\nthat PRM outperforms ORM, which demonstrates strong generalization performance\nusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms still\nstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,\nmathematical reasoning). In this work, we reveal two problems that impede the\nperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low data\nutilization refers to that GRPO cannot acquire positive rewards to update the\nMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypasses\nimage condition and solely relies on text condition for generation after GRPO\ntraining. To tackle these problems, this work proposes Hint-GRPO that improves\ndata utilization by adaptively providing hints for samples of varying\ndifficulty, and text-bias calibration that mitigates text-bias by calibrating\nthe token prediction logits with image condition in test-time. Experiment\nresults on three base MLLMs across eleven datasets demonstrate that our\nproposed methods advance the reasoning capability of original MLLM by a large\nmargin, exhibiting superior performance to existing MLLM reasoning methods. Our\ncode is available at https://github.com/hqhQAQ/Hint-GRPO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23956", "pdf": "https://arxiv.org/pdf/2503.23956", "abs": "https://arxiv.org/abs/2503.23956", "authors": ["Kai Huang", "Hao Zou", "Bochen Wang", "Ye Xi", "Zhen Xie", "Hao Wang"], "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23958", "pdf": "https://arxiv.org/pdf/2503.23958", "abs": "https://arxiv.org/abs/2503.23958", "authors": ["Nima Torbati", "Anastasia Meshcheryakova", "Diana Mechtcheriakova", "Amirreza Mahbod"], "title": "A Multi-Stage Auto-Context Deep Learning Framework for Tissue and Nuclei Segmentation and Classification in H&E-Stained Histological Images of Advanced Melanoma", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Melanoma is the most lethal form of skin cancer, with an increasing incidence\nrate worldwide. Analyzing histological images of melanoma by localizing and\nclassifying tissues and cell nuclei is considered the gold standard method for\ndiagnosis and treatment options for patients. While many computerized\napproaches have been proposed for automatic analysis, most perform tissue-based\nanalysis and nuclei (cell)-based analysis as separate tasks, which might be\nsuboptimal.\n  In this work, using the PUMA challenge dataset, we proposed a novel\nmulti-stage deep learning approach by combining tissue and nuclei information\nin a unified framework based on the auto-context concept to perform\nsegmentation and classification in histological images of melanoma. Through\npre-training and further post-processing, our approach achieved second and\nfirst place rankings in the PUMA challenge, with average micro Dice tissue\nscore and summed nuclei F1-score of 73.40% for Track 1 and 63.48% for Track 2,\nrespectively. Our implementation for training and testing is available at:\nhttps://github.com/NimaTorbati/PumaSubmit", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23965", "pdf": "https://arxiv.org/pdf/2503.23965", "abs": "https://arxiv.org/abs/2503.23965", "authors": ["Miao Fan", "Xuxu Kong", "Shengtong Xu", "Haoyi Xiong", "Xiangzeng Liu"], "title": "Video-based Traffic Light Recognition by Rockchip RV1126 for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by IEEE IV'25", "summary": "Real-time traffic light recognition is fundamental for autonomous driving\nsafety and navigation in urban environments. While existing approaches rely on\nsingle-frame analysis from onboard cameras, they struggle with complex\nscenarios involving occlusions and adverse lighting conditions. We present\n\\textit{ViTLR}, a novel video-based end-to-end neural network that processes\nmultiple consecutive frames to achieve robust traffic light detection and state\nclassification. The architecture leverages a transformer-like design with\nconvolutional self-attention modules, which is optimized specifically for\ndeployment on the Rockchip RV1126 embedded platform. Extensive evaluations on\ntwo real-world datasets demonstrate that \\textit{ViTLR} achieves\nstate-of-the-art performance while maintaining real-time processing\ncapabilities (>25 FPS) on RV1126's NPU. The system shows superior robustness\nacross temporal stability, varying target distances, and challenging\nenvironmental conditions compared to existing single-frame approaches. We have\nsuccessfully integrated \\textit{ViTLR} into an ego-lane traffic light\nrecognition system using HD maps for autonomous driving applications. The\ncomplete implementation, including source code and datasets, is made publicly\navailable to facilitate further research in this domain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24057", "pdf": "https://arxiv.org/pdf/2503.24057", "abs": "https://arxiv.org/abs/2503.24057", "authors": ["Xuxiong Liu", "Tengteng Dong", "Fei Wang", "Weijie Feng", "Xiao Sun"], "title": "AMMSM: Adaptive Motion Magnification and Sparse Mamba for Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Micro-expressions are typically regarded as unconscious manifestations of a\nperson's genuine emotions. However, their short duration and subtle signals\npose significant challenges for downstream recognition. We propose a multi-task\nlearning framework named the Adaptive Motion Magnification and Sparse Mamba\n(AMMSM) to address this. This framework aims to enhance the accurate capture of\nmicro-expressions through self-supervised subtle motion magnification, while\nthe sparse spatial selection Mamba architecture combines sparse activation with\nthe advanced Visual Mamba model to model key motion regions and their valuable\nrepresentations more effectively. Additionally, we employ evolutionary search\nto optimize the magnification factor and the sparsity ratios of spatial\nselection, followed by fine-tuning to improve performance further. Extensive\nexperiments on two standard datasets demonstrate that the proposed AMMSM\nachieves state-of-the-art (SOTA) accuracy and robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24088", "pdf": "https://arxiv.org/pdf/2503.24088", "abs": "https://arxiv.org/abs/2503.24088", "authors": ["Lars Möllenbrok", "Behnood Rasti", "Begüm Demir"], "title": "A Plasticity-Aware Method for Continual Self-Supervised Learning in Remote Sensing", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2025", "summary": "Continual self-supervised learning (CSSL) methods have gained increasing\nattention in remote sensing (RS) due to their capability to learn new tasks\nsequentially from continuous streams of unlabeled data.\n  Existing CSSL methods, while learning new tasks, focus on preventing\ncatastrophic forgetting. To this end, most of them use regularization\nstrategies to retain knowledge of previous tasks. This reduces the model's\nability to adapt to the data of new tasks (i.e., learning plasticity), which\ncan degrade performance. To address this problem, in this paper, we propose a\nnovel CSSL method that aims to learn tasks sequentially, while achieving high\nlearning plasticity. To this end, the proposed method uses a knowledge\ndistillation strategy with an integrated decoupling mechanism. The decoupling\nis achieved by first dividing the feature dimensions into task-common and\ntask-specific parts. Then, the task-common features are forced to be correlated\nto ensure memory stability while the task-specific features are forced to be\nde-correlated facilitating the learning of new features. Experimental results\nshow the effectiveness of the proposed method compared to CaSSLe, which is a\nwidely used CSSL framework, with improvements of up to 1.12% in average\naccuracy and 2.33% in intransigence in a task-incremental scenario, and 1.24%\nin average accuracy and 2.01% in intransigence in a class-incremental scenario.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24096", "pdf": "https://arxiv.org/pdf/2503.24096", "abs": "https://arxiv.org/abs/2503.24096", "authors": ["Adrienne Deganutti", "Simon Hadfield", "Andrew Gilbert"], "title": "DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description", "categories": ["cs.CV"], "comment": null, "summary": "Audio Description is a narrated commentary designed to aid vision-impaired\naudiences in perceiving key visual elements in a video. While short-form video\nunderstanding has advanced rapidly, a solution for maintaining coherent\nlong-term visual storytelling remains unresolved. Existing methods rely solely\non frame-level embeddings, effectively describing object-based content but\nlacking contextual information across scenes. We introduce DANTE-AD, an\nenhanced video description model leveraging a dual-vision Transformer-based\narchitecture to address this gap. DANTE-AD sequentially fuses both frame and\nscene level embeddings to improve long-term contextual understanding. We\npropose a novel, state-of-the-art method for sequential cross-attention to\nachieve contextual grounding for fine-grained audio description generation.\nEvaluated on a broad range of key scenes from well-known movie clips, DANTE-AD\noutperforms existing methods across traditional NLP metrics and LLM-based\nevaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24210", "pdf": "https://arxiv.org/pdf/2503.24210", "abs": "https://arxiv.org/abs/2503.24210", "authors": ["Seungjun Lee", "Gim Hee Lee"], "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "CVPR 2025. Project Page: https://diet-gs.github.io", "summary": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24258", "pdf": "https://arxiv.org/pdf/2503.24258", "abs": "https://arxiv.org/abs/2503.24258", "authors": ["Lorenzo Tronchin", "Tommy Löfstedt", "Paolo Soda", "Valerio Guarrasi"], "title": "Beyond a Single Mode: GAN Ensembles for Diverse Medical Data Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advancement of generative AI, particularly in medical imaging, confronts\nthe trilemma of ensuring high fidelity, diversity, and efficiency in synthetic\ndata generation. While Generative Adversarial Networks (GANs) have shown\npromise across various applications, they still face challenges like mode\ncollapse and insufficient coverage of real data distributions. This work\nexplores the use of GAN ensembles to overcome these limitations, specifically\nin the context of medical imaging. By solving a multi-objective optimisation\nproblem that balances fidelity and diversity, we propose a method for selecting\nan optimal ensemble of GANs tailored for medical data. The selected ensemble is\ncapable of generating diverse synthetic medical images that are representative\nof true data distributions and computationally efficient. Each model in the\nensemble brings a unique contribution, ensuring minimal redundancy. We\nconducted a comprehensive evaluation using three distinct medical datasets,\ntesting 22 different GAN architectures with various loss functions and\nregularisation techniques. By sampling models at different training epochs, we\ncrafted 110 unique configurations. The results highlight the capability of GAN\nensembles to enhance the quality and utility of synthetic medical images,\nthereby improving the efficacy of downstream tasks such as diagnostic\nmodelling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24270", "pdf": "https://arxiv.org/pdf/2503.24270", "abs": "https://arxiv.org/abs/2503.24270", "authors": ["Yuelei Li", "Hyunjin Kim", "Fangneng Zhan", "Ri-Zhao Qiu", "Mazeyu Ji", "Xiaojun Shan", "Xueyan Zou", "Paul Liang", "Hanspeter Pfister", "Xiaolong Wang"], "title": "Visual Acoustic Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24320", "pdf": "https://arxiv.org/pdf/2503.24320", "abs": "https://arxiv.org/abs/2503.24320", "authors": ["Wenyan Cong", "Hanqing Zhu", "Peihao Wang", "Bangya Liu", "Dejia Xu", "Kevin Wang", "David Z. Pan", "Yan Wang", "Zhiwen Fan", "Zhangyang Wang"], "title": "Can Test-Time Scaling Improve World Foundation Model?", "categories": ["cs.CV"], "comment": null, "summary": "World foundation models, which simulate the physical world by predicting\nfuture states from current observations and inputs, have become central to many\napplications in physical intelligence, including autonomous driving and\nrobotics. However, these models require substantial computational resources for\npretraining and are further constrained by available data during post-training.\nAs such, scaling computation at test time emerges as both a critical and\npractical alternative to traditional model enlargement or re-training. In this\nwork, we introduce SWIFT, a test-time scaling framework tailored for WFMs.\nSWIFT integrates our extensible WFM evaluation toolkit with process-level\ninference strategies, including fast tokenization, probability-based Top-K\npruning, and efficient beam search. Empirical results on the COSMOS model\ndemonstrate that test-time scaling exists even in a compute-optimal way. Our\nfindings reveal that test-time scaling laws hold for WFMs and that SWIFT\nprovides a scalable and effective pathway for improving WFM inference without\nretraining or increasing model size. The code is available at\nhttps://github.com/Mia-Cong/SWIFT.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling", "beam search"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24368", "pdf": "https://arxiv.org/pdf/2503.24368", "abs": "https://arxiv.org/abs/2503.24368", "authors": ["Xiaoran Zhang", "Eric Z. Chen", "Lin Zhao", "Xiao Chen", "Yikang Liu", "Boris Maihe", "James S. Duncan", "Terrence Chen", "Shanhui Sun"], "title": "Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel approach that adapts hierarchical vision foundation models\nfor real-time ultrasound image segmentation. Existing ultrasound segmentation\nmethods often struggle with adaptability to new tasks, relying on costly manual\nannotations, while real-time approaches generally fail to match\nstate-of-the-art performance. To overcome these limitations, we introduce an\nadaptive framework that leverages the vision foundation model Hiera to extract\nmulti-scale features, interleaved with DINOv2 representations to enhance visual\nexpressiveness. These enriched features are then decoded to produce precise and\nrobust segmentation. We conduct extensive evaluations on six public datasets\nand one in-house dataset, covering both cardiac and thyroid ultrasound\nsegmentation. Experiments show that our approach outperforms state-of-the-art\nmethods across multiple datasets and excels with limited supervision,\nsurpassing nnUNet by over 20\\% on average in the 1\\% and 10\\% data settings.\nOur method achieves $\\sim$77 FPS inference speed with TensorRT on a single GPU,\nenabling real-time clinical applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24379", "pdf": "https://arxiv.org/pdf/2503.24379", "abs": "https://arxiv.org/abs/2503.24379", "authors": ["Shengqiong Wu", "Weicai Ye", "Jiahao Wang", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Shuicheng Yan", "Hao Fei", "Tat-Seng Chua"], "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://sqwu.top/Any2Cap/", "summary": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24382", "pdf": "https://arxiv.org/pdf/2503.24382", "abs": "https://arxiv.org/abs/2503.24382", "authors": ["Chong Bao", "Xiyu Zhang", "Zehao Yu", "Jiale Shi", "Guofeng Zhang", "Songyou Peng", "Zhaopeng Cui"], "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://zju3dv.github.io/free360/", "summary": "Neural rendering has demonstrated remarkable success in high-quality 3D\nneural reconstruction and novel view synthesis with dense input views and\naccurate poses. However, applying it to extremely sparse, unposed views in\nunbounded 360{\\deg} scenes remains a challenging problem. In this paper, we\npropose a novel neural rendering framework to accomplish the unposed and\nextremely sparse-view 3D reconstruction in unbounded 360{\\deg} scenes. To\nresolve the spatial ambiguity inherent in unbounded scenes with sparse input\nviews, we propose a layered Gaussian-based representation to effectively model\nthe scene with distinct spatial layers. By employing a dense stereo\nreconstruction model to recover coarse geometry, we introduce a layer-specific\nbootstrap optimization to refine the noise and fill occluded regions in the\nreconstruction. Furthermore, we propose an iterative fusion of reconstruction\nand generation alongside an uncertainty-aware training approach to facilitate\nmutual conditioning and enhancement between these two processes. Comprehensive\nexperiments show that our approach outperforms existing state-of-the-art\nmethods in terms of rendering quality and surface reconstruction accuracy.\nProject page: https://zju3dv.github.io/free360/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22728", "pdf": "https://arxiv.org/pdf/2503.22728", "abs": "https://arxiv.org/abs/2503.22728", "authors": ["Ao Fu", "Ziqi Ni", "Yi Zhou"], "title": "Dual Audio-Centric Modality Coupling for Talking Head Generation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "9 pages, 4 figures", "summary": "The generation of audio-driven talking head videos is a key challenge in\ncomputer vision and graphics, with applications in virtual avatars and digital\nmedia. Traditional approaches often struggle with capturing the complex\ninteraction between audio and facial dynamics, leading to lip synchronization\nand visual quality issues. In this paper, we propose a novel NeRF-based\nframework, Dual Audio-Centric Modality Coupling (DAMC), which effectively\nintegrates content and dynamic features from audio inputs. By leveraging a dual\nencoder structure, DAMC captures semantic content through the Content-Aware\nEncoder and ensures precise visual synchronization through the Dynamic-Sync\nEncoder. These features are fused using a Cross-Synchronized Fusion Module\n(CSFM), enhancing content representation and lip synchronization. Extensive\nexperiments show that our method outperforms existing state-of-the-art\napproaches in key metrics such as lip synchronization accuracy and image\nquality, demonstrating robust generalization across various audio inputs,\nincluding synthetic speech from text-to-speech (TTS) systems. Our results\nprovide a promising solution for high-quality, audio-driven talking head\ngeneration and present a scalable approach for creating realistic talking\nheads.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22729", "pdf": "https://arxiv.org/pdf/2503.22729", "abs": "https://arxiv.org/abs/2503.22729", "authors": ["Jiahao Qin", "Feng Liu", "Lu Zong"], "title": "Ancestral Mamba: Enhancing Selective Discriminant Space Model with Online Visual Prototype Learning for Efficient and Robust Discriminant Approach", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "In the realm of computer graphics, the ability to learn continuously from\nnon-stationary data streams while adapting to new visual patterns and\nmitigating catastrophic forgetting is of paramount importance. Existing\napproaches often struggle to capture and represent the essential\ncharacteristics of evolving visual concepts, hindering their applicability to\ndynamic graphics tasks. In this paper, we propose Ancestral Mamba, a novel\napproach that integrates online prototype learning into a selective\ndiscriminant space model for efficient and robust online continual learning.\nThe key components of our approach include Ancestral Prototype Adaptation\n(APA), which continuously refines and builds upon learned visual prototypes,\nand Mamba Feedback (MF), which provides targeted feedback to adapt to\nchallenging visual patterns. APA enables the model to continuously adapt its\nprototypes, building upon ancestral knowledge to tackle new challenges, while\nMF acts as a targeted feedback mechanism, focusing on challenging classes and\nrefining their representations. Extensive experiments on graphics-oriented\ndatasets, such as CIFAR-10 and CIFAR-100, demonstrate the superior performance\nof Ancestral Mamba compared to state-of-the-art baselines, achieving\nsignificant improvements in accuracy and forgetting mitigation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22742", "pdf": "https://arxiv.org/pdf/2503.22742", "abs": "https://arxiv.org/abs/2503.22742", "authors": ["William Claster", "Suhas KM", "Dhairya Gundechia"], "title": "Adaptive Integrated Layered Attention (AILA)", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.NE"], "comment": null, "summary": "We propose Adaptive Integrated Layered Attention (AILA), a neural network\narchitecture that combines dense skip connections with different mechanisms for\nadaptive feature reuse across network layers. We evaluate AILA on three\nchallenging tasks: price forecasting for various commodities and indices (S&P\n500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the\nCIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In\nall cases, AILA matches strong deep learning baselines (LSTMs, Transformers,\nand ResNets), achieving it at a fraction of the training and inference time.\nNotably, we implement and test two versions of the model - AILA-Architecture 1,\nwhich uses simple linear layers as the connection mechanism between layers, and\nAILA-Architecture 2, which implements an attention mechanism to selectively\nfocus on outputs from previous layers. Both architectures are applied in a\nsingle-task learning setting, with each model trained separately for individual\ntasks. Results confirm that AILA's adaptive inter-layer connections yield\nrobust gains by flexibly reusing pertinent features at multiple network depths.\nThe AILA approach thus presents an extension to existing architectures,\nimproving long-range sequence modeling, image recognition with optimised\ncomputational speed, and SOTA classification performance in practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22829", "pdf": "https://arxiv.org/pdf/2503.22829", "abs": "https://arxiv.org/abs/2503.22829", "authors": ["Zhen Lin", "Hongyu Yuan", "Richard Barcus", "Qing Lyu", "Sucheta Chakravarty", "Megan E. Lipford", "Carol A. Shively", "Suzanne Craft", "Mohammad Kawas", "Jeongchul Kim", "Christopher T. Whitlow"], "title": "Nonhuman Primate Brain Tissue Segmentation Using a Transfer Learning Approach", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Non-human primates (NHPs) serve as critical models for understanding human\nbrain function and neurological disorders due to their close evolutionary\nrelationship with humans. Accurate brain tissue segmentation in NHPs is\ncritical for understanding neurological disorders, but challenging due to the\nscarcity of annotated NHP brain MRI datasets, the small size of the NHP brain,\nthe limited resolution of available imaging data and the anatomical differences\nbetween human and NHP brains. To address these challenges, we propose a novel\napproach utilizing STU-Net with transfer learning to leverage knowledge\ntransferred from human brain MRI data to enhance segmen-tation accuracy in the\nNHP brain MRI, particularly when training data is limited.The combination of\nSTU-Net and transfer learning effectively delineates complex tissue boundaries\nand captures fine anatomical details specific to NHP brains. Notably, our\nmethod demonstrated improvement in segmenting small subcortical structures such\nas putamen and thalamus that are challenging to resolve with limited spatial\nresolution and tissue contrast, and achieved DSC of over 0.88, IoU over 0.8 and\nHD95 under 7. This study introduces a robust method for multi-class brain\ntissue segmentation in NHPs, potentially accelerating research in evolutionary\nneuroscience and preclinical studies of neurological disorders relevant to\nhuman health.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22876", "pdf": "https://arxiv.org/pdf/2503.22876", "abs": "https://arxiv.org/abs/2503.22876", "authors": ["Kushagra Srivastava", "Rutwik Kulkarni", "Manoj Velmurugan", "Nitin J. Sanket"], "title": "VizFlyt: Perception-centric Pedagogical Framework For Autonomous Aerial Robots", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025. Projected Page:\n  https://pear.wpi.edu/research/vizflyt.html", "summary": "Autonomous aerial robots are becoming commonplace in our lives. Hands-on\naerial robotics courses are pivotal in training the next-generation workforce\nto meet the growing market demands. Such an efficient and compelling course\ndepends on a reliable testbed. In this paper, we present \\textit{VizFlyt}, an\nopen-source perception-centric Hardware-In-The-Loop (HITL) photorealistic\ntesting framework for aerial robotics courses. We utilize pose from an external\nlocalization system to hallucinate real-time and photorealistic visual sensors\nusing 3D Gaussian Splatting. This enables stress-free testing of autonomy\nalgorithms on aerial robots without the risk of crashing into obstacles. We\nachieve over 100Hz of system update rate. Lastly, we build upon our past\nexperiences of offering hands-on aerial robotics courses and propose a new\nopen-source and open-hardware curriculum based on \\textit{VizFlyt} for the\nfuture. We test our framework on various course projects in real-world HITL\nexperiments and present the results showing the efficacy of such a system and\nits large potential use cases. Code, datasets, hardware guides and demo videos\nare available at https://pear.wpi.edu/research/vizflyt.html", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22943", "pdf": "https://arxiv.org/pdf/2503.22943", "abs": "https://arxiv.org/abs/2503.22943", "authors": ["Haoyang Wang", "Ruishan Guo", "Pengtao Ma", "Ciyu Ruan", "Xinyu Luo", "Wenhua Ding", "Tianyang Zhong", "Jingao Xu", "Yunhao Liu", "Xinlei Chen"], "title": "Towards Mobile Sensing with Event Cameras on High-mobility Resource-constrained Devices: A Survey", "categories": ["cs.RO", "cs.CV"], "comment": "32 pages, 9 figures", "summary": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high mobility. This shift imposes new demands on mobile\nsensing, particularly in terms of achieving high accuracy and low latency.\nEvent-based vision has emerged as a disruptive paradigm, offering high temporal\nresolution, low latency, and energy efficiency, making it well-suited for\nhigh-accuracy and low-latency sensing tasks on high-mobility platforms.\nHowever, the presence of substantial noisy events, the lack of inherent\nsemantic information, and the large data volume pose significant challenges for\nevent-based data processing on resource-constrained mobile devices. This paper\nsurveys the literature over the period 2014-2024, provides a comprehensive\noverview of event-based mobile sensing systems, covering fundamental\nprinciples, event abstraction methods, algorithmic advancements, hardware and\nsoftware acceleration strategies. We also discuss key applications of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow estimation, and 3D reconstruction, while highlighting the challenges\nassociated with event data processing, sensor fusion, and real-time deployment.\nFurthermore, we outline future research directions, such as improving event\ncamera hardware with advanced optics, leveraging neuromorphic computing for\nefficient processing, and integrating bio-inspired algorithms to enhance\nperception. To support ongoing research, we provide an open-source\n\\textit{Online Sheet} with curated resources and recent developments. We hope\nthis survey serves as a valuable reference, facilitating the adoption of\nevent-based vision across diverse applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23050", "pdf": "https://arxiv.org/pdf/2503.23050", "abs": "https://arxiv.org/abs/2503.23050", "authors": ["Tiago Almeida", "Plinio Moreno", "Catarina Barata"], "title": "Prediction of 30-day hospital readmission with clinical notes and EHR information", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "High hospital readmission rates are associated with significant costs and\nhealth risks for patients. Therefore, it is critical to develop predictive\nmodels that can support clinicians to determine whether or not a patient will\nreturn to the hospital in a relatively short period of time (e.g, 30-days).\nNowadays, it is possible to collect both structured (electronic health records\n- EHR) and unstructured information (clinical notes) about a patient hospital\nevent, all potentially containing relevant information for a predictive model.\nHowever, their integration is challenging. In this work we explore the\ncombination of clinical notes and EHRs to predict 30-day hospital readmissions.\nWe address the representation of the various types of information available in\nthe EHR data, as well as exploring LLMs to characterize the clinical notes. We\ncollect both information sources as the nodes of a graph neural network (GNN).\nOur model achieves an AUROC of 0.72 and a balanced accuracy of 66.7\\%,\nhighlighting the importance of combining the multimodal information.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23179", "pdf": "https://arxiv.org/pdf/2503.23179", "abs": "https://arxiv.org/abs/2503.23179", "authors": ["Wiebke Heyer", "Yannic Elser", "Lennart Berkel", "Xinrui Song", "Xuanang Xu", "Pingkun Yan", "Xi Jia", "Zi Li", "Tony C. W. Mok", "BoWen LI", "Christian Staackmann", "Christoph Großbröhmer", "Alessa Hering", "Malte M. Sieren", "Mattias P. Heinrich"], "title": "OncoReg: Medical Image Registration for Oncological Challenges", "categories": ["eess.IV", "cs.CV"], "comment": "26 pages, 6 figures", "summary": "In modern cancer research, the vast volume of medical data generated is often\nunderutilised due to challenges related to patient privacy. The OncoReg\nChallenge addresses this issue by enabling researchers to develop and validate\nimage registration methods through a two-phase framework that ensures patient\nprivacy while fostering the development of more generalisable AI models. Phase\none involves working with a publicly available dataset, while phase two focuses\non training models on a private dataset within secure hospital networks.\nOncoReg builds upon the foundation established by the Learn2Reg Challenge by\nincorporating the registration of interventional cone-beam computed tomography\n(CBCT) with standard planning fan-beam CT (FBCT) images in radiotherapy.\nAccurate image registration is crucial in oncology, particularly for dynamic\ntreatment adjustments in image-guided radiotherapy, where precise alignment is\nnecessary to minimise radiation exposure to healthy tissues while effectively\ntargeting tumours. This work details the methodology and data behind the\nOncoReg Challenge and provides a comprehensive analysis of the competition\nentries and results. Findings reveal that feature extraction plays a pivotal\nrole in this registration task. A new method emerging from this challenge\ndemonstrated its versatility, while established approaches continue to perform\ncomparably to newer techniques. Both deep learning and classical approaches\nstill play significant roles in image registration, with the combination of\nmethods - particularly in feature extraction - proving most effective.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23219", "pdf": "https://arxiv.org/pdf/2503.23219", "abs": "https://arxiv.org/abs/2503.23219", "authors": ["Sanjoy Chowdhury", "Hanan Gani", "Nishit Anand", "Sayan Nag", "Ruohan Gao", "Mohamed Elhoseiny", "Salman Khan", "Dinesh Manocha"], "title": "Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in reasoning optimization have greatly enhanced the\nperformance of large language models (LLMs). However, existing work fails to\naddress the complexities of audio-visual scenarios, underscoring the need for\nfurther research. In this paper, we introduce AURELIA, a novel actor-critic\nbased audio-visual (AV) reasoning framework that distills structured,\nstep-by-step reasoning into AVLLMs at test time, improving their ability to\nprocess complex multi-modal inputs without additional training or fine-tuning.\nTo further advance AVLLM reasoning skills, we present AVReasonBench, a\nchallenging benchmark comprising 4500 audio-visual questions, each paired with\ndetailed step-by-step reasoning. Our benchmark spans six distinct tasks,\nincluding AV-GeoIQ, which evaluates AV reasoning combined with geographical and\ncultural knowledge. Evaluating 18 AVLLMs on AVReasonBench reveals significant\nlimitations in their multi-modal reasoning capabilities. Using AURELIA, we\nachieve up to a 100% relative improvement, demonstrating its effectiveness.\nThis performance gain highlights the potential of reasoning-enhanced data\ngeneration for advancing AVLLMs in real-world applications. Our code and data\nwill be publicly released at: https: //github.com/schowdhury671/aurelia.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23265", "pdf": "https://arxiv.org/pdf/2503.23265", "abs": "https://arxiv.org/abs/2503.23265", "authors": ["Björn Möller", "Lucas Görnhardt", "Tim Fingscheidt"], "title": "A Lightweight Image Super-Resolution Transformer Trained on Low-Resolution Images Only", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Transformer architectures prominently lead single-image super-resolution\n(SISR) benchmarks, reconstructing high-resolution (HR) images from their\nlow-resolution (LR) counterparts. Their strong representative power, however,\ncomes with a higher demand for training data compared to convolutional neural\nnetworks (CNNs). For many real-world SR applications, the availability of\nhigh-quality HR training images is not given, sparking interest in LR-only\ntraining methods. The LR-only SISR benchmark mimics this condition by allowing\nonly low-resolution (LR) images for model training. For a 4x super-resolution,\nthis effectively reduces the amount of available training data to 6.25% of the\nHR image pixels, which puts the employment of a data-hungry transformer model\ninto question. In this work, we are the first to utilize a lightweight vision\ntransformer model with LR-only training methods addressing the unsupervised\nSISR LR-only benchmark. We adopt and configure a recent LR-only training method\nfrom microscopy image super-resolution to macroscopic real-world data,\nresulting in our multi-scale training method for bicubic degradation (MSTbic).\nFurthermore, we compare it with reference methods and prove its effectiveness\nboth for a transformer and a CNN model. We evaluate on the classic SR benchmark\ndatasets Set5, Set14, BSD100, Urban100, and Manga109, and show superior\nperformance over state-of-the-art (so far: CNN-based) LR-only SISR methods. The\ncode is available on GitHub:\nhttps://github.com/ifnspaml/SuperResolutionMultiscaleTraining.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23515", "pdf": "https://arxiv.org/pdf/2503.23515", "abs": "https://arxiv.org/abs/2503.23515", "authors": ["Alice E. A. Allen", "Emily Shinkle", "Roxana Bujack", "Nicholas Lubbers"], "title": "Optimal Invariant Bases for Atomistic Machine Learning", "categories": ["physics.chem-ph", "cs.CV", "stat.ML"], "comment": null, "summary": "The representation of atomic configurations for machine learning models has\nled to the development of numerous descriptors, often to describe the local\nenvironment of atoms. However, many of these representations are incomplete\nand/or functionally dependent. Incomplete descriptor sets are unable to\nrepresent all meaningful changes in the atomic environment. Complete\nconstructions of atomic environment descriptors, on the other hand, often\nsuffer from a high degree of functional dependence, where some descriptors can\nbe written as functions of the others. These redundant descriptors do not\nprovide additional power to discriminate between different atomic environments\nand increase the computational burden. By employing techniques from the pattern\nrecognition literature to existing atomistic representations, we remove\ndescriptors that are functions of other descriptors to produce the smallest\npossible set that satisfies completeness. We apply this in two ways: first we\nrefine an existing description, the Atomistic Cluster Expansion. We show that\nthis yields a more efficient subset of descriptors. Second, we augment an\nincomplete construction based on a scalar neural network, yielding a new\nmessage-passing network architecture that can recognize up to 5-body patterns\nin each neuron by taking advantage of an optimal set of Cartesian tensor\ninvariants. This architecture shows strong accuracy on state-of-the-art\nbenchmarks while retaining low computational cost. Our results not only yield\nimproved models, but point the way to classes of invariant bases that minimize\ncost while maximizing expressivity for a host of applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23752", "pdf": "https://arxiv.org/pdf/2503.23752", "abs": "https://arxiv.org/abs/2503.23752", "authors": ["Jin Zhou", "Yi Zhou", "Pengfei Xu", "Hui Huang"], "title": "StrokeFusion: Vector Sketch Generation via Joint Stroke-UDF Encoding and Latent Sequence Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In the field of sketch generation, raster-format trained models often produce\nnon-stroke artifacts, while vector-format trained models typically lack a\nholistic understanding of sketches, leading to compromised recognizability.\nMoreover, existing methods struggle to extract common features from similar\nelements (e.g., eyes of animals) appearing at varying positions across\nsketches. To address these challenges, we propose StrokeFusion, a two-stage\nframework for vector sketch generation. It contains a dual-modal sketch feature\nlearning network that maps strokes into a high-quality latent space. This\nnetwork decomposes sketches into normalized strokes and jointly encodes stroke\nsequences with Unsigned Distance Function (UDF) maps, representing sketches as\nsets of stroke feature vectors. Building upon this representation, our\nframework exploits a stroke-level latent diffusion model that simultaneously\nadjusts stroke position, scale, and trajectory during generation. This enables\nhigh-fidelity sketch generation while supporting stroke interpolation editing.\nExtensive experiments on the QuickDraw dataset demonstrate that our framework\noutperforms state-of-the-art techniques, validating its effectiveness in\npreserving structural integrity and semantic features. Code and models will be\nmade publicly available upon publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23819", "pdf": "https://arxiv.org/pdf/2503.23819", "abs": "https://arxiv.org/abs/2503.23819", "authors": ["Swarnava Bhattacharyya", "Umapada Pal", "Tapabrata Chakraborti"], "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep learning based diagnostic AI systems based on medical images are\nstarting to provide similar performance as human experts. However these data\nhungry complex systems are inherently black boxes and therefore slow to be\nadopted for high risk applications like healthcare. This problem of lack of\ntransparency is exacerbated in the case of recent large foundation models,\nwhich are trained in a self supervised manner on millions of data points to\nprovide robust generalisation across a range of downstream tasks, but the\nembeddings generated from them happen through a process that is not\ninterpretable, and hence not easily trustable for clinical applications. To\naddress this timely issue, we deploy conformal analysis to quantify the\npredictive uncertainty of a vision transformer (ViT) based foundation model\nacross patient demographics with respect to sex, age and ethnicity for the\ntasks of skin lesion classification using several public benchmark datasets.\nThe significant advantage of this method is that conformal analysis is method\nindependent and it not only provides a coverage guarantee at population level\nbut also provides an uncertainty score for each individual. We used a\nmodel-agnostic dynamic F1-score-based sampling during model training, which\nhelped to stabilize the class imbalance and we investigate the effects on\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\nwe show how this can be used as a fairness metric to evaluate the robustness of\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\nadvance the trustworthiness and fairness of clinical AI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23877", "pdf": "https://arxiv.org/pdf/2503.23877", "abs": "https://arxiv.org/abs/2503.23877", "authors": ["Junyao Shi", "Zhuolun Zhao", "Tianyou Wang", "Ian Pedroza", "Amy Luo", "Jie Wang", "Jason Ma", "Dinesh Jayaraman"], "title": "ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "ICRA 2025. Project website: https://zeromimic.github.io/", "summary": "Many recent advances in robotic manipulation have come through imitation\nlearning, yet these rely largely on mimicking a particularly hard-to-acquire\nform of demonstrations: those collected on the same robot in the same room with\nthe same objects as the trained policy must handle at test time. In contrast,\nlarge pre-recorded human video datasets demonstrating manipulation skills\nin-the-wild already exist, which contain valuable information for robots. Is it\npossible to distill a repository of useful robotic skill policies out of such\ndata without any additional requirements on robot-specific demonstrations or\nexploration? We present the first such system ZeroMimic, that generates\nimmediately deployable image goal-conditioned skill policies for several common\ncategories of manipulation tasks (opening, closing, pouring, pick&place,\ncutting, and stirring) each capable of acting upon diverse objects and across\ndiverse unseen task setups. ZeroMimic is carefully designed to exploit recent\nadvances in semantic and geometric visual understanding of human videos,\ntogether with modern grasp affordance detectors and imitation policy classes.\nAfter training ZeroMimic on the popular EpicKitchens dataset of ego-centric\nhuman videos, we evaluate its out-of-the-box performance in varied real-world\nand simulated kitchen settings with two different robot embodiments,\ndemonstrating its impressive abilities to handle these varied tasks. To enable\nplug-and-play reuse of ZeroMimic policies on other task setups and robots, we\nrelease software and policy checkpoints of our skill policies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24009", "pdf": "https://arxiv.org/pdf/2503.24009", "abs": "https://arxiv.org/abs/2503.24009", "authors": ["Mikel Zhobro", "Andreas René Geist", "Georg Martius"], "title": "Learning 3D-Gaussian Simulators from RGB Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Learning physics simulations from video data requires maintaining spatial and\ntemporal consistency, a challenge often addressed with strong inductive biases\nor ground-truth 3D information -- limiting scalability and generalization. We\nintroduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end\nfrom multi-view RGB videos. It encodes images into a 3D Gaussian particle\nrepresentation, propagates dynamics via a transformer, and renders frames using\n3D Gaussian splatting. By jointly training inverse rendering with a dynamics\ntransformer using a temporal encoding and merging layer, 3DGSimembeds physical\nproperties into point-wise latent vectors without enforcing explicit\nconnectivity constraints. This enables the model to capture diverse physical\nbehaviors, from rigid to elastic and cloth-like interactions, along with\nrealistic lighting effects that also generalize to unseen multi-body\ninteractions and novel scene edits.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24160", "pdf": "https://arxiv.org/pdf/2503.24160", "abs": "https://arxiv.org/abs/2503.24160", "authors": ["Angela Lopez-Cardona", "Parvin Emami", "Sebastian Idesis", "Saravanakumar Duraisamy", "Luis A. Leiva", "Ioannis Arapakis"], "title": "A Comparative Study of Scanpath Models in Graph-Based Visualization", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Information Visualization (InfoVis) systems utilize visual representations to\nenhance data interpretation. Understanding how visual attention is allocated is\nessential for optimizing interface design. However, collecting Eye-tracking\n(ET) data presents challenges related to cost, privacy, and scalability.\nComputational models provide alternatives for predicting gaze patterns, thereby\nadvancing InfoVis research. In our study, we conducted an ET experiment with 40\nparticipants who analyzed graphs while responding to questions of varying\ncomplexity within the context of digital forensics. We compared human scanpaths\nwith synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.\nOur research evaluates the accuracy of these models and examines how question\ncomplexity and number of nodes influence performance. This work contributes to\nthe development of predictive modeling in visual analytics, offering insights\nthat can enhance the design and effectiveness of InfoVis systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
