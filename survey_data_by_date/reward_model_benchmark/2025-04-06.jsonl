{"id": "2504.02382", "pdf": "https://arxiv.org/pdf/2504.02382", "abs": "https://arxiv.org/abs/2504.02382", "authors": ["Yudi Sang", "Yanzhen Liu", "Sutuke Yibulayimu", "Yunning Wang", "Benjamin D. Killeen", "Mingxu Liu", "Ping-Cheng Ku", "Ole Johannsen", "Karol Gotkowski", "Maximilian Zenk", "Klaus Maier-Hein", "Fabian Isensee", "Peiyan Yue", "Yi Wang", "Haidong Yu", "Zhaohong Pan", "Yutong He", "Xiaokun Liang", "Daiqi Liu", "Fuxin Fan", "Artur Jurgas", "Andrzej Skalski", "Yuxi Ma", "Jing Yang", "Szymon Płotka", "Rafał Litka", "Gang Zhu", "Yingchun Song", "Mathias Unberath", "Mehran Armand", "Dan Ruan", "S. Kevin Zhou", "Qiyong Cao", "Chunpeng Zhao", "Xinbao Wu", "Yu Wang"], "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "PENGWIN 2024 Challenge Report", "summary": "The segmentation of pelvic fracture fragments in CT and X-ray images is\ncrucial for trauma diagnosis, surgical planning, and intraoperative guidance.\nHowever, accurately and efficiently delineating the bone fragments remains a\nsignificant challenge due to complex anatomy and imaging limitations. The\nPENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance\nautomated fracture segmentation by benchmarking state-of-the-art algorithms on\nthese complex tasks. A diverse dataset of 150 CT scans was collected from\nmultiple clinical centers, and a large set of simulated X-ray images was\ngenerated using the DeepDRR method. Final submissions from 16 teams worldwide\nwere evaluated under a rigorous multi-metric testing scheme. The top-performing\nCT algorithm achieved an average fragment-wise intersection over union (IoU) of\n0.930, demonstrating satisfactory accuracy. However, in the X-ray task, the\nbest algorithm attained an IoU of 0.774, highlighting the greater challenges\nposed by overlapping anatomical structures. Beyond the quantitative evaluation,\nthe challenge revealed methodological diversity in algorithm design. Variations\nin instance representation, such as primary-secondary classification versus\nboundary-core separation, led to differing segmentation strategies. Despite\npromising results, the challenge also exposed inherent uncertainties in\nfragment definition, particularly in cases of incomplete fractures. These\nfindings suggest that interactive segmentation approaches, integrating human\ndecision-making with task-relevant information, may be essential for improving\nmodel reliability and clinical applicability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02259", "pdf": "https://arxiv.org/pdf/2504.02259", "abs": "https://arxiv.org/abs/2504.02259", "authors": ["Jinhui Ye", "Zihan Wang", "Haosen Sun", "Keshigeyan Chandrasegaran", "Zane Durante", "Cristobal Eyzaguirre", "Yonatan Bisk", "Juan Carlos Niebles", "Ehsan Adeli", "Li Fei-Fei", "Jiajun Wu", "Manling Li"], "title": "Re-thinking Temporal Search for Long-Form Video Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; A real-world long video needle-in-haystack\n  benchmark; long-video QA with human ref frames", "summary": "Efficient understanding of long-form videos remains a significant challenge\nin computer vision. In this work, we revisit temporal search paradigms for\nlong-form video understanding, studying a fundamental issue pertaining to all\nstate-of-the-art (SOTA) long-context vision-language models (VLMs). In\nparticular, our contributions are two-fold: First, we formulate temporal search\nas a Long Video Haystack problem, i.e., finding a minimal set of relevant\nframes (typically one to five) among tens of thousands of frames from\nreal-world long videos given specific queries. To validate our formulation, we\ncreate LV-Haystack, the first benchmark containing 3,874 human-annotated\ninstances with fine-grained evaluation metrics for assessing keyframe search\nquality and computational efficiency. Experimental results on LV-Haystack\nhighlight a significant research gap in temporal search capabilities, with SOTA\nkeyframe selection methods achieving only 2.1% temporal F1 score on the LVBench\nsubset.\n  Next, inspired by visual search in images, we re-think temporal searching and\npropose a lightweight keyframe searching framework, T*, which casts the\nexpensive temporal search as a spatial search problem. T* leverages superior\nvisual localization capabilities typically used in images and introduces an\nadaptive zooming-in mechanism that operates across both temporal and spatial\ndimensions. Our extensive experiments show that when integrated with existing\nmethods, T* significantly improves SOTA long-form video understanding\nperformance. Specifically, under an inference budget of 32 frames, T* improves\nGPT-4o's performance from 50.5% to 53.1% and LLaVA-OneVision-72B's performance\nfrom 56.5% to 62.4% on LongVideoBench XL subset. Our PyTorch code, benchmark\ndataset and models are included in the Supplementary material.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02404", "pdf": "https://arxiv.org/pdf/2504.02404", "abs": "https://arxiv.org/abs/2504.02404", "authors": ["Xiang Feng", "Wentao Jiang", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Baosheng Yu", "Hua Jin", "Bo Du", "Jing Zhang"], "title": "AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology", "categories": ["cs.CL"], "comment": "23 pages, 9 figures", "summary": "The application of large language models (LLMs) in the medical field has\ngained significant attention, yet their reasoning capabilities in more\nspecialized domains like anesthesiology remain underexplored. In this paper, we\nsystematically evaluate the reasoning capabilities of LLMs in anesthesiology\nand analyze key factors influencing their performance. To this end, we\nintroduce AnesBench, a cross-lingual benchmark designed to assess\nanesthesiology-related reasoning across three levels: factual retrieval (System\n1), hybrid reasoning (System 1.x), and complex decision-making (System 2).\nThrough extensive experiments, we first explore how model characteristics,\nincluding model scale, Chain of Thought (CoT) length, and language\ntransferability, affect reasoning performance. Then, we further evaluate the\neffectiveness of different training strategies, leveraging our curated\nanesthesiology-related dataset, including continuous pre-training (CPT) and\nsupervised fine-tuning (SFT). Additionally, we also investigate how the\ntest-time reasoning techniques, such as Best-of-N sampling and beam search,\ninfluence reasoning performance, and assess the impact of reasoning-enhanced\nmodel distillation, specifically DeepSeek-R1. We will publicly release\nAnesBench, along with our CPT and SFT training datasets and evaluation code at\nhttps://github.com/MiliLab/AnesBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "chain of thought", "beam search"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "multi-dimensional"], "score": 4}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02496", "pdf": "https://arxiv.org/pdf/2504.02496", "abs": "https://arxiv.org/abs/2504.02496", "authors": ["Jiuniu Wang", "Wenjia Xu", "Qingzhong Wang", "Antoni B. Chan"], "title": "Group-based Distinctive Image Captioning with Memory Difference Encoding and Attention", "categories": ["cs.CV", "cs.MM"], "comment": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:2108.09151", "summary": "Recent advances in image captioning have focused on enhancing accuracy by\nsubstantially increasing the dataset and model size. While conventional\ncaptioning models exhibit high performance on established metrics such as BLEU,\nCIDEr, and SPICE, the capability of captions to distinguish the target image\nfrom other similar images is under-explored. To generate distinctive captions,\na few pioneers employed contrastive learning or re-weighted the ground-truth\ncaptions. However, these approaches often overlook the relationships among\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events). In this paper, we introduce a novel approach to\nenhance the distinctiveness of image captions, namely Group-based Differential\nDistinctive Captioning Method, which visually compares each image with other\nimages in one similar group and highlights the uniqueness of each image. In\nparticular, we introduce a Group-based Differential Memory Attention (GDMA)\nmodule, designed to identify and emphasize object features in an image that are\nuniquely distinguishable within its image group, i.e., those exhibiting low\nsimilarity with objects in other images. This mechanism ensures that such\nunique object features are prioritized during caption generation for the image,\nthereby enhancing the distinctiveness of the resulting captions. To further\nrefine this process, we select distinctive words from the ground-truth captions\nto guide both the language decoder and the GDMA module. Additionally, we\npropose a new evaluation metric, the Distinctive Word Rate (DisWordRate), to\nquantitatively assess caption distinctiveness. Quantitative results indicate\nthat the proposed method significantly improves the distinctiveness of several\nbaseline models, and achieves state-of-the-art performance on distinctiveness\nwhile not excessively sacrificing accuracy...", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02106", "pdf": "https://arxiv.org/pdf/2504.02106", "abs": "https://arxiv.org/abs/2504.02106", "authors": ["Xiao Wang", "Daniil Larionov", "Siwei Wu", "Yiqi Liu", "Steffen Eger", "Nafise Sadat Moosavi", "Chenghua Lin"], "title": "ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating the quality of generated text automatically remains a significant\nchallenge. Conventional reference-based metrics have been shown to exhibit\nrelatively weak correlation with human evaluations. Recent research advocates\nthe use of large language models (LLMs) as source-based metrics for natural\nlanguage generation (NLG) assessment. While promising, LLM-based metrics,\nparticularly those using smaller models, still fall short in aligning with\nhuman judgments. In this work, we introduce ContrastScore, a contrastive\nevaluation metric designed to enable higher-quality, less biased, and more\nefficient assessment of generated text. We evaluate ContrastScore on two NLG\ntasks: machine translation and summarization. Experimental results show that\nContrastScore consistently achieves stronger correlation with human judgments\nthan both single-model and ensemble-based baselines. Notably, ContrastScore\nbased on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as\nmany parameters, demonstrating its efficiency. Furthermore, it effectively\nmitigates common evaluation biases such as length and likelihood preferences,\nresulting in more robust automatic evaluation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "summarization"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02178", "pdf": "https://arxiv.org/pdf/2504.02178", "abs": "https://arxiv.org/abs/2504.02178", "authors": ["Shanilka Haturusinghe", "Tharindu Cyril Weerasooriya", "Marcos Zampieri", "Christopher M. Homan", "S. R. Liyanage"], "title": "Subasa -- Adapting Language Models for Low-resourced Offensive Language Detection in Sinhala", "categories": ["cs.CL"], "comment": "Accepted to appear at NAACL SRW 2025", "summary": "Accurate detection of offensive language is essential for a number of\napplications related to social media safety. There is a sharp contrast in\nperformance in this task between low and high-resource languages. In this\npaper, we adapt fine-tuning strategies that have not been previously explored\nfor Sinhala in the downstream task of offensive language detection. Using this\napproach, we introduce four models: \"Subasa-XLM-R\", which incorporates an\nintermediate Pre-Finetuning step using Masked Rationale Prediction. Two\nvariants of \"Subasa-Llama\" and \"Subasa-Mistral\", are fine-tuned versions of\nLlama (3.2) and Mistral (v0.3), respectively, with a task-specific strategy. We\nevaluate our models on the SOLD benchmark dataset for Sinhala offensive\nlanguage detection. All our models outperform existing baselines. Subasa-XLM-R\nachieves the highest Macro F1 score (0.84) surpassing state-of-the-art large\nlanguage models like GPT-4o when evaluated on the same SOLD benchmark dataset\nunder zero-shot settings. The models and code are publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02244", "pdf": "https://arxiv.org/pdf/2504.02244", "abs": "https://arxiv.org/abs/2504.02244", "authors": ["Xu Cao", "Pranav Virupaksha", "Wenqi Jia", "Bolin Lai", "Fiona Ryan", "Sangmin Lee", "James M. Rehg"], "title": "SocialGesture: Delving into Multi-person Gesture Understanding", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Previous research in human gesture recognition has largely overlooked\nmulti-person interactions, which are crucial for understanding the social\ncontext of naturally occurring gestures. This limitation in existing datasets\npresents a significant challenge in aligning human gestures with other\nmodalities like language and speech. To address this issue, we introduce\nSocialGesture, the first large-scale dataset specifically designed for\nmulti-person gesture analysis. SocialGesture features a diverse range of\nnatural scenarios and supports multiple gesture analysis tasks, including\nvideo-based recognition and temporal localization, providing a valuable\nresource for advancing the study of gesture during complex social interactions.\nFurthermore, we propose a novel visual question answering (VQA) task to\nbenchmark vision language models'(VLMs) performance on social gesture\nunderstanding. Our findings highlight several limitations of current gesture\nrecognition models, offering insights into future directions for improvement in\nthis field. SocialGesture is available at\nhuggingface.co/datasets/IrohXu/SocialGesture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02590", "pdf": "https://arxiv.org/pdf/2504.02590", "abs": "https://arxiv.org/abs/2504.02590", "authors": ["Kepu Zhang", "Guofu Xie", "Weijie Yu", "Mingyue Xu", "Xu Tang", "Yaxin Li", "Jun Xu"], "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02671", "pdf": "https://arxiv.org/pdf/2504.02671", "abs": "https://arxiv.org/abs/2504.02671", "authors": ["Zishuo Liu", "Carlos Rabat Villarreal", "Mostafa Rahgouy", "Amit Das", "Zheng Zhang", "Chang Ren", "Dongji Feng"], "title": "LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems", "categories": ["cs.CL"], "comment": "7 pages,7 tables, 5 figures", "summary": "Fermi Problems (FPs) are mathematical reasoning tasks that require human-like\nlogic and numerical reasoning. Unlike other reasoning questions, FPs often\ninvolve real-world impracticalities or ambiguous concepts, making them\nchallenging even for humans to solve. Despite advancements in AI, particularly\nwith large language models (LLMs) in various reasoning tasks, FPs remain\nrelatively under-explored. This work conducted an exploratory study to examine\nthe capabilities and limitations of LLMs in solving FPs. We first evaluated the\noverall performance of three advanced LLMs using a publicly available FP\ndataset. We designed prompts according to the recently proposed TELeR taxonomy,\nincluding a zero-shot scenario. Results indicated that all three LLMs achieved\na fp_score (range between 0 - 1) below 0.5, underscoring the inherent\ndifficulty of these reasoning tasks. To further investigate, we categorized FPs\ninto standard and specific questions, hypothesizing that LLMs would perform\nbetter on standard questions, which are characterized by clarity and\nconciseness, than on specific ones. Comparative experiments confirmed this\nhypothesis, demonstrating that LLMs performed better on standard FPs in terms\nof both accuracy and efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02807", "pdf": "https://arxiv.org/pdf/2504.02807", "abs": "https://arxiv.org/abs/2504.02807", "authors": ["Fan Zhou", "Zengzhi Wang", "Nikhil Ranjan", "Zhoujun Cheng", "Liping Tang", "Guowei He", "Zhengzhong Liu", "Eric P. Xing"], "title": "MegaMath: Pushing the Limits of Open Math Corpora", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 15 figures, 22 tables", "summary": "Mathematical reasoning is a cornerstone of human intelligence and a key\nbenchmark for advanced capabilities in large language models (LLMs). However,\nthe research community still lacks an open, large-scale, high-quality corpus\ntailored to the demands of math-centric LLM pre-training. We present MegaMath,\nan open dataset curated from diverse, math-focused sources through following\npractices: (1) Revisiting web data: We re-extracted mathematical documents from\nCommon Crawl with math-oriented HTML optimizations, fasttext-based filtering\nand deduplication, all for acquiring higher-quality data on the Internet. (2)\nRecalling Math-related code data: We identified high quality math-related code\nfrom large code training corpus, Stack-V2, further enhancing data diversity.\n(3) Exploring Synthetic data: We synthesized QA-style text, math-related code,\nand interleaved text-code blocks from web data or code data. By integrating\nthese strategies and validating their effectiveness through extensive\nablations, MegaMath delivers 371B tokens with the largest quantity and top\nquality among existing open math pre-training datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02436", "pdf": "https://arxiv.org/pdf/2504.02436", "abs": "https://arxiv.org/abs/2504.02436", "authors": ["Zhengcong Fei", "Debang Li", "Di Qiu", "Jiahua Wang", "Yikun Dou", "Rui Wang", "Jingtao Xu", "Mingyuan Fan", "Guibin Chen", "Yang Li", "Yahui Zhou"], "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02107", "pdf": "https://arxiv.org/pdf/2504.02107", "abs": "https://arxiv.org/abs/2504.02107", "authors": ["Jeffrey Li", "Mohammadreza Armandpour", "Iman Mirzadeh", "Sachin Mehta", "Vaishaal Shankar", "Raviteja Vemulapalli", "Samy Bengio", "Oncel Tuzel", "Mehrdad Farajtabar", "Hadi Pouransari", "Fartash Faghri"], "title": "TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": "Code available at: https://github.com/apple/ml-tic-lm", "summary": "Large Language Models (LLMs) trained on historical web data inevitably become\noutdated. We investigate evaluation strategies and update methods for LLMs as\nnew data becomes available. We introduce a web-scale dataset for time-continual\npretraining of LLMs derived from 114 dumps of Common Crawl (CC) - orders of\nmagnitude larger than previous continual language modeling benchmarks. We also\ndesign time-stratified evaluations across both general CC data and specific\ndomains (Wikipedia, StackExchange, and code documentation) to assess how well\nvarious continual learning methods adapt to new data while retaining past\nknowledge. Our findings demonstrate that, on general CC data, autoregressive\nmeta-schedules combined with a fixed-ratio replay of older data can achieve\ncomparable held-out loss to re-training from scratch, while requiring\nsignificantly less computation (2.6x). However, the optimal balance between\nincorporating new data and replaying old data differs as replay is crucial to\navoid forgetting on generic web data but less so on specific domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02111", "pdf": "https://arxiv.org/pdf/2504.02111", "abs": "https://arxiv.org/abs/2504.02111", "authors": ["Giannis Chatziveroglou", "Richard Yun", "Maura Kelleher"], "title": "Exploring LLM Reasoning Through Controlled Prompt Variations", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study investigates the reasoning robustness of large language models\n(LLMs) on mathematical problem-solving tasks under systematically introduced\ninput perturbations. Using the GSM8K dataset as a controlled testbed, we\nevaluate how well state-of-the-art models maintain logical consistency and\ncorrectness when confronted with four categories of prompt perturbations:\nirrelevant context, pathological instructions, factually relevant but\nnon-essential context, and a combination of the latter two. Our experiments,\nconducted on thirteen open-source and closed-source LLMs, reveal that\nintroducing irrelevant context within the model's context window significantly\ndegrades performance, suggesting that distinguishing essential from extraneous\ndetails remains a pressing challenge. Surprisingly, performance regressions are\nrelatively insensitive to the complexity of the reasoning task, as measured by\nthe number of steps required, and are not strictly correlated with model size.\nMoreover, we observe that certain perturbations inadvertently trigger\nchain-of-thought-like reasoning behaviors, even without explicit prompting. Our\nfindings highlight critical vulnerabilities in current LLMs and underscore the\nneed for improved robustness against noisy, misleading, and contextually dense\ninputs, paving the way for more resilient and reliable reasoning in real-world\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "testbed", "consistency"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02128", "pdf": "https://arxiv.org/pdf/2504.02128", "abs": "https://arxiv.org/abs/2504.02128", "authors": ["Apurba Pokharel", "Ram Dantu", "Shakila Zaman", "Sirisha Talapuru", "Vinh Quach"], "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figure, 3 tables", "summary": "Blockchain consensus mechanisms have relied on algorithms such as\nProof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality\nand integrity. However, these approaches struggle with adaptability for\ndecision-making where the opinions of each matter rather than reaching an\nagreement based on honest majority or weighted consensus. This paper introduces\na novel deliberation-based consensus mechanism where Large Language Models\n(LLMs) act as rational agents engaging in structured discussions to reach a\nunanimous consensus. By leveraging graded consensus and a multi-round\ndeliberation process, our approach ensures both unanimous consensus for\ndefinitive problems and graded confidence for prioritized decisions and\npolicies. We provide a formalization of our system and use it to show that the\nproperties of blockchains: consistency, agreement, liveness, and determinism\nare maintained. Moreover, experimental results demonstrate our system's\nfeasibility, showcasing how our deliberation method's convergence, block\nproperties, and accuracy enable decision-making on blockchain networks. We also\naddress key challenges with this novel approach such as degeneration of\nthoughts, hallucinations, malicious models and nodes, resource consumption, and\nscalability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02464", "pdf": "https://arxiv.org/pdf/2504.02464", "abs": "https://arxiv.org/abs/2504.02464", "authors": ["Ruixiao Zhang", "Runwei Guan", "Xiangyu Chen", "Adam Prugel-Bennett", "Xiaohao Cai"], "title": "CornerPoint3D: Look at the Nearest Corner Instead of the Center", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.04061", "summary": "3D object detection aims to predict object centers, dimensions, and rotations\nfrom LiDAR point clouds. Despite its simplicity, LiDAR captures only the near\nside of objects, making center-based detectors prone to poor localization\naccuracy in cross-domain tasks with varying point distributions. Meanwhile,\nexisting evaluation metrics designed for single-domain assessment also suffer\nfrom overfitting due to dataset-specific size variations. A key question\narises: Do we really need models to maintain excellent performance in the\nentire 3D bounding boxes after being applied across domains? Actually, one of\nour main focuses is on preventing collisions between vehicles and other\nobstacles, especially in cross-domain scenarios where correctly predicting the\nsizes is much more difficult. To address these issues, we rethink cross-domain\n3D object detection from a practical perspective. We propose two new metrics\nthat evaluate a model's ability to detect objects' closer-surfaces to the LiDAR\nsensor. Additionally, we introduce EdgeHead, a refinement head that guides\nmodels to focus more on learnable closer surfaces, significantly improving\ncross-domain performance under both our new and traditional BEV/3D metrics.\nFurthermore, we argue that predicting the nearest corner rather than the object\ncenter enhances robustness. We propose a novel 3D object detector, coined as\nCornerPoint3D, which is built upon CenterPoint and uses heatmaps to supervise\nthe learning and detection of the nearest corner of each object. Our proposed\nmethods realize a balanced trade-off between the detection quality of entire\nbounding boxes and the locating accuracy of closer surfaces to the LiDAR\nsensor, outperforming the traditional center-based detector CenterPoint in\nmultiple cross-domain tasks and providing a more practically reasonable and\nrobust cross-domain 3D object detection solution.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02605", "pdf": "https://arxiv.org/pdf/2504.02605", "abs": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02519", "pdf": "https://arxiv.org/pdf/2504.02519", "abs": "https://arxiv.org/abs/2504.02519", "authors": ["Christian Alexander Holz", "Christian Bader", "Markus Enzweiler", "Matthias Drüppel"], "title": "Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents novel Machine Learning (ML) methodologies for\nMulti-Object Tracking (MOT), specifically designed to meet the increasing\ncomplexity and precision demands of Advanced Driver Assistance Systems (ADAS).\nWe introduce three Neural Network (NN) models that address key challenges in\nMOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii)\nthe Single-Association Network (SANT) for mapping individual Sensor Object (SO)\nto existing tracks, and (iii) the Multi-Association Network (MANTa) for\nassociating multiple SOs to multiple tracks. These models are seamlessly\nintegrated into a traditional Kalman Filter (KF) framework, maintaining the\nsystem's modularity by replacing relevant components without disrupting the\noverall architecture. Importantly, all three networks are designed to be run in\na realtime, embedded environment. Each network contains less than 50k trainable\nparameters. Our evaluation, conducted on the public KITTI tracking dataset,\ndemonstrates significant improvements in tracking performance. SPENT reduces\nthe Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT\nand MANTa achieve up to 95% accuracy in sensor object-to-track assignments.\nThese results underscore the effectiveness of incorporating task-specific NNs\ninto traditional tracking systems, boosting performance and robustness while\npreserving modularity, maintainability, and interpretability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02602", "pdf": "https://arxiv.org/pdf/2504.02602", "abs": "https://arxiv.org/abs/2504.02602", "authors": ["Abdul Rehman", "Talha Meraj", "Aiman Mahmood Minhas", "Ayisha Imran", "Mohsen Ali", "Waqas Sultani", "Mubarak Shah"], "title": "Leveraging Sparse Annotations for Leukemia Diagnosis on the Large Leukemia Dataset", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Leukemia is 10th most frequently diagnosed cancer and one of the leading\ncauses of cancer related deaths worldwide. Realistic analysis of Leukemia\nrequires White Blook Cells (WBC) localization, classification, and\nmorphological assessment. Despite deep learning advances in medical imaging,\nleukemia analysis lacks a large, diverse multi-task dataset, while existing\nsmall datasets lack domain diversity, limiting real world applicability. To\novercome dataset challenges, we present a large scale WBC dataset named Large\nLeukemia Dataset (LLD) and novel methods for detecting WBC with their\nattributes. Our contribution here is threefold. First, we present a large-scale\nLeukemia dataset collected through Peripheral Blood Films (PBF) from several\npatients, through multiple microscopes, multi cameras, and multi magnification.\nTo enhance diagnosis explainability and medical expert acceptance, each\nleukemia cell is annotated at 100x with 7 morphological attributes, ranging\nfrom Cell Size to Nuclear Shape. Secondly, we propose a multi task model that\nnot only detects WBCs but also predicts their attributes, providing an\ninterpretable and clinically meaningful solution. Third, we propose a method\nfor WBC detection with attribute analysis using sparse annotations. This\napproach reduces the annotation burden on hematologists, requiring them to mark\nonly a small area within the field of view. Our method enables the model to\nleverage the entire field of view rather than just the annotated regions,\nenhancing learning efficiency and diagnostic accuracy. From diagnosis\nexplainability to overcoming domain shift challenges, presented datasets could\nbe used for many challenging aspects of microscopic image analysis. The\ndatasets, code, and demo are available at:\nhttps://im.itu.edu.pk/sparse-leukemiaattri/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02617", "pdf": "https://arxiv.org/pdf/2504.02617", "abs": "https://arxiv.org/abs/2504.02617", "authors": ["Lihua Liu", "Jiehong Lin", "Zhenxin Liu", "Kui Jia"], "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Novel object pose estimation from RGB images presents a significant challenge\nfor zero-shot generalization, as it involves estimating the relative 6D\ntransformation between an RGB observation and a CAD model of an object that was\nnot seen during training. In this paper, we introduce PicoPose, a novel\nframework designed to tackle this task using a three-stage pixel-to-pixel\ncorrespondence learning process. Firstly, PicoPose matches features from the\nRGB observation with those from rendered object templates, identifying the\nbest-matched template and establishing coarse correspondences. Secondly,\nPicoPose smooths the correspondences by globally regressing a 2D affine\ntransformation, including in-plane rotation, scale, and 2D translation, from\nthe coarse correspondence map. Thirdly, PicoPose applies the affine\ntransformation to the feature map of the best-matched template and learns\ncorrespondence offsets within local regions to achieve fine-grained\ncorrespondences. By progressively refining the correspondences, PicoPose\nsignificantly improves the accuracy of object poses computed via PnP/RANSAC.\nPicoPose achieves state-of-the-art performance on the seven core datasets of\nthe BOP benchmark, demonstrating exceptional generalization to novel objects\nrepresented by CAD models or object reference images. Code and models are\navailable at https://github.com/foollh/PicoPose.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02782", "pdf": "https://arxiv.org/pdf/2504.02782", "abs": "https://arxiv.org/abs/2504.02782", "authors": ["Zhiyuan Yan", "Junyan Ye", "Weijia Li", "Zilong Huang", "Shenghai Yuan", "Xiangyang He", "Kaiqing Lin", "Jun He", "Conghui He", "Li Yuan"], "title": "GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The recent breakthroughs in OpenAI's GPT4o model have demonstrated\nsurprisingly good capabilities in image generation and editing, resulting in\nsignificant excitement in the community. This technical report presents the\nfirst-look evaluation benchmark (named GPT-ImgEval), quantitatively and\nqualitatively diagnosing GPT-4o's performance across three critical dimensions:\n(1) generation quality, (2) editing proficiency, and (3) world\nknowledge-informed semantic synthesis. Across all three tasks, GPT-4o\ndemonstrates strong performance, significantly surpassing existing methods in\nboth image generation control and output quality, while also showcasing\nexceptional knowledge reasoning capabilities. Furthermore, based on the\nGPT-4o's generated data, we propose a classification-model-based approach to\ninvestigate the underlying architecture of GPT-4o, where our empirical results\nsuggest the model consists of an auto-regressive (AR) combined with a\ndiffusion-based head for image decoding, rather than the VAR-like\narchitectures. We also provide a complete speculation on GPT-4o's overall\narchitecture. In addition, we conduct a series of analyses to identify and\nvisualize GPT-4o's specific limitations and the synthetic artifacts commonly\nobserved in its image generation. We also present a comparative study of\nmulti-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the\nsafety implications of GPT-4o's outputs, particularly their detectability by\nexisting image forensic models. We hope that our work can offer valuable\ninsight and provide a reliable benchmark to guide future research, foster\nreproducibility, and accelerate innovation in the field of image generation and\nbeyond. The codes and datasets used for evaluating GPT-4o can be found at\nhttps://github.com/PicoTrex/GPT-ImgEval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02812", "pdf": "https://arxiv.org/pdf/2504.02812", "abs": "https://arxiv.org/abs/2504.02812", "authors": ["Van Nguyen Nguyen", "Stephen Tyree", "Andrew Guo", "Mederic Fourmy", "Anas Gouda", "Taeyeop Lee", "Sungphill Moon", "Hyeontae Son", "Lukas Ranftl", "Jonathan Tremblay", "Eric Brachmann", "Bertram Drost", "Vincent Lepetit", "Carsten Rother", "Stan Birchfield", "Jiri Matas", "Yann Labbe", "Martin Sundermeyer", "Tomas Hodan"], "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2403.09799", "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02826", "pdf": "https://arxiv.org/pdf/2504.02826", "abs": "https://arxiv.org/abs/2504.02826", "authors": ["Xiangyu Zhao", "Peiyuan Zhang", "Kexian Tang", "Hao Li", "Zicheng Zhang", "Guangtao Zhai", "Junchi Yan", "Hua Yang", "Xue Yang", "Haodong Duan"], "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "categories": ["cs.CV"], "comment": "27 pages, 23 figures, 1 table. Technical Report", "summary": "Large Multi-modality Models (LMMs) have made significant progress in visual\nunderstanding and generation, but they still face challenges in General Visual\nEditing, particularly in following complex instructions, preserving appearance\nconsistency, and supporting flexible input formats. To address this gap, we\nintroduce RISEBench, the first benchmark for evaluating Reasoning-Informed\nviSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal,\nCausal, Spatial, and Logical Reasoning. We curate high-quality test cases for\neach category and propose an evaluation framework that assesses Instruction\nReasoning, Appearance Consistency, and Visual Plausibility with both human\njudges and an LMM-as-a-judge approach. Our experiments reveal that while\nGPT-4o-Native significantly outperforms other open-source and proprietary\nmodels, even this state-of-the-art system struggles with logical reasoning\ntasks, highlighting an area that remains underexplored. As an initial effort,\nRISEBench aims to provide foundational insights into reasoning-aware visual\nediting and to catalyze future research. Though still in its early stages, we\nare committed to continuously expanding and refining the benchmark to support\nmore comprehensive, reliable, and scalable evaluations of next-generation\nmultimodal systems. Our code and data will be released at\nhttps://github.com/PhoenixZ810/RISEBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.01989", "pdf": "https://arxiv.org/pdf/2504.01989", "abs": "https://arxiv.org/abs/2504.01989", "authors": ["Yi Yao", "Miao Fan", "Shengtong Xu", "Haoyi Xiong", "Xiangzeng Liu", "Wenbo Hu", "Wenbing Huang"], "title": "A Concise Survey on Lane Topology Reasoning for HD Mapping", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IEEE IV'25", "summary": "Lane topology reasoning techniques play a crucial role in high-definition\n(HD) mapping and autonomous driving applications. While recent years have\nwitnessed significant advances in this field, there has been limited effort to\nconsolidate these works into a comprehensive overview. This survey\nsystematically reviews the evolution and current state of lane topology\nreasoning methods, categorizing them into three major paradigms: procedural\nmodeling-based methods, aerial imagery-based methods, and onboard sensors-based\nmethods. We analyze the progression from early rule-based approaches to modern\nlearning-based solutions utilizing transformers, graph neural networks (GNNs),\nand other deep learning architectures. The paper examines standardized\nevaluation metrics, including road-level measures (APLS and TLTS score), and\nlane-level metrics (DET and TOP score), along with performance comparisons on\nbenchmark datasets such as OpenLane-V2. We identify key technical challenges,\nincluding dataset availability and model efficiency, and outline promising\ndirections for future research. This comprehensive review provides researchers\nand practitioners with insights into the theoretical frameworks, practical\nimplementations, and emerging trends in lane topology reasoning for HD mapping\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02408", "pdf": "https://arxiv.org/pdf/2504.02408", "abs": "https://arxiv.org/abs/2504.02408", "authors": ["Naomi Silverstein", "Efrat Leibowitz", "Ron Beloosesky", "Haim Azhari"], "title": "Translation of Fetal Brain Ultrasound Images into Pseudo-MRI Images using Artificial Intelligence", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 7 figures", "summary": "Ultrasound is a widely accessible and cost-effective medical imaging tool\ncommonly used for prenatal evaluation of the fetal brain. However, it has\nlimitations, particularly in the third trimester, where the complexity of the\nfetal brain requires high image quality for extracting quantitative data. In\ncontrast, magnetic resonance imaging (MRI) offers superior image quality and\ntissue differentiation but is less available, expensive, and requires\ntime-consuming acquisition. Thus, transforming ultrasonic images into an\nMRI-mimicking display may be advantageous and allow better tissue anatomy\npresentation. To address this goal, we have examined the use of artificial\nintelligence, implementing a diffusion model renowned for generating\nhigh-quality images. The proposed method, termed \"Dual Diffusion Imposed\nCorrelation\" (DDIC), leverages a diffusion-based translation methodology,\nassuming a shared latent space between ultrasound and MRI domains. Model\ntraining was obtained utilizing the \"HC18\" dataset for ultrasound and the \"CRL\nfetal brain atlas\" along with the \"FeTA \" datasets for MRI. The generated\npseudo-MRI images provide notable improvements in visual discrimination of\nbrain tissue, especially in the lateral ventricles and the Sylvian fissure,\ncharacterized by enhanced contrast clarity. Improvement was demonstrated in\nMutual information, Peak signal-to-noise ratio, Fr\\'echet Inception Distance,\nand Contrast-to-noise ratio. Findings from these evaluations indicate\nstatistically significant superior performance of the DDIC compared to other\ntranslation methodologies. In addition, a Medical Opinion Test was obtained\nfrom 5 gynecologists. The results demonstrated display improvement in 81% of\nthe tested images. In conclusion, the presented pseudo-MRI images hold the\npotential for streamlining diagnosis and enhancing clinical outcomes through\nimproved representation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02060", "pdf": "https://arxiv.org/pdf/2504.02060", "abs": "https://arxiv.org/abs/2504.02060", "authors": ["Minh-Quan Ho-Le", "Duy-Khang Ho", "Van-Tu Ninh", "Cathal Gurrin", "Minh-Triet Tran"], "title": "LSC-ADL: An Activity of Daily Living (ADL)-Annotated Lifelog Dataset Generated via Semi-Automatic Clustering", "categories": ["cs.CV", "cs.IR"], "comment": "11 pages, 4 figures", "summary": "Lifelogging involves continuously capturing personal data through wearable\ncameras, providing an egocentric view of daily activities. Lifelog retrieval\naims to search and retrieve relevant moments from this data, yet existing\nmethods largely overlook activity-level annotations, which capture temporal\nrelationships and enrich semantic understanding. In this work, we introduce\nLSC-ADL, an ADL-annotated lifelog dataset derived from the LSC dataset,\nincorporating Activities of Daily Living (ADLs) as a structured semantic layer.\nUsing a semi-automatic approach featuring the HDBSCAN algorithm for intra-class\nclustering and human-in-the-loop verification, we generate accurate ADL\nannotations to enhance retrieval explainability. By integrating action\nrecognition into lifelog retrieval, LSC-ADL bridges a critical gap in existing\nresearch, offering a more context-aware representation of daily life. We\nbelieve this dataset will advance research in lifelog retrieval, activity\nrecognition, and egocentric vision, ultimately improving the accuracy and\ninterpretability of retrieved content. The ADL annotations can be downloaded at\nhttps://bit.ly/lsc-adl-annotations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02061", "pdf": "https://arxiv.org/pdf/2504.02061", "abs": "https://arxiv.org/abs/2504.02061", "authors": ["Yuxin Guo", "Shuailei Ma", "Shijie Ma", "Xiaoyi Bao", "Chen-Wei Xie", "Kecheng Zheng", "Tingyu Weng", "Siyang Sun", "Yun Zheng", "Wei Zou"], "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ICLR 2025", "summary": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02168", "pdf": "https://arxiv.org/pdf/2504.02168", "abs": "https://arxiv.org/abs/2504.02168", "authors": ["Xinglong Sun", "Barath Lakshmanan", "Maying Shen", "Shiyi Lan", "Jingde Chen", "Jose M. Alvarez"], "title": "MDP: Multidimensional Vision Model Pruning with Latency Constraint", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at CVPR 2025", "summary": "Current structural pruning methods face two significant limitations: (i) they\noften limit pruning to finer-grained levels like channels, making aggressive\nparameter reduction challenging, and (ii) they focus heavily on parameter and\nFLOP reduction, with existing latency-aware methods frequently relying on\nsimplistic, suboptimal linear models that fail to generalize well to\ntransformers, where multiple interacting dimensions impact latency. In this\npaper, we address both limitations by introducing Multi-Dimensional Pruning\n(MDP), a novel paradigm that jointly optimizes across a variety of pruning\ngranularities-including channels, query, key, heads, embeddings, and blocks.\nMDP employs an advanced latency modeling technique to accurately capture\nlatency variations across all prunable dimensions, achieving an optimal balance\nbetween latency and accuracy. By reformulating pruning as a Mixed-Integer\nNonlinear Program (MINLP), MDP efficiently identifies the optimal pruned\nstructure across all prunable dimensions while respecting latency constraints.\nThis versatile framework supports both CNNs and transformers. Extensive\nexperiments demonstrate that MDP significantly outperforms previous methods,\nespecially at high pruning ratios. On ImageNet, MDP achieves a 28% speed\nincrease with a +1.4 Top-1 accuracy improvement over prior work like HALP for\nResNet50 pruning. Against the latest transformer pruning method, Isomorphic,\nMDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy\nimprovement.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "multi-dimensional"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02293", "pdf": "https://arxiv.org/pdf/2504.02293", "abs": "https://arxiv.org/abs/2504.02293", "authors": ["Sharif Md. Abdullah", "Abhijit Paul", "Shebuti Rayana", "Ahmedul Kabir", "Zarif Masud"], "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "Initial Version", "summary": "Despite a large deaf and dumb population of 1.7 million, Bangla Sign Language\n(BdSL) remains a understudied domain. Specifically, there are no works on\nBangla text-to-gloss translation task. To address this gap, we begin by\naddressing the dataset problem. We take inspiration from grammatical rule based\ngloss generation used in Germany and American sign langauage (ASL) and adapt it\nfor BdSL. We also leverage LLM to generate synthetic data and use\nback-translation, text generation for data augmentation. With dataset prepared,\nwe started experimentation. We fine-tuned pretrained mBART-50 and\nmBERT-multiclass-uncased model on our dataset. We also trained GRU, RNN and a\nnovel seq-to-seq model with multi-head attention. We observe significant high\nperformance (ScareBLEU=79.53) with fine-tuning pretrained mBART-50 multilingual\nmodel from Facebook. We then explored why we observe such high performance with\nmBART. We soon notice an interesting property of mBART -- it was trained on\nshuffled and masked text data. And as we know, gloss form has shuffling\nproperty. So we hypothesize that mBART is inherently good at text-to-gloss\ntasks. To find support against this hypothesis, we trained mBART-50 on\nPHOENIX-14T benchmark and evaluated it with existing literature. Our mBART-50\nfinetune demonstrated State-of-the-Art performance on PHOENIX-14T benchmark,\nfar outperforming existing models in all 6 metrics (ScareBLEU = 63.89, BLEU-1 =\n55.14, BLEU-2 = 38.07, BLEU-3 = 27.13, BLEU-4 = 20.68, COMET = 0.624). Based on\nthe results, this study proposes a new paradigm for text-to-gloss task using\nmBART models. Additionally, our results show that BdSL text-to-gloss task can\ngreatly benefit from rule-based synthetic dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02304", "pdf": "https://arxiv.org/pdf/2504.02304", "abs": "https://arxiv.org/abs/2504.02304", "authors": ["Minheng Ni", "Ennan Wu", "Zidong Gong", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Lijuan Wang", "Wangmeng Zuo"], "title": "Measurement of LLM's Philosophies of Human Nature", "categories": ["cs.CL"], "comment": null, "summary": "The widespread application of artificial intelligence (AI) in various tasks,\nalong with frequent reports of conflicts or violations involving AI, has\nsparked societal concerns about interactions with AI systems. Based on\nWrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically\nvalidated over decades to effectively assess individuals' attitudes toward\nhuman nature, we design the standardized psychological scale specifically\ntargeting large language models (LLM), named the Machine-based Philosophies of\nHuman Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature\nacross six dimensions, we reveal that current LLMs exhibit a systemic lack of\ntrust in humans, and there is a significant negative correlation between the\nmodel's intelligence level and its trust in humans. Furthermore, we propose a\nmental loop learning framework, which enables LLM to continuously optimize its\nvalue system during virtual interactions by constructing moral scenarios,\nthereby improving its attitude toward human nature. Experiments demonstrate\nthat mental loop learning significantly enhances their trust in humans compared\nto persona or instruction prompts. This finding highlights the potential of\nhuman-based psychological assessments for LLM, which can not only diagnose\ncognitive biases but also provide a potential solution for ethical learning in\nartificial intelligence. We release the M-PHNS evaluation code and data at\nhttps://github.com/kodenii/M-PHNS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02310", "pdf": "https://arxiv.org/pdf/2504.02310", "abs": "https://arxiv.org/abs/2504.02310", "authors": ["Zidong Yu", "Shuo Wang", "Nan Jiang", "Weiqiang Huang", "Xu Han", "Junliang Du"], "title": "Improving Harmful Text Detection with Joint Retrieval and External Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Harmful text detection has become a crucial task in the development and\ndeployment of large language models, especially as AI-generated content\ncontinues to expand across digital platforms. This study proposes a joint\nretrieval framework that integrates pre-trained language models with knowledge\ngraphs to improve the accuracy and robustness of harmful text detection.\nExperimental results demonstrate that the joint retrieval approach\nsignificantly outperforms single-model baselines, particularly in low-resource\ntraining scenarios and multilingual environments. The proposed method\neffectively captures nuanced harmful content by leveraging external contextual\ninformation, addressing the limitations of traditional detection models. Future\nresearch should focus on optimizing computational efficiency, enhancing model\ninterpretability, and expanding multimodal detection capabilities to better\ntackle evolving harmful content patterns. This work contributes to the\nadvancement of AI safety, ensuring more trustworthy and reliable content\nmoderation systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02327", "pdf": "https://arxiv.org/pdf/2504.02327", "abs": "https://arxiv.org/abs/2504.02327", "authors": ["Weibin Liao", "Xin Gao", "Tianyu Jia", "Rihong Qiu", "Yifan Zhu", "Yang Lin", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling\nseamless interaction with databases. Recent advancements in Large Language\nModels (LLMs) have demonstrated remarkable performance in this domain. However,\nexisting NL2SQL methods predominantly rely on closed-source LLMs leveraging\nprompt engineering, while open-source models typically require fine-tuning to\nacquire domain-specific knowledge. Despite these efforts, open-source LLMs\nstruggle with complex NL2SQL tasks due to the indirect expression of user query\nobjectives and the semantic gap between user queries and database schemas.\nInspired by the application of reinforcement learning in mathematical\nproblem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT\n(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that\nimproves the performance of open-source LLMs on complex NL2SQL tasks through\ntask decomposition and reinforcement learning. LearNAT introduces three key\ncomponents: (1) a Decomposition Synthesis Procedure that leverages Abstract\nSyntax Trees (ASTs) to guide efficient search and pruning strategies for task\ndecomposition, (2) Margin-aware Reinforcement Learning, which employs\nfine-grained step-level optimization via DPO with AST margins, and (3) Adaptive\nDemonstration Reasoning, a mechanism for dynamically selecting relevant\nexamples to enhance decomposition capabilities. Extensive experiments on two\nbenchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a\n7B-parameter open-source LLM to achieve performance comparable to GPT-4, while\noffering improved efficiency and accessibility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02261", "pdf": "https://arxiv.org/pdf/2504.02261", "abs": "https://arxiv.org/abs/2504.02261", "authors": ["Chaojun Ni", "Xiaofeng Wang", "Zheng Zhu", "Weijie Wang", "Haoyun Li", "Guosheng Zhao", "Jie Li", "Wenkang Qin", "Guan Huang", "Wenjun Mei"], "title": "WonderTurbo: Generating Interactive 3D World in 0.72 Seconds", "categories": ["cs.CV"], "comment": "Project Page: https://wonderturbo.github.io", "summary": "Interactive 3D generation is gaining momentum and capturing extensive\nattention for its potential to create immersive virtual experiences. However, a\ncritical challenge in current 3D generation technologies lies in achieving\nreal-time interactivity. To address this issue, we introduce WonderTurbo, the\nfirst real-time interactive 3D scene generation framework capable of generating\nnovel perspectives of 3D scenes within 0.72 seconds. Specifically, WonderTurbo\naccelerates both geometric and appearance modeling in 3D scene generation. In\nterms of geometry, we propose StepSplat, an innovative method that constructs\nefficient 3D geometric representations through dynamic updates, each taking\nonly 0.26 seconds. Additionally, we design QuickDepth, a lightweight depth\ncompletion module that provides consistent depth input for StepSplat, further\nenhancing geometric accuracy. For appearance modeling, we develop FastPaint, a\n2-steps diffusion model tailored for instant inpainting, which focuses on\nmaintaining spatial appearance consistency. Experimental results demonstrate\nthat WonderTurbo achieves a remarkable 15X speedup compared to baseline\nmethods, while preserving excellent spatial consistency and delivering\nhigh-quality output.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02403", "pdf": "https://arxiv.org/pdf/2504.02403", "abs": "https://arxiv.org/abs/2504.02403", "authors": ["Max Müller-Eberstein", "Mike Zhang", "Elisa Bassignana", "Peter Brunsgaard Trolle", "Rob van der Goot"], "title": "DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted at C3NLP at NAACL", "summary": "Large Language Models (LLMs) have seen widespread societal adoption. However,\nwhile they are able to interact with users in languages beyond English, they\nhave been shown to lack cultural awareness, providing anglocentric or\ninappropriate responses for underrepresented language communities. To\ninvestigate this gap and disentangle linguistic versus cultural proficiency, we\nconduct the first cultural evaluation study for the mid-resource language of\nDanish, in which native speakers prompt different models to solve tasks\nrequiring cultural awareness. Our analysis of the resulting 1,038 interactions\nfrom 63 demographically diverse participants highlights open challenges to\ncultural adaptation: Particularly, how currently employed automatically\ntranslated data are insufficient to train or measure cultural adaptation, and\nhow training on native-speaker data can more than double response acceptance\nrates. We release our study data as DaKultur - the first native Danish cultural\nawareness dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02270", "pdf": "https://arxiv.org/pdf/2504.02270", "abs": "https://arxiv.org/abs/2504.02270", "authors": ["Samuel Sze", "Daniele De Martini", "Lars Kunze"], "title": "MinkOcc: Towards real-time label-efficient semantic occupancy prediction", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages", "summary": "Developing 3D semantic occupancy prediction models often relies on dense 3D\nannotations for supervised learning, a process that is both labor and\nresource-intensive, underscoring the need for label-efficient or even\nlabel-free approaches. To address this, we introduce MinkOcc, a multi-modal 3D\nsemantic occupancy prediction framework for cameras and LiDARs that proposes a\ntwo-step semi-supervised training procedure. Here, a small dataset of\nexplicitly 3D annotations warm-starts the training process; then, the\nsupervision is continued by simpler-to-annotate accumulated LiDAR sweeps and\nimages -- semantically labelled through vision foundational models. MinkOcc\neffectively utilizes these sensor-rich supervisory cues and reduces reliance on\nmanual labeling by 90\\% while maintaining competitive accuracy. In addition,\nthe proposed model incorporates information from LiDAR and camera data through\nearly fusion and leverages sparse convolution networks for real-time\nprediction. With its efficiency in both supervision and computation, we aim to\nextend MinkOcc beyond curated datasets, enabling broader real-world deployment\nof 3D semantic occupancy prediction in autonomous driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02411", "pdf": "https://arxiv.org/pdf/2504.02411", "abs": "https://arxiv.org/abs/2504.02411", "authors": ["Alexandre Misrahi", "Nadezhda Chirkova", "Maxime Louis", "Vassilina Nikoulina"], "title": "Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation", "categories": ["cs.CL"], "comment": "25 pages, 8 figures, 21 tables", "summary": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, but\nmulti-domain applications face challenges like lack of diverse benchmarks and\npoor out-of-domain generalization. The first contribution of this work is to\nintroduce a diverse benchmark comprising a variety of question-answering tasks\nfrom 8 sources and covering 13 domains. Our second contribution consists in\nsystematically testing out-of-domain generalization for typical RAG tuning\nstrategies. While our findings reveal that standard fine-tuning fails to\ngeneralize effectively, we show that sequence-level distillation with\nteacher-generated labels improves out-of-domain performance by providing more\ncoherent supervision. Our findings highlight key strategies for improving\nmulti-domain RAG robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "factuality"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02272", "pdf": "https://arxiv.org/pdf/2504.02272", "abs": "https://arxiv.org/abs/2504.02272", "authors": ["Shaocong Long", "Qianyu Zhou", "Xiangtai Li", "Chenhao Ying", "Yunhai Tong", "Lizhuang Ma", "Yuan Luo", "Dacheng Tao"], "title": "Generative Classifier for Domain Generalization", "categories": ["cs.CV"], "comment": "Code will be available at https://github.com/longshaocong/GCDG", "summary": "Domain generalization (DG) aims to improve the generalizability of computer\nvision models toward distribution shifts. The mainstream DG methods focus on\nlearning domain invariance, however, such methods overlook the potential\ninherent in domain-specific information. While the prevailing practice of\ndiscriminative linear classifier has been tailored to domain-invariant\nfeatures, it struggles when confronted with diverse domain-specific\ninformation, e.g., intra-class shifts, that exhibits multi-modality. To address\nthese issues, we explore the theoretical implications of relying on domain\ninvariance, revealing the crucial role of domain-specific information in\nmitigating the target risk for DG. Drawing from these insights, we propose\nGenerative Classifier-driven Domain Generalization (GCDG), introducing a\ngenerative paradigm for the DG classifier based on Gaussian Mixture Models\n(GMMs) for each class across domains. GCDG consists of three key modules:\nHeterogeneity Learning Classifier~(HLC), Spurious Correlation Blocking~(SCB),\nand Diverse Component Balancing~(DCB). Concretely, HLC attempts to model the\nfeature distributions and thereby capture valuable domain-specific information\nvia GMMs. SCB identifies the neural units containing spurious correlations and\nperturbs them, mitigating the risk of HLC learning spurious patterns.\nMeanwhile, DCB ensures a balanced contribution of components in HLC, preventing\nthe underestimation or neglect of critical components. In this way, GCDG excels\nin capturing the nuances of domain-specific information characterized by\ndiverse distributions. GCDG demonstrates the potential to reduce the target\nrisk and encourage flat minima, improving the generalizability. Extensive\nexperiments show GCDG's comparable performance on five DG benchmarks and one\nface anti-spoofing dataset, seamlessly integrating into existing DG methods\nwith consistent improvements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02277", "pdf": "https://arxiv.org/pdf/2504.02277", "abs": "https://arxiv.org/abs/2504.02277", "authors": ["Amit Rand", "Hadi Ibrahim"], "title": "Beyond Conventional Transformers: The Medical X-ray Attention (MXA) Block for Improved Multi-Label Diagnosis Using Knowledge Distillation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 4 figures, 5 tables. For supplementary material and code,\n  see https://github.com/Hadi-M-Ibrahim/Beyond-Conventional-Transformers/", "summary": "Medical imaging, particularly X-ray analysis, often involves detecting\nmultiple conditions simultaneously within a single scan, making multi-label\nclassification crucial for real-world clinical applications. We present the\nMedical X-ray Attention (MXA) block, a novel attention mechanism tailored\nspecifically to address the unique challenges of X-ray abnormality detection.\nThe MXA block enhances traditional Multi-Head Self Attention (MHSA) by\nintegrating a specialized module that efficiently captures both detailed local\ninformation and broader global context. To the best of our knowledge, this is\nthe first work to propose a task-specific attention mechanism for diagnosing\nchest X-rays, as well as to attempt multi-label classification using an\nEfficient Vision Transformer (EfficientViT). By embedding the MXA block within\nthe EfficientViT architecture and employing knowledge distillation, our\nproposed model significantly improves performance on the CheXpert dataset, a\nwidely used benchmark for multi-label chest X-ray abnormality detection. Our\napproach achieves an area under the curve (AUC) of 0.85, an absolute\nimprovement of 0.19 compared to our baseline model's AUC of 0.66, corresponding\nto a substantial approximate 233% relative improvement over random guessing\n(AUC = 0.5).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02279", "pdf": "https://arxiv.org/pdf/2504.02279", "abs": "https://arxiv.org/abs/2504.02279", "authors": ["Trung Thanh Nguyen", "Yasutomo Kawanishi", "Vijay John", "Takahiro Komamizu", "Ichiro Ide"], "title": "MultiTSF: Transformer-based Sensor Fusion for Human-Centric Multi-view and Multi-modal Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Action recognition from multi-modal and multi-view observations holds\nsignificant potential for applications in surveillance, robotics, and smart\nenvironments. However, existing methods often fall short of addressing\nreal-world challenges such as diverse environmental conditions, strict sensor\nsynchronization, and the need for fine-grained annotations. In this study, we\npropose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF).\nThe proposed method leverages a Transformer-based to dynamically model\ninter-view relationships and capture temporal dependencies across multiple\nviews. Additionally, we introduce a Human Detection Module to generate\npseudo-ground-truth labels, enabling the model to prioritize frames containing\nhuman activity and enhance spatial feature learning. Comprehensive experiments\nconducted on our in-house MultiSensor-Home dataset and the existing MM-Office\ndataset demonstrate that MultiTSF outperforms state-of-the-art methods in both\nvideo sequence-level and frame-level action recognition settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02287", "pdf": "https://arxiv.org/pdf/2504.02287", "abs": "https://arxiv.org/abs/2504.02287", "authors": ["Trung Thanh Nguyen", "Yasutomo Kawanishi", "Vijay John", "Takahiro Komamizu", "Ichiro Ide"], "title": "MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal multi-view action recognition is a rapidly growing field in\ncomputer vision, offering significant potential for applications in\nsurveillance. However, current datasets often fail to address real-world\nchallenges such as wide-area environmental conditions, asynchronous data\nstreams, and the lack of frame-level annotations. Furthermore, existing methods\nface difficulties in effectively modeling inter-view relationships and\nenhancing spatial feature learning. In this study, we propose the Multi-modal\nMulti-view Transformer-based Sensor Fusion (MultiTSF) method and introduce the\nMultiSensor-Home dataset, a novel benchmark designed for comprehensive action\nrecognition in home environments. The MultiSensor-Home dataset features\nuntrimmed videos captured by distributed sensors, providing high-resolution RGB\nand audio data along with detailed multi-view frame-level action labels. The\nproposed MultiTSF method leverages a Transformer-based fusion mechanism to\ndynamically model inter-view relationships. Furthermore, the method also\nintegrates a external human detection module to enhance spatial feature\nlearning. Experiments on MultiSensor-Home and MM-Office datasets demonstrate\nthe superiority of MultiTSF over the state-of-the-art methods. The quantitative\nand qualitative results highlight the effectiveness of the proposed method in\nadvancing real-world multi-modal multi-view action recognition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02559", "pdf": "https://arxiv.org/pdf/2504.02559", "abs": "https://arxiv.org/abs/2504.02559", "authors": ["Siddharth Khincha", "Tushar Kataria", "Ankita Anand", "Dan Roth", "Vivek Gupta"], "title": "Leveraging LLM For Synchronizing Information Across Multilingual Tables", "categories": ["cs.CL"], "comment": "17 Pages, 11 Tables, 2 Figures", "summary": "The vast amount of online information today poses challenges for non-English\nspeakers, as much of it is concentrated in high-resource languages such as\nEnglish and French. Wikipedia reflects this imbalance, with content in\nlow-resource languages frequently outdated or incomplete. Recent research has\nsought to improve cross-language synchronization of Wikipedia tables using\nrule-based methods. These approaches can be effective, but they struggle with\ncomplexity and generalization. This paper explores large language models (LLMs)\nfor multilingual information synchronization, using zero-shot prompting as a\nscalable solution. We introduce the Information Updation dataset, simulating\nthe real-world process of updating outdated Wikipedia tables, and evaluate LLM\nperformance. Our findings reveal that single-prompt approaches often produce\nsuboptimal results, prompting us to introduce a task decomposition strategy\nthat enhances coherence and accuracy. Our proposed method outperforms existing\nbaselines, particularly in Information Updation (1.79%) and Information\nAddition (20.58%), highlighting the model strength in dynamically updating and\nenriching data across architectures", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02572", "pdf": "https://arxiv.org/pdf/2504.02572", "abs": "https://arxiv.org/abs/2504.02572", "authors": ["Fabio Celli", "Georgios Spathulas"], "title": "Language Models reach higher Agreement than Humans in Historical Interpretation", "categories": ["cs.CL"], "comment": null, "summary": "This paper compares historical annotations by humans and Large Language\nModels. The findings reveal that both exhibit some cultural bias, but Large\nLanguage Models achieve a higher consensus on the interpretation of historical\nfacts from short texts. While humans tend to disagree on the basis of their\npersonal biases, Large Models disagree when they skip information or produce\nhallucinations. These findings have significant implications for digital\nhumanities, enabling large-scale annotation and quantitative analysis of\nhistorical data. This offers new educational and research opportunities to\nexplore historical interpretations from different Language Models, fostering\ncritical thinking about bias.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02335", "pdf": "https://arxiv.org/pdf/2504.02335", "abs": "https://arxiv.org/abs/2504.02335", "authors": ["Seif Mzoughi", "Mohamed Elshafeia", "Foutse Khomh"], "title": "Evaluating and Enhancing Segmentation Model Robustness with Metamorphic Testing", "categories": ["cs.CV", "cs.SE"], "comment": null, "summary": "Image segmentation is critical for applications such as medical imaging,\naugmented reality, and video surveillance. However, segmentation models often\nlack robustness, making them vulnerable to adversarial perturbations from\nsubtle image distortions. In this work, we propose SegRMT, a metamorphic\ntesting approach that leverages genetic algorithms (GA) to optimize sequences\nof spatial and spectral transformations while preserving image fidelity via a\npredefined PSNR threshold. Using the Cityscapes dataset, our method generates\nadversarial examples that effectively challenge the DeepLabV3 segmentation\nmodel. Our experiments show that SegRMT reduces DeepLabV3's mean Intersection\nover Union (mIoU) to 6.4%, outperforming other adversarial baselines that\ndecrease mIoU to between 8.5% and 21.7%. Furthermore, when used for adversarial\ntraining, SegRMT boosts model performance, achieving mIoU improvements up to\n73% on dedicated adversarial datasets and increasing cross-adversarial mIoU to\n53.8%, compared to only 2%-10% for other methods. These findings demonstrate\nthat SegRMT not only simulates realistic image distortions but also enhances\nthe robustness of segmentation models, making it a valuable tool for ensuring\nreliable performance in safety-critical applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02416", "pdf": "https://arxiv.org/pdf/2504.02416", "abs": "https://arxiv.org/abs/2504.02416", "authors": ["Peifu Liu", "Huiyan Bai", "Tingfa Xu", "Jihui Wang", "Huan Chen", "Jianan Li"], "title": "Hyperspectral Remote Sensing Images Salient Object Detection: The First Benchmark Dataset and Baseline", "categories": ["cs.CV"], "comment": "Accepted by TGRS 2025", "summary": "The objective of hyperspectral remote sensing image salient object detection\n(HRSI-SOD) is to identify objects or regions that exhibit distinct spectrum\ncontrasts with the background. This area holds significant promise for\npractical applications; however, progress has been limited by a notable\nscarcity of dedicated datasets and methodologies. To bridge this gap and\nstimulate further research, we introduce the first HRSI-SOD dataset, termed\nHRSSD, which includes 704 hyperspectral images and 5327 pixel-level annotated\nsalient objects. The HRSSD dataset poses substantial challenges for salient\nobject detection algorithms due to large scale variation, diverse\nforeground-background relations, and multi-salient objects. Additionally, we\npropose an innovative and efficient baseline model for HRSI-SOD, termed the\nDeep Spectral Saliency Network (DSSN). The core of DSSN is the Cross-level\nSaliency Assessment Block, which performs pixel-wise attention and evaluates\nthe contributions of multi-scale similarity maps at each spatial location,\neffectively reducing erroneous responses in cluttered regions and emphasizes\nsalient regions across scales. Additionally, the High-resolution Fusion Module\ncombines bottom-up fusion strategy and learned spatial upsampling to leverage\nthe strengths of multi-scale saliency maps, ensuring accurate localization of\nsmall objects. Experiments on the HRSSD dataset robustly validate the\nsuperiority of DSSN, underscoring the critical need for specialized datasets\nand methodologies in this domain. Further evaluations on the HSOD-BIT and\nHS-SOD datasets demonstrate the generalizability of the proposed method. The\ndataset and source code are publicly available at\nhttps://github.com/laprf/HRSSD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02810", "pdf": "https://arxiv.org/pdf/2504.02810", "abs": "https://arxiv.org/abs/2504.02810", "authors": ["Haowei Lin", "Xiangyu Wang", "Ruilin Yan", "Baizhou Huang", "Haotian Ye", "Jianhua Zhu", "Zihao Wang", "James Zou", "Jianzhu Ma", "Yitao Liang"], "title": "Generative Evaluation of Complex Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "With powerful large language models (LLMs) demonstrating superhuman reasoning\ncapabilities, a critical question arises: Do LLMs genuinely reason, or do they\nmerely recall answers from their extensive, web-scraped training datasets?\nPublicly released benchmarks inevitably become contaminated once incorporated\ninto subsequent LLM training sets, undermining their reliability as faithful\nassessments. To address this, we introduce KUMO, a generative evaluation\nframework designed specifically for assessing reasoning in LLMs. KUMO\nsynergistically combines LLMs with symbolic engines to dynamically produce\ndiverse, multi-turn reasoning tasks that are partially observable and\nadjustable in difficulty. Through an automated pipeline, KUMO continuously\ngenerates novel tasks across open-ended domains, compelling models to\ndemonstrate genuine generalization rather than memorization. We evaluated 23\nstate-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO,\nbenchmarking their reasoning abilities against university students. Our\nfindings reveal that many LLMs have outperformed university-level performance\non easy reasoning tasks, and reasoning-scaled LLMs reach university-level\nperformance on complex reasoning challenges. Moreover, LLM performance on KUMO\ntasks correlates strongly with results on newly released real-world reasoning\nbenchmarks, underscoring KUMO's value as a robust, enduring assessment tool for\ngenuine LLM reasoning capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02417", "pdf": "https://arxiv.org/pdf/2504.02417", "abs": "https://arxiv.org/abs/2504.02417", "authors": ["Lili Liang", "Guanglu Sun"], "title": "Leveraging Static Relationships for Intra-Type and Inter-Type Message Passing in Video Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) is an important research direction in the\nfield of artificial intelligence, enabling machines to understand video content\nand perform reasoning and answering based on natural language questions.\nAlthough methods based on static relationship reasoning have made certain\nprogress, there are still deficiencies in the accuracy of static relationship\nrecognition and representation, and they have not fully utilized the static\nrelationship information in videos for in-depth reasoning and analysis.\nTherefore, this paper proposes a reasoning method for intra-type and inter-type\nmessage passing based on static relationships. This method constructs a dual\ngraph for intra-type message passing reasoning and builds a heterogeneous graph\nbased on static relationships for inter-type message passing reasoning. The\nintra-type message passing reasoning model captures the neighborhood\ninformation of targets and relationships related to the question in the dual\ngraph, updating the dual graph to obtain intra-type clues for answering the\nquestion. The inter-type message passing reasoning model captures the\nneighborhood information of targets and relationships from different categories\nrelated to the question in the heterogeneous graph, updating the heterogeneous\ngraph to obtain inter-type clues for answering the question. Finally, the\nanswers are inferred by combining the intra-type and inter-type clues based on\nstatic relationships. Experimental results on the ANetQA and Next-QA datasets\ndemonstrate the effectiveness of this method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02451", "pdf": "https://arxiv.org/pdf/2504.02451", "abs": "https://arxiv.org/abs/2504.02451", "authors": ["Jiayi Gao", "Zijin Yin", "Changcheng Hua", "Yuxin Peng", "Kongming Liang", "Zhanyu Ma", "Jun Guo", "Yang Liu"], "title": "ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer", "categories": ["cs.CV"], "comment": null, "summary": "The development of Text-to-Video (T2V) generation has made motion transfer\npossible, enabling the control of video motion based on existing footage.\nHowever, current methods have two limitations: 1) struggle to handle\nmulti-subjects videos, failing to transfer specific subject motion; 2) struggle\nto preserve the diversity and accuracy of motion as transferring to subjects\nwith varying shapes. To overcome these, we introduce \\textbf{ConMo}, a\nzero-shot framework that disentangle and recompose the motions of subjects and\ncamera movements. ConMo isolates individual subject and background motion cues\nfrom complex trajectories in source videos using only subject masks, and\nreassembles them for target video generation. This approach enables more\naccurate motion control across diverse subjects and improves performance in\nmulti-subject scenarios. Additionally, we propose soft guidance in the\nrecomposition stage which controls the retention of original motion to adjust\nshape constraints, aiding subject shape adaptation and semantic transformation.\nUnlike previous methods, ConMo unlocks a wide range of applications, including\nsubject size and position editing, subject removal, semantic modifications, and\ncamera motion simulation. Extensive experiments demonstrate that ConMo\nsignificantly outperforms state-of-the-art methods in motion fidelity and\nsemantic consistency. The code is available at\nhttps://github.com/Andyplus1/ConMo.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02558", "pdf": "https://arxiv.org/pdf/2504.02558", "abs": "https://arxiv.org/abs/2504.02558", "authors": ["Andrei Dumitriu", "Florin Tatui", "Florin Miron", "Radu Tudor Ionescu", "Radu Timofte"], "title": "Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results", "categories": ["cs.CV", "cs.AI", "I.4.0; I.4.9"], "comment": "Accepted at CVPR 2023 NTIRE Workshop", "summary": "Rip currents are the leading cause of fatal accidents and injuries on many\nbeaches worldwide, emphasizing the importance of automatically detecting these\nhazardous surface water currents. In this paper, we address a novel task: rip\ncurrent instance segmentation. We introduce a comprehensive dataset containing\n$2,466$ images with newly created polygonal annotations for instance\nsegmentation, used for training and validation. Additionally, we present a\nnovel dataset comprising $17$ drone videos (comprising about $24K$ frames)\ncaptured at $30 FPS$, annotated with both polygons for instance segmentation\nand bounding boxes for object detection, employed for testing purposes. We\ntrain various versions of YOLOv8 for instance segmentation on static images and\nassess their performance on the test dataset (videos). The best results were\nachieved by the YOLOv8-nano model (runnable on a portable device), with an\nmAP50 of $88.94%$ on the validation dataset and $81.21%$ macro average on the\ntest dataset. The results provide a baseline for future research in rip current\nsegmentation. Our work contributes to the existing literature by introducing a\ndetailed, annotated dataset, and training a deep learning model for instance\nsegmentation of rip currents. The code, training details and the annotated\ndataset are made publicly available at https://github.com/Irikos/rip_currents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02778", "pdf": "https://arxiv.org/pdf/2504.02778", "abs": "https://arxiv.org/abs/2504.02778", "authors": ["Vincent Gbouna Zakka", "Luis J. Manso", "Zhuangzhuang Dai"], "title": "Multi-Head Adaptive Graph Convolution Network for Sparse Point Cloud-Based Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human activity recognition is increasingly vital for supporting independent\nliving, particularly for the elderly and those in need of assistance. Domestic\nservice robots with monitoring capabilities can enhance safety and provide\nessential support. Although image-based methods have advanced considerably in\nthe past decade, their adoption remains limited by concerns over privacy and\nsensitivity to low-light or dark conditions. As an alternative, millimetre-wave\n(mmWave) radar can produce point cloud data which is privacy-preserving.\nHowever, processing the sparse and noisy point clouds remains a long-standing\nchallenge. While graph-based methods and attention mechanisms show promise,\nthey predominantly rely on \"fixed\" kernels; kernels that are applied uniformly\nacross all neighbourhoods, highlighting the need for adaptive approaches that\ncan dynamically adjust their kernels to the specific geometry of each local\nneighbourhood in point cloud data. To overcome this limitation, we introduce an\nadaptive approach within the graph convolutional framework. Instead of a single\nshared weight function, our Multi-Head Adaptive Kernel (MAK) module generates\nmultiple dynamic kernels, each capturing different aspects of the local feature\nspace. By progressively refining local features while maintaining global\nspatial context, our method enables convolution kernels to adapt to varying\nlocal features. Experimental results on benchmark datasets confirm the\neffectiveness of our approach, achieving state-of-the-art performance in human\nactivity recognition. Our source code is made publicly available at:\nhttps://github.com/Gbouna/MAK-GCN", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02823", "pdf": "https://arxiv.org/pdf/2504.02823", "abs": "https://arxiv.org/abs/2504.02823", "authors": ["Divya Velayudhan", "Abdelfatah Ahmed", "Mohamad Alansari", "Neha Gour", "Abderaouf Behouch", "Taimur Hassan", "Syed Talal Wasim", "Nabil Maalej", "Muzammal Naseer", "Juergen Gall", "Mohammed Bennamoun", "Ernesto Damiani", "Naoufel Werghi"], "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted at CVPR 2025", "summary": "Advancements in Computer-Aided Screening (CAS) systems are essential for\nimproving the detection of security threats in X-ray baggage scans. However,\ncurrent datasets are limited in representing real-world, sophisticated threats\nand concealment tactics, and existing approaches are constrained by a\nclosed-set paradigm with predefined labels. To address these challenges, we\nintroduce STCray, the first multimodal X-ray baggage security dataset,\ncomprising 46,642 image-caption paired scans across 21 threat categories,\ngenerated using an X-ray scanner for airport security. STCray is meticulously\ndeveloped with our specialized protocol that ensures domain-aware, coherent\ncaptions, that lead to the multi-modal instruction following data in X-ray\nbaggage security. This allows us to train a domain-aware visual AI assistant\nnamed STING-BEE that supports a range of vision-language tasks, including scene\ncomprehension, referring threat localization, visual grounding, and visual\nquestion answering (VQA), establishing novel baselines for multi-modal learning\nin X-ray baggage security. Further, STING-BEE shows state-of-the-art\ngeneralization in cross-domain settings. Code, data, and models are available\nat https://divs1159.github.io/STING-BEE/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02828", "pdf": "https://arxiv.org/pdf/2504.02828", "abs": "https://arxiv.org/abs/2504.02828", "authors": ["Jinqi Luo", "Tianjiao Ding", "Kwan Ho Ryan Chan", "Hancheng Min", "Chris Callison-Burch", "René Vidal"], "title": "Concept Lancet: Image Editing with Compositional Representation Transplant", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in CVPR 2025. Project page at\n  https://peterljq.github.io/project/colan", "summary": "Diffusion models are widely used for image editing tasks. Existing editing\nmethods often design a representation manipulation procedure by curating an\nedit direction in the text embedding or score space. However, such a procedure\nfaces a key challenge: overestimating the edit strength harms visual\nconsistency while underestimating it fails the editing task. Notably, each\nsource image may require a different editing strength, and it is costly to\nsearch for an appropriate strength via trial-and-error. To address this\nchallenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play\nframework for principled representation manipulation in diffusion-based image\nediting. At inference time, we decompose the source input in the latent (text\nembedding or diffusion score) space as a sparse linear combination of the\nrepresentations of the collected visual concepts. This allows us to accurately\nestimate the presence of concepts in each image, which informs the edit. Based\non the editing task (replace/add/remove), we perform a customized concept\ntransplant process to impose the corresponding editing direction. To\nsufficiently model the concept space, we curate a conceptual representation\ndataset, CoLan-150K, which contains diverse descriptions and scenarios of\nvisual terms and phrases for the latent dictionary. Experiments on multiple\ndiffusion-based image editing baselines show that methods equipped with CoLan\nachieve state-of-the-art performance in editing effectiveness and consistency\npreservation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02084", "pdf": "https://arxiv.org/pdf/2504.02084", "abs": "https://arxiv.org/abs/2504.02084", "authors": ["Nick Chodura", "Melissa Greeff", "Joshua Woods"], "title": "Evaluation of Flight Parameters in UAV-based 3D Reconstruction for Rooftop Infrastructure Assessment", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "8 pages, 6 figures, 2 tables", "summary": "Rooftop 3D reconstruction using UAV-based photogrammetry offers a promising\nsolution for infrastructure assessment, but existing methods often require high\npercentages of image overlap and extended flight times to ensure model accuracy\nwhen using autonomous flight paths. This study systematically evaluates key\nflight parameters-ground sampling distance (GSD) and image overlap-to optimize\nthe 3D reconstruction of complex rooftop infrastructure. Controlled UAV flights\nwere conducted over a multi-segment rooftop at Queen's University using a DJI\nPhantom 4 Pro V2, with varied GSD and overlap settings. The collected data were\nprocessed using Reality Capture software and evaluated against ground truth\nmodels generated from UAV-based LiDAR and terrestrial laser scanning (TLS).\nExperimental results indicate that a GSD range of 0.75-1.26 cm combined with\n85% image overlap achieves a high degree of model accuracy, while minimizing\nimages collected and flight time. These findings provide guidance for planning\nautonomous UAV flight paths for efficient rooftop assessments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02161", "pdf": "https://arxiv.org/pdf/2504.02161", "abs": "https://arxiv.org/abs/2504.02161", "authors": ["Zhen Meng", "Kan Chen", "Xiangmin Xu", "Erwin Jose Lopez Pulgarin", "Emma Li", "Philip G. Zhao", "David Flynn"], "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025", "summary": "Active 3D scene representation is pivotal in modern robotics applications,\nincluding remote inspection, manipulation, and telepresence. Traditional\nmethods primarily optimize geometric fidelity or rendering accuracy, but often\noverlook operator-specific objectives, such as safety-critical coverage or\ntask-driven viewpoints. This limitation leads to suboptimal viewpoint\nselection, particularly in constrained environments such as nuclear\ndecommissioning. To bridge this gap, we introduce a novel framework that\nintegrates expert operator preferences into the active 3D scene representation\npipeline. Specifically, we employ Reinforcement Learning from Human Feedback\n(RLHF) to guide robotic path planning, reshaping the reward function based on\nexpert input. To capture operator-specific priorities, we conduct interactive\nchoice experiments that evaluate user preferences in 3D scene representation.\nWe validate our framework using a UR3e robotic arm for reactor tile inspection\nin a nuclear decommissioning scenario. Compared to baseline methods, our\napproach enhances scene representation while optimizing trajectory efficiency.\nThe RLHF-based policy consistently outperforms random selection, prioritizing\ntask-critical details. By unifying explicit 3D geometric modeling with implicit\nhuman-in-the-loop optimization, this work establishes a foundation for\nadaptive, safety-critical robotic perception systems, paving the way for\nenhanced automation in nuclear decommissioning, remote maintenance, and other\nhigh-risk environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02216", "pdf": "https://arxiv.org/pdf/2504.02216", "abs": "https://arxiv.org/abs/2504.02216", "authors": ["Samuel Fernández-Menduiña", "Eduardo Pavez", "Antonio Ortega"], "title": "Image Coding for Machines via Feature-Preserving Rate-Distortion Optimization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Many images and videos are primarily processed by computer vision algorithms,\ninvolving only occasional human inspection. When this content requires\ncompression before processing, e.g., in distributed applications, coding\nmethods must optimize for both visual quality and downstream task performance.\nWe first show that, given the features obtained from the original and the\ndecoded images, an approach to reduce the effect of compression on a task loss\nis to perform rate-distortion optimization (RDO) using the distance between\nfeatures as a distortion metric. However, optimizing directly such a\nrate-distortion trade-off requires an iterative workflow of encoding, decoding,\nand feature evaluation for each coding parameter, which is computationally\nimpractical. We address this problem by simplifying the RDO formulation to make\nthe distortion term computable using block-based encoders. We first apply\nTaylor's expansion to the feature extractor, recasting the feature distance as\na quadratic metric with the Jacobian matrix of the neural network. Then, we\nreplace the linearized metric with a block-wise approximation, which we call\ninput-dependent squared error (IDSE). To reduce computational complexity, we\napproximate IDSE using Jacobian sketches. The resulting loss can be evaluated\nblock-wise in the transform domain and combined with the sum of squared errors\n(SSE) to address both visual quality and computer vision performance.\nSimulations with AVC across multiple feature extractors and downstream neural\nnetworks show up to 10% bit-rate savings for the same computer vision accuracy\ncompared to RDO based on SSE, with no decoder complexity overhead and just a 7%\nencoder complexity increase.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02280", "pdf": "https://arxiv.org/pdf/2504.02280", "abs": "https://arxiv.org/abs/2504.02280", "authors": ["YiMing Yu", "Jason Zutty"], "title": "LLM-Guided Evolution: An Autonomous Model Optimization for Object Detection", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "In machine learning, Neural Architecture Search (NAS) requires domain\nknowledge of model design and a large amount of trial-and-error to achieve\npromising performance. Meanwhile, evolutionary algorithms have traditionally\nrelied on fixed rules and pre-defined building blocks. The Large Language Model\n(LLM)-Guided Evolution (GE) framework transformed this approach by\nincorporating LLMs to directly modify model source code for image\nclassification algorithms on CIFAR data and intelligently guide mutations and\ncrossovers. A key element of LLM-GE is the \"Evolution of Thought\" (EoT)\ntechnique, which establishes feedback loops, allowing LLMs to refine their\ndecisions iteratively based on how previous operations performed. In this\nstudy, we perform NAS for object detection by improving LLM-GE to modify the\narchitecture of You Only Look Once (YOLO) models to enhance performance on the\nKITTI dataset. Our approach intelligently adjusts the design and settings of\nYOLO to find the optimal algorithms against objective such as detection\naccuracy and speed. We show that LLM-GE produced variants with significant\nperformance improvements, such as an increase in Mean Average Precision from\n92.5% to 94.5%. This result highlights the flexibility and effectiveness of\nLLM-GE on real-world challenges, offering a novel paradigm for automated\nmachine learning that combines LLM-driven reasoning with evolutionary\nstrategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02329", "pdf": "https://arxiv.org/pdf/2504.02329", "abs": "https://arxiv.org/abs/2504.02329", "authors": ["Seif Mzoughi", "Ahmed Hajyahmed", "Mohamed Elshafei", "Foutse Khomh anb Diego Elias Costa"], "title": "Towards Assessing Deep Learning Test Input Generators", "categories": ["cs.LG", "cs.CV", "cs.SE"], "comment": "Accepted to EASE 2025", "summary": "Deep Learning (DL) systems are increasingly deployed in safety-critical\napplications, yet they remain vulnerable to robustness issues that can lead to\nsignificant failures. While numerous Test Input Generators (TIGs) have been\ndeveloped to evaluate DL robustness, a comprehensive assessment of their\neffectiveness across different dimensions is still lacking. This paper presents\na comprehensive assessment of four state-of-the-art TIGs--DeepHunter,\nDeepFault, AdvGAN, and SinVAD--across multiple critical aspects:\nfault-revealing capability, naturalness, diversity, and efficiency. Our\nempirical study leverages three pre-trained models (LeNet-5, VGG16, and\nEfficientNetB3) on datasets of varying complexity (MNIST, CIFAR-10, and\nImageNet-1K) to evaluate TIG performance. Our findings reveal important\ntrade-offs in robustness revealing capability, variation in test case\ngeneration, and computational efficiency across TIGs. The results also show\nthat TIG performance varies significantly with dataset complexity, as tools\nthat perform well on simpler datasets may struggle with more complex ones. In\ncontrast, others maintain steadier performance or better scalability. This\npaper offers practical guidance for selecting appropriate TIGs aligned with\nspecific objectives and dataset characteristics. Nonetheless, more work is\nneeded to address TIG limitations and advance TIGs for real-world,\nsafety-critical systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02361", "pdf": "https://arxiv.org/pdf/2504.02361", "abs": "https://arxiv.org/abs/2504.02361", "authors": ["Takahiro Shirakawa", "Tomoyuki Suzuki", "Daichi Haraguchi"], "title": "MG-Gen: Single Image to Motion Graphics Generation with Layer Decomposition", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "General image-to-video generation methods often produce suboptimal animations\nthat do not meet the requirements of animated graphics, as they lack active\ntext motion and exhibit object distortion. Also, code-based animation\ngeneration methods typically require layer-structured vector data which are\noften not readily available for motion graphic generation. To address these\nchallenges, we propose a novel framework named MG-Gen that reconstructs data in\nvector format from a single raster image to extend the capabilities of\ncode-based methods to enable motion graphics generation from a raster image in\nthe framework of general image-to-video generation. MG-Gen first decomposes the\ninput image into layer-wise elements, reconstructs them as HTML format data and\nthen generates executable JavaScript code for the reconstructed HTML data. We\nexperimentally confirm that \\ours{} generates motion graphics while preserving\ntext readability and input consistency. These successful results indicate that\ncombining layer decomposition and animation code generation is an effective\nstrategy for motion graphics generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "code generation"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02628", "pdf": "https://arxiv.org/pdf/2504.02628", "abs": "https://arxiv.org/abs/2504.02628", "authors": ["Chu Han", "Bingchao Zhao", "Jiatai Lin", "Shanshan Lyu", "Longfei Wang", "Tianpeng Deng", "Cheng Lu", "Changhong Liang", "Hannah Y. Wen", "Xiaojing Guo", "Zhenwei Shi", "Zaiyi Liu"], "title": "Towards Computation- and Communication-efficient Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Despite the impressive performance across a wide range of applications,\ncurrent computational pathology models face significant diagnostic efficiency\nchallenges due to their reliance on high-magnification whole-slide image\nanalysis. This limitation severely compromises their clinical utility,\nespecially in time-sensitive diagnostic scenarios and situations requiring\nefficient data transfer. To address these issues, we present a novel\ncomputation- and communication-efficient framework called Magnification-Aligned\nGlobal-Local Transformer (MAGA-GLTrans). Our approach significantly reduces\ncomputational time, file transfer requirements, and storage overhead by\nenabling effective analysis using low-magnification inputs rather than\nhigh-magnification ones. The key innovation lies in our proposed magnification\nalignment (MAGA) mechanism, which employs self-supervised learning to bridge\nthe information gap between low and high magnification levels by effectively\naligning their feature representations. Through extensive evaluation across\nvarious fundamental CPath tasks, MAGA-GLTrans demonstrates state-of-the-art\nclassification performance while achieving remarkable efficiency gains: up to\n10.7 times reduction in computational time and over 20 times reduction in file\ntransfer and storage requirements. Furthermore, we highlight the versatility of\nour MAGA framework through two significant extensions: (1) its applicability as\na feature extractor to enhance the efficiency of any CPath architecture, and\n(2) its compatibility with existing foundation models and\nhistopathology-specific encoders, enabling them to process low-magnification\ninputs with minimal information loss. These advancements position MAGA-GLTrans\nas a particularly promising solution for time-sensitive applications,\nespecially in the context of intraoperative frozen section diagnosis where both\naccuracy and efficiency are paramount.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02091", "pdf": "https://arxiv.org/pdf/2504.02091", "abs": "https://arxiv.org/abs/2504.02091", "authors": ["Joseph Heffner", "Chongyu Qin", "Martin Chadwick", "Chris Knutsen", "Christopher Summerfield", "Zeb Kurth-Nelson", "Robb B. Rutledge"], "title": "Increasing happiness through conversations with artificial intelligence", "categories": ["cs.CL"], "comment": "26 pages, 4 figures", "summary": "Chatbots powered by artificial intelligence (AI) have rapidly become a\nsignificant part of everyday life, with over a quarter of American adults using\nthem multiple times per week. While these tools offer potential benefits and\nrisks, a fundamental question remains largely unexplored: How do conversations\nwith AI influence subjective well-being? To investigate this, we conducted a\nstudy where participants either engaged in conversations with an AI chatbot (N\n= 334) or wrote journal entires (N = 193) on the same randomly assigned topics\nand reported their momentary happiness afterward. We found that happiness after\nAI chatbot conversations was higher than after journaling, particularly when\ndiscussing negative topics such as depression or guilt. Leveraging large\nlanguage models for sentiment analysis, we found that the AI chatbot mirrored\nparticipants' sentiment while maintaining a consistent positivity bias. When\ndiscussing negative topics, participants gradually aligned their sentiment with\nthe AI's positivity, leading to an overall increase in happiness. We\nhypothesized that the history of participants' sentiment prediction errors, the\ndifference between expected and actual emotional tone when responding to the AI\nchatbot, might explain this happiness effect. Using computational modeling, we\nfind the history of these sentiment prediction errors over the course of a\nconversation predicts greater post-conversation happiness, demonstrating a\ncentral role of emotional expectations during dialogue. Our findings underscore\nthe effect that AI interactions can have on human well-being.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02116", "pdf": "https://arxiv.org/pdf/2504.02116", "abs": "https://arxiv.org/abs/2504.02116", "authors": ["Xiulin Yang"], "title": "Language Models at the Syntax-Semantics Interface: A Case Study of the Long-Distance Binding of Chinese Reflexive ziji", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores whether language models can effectively resolve the\ncomplex binding patterns of the Mandarin Chinese reflexive ziji, which are\nconstrained by both syntactic and semantic factors. We construct a dataset of\n240 synthetic sentences using templates and examples from syntactic literature,\nalong with 320 natural sentences from the BCC corpus. Evaluating 21 language\nmodels against this dataset and comparing their performance to judgments from\nnative Mandarin speakers, we find that none of the models consistently\nreplicates human-like judgments. The results indicate that existing language\nmodels tend to rely heavily on sequential cues, though not always favoring the\nclosest strings, and often overlooking subtle semantic and syntactic\nconstraints. They tend to be more sensitive to noun-related than verb-related\nsemantics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02154", "pdf": "https://arxiv.org/pdf/2504.02154", "abs": "https://arxiv.org/abs/2504.02154", "authors": ["Chao Huang", "Susan Liang", "Yunlong Tang", "Li Ma", "Yapeng Tian", "Chenliang Xu"], "title": "FreSca: Unveiling the Scaling Space in Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://wikichao.github.io/FreSca/", "summary": "Diffusion models offer impressive controllability for image tasks, primarily\nthrough noise predictions that encode task-specific information and\nclassifier-free guidance enabling adjustable scaling. This scaling mechanism\nimplicitly defines a ``scaling space'' whose potential for fine-grained\nsemantic manipulation remains underexplored. We investigate this space,\nstarting with inversion-based editing where the difference between\nconditional/unconditional noise predictions carries key semantic information.\nOur core contribution stems from a Fourier analysis of noise predictions,\nrevealing that its low- and high-frequency components evolve differently\nthroughout diffusion. Based on this insight, we introduce FreSca, a\nstraightforward method that applies guidance scaling independently to different\nfrequency bands in the Fourier domain. FreSca demonstrably enhances existing\nimage editing methods without retraining. Excitingly, its effectiveness extends\nto image understanding tasks such as depth estimation, yielding quantitative\ngains across multiple datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02160", "pdf": "https://arxiv.org/pdf/2504.02160", "abs": "https://arxiv.org/abs/2504.02160", "authors": ["Shaojin Wu", "Mengqi Huang", "Wenxu Wu", "Yufeng Cheng", "Fei Ding", "Qian He"], "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/UNO Code and model:\n  https://github.com/bytedance/UNO", "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02323", "pdf": "https://arxiv.org/pdf/2504.02323", "abs": "https://arxiv.org/abs/2504.02323", "authors": ["Clayton Cohn", "Nicole Hutchins", "Ashwin T S", "Gautam Biswas"], "title": "CoTAL: Human-in-the-Loop Prompt Engineering, Chain-of-Thought Reasoning, and Active Learning for Generalizable Formative Assessment Scoring", "categories": ["cs.CL"], "comment": "Submitted to IEEE Transactions on Learning Technologies. Currently\n  under review", "summary": "Large language models (LLMs) have created new opportunities to assist\nteachers and support student learning. Methods such as chain-of-thought (CoT)\nprompting enable LLMs to grade formative assessments in science, providing\nscores and relevant feedback to students. However, the extent to which these\nmethods generalize across curricula in multiple domains (such as science,\ncomputing, and engineering) remains largely untested. In this paper, we\nintroduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based\napproach to formative assessment scoring that (1) leverages Evidence-Centered\nDesign (ECD) principles to develop curriculum-aligned formative assessments and\nrubrics, (2) applies human-in-the-loop prompt engineering to automate response\nscoring, and (3) incorporates teacher and student feedback to iteratively\nrefine assessment questions, grading rubrics, and LLM prompts for automated\ngrading. Our findings demonstrate that CoTAL improves GPT-4's scoring\nperformance, achieving gains of up to 24.5% over a non-prompt-engineered\nbaseline. Both teachers and students view CoTAL as effective in scoring and\nexplaining student responses, each providing valuable refinements to enhance\ngrading accuracy and explanation quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02264", "pdf": "https://arxiv.org/pdf/2504.02264", "abs": "https://arxiv.org/abs/2504.02264", "authors": ["Wenzhuo Liu", "Wenshuo Wang", "Yicheng Qiao", "Qiannan Guo", "Jiayin Zhu", "Pengfei Li", "Zilong Chen", "Huiming Yang", "Zhiwei Li", "Lening Wang", "Tiao Tan", "Huaping Liu"], "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception", "categories": ["cs.CV"], "comment": null, "summary": "Advanced driver assistance systems require a comprehensive understanding of\nthe driver's mental/physical state and traffic context but existing works often\nneglect the potential benefits of joint learning between these tasks. This\npaper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework\nthat simultaneously recognizes driver behavior (e.g., looking around, talking),\ndriver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking,\nturning), and traffic context (e.g., traffic jam, traffic smooth). A key\nchallenge is avoiding negative transfer between tasks, which can impair\nlearning performance. To address this, we introduce two key components into the\nframework: one is the multi-axis region attention network to extract global\ncontext-sensitive features, and the other is the dual-branch multimodal\nembedding to learn multimodal embeddings from both task-shared and\ntask-specific features. The former uses a multi-attention mechanism to extract\ntask-relevant features, mitigating negative transfer caused by task-unrelated\nfeatures. The latter employs a dual-branch structure to adaptively adjust\ntask-shared and task-specific parameters, enhancing cross-task knowledge\ntransfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE\ndataset, using a series of ablation studies, and show that it outperforms\nstate-of-the-art methods across all four tasks. The code is available on\nhttps://github.com/Wenzhuo-Liu/MMTL-UniAD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02441", "pdf": "https://arxiv.org/pdf/2504.02441", "abs": "https://arxiv.org/abs/2504.02441", "authors": ["Lianlei Shan", "Shixian Luo", "Zezhou Zhu", "Yu Yuan", "Yong Wu"], "title": "Cognitive Memory in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 9 figures", "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02312", "pdf": "https://arxiv.org/pdf/2504.02312", "abs": "https://arxiv.org/abs/2504.02312", "authors": ["Xiaoda Yang", "Jiayang Xu", "Kaixuan Luan", "Xinyu Zhan", "Hongshun Qiu", "Shijun Shi", "Hao Li", "Shuai Yang", "Li Zhang", "Checheng Yu", "Cewu Lu", "Lixin Yang"], "title": "OmniCam: Unified Multimodal Video Generation via Camera Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Camera control, which achieves diverse visual effects by changing camera\nposition and pose, has attracted widespread attention. However, existing\nmethods face challenges such as complex interaction and limited control\ncapabilities. To address these issues, we present OmniCam, a unified multimodal\ncamera control framework. Leveraging large language models and video diffusion\nmodels, OmniCam generates spatio-temporally consistent videos. It supports\nvarious combinations of input modalities: the user can provide text or video\nwith expected trajectory as camera path guidance, and image or video as content\nreference, enabling precise control over camera motion. To facilitate the\ntraining of OmniCam, we introduce the OmniTr dataset, which contains a large\ncollection of high-quality long-sequence trajectories, videos, and\ncorresponding descriptions. Experimental results demonstrate that our model\nachieves state-of-the-art performance in high-quality camera-controlled video\ngeneration across various metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02316", "pdf": "https://arxiv.org/pdf/2504.02316", "abs": "https://arxiv.org/abs/2504.02316", "authors": ["Yuan Zhou", "Shilong Jin", "Litao Hua", "Wanjun Lv", "Haoran Duan", "Jungong Han"], "title": "ConsDreamer: Advancing Multi-View Consistency for Zero-Shot Text-to-3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 11 figures, 3 tables", "summary": "Recent advances in zero-shot text-to-3D generation have revolutionized 3D\ncontent creation by enabling direct synthesis from textual descriptions. While\nstate-of-the-art methods leverage 3D Gaussian Splatting with score distillation\nto enhance multi-view rendering through pre-trained text-to-image (T2I) models,\nthey suffer from inherent view biases in T2I priors. These biases lead to\ninconsistent 3D generation, particularly manifesting as the multi-face Janus\nproblem, where objects exhibit conflicting features across views. To address\nthis fundamental challenge, we propose ConsDreamer, a novel framework that\nmitigates view bias by refining both the conditional and unconditional terms in\nthe score distillation process: (1) a View Disentanglement Module (VDM) that\neliminates viewpoint biases in conditional prompts by decoupling irrelevant\nview components and injecting precise camera parameters; and (2) a\nsimilarity-based partial order loss that enforces geometric consistency in the\nunconditional term by aligning cosine similarities with azimuth relationships.\nExtensive experiments demonstrate that ConsDreamer effectively mitigates the\nmulti-face Janus problem in text-to-3D generation, outperforming existing\nmethods in both visual quality and consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02318", "pdf": "https://arxiv.org/pdf/2504.02318", "abs": "https://arxiv.org/abs/2504.02318", "authors": ["Samuel Clarke", "Suzannah Wistreich", "Yanjie Ze", "Jiajun Wu"], "title": "X-Capture: An Open-Source Portable Device for Multi-Sensory Learning", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://xcapture.github.io/", "summary": "Understanding objects through multiple sensory modalities is fundamental to\nhuman perception, enabling cross-sensory integration and richer comprehension.\nFor AI and robotic systems to replicate this ability, access to diverse,\nhigh-quality multi-sensory data is critical. Existing datasets are often\nlimited by their focus on controlled environments, simulated objects, or\nrestricted modality pairings. We introduce X-Capture, an open-source, portable,\nand cost-effective device for real-world multi-sensory data collection, capable\nof capturing correlated RGBD images, tactile readings, and impact audio. With a\nbuild cost under $1,000, X-Capture democratizes the creation of multi-sensory\ndatasets, requiring only consumer-grade tools for assembly. Using X-Capture, we\ncurate a sample dataset of 3,000 total points on 500 everyday objects from\ndiverse, real-world environments, offering both richness and variety. Our\nexperiments demonstrate the value of both the quantity and the sensory breadth\nof our data for both pretraining and fine-tuning multi-modal representations\nfor object-centric tasks such as cross-sensory retrieval and reconstruction.\nX-Capture lays the groundwork for advancing human-like sensory representations\nin AI, emphasizing scalability, accessibility, and real-world applicability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02604", "pdf": "https://arxiv.org/pdf/2504.02604", "abs": "https://arxiv.org/abs/2504.02604", "authors": ["Hedi Naouara", "Jean-Pierre Lorré", "Jérôme Louradour"], "title": "LinTO Audio and Textual Datasets to Train and Evaluate Automatic Speech Recognition in Tunisian Arabic Dialect", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing Automatic Speech Recognition (ASR) systems for Tunisian Arabic\nDialect is challenging due to the dialect's linguistic complexity and the\nscarcity of annotated speech datasets. To address these challenges, we propose\nthe LinTO audio and textual datasets -- comprehensive resources that capture\nphonological and lexical features of Tunisian Arabic Dialect. These datasets\ninclude a variety of texts from numerous sources and real-world audio samples\nfeaturing diverse speakers and code-switching between Tunisian Arabic Dialect\nand English or French. By providing high-quality audio paired with precise\ntranscriptions, the LinTO audio and textual datasets aim to provide qualitative\nmaterial to build and benchmark ASR systems for the Tunisian Arabic Dialect.\n  Keywords -- Tunisian Arabic Dialect, Speech-to-Text, Low-Resource Languages,\nAudio Data Augmentation", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02328", "pdf": "https://arxiv.org/pdf/2504.02328", "abs": "https://arxiv.org/abs/2504.02328", "authors": ["Congpei Qiu", "Yanhao Wu", "Wei Ke", "Xiuxiu Bai", "Tong Zhang"], "title": "Refining CLIP's Spatial Awareness: A Visual-Centric Perspective", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in global alignment\nwith language but exhibits limited sensitivity to spatial information, leading\nto strong performance in zero-shot classification tasks but underperformance in\ntasks requiring precise spatial understanding. Recent approaches have\nintroduced Region-Language Alignment (RLA) to enhance CLIP's performance in\ndense multimodal tasks by aligning regional visual representations with\ncorresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA\nsuffer from notable loss in spatial awareness, which is crucial for dense\nprediction tasks. To address this, we propose the Spatial Correlation\nDistillation (SCD) framework, which preserves CLIP's inherent spatial structure\nand mitigates the above degradation. To further enhance spatial correlations,\nwe introduce a lightweight Refiner that extracts refined correlations directly\nfrom CLIP before feeding them into SCD, based on an intriguing finding that\nCLIP naturally captures high-quality dense features. Together, these components\nform a robust distillation framework that enables CLIP ViTs to integrate both\nvisual-language and visual-centric improvements, achieving state-of-the-art\nresults across various open-vocabulary dense prediction benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02674", "pdf": "https://arxiv.org/pdf/2504.02674", "abs": "https://arxiv.org/abs/2504.02674", "authors": ["Jacqueline Rowe", "Edward Gow-Smith", "Mark Hepple"], "title": "Limitations of Religious Data and the Importance of the Target Domain: Towards Machine Translation for Guinea-Bissau Creole", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 7 tables. To be published in Proceedings of the\n  8th Workshop on Technologies for Machine Translation of Low-Resource\n  Languages (NAACL 2025)", "summary": "We introduce a new dataset for machine translation of Guinea-Bissau Creole\n(Kiriol), comprising around 40 thousand parallel sentences to English and\nPortuguese. This dataset is made up of predominantly religious data (from the\nBible and texts from the Jehovah's Witnesses), but also a small amount of\ngeneral domain data (from a dictionary). This mirrors the typical resource\navailability of many low resource languages. We train a number of\ntransformer-based models to investigate how to improve domain transfer from\nreligious data to a more general domain. We find that adding even 300 sentences\nfrom the target domain when training substantially improves the translation\nperformance, highlighting the importance and need for data collection for\nlow-resource languages, even on a small-scale. We additionally find that\nPortuguese-to-Kiriol translation models perform better on average than other\nsource and target language pairs, and investigate how this relates to the\nmorphological complexity of the languages involved and the degree of lexical\noverlap between creoles and lexifiers. Overall, we hope our work will stimulate\nresearch into Kiriol and into how machine translation might better support\ncreole languages in general.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02337", "pdf": "https://arxiv.org/pdf/2504.02337", "abs": "https://arxiv.org/abs/2504.02337", "authors": ["Ming-Jia Yang", "Yu-Xiao Guo", "Yang Liu", "Bin Zhou", "Xin Tong"], "title": "LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic, room-level indoor scenes with semantically plausible\nand detailed appearances from in-the-wild images is crucial for various\napplications in VR, AR, and robotics. The success of NeRF-based generative\nmethods indicates a promising direction to address this challenge. However,\nunlike their success at the object level, existing scene-level generative\nmethods require additional information, such as multiple views, depth images,\nor semantic guidance, rather than relying solely on RGB images. This is because\nNeRF-based methods necessitate prior knowledge of camera poses, which is\nchallenging to approximate for indoor scenes due to the complexity of defining\nalignment and the difficulty of globally estimating poses from a single image,\ngiven the unseen parts behind the camera. To address this challenge, we\nredefine global poses within the framework of Local-Pose-Alignment (LPA) -- an\nanchor-based multi-local-coordinate system that uses a selected number of\nanchors as the roots of these coordinates. Building on this foundation, we\nintroduce LPA-GAN, a novel NeRF-based generative approach that incorporates\nspecific modifications to estimate the priors of camera poses under LPA. It\nalso co-optimizes the pose predictor and scene generation processes. Our\nablation study and comparisons with straightforward extensions of NeRF-based\nobject generative methods demonstrate the effectiveness of our approach.\nFurthermore, visual comparisons with other techniques reveal that our method\nachieves superior view-to-view consistency and semantic normality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02708", "pdf": "https://arxiv.org/pdf/2504.02708", "abs": "https://arxiv.org/abs/2504.02708", "authors": ["Nikhil Verma", "Manasa Bharadwaj"], "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context", "categories": ["cs.CL"], "comment": "14 pages, 11 Figures, 2 Tables, currently under review at ACL 2025", "summary": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02345", "pdf": "https://arxiv.org/pdf/2504.02345", "abs": "https://arxiv.org/abs/2504.02345", "authors": ["Masakazu Yoshimura", "Junji Otsuka", "Radu Berdan", "Takeshi Ohashi"], "title": "SemiISP/SemiIE: Semi-Supervised Image Signal Processor and Image Enhancement Leveraging One-to-Many Mapping sRGB-to-RAW", "categories": ["cs.CV"], "comment": null, "summary": "DNN-based methods have been successful in Image Signal Processor (ISP) and\nimage enhancement (IE) tasks. However, the cost of creating training data for\nthese tasks is considerably higher than for other tasks, making it difficult to\nprepare large-scale datasets. Also, creating personalized ISP and IE with\nminimal training data can lead to new value streams since preferred image\nquality varies depending on the person and use case. While semi-supervised\nlearning could be a potential solution in such cases, it has rarely been\nutilized for these tasks. In this paper, we realize semi-supervised learning\nfor ISP and IE leveraging a RAW image reconstruction (sRGB-to-RAW) method.\nAlthough existing sRGB-to-RAW methods can generate pseudo-RAW image datasets\nthat improve the accuracy of RAW-based high-level computer vision tasks such as\nobject detection, their quality is not sufficient for ISP and IE tasks that\nrequire precise image quality definition. Therefore, we also propose a\nsRGB-to-RAW method that can improve the image quality of these tasks. The\nproposed semi-supervised learning with the proposed sRGB-to-RAW method\nsuccessfully improves the image quality of various models on various datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02725", "pdf": "https://arxiv.org/pdf/2504.02725", "abs": "https://arxiv.org/abs/2504.02725", "authors": ["Kehua Feng", "Keyan Ding", "Jing Yu", "Menghan Li", "Yuhao Wang", "Tong Xu", "Xinda Wang", "Qiang Zhang", "Huajun Chen"], "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02356", "pdf": "https://arxiv.org/pdf/2504.02356", "abs": "https://arxiv.org/abs/2504.02356", "authors": ["Janghyun Kim", "Minseong Kweon", "Jinsun Park", "Ukcheol Shin"], "title": "All-day Depth Completion via Thermal-LiDAR Fusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth completion, which estimates dense depth from sparse LiDAR and RGB\nimages, has demonstrated outstanding performance in well-lit conditions.\nHowever, due to the limitations of RGB sensors, existing methods often struggle\nto achieve reliable performance in harsh environments, such as heavy rain and\nlow-light conditions. Furthermore, we observe that ground truth depth maps\noften suffer from large missing measurements in adverse weather conditions such\nas heavy rain, leading to insufficient supervision. In contrast, thermal\ncameras are known for providing clear and reliable visibility in such\nconditions, yet research on thermal-LiDAR depth completion remains\nunderexplored. Moreover, the characteristics of thermal images, such as\nblurriness, low contrast, and noise, bring unclear depth boundary problems. To\naddress these challenges, we first evaluate the feasibility and robustness of\nthermal-LiDAR depth completion across diverse lighting (eg., well-lit,\nlow-light), weather (eg., clear-sky, rainy), and environment (eg., indoor,\noutdoor) conditions, by conducting extensive benchmarks on the MS$^2$ and ViViD\ndatasets. In addition, we propose a framework that utilizes COntrastive\nlearning and Pseudo-Supervision (COPS) to enhance depth boundary clarity and\nimprove completion accuracy by leveraging a depth foundation model in two key\nways. First, COPS enforces a depth-aware contrastive loss between different\ndepth points by mining positive and negative samples using a monocular depth\nfoundation model to sharpen depth boundaries. Second, it mitigates the issue of\nincomplete supervision from ground truth depth maps by leveraging foundation\nmodel predictions as dense depth priors. We also provide in-depth analyses of\nthe key challenges in thermal-LiDAR depth completion to aid in understanding\nthe task and encourage future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02362", "pdf": "https://arxiv.org/pdf/2504.02362", "abs": "https://arxiv.org/abs/2504.02362", "authors": ["Haodian Wang", "Long Peng", "Yuejin Sun", "Zengyu Wan", "Yang Wang", "Yang Cao"], "title": "Brightness Perceiving for Recursive Low-Light Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to the wide dynamic range in real low-light scenes, there will be large\ndifferences in the degree of contrast degradation and detail blurring of\ncaptured images, making it difficult for existing end-to-end methods to enhance\nlow-light images to normal exposure. To address the above issue, we decompose\nlow-light image enhancement into a recursive enhancement task and propose a\nbrightness-perceiving-based recursive enhancement framework for high dynamic\nrange low-light image enhancement. Specifically, our recursive enhancement\nframework consists of two parallel sub-networks: Adaptive Contrast and Texture\nenhancement network (ACT-Net) and Brightness Perception network (BP-Net). The\nACT-Net is proposed to adaptively enhance image contrast and details under the\nguidance of the brightness adjustment branch and gradient adjustment branch,\nwhich are proposed to perceive the degradation degree of contrast and details\nin low-light images. To adaptively enhance images captured under different\nbrightness levels, BP-Net is proposed to control the recursive enhancement\ntimes of ACT-Net by exploring the image brightness distribution properties.\nFinally, in order to coordinate ACT-Net and BP-Net, we design a novel\nunsupervised training strategy to facilitate the training procedure. To further\nvalidate the effectiveness of the proposed method, we construct a new dataset\nwith a broader brightness distribution by mixing three low-light datasets.\nCompared with eleven existing representative methods, the proposed method\nachieves new SOTA performance on six reference and no reference metrics.\nSpecifically, the proposed method improves the PSNR by 0.9 dB compared to the\nexisting SOTA method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768", "abs": "https://arxiv.org/abs/2504.02768", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Arianna Bisazza"], "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages, 6 linguistic phenomena and containing\nmore than 125,000 minimal pairs. Our minimal pairs are created using a fully\nautomated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02386", "pdf": "https://arxiv.org/pdf/2504.02386", "abs": "https://arxiv.org/abs/2504.02386", "authors": ["Kim Sung-Bin", "Jeongsoo Choi", "Puyuan Peng", "Joon Son Chung", "Tae-Hyun Oh", "David Harwath"], "title": "VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models", "categories": ["cs.CV", "eess.AS"], "comment": "https://voicecraft-dub.github.io/", "summary": "We present VoiceCraft-Dub, a novel approach for automated video dubbing that\nsynthesizes high-quality speech from text and facial cues. This task has broad\napplications in filmmaking, multimedia creation, and assisting voice-impaired\nindividuals. Building on the success of Neural Codec Language Models (NCLMs)\nfor speech synthesis, our method extends their capabilities by incorporating\nvideo features, ensuring that synthesized speech is time-synchronized and\nexpressively aligned with facial movements while preserving natural prosody. To\ninject visual cues, we design adapters to align facial features with the NCLM\ntoken space and introduce audio-visual fusion layers to merge audio-visual\ninformation within the NCLM framework. Additionally, we curate CelebV-Dub, a\nnew dataset of expressive, real-world videos specifically designed for\nautomated video dubbing. Extensive experiments show that our model achieves\nhigh-quality, intelligible, and natural speech synthesis with accurate lip\nsynchronization, outperforming existing methods in human perception and\nperforming favorably in objective evaluations. We also adapt VoiceCraft-Dub for\nthe video-to-speech task, demonstrating its versatility for various\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02789", "pdf": "https://arxiv.org/pdf/2504.02789", "abs": "https://arxiv.org/abs/2504.02789", "authors": ["Karin de Langis", "Jong Inn Park", "Bin Hu", "Khanh Chi Le", "Andreas Schramm", "Michael C. Mensink", "Andrew Elfenbein", "Dongyeop Kang"], "title": "A Framework for Robust Cognitive Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emergent cognitive abilities in large language models (LLMs) have been widely\nobserved, but their nature and underlying mechanisms remain poorly understood.\nA growing body of research draws on cognitive science to investigate LLM\ncognition, but standard methodologies and experimen-tal pipelines have not yet\nbeen established. To address this gap we develop CognitivEval, a framework for\nsystematically evaluating the artificial cognitive capabilities of LLMs, with a\nparticular emphasis on robustness in response collection. The key features of\nCognitivEval include: (i) automatic prompt permutations, and (ii) testing that\ngathers both generations and model probability estimates. Our experiments\ndemonstrate that these features lead to more robust experimental outcomes.\nUsing CognitivEval, we replicate five classic experiments in cognitive science,\nillustrating the framework's generalizability across various experimental tasks\nand obtaining a cognitive profile of several state of the art LLMs.\nCognitivEval will be released publicly to foster broader collaboration within\nthe cognitive science community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02391", "pdf": "https://arxiv.org/pdf/2504.02391", "abs": "https://arxiv.org/abs/2504.02391", "authors": ["Laibin Chang", "Yunke Wang", "JiaXing Huang", "Longxiang Deng", "Bo Du", "Chang Xu"], "title": "Marine Saliency Segmenter: Object-Focused Conditional Diffusion with Region-Level Semantic Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Marine Saliency Segmentation (MSS) plays a pivotal role in various\nvision-based marine exploration tasks. However, existing marine segmentation\ntechniques face the dilemma of object mislocalization and imprecise boundaries\ndue to the complex underwater environment. Meanwhile, despite the impressive\nperformance of diffusion models in visual segmentation, there remains potential\nto further leverage contextual semantics to enhance feature learning of\nregion-level salient objects, thereby improving segmentation outcomes. Building\non this insight, we propose DiffMSS, a novel marine saliency segmenter based on\nthe diffusion model, which utilizes semantic knowledge distillation to guide\nthe segmentation of marine salient objects. Specifically, we design a\nregion-word similarity matching mechanism to identify salient terms at the word\nlevel from the text descriptions. These high-level semantic features guide the\nconditional feature learning network in generating salient and accurate\ndiffusion conditions with semantic knowledge distillation. To further refine\nthe segmentation of fine-grained structures in unique marine organisms, we\ndevelop the dedicated consensus deterministic sampling to suppress\noverconfident missegmentations. Comprehensive experiments demonstrate the\nsuperior performance of DiffMSS over state-of-the-art methods in both\nquantitative and qualitative evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02800", "pdf": "https://arxiv.org/pdf/2504.02800", "abs": "https://arxiv.org/abs/2504.02800", "authors": ["Zhuohan Ge", "Nicole Hu", "Darian Li", "Yubo Wang", "Shihao Qi", "Yuming Xu", "Han Shi", "Jason Zhang"], "title": "A Survey of Large Language Models in Mental Health Disorder Detection on Social Media", "categories": ["cs.CL", "I.2.7; J.3; J.4"], "comment": "13 pages, 4 figures", "summary": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02433", "pdf": "https://arxiv.org/pdf/2504.02433", "abs": "https://arxiv.org/abs/2504.02433", "authors": ["Zhongjian Wang", "Peng Zhang", "Jinwei Qi", "Guangyuan Wang Sheng Xu", "Bang Zhang", "Liefeng Bo"], "title": "OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication", "categories": ["cs.CV"], "comment": "Project Page https://humanaigc.github.io/omnitalker", "summary": "Recent years have witnessed remarkable advances in talking head generation,\nowing to its potential to revolutionize the human-AI interaction from text\ninterfaces into realistic video chats. However, research on text-driven talking\nheads remains underexplored, with existing methods predominantly adopting a\ncascaded pipeline that combines TTS systems with audio-driven talking head\nmodels. This conventional pipeline not only introduces system complexity and\nlatency overhead but also fundamentally suffers from asynchronous audiovisual\noutput and stylistic discrepancies between generated speech and visual\nexpressions. To address these limitations, we introduce OmniTalker, an\nend-to-end unified framework that simultaneously generates synchronized speech\nand talking head videos from text and reference video in real-time zero-shot\nscenarios, while preserving both speech style and facial styles. The framework\nemploys a dual-branch diffusion transformer architecture: the audio branch\nsynthesizes mel-spectrograms from text, while the visual branch predicts\nfine-grained head poses and facial dynamics. To bridge modalities, we introduce\na novel audio-visual fusion module that integrates cross-modal information to\nensure temporal synchronization and stylistic coherence between audio and\nvisual outputs. Furthermore, our in-context reference learning module\neffectively captures both speech and facial style characteristics from a single\nreference video without introducing an extra style extracting module. To the\nbest of our knowledge, OmniTalker presents the first unified framework that\njointly models speech style and facial style in a zero-shot setting, achieving\nreal-time inference speed of 25 FPS. Extensive experiments demonstrate that our\nmethod surpasses existing approaches in generation quality, particularly\nexcelling in style preservation and audio-video synchronization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02009", "pdf": "https://arxiv.org/pdf/2504.02009", "abs": "https://arxiv.org/abs/2504.02009", "authors": ["Zhonghang Li", "Lianghao Xia", "Xubin Ren", "Jiabin Tang", "Tianyi Chen", "Yong Xu", "Chao Huang"], "title": "Urban Computing in the Era of Large Language Models", "categories": ["cs.CY", "cs.CL"], "comment": "36 pages", "summary": "Urban computing has emerged as a multidisciplinary field that harnesses\ndata-driven technologies to address challenges and improve urban living.\nTraditional approaches, while beneficial, often face challenges with\ngeneralization, scalability, and contextual understanding. The advent of Large\nLanguage Models (LLMs) offers transformative potential in this domain. This\nsurvey explores the intersection of LLMs and urban computing, emphasizing the\nimpact of LLMs in processing and analyzing urban data, enhancing\ndecision-making, and fostering citizen engagement. We provide a concise\noverview of the evolution and core technologies of LLMs. Additionally, we\nsurvey their applications across key urban domains, such as transportation,\npublic safety, and environmental monitoring, summarizing essential tasks and\nprior works in various urban contexts, while highlighting LLMs' functional\nroles and implementation patterns. Building on this, we propose potential\nLLM-based solutions to address unresolved challenges. To facilitate in-depth\nresearch, we compile a list of available datasets and tools applicable to\ndiverse urban scenarios. Finally, we discuss the limitations of current\napproaches and outline future directions for advancing LLMs in urban computing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02051", "pdf": "https://arxiv.org/pdf/2504.02051", "abs": "https://arxiv.org/abs/2504.02051", "authors": ["Alfonso Amayuelas", "Jingbo Yang", "Saaket Agashe", "Ashwin Nagarajan", "Antonis Antoniades", "Xin Eric Wang", "William Wang"], "title": "Self-Resource Allocation in Multi-Agent LLM Systems", "categories": ["cs.MA", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of LLMs as agents, there is a growing interest in\nconnecting multiple agents into multi-agent systems to solve tasks\nconcurrently, focusing on their role in task assignment and coordination. This\npaper explores how LLMs can effectively allocate computational tasks among\nmultiple agents, considering factors such as cost, efficiency, and performance.\nIn this work, we address key questions, including the effectiveness of LLMs as\norchestrators and planners, comparing their effectiveness in task assignment\nand coordination. Our experiments demonstrate that LLMs can achieve high\nvalidity and accuracy in resource allocation tasks. We find that the planner\nmethod outperforms the orchestrator method in handling concurrent actions,\nresulting in improved efficiency and better utilization of agents.\nAdditionally, we show that providing explicit information about worker\ncapabilities enhances the allocation strategies of planners, particularly when\ndealing with suboptimal workers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02163", "pdf": "https://arxiv.org/pdf/2504.02163", "abs": "https://arxiv.org/abs/2504.02163", "authors": ["Lewis Matheson Creed"], "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "50 Pages, 10 figures, Honours Thesis", "summary": "The limited availability of training data for low-resource languages makes\napplying machine learning techniques challenging. Ancient Egyptian is one such\nlanguage with few resources. However, innovative applications of data\naugmentation methods, such as Neural Style Transfer, could overcome these\nbarriers. This paper presents a novel method for generating datasets of ancient\nEgyptian hieroglyphs by applying NST to a digital typeface. Experimental\nresults found that image classification models trained on NST-generated\nexamples and photographs demonstrate equal performance and transferability to\nreal unseen images of hieroglyphs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02471", "pdf": "https://arxiv.org/pdf/2504.02471", "abs": "https://arxiv.org/abs/2504.02471", "authors": ["Håkon Næss Sandum", "Hans Ole Ørka", "Oliver Tomic", "Erik Næsset", "Terje Gobakken"], "title": "Semantic segmentation of forest stands using deep learning", "categories": ["cs.CV"], "comment": "31 pages, 7 figures, 4 tables", "summary": "Forest stands are the fundamental units in forest management inventories,\nsilviculture, and financial analysis within operational forestry. Over the past\ntwo decades, a common method for mapping stand borders has involved delineation\nthrough manual interpretation of stereographic aerial images. This is a\ntime-consuming and subjective process, limiting operational efficiency and\nintroducing inconsistencies. Substantial effort has been devoted to automating\nthe process, using various algorithms together with aerial images and canopy\nheight models constructed from airborne laser scanning (ALS) data, but manual\ninterpretation remains the preferred method. Deep learning (DL) methods have\ndemonstrated great potential in computer vision, yet their application to\nforest stand delineation remains unexplored in published research. This study\npresents a novel approach, framing stand delineation as a multiclass\nsegmentation problem and applying a U-Net based DL framework. The model was\ntrained and evaluated using multispectral images, ALS data, and an existing\nstand map created by an expert interpreter. Performance was assessed on\nindependent data using overall accuracy, a standard metric for classification\ntasks that measures the proportions of correctly classified pixels. The model\nachieved an overall accuracy of 0.73. These results demonstrate strong\npotential for DL in automated stand delineation. However, a few key challenges\nwere noted, especially for complex forest environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02478", "pdf": "https://arxiv.org/pdf/2504.02478", "abs": "https://arxiv.org/abs/2504.02478", "authors": ["Bizhu Wu", "Jinheng Xie", "Keming Shen", "Zhe Kong", "Jianfeng Ren", "Ruibin Bai", "Rong Qu", "Linlin Shen"], "title": "MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities", "categories": ["cs.CV"], "comment": null, "summary": "Recent motion-aware large language models have demonstrated promising\npotential in unifying motion comprehension and generation. However, existing\napproaches primarily focus on coarse-grained motion-text modeling, where text\ndescribes the overall semantics of an entire motion sequence in just a few\nwords. This limits their ability to handle fine-grained motion-relevant tasks,\nsuch as understanding and controlling the movements of specific body parts. To\novercome this limitation, we pioneer MG-MotionLLM, a unified motion-language\nmodel for multi-granular motion comprehension and generation. We further\nintroduce a comprehensive multi-granularity training scheme by incorporating a\nset of novel auxiliary tasks, such as localizing temporal boundaries of motion\nsegments via detailed text as well as motion detailed captioning, to facilitate\nmutual reinforcement for motion-text modeling across various levels of\ngranularity. Extensive experiments show that our MG-MotionLLM achieves superior\nperformance on classical text-to-motion and motion-to-text tasks, and exhibits\npotential in novel fine-grained motion comprehension and editing tasks. Project\npage: CVI-SZU/MG-MotionLLM", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02268", "pdf": "https://arxiv.org/pdf/2504.02268", "abs": "https://arxiv.org/abs/2504.02268", "authors": ["Waris Gill", "Justin Cechmanek", "Tyler Hutcherson", "Srijith Rajamohan", "Jen Agarwal", "Muhammad Ali Gulzar", "Manvinder Singh", "Benoit Dion"], "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data", "categories": ["cs.LG", "cs.CL"], "comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences", "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02480", "pdf": "https://arxiv.org/pdf/2504.02480", "abs": "https://arxiv.org/abs/2504.02480", "authors": ["Kyungmin Choi", "JaKeoung Koo", "Stephen McLaughlin", "Abderrahim Halimi"], "title": "Graph Attention-Driven Bayesian Deep Unrolling for Dual-Peak Single-Photon Lidar Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-photon Lidar imaging offers a significant advantage in 3D imaging due\nto its high resolution and long-range capabilities, however it is challenging\nto apply in noisy environments with multiple targets per pixel. To tackle these\nchallenges, several methods have been proposed. Statistical methods demonstrate\ninterpretability on the inferred parameters, but they are often limited in\ntheir ability to handle complex scenes. Deep learning-based methods have shown\nsuperior performance in terms of accuracy and robustness, but they lack\ninterpretability or they are limited to a single-peak per pixel. In this paper,\nwe propose a deep unrolling algorithm for dual-peak single-photon Lidar\nimaging. We introduce a hierarchical Bayesian model for multiple targets and\npropose a neural network that unrolls the underlying statistical method. To\nsupport multiple targets, we adopt a dual depth maps representation and exploit\ngeometric deep learning to extract features from the point cloud. The proposed\nmethod takes advantages of statistical methods and learning-based methods in\nterms of accuracy and quantifying uncertainty. The experimental results on\nsynthetic and real data demonstrate the competitive performance when compared\nto existing methods, while also providing uncertainty information.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02494", "pdf": "https://arxiv.org/pdf/2504.02494", "abs": "https://arxiv.org/abs/2504.02494", "authors": ["Faisal Mohammad", "Duksan Ryu"], "title": "Semiconductor Wafer Map Defect Classification with Tiny Vision Transformers", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Semiconductor wafer defect classification is critical for ensuring high\nprecision and yield in manufacturing. Traditional CNN-based models often\nstruggle with class imbalances and recognition of the multiple overlapping\ndefect types in wafer maps. To address these challenges, we propose ViT-Tiny, a\nlightweight Vision Transformer (ViT) framework optimized for wafer defect\nclassification. Trained on the WM-38k dataset. ViT-Tiny outperforms its\nViT-Base counterpart and state-of-the-art (SOTA) models, such as MSF-Trans and\nCNN-based architectures. Through extensive ablation studies, we determine that\na patch size of 16 provides optimal performance. ViT-Tiny achieves an F1-score\nof 98.4%, surpassing MSF-Trans by 2.94% in four-defect classification,\nimproving recall by 2.86% in two-defect classification, and increasing\nprecision by 3.13% in three-defect classification. Additionally, it\ndemonstrates enhanced robustness under limited labeled data conditions, making\nit a computationally efficient and reliable solution for real-world\nsemiconductor defect detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02577", "pdf": "https://arxiv.org/pdf/2504.02577", "abs": "https://arxiv.org/abs/2504.02577", "authors": ["Erik Arakelyan"], "title": "Reasoning Inconsistencies and How to Mitigate Them in Deep Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": "PhD thesis", "summary": "The recent advancements in Deep Learning models and techniques have led to\nsignificant strides in performance across diverse tasks and modalities.\nHowever, while the overall capabilities of models show promising growth, our\nunderstanding of their internal reasoning processes remains limited,\nparticularly concerning systematic inconsistencies or errors patterns of\nlogical or inferential flaws. These inconsistencies may manifest as\ncontradictory outputs, failure to generalize across similar tasks, or erroneous\nconclusions in specific contexts. Even detecting and measuring such reasoning\ndiscrepancies is challenging, as they may arise from opaque internal\nprocedures, biases and imbalances in training data, or the inherent complexity\nof the task. Without effective methods to detect, measure, and mitigate these\nerrors, there is a risk of deploying models that are biased, exploitable, or\nlogically unreliable. This thesis aims to address these issues by producing\nnovel methods for deep learning models that reason over knowledge graphs,\nnatural language, and images. The thesis contributes two techniques for\ndetecting and quantifying predictive inconsistencies originating from opaque\ninternal procedures in natural language and image processing models. To\nmitigate inconsistencies from biases in training data, this thesis presents a\ndata efficient sampling method to improve fairness and performance and a\nsynthetic dataset generation approach in low resource scenarios. Finally, the\nthesis offers two techniques to optimize the models for complex reasoning\ntasks. These methods enhance model performance while allowing for more faithful\nand interpretable exploration and exploitation during inference. Critically,\nthis thesis provides a comprehensive framework to improve the robustness,\nfairness, and interpretability of deep learning models across diverse tasks and\nmodalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02508", "pdf": "https://arxiv.org/pdf/2504.02508", "abs": "https://arxiv.org/abs/2504.02508", "authors": ["Zhuguanyu Wu", "Jiayi Zhang", "Jiaxin Chen", "Jinyang Guo", "Di Huang", "Yunhong Wang"], "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision Transformers (ViTs) have become one of the most commonly used\nbackbones for vision tasks. Despite their remarkable performance, they often\nsuffer significant accuracy drops when quantized for practical deployment,\nparticularly by post-training quantization (PTQ) under ultra-low bits.\nRecently, reconstruction-based PTQ methods have shown promising performance in\nquantizing Convolutional Neural Networks (CNNs). However, they fail when\napplied to ViTs, primarily due to the inaccurate estimation of output\nimportance and the substantial accuracy degradation in quantizing post-GELU\nactivations. To address these issues, we propose \\textbf{APHQ-ViT}, a novel PTQ\napproach based on importance estimation with Average Perturbation Hessian\n(APH). Specifically, we first thoroughly analyze the current approximation\napproaches with Hessian loss, and propose an improved average perturbation\nHessian loss. To deal with the quantization of the post-GELU activations, we\ndesign an MLP Reconstruction (MR) method by replacing the GELU function in MLP\nwith ReLU and reconstructing it by the APH loss on a small unlabeled\ncalibration set. Extensive experiments demonstrate that APHQ-ViT using linear\nquantizers outperforms existing PTQ methods by substantial margins in 3-bit and\n4-bit across different vision tasks. The source code is available at\nhttps://github.com/GoatWu/APHQ-ViT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02512", "pdf": "https://arxiv.org/pdf/2504.02512", "abs": "https://arxiv.org/abs/2504.02512", "authors": ["Emad Bahrami", "Olga Zatsarynna", "Gianpiero Francesca", "Juergen Gall"], "title": "Towards Generalizing Temporal Action Segmentation to Unseen Views", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While there has been substantial progress in temporal action segmentation,\nthe challenge to generalize to unseen views remains unaddressed. Hence, we\ndefine a protocol for unseen view action segmentation where camera views for\nevaluating the model are unavailable during training. This includes changing\nfrom top-frontal views to a side view or even more challenging from exocentric\nto egocentric views. Furthermore, we present an approach for temporal action\nsegmentation that tackles this challenge. Our approach leverages a shared\nrepresentation at both the sequence and segment levels to reduce the impact of\nview differences during training. We achieve this by introducing a sequence\nloss and an action loss, which together facilitate consistent video and action\nrepresentations across different views. The evaluation on the Assembly101,\nIkeaASM, and EgoExoLearn datasets demonstrate significant improvements, with a\n12.8% increase in F1@50 for unseen exocentric views and a substantial 54%\nimprovement for unseen egocentric views.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02515", "pdf": "https://arxiv.org/pdf/2504.02515", "abs": "https://arxiv.org/abs/2504.02515", "authors": ["Nedko Savov", "Naser Kazemi", "Mohammad Mahdi", "Danda Pani Paudel", "Xi Wang", "Luc Van Gool"], "title": "Exploration-Driven Generative Interactive Environments", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Modern world models require costly and time-consuming collection of large\nvideo datasets with action demonstrations by people or by environment-specific\nagents. To simplify training, we focus on using many virtual environments for\ninexpensive, automatically collected interaction data. Genie, a recent\nmulti-environment world model, demonstrates simulation abilities of many\nenvironments with shared behavior. Unfortunately, training their model requires\nexpensive demonstrations. Therefore, we propose a training framework merely\nusing a random agent in virtual environments. While the model trained in this\nmanner exhibits good controls, it is limited by the random exploration\npossibilities. To address this limitation, we propose AutoExplore Agent - an\nexploration agent that entirely relies on the uncertainty of the world model,\ndelivering diverse data from which it can learn the best. Our agent is fully\nindependent of environment-specific rewards and thus adapts easily to new\nenvironments. With this approach, the pretrained multi-environment model can\nquickly adapt to new environments achieving video fidelity and controllability\nimprovement. In order to obtain automatically large-scale interaction datasets\nfor pretraining, we group environments with similar behavior and controls. To\nthis end, we annotate the behavior and controls of 974 virtual environments - a\ndataset that we name RetroAct. For building our model, we first create an open\nimplementation of Genie - GenieRedux and apply enhancements and adaptations in\nour version GenieRedux-G. Our code and data are available at\nhttps://github.com/insait-institute/GenieRedux.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02670", "pdf": "https://arxiv.org/pdf/2504.02670", "abs": "https://arxiv.org/abs/2504.02670", "authors": ["Maciej Besta", "Lorenzo Paleari", "Jia Hao Andrea Jiang", "Robert Gerstenberger", "You Wu", "Patrick Iff", "Ales Kubicek", "Piotr Nyczyk", "Diana Khimey", "Jón Gunnar Hannesson", "Grzegorz Kwaśniewski", "Marcin Copik", "Hubert Niewiadomski", "Torsten Hoefler"], "title": "Affordable AI Assistants with Knowledge Graph of Thoughts", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose the Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively. For example, KGoT achieves a 29%\nimprovement in task success rates on the GAIA benchmark compared to Hugging\nFace Agents with GPT-4o mini, while reducing costs by over 36x compared to\nGPT-4o. Improvements for recent reasoning models are similar, e.g., 36% and\n37.5% for Qwen2.5-32B and Deepseek-R1-70B, respectively. KGoT offers a\nscalable, affordable, and high-performing solution for AI assistants.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02524", "pdf": "https://arxiv.org/pdf/2504.02524", "abs": "https://arxiv.org/abs/2504.02524", "authors": ["Yunhao Lv", "Lingyu Chen", "Jian Wang", "Yangxi Li", "Fang Chen"], "title": "SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2304.05919 by other authors", "summary": "In recent years, deep learning methods such as convolutional neural network\n(CNN) and transformers have made significant progress in CT multi-organ\nsegmentation. However, CT multi-organ segmentation methods based on masked\nimage modeling (MIM) are very limited. There are already methods using MAE for\nCT multi-organ segmentation task, we believe that the existing methods do not\nidentify the most difficult areas to reconstruct. To this end, we propose a MIM\nself-training framework with hard patches mining masked autoencoders for CT\nmulti-organ segmentation tasks (selfMedHPM). The method performs ViT\nself-pretraining on the training set of the target data and introduces an\nauxiliary loss predictor, which first predicts the patch loss and determines\nthe location of the next mask. SelfMedHPM implementation is better than various\ncompetitive methods in abdominal CT multi-organ segmentation and body CT\nmulti-organ segmentation. We have validated the performance of our method on\nthe Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen\nmult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body\nmulti-organ segmentation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02534", "pdf": "https://arxiv.org/pdf/2504.02534", "abs": "https://arxiv.org/abs/2504.02534", "authors": ["Mykola Lavreniuk", "Nataliia Kussul", "Andrii Shelestov", "Bohdan Yailymov", "Yevhenii Salii", "Volodymyr Kuzin", "Zoltan Szantoi"], "title": "Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "The accurate delineation of agricultural field boundaries from satellite\nimagery is vital for land management and crop monitoring. However, current\nmethods face challenges due to limited dataset sizes, resolution discrepancies,\nand diverse environmental conditions. We address this by reformulating the task\nas instance segmentation and introducing the Field Boundary Instance\nSegmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset\ncomprising 672,909 high-resolution satellite image patches (ranging from 0.25 m\nto 10 m) and 22,926,427 instance masks of individual fields, significantly\nnarrowing the gap between agricultural datasets and those in other computer\nvision domains. We further propose Delineate Anything, an instance segmentation\nmodel trained on our new FBIS-22M dataset. Our proposed model sets a new\nstate-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and\n103% in mAP@0.5:0.95 over existing methods, while also demonstrating\nsignificantly faster inference and strong zero-shot generalization across\ndiverse image resolutions and unseen geographic regions. Code, pre-trained\nmodels, and the FBIS-22M dataset are available at\nhttps://lavreniuk.github.io/Delineate-Anything.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02536", "pdf": "https://arxiv.org/pdf/2504.02536", "abs": "https://arxiv.org/abs/2504.02536", "authors": ["Konrad Gadzicki", "Kerstin Schill", "Christoph Zetzsche"], "title": "A Sensorimotor Vision Transformer", "categories": ["cs.CV", "I.4.8; I.5.1"], "comment": "14 pages, 5 figures", "summary": "This paper presents the Sensorimotor Transformer (SMT), a vision model\ninspired by human saccadic eye movements that prioritize high-saliency regions\nin visual input to enhance computational efficiency and reduce memory\nconsumption. Unlike traditional models that process all image patches\nuniformly, SMT identifies and selects the most salient patches based on\nintrinsic two-dimensional (i2D) features, such as corners and occlusions, which\nare known to convey high-information content and align with human fixation\npatterns. The SMT architecture uses this biological principle to leverage\nvision transformers to process only the most informative patches, allowing for\na substantial reduction in memory usage that scales with the sequence length of\nselected patches. This approach aligns with visual neuroscience findings,\nsuggesting that the human visual system optimizes information gathering through\nselective, spatially dynamic focus. Experimental evaluations on Imagenet-1k\ndemonstrate that SMT achieves competitive top-1 accuracy while significantly\nreducing memory consumption and computational complexity, particularly when a\nlimited number of patches is used. This work introduces a saccade-like\nselection mechanism into transformer-based vision models, offering an efficient\nalternative for image analysis and providing new insights into biologically\nmotivated architectures for resource-constrained applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02545", "pdf": "https://arxiv.org/pdf/2504.02545", "abs": "https://arxiv.org/abs/2504.02545", "authors": ["Bo-Kai Ruan", "Hong-Han Shuai"], "title": "MAD: Makeup All-in-One with Cross-Domain Diffusion Model", "categories": ["cs.CV"], "comment": "Project page: https://basiclab.github.io/MAD", "summary": "Existing makeup techniques often require designing multiple models to handle\ndifferent inputs and align features across domains for different makeup tasks,\ne.g., beauty filter, makeup transfer, and makeup removal, leading to increased\ncomplexity. Another limitation is the absence of text-guided makeup try-on,\nwhich is more user-friendly without needing reference images. In this study, we\nmake the first attempt to use a single model for various makeup tasks.\nSpecifically, we formulate different makeup tasks as cross-domain translations\nand leverage a cross-domain diffusion model to accomplish all tasks. Unlike\nexisting methods that rely on separate encoder-decoder configurations or\ncycle-based mechanisms, we propose using different domain embeddings to\nfacilitate domain control. This allows for seamless domain switching by merely\nchanging embeddings with a single model, thereby reducing the reliance on\nadditional modules for different tasks. Moreover, to support precise\ntext-to-makeup applications, we introduce the MT-Text dataset by extending the\nMT dataset with textual annotations, advancing the practicality of makeup\ntechnologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02555", "pdf": "https://arxiv.org/pdf/2504.02555", "abs": "https://arxiv.org/abs/2504.02555", "authors": ["Hesong Li", "Ziqi Wu", "Ruiwen Shao", "Tao Zhang", "Ying Fu"], "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement", "categories": ["cs.CV"], "comment": "Acceped by CVPR2025", "summary": "Scanning Transmission Electron Microscopy (STEM) enables the observation of\natomic arrangements at sub-angstrom resolution, allowing for atomically\nresolved analysis of the physical and chemical properties of materials.\nHowever, due to the effects of noise, electron beam damage, sample thickness,\netc, obtaining satisfactory atomic-level images is often challenging. Enhancing\nSTEM images can reveal clearer structural details of materials. Nonetheless,\nexisting STEM image enhancement methods usually overlook unique features in the\nfrequency domain, and existing datasets lack realism and generality. To resolve\nthese issues, in this paper, we develop noise calibration, data synthesis, and\nenhancement methods for STEM images. We first present a STEM noise calibration\nmethod, which is used to synthesize more realistic STEM images. The parameters\nof background noise, scan noise, and pointwise noise are obtained by\nstatistical analysis and fitting of real STEM images containing atoms. Then we\nuse these parameters to develop a more general dataset that considers both\nregular and random atomic arrangements and includes both HAADF and BF mode\nimages. Finally, we design a spatial-frequency interactive network for STEM\nimage enhancement, which can explore the information in the frequency domain\nformed by the periodicity of atomic arrangement. Experimental results show that\nour data is closer to real STEM images and achieves better enhancement\nperformances together with our network. Code will be available at\nhttps://github.com/HeasonLee/SFIN}{https://github.com/HeasonLee/SFIN.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02560", "pdf": "https://arxiv.org/pdf/2504.02560", "abs": "https://arxiv.org/abs/2504.02560", "authors": ["Yongqi Zhai", "Luyang Tang", "Wei Jiang", "Jiayu Yang", "Ronggang Wang"], "title": "L-LBVC: Long-Term Motion Estimation and Prediction for Learned Bi-Directional Video Compression", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted to 2025 Data Compression Conference (DCC)", "summary": "Recently, learned video compression (LVC) has shown superior performance\nunder low-delay configuration. However, the performance of learned\nbi-directional video compression (LBVC) still lags behind traditional\nbi-directional coding. The performance gap mainly arises from inaccurate\nlong-term motion estimation and prediction of distant frames, especially in\nlarge motion scenes. To solve these two critical problems, this paper proposes\na novel LBVC framework, namely L-LBVC. Firstly, we propose an adaptive motion\nestimation module that can handle both short-term and long-term motions.\nSpecifically, we directly estimate the optical flows for adjacent frames and\nnon-adjacent frames with small motions. For non-adjacent frames with large\nmotions, we recursively accumulate local flows between adjacent frames to\nestimate long-term flows. Secondly, we propose an adaptive motion prediction\nmodule that can largely reduce the bit cost for motion coding. To improve the\naccuracy of long-term motion prediction, we adaptively downsample reference\nframes during testing to match the motion ranges observed during training.\nExperiments show that our L-LBVC significantly outperforms previous\nstate-of-the-art LVC methods and even surpasses VVC (VTM) on some test datasets\nunder random access configuration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02762", "pdf": "https://arxiv.org/pdf/2504.02762", "abs": "https://arxiv.org/abs/2504.02762", "authors": ["Ahmet Burak Yildirim", "Mustafa Utku Aydogdu", "Duygu Ceylan", "Aysegul Dundar"], "title": "MD-ProjTex: Texturing 3D Shapes with Multi-Diffusion Projection", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MD-ProjTex, a method for fast and consistent text-guided texture\ngeneration for 3D shapes using pretrained text-to-image diffusion models. At\nthe core of our approach is a multi-view consistency mechanism in UV space,\nwhich ensures coherent textures across different viewpoints. Specifically,\nMD-ProjTex fuses noise predictions from multiple views at each diffusion step\nand jointly updates the per-view denoising directions to maintain 3D\nconsistency. In contrast to existing state-of-the-art methods that rely on\noptimization or sequential view synthesis, MD-ProjTex is computationally more\nefficient and achieves better quantitative and qualitative results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02763", "pdf": "https://arxiv.org/pdf/2504.02763", "abs": "https://arxiv.org/abs/2504.02763", "authors": ["Benjy Friedmann", "Michael Werman"], "title": "CanonNet: Canonical Ordering and Curvature Learning for Point Cloud Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud processing poses two fundamental challenges: establishing\nconsistent point ordering and effectively learning fine-grained geometric\nfeatures. Current architectures rely on complex operations that limit\nexpressivity while struggling to capture detailed surface geometry. We present\nCanonNet, a lightweight neural network composed of two complementary\ncomponents: (1) a preprocessing pipeline that creates a canonical point\nordering and orientation, and (2) a geometric learning framework where networks\nlearn from synthetic surfaces with precise curvature values. This modular\napproach eliminates the need for complex transformation-invariant architectures\nwhile effectively capturing local geometric properties. Our experiments\ndemonstrate state-of-the-art performance in curvature estimation and\ncompetitive results in geometric descriptor tasks with significantly fewer\nparameters (\\textbf{100X}) than comparable methods. CanonNet's efficiency makes\nit particularly suitable for real-world applications where computational\nresources are limited, demonstrating that mathematical preprocessing can\neffectively complement neural architectures for point cloud analysis. The code\nfor the project is publicly available\n\\hyperlink{https://benjyfri.github.io/CanonNet/}{https://benjyfri.github.io/CanonNet/}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02764", "pdf": "https://arxiv.org/pdf/2504.02764", "abs": "https://arxiv.org/abs/2504.02764", "authors": ["Shengjun Zhang", "Jinzhao Li", "Xin Fei", "Hao Liu", "Yueqi Duan"], "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "In this paper, we propose Scene Splatter, a momentum-based paradigm for video\ndiffusion to generate generic scenes from single image. Existing methods, which\nemploy video generation models to synthesize novel views, suffer from limited\nvideo length and scene inconsistency, leading to artifacts and distortions\nduring further reconstruction. To address this issue, we construct noisy\nsamples from original features as momentum to enhance video details and\nmaintain scene consistency. However, for latent features with the perception\nfield that spans both known and unknown regions, such latent-level momentum\nrestricts the generative ability of video diffusion in unknown regions.\nTherefore, we further introduce the aforementioned consistent video as a\npixel-level momentum to a directly generated video without momentum for better\nrecovery of unseen regions. Our cascaded momentum enables video diffusion\nmodels to generate both high-fidelity and consistent novel views. We further\nfinetune the global Gaussian representations with enhanced frames and render\nnew frames for momentum update in the next step. In this manner, we can\niteratively recover a 3D scene, avoiding the limitation of video length.\nExtensive experiments demonstrate the generalization capability and superior\nperformance of our method in high-fidelity and consistent scene generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02775", "pdf": "https://arxiv.org/pdf/2504.02775", "abs": "https://arxiv.org/abs/2504.02775", "authors": ["Yoon Gyo Jung", "Jaewoo Park", "Jaeho Yoon", "Kuan-Chuan Peng", "Wonchul Kim", "Andrew Beng Jin Teoh", "Octavia Camps"], "title": "TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR2025", "summary": "We aim to solve unsupervised anomaly detection in a practical challenging\nenvironment where the normal dataset is both contaminated with defective\nregions and its product class distribution is tailed but unknown. We observe\nthat existing models suffer from tail-versus-noise trade-off where if a model\nis robust against pixel noise, then its performance deteriorates on tail class\nsamples, and vice versa. To mitigate the issue, we handle the tail class and\nnoise samples independently. To this end, we propose TailSampler, a novel class\nsize predictor that estimates the class cardinality of samples based on a\nsymmetric assumption on the class-wise distribution of embedding similarities.\nTailSampler can be utilized to sample the tail class samples exclusively,\nallowing to handle them separately. Based on these facets, we build a\nmemory-based anomaly detection model TailedCore, whose memory both well\ncaptures tail class information and is noise-robust. We extensively validate\nthe effectiveness of TailedCore on the unsupervised long-tail noisy anomaly\ndetection setting, and show that TailedCore outperforms the state-of-the-art in\nmost settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02799", "pdf": "https://arxiv.org/pdf/2504.02799", "abs": "https://arxiv.org/abs/2504.02799", "authors": ["Anita Rau", "Mark Endo", "Josiah Aklilu", "Jaewoo Heo", "Khaled Saab", "Alberto Paderno", "Jeffrey Jopling", "F. Christopher Holsinger", "Serena Yeung-Levy"], "title": "Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models offer a new paradigm for AI-driven image\nunderstanding, enabling models to perform tasks without task-specific training.\nThis flexibility holds particular promise across medicine, where\nexpert-annotated data is scarce. Yet, VLMs' practical utility in\nintervention-focused domains--especially surgery, where decision-making is\nsubjective and clinical scenarios are variable--remains uncertain. Here, we\npresent a comprehensive analysis of 11 state-of-the-art VLMs across 17 key\nvisual understanding tasks in surgical AI--from anatomy recognition to skill\nassessment--using 13 datasets spanning laparoscopic, robotic, and open\nprocedures. In our experiments, VLMs demonstrate promising generalizability, at\ntimes outperforming supervised models when deployed outside their training\nsetting. In-context learning, incorporating examples during testing, boosted\nperformance up to three-fold, suggesting adaptability as a key strength. Still,\ntasks requiring spatial or temporal reasoning remained difficult. Beyond\nsurgery, our findings offer insights into VLMs' potential for tackling complex\nand dynamic scenarios in clinical and broader real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02817", "pdf": "https://arxiv.org/pdf/2504.02817", "abs": "https://arxiv.org/abs/2504.02817", "authors": ["Kangle Deng", "Hsueh-Ti Derek Liu", "Yiheng Zhu", "Xiaoxia Sun", "Chong Shang", "Kiran Bhat", "Deva Ramanan", "Jun-Yan Zhu", "Maneesh Agrawala", "Tinghui Zhou"], "title": "Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization", "categories": ["cs.CV"], "comment": "Project Page: https://oat-3d.github.io/", "summary": "Many 3D generative models rely on variational autoencoders (VAEs) to learn\ncompact shape representations. However, existing methods encode all shapes into\na fixed-size token, disregarding the inherent variations in scale and\ncomplexity across 3D data. This leads to inefficient latent representations\nthat can compromise downstream generation. We address this challenge by\nintroducing Octree-based Adaptive Tokenization, a novel framework that adjusts\nthe dimension of latent representations according to shape complexity. Our\napproach constructs an adaptive octree structure guided by a\nquadric-error-based subdivision criterion and allocates a shape latent vector\nto each octree cell using a query-based transformer. Building upon this\ntokenization, we develop an octree-based autoregressive generative model that\neffectively leverages these variable-sized representations in shape generation.\nExtensive experiments demonstrate that our approach reduces token counts by 50%\ncompared to fixed-size methods while maintaining comparable visual quality.\nWhen using a similar token length, our method produces significantly\nhigher-quality shapes. When incorporated with our downstream generative model,\nour method creates more detailed and diverse 3D content than existing\napproaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.01987", "pdf": "https://arxiv.org/pdf/2504.01987", "abs": "https://arxiv.org/abs/2504.01987", "authors": ["Ilir Tahiraj", "Markus Edinger", "Dominik Kulmer", "Markus Lienkamp"], "title": "CaLiV: LiDAR-to-Vehicle Calibration of Arbitrary Sensor Setups via Object Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In autonomous systems, sensor calibration is essential for a safe and\nefficient navigation in dynamic environments. Accurate calibration is a\nprerequisite for reliable perception and planning tasks such as object\ndetection and obstacle avoidance. Many existing LiDAR calibration methods\nrequire overlapping fields of view, while others use external sensing devices\nor postulate a feature-rich environment. In addition, Sensor-to-Vehicle\ncalibration is not supported by the vast majority of calibration algorithms. In\nthis work, we propose a novel target-based technique for extrinsic\nSensor-to-Sensor and Sensor-to-Vehicle calibration of multi-LiDAR systems\ncalled CaLiV. This algorithm works for non-overlapping FoVs, as well as\narbitrary calibration targets, and does not require any external sensing\ndevices. First, we apply motion to produce FoV overlaps and utilize a simple\nunscented Kalman filter to obtain vehicle poses. Then, we use the Gaussian\nmixture model-based registration framework GMMCalib to align the point clouds\nin a common calibration frame. Finally, we reduce the task of recovering the\nsensor extrinsics to a minimization problem. We show that both translational\nand rotational Sensor-to-Sensor errors can be solved accurately by our method.\nIn addition, all Sensor-to-Vehicle rotation angles can also be calibrated with\nhigh accuracy. We validate the simulation results in real-world experiments.\nThe code is open source and available on https://github.com/TUMFTM/CaLiV.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.01996", "pdf": "https://arxiv.org/pdf/2504.01996", "abs": "https://arxiv.org/abs/2504.01996", "authors": ["Khizar Anjum", "Parul Pandey", "Vidyasagar Sadhu", "Roberto Tron", "Dario Pompili"], "title": "Real-Time Navigation for Autonomous Aerial Vehicles Using Video", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Submitted to Journal of Real-Time Image Processing", "summary": "Most applications in autonomous navigation using mounted cameras rely on the\nconstruction and processing of geometric 3D point clouds, which is an expensive\nprocess. However, there is another simpler way to make a space navigable\nquickly: to use semantic information (e.g., traffic signs) to guide the agent.\nHowever, detecting and acting on semantic information involves Computer\nVision~(CV) algorithms such as object detection, which themselves are demanding\nfor agents such as aerial drones with limited onboard resources. To solve this\nproblem, we introduce a novel Markov Decision Process~(MDP) framework to reduce\nthe workload of these CV approaches. We apply our proposed framework to both\nfeature-based and neural-network-based object-detection tasks, using open-loop\nand closed-loop simulations as well as hardware-in-the-loop emulations. These\nholistic tests show significant benefits in energy consumption and speed with\nonly a limited loss in accuracy compared to models based on static features and\nneural networks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02045", "pdf": "https://arxiv.org/pdf/2504.02045", "abs": "https://arxiv.org/abs/2504.02045", "authors": ["Zhaoyang Zhang", "Yannick Hold-Geoffroy", "Miloš Hašan", "Chen Ziwen", "Fujun Luan", "Julie Dorsey", "Yiwei Hu"], "title": "WorldPrompter: Traversable Text-to-Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Scene-level 3D generation is a challenging research topic, with most existing\nmethods generating only partial scenes and offering limited navigational\nfreedom. We introduce WorldPrompter, a novel generative pipeline for\nsynthesizing traversable 3D scenes from text prompts. We leverage panoramic\nvideos as an intermediate representation to model the 360{\\deg} details of a\nscene. WorldPrompter incorporates a conditional 360{\\deg} panoramic video\ngenerator, capable of producing a 128-frame video that simulates a person\nwalking through and capturing a virtual environment. The resulting video is\nthen reconstructed as Gaussian splats by a fast feedforward 3D reconstructor,\nenabling a true walkable experience within the 3D scene. Experiments\ndemonstrate that our panoramic video generation model achieves convincing view\nconsistency across frames, enabling high-quality panoramic Gaussian splat\nreconstruction and facilitating traversal over an area of the scene.\nQualitative and quantitative results also show it outperforms the\nstate-of-the-art 360{\\deg} video generators and 3D scene generation models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02151", "pdf": "https://arxiv.org/pdf/2504.02151", "abs": "https://arxiv.org/abs/2504.02151", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages", "summary": "The rapid use of artificial intelligence (AI) in processes such as coding,\nimage processing, and data prediction means it is crucial to understand and\nvalidate the data we are working with fully. This paper dives into the hurdles\nof analyzing high-dimensional data, especially when it gets too complex.\nTraditional methods in data analysis often look at direct connections between\ninput variables, which can miss out on the more complicated relationships\nwithin the data.\n  To address these issues, we explore several tested techniques, such as\nremoving specific variables to see their impact and using statistical analysis\nto find connections between multiple variables. We also consider the role of\nsynthetic data and how information can sometimes be redundant across different\nsensors. These analyses are typically very computationally demanding and often\nrequire much human effort to make sense of the results.\n  A common approach is to treat the entire dataset as one unit and apply\nadvanced models to handle it. However, this can become problematic with larger,\nnoisier datasets and more complex models. So, we suggest methods to identify\noverall patterns that can help with tasks like classification or regression\nbased on the idea that more straightforward approaches might be more\nunderstandable.\n  Our research looks at two datasets: a real-world dataset and a synthetic one.\nThe goal is to create a methodology that highlights key features on a global\nscale that lead to predictions, making it easier to validate or quantify the\ndata set. By reducing the dimensionality with this method, we can simplify the\nmodels used and thus clarify the insights we gain. Furthermore, our method can\nreveal unexplored relationships between specific inputs and outcomes, providing\na way to validate these new connections further.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02163", "pdf": "https://arxiv.org/pdf/2504.02163", "abs": "https://arxiv.org/abs/2504.02163", "authors": ["Lewis Matheson Creed"], "title": "Neural Style Transfer for Synthesising a Dataset of Ancient Egyptian Hieroglyphs", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "50 Pages, 10 figures, Honours Thesis", "summary": "The limited availability of training data for low-resource languages makes\napplying machine learning techniques challenging. Ancient Egyptian is one such\nlanguage with few resources. However, innovative applications of data\naugmentation methods, such as Neural Style Transfer, could overcome these\nbarriers. This paper presents a novel method for generating datasets of ancient\nEgyptian hieroglyphs by applying NST to a digital typeface. Experimental\nresults found that image classification models trained on NST-generated\nexamples and photographs demonstrate equal performance and transferability to\nreal unseen images of hieroglyphs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02222", "pdf": "https://arxiv.org/pdf/2504.02222", "abs": "https://arxiv.org/abs/2504.02222", "authors": ["Liying Xu", "Hongliang He", "Wei Han", "Hanbin Huang", "Siwei Feng", "Guohong Fu"], "title": "APSeg: Auto-Prompt Model with Acquired and Injected Knowledge for Nuclear Instance Segmentation and Classification", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 3 figures", "summary": "Nuclear instance segmentation and classification provide critical\nquantitative foundations for digital pathology diagnosis. With the advent of\nthe foundational Segment Anything Model (SAM), the accuracy and efficiency of\nnuclear segmentation have improved significantly. However, SAM imposes a strong\nreliance on precise prompts, and its class-agnostic design renders its\nclassification results entirely dependent on the provided prompts. Therefore,\nwe focus on generating prompts with more accurate localization and\nclassification and propose \\textbf{APSeg}, \\textbf{A}uto-\\textbf{P}rompt model\nwith acquired and injected knowledge for nuclear instance \\textbf{Seg}mentation\nand classification. APSeg incorporates two knowledge-aware modules: (1)\nDistribution-Guided Proposal Offset Module (\\textbf{DG-POM}), which learns\ndistribution knowledge through density map guided, and (2) Category Knowledge\nSemantic Injection Module (\\textbf{CK-SIM}), which injects morphological\nknowledge derived from category descriptions. We conducted extensive\nexperiments on the PanNuke and CoNSeP datasets, demonstrating the effectiveness\nof our approach. The code will be released upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02473", "pdf": "https://arxiv.org/pdf/2504.02473", "abs": "https://arxiv.org/abs/2504.02473", "authors": ["Rick van Essen", "Eldert van Henten", "Lammert Kooistra", "Gert Kootstra"], "title": "Adaptive path planning for efficient object search by UAVs in agricultural fields", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents an adaptive path planner for object search in\nagricultural fields using UAVs. The path planner uses a high-altitude coverage\nflight path and plans additional low-altitude inspections when the detection\nnetwork is uncertain. The path planner was evaluated in an offline simulation\nenvironment containing real-world images. We trained a YOLOv8 detection network\nto detect artificial plants placed in grass fields to showcase the potential of\nour path planner. We evaluated the effect of different detection certainty\nmeasures, optimized the path planning parameters, investigated the effects of\nlocalization errors and different numbers of objects in the field. The YOLOv8\ndetection confidence worked best to differentiate between true and false\npositive detections and was therefore used in the adaptive planner. The optimal\nparameters of the path planner depended on the distribution of objects in the\nfield, when the objects were uniformly distributed, more low-altitude\ninspections were needed compared to a non-uniform distribution of objects,\nresulting in a longer path length. The adaptive planner proved to be robust\nagainst localization uncertainty. When increasing the number of objects, the\nflight path length increased, especially when the objects were uniformly\ndistributed. When the objects were non-uniformly distributed, the adaptive path\nplanner yielded a shorter path than a low-altitude coverage path, even with\nhigh number of objects. Overall, the presented adaptive path planner allowed to\nfind non-uniformly distributed objects in a field faster than a coverage path\nplanner and resulted in a compatible detection accuracy. The path planner is\nmade available at https://github.com/wur-abe/uav_adaptive_planner.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-06.jsonl"}
