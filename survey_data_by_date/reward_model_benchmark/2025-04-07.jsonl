{"id": "2504.03486", "pdf": "https://arxiv.org/pdf/2504.03486", "abs": "https://arxiv.org/abs/2504.03486", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy", "summarization"], "score": 5}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02906", "pdf": "https://arxiv.org/pdf/2504.02906", "abs": "https://arxiv.org/abs/2504.02906", "authors": ["Zhihan Zhang", "Yixin Cao", "Lizi Liao"], "title": "Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 5 figures", "summary": "Chart-to-code generation, the process of converting chart images into\nexecutable plotting scripts, provides a lossless representation of chart\ninformation, requiring models to accurately capture and summarize all visual\nand structural elements. However, this remains a significant challenge for\nmultimodal large language models (MLLMs), which are not inherently well-aligned\nwith code generation tasks. To bridge this gap, we introduce Chart2Code, a\nnovel iterative dual preference learning framework designed to enhance MLLMs'\nchart-to-code generation capabilities through structured code variant\ngeneration and fine-grained dual reward signals. We validate Chart2Code across\nthree MLLMs and find that iterative preference learning consistently improves\nout-of-distribution chart-to-code generation quality. Throughout this process,\nour dual scoring method, which evaluates both the textual code structure and\nits visual representation, leads to greater performance improvements, even with\na reduced preference dataset size. Further analysis explores the key components\nof our framework and highlights the interplay between chart-to-code generation\nand broader chart reasoning, paving the way for future advancements in chart\ncomprehension.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "code generation", "fine-grained"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03135", "pdf": "https://arxiv.org/pdf/2504.03135", "abs": "https://arxiv.org/abs/2504.03135", "authors": ["Junkai Zhang", "Bin Li", "Shoujun Zhou", "Yue Du"], "title": "Hierarchical Modeling for Medical Visual Question Answering with Cross-Attention Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical Visual Question Answering (Med-VQA) answers clinical questions using\nmedical images, aiding diagnosis. Designing the MedVQA system holds profound\nimportance in assisting clinical diagnosis and enhancing diagnostic accuracy.\nBuilding upon this foundation, Hierarchical Medical VQA extends Medical VQA by\norganizing medical questions into a hierarchical structure and making\nlevel-specific predictions to handle fine-grained distinctions. Recently, many\nstudies have proposed hierarchical MedVQA tasks and established datasets,\nHowever, several issues still remain: (1) imperfect hierarchical modeling leads\nto poor differentiation between question levels causing semantic fragmentation\nacross hierarchies. (2) Excessive reliance on implicit learning in\nTransformer-based cross-modal self-attention fusion methods, which obscures\ncrucial local semantic correlations in medical scenarios. To address these\nissues, this study proposes a HiCA-VQA method, including two modules:\nHierarchical Prompting for fine-grained medical questions and Hierarchical\nAnswer Decoders. The hierarchical prompting module pre-aligns hierarchical text\nprompts with image features to guide the model in focusing on specific image\nregions according to question types, while the hierarchical decoder performs\nseparate predictions for questions at different levels to improve accuracy\nacross granularities. The framework also incorporates a cross-attention fusion\nmodule where images serve as queries and text as key-value pairs. Experiments\non the Rad-Restruct benchmark demonstrate that the HiCA-VQA framework better\noutperforms existing state-of-the-art methods in answering hierarchical\nfine-grained questions. This study provides an effective pathway for\nhierarchical visual question answering systems, advancing medical image\nunderstanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03051", "pdf": "https://arxiv.org/pdf/2504.03051", "abs": "https://arxiv.org/abs/2504.03051", "authors": ["Chengyang He", "Wenlong Zhang", "Violet Xinying Chen", "Yue Ning", "Ping Wang"], "title": "Task as Context Prompting for Accurate Medical Symptom Coding Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures, 5 Tables, ACM/IEEE International Conference on\n  Connected Health: Applications, Systems and Engineering Technologies (CHASE\n  '25), June 24--26, 2025, New York, NY, USA", "summary": "Accurate medical symptom coding from unstructured clinical text, such as\nvaccine safety reports, is a critical task with applications in\npharmacovigilance and safety monitoring. Symptom coding, as tailored in this\nstudy, involves identifying and linking nuanced symptom mentions to\nstandardized vocabularies like MedDRA, differentiating it from broader medical\ncoding tasks. Traditional approaches to this task, which treat symptom\nextraction and linking as independent workflows, often fail to handle the\nvariability and complexity of clinical narratives, especially for rare cases.\nRecent advancements in Large Language Models (LLMs) offer new opportunities but\nface challenges in achieving consistent performance. To address these issues,\nwe propose Task as Context (TACO) Prompting, a novel framework that unifies\nextraction and linking tasks by embedding task-specific context into LLM\nprompts. Our study also introduces SYMPCODER, a human-annotated dataset derived\nfrom Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage\nevaluation framework to comprehensively assess both symptom linking and mention\nfidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat,\nJackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's\neffectiveness in improving flexibility and accuracy for tailored tasks like\nsymptom coding, paving the way for more specific coding tasks and advancing\nclinical text processing methodologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "accuracy"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03206", "pdf": "https://arxiv.org/pdf/2504.03206", "abs": "https://arxiv.org/abs/2504.03206", "authors": ["Yanming Wan", "Jiaxing Wu", "Marwa Abdulhai", "Lior Shani", "Natasha Jaques"], "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "safety", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02878", "pdf": "https://arxiv.org/pdf/2504.02878", "abs": "https://arxiv.org/abs/2504.02878", "authors": ["Lilin Xu", "Kaiyuan Hou", "Xiaofan Jiang"], "title": "Exploring the Capabilities of LLMs for IMU-based Fine-grained Human Activity Understanding", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to The 2nd International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys 2025)", "summary": "Human activity recognition (HAR) using inertial measurement units (IMUs)\nincreasingly leverages large language models (LLMs), yet existing approaches\nfocus on coarse activities like walking or running. Our preliminary study\nindicates that pretrained LLMs fail catastrophically on fine-grained HAR tasks\nsuch as air-written letter recognition, achieving only near-random guessing\naccuracy. In this work, we first bridge this gap for flat-surface writing\nscenarios: by fine-tuning LLMs with a self-collected dataset and few-shot\nlearning, we achieved up to a 129x improvement on 2D data. To extend this to 3D\nscenarios, we designed an encoder-based pipeline that maps 3D data into 2D\nequivalents, preserving the spatiotemporal information for robust letter\nprediction. Our end-to-end pipeline achieves 78% accuracy on word recognition\nwith up to 5 letters in mid-air writing scenarios, establishing LLMs as viable\ntools for fine-grained HAR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02900", "pdf": "https://arxiv.org/pdf/2504.02900", "abs": "https://arxiv.org/abs/2504.02900", "authors": ["Matheus Martins Batista"], "title": "Comparative Analysis of Deepfake Detection Models: New Approaches and Perspectives", "categories": ["cs.CV", "cs.LG", "stat.CO", "stat.ML"], "comment": "Bachelor's thesis", "summary": "The growing threat posed by deepfake videos, capable of manipulating\nrealities and disseminating misinformation, drives the urgent need for\neffective detection methods. This work investigates and compares different\napproaches for identifying deepfakes, focusing on the GenConViT model and its\nperformance relative to other architectures present in the DeepfakeBenchmark.\nTo contextualize the research, the social and legal impacts of deepfakes are\naddressed, as well as the technical fundamentals of their creation and\ndetection, including digital image processing, machine learning, and artificial\nneural networks, with emphasis on Convolutional Neural Networks (CNNs),\nGenerative Adversarial Networks (GANs), and Transformers. The performance\nevaluation of the models was conducted using relevant metrics and new datasets\nestablished in the literature, such as WildDeep-fake and DeepSpeak, aiming to\nidentify the most effective tools in the battle against misinformation and\nmedia manipulation. The obtained results indicated that GenConViT, after\nfine-tuning, exhibited superior performance in terms of accuracy (93.82%) and\ngeneralization capacity, surpassing other architectures in the\nDeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the\nadvancement of deepfake detection techniques, offering contributions to the\ndevelopment of more robust and effective solutions against the dissemination of\nfalse information.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02867", "pdf": "https://arxiv.org/pdf/2504.02867", "abs": "https://arxiv.org/abs/2504.02867", "authors": ["Hongliu Cao", "Ilias Driouich", "Robin Singh", "Eoin Thomas"], "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at SophiaSummit2024", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\ndiverse domains, yet they still encounter challenges such as insufficient\ndomain-specific knowledge, biases, and hallucinations. This underscores the\nneed for robust evaluation methodologies to accurately assess LLM-based\napplications. Traditional evaluation methods, which rely on word overlap or\ntext embeddings, are inadequate for capturing the nuanced semantic information\nnecessary to evaluate dynamic, open-ended text generation. Recent research has\nexplored leveraging LLMs to mimic human reasoning and decision-making processes\nfor evaluation purposes known as LLM-as-a-judge framework. However, these\nexisting frameworks have two significant limitations. First, they lack the\nflexibility to adapt to different text styles, including various answer and\nground truth styles, thereby reducing their generalization performance. Second,\nthe evaluation scores produced by these frameworks are often skewed and hard to\ninterpret, showing a low correlation with human judgment. To address these\nchallenges, we propose a novel dynamic multi-agent system that automatically\ndesigns personalized LLM judges for various natural language generation\napplications. This system iteratively refines evaluation prompts and balances\nthe trade-off between the adaptive requirements of downstream tasks and the\nalignment with human perception. Our experimental results show that the\nproposed multi-agent LLM Judge framework not only enhances evaluation accuracy\ncompared to existing methods but also produces evaluation scores that better\nalign with human perception.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02894", "pdf": "https://arxiv.org/pdf/2504.02894", "abs": "https://arxiv.org/abs/2504.02894", "authors": ["Ahsan Bilal", "Beiyu Lin", "Mehdi Zaeifi"], "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been widely used for various tasks and\napplications. However, LLMs and fine-tuning are limited to the pre-trained\ndata. For example, ChatGPT's world knowledge until 2021 can be outdated or\ninaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation\n(RAG), is proposed to augment LLMs with additional, new, latest details and\ninformation to LLMs. While RAG offers the correct information, it may not best\npresent it, especially to different population groups with personalizations.\nReinforcement Learning from Human Feedback (RLHF) adapts to user needs by\naligning model responses with human preference through feedback loops. In\nreal-life applications, such as mental health problems, a dynamic and\nfeedback-based model would continuously adapt to new information and offer\npersonalized assistance due to complex factors fluctuating in a daily\nenvironment. Thus, we propose an Online Reinforcement Learning-based\nRetrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the\nresponding systems to mental health problems, such as stress, anxiety, and\ndepression. We use an open-source dataset collected from 2028 College Students\nwith 28 survey questions for each student to demonstrate the performance of our\nproposed system with the existing systems. Our system achieves superior\nperformance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,\nGemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life\napplications of LLMs for personalized services in the everyday environment. The\nresults will also help researchers in the fields of sociology, psychology, and\nneuroscience to align their theories more closely with the actual human daily\nenvironment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03164", "pdf": "https://arxiv.org/pdf/2504.03164", "abs": "https://arxiv.org/abs/2504.03164", "authors": ["Kexin Tian", "Jingrui Mao", "Yunlong Zhang", "Jiwan Jiang", "Yang Zhou", "Zhengzhong Tu"], "title": "NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Vision-Language Models (VLMs) have demonstrated strong\npotential for autonomous driving tasks. However, their spatial understanding\nand reasoning-key capabilities for autonomous driving-still exhibit significant\nlimitations. Notably, none of the existing benchmarks systematically evaluate\nVLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we\npropose NuScenes-SpatialQA, the first large-scale ground-truth-based\nQuestion-Answer (QA) benchmark specifically designed to evaluate the spatial\nunderstanding and reasoning capabilities of VLMs in autonomous driving. Built\nupon the NuScenes dataset, the benchmark is constructed through an automated 3D\nscene graph generation pipeline and a QA generation pipeline. The benchmark\nsystematically evaluates VLMs' performance in both spatial understanding and\nreasoning across multiple dimensions. Using this benchmark, we conduct\nextensive experiments on diverse VLMs, including both general and\nspatial-enhanced models, providing the first comprehensive evaluation of their\nspatial capabilities in autonomous driving. Surprisingly, the experimental\nresults show that the spatial-enhanced VLM outperforms in qualitative QA but\ndoes not demonstrate competitiveness in quantitative QA. In general, VLMs still\nface considerable challenges in spatial understanding and reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03168", "pdf": "https://arxiv.org/pdf/2504.03168", "abs": "https://arxiv.org/abs/2504.03168", "authors": ["Lucas Choi", "Ross Greer"], "title": "Finding the Reflection Point: Unpadding Images to Remove Data Augmentation Artifacts in Large Open Source Image Datasets for Machine Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we address a novel image restoration problem relevant to\nmachine learning dataset curation: the detection and removal of noisy mirrored\npadding artifacts. While data augmentation techniques like padding are\nnecessary for standardizing image dimensions, they can introduce artifacts that\ndegrade model evaluation when datasets are repurposed across domains. We\npropose a systematic algorithm to precisely delineate the reflection boundary\nthrough a minimum mean squared error approach with thresholding and remove\nreflective padding. Our method effectively identifies the transition between\nauthentic content and its mirrored counterpart, even in the presence of\ncompression or interpolation noise. We demonstrate our algorithm's efficacy on\nthe SHEL5k dataset, showing significant performance improvements in zero-shot\nobject detection tasks using OWLv2, with average precision increasing from 0.47\nto 0.61 for hard hat detection and from 0.68 to 0.73 for person detection. By\naddressing annotation inconsistencies and distorted objects in padded regions,\nour approach enhances dataset integrity, enabling more reliable model\nevaluation across computer vision tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03197", "pdf": "https://arxiv.org/pdf/2504.03197", "abs": "https://arxiv.org/abs/2504.03197", "authors": ["Jaewoo Park", "Jungyang Park", "Dongju Jang", "Jiwan Chung", "Byungwoo Yoo", "Jaewoo Shin", "Seonjoon Park", "Taehyeong Kim", "Youngjae Yu"], "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation", "categories": ["cs.CL"], "comment": "18 pages, 4 figures", "summary": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03254", "pdf": "https://arxiv.org/pdf/2504.03254", "abs": "https://arxiv.org/abs/2504.03254", "authors": ["Yimin Wei", "Aoran Xiao", "Yexian Ren", "Yuting Zhu", "Hongruixuan Chen", "Junshi Xia", "Naoto Yokoya"], "title": "SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic Aperture Radar (SAR) is a crucial remote sensing technology,\nenabling all-weather, day-and-night observation with strong surface penetration\nfor precise and continuous environmental monitoring and analysis. However, SAR\nimage interpretation remains challenging due to its complex physical imaging\nmechanisms and significant visual disparities from human perception. Recently,\nVision-Language Models (VLMs) have demonstrated remarkable success in RGB image\nunderstanding, offering powerful open-vocabulary interpretation and flexible\nlanguage interaction. However, their application to SAR images is severely\nconstrained by the absence of SAR-specific knowledge in their training\ndistributions, leading to suboptimal performance. To address this limitation,\nwe introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR\nimage understanding, with a primary focus on integrating SAR with textual\nmodality. SARLANG-1M comprises more than 1 million high-quality SAR image-text\npairs collected from over 59 cities worldwide. It features hierarchical\nresolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions\n(including both concise and detailed captions), diverse remote sensing\ncategories (1,696 object types and 16 land cover classes), and multi-task\nquestion-answering pairs spanning seven applications and 1,012 question types.\nExtensive experiments on mainstream VLMs demonstrate that fine-tuning with\nSARLANG-1M significantly enhances their performance in SAR image\ninterpretation, reaching performance comparable to human experts. The dataset\nand code will be made publicly available at\nhttps://github.com/Jimmyxichen/SARLANG-1M.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03302", "pdf": "https://arxiv.org/pdf/2504.03302", "abs": "https://arxiv.org/abs/2504.03302", "authors": ["Afshin Khadangi", "Amir Sartipi", "Igor Tchappi", "Ramin Bahmani"], "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03313", "pdf": "https://arxiv.org/pdf/2504.03313", "abs": "https://arxiv.org/abs/2504.03313", "authors": ["Bram de Wilde", "Max T. Rietberg", "Guillaume Lajoinie", "Jelmer M. Wolterink"], "title": "Steerable Anatomical Shape Synthesis with Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Generative modeling of anatomical structures plays a crucial role in virtual\nimaging trials, which allow researchers to perform studies without the costs\nand constraints inherent to in vivo and phantom studies. For clinical\nrelevance, generative models should allow targeted control to simulate specific\npatient populations rather than relying on purely random sampling. In this\nwork, we propose a steerable generative model based on implicit neural\nrepresentations. Implicit neural representations naturally support topology\nchanges, making them well-suited for anatomical structures with varying\ntopology, such as the thyroid. Our model learns a disentangled latent\nrepresentation, enabling fine-grained control over shape variations. Evaluation\nincludes reconstruction accuracy and anatomical plausibility. Our results\ndemonstrate that the proposed model achieves high-quality shape generation\nwhile enabling targeted anatomical modifications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03520", "pdf": "https://arxiv.org/pdf/2504.03520", "abs": "https://arxiv.org/abs/2504.03520", "authors": ["Chen Wei Kuo", "Kevin Chu", "Nouar AlDahoul", "Hazem Ibrahim", "Talal Rahwan", "Yasir Zaki"], "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles", "categories": ["cs.CL", "cs.CY"], "comment": "23 pages, 3 figures", "summary": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612", "abs": "https://arxiv.org/abs/2504.03612", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "categories": ["cs.CL"], "comment": "29 pages, 11 figures", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "annotation"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03048", "pdf": "https://arxiv.org/pdf/2504.03048", "abs": "https://arxiv.org/abs/2504.03048", "authors": ["Ian Berlot-Attwell", "Frank Rudzicz", "Xujie Si"], "title": "LLM Library Learning Fails: A LEGO-Prover Case Study", "categories": ["cs.LG", "cs.CL"], "comment": "24 pages, 5 figures", "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03439", "pdf": "https://arxiv.org/pdf/2504.03439", "abs": "https://arxiv.org/abs/2504.03439", "authors": ["Mohammad Reza Yousefi", "Ali Bakrani", "Amin Dehghani"], "title": "Early detection of diabetes through transfer learning-based eye (vision) screening and improvement of machine learning model performance and advanced parameter setting algorithms", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "25 pages,12 Figures, 1 Table", "summary": "Diabetic Retinopathy (DR) is a serious and common complication of diabetes,\ncaused by prolonged high blood sugar levels that damage the small retinal blood\nvessels. If left untreated, DR can progress to retinal vein occlusion and\nstimulate abnormal blood vessel growth, significantly increasing the risk of\nblindness. Traditional diabetes diagnosis methods often utilize convolutional\nneural networks (CNNs) to extract visual features from retinal images, followed\nby classification algorithms such as decision trees and k-nearest neighbors\n(KNN) for disease detection. However, these approaches face several challenges,\nincluding low accuracy and sensitivity, lengthy machine learning (ML) model\ntraining due to high data complexity and volume, and the use of limited\ndatasets for testing and evaluation. This study investigates the application of\ntransfer learning (TL) to enhance ML model performance in DR detection. Key\nimprovements include dimensionality reduction, optimized learning rate\nadjustments, and advanced parameter tuning algorithms, aimed at increasing\nefficiency and diagnostic accuracy. The proposed model achieved an overall\naccuracy of 84% on the testing dataset, outperforming prior studies. The\nhighest class-specific accuracy reached 89%, with a maximum sensitivity of 97%\nand an F1-score of 92%, demonstrating strong performance in identifying DR\ncases. These findings suggest that TL-based DR screening is a promising\napproach for early diagnosis, enabling timely interventions to prevent vision\nloss and improve patient outcomes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03478", "pdf": "https://arxiv.org/pdf/2504.03478", "abs": "https://arxiv.org/abs/2504.03478", "authors": ["Spyros Kondylatos", "Nikolaos Ioannis Bountos", "Ioannis Prapas", "Angelos Zavras", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "Probabilistic Machine Learning for Noisy Labels in Earth Observation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Label noise poses a significant challenge in Earth Observation (EO), often\ndegrading the performance and reliability of supervised Machine Learning (ML)\nmodels. Yet, given the critical nature of several EO applications, developing\nrobust and trustworthy ML solutions is essential. In this study, we take a step\nin this direction by leveraging probabilistic ML to model input-dependent label\nnoise and quantify data uncertainty in EO tasks, accounting for the unique\nnoise sources inherent in the domain. We train uncertainty-aware probabilistic\nmodels across a broad range of high-impact EO applications-spanning diverse\nnoise sources, input modalities, and ML configurations-and introduce a\ndedicated pipeline to assess their accuracy and reliability. Our experimental\nresults show that the uncertainty-aware models consistently outperform the\nstandard deterministic approaches across most datasets and evaluation metrics.\nMoreover, through rigorous uncertainty evaluation, we validate the reliability\nof the predicted uncertainty estimates, enhancing the interpretability of model\npredictions. Our findings emphasize the importance of modeling label noise and\nincorporating uncertainty quantification in EO, paving the way for more\naccurate, reliable, and trustworthy ML solutions in the field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02866", "pdf": "https://arxiv.org/pdf/2504.02866", "abs": "https://arxiv.org/abs/2504.02866", "authors": ["Xiucheng Liang", "Jinheng Xie", "Tianhong Zhao", "Rudi Stouffs", "Filip Biljecki"], "title": "OpenFACADES: An Open Framework for Architectural Caption and Attribute Data Enrichment via Street View Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Building properties, such as height, usage, and material composition, play a\ncrucial role in spatial data infrastructures, supporting applications such as\nenergy simulation, risk assessment, and environmental modeling. Despite their\nimportance, comprehensive and high-quality building attribute data remain\nscarce in many urban areas. Recent advances have enabled the extraction and\ntagging of objective building attributes using remote sensing and street-level\nimagery. However, establishing a method and pipeline that integrates diverse\nopen datasets, acquires holistic building imagery at scale, and infers\ncomprehensive building attributes remains a significant challenge. Among the\nfirst, this study bridges the gaps by introducing OpenFACADES, an open\nframework that leverages multimodal crowdsourced data to enrich building\nprofiles with both objective attributes and semantic descriptors through\nmultimodal large language models. Our methodology proceeds in three major\nsteps. First, we integrate street-level image metadata from Mapillary with\nOpenStreetMap geometries via isovist analysis, effectively identifying images\nthat provide suitable vantage points for observing target buildings. Second, we\nautomate the detection of building facades in panoramic imagery and tailor a\nreprojection approach to convert objects into holistic perspective views that\napproximate real-world observation. Third, we introduce an innovative approach\nthat harnesses and systematically investigates the capabilities of open-source\nlarge vision-language models (VLMs) for multi-attribute prediction and\nopen-vocabulary captioning in building-level analytics, leveraging a globally\nsourced dataset of 30,180 labeled images from seven cities. Evaluation shows\nthat fine-tuned VLM excel in multi-attribute inference, outperforming\nsingle-attribute computer vision models and zero-shot ChatGPT-4o.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02858", "pdf": "https://arxiv.org/pdf/2504.02858", "abs": "https://arxiv.org/abs/2504.02858", "authors": ["Evgenii Evstafev"], "title": "Optimizing Humor Generation in Large Language Models: Temperature Configurations and Architectural Trade-offs", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 4 figures", "summary": "Large language models (LLMs) demonstrate increasing capabilities in creative\ntext generation, yet systematic evaluations of their humor production remain\nunderexplored. This study presents a comprehensive analysis of 13\nstate-of-the-art LLMs across five architectural families, evaluating their\nperformance in generating technically relevant humor for software developers.\nThrough a full factorial design testing 715 unique configurations of\ntemperature settings and prompt variations, we assess model outputs using five\nweighted criteria: humor quality, domain relevance, concept originality, tone\nprecision, and delivery efficiency. Our methodology employs rigorous\nstatistical analysis including ANOVA, correlation studies, and quadratic\nregression to identify optimal configurations and architectural influences.\nResults reveal significant performance variations across models, with certain\narchitectures achieving 21.8% superiority over baseline systems. Temperature\nsensitivity analysis demonstrates that 73% of models achieve peak performance\nat lower stochasticity settings (<= 0.5), though optimal ranges vary\nsubstantially by architecture. We identify distinct model clusters: compact\nhigh-performers maintaining efficiency-quality balance versus verbose\nspecialists requiring longer outputs for marginal gains. Statistical validation\nconfirms model architecture explains 38.7% of performance variance, with\nsignificant correlations between humor quality and concept originality. The\nstudy establishes practical guidelines for model selection and configuration,\ndemonstrating how temperature adjustments and architectural considerations\nimpact humor generation effectiveness. These findings advance understanding of\nLLM capabilities in creative technical writing and provide empirically\nvalidated configuration strategies for developers implementing humor-generation\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "criteria"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02864", "pdf": "https://arxiv.org/pdf/2504.02864", "abs": "https://arxiv.org/abs/2504.02864", "authors": ["Peter Adelson", "Julian Nyarko"], "title": "The Material Contracts Corpus", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Material Contracts Corpus (MCC), a publicly\navailable dataset comprising over one million contracts filed by public\ncompanies with the U.S. Securities and Exchange Commission (SEC) between 2000\nand 2023. The MCC facilitates empirical research on contract design and legal\nlanguage, and supports the development of AI-based legal tools. Contracts in\nthe corpus are categorized by agreement type and linked to specific parties\nusing machine learning and natural language processing techniques, including a\nfine-tuned LLaMA-2 model for contract classification. The MCC further provides\nmetadata such as filing form, document format, and amendment status. We\ndocument trends in contractual language, length, and complexity over time, and\nhighlight the dominance of employment and security agreements in SEC filings.\nThis resource is available for bulk download and online access at\nhttps://mcc.law.stanford.edu.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "agreement"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02895", "pdf": "https://arxiv.org/pdf/2504.02895", "abs": "https://arxiv.org/abs/2504.02895", "authors": ["Farida Al Haddad", "Yuxin Wang", "Malcolm Mielle"], "title": "UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture Detection", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "Artificial intelligence has the potential to impact safety and efficiency in\nsafety-critical domains such as construction, manufacturing, and healthcare.\nFor example, using sensor data from wearable devices, such as inertial\nmeasurement units (IMUs), human gestures can be detected while maintaining\nprivacy, thereby ensuring that safety protocols are followed. However, strict\nsafety requirements in these domains have limited the adoption of AI, since\naccurate calibration of predicted probabilities and robustness against\nout-of-distribution (OOD) data is necessary.\n  This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step\nmethod to address these challenges in IMU-based gesture recognition. First, we\npresent an uncertainty-aware gesture network architecture that predicts both\ngesture probabilities and their associated uncertainties from IMU data. This\nuncertainty is then used to calibrate the probabilities of each potential\ngesture. Second, an entropy-weighted expectation of predictions over multiple\nIMU data windows is used to improve accuracy while maintaining correct\ncalibration.\n  Our method is evaluated using three publicly available IMU datasets for\ngesture detection and is compared to three state-of-the-art calibration methods\nfor neural networks: temperature scaling, entropy maximization, and Laplace\napproximation. UAC outperforms existing methods, achieving improved accuracy\nand calibration in both OOD and in-distribution scenarios. Moreover, we find\nthat, unlike our method, none of the state-of-the-art methods significantly\nimprove the calibration of IMU-based gesture recognition models. In conclusion,\nour work highlights the advantages of uncertainty-aware calibration of neural\nnetworks, demonstrating improvements in both calibration and accuracy for\ngesture detection using IMU data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02865", "pdf": "https://arxiv.org/pdf/2504.02865", "abs": "https://arxiv.org/abs/2504.02865", "authors": ["Yining Wang", "Yuquan Wang", "Xi Li", "Mi Zhang", "Geng Hong", "Min Yang"], "title": "The Illusionist's Prompt: Exposing the Factual Vulnerabilities of Large Language Models with Linguistic Nuances", "categories": ["cs.CL", "cs.LG"], "comment": "work in progress", "summary": "As Large Language Models (LLMs) continue to advance, they are increasingly\nrelied upon as real-time sources of information by non-expert users. To ensure\nthe factuality of the information they provide, much research has focused on\nmitigating hallucinations in LLM responses, but only in the context of formal\nuser queries, rather than maliciously crafted ones. In this study, we introduce\nThe Illusionist's Prompt, a novel hallucination attack that incorporates\nlinguistic nuances into adversarial queries, challenging the factual accuracy\nof LLMs against five types of fact-enhancing strategies. Our attack\nautomatically generates highly transferrable illusory prompts to induce\ninternal factual errors, all while preserving user intent and semantics.\nExtensive experiments confirm the effectiveness of our attack in compromising\nblack-box LLMs, including commercial APIs like GPT-4o and Gemini-2.0, even with\nvarious defensive mechanisms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02870", "pdf": "https://arxiv.org/pdf/2504.02870", "abs": "https://arxiv.org/abs/2504.02870", "authors": ["Frank P. -W. Lo", "Jianing Qiu", "Zeyu Wang", "Haibao Yu", "Yeming Chen", "Gao Zhang", "Benny Lo"], "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by CVPR 2025 Workshop", "summary": "Resume screening is a critical yet time-intensive process in talent\nacquisition, requiring recruiters to analyze vast volume of job applications\nwhile remaining objective, accurate, and fair. With the advancements in Large\nLanguage Models (LLMs), their reasoning capabilities and extensive knowledge\nbases demonstrate new opportunities to streamline and automate recruitment\nworkflows. In this work, we propose a multi-agent framework for resume\nscreening using LLMs to systematically process and evaluate resumes. The\nframework consists of four core agents, including a resume extractor, an\nevaluator, a summarizer, and a score formatter. To enhance the contextual\nrelevance of candidate assessments, we integrate Retrieval-Augmented Generation\n(RAG) within the resume evaluator, allowing incorporation of external knowledge\nsources, such as industry-specific expertise, professional certifications,\nuniversity rankings, and company-specific hiring criteria. This dynamic\nadaptation enables personalized recruitment, bridging the gap between AI\nautomation and talent acquisition. We assess the effectiveness of our approach\nby comparing AI-generated scores with ratings provided by HR professionals on a\ndataset of anonymized online resumes. The findings highlight the potential of\nmulti-agent RAG-LLM systems in automating resume screening, enabling more\nefficient and scalable hiring workflows.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "criteria"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02871", "pdf": "https://arxiv.org/pdf/2504.02871", "abs": "https://arxiv.org/abs/2504.02871", "authors": ["Enshuo Hsu", "Martin Ugbala", "Krishna Kumar Kookal", "Zouaidi Kawtar", "Nicholas L. Rider", "Muhammad F. Walji", "Kirk Roberts"], "title": "Synthesized Annotation Guidelines are Knowledge-Lite Boosters for Clinical Information Extraction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Generative information extraction using large language models, particularly\nthrough few-shot learning, has become a popular method. Recent studies indicate\nthat providing a detailed, human-readable guideline-similar to the annotation\nguidelines traditionally used for training human annotators can significantly\nimprove performance. However, constructing these guidelines is both labor- and\nknowledge-intensive. Additionally, the definitions are often tailored to meet\nspecific needs, making them highly task-specific and often non-reusable.\nHandling these subtle differences requires considerable effort and attention to\ndetail. In this study, we propose a self-improving method that harvests the\nknowledge summarization and text generation capacity of LLMs to synthesize\nannotation guidelines while requiring virtually no human input. Our zero-shot\nexperiments on the clinical named entity recognition benchmarks, 2012 i2b2\nEVENT, 2012 i2b2 TIMEX, 2014 i2b2, and 2018 n2c2 showed 25.86%, 4.36%, 0.20%,\nand 7.75% improvements in strict F1 scores from the no-guideline baseline. The\nLLM-synthesized guidelines showed equivalent or better performance compared to\nhuman-written guidelines by 1.15% to 4.14% in most tasks. In conclusion, this\nstudy proposes a novel LLM self-improving method that requires minimal\nknowledge and human input and is applicable to multiple biomedical domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "summarization"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02920", "pdf": "https://arxiv.org/pdf/2504.02920", "abs": "https://arxiv.org/abs/2504.02920", "authors": ["Anurag Kulkarni"], "title": "LiDAR-based Object Detection with Real-time Voice Specifications", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures, submitted as part of MSc research", "summary": "This paper presents a LiDAR-based object detection system with real-time\nvoice specifications, integrating KITTI's 3D point clouds and RGB images\nthrough a multi-modal PointNet framework. It achieves 87.0% validation accuracy\non a 3000-sample subset, surpassing a 200-sample baseline of 67.5% by combining\nspatial and visual data, addressing class imbalance with weighted loss, and\nrefining training via adaptive techniques. A Tkinter prototype provides natural\nIndian male voice output using Edge TTS (en-IN-PrabhatNeural), alongside 3D\nvisualizations and real-time feedback, enhancing accessibility and safety in\nautonomous navigation, assistive technology, and beyond. The study offers a\ndetailed methodology, comprehensive experimental analysis, and a broad review\nof applications and challenges, establishing this work as a scalable\nadvancement in human-computer interaction and environmental perception, aligned\nwith current research trends.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02874", "pdf": "https://arxiv.org/pdf/2504.02874", "abs": "https://arxiv.org/abs/2504.02874", "authors": ["Luis Felipe", "Carlos Garcia", "Issam El Naqa", "Monique Shotande", "Aakash Tripathi", "Vivek Rudrapatna", "Ghulam Rasool", "Danielle Bitterman", "Gilmer Valdes"], "title": "TheBlueScrubs-v1, a comprehensive curated medical dataset derived from the internet", "categories": ["cs.CL"], "comment": "22 pages, 8 figures, 10 tables", "summary": "The need for robust and diverse data sets to train clinical large language\nmodels (cLLMs) is critical given that currently available public repositories\noften prove too limited in size or scope for comprehensive medical use. While\nresources like PubMed provide foundational medical literature, they capture\nonly a narrow range of formal publications and omit the broader medical\ndiscourse on the internet. To address these deficits, we introduce\nTheBlueScrubs-v1, a curated dataset of over 25 billion medical tokens - nearly\nthree times larger than PubMed - drawn from a broad-scale internet corpus. Our\ntwo-stage filtering pipeline employs a Logistic Regression model for document\nscreening (achieving an AUC of approximately 0.95 on external validation),\nfollowed by verification via a 70B-parameter Llama 3.1 instruct model. Each\ntext is assigned three LLM-based quality scores encompassing medical relevance,\nprecision and factual detail, and safety and ethical standards. Clinician\nreviews confirm high concordance with these automated evaluations, and a\nspecialized cancer classifier further labels approximately 11 billion oncology\ntokens. Two demonstration tasks highlight the dataset's practical value: first,\nwe distill the safety evaluations to a smaller BERT-style model that reaches an\nAUC near 0.96 on unseen data; second, we fine-tune a compact LLM on a filtered\nsubset, showing measurable improvements over standard baselines in medical\nbenchmarks as well as private ones. This Data Descriptor details the dataset's\ncreation and validation, underscoring its potential utility for medical AI\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03010", "pdf": "https://arxiv.org/pdf/2504.03010", "abs": "https://arxiv.org/abs/2504.03010", "authors": ["Shaoyuan Xu", "Yang Cheng", "Qian Lin", "Jan P. Allebach"], "title": "Emotion Recognition Using Convolutional Neural Networks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Emotion has an important role in daily life, as it helps people better\ncommunicate with and understand each other more efficiently. Facial expressions\ncan be classified into 7 categories: angry, disgust, fear, happy, neutral, sad\nand surprise. How to detect and recognize these seven emotions has become a\npopular topic in the past decade. In this paper, we develop an emotion\nrecognition system that can apply emotion recognition on both still images and\nreal-time videos by using deep learning.\n  We build our own emotion recognition classification and regression system\nfrom scratch, which includes dataset collection, data preprocessing , model\ntraining and testing. Given a certain image or a real-time video, our system is\nable to show the classification and regression results for all of the 7\nemotions. The proposed system is tested on 2 different datasets, and achieved\nan accuracy of over 80\\%. Moreover, the result obtained from real-time testing\nproves the feasibility of implementing convolutional neural networks in real\ntime to detect emotions accurately and efficiently.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03011", "pdf": "https://arxiv.org/pdf/2504.03011", "abs": "https://arxiv.org/abs/2504.03011", "authors": ["Junying Wang", "Jingyuan Liu", "Xin Sun", "Krishna Kumar Singh", "Zhixin Shu", "He Zhang", "Jimei Yang", "Nanxuan Zhao", "Tuanfeng Y. Wang", "Simon S. Chen", "Ulrich Neumann", "Jae Shin Yoon"], "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization", "categories": ["cs.CV"], "comment": "Project page:https://junyingw.github.io/paper/relighting. Accepted by\n  CVPR 2025", "summary": "This paper introduces Comprehensive Relighting, the first all-in-one approach\nthat can both control and harmonize the lighting from an image or video of\nhumans with arbitrary body parts from any scene. Building such a generalizable\nmodel is extremely challenging due to the lack of dataset, restricting existing\nimage-based relighting models to a specific scenario (e.g., face or static\nhuman). To address this challenge, we repurpose a pre-trained diffusion model\nas a general image prior and jointly model the human relighting and background\nharmonization in the coarse-to-fine framework. To further enhance the temporal\ncoherence of the relighting, we introduce an unsupervised temporal lighting\nmodel that learns the lighting cycle consistency from many real-world videos\nwithout any ground truth. In inference time, our temporal lighting module is\ncombined with the diffusion models through the spatio-temporal feature blending\nalgorithms without extra training; and we apply a new guided refinement as a\npost-processing to preserve the high-frequency details from the input image. In\nthe experiments, Comprehensive Relighting shows a strong generalizability and\nlighting temporal coherence, outperforming existing image-based human\nrelighting and harmonization methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02882", "pdf": "https://arxiv.org/pdf/2504.02882", "abs": "https://arxiv.org/abs/2504.02882", "authors": ["Sunghee Jung", "Donghun Lee", "Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Junrae Cho", "Kihyun Kim", "Eunggyun Kim", "Myeongcheol Shin"], "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02887", "pdf": "https://arxiv.org/pdf/2504.02887", "abs": "https://arxiv.org/abs/2504.02887", "authors": ["John Chen", "Alexandros Lotsos", "Grace Wang", "Lexie Zhao", "Bruce Sherin", "Uri Wilensky", "Michael Horn"], "title": "Processes Matter: How ML/GAI Approaches Could Support Open Qualitative Coding of Online Discourse Datasets", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "This paper was recommended for acceptance as a long paper by CSCL\n  reviewers, but ends up as a short paper. The arXiv version here is its longer\n  form, revised with reviewers' comments", "summary": "Open coding, a key inductive step in qualitative research, discovers and\nconstructs concepts from human datasets. However, capturing extensive and\nnuanced aspects or \"coding moments\" can be challenging, especially with large\ndiscourse datasets. While some studies explore machine learning (ML)/Generative\nAI (GAI)'s potential for open coding, few evaluation studies exist. We compare\nopen coding results by five recently published ML/GAI approaches and four human\ncoders, using a dataset of online chat messages around a mobile learning\nsoftware. Our systematic analysis reveals ML/GAI approaches' strengths and\nweaknesses, uncovering the complementary potential between humans and AI.\nLine-by-line AI approaches effectively identify content-based codes, while\nhumans excel in interpreting conversational dynamics. We discussed how embedded\nanalytical processes could shape the results of ML/GAI approaches. Instead of\nreplacing humans in open coding, researchers should integrate AI with and\naccording to their analytical processes, e.g., as parallel co-coders.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03052", "pdf": "https://arxiv.org/pdf/2504.03052", "abs": "https://arxiv.org/abs/2504.03052", "authors": ["Hyun-Ho Choi", "Kangsoo Kim", "Ki-Ho Lee", "Kisong Lee"], "title": "Cooperative Inference for Real-Time 3D Human Pose Estimation in Multi-Device Edge Networks", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 12 figures", "summary": "Accurate and real-time three-dimensional (3D) pose estimation is challenging\nin resource-constrained and dynamic environments owing to its high\ncomputational complexity. To address this issue, this study proposes a novel\ncooperative inference method for real-time 3D human pose estimation in mobile\nedge computing (MEC) networks. In the proposed method, multiple end devices\nequipped with lightweight inference models employ dual confidence thresholds to\nfilter ambiguous images. Only the filtered images are offloaded to an edge\nserver with a more powerful inference model for re-evaluation, thereby\nimproving the estimation accuracy under computational and communication\nconstraints. We numerically analyze the performance of the proposed inference\nmethod in terms of the inference accuracy and end-to-end delay and formulate a\njoint optimization problem to derive the optimal confidence thresholds and\ntransmission time for each device, with the objective of minimizing the mean\nper-joint position error (MPJPE) while satisfying the required end-to-end delay\nconstraint. To solve this problem, we demonstrate that minimizing the MPJPE is\nequivalent to maximizing the sum of the inference accuracies for all devices,\ndecompose the problem into manageable subproblems, and present a low-complexity\noptimization algorithm to obtain a near-optimal solution. The experimental\nresults show that a trade-off exists between the MPJPE and end-to-end delay\ndepending on the confidence thresholds. Furthermore, the results confirm that\nthe proposed cooperative inference method achieves a significant reduction in\nthe MPJPE through the optimal selection of confidence thresholds and\ntransmission times, while consistently satisfying the end-to-end delay\nrequirement in various MEC environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03059", "pdf": "https://arxiv.org/pdf/2504.03059", "abs": "https://arxiv.org/abs/2504.03059", "authors": ["Haishan Wang", "Mohammad Hassan Vali", "Arno Solin"], "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D\nreconstruction, achieving high-quality results with real-time radiance field\nrendering. However, a key challenge is the substantial storage cost:\nreconstructing a single scene typically requires millions of Gaussian splats,\neach represented by 59 floating-point parameters, resulting in approximately\n1~GB of memory. To address this challenge, we propose a compression method by\nbuilding separate attribute codebooks and storing only discrete code indices.\nSpecifically, we employ noise-substituted vector quantization technique to\njointly train the codebooks and model features, ensuring consistency between\ngradient descent optimization and parameter discretization. Our method reduces\nthe memory consumption efficiently (around $45\\times$) while maintaining\ncompetitive reconstruction quality on standard 3D benchmark scenes. Experiments\non different codebook sizes show the trade-off between compression ratio and\nimage quality. Furthermore, the trained compressed model remains fully\ncompatible with popular 3DGS viewers and enables faster rendering speed, making\nit well-suited for practical applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02902", "pdf": "https://arxiv.org/pdf/2504.02902", "abs": "https://arxiv.org/abs/2504.02902", "authors": ["Liangjie Huang", "Dawei Li", "Huan Liu", "Lu Cheng"], "title": "Beyond Accuracy: The Role of Calibration in Self-Improving Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable self-improvement\ncapabilities, whereby models iteratively revise their outputs through\nself-generated feedback. While this reflective mechanism has shown promise in\nenhancing task performance, recent studies suggest that it may also introduce\nundesirable biases-most notably, self-bias, or the tendency of LLMs to favor\ntheir own prior outputs. In this work, we extend this line of inquiry by\ninvestigating the impact on confidence estimation. We evaluate three\nrepresentative self-improvement paradigms-basic prompting, Chain-of-Thought\n(CoT) prompting, and tuning-based methods and find that iterative\nself-improvement can lead to systematic overconfidence, as evidenced by a\nsteadily increasing Expected Calibration Error (ECE) and lower accuracy with\nhigh confidence. We then further explore the integration of confidence\ncalibration techniques with self-improvement. Specifically, we compare three\nstrategies: (1) applying calibration after multiple rounds of self-improvement,\n(2) calibrating before self-improvement, and (3) applying calibration\niteratively at each self-improvement step. Our results show that iterative\ncalibration is most effective in reducing ECE, yielding improved calibration.\nOur work pioneers the study of self-improving LLMs from a calibration\nperspective, offering valuable insights into balancing model performance and\nreliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03096", "pdf": "https://arxiv.org/pdf/2504.03096", "abs": "https://arxiv.org/abs/2504.03096", "authors": ["Zhen Hao Sia", "Yogesh Singh Rawat"], "title": "Scaling Open-Vocabulary Action Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we focus on scaling open-vocabulary action detection. Existing\napproaches for action detection are predominantly limited to closed-set\nscenarios and rely on complex, parameter-heavy architectures. Extending these\nmodels to the open-vocabulary setting poses two key challenges: (1) the lack of\nlarge-scale datasets with many action classes for robust training, and (2)\nparameter-heavy adaptations to a pretrained vision-language contrastive model\nto convert it for detection, risking overfitting the additional non-pretrained\nparameters to base action classes. Firstly, we introduce an encoder-only\nmultimodal model for video action detection, reducing the reliance on\nparameter-heavy additions for video action detection. Secondly, we introduce a\nsimple weakly supervised training strategy to exploit an existing closed-set\naction detection dataset for pretraining. Finally, we depart from the ill-posed\nbase-to-novel benchmark used by prior works in open-vocabulary action detection\nand devise a new benchmark to evaluate on existing closed-set action detection\ndatasets without ever using them for training, showing novel results to serve\nas baselines for future work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03128", "pdf": "https://arxiv.org/pdf/2504.03128", "abs": "https://arxiv.org/abs/2504.03128", "authors": ["Kahim Wong", "Jicheng Zhou", "Kemou Li", "Yain-Whar Si", "Xiaowei Wu", "Jiantao Zhou"], "title": "FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of AI-generated content brings significant concerns on the\nforensic and security issues such as source tracing, copyright protection, etc,\nhighlighting the need for effective watermarking technologies. Font-based text\nwatermarking has emerged as an effective solution to embed information, which\ncould ensure copyright, traceability, and compliance of the generated text\ncontent. Existing font watermarking methods usually neglect essential font\nknowledge, which leads to watermarked fonts of low quality and limited\nembedding capacity. These methods are also vulnerable to real-world\ndistortions, low-resolution fonts, and inaccurate character segmentation. In\nthis paper, we introduce FontGuard, a novel font watermarking model that\nharnesses the capabilities of font models and language-guided contrastive\nlearning. Unlike previous methods that focus solely on the pixel-level\nalteration, FontGuard modifies fonts by altering hidden style features,\nresulting in better font quality upon watermark embedding. We also leverage the\nfont manifold to increase the embedding capacity of our proposed method by\ngenerating substantial font variants closely resembling the original font.\nFurthermore, in the decoder, we employ an image-text contrastive learning to\nreconstruct the embedded bits, which can achieve desirable robustness against\nvarious real-world transmission distortions. FontGuard outperforms\nstate-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under\nsynthetic, cross-media, and online social network distortions, respectively,\nwhile improving the visual quality by 52.7% in terms of LPIPS. Moreover,\nFontGuard uniquely allows the generation of watermarked fonts for unseen fonts\nwithout re-training the network. The code and dataset are available at\nhttps://github.com/KAHIMWONG/FontGuard.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02917", "pdf": "https://arxiv.org/pdf/2504.02917", "abs": "https://arxiv.org/abs/2504.02917", "authors": ["Thanathip Suenghataiphorn", "Narisara Tribuddharat", "Pojsakorn Danpanichkul", "Narathorn Kulthamrongsri"], "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Large language models (LLMs) are rapidly being integrated into\nhealthcare, promising to enhance various clinical tasks. However, concerns\nexist regarding their potential for bias, which could compromise patient care\nand exacerbate health inequities. This systematic review investigates the\nprevalence, sources, manifestations, and clinical implications of bias in LLMs.\nMethods: We conducted a systematic search of PubMed, OVID, and EMBASE from\ndatabase inception through 2025, for studies evaluating bias in LLMs applied to\nclinical tasks. We extracted data on LLM type, bias source, bias manifestation,\naffected attributes, clinical task, evaluation methods, and outcomes. Risk of\nbias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies\nmet inclusion criteria, revealing pervasive bias across various LLMs and\nclinical applications. Both data-related bias (from biased training data) and\nmodel-related bias (from model training) were significant contributors. Biases\nmanifested as: allocative harm (e.g., differential treatment recommendations);\nrepresentational harm (e.g., stereotypical associations, biased image\ngeneration); and performance disparities (e.g., variable output quality). These\nbiases affected multiple attributes, most frequently race/ethnicity and gender,\nbut also age, disability, and language. Conclusions: Bias in clinical LLMs is a\npervasive and systemic issue, with a potential to lead to misdiagnosis and\ninappropriate treatment, particularly for marginalized patient populations.\nRigorous evaluation of the model is crucial. Furthermore, the development and\nimplementation of effective mitigation strategies, coupled with continuous\nmonitoring in real-world clinical settings, are essential to ensure the safe,\nequitable, and trustworthy deployment of LLMs in healthcare.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03133", "pdf": "https://arxiv.org/pdf/2504.03133", "abs": "https://arxiv.org/abs/2504.03133", "authors": ["Zahid Hassan Tushar", "Adeleke Ademakinwa", "Jianwu Wang", "Zhibo Zhang", "Sanjay Purushotham"], "title": "Joint Retrieval of Cloud properties using Attention-based Deep Learning Models", "categories": ["cs.CV"], "comment": "6 Pages, 4 figures, to be published in 2025 IEEE International\n  Geoscience and Remote Sensing Symposium (IGARSS 2025)", "summary": "Accurate cloud property retrieval is vital for understanding cloud behavior\nand its impact on climate, including applications in weather forecasting,\nclimate modeling, and estimating Earth's radiation balance. The Independent\nPixel Approximation (IPA), a widely used physics-based approach, simplifies\nradiative transfer calculations by assuming each pixel is independent of its\nneighbors. While computationally efficient, IPA has significant limitations,\nsuch as inaccuracies from 3D radiative effects, errors at cloud edges, and\nineffectiveness for overlapping or heterogeneous cloud fields. Recent\nAI/ML-based deep learning models have improved retrieval accuracy by leveraging\nspatial relationships across pixels. However, these models are often\nmemory-intensive, retrieve only a single cloud property, or struggle with joint\nproperty retrievals. To overcome these challenges, we introduce CloudUNet with\nAttention Module (CAM), a compact UNet-based model that employs attention\nmechanisms to reduce errors in thick, overlapping cloud regions and a\nspecialized loss function for joint retrieval of Cloud Optical Thickness (COT)\nand Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES)\ndataset show that our CAM model outperforms state-of-the-art deep learning\nmethods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER,\nand achieving 76% and 86% lower MAE for COT and CER retrievals compared to the\nIPA method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02983", "pdf": "https://arxiv.org/pdf/2504.02983", "abs": "https://arxiv.org/abs/2504.02983", "authors": ["Xiaoyu Tong", "Zhi Zhang", "Martha Lewis", "Ekaterina Shutova"], "title": "Hummus: A Dataset of Humorous Multimodal Metaphor Use", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Metaphor and humor share a lot of common ground, and metaphor is one of the\nmost common humorous mechanisms. This study focuses on the humorous capacity of\nmultimodal metaphors, which has not received due attention in the community. We\ntake inspiration from the Incongruity Theory of humor, the Conceptual Metaphor\nTheory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and\ndeveloped a novel annotation scheme for humorous multimodal metaphor use in\nimage-caption pairs. We create the Hummus Dataset of Humorous Multimodal\nMetaphor Use, providing expert annotation on 1k image-caption pairs sampled\nfrom the New Yorker Caption Contest corpus. Using the dataset, we test\nstate-of-the-art multimodal large language models (MLLMs) on their ability to\ndetect and understand humorous multimodal metaphor use. Our experiments show\nthat current MLLMs still struggle with processing humorous multimodal\nmetaphors, particularly with regard to integrating visual and textual\ninformation. We release our dataset and code at\ngithub.com/xiaoyuisrain/humorous-multimodal-metaphor-use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03169", "pdf": "https://arxiv.org/pdf/2504.03169", "abs": "https://arxiv.org/abs/2504.03169", "authors": ["Shabnam Choudhury", "Yash Salunkhe", "Sarthak Mehrotra", "Biplab Banerjee"], "title": "REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval", "categories": ["cs.CV"], "comment": "14 pages", "summary": "The rapid expansion of remote sensing image archives demands the development\nof strong and efficient techniques for content-based image retrieval (RS-CBIR).\nThis paper presents REJEPA (Retrieval with Joint-Embedding Predictive\nArchitecture), an innovative self-supervised framework designed for unimodal\nRS-CBIR. REJEPA utilises spatially distributed context token encoding to\nforecast abstract representations of target tokens, effectively capturing\nhigh-level semantic features and eliminating unnecessary pixel-level details.\nIn contrast to generative methods that focus on pixel reconstruction or\ncontrastive techniques that depend on negative pairs, REJEPA functions within\nfeature space, achieving a reduction in computational complexity of 40-60% when\ncompared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To\nguarantee strong and varied representations, REJEPA incorporates\nVariance-Invariance-Covariance Regularisation (VICReg), which prevents encoder\ncollapse by promoting feature diversity and reducing redundancy. The method\ndemonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K\n(S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel\ncompared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE,\nScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and\nSAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across\nsensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for\nefficient, scalable, and precise RS-CBIR, addressing challenges like varying\nresolutions, high object density, and complex backgrounds with computational\nefficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03171", "pdf": "https://arxiv.org/pdf/2504.03171", "abs": "https://arxiv.org/abs/2504.03171", "authors": ["Zeyang Zheng", "Arman Hosseini", "Dong Chen", "Omid Shoghli", "Arsalan Heydarian"], "title": "Real-Time Roadway Obstacle Detection for Electric Scooters Using Deep Learning and Multi-Sensor Fusion", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted at ASCE International Conference on Computing in Civil\n  Engineering (i3ce)", "summary": "The increasing adoption of electric scooters (e-scooters) in urban areas has\ncoincided with a rise in traffic accidents and injuries, largely due to their\nsmall wheels, lack of suspension, and sensitivity to uneven surfaces. While\ndeep learning-based object detection has been widely used to improve automobile\nsafety, its application for e-scooter obstacle detection remains unexplored.\nThis study introduces a novel ground obstacle detection system for e-scooters,\nintegrating an RGB camera, and a depth camera to enhance real-time road hazard\ndetection. Additionally, the Inertial Measurement Unit (IMU) measures linear\nvertical acceleration to identify surface vibrations, guiding the selection of\nsix obstacle categories: tree branches, manhole covers, potholes, pine cones,\nnon-directional cracks, and truncated domes. All sensors, including the RGB\ncamera, depth camera, and IMU, are integrated within the Intel RealSense Camera\nD435i. A deep learning model powered by YOLO detects road hazards and utilizes\ndepth data to estimate obstacle proximity. Evaluated on the seven hours of\nnaturalistic riding dataset, the system achieves a high mean average precision\n(mAP) of 0.827 and demonstrates excellent real-time performance. This approach\nprovides an effective solution to enhance e-scooter safety through advanced\ncomputer vision and data fusion. The dataset is accessible at\nhttps://zenodo.org/records/14583718, and the project code is hosted on\nhttps://github.com/Zeyang-Zheng/Real-Time-Roadway-Obstacle-Detection-for-Electric-Scooters.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03071", "pdf": "https://arxiv.org/pdf/2504.03071", "abs": "https://arxiv.org/abs/2504.03071", "authors": ["Ziyu Liu", "Lintao Tang", "Zeliang Sun", "Zhengliang Liu", "Yanjun Lyu", "Wei Ruan", "Yangshuang Xu", "Liang Shan", "Jiyoon Shin", "Xiaohe Chen", "Dajiang Zhu", "Tianming Liu", "Rongjie Liu", "Chao Huang"], "title": "AD-GPT: Large Language Models in Alzheimer's Disease", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have emerged as powerful tools for medical\ninformation retrieval, yet their accuracy and depth remain limited in\nspecialized domains such as Alzheimer's disease (AD), a growing global health\nchallenge. To address this gap, we introduce AD-GPT, a domain-specific\ngenerative pre-trained transformer designed to enhance the retrieval and\nanalysis of AD-related genetic and neurobiological information. AD-GPT\nintegrates diverse biomedical data sources, including potential AD-associated\ngenes, molecular genetic information, and key gene variants linked to brain\nregions. We develop a stacked LLM architecture combining Llama3 and BERT,\noptimized for four critical tasks in AD research: (1) genetic information\nretrieval, (2) gene-brain region relationship assessment, (3) gene-AD\nrelationship analysis, and (4) brain region-AD relationship mapping.\nComparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's\nsuperior precision and reliability across these tasks, underscoring its\npotential as a robust and specialized AI tool for advancing AD research and\nbiomarker discovery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03159", "pdf": "https://arxiv.org/pdf/2504.03159", "abs": "https://arxiv.org/abs/2504.03159", "authors": ["Junlang Qian", "Zixiao Zhu", "Hanzhang Zhou", "Zijian Feng", "Zepeng Zhai", "Kezhi Mao"], "title": "Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification via Efficient Multi-Token Prediction", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 (main Oral)", "summary": "Zero-shot text classification typically relies on prompt engineering, but the\ninherent prompt brittleness of large language models undermines its\nreliability. Minor changes in prompt can cause significant discrepancies in\nmodel performance. We attribute this prompt brittleness largely to the narrow\nfocus on nexttoken probabilities in existing methods. To address this, we\npropose Placeholding Parallel Prediction (P3), a novel approach that predicts\ntoken probabilities across multiple positions and simulates comprehensive\nsampling of generation paths in a single run of a language model. Experiments\nshow improved accuracy and up to 98% reduction in the standard deviation across\nprompts, boosting robustness. Even without a prompt, P3 maintains comparable\nperformance, reducing the need for prompt engineering.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03230", "pdf": "https://arxiv.org/pdf/2504.03230", "abs": "https://arxiv.org/abs/2504.03230", "authors": ["Yasmine Mustafa", "Mohamed Elmahallawy", "Tie Luo"], "title": "Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Alzheimer's disease (AD) leads to progressive cognitive decline, making early\ndetection crucial for effective intervention. While deep learning models have\nshown high accuracy in AD diagnosis, their lack of interpretability limits\nclinical trust and adoption. This paper introduces a novel pre-model approach\nleveraging Jacobian Maps (JMs) within a multi-modal framework to enhance\nexplainability and trustworthiness in AD detection. By capturing localized\nbrain volume changes, JMs establish meaningful correlations between model\npredictions and well-known neuroanatomical biomarkers of AD. We validate JMs\nthrough experiments comparing a 3D CNN trained on JMs versus on traditional\npreprocessed data, which demonstrates superior accuracy. We also employ 3D\nGrad-CAM analysis to provide both visual and quantitative insights, further\nshowcasing improved interpretability and diagnostic reliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03235", "pdf": "https://arxiv.org/pdf/2504.03235", "abs": "https://arxiv.org/abs/2504.03235", "authors": ["Ibne Farabi Shihab", "Anuj Sharma"], "title": "Crash Time Matters: HybridMamba for Fine-Grained Temporal Localization in Traffic Surveillance Footage", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traffic crash detection in long-form surveillance videos is critical for\nemergency response and infrastructure planning but remains difficult due to the\nbrief and rare nature of crash events. We introduce HybridMamba, a novel\narchitecture that combines visual transformers with state-space temporal\nmodeling to achieve accurate crash time localization. Our method uses\nmulti-level token compression and hierarchical temporal processing to remain\ncomputationally efficient without sacrificing temporal resolution. Evaluated on\na large-scale dataset from the Iowa Department of Transportation, HybridMamba\nachieves a mean absolute error of 1.50 seconds, with 65.2 percent of\npredictions within one second of the ground truth. It outperforms recent\nvideo-language models such as TimeChat and VideoLLaMA2 by up to 2.8 seconds,\nwhile using significantly fewer parameters. Our results demonstrate strong\ngeneralization across videos ranging from 2 to 40 minutes in diverse\nconditions. HybridMamba offers a robust and efficient solution for fine-grained\ntemporal localization in traffic surveillance. The code will be released upon\npublication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03295", "pdf": "https://arxiv.org/pdf/2504.03295", "abs": "https://arxiv.org/abs/2504.03295", "authors": ["Bingqian Wang", "Quan Fang", "Jiachen Sun", "Xiaoxiao Ma"], "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset and Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03337", "pdf": "https://arxiv.org/pdf/2504.03337", "abs": "https://arxiv.org/abs/2504.03337", "authors": ["Quanxing Xu", "Ling Zhou", "Xian Zhong", "Feifei Zhang", "Rubing Huang", "Chia-Wen Lin"], "title": "QIRL: Boosting Visual Question Answering via Optimized Question-Image Relation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing debiasing approaches in Visual Question Answering (VQA) primarily\nfocus on enhancing visual learning, integrating auxiliary models, or employing\ndata augmentation strategies. However, these methods exhibit two major\ndrawbacks. First, current debiasing techniques fail to capture the superior\nrelation between images and texts because prevalent learning frameworks do not\nenable models to extract deeper correlations from highly contrasting samples.\nSecond, they do not assess the relevance between the input question and image\nduring inference, as no prior work has examined the degree of input relevance\nin debiasing studies. Motivated by these limitations, we propose a novel\nframework, Optimized Question-Image Relation Learning (QIRL), which employs a\ngeneration-based self-supervised learning strategy. Specifically, two modules\nare introduced to address the aforementioned issues. The Negative Image\nGeneration (NIG) module automatically produces highly irrelevant question-image\npairs during training to enhance correlation learning, while the Irrelevant\nSample Identification (ISI) module improves model robustness by detecting and\nfiltering irrelevant inputs, thereby reducing prediction errors. Furthermore,\nto validate our concept of reducing output errors through filtering unrelated\nquestion-image inputs, we propose a specialized metric to evaluate the\nperformance of the ISI module. Notably, our approach is model-agnostic and can\nbe integrated with various VQA models. Extensive experiments on VQA-CPv2 and\nVQA-v2 demonstrate the effectiveness and generalization ability of our method.\nAmong data augmentation strategies, our approach achieves state-of-the-art\nresults.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "question answering"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03438", "pdf": "https://arxiv.org/pdf/2504.03438", "abs": "https://arxiv.org/abs/2504.03438", "authors": ["Sheng Yang", "Tong Zhan", "Shichen Qiao", "Jicheng Gong", "Qing Yang", "Yanfeng Lu", "Jian Wang"], "title": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object Perception in Autonomous Driving", "categories": ["cs.CV"], "comment": "CVPR 2025 WDFM-AD", "summary": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03442", "pdf": "https://arxiv.org/pdf/2504.03442", "abs": "https://arxiv.org/abs/2504.03442", "authors": ["Nasar Iqbal", "Niki Martinel"], "title": "Pyramid-based Mamba Multi-class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in convolutional neural networks (CNNs) and transformer-based\nmethods have improved anomaly detection and localization, but challenges\npersist in precisely localizing small anomalies. While CNNs face limitations in\ncapturing long-range dependencies, transformer architectures often suffer from\nsubstantial computational overheads. We introduce a state space model\n(SSM)-based Pyramidal Scanning Strategy (PSS) for multi-class anomaly detection\nand localization--a novel approach designed to address the challenge of small\nanomaly localization. Our method captures fine-grained details at multiple\nscales by integrating the PSS with a pre-trained encoder for multi-scale\nfeature extraction and a feature-level synthetic anomaly generator. An\nimprovement of $+1\\%$ AP for multi-class anomaly localization and a +$1\\%$\nincrease in AU-PRO on MVTec benchmark demonstrate our method's superiority in\nprecise anomaly localization across diverse industrial scenarios. The code is\navailable at https://github.com/iqbalmlpuniud/Pyramid Mamba.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03622", "pdf": "https://arxiv.org/pdf/2504.03622", "abs": "https://arxiv.org/abs/2504.03622", "authors": ["Zae Myung Kim", "Anand Ramachandran", "Farideh Tavazoee", "Joo-Kyung Kim", "Oleg Rokhlenko", "Dongyeop Kang"], "title": "Align to Structure: Aligning Large Language Models with Structural Information", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "policy optimization", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03641", "pdf": "https://arxiv.org/pdf/2504.03641", "abs": "https://arxiv.org/abs/2504.03641", "authors": ["Wulin Xie", "Yi-Fan Zhang", "Chaoyou Fu", "Yang Shi", "Bingyan Nie", "Hongkai Chen", "Zhang Zhang", "Liang Wang", "Tieniu Tan"], "title": "MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models", "categories": ["cs.CV"], "comment": "Project page: https://mme-unify.github.io/", "summary": "Existing MLLM benchmarks face significant challenges in evaluating Unified\nMLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional\ntasks, leading to inconsistent comparisons; 2) absence of benchmarks for\nmixed-modality generation, which fails to assess multimodal reasoning\ncapabilities. We present a comprehensive evaluation framework designed to\nsystematically assess U-MLLMs. Our benchmark includes: Standardized Traditional\nTask Evaluation. We sample from 12 datasets, covering 10 tasks with 30\nsubtasks, ensuring consistent and fair comparisons across studies.\" 2. Unified\nTask Assessment. We introduce five novel tasks testing multimodal reasoning,\nincluding image editing, commonsense QA with image generation, and geometric\nreasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs,\nsuch as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized\nunderstanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3).\nOur findings reveal substantial performance gaps in existing U-MLLMs,\nhighlighting the need for more robust models capable of handling mixed-modality\ntasks effectively. The code and evaluation data can be found in\nhttps://mme-unify.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03327", "pdf": "https://arxiv.org/pdf/2504.03327", "abs": "https://arxiv.org/abs/2504.03327", "authors": ["Makoto Takamoto", "Daniel Oñoro-Rubio", "Wiem Ben Rim", "Takashi Maruyama", "Bhushan Kotnis"], "title": "Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "11 pages, 6 figures, 15 Tables, accepted and to be published in TMLR", "summary": "Knowledge graph embedding (KGE) models encode the structural information of\nknowledge graphs to predicting new links. Effective training of these models\nrequires distinguishing between positive and negative samples with high\nprecision. Although prior research has shown that improving the quality of\nnegative samples can significantly enhance model accuracy, identifying\nhigh-quality negative samples remains a challenging problem. This paper\ntheoretically investigates the condition under which negative samples lead to\noptimal KG embedding and identifies a sufficient condition for an effective\nnegative sample distribution. Based on this theoretical foundation, we propose\n\\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that\n\\emph{generates} negative samples satisfying this condition, in contrast to\nconventional methods that focus on \\emph{identifying} challenging negative\nsamples within the training data. Importantly, the simplicity of \\textsc{EMU}\nensures seamless integration with existing KGE models and negative sampling\nmethods. To evaluate its efficacy, we conducted comprehensive experiments\nacross multiple datasets. The results consistently demonstrate significant\nimprovements in link prediction performance across various KGE models and\nnegative sampling methods. Notably, \\textsc{EMU} enables performance\nimprovements comparable to those achieved by models with embedding dimension\nfive times larger. An implementation of the method and experiments are\navailable at https://github.com/nec-research/EMU-KG.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02880", "pdf": "https://arxiv.org/pdf/2504.02880", "abs": "https://arxiv.org/abs/2504.02880", "authors": ["Junchi Zhou", "Haozhou Wang", "Yoichiro Kato", "Tejasri Nampally", "P. Rajalakshmi", "M. Balram", "Keisuke Katsura", "Hao Lu", "Yue Mu", "Wanneng Yang", "Yangmingrui Gao", "Feng Xiao", "Hongtao Chen", "Yuhao Chen", "Wenjuan Li", "Jingwen Wang", "Fenghua Yu", "Jian Zhou", "Wensheng Wang", "Xiaochun Hu", "Yuanzhu Yang", "Yanfeng Ding", "Wei Guo", "Shouyang Liu"], "title": "Global Rice Multi-Class Segmentation Dataset (RiceSEG): A Comprehensive and Diverse High-Resolution RGB-Annotated Images for the Development and Benchmarking of Rice Segmentation Algorithms", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Developing computer vision-based rice phenotyping techniques is crucial for\nprecision field management and accelerating breeding, thereby continuously\nadvancing rice production. Among phenotyping tasks, distinguishing image\ncomponents is a key prerequisite for characterizing plant growth and\ndevelopment at the organ scale, enabling deeper insights into eco-physiological\nprocesses. However, due to the fine structure of rice organs and complex\nillumination within the canopy, this task remains highly challenging,\nunderscoring the need for a high-quality training dataset. Such datasets are\nscarce, both due to a lack of large, representative collections of rice field\nimages and the time-intensive nature of annotation. To address this gap, we\nestablished the first comprehensive multi-class rice semantic segmentation\ndataset, RiceSEG. We gathered nearly 50,000 high-resolution, ground-based\nimages from five major rice-growing countries (China, Japan, India, the\nPhilippines, and Tanzania), encompassing over 6,000 genotypes across all growth\nstages. From these original images, 3,078 representative samples were selected\nand annotated with six classes (background, green vegetation, senescent\nvegetation, panicle, weeds, and duckweed) to form the RiceSEG dataset. Notably,\nthe sub-dataset from China spans all major genotypes and rice-growing\nenvironments from the northeast to the south. Both state-of-the-art\nconvolutional neural networks and transformer-based semantic segmentation\nmodels were used as baselines. While these models perform reasonably well in\nsegmenting background and green vegetation, they face difficulties during the\nreproductive stage, when canopy structures are more complex and multiple\nclasses are involved. These findings highlight the importance of our dataset\nfor developing specialized segmentation models for rice and other crops.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02983", "pdf": "https://arxiv.org/pdf/2504.02983", "abs": "https://arxiv.org/abs/2504.02983", "authors": ["Xiaoyu Tong", "Zhi Zhang", "Martha Lewis", "Ekaterina Shutova"], "title": "Hummus: A Dataset of Humorous Multimodal Metaphor Use", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Metaphor and humor share a lot of common ground, and metaphor is one of the\nmost common humorous mechanisms. This study focuses on the humorous capacity of\nmultimodal metaphors, which has not received due attention in the community. We\ntake inspiration from the Incongruity Theory of humor, the Conceptual Metaphor\nTheory, and the annotation scheme behind the VU Amsterdam Metaphor Corpus, and\ndeveloped a novel annotation scheme for humorous multimodal metaphor use in\nimage-caption pairs. We create the Hummus Dataset of Humorous Multimodal\nMetaphor Use, providing expert annotation on 1k image-caption pairs sampled\nfrom the New Yorker Caption Contest corpus. Using the dataset, we test\nstate-of-the-art multimodal large language models (MLLMs) on their ability to\ndetect and understand humorous multimodal metaphor use. Our experiments show\nthat current MLLMs still struggle with processing humorous multimodal\nmetaphors, particularly with regard to integrating visual and textual\ninformation. We release our dataset and code at\ngithub.com/xiaoyuisrain/humorous-multimodal-metaphor-use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03146", "pdf": "https://arxiv.org/pdf/2504.03146", "abs": "https://arxiv.org/abs/2504.03146", "authors": ["Fatemeh Javadian", "Zahra Aminparast", "Johannes Stegmaier", "Abin Jose"], "title": "Comparative Analysis of Unsupervised and Supervised Autoencoders for Nuclei Classification in Clear Cell Renal Cell Carcinoma Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "I.2.10; I.4.9"], "comment": "Accepted 4-page paper at IEEE ISBI 2025. 3 figures, 3 tables", "summary": "This study explores the application of supervised and unsupervised\nautoencoders (AEs) to automate nuclei classification in clear cell renal cell\ncarcinoma (ccRCC) images, a diagnostic task traditionally reliant on subjective\nvisual grading by pathologists. We evaluate various AE architectures, including\nstandard AEs, contractive AEs (CAEs), and discriminative AEs (DAEs), as well as\na classifier-based discriminative AE (CDAE), optimized using the hyperparameter\ntuning tool Optuna. Bhattacharyya distance is selected from several metrics to\nassess class separability in the latent space, revealing challenges in\ndistinguishing adjacent grades using unsupervised models. CDAE, integrating a\nsupervised classifier branch, demonstrated superior performance in both latent\nspace separation and classification accuracy. Given that CDAE-CNN achieved\nnotable improvements in classification metrics, affirming the value of\nsupervised learning for class-specific feature extraction, F1 score was\nincorporated into the tuning process to optimize classification performance.\nResults show significant improvements in identifying aggressive ccRCC grades by\nleveraging the classification capability of AE through latent clustering\nfollowed by fine-grained classification. Our model outperforms the current\nstate of the art, CHR-Network, across all evaluated metrics. These findings\nsuggest that integrating a classifier branch in AEs, combined with neural\narchitecture search and contrastive learning, enhances grading automation in\nccRCC pathology, particularly in detecting aggressive tumor grades, and may\nimprove diagnostic accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03600", "pdf": "https://arxiv.org/pdf/2504.03600", "abs": "https://arxiv.org/abs/2504.03600", "authors": ["Jun Ma", "Zongxin Yang", "Sumin Kim", "Bihui Chen", "Mohammed Baharoon", "Adibvafa Fallahpour", "Reza Asakereh", "Hongwei Lyu", "Bo Wang"], "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "https://medsam2.github.io/", "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02876", "pdf": "https://arxiv.org/pdf/2504.02876", "abs": "https://arxiv.org/abs/2504.02876", "authors": ["Yangxiao Lu", "Ruosen Li", "Liqiang Jing", "Jikai Wang", "Xinya Du", "Yunhui Guo", "Nicholas Ruozzi", "Yu Xiang"], "title": "Multimodal Reference Visual Grounding", "categories": ["cs.CV", "cs.LG"], "comment": "Project page with our code and dataset:\n  https://irvlutd.github.io/MultiGrounding", "summary": "Visual grounding focuses on detecting objects from images based on language\nexpressions. Recent Large Vision-Language Models (LVLMs) have significantly\nadvanced visual grounding performance by training large models with large-scale\ndatasets. However, the problem remains challenging, especially when similar\nobjects appear in the input image. For example, an LVLM may not be able to\ndifferentiate Diet Coke and regular Coke in an image. In this case, if\nadditional reference images of Diet Coke and regular Coke are available, it can\nhelp the visual grounding of similar objects.\n  In this work, we introduce a new task named Multimodal Reference Visual\nGrounding (MRVG). In this task, a model has access to a set of reference images\nof objects in a database. Based on these reference images and a language\nexpression, the model is required to detect a target object from a query image.\nWe first introduce a new dataset to study the MRVG problem. Then we introduce a\nnovel method, named MRVG-Net, to solve this visual grounding problem. We show\nthat by efficiently using reference images with few-shot object detection and\nusing Large Language Models (LLMs) for object matching, our method achieves\nsuperior visual grounding performance compared to the state-of-the-art LVLMs\nsuch as Qwen2.5-VL-7B. Our approach bridges the gap between few-shot detection\nand visual grounding, unlocking new capabilities for visual understanding.\nProject page with our code and dataset:\nhttps://irvlutd.github.io/MultiGrounding", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02884", "pdf": "https://arxiv.org/pdf/2504.02884", "abs": "https://arxiv.org/abs/2504.02884", "authors": ["Baba Ibrahim", "Zhou Kui"], "title": "Enhancing Traffic Sign Recognition On The Performance Based On Yolov8", "categories": ["cs.CV", "cs.PF"], "comment": "27 Pages, 6 Figures, 10 Tables and 20 References", "summary": "This paper Traffic sign recognition plays a crucial role in the development\nof autonomous vehicles and advanced driver-assistance systems (ADAS). Despite\nsignificant advances in deep learning and object detection, accurately\ndetecting and classifying traffic signs remains challenging due to their small\nsizes, variable environmental conditions, occlusion, and class imbalance. This\nthesis presents an enhanced YOLOv8-based detection system that integrates\nadvanced data augmentation techniques, novel architectural enhancements\nincluding Coordinate Attention (CA), Bidirectional Feature Pyramid Network\n(BiFPN), and dynamic modules such as ODConv and LSKA, along with refined loss\nfunctions (EIoU and WIoU combined with Focal Loss). Extensive experiments\nconducted on datasets including GTSRB, TT100K, and GTSDB demonstrate marked\nimprovements in detection accuracy, robustness under adverse conditions, and\nreal-time inference on edge devices. The findings contribute actionable\ninsights for deploying reliable traffic sign recognition systems in real-world\nautonomous driving scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02912", "pdf": "https://arxiv.org/pdf/2504.02912", "abs": "https://arxiv.org/abs/2504.02912", "authors": ["Rohit Agarwal", "Aryan Dessai", "Arif Ahmed Sekh", "Krishna Agarwal", "Alexander Horsch", "Dilip K. Prasad"], "title": "Haphazard Inputs as Images in Online Learning", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "Accepted at IJCNN 2025", "summary": "The field of varying feature space in online learning settings, also known as\nhaphazard inputs, is very prominent nowadays due to its applicability in\nvarious fields. However, the current solutions to haphazard inputs are\nmodel-dependent and cannot benefit from the existing advanced deep-learning\nmethods, which necessitate inputs of fixed dimensions. Therefore, we propose to\ntransform the varying feature space in an online learning setting to a\nfixed-dimension image representation on the fly. This simple yet novel approach\nis model-agnostic, allowing any vision-based models to be applicable for\nhaphazard inputs, as demonstrated using ResNet and ViT. The image\nrepresentation handles the inconsistent input data seamlessly, making our\nproposed approach scalable and robust. We show the efficacy of our method on\nfour publicly available datasets. The code is available at\nhttps://github.com/Rohit102497/HaphazardInputsAsImages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02918", "pdf": "https://arxiv.org/pdf/2504.02918", "abs": "https://arxiv.org/abs/2504.02918", "authors": ["Chenyu Zhang", "Daniil Cherniavskii", "Andrii Zadaianchuk", "Antonios Tragoudaras", "Antonios Vozikis", "Thijmen Nijdam", "Derck W. E. Prinzhorn", "Mark Bodracska", "Nicu Sebe", "Efstratios Gavves"], "title": "Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image and video generation raise hopes that these models\npossess world modeling capabilities, the ability to generate realistic,\nphysically plausible videos. This could revolutionize applications in robotics,\nautonomous driving, and scientific simulation. However, before treating these\nmodels as world models, we must ask: Do they adhere to physical conservation\nlaws? To answer this, we introduce Morpheus, a benchmark for evaluating video\ngeneration models on physical reasoning. It features 80 real-world videos\ncapturing physical phenomena, guided by conservation laws. Since artificial\ngenerations lack ground truth, we assess physical plausibility using\nphysics-informed metrics evaluated with respect to infallible conservation laws\nknown per physical setting, leveraging advances in physics-informed neural\nnetworks and vision-language foundation models. Our findings reveal that even\nwith advanced prompting and video conditioning, current models struggle to\nencode physical principles despite generating aesthetically pleasing videos.\nAll data, leaderboard, and code are open-sourced at our project page.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02872", "pdf": "https://arxiv.org/pdf/2504.02872", "abs": "https://arxiv.org/abs/2504.02872", "authors": ["Ingmar Bakermans", "Daniel De Pascale", "Gonçalo Marcelino", "Giuseppe Cascavilla", "Zeno Geradts"], "title": "Scraping the Shadows: Deep Learning Breakthroughs in Dark Web Intelligence", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "comment": "17 pages, 17 images", "summary": "Darknet markets (DNMs) facilitate the trade of illegal goods on a global\nscale. Gathering data on DNMs is critical to ensuring law enforcement agencies\ncan effectively combat crime. Manually extracting data from DNMs is an\nerror-prone and time-consuming task. Aiming to automate this process we develop\na framework for extracting data from DNMs and evaluate the application of three\nstate-of-the-art Named Entity Recognition (NER) models, ELMo-BiLSTM\n\\citep{ShahEtAl2022}, UniversalNER \\citep{ZhouEtAl2024}, and GLiNER\n\\citep{ZaratianaEtAl2023}, at the task of extracting complex entities from DNM\nproduct listing pages. We propose a new annotated dataset, which we use to\ntrain, fine-tune, and evaluate the models. Our findings show that\nstate-of-the-art NER models perform well in information extraction from DNMs,\nachieving 91% Precision, 96% Recall, and an F1 score of 94%. In addition,\nfine-tuning enhances model performance, with UniversalNER achieving the best\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02949", "pdf": "https://arxiv.org/pdf/2504.02949", "abs": "https://arxiv.org/abs/2504.02949", "authors": ["Xianwei Zhuang", "Yuxin Xie", "Yufan Deng", "Dongchao Yang", "Liming Liang", "Jinghan Ru", "Yuguo Yin", "Yuexian Zou"], "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at: https://github.com/VARGPT-family/VARGPT-v1.1.\n  arXiv admin note: text overlap with arXiv:2501.12327", "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02873", "pdf": "https://arxiv.org/pdf/2504.02873", "abs": "https://arxiv.org/abs/2504.02873", "authors": ["Dongjun Wei", "Minjia Mao", "Xiao Fang", "Michael Chau"], "title": "Short-PHD: Detecting Short LLM-generated Text with Topological Data Analysis After Off-topic Content Insertion", "categories": ["cs.CL"], "comment": null, "summary": "The malicious usage of large language models (LLMs) has motivated the\ndetection of LLM-generated texts. Previous work in topological data analysis\nshows that the persistent homology dimension (PHD) of text embeddings can serve\nas a more robust and promising score than other zero-shot methods. However,\neffectively detecting short LLM-generated texts remains a challenge. This paper\npresents Short-PHD, a zero-shot LLM-generated text detection method tailored\nfor short texts. Short-PHD stabilizes the estimation of the previous PHD method\nfor short texts by inserting off-topic content before the given input text and\nidentifies LLM-generated text based on an established detection threshold.\nExperimental results on both public and generated datasets demonstrate that\nShort-PHD outperforms existing zero-shot methods in short LLM-generated text\ndetection. Implementation codes are available online.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02877", "pdf": "https://arxiv.org/pdf/2504.02877", "abs": "https://arxiv.org/abs/2504.02877", "authors": ["DongHyun Choi", "Lucas Spangher", "Chris Hidey", "Peter Grabowski", "Ramy Eskander"], "title": "Revisiting Funnel Transformers for Modern LLM Architectures with Comprehensive Ablations in Training and Inference Configurations", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Large Language Models, which suffer from high computational\ncosts, advance so quickly that techniques proposed to streamline earlier\niterations are not guaranteed to benefit more modern models. Building upon the\nFunnel Transformer proposed by Dai and Le (2020), which progressively\ncompresses intermediate representations, we investigate the impact of funneling\nin contemporary Gemma2 Transformer architectures. We systematically evaluate\nvarious funnel configurations and recovery methods, comparing: (1) standard\npretraining to funnel-aware pretraining strategies, (2) the impact of\nfunnel-aware fine-tuning, and (3) the type of sequence recovery operation. Our\nresults demonstrate that funneling creates information bottlenecks that\npropagate through deeper network layers, particularly in larger models (e.g.,\nGemma 7B), leading to at times unmanageable performance lost. However,\ncarefully selecting the funneling layer and employing effective recovery\nstrategies, can substantially mitigate performance losses, achieving up to a\n44\\% reduction in latency. Our findings highlight key trade-offs between\ncomputational efficiency and model accuracy, providing practical guidance for\ndeploying funnel-based approaches in large-scale natural language applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02881", "pdf": "https://arxiv.org/pdf/2504.02881", "abs": "https://arxiv.org/abs/2504.02881", "authors": ["Nick Whitehouse", "Nicole Lincoln", "Stephanie Yiu", "Lizzie Catterson", "Rivindu Perera"], "title": "Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers", "categories": ["cs.CL"], "comment": null, "summary": "Legal invoice review is a costly, inconsistent, and time-consuming process,\ntraditionally performed by Legal Operations, Lawyers or Billing Specialists who\nscrutinise billing compliance line by line. This study presents the first\nempirical comparison of Large Language Models (LLMs) against human invoice\nreviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations\nProfessionals-assessing their accuracy, speed, and cost-effectiveness.\nBenchmarking state-of-the-art LLMs against a ground truth set by expert legal\nprofessionals, our empirically substantiated findings reveal that LLMs\ndecisively outperform humans across every metric. In invoice approval\ndecisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by\nexperienced lawyers. On a granular level, LLMs dominate line-item\nclassification, with top models reaching F-scores of 81%, compared to just 43%\nfor the best-performing human group. Speed comparisons are even more striking -\nwhile lawyers take 194 to 316 seconds per invoice, LLMs are capable of\ncompleting reviews in as fast as 3.6 seconds. And cost? AI slashes review\nexpenses by 99.97%, reducing invoice processing costs from an average of $4.27\nper invoice for human invoice reviewers to mere cents. These results highlight\nthe evolving role of AI in legal spend management. As law firms and corporate\nlegal departments struggle with inefficiencies, this study signals a seismic\nshift: The era of LLM-powered legal spend management is not on the horizon, it\nhas arrived. The challenge ahead is not whether AI can perform as well as human\nreviewers, but how legal teams will strategically incorporate it, balancing\nautomation with human discretion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02883", "pdf": "https://arxiv.org/pdf/2504.02883", "abs": "https://arxiv.org/abs/2504.02883", "authors": ["Anil Ramakrishna", "Yixin Wan", "Xiaomeng Jin", "Kai-Wei Chang", "Zhiqi Bu", "Bhanukiran Vinzamuri", "Volkan Cevher", "Mingyi Hong", "Rahul Gupta"], "title": "SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SemEval-2025 Task 4: unlearning sensitive content from Large\nLanguage Models (LLMs). The task features 3 subtasks for LLM unlearning\nspanning different use cases: (1) unlearn long form synthetic creative\ndocuments spanning different genres; (2) unlearn short form synthetic\nbiographies containing personally identifiable information (PII), including\nfake names, phone number, SSN, email and home addresses, and (3) unlearn real\ndocuments sampled from the target model's training dataset. We received over\n100 submissions from over 30 institutions and we summarize the key techniques\nand lessons in this paper.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03026", "pdf": "https://arxiv.org/pdf/2504.03026", "abs": "https://arxiv.org/abs/2504.03026", "authors": ["Yiran Xu", "Siqi Xie", "Zhuofang Li", "Harris Shadmany", "Yinxiao Li", "Luciano Sbaiz", "Miaosen Wang", "Junjie Ke", "Jose Lezama", "Hang Qi", "Han Zhang", "Jesse Berent", "Ming-Hsuan Yang", "Irfan Essa", "Jia-Bin Huang", "Feng Yang"], "title": "HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations", "categories": ["cs.CV"], "comment": null, "summary": "Image retargeting aims to change the aspect-ratio of an image while\nmaintaining its content and structure with less visual artifacts. Existing\nmethods still generate many artifacts or fail to maintain original content or\nstructure. To address this, we introduce HALO, an end-to-end trainable solution\nfor image retargeting. Since humans are more sensitive to distortions in\nsalient areas than non-salient areas of an image, HALO decomposes the input\nimage into salient/non-salient layers and applies different wrapping fields to\ndifferent layers. To further minimize the structure distortion in the output\nimages, we propose perceptual structure similarity loss which measures the\nstructure similarity between input and output images and aligns with human\nperception. Both quantitative results and a user study on the RetargetMe\ndataset show that HALO achieves SOTA. Especially, our method achieves an 18.4%\nhigher user preference compared to the baselines on average.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02885", "pdf": "https://arxiv.org/pdf/2504.02885", "abs": "https://arxiv.org/abs/2504.02885", "authors": ["Hao Wang", "Shuchang Ye", "Jinghao Lin", "Usman Naseem", "Jinman Kim"], "title": "LVMed-R2: Perception and Reflection-driven Complex Reasoning for Medical Report Generation", "categories": ["cs.CL"], "comment": "10 pages, 3 figures, 1 table", "summary": "Large vision-language models (LVMs) hold a great promise for automating\nmedical report generation, potentially reducing the burden of manual reporting.\nState-of-the-art (SOTA) research fine-tunes general LVMs with medical data to\nalign radiology images to corresponding medical reports. However, there are two\nkey factors that limit these LVM's performance. Firstly, LVMs lack complex\nreasoning capability that leads to logical inconsistencies and potential\ndiagnostic errors in generated reports. Secondly, LVMs lack reflection\nmechanism that leads to an inability to discover errors in the thinking\nprocess. To address these gaps, we propose LVMed-R2, a new fine-tuning strategy\nthat introduces complex reasoning and reflection mechanisms for LVMs to enhance\nmedical report generation. To the best of our knowledge, this is the first work\nto introduce complex reasoning to the medical report generation (MRG) task. Our\nproposed complex reasoning contains medical knowledge injection and\nperception-enhancing modules which improve the accuracy of LVMs diagnosis,\ncoupled with a perception tree to provide guidance to limit the perception\nrange. Further, the reflection mechanism forces self-verification for outputs\nto correct for potential errors. We experimented by fine-tuning LVMs with our\nproposed LVMed-R2 strategy, using IU-Xray and MIMIC-CXR datasets. Our results,\nmeasured on natural language generation (NLG) metrics and clinical efficacy\n(CE) metrics, demonstrate that LVMs fine-tuned with the proposed reflection\nmechanism possess the ability to correct outputs and complex reasoning\neffectively and improve LVMs performance for MRG.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03041", "pdf": "https://arxiv.org/pdf/2504.03041", "abs": "https://arxiv.org/abs/2504.03041", "authors": ["Huiming Sun", "Yikang Li", "Kangning Yang", "Ruineng Li", "Daitao Xing", "Yangbo Xie", "Lan Fu", "Kaiyu Zhang", "Ming Chen", "Jiaming Ding", "Jiang Geng", "Jie Cai", "Zibo Meng", "Chiuman Ho"], "title": "VIP: Video Inpainting Pipeline for Real World Human Removal", "categories": ["cs.CV"], "comment": null, "summary": "Inpainting for real-world human and pedestrian removal in high-resolution\nvideo clips presents significant challenges, particularly in achieving\nhigh-quality outcomes, ensuring temporal consistency, and managing complex\nobject interactions that involve humans, their belongings, and their shadows.\nIn this paper, we introduce VIP (Video Inpainting Pipeline), a novel promptless\nvideo inpainting framework for real-world human removal applications. VIP\nenhances a state-of-the-art text-to-video model with a motion module and\nemploys a Variational Autoencoder (VAE) for progressive denoising in the latent\nspace. Additionally, we implement an efficient human-and-belongings\nsegmentation for precise mask generation. Sufficient experimental results\ndemonstrate that VIP achieves superior temporal consistency and visual fidelity\nacross diverse real-world scenarios, surpassing state-of-the-art methods on\nchallenging datasets. Our key contributions include the development of the VIP\npipeline, a reference frame integration technique, and the Dual-Fusion Latent\nSegment Refinement method, all of which address the complexities of inpainting\nin long, high-resolution video sequences.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03047", "pdf": "https://arxiv.org/pdf/2504.03047", "abs": "https://arxiv.org/abs/2504.03047", "authors": ["Reef Alturki", "Adrian Hilton", "Jean-Yves Guillemaut"], "title": "Attention-Aware Multi-View Pedestrian Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In spite of the recent advancements in multi-object tracking, occlusion poses\na significant challenge. Multi-camera setups have been used to address this\nchallenge by providing a comprehensive coverage of the scene. Recent multi-view\npedestrian detection models have highlighted the potential of an early-fusion\nstrategy, projecting feature maps of all views to a common ground plane or the\nBird's Eye View (BEV), and then performing detection. This strategy has been\nshown to improve both detection and tracking performance. However, the\nperspective transformation results in significant distortion on the ground\nplane, affecting the robustness of the appearance features of the pedestrians.\nTo tackle this limitation, we propose a novel model that incorporates attention\nmechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an\nearly-fusion strategy for detection, and a cross-attention mechanism to\nestablish robust associations between pedestrians in different frames, while\nefficiently propagating pedestrian features across frames, resulting in a more\nrobust feature representation for each pedestrian. Extensive experiments\ndemonstrate that our model outperforms state-of-the-art models, with an IDF1\nscore of $96.1\\%$ on Wildtrack dataset, and $85.7\\%$ on MultiviewX dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02891", "pdf": "https://arxiv.org/pdf/2504.02891", "abs": "https://arxiv.org/abs/2504.02891", "authors": ["Kurmanbek Kaiyrbekov", "Nicholas J Dobbins", "Sean D Mooney"], "title": "Automated Survey Collection with LLM-based Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: Traditional phone-based surveys are among the most accessible and\nwidely used methods to collect biomedical and healthcare data, however, they\nare often costly, labor intensive, and difficult to scale effectively. To\novercome these limitations, we propose an end-to-end survey collection\nframework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for\ndesigning the survey and recruiting participants, a conversational phone agent\npowered by an LLM that calls participants and administers the survey, a second\nLLM (GPT-4o) that analyzes the conversation transcripts generated during the\nsurveys, and a database for storing and organizing the results. To test our\nframework, we recruited 8 participants consisting of 5 native and 3 non-native\nenglish speakers and administered 40 surveys. We evaluated the correctness of\nLLM-generated conversation transcripts, accuracy of survey responses inferred\nby GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from\nconversation transcripts with an average accuracy of 98% despite transcripts\nexhibiting an average per-line word error rate of 7.7%. While participants\nnoted occasional errors made by the conversational LLM agent, they reported\nthat the agent effectively conveyed the purpose of the survey, demonstrated\ngood comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting\nand analyzing phone surveys for healthcare applications. By reducing the\nworkload on human interviewers and offering a scalable solution, this approach\npaves the way for real-world, end-to-end AI-powered phone survey collection\nsystems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03072", "pdf": "https://arxiv.org/pdf/2504.03072", "abs": "https://arxiv.org/abs/2504.03072", "authors": ["Pascal Chang", "Jingwei Tang", "Markus Gross", "Vinicius C. Azevedo"], "title": "How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2024 (Oral)", "summary": "Video editing and generation methods often rely on pre-trained image-based\ndiffusion models. During the diffusion process, however, the reliance on\nrudimentary noise sampling techniques that do not preserve correlations present\nin subsequent frames of a video is detrimental to the quality of the results.\nThis either produces high-frequency flickering, or texture-sticking artifacts\nthat are not amenable to post-processing. With this in mind, we propose a novel\nmethod for preserving temporal correlations in a sequence of noise samples.\nThis approach is materialized by a novel noise representation, dubbed\n$\\int$-noise (integral noise), that reinterprets individual noise samples as a\ncontinuously integrated noise field: pixel values do not represent discrete\nvalues, but are rather the integral of an underlying infinite-resolution noise\nover the pixel area. Additionally, we propose a carefully tailored transport\nmethod that uses $\\int$-noise to accurately advect noise samples over a\nsequence of frames, maximizing the correlation between different frames while\nalso preserving the noise properties. Our results demonstrate that the proposed\n$\\int$-noise can be used for a variety of tasks, such as video restoration,\nsurrogate rendering, and conditional video generation. See\nhttps://warpyournoise.github.io/ for video results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03089", "pdf": "https://arxiv.org/pdf/2504.03089", "abs": "https://arxiv.org/abs/2504.03089", "authors": ["Prashant Kumar", "Dheeraj Vattikonda", "Kshitij Madhav Bhat", "Kunal Dargan", "Prem Kalra"], "title": "SLACK: Attacking LiDAR-based SLAM with Adversarial Point Injections", "categories": ["cs.CV"], "comment": null, "summary": "The widespread adoption of learning-based methods for the LiDAR makes\nautonomous vehicles vulnerable to adversarial attacks through adversarial\n\\textit{point injections (PiJ)}. It poses serious security challenges for\nnavigation and map generation. Despite its critical nature, no major work\nexists that studies learning-based attacks on LiDAR-based SLAM. Our work\nproposes SLACK, an end-to-end deep generative adversarial model to attack LiDAR\nscans with several point injections without deteriorating LiDAR quality. To\nfacilitate SLACK, we design a novel yet simple autoencoder that augments\ncontrastive learning with segmentation-based attention for precise\nreconstructions. SLACK demonstrates superior performance on the task of\n\\textit{point injections (PiJ)} compared to the best baselines on KITTI and\nCARLA-64 dataset while maintaining accurate scan quality. We qualitatively and\nquantitatively demonstrate PiJ attacks using a fraction of LiDAR points. It\nseverely degrades navigation and map quality without deteriorating the LiDAR\nscan quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02904", "pdf": "https://arxiv.org/pdf/2504.02904", "abs": "https://arxiv.org/abs/2504.02904", "authors": ["Hongzhe Du", "Weikai Li", "Min Cai", "Karim Saraipour", "Zimin Zhang", "Himabindu Lakkaraju", "Yizhou Sun", "Shichang Zhang"], "title": "How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Post-training is essential for the success of large language models (LLMs),\ntransforming pre-trained base models into more useful and aligned post-trained\nmodels. While plenty of works have studied post-training algorithms and\nevaluated post-training models by their outputs, it remains understudied how\npost-training reshapes LLMs internally. In this paper, we compare base and\npost-trained LLMs mechanistically from four perspectives to better understand\npost-training effects. Our findings across model families and datasets reveal\nthat: (1) Post-training does not change the factual knowledge storage\nlocations, and it adapts knowledge representations from the base model while\ndeveloping new knowledge representations; (2) Both truthfulness and refusal can\nbe represented by linear vectors in the hidden representation space. The\ntruthfulness direction is highly similar between the base and post-trained\nmodel, and it is effectively transferable for interventions; (3) The refusal\ndirection is different between the base and post-trained models, and it shows\nlimited forward transferability; (4) Differences in confidence between the base\nand post-trained models cannot be attributed to entropy neurons. Our study\nprovides insights into the fundamental mechanisms preserved and altered during\npost-training, facilitates downstream tasks like model steering, and could\npotentially benefit future research in interpretability and LLM post-training.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["truthfulness"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03108", "pdf": "https://arxiv.org/pdf/2504.03108", "abs": "https://arxiv.org/abs/2504.03108", "authors": ["Xuanyu Liu", "Huiyun Yao", "Jinggui Gao", "Zhongyi Guo", "Xue Zhang", "Yulin Dong"], "title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT)\nare the main techniques used in Medical image segmentation. However, CNN is\nlimited to local contextual information, and ViT's quadratic complexity results\nin significant computational costs. At the same time, equipping the model to\ndistinguish lesion boundaries with varying degrees of severity is also a\nchallenge encountered in skin lesion segmentation. Purpose:This research aims\nto optimize the balance between computational costs and long-range dependency\nmodelling and achieve excellent generalization across lesions with different\ndegrees of severity. Methods:we propose a lightweight U-shape network that\nutilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the\nadvantages of Fastformer's additive attention mechanism, combining element-wise\nproduct and matrix product for comprehensive feature extraction and channel\nreduction to save computational costs. In order to accurately identify the\nlesion boundaries with varying degrees of severity, we designed Fusion\nMechanism including Multi-Granularity Fusion and Channel Fusion, which can\nprocess the feature maps in the granularity and channel levels to obtain\ndifferent contextual information. Results:Comprehensive experiments on the\nISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms\nexisting state-of-the-art models regarding parameter numbers, computational\ncomplexity and segmentation performance. In short, compared to MISSFormer, our\nmodel achieves superior segmentation performance while reducing parameter and\ncomputation costs by 101x and 15x, respectively. Conclusions:Both quantitative\nand qualitative analyses show that VFFM-UNet sets a new benchmark by reaching\nan ideal balance between parameter numbers, computational complexity, and\nsegmentation performance compared to existing state-of-the-art models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03118", "pdf": "https://arxiv.org/pdf/2504.03118", "abs": "https://arxiv.org/abs/2504.03118", "authors": ["Ziteng Wei", "Qiang He", "Bing Li", "Feifei Chen", "Yun Yang"], "title": "NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 12 figures, 6 tables", "summary": "Vision Transformers (ViTs) excel in computer vision tasks but lack\nflexibility for edge devices' diverse needs. A vital issue is that ViTs\npre-trained to cover a broad range of tasks are \\textit{over-qualified} for\nedge devices that usually demand only part of a ViT's knowledge for specific\ntasks. Their task-specific accuracy on these edge devices is suboptimal. We\ndiscovered that small ViTs that focus on device-specific tasks can improve\nmodel accuracy and in the meantime, accelerate model inference. This paper\npresents NuWa, an approach that derives small ViTs from the base ViT for edge\ndevices with specific task requirements. NuWa can transfer task-specific\nknowledge extracted from the base ViT into small ViTs that fully leverage\nconstrained resources on edge devices to maximize model accuracy with inference\nlatency assurance. Experiments with three base ViTs on three public datasets\ndemonstrate that compared with state-of-the-art solutions, NuWa improves model\naccuracy by up to $\\text{11.83}\\%$ and accelerates model inference by\n1.29$\\times$ - 2.79$\\times$. Code for reproduction is available at\nhttps://anonymous.4open.science/r/Task_Specific-3A5E.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02911", "pdf": "https://arxiv.org/pdf/2504.02911", "abs": "https://arxiv.org/abs/2504.02911", "authors": ["Mohammad Reza Ghasemi Madani", "Aryo Pradipta Gema", "Gabriele Sarti", "Yu Zhao", "Pasquale Minervini", "Andrea Passerini"], "title": "Noiser: Bounded Input Perturbations for Attributing Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2402.00794 by other authors", "summary": "Feature attribution (FA) methods are common post-hoc approaches that explain\nhow Large Language Models (LLMs) make predictions. Accordingly, generating\nfaithful attributions that reflect the actual inner behavior of the model is\ncrucial. In this paper, we introduce Noiser, a perturbation-based FA method\nthat imposes bounded noise on each input embedding and measures the robustness\nof the model against partially noised input to obtain the input attributions.\nAdditionally, we propose an answerability metric that employs an instructed\njudge model to assess the extent to which highly scored tokens suffice to\nrecover the predicted output. Through a comprehensive evaluation across six\nLLMs and three tasks, we demonstrate that Noiser consistently outperforms\nexisting gradient-based, attention-based, and perturbation-based FA methods in\nterms of both faithfulness and answerability, making it a robust and effective\napproach for explaining language model predictions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02965", "pdf": "https://arxiv.org/pdf/2504.02965", "abs": "https://arxiv.org/abs/2504.02965", "authors": ["Abhishek Sharma", "Dan Goldwasser"], "title": "CoLa -- Learning to Interactively Collaborate with Large LMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "LLMs' remarkable ability to tackle a wide range of language tasks opened new\nopportunities for collaborative human-AI problem solving. LLMs can amplify\nhuman capabilities by applying their intuitions and reasoning strategies at\nscale. We explore whether human guides can be simulated, by generalizing from\nhuman demonstrations of guiding an AI system to solve complex language\nproblems. We introduce CoLa, a novel self-guided learning paradigm for training\nautomated $\\textit{guides}$ and evaluate it on two QA datasets, a\npuzzle-solving task, and a constrained text generation task. Our empirical\nresults show that CoLa consistently outperforms competitive approaches across\nall domains. Moreover, a small-sized trained guide outperforms a strong model\nlike GPT-4 when acting as a guide. We compare the strategies employed by humans\nand automated guides by conducting a human study on a QA dataset. We show that\nautomated guides outperform humans by adapting their strategies to reasoners'\ncapabilities and conduct qualitative analyses highlighting distinct differences\nin guiding strategies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03154", "pdf": "https://arxiv.org/pdf/2504.03154", "abs": "https://arxiv.org/abs/2504.03154", "authors": ["Junshan Hu", "Jialiang Mao", "Zhikang Liu", "Zhongpu Xia", "Peng Jia", "Xianpeng Lang"], "title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference", "categories": ["cs.CV"], "comment": null, "summary": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03036", "pdf": "https://arxiv.org/pdf/2504.03036", "abs": "https://arxiv.org/abs/2504.03036", "authors": ["Zébulon Goriely", "Paula Buttery"], "title": "IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling", "categories": ["cs.CL"], "comment": "19 pages, 7 figures. Submitted to CoNLL 2025", "summary": "In this paper, we introduce two resources: (i) G2P+, a tool for converting\northographic datasets to a consistent phonemic representation; and (ii) IPA\nCHILDES, a phonemic dataset of child-centered speech across 31 languages. Prior\ntools for grapheme-to-phoneme conversion result in phonemic vocabularies that\nare inconsistent with established phonemic inventories, an issue which G2P+\naddresses by leveraging the inventories in the Phoible database. Using this\ntool, we augment CHILDES with phonemic transcriptions to produce IPA CHILDES.\nThis new resource fills several gaps in existing phonemic datasets, which often\nlack multilingual coverage, spontaneous speech, and a focus on child-directed\nlanguage. We demonstrate the utility of this dataset for phonological research\nby training phoneme language models on 11 languages and probing them for\ndistinctive features, finding that the distributional properties of phonemes\nare sufficient to learn major class and place features cross-lingually.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03177", "pdf": "https://arxiv.org/pdf/2504.03177", "abs": "https://arxiv.org/abs/2504.03177", "authors": ["Yuki Kawana", "Tatsuya Harada"], "title": "Detection Based Part-level Articulated Object Reconstruction from Single RGBD Image", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2023", "summary": "We propose an end-to-end trainable, cross-category method for reconstructing\nmultiple man-made articulated objects from a single RGBD image, focusing on\npart-level shape reconstruction and pose and kinematics estimation. We depart\nfrom previous works that rely on learning instance-level latent space, focusing\non man-made articulated objects with predefined part counts. Instead, we\npropose a novel alternative approach that employs part-level representation,\nrepresenting instances as combinations of detected parts. While our\ndetect-then-group approach effectively handles instances with diverse part\nstructures and various part counts, it faces issues of false positives, varying\npart sizes and scales, and an increasing model size due to end-to-end training.\nTo address these challenges, we propose 1) test-time kinematics-aware part\nfusion to improve detection performance while suppressing false positives, 2)\nanisotropic scale normalization for part shape learning to accommodate various\npart sizes and scales, and 3) a balancing strategy for cross-refinement between\nfeature space and output space to improve part detection while maintaining\nmodel size. Evaluation on both synthetic and real data demonstrates that our\nmethod successfully reconstructs variously structured multiple instances that\nprevious works cannot handle, and outperforms prior works in shape\nreconstruction and kinematics estimation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03101", "pdf": "https://arxiv.org/pdf/2504.03101", "abs": "https://arxiv.org/abs/2504.03101", "authors": ["Weili Cao", "Jianyou Wang", "Youze Zheng", "Longtian Bao", "Qirui Zheng", "Taylor Berg-Kirkpatrick", "Ramamohan Paturi", "Leon Bergen"], "title": "Single-Pass Document Scanning for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Handling extremely large documents for question answering is challenging:\nchunk-based embedding methods often lose track of important global context,\nwhile full-context transformers can be prohibitively expensive for hundreds of\nthousands of tokens. We propose a single-pass document scanning approach that\nprocesses the entire text in linear time, preserving global coherence while\ndeciding which sentences are most relevant to the query. On 41 QA benchmarks,\nour single-pass scanner consistently outperforms chunk-based embedding methods\nand competes with large language models at a fraction of the computational\ncost. By conditioning on the entire preceding context without chunk breaks, the\nmethod preserves global coherence, which is especially important for long\ndocuments. Overall, single-pass document scanning offers a simple solution for\nquestion answering over massive text. All code, datasets, and model checkpoints\nare available at https://github.com/MambaRetriever/MambaRetriever", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03151", "pdf": "https://arxiv.org/pdf/2504.03151", "abs": "https://arxiv.org/abs/2504.03151", "authors": ["Jing Bi", "Susan Liang", "Xiaofei Zhou", "Pinxin Liu", "Junjia Guo", "Yunlong Tang", "Luchuan Song", "Chao Huang", "Guangyu Sun", "Jinxi He", "Jiarui Wu", "Shu Yang", "Daoan Zhang", "Chen Chen", "Lianggong Bruce Wen", "Zhang Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03193", "pdf": "https://arxiv.org/pdf/2504.03193", "abs": "https://arxiv.org/abs/2504.03193", "authors": ["Xin Zhang", "Robby T. Tan"], "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03198", "pdf": "https://arxiv.org/pdf/2504.03198", "abs": "https://arxiv.org/abs/2504.03198", "authors": ["Jiaxin Guo", "Wenzhen Dong", "Tianyu Huang", "Hao Ding", "Ziyi Wang", "Haomin Kuang", "Qi Dou", "Yun-Hui Liu"], "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03165", "pdf": "https://arxiv.org/pdf/2504.03165", "abs": "https://arxiv.org/abs/2504.03165", "authors": ["Weitao Li", "Kaiming Liu", "Xiangyu Zhang", "Xuanyu Lei", "Weizhi Ma", "Yang Liu"], "title": "Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03174", "pdf": "https://arxiv.org/pdf/2504.03174", "abs": "https://arxiv.org/abs/2504.03174", "authors": ["Abhishek Singhania", "Christophe Dupuy", "Shivam Mangale", "Amani Namboori"], "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs", "categories": ["cs.CL"], "comment": "Accepted at TrustNLP@NAACL 2025", "summary": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03221", "pdf": "https://arxiv.org/pdf/2504.03221", "abs": "https://arxiv.org/abs/2504.03221", "authors": ["Jungpil Shin", "Abu Saleh Musa Miah", "Sota Konnai", "Shu Hoshitaka", "Pankoo Kim"], "title": "Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Hand gesture recognition using multichannel surface electromyography (sEMG)\nis challenging due to unstable predictions and inefficient time-varying feature\nenhancement. To overcome the lack of signal based time-varying feature\nproblems, we propose a lightweight squeeze-excitation deep learning-based multi\nstream spatial temporal dynamics time-varying feature extraction approach to\nbuild an effective sEMG-based hand gesture recognition system. Each branch of\nthe proposed model was designed to extract hierarchical features, capturing\nboth global and detailed spatial-temporal relationships to ensure feature\neffectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN),\nfocuses on capturing long-term temporal dependencies by modelling past and\nfuture temporal contexts, providing a holistic view of gesture dynamics. The\nsecond branch, incorporating a 1D Convolutional layer, separable CNN, and\nSqueeze-and-Excitation (SE) block, efficiently extracts spatial-temporal\nfeatures while emphasizing critical feature channels, enhancing feature\nrelevance. The third branch, combining a Temporal Convolutional Network (TCN)\nand Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships\nand time-varying patterns. Outputs from all branches are fused using\nconcatenation to capture subtle variations in the data and then refined with a\nchannel attention module, selectively focusing on the most informative features\nwhile improving computational efficiency. The proposed model was tested on the\nNinapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%,\nand 93.34%, respectively. These results demonstrate the capability of the\nsystem to handle complex sEMG dynamics, offering advancements in prosthetic\nlimb control and human-machine interface technologies with significant\nimplications for assistive technologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03185", "pdf": "https://arxiv.org/pdf/2504.03185", "abs": "https://arxiv.org/abs/2504.03185", "authors": ["Jaymari Chua", "Chen Wang", "Lina Yao"], "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4; I.2.6; I.2.8"], "comment": null, "summary": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03234", "pdf": "https://arxiv.org/pdf/2504.03234", "abs": "https://arxiv.org/abs/2504.03234", "authors": ["Junjie Yang", "Ke Lin", "Xing Yu"], "title": "Think When You Need: Self-Adaptive Chain-of-Thought Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages", "summary": "Chain of Thought (CoT) reasoning enhances language models' performance but\noften leads to inefficient \"overthinking\" on simple problems. We identify that\nexisting approaches directly penalizing reasoning length fail to account for\nvarying problem complexity. Our approach constructs rewards through length and\nquality comparisons, guided by theoretical assumptions that jointly enhance\nsolution correctness with conciseness. Moreover, we further demonstrate our\nmethod to fuzzy tasks where ground truth is unavailable. Experiments across\nmultiple reasoning benchmarks demonstrate that our method maintains accuracy\nwhile generating significantly more concise explanations, effectively teaching\nmodels to \"think when needed.\"", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03258", "pdf": "https://arxiv.org/pdf/2504.03258", "abs": "https://arxiv.org/abs/2504.03258", "authors": ["Shuxiao Ding", "Yutong Yang", "Julian Wiederer", "Markus Braun", "Peizheng Li", "Juergen Gall", "Bin Yang"], "title": "TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Query denoising has become a standard training strategy for DETR-based\ndetectors by addressing the slow convergence issue. Besides that, query\ndenoising can be used to increase the diversity of training samples for\nmodeling complex scenarios which is critical for Multi-Object Tracking (MOT),\nshowing its potential in MOT application. Existing approaches integrate query\ndenoising within the tracking-by-attention paradigm. However, as the denoising\nprocess only happens within the single frame, it cannot benefit the tracker to\nlearn temporal-related information. In addition, the attention mask in query\ndenoising prevents information exchange between denoising and object queries,\nlimiting its potential in improving association using self-attention. To\naddress these issues, we propose TQD-Track, which introduces Temporal Query\nDenoising (TQD) tailored for MOT, enabling denoising queries to carry temporal\ninformation and instance-specific feature representation. We introduce diverse\nnoise types onto denoising queries that simulate real-world challenges in MOT.\nWe analyze our proposed TQD for different tracking paradigms, and find out the\nparadigm with explicit learned data association module, e.g.\ntracking-by-detection or alternating detection and association, benefit from\nTQD by a larger margin. For these paradigms, we further design an association\nmask in the association module to ensure the consistent interaction between\ntrack and detection queries as during inference. Extensive experiments on the\nnuScenes dataset demonstrate that our approach consistently enhances different\ntracking methods by only changing the training process, especially the\nparadigms with explicit association module.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03312", "pdf": "https://arxiv.org/pdf/2504.03312", "abs": "https://arxiv.org/abs/2504.03312", "authors": ["Luís Couto Seller", "Íñigo Sanz Torres", "Adrián Vogel-Fernández", "Carlos González Carballo", "Pedro Miguel Sánchez Sánchez", "Adrián Carruana Martín", "Enrique de Miguel Ambite"], "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices", "categories": ["cs.CL", "cs.LG"], "comment": "Under Revision al SEPLN conference", "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03352", "pdf": "https://arxiv.org/pdf/2504.03352", "abs": "https://arxiv.org/abs/2504.03352", "authors": ["Kaustubh Shivshankar Shejole", "Pushpak Bhattacharyya"], "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social Psychological Underpinnings", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03380", "pdf": "https://arxiv.org/pdf/2504.03380", "abs": "https://arxiv.org/abs/2504.03380", "authors": ["Sanghwan Bae", "Jiwoo Hong", "Min Young Lee", "Hanbyul Kim", "JeongYeon Nam", "Donghyun Kwak"], "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03434", "pdf": "https://arxiv.org/pdf/2504.03434", "abs": "https://arxiv.org/abs/2504.03434", "authors": ["Batuhan Ozyurt", "Roya Arkhmammadova", "Deniz Yuret"], "title": "Locations of Characters in Narratives: Andersen and Persuasion Datasets", "categories": ["cs.CL", "I.2.7"], "comment": "14 pages, 3 figures, 10 tables", "summary": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03454", "pdf": "https://arxiv.org/pdf/2504.03454", "abs": "https://arxiv.org/abs/2504.03454", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03376", "pdf": "https://arxiv.org/pdf/2504.03376", "abs": "https://arxiv.org/abs/2504.03376", "authors": ["Edern Le Bot", "Rémi Giraud", "Boris Mansencal", "Thomas Tourdias", "Josè V. Manjon", "Pierrick Coupé"], "title": "FLAIRBrainSeg: Fine-grained brain segmentation using FLAIR MRI only", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "This paper introduces a novel method for brain segmentation using only FLAIR\nMRIs, specifically targeting cases where access to other imaging modalities is\nlimited. By leveraging existing automatic segmentation methods, we train a\nnetwork to approximate segmentations, typically obtained from T1-weighted MRIs.\nOur method, called FLAIRBrainSeg, produces segmentations of 132 structures and\nis robust to multiple sclerosis lesions. Experiments on both in-domain and\nout-of-domain datasets demonstrate that our method outperforms\nmodality-agnostic approaches based on image synthesis, the only currently\navailable alternative for performing brain parcellation using FLAIR MRI alone.\nThis technique holds promise for scenarios where T1-weighted MRIs are\nunavailable and offers a valuable alternative for clinicians and researchers in\nneed of reliable anatomical segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03541", "pdf": "https://arxiv.org/pdf/2504.03541", "abs": "https://arxiv.org/abs/2504.03541", "authors": ["Mayank Kothyari", "Sunita Sarawagi", "Soumen Chakrabarti", "Gaurav Arora", "Srujana Merugu"], "title": "Diverse In-Context Example Selection After Decomposing Programs and Aligned Utterances Improves Semantic Parsing", "categories": ["cs.CL"], "comment": "To appear at NAACL 2025 (Main)", "summary": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03546", "pdf": "https://arxiv.org/pdf/2504.03546", "abs": "https://arxiv.org/abs/2504.03546", "authors": ["Khai Le-Duc", "Tuyen Tran", "Bach Phan Tat", "Nguyen Kim Hai Bui", "Quan Dang", "Hung-Phong Tran", "Thanh-Thuy Nguyen", "Ly Nguyen", "Tuan-Minh Phan", "Thi Thu Phuong Tran", "Chris Ngo", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "title": "MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Preprint, 122 pages", "summary": "Multilingual speech translation (ST) in the medical domain enhances patient\ncare by enabling efficient communication across language barriers, alleviating\nspecialized workforce shortages, and facilitating improved diagnosis and\ntreatment, particularly during pandemics. In this work, we present the first\nsystematic study on medical ST, to our best knowledge, by releasing\nMultiMed-ST, a large-scale ST dataset for the medical domain, spanning all\ntranslation directions in five languages: Vietnamese, English, German, French,\nTraditional Chinese and Simplified Chinese, together with the models. With\n290,000 samples, our dataset is the largest medical machine translation (MT)\ndataset and the largest many-to-many multilingual ST among all domains.\nSecondly, we present the most extensive analysis study in ST research to date,\nincluding: empirical baselines, bilingual-multilingual comparative study,\nend-to-end vs. cascaded comparative study, task-specific vs. multi-task\nsequence-to-sequence (seq2seq) comparative study, code-switch analysis, and\nquantitative-qualitative error analysis. All code, data, and models are\navailable online: https://github.com/leduckhai/MultiMed-ST.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03471", "pdf": "https://arxiv.org/pdf/2504.03471", "abs": "https://arxiv.org/abs/2504.03471", "authors": ["Xi Wang", "Ziqi He", "Yang Zhou"], "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025. Appendix & Code:\n  https://github.com/Hytidel/UNetReweighting", "summary": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03595", "pdf": "https://arxiv.org/pdf/2504.03595", "abs": "https://arxiv.org/abs/2504.03595", "authors": ["Fabio Lilliu", "Amir Laadhar", "Christian Thomsen", "Diego Reforgiato Recupero", "Torben Bach Pedersen"], "title": "Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers", "categories": ["cs.CL"], "comment": "13 pages, 5 figures, 4 tables. Submitted to SmartGridComm 2025", "summary": "A key element to support the increased amounts of renewable energy in the\nenergy system is flexibility, i.e., the possibility of changing energy loads in\ntime and amount. Many flexibility models have been designed; however, exact\nmodels fail to scale for long time horizons or many devices. Because of this,\nthe FlexOffer (FOs) model has been designed, to provide device-independent\napproximations of flexibility with good accuracy, and much better scaling for\nlong time horizons and many devices. An important aspect of the real-life\nimplementation of energy flexibility is enabling flexible data exchange with\nmany types of smart energy appliances and market systems, e.g., in smart\nbuildings. For this, ontologies standardizing data formats are required.\nHowever, the current industry standard ontology for integrating smart devices\nfor energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited\nsupport for flexibility and thus cannot support important use cases. In this\npaper we propose an extension of SAREF4ENER that integrates full support for\nthe complete FlexOffer model, including advanced use cases, while maintaining\nbackward compatibility. This novel ontology module can accurately describe\nflexibility for advanced devices such as electric vehicles, batteries, and heat\npumps. It can also capture the inherent uncertainty associated with many\nflexible load types.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03474", "pdf": "https://arxiv.org/pdf/2504.03474", "abs": "https://arxiv.org/abs/2504.03474", "authors": ["Seyedeh Sahar Taheri Otaghsara", "Reza Rahmanzadeh"], "title": "Multi-encoder nnU-Net outperforms Transformer models with self-supervised pretraining", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the essential task of medical image segmentation, which\ninvolves the automatic identification and delineation of anatomical structures\nand pathological regions in medical images. Accurate segmentation is crucial in\nradiology, as it aids in the precise localization of abnormalities such as\ntumors, thereby enabling effective diagnosis, treatment planning, and\nmonitoring of disease progression. Specifically, the size, shape, and location\nof tumors can significantly influence clinical decision-making and therapeutic\nstrategies, making accurate segmentation a key component of radiological\nworkflows. However, challenges posed by variations in MRI modalities, image\nartifacts, and the scarcity of labeled data complicate the segmentation task\nand impact the performance of traditional models. To overcome these\nlimitations, we propose a novel self-supervised learning Multi-encoder nnU-Net\narchitecture designed to process multiple MRI modalities independently through\nseparate encoders. This approach allows the model to capture modality-specific\nfeatures before fusing them for the final segmentation, thus improving\naccuracy. Our Multi-encoder nnU-Net demonstrates exceptional performance,\nachieving a Dice Similarity Coefficient (DSC) of 93.72%, which surpasses that\nof other models such as vanilla nnU-Net, SegResNet, and Swin UNETR. By\nleveraging the unique information provided by each modality, the model enhances\nsegmentation tasks, particularly in scenarios with limited annotated data.\nEvaluations highlight the effectiveness of this architecture in improving tumor\nsegmentation outcomes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03476", "pdf": "https://arxiv.org/pdf/2504.03476", "abs": "https://arxiv.org/abs/2504.03476", "authors": ["Sheng Lian", "Dengfeng Pan", "Jianlong Cai", "Guang-Yong Chen", "Zhun Zhong", "Zhiming Luo", "Shen Zhao", "Shuo Li"], "title": "ATM-Net: Anatomy-Aware Text-Guided Multi-Modal Fusion for Fine-Grained Lumbar Spine Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate lumbar spine segmentation is crucial for diagnosing spinal\ndisorders. Existing methods typically use coarse-grained segmentation\nstrategies that lack the fine detail needed for precise diagnosis.\nAdditionally, their reliance on visual-only models hinders the capture of\nanatomical semantics, leading to misclassified categories and poor segmentation\ndetails. To address these limitations, we present ATM-Net, an innovative\nframework that employs an anatomy-aware, text-guided, multi-modal fusion\nmechanism for fine-grained segmentation of lumbar substructures, i.e.,\nvertebrae (VBs), intervertebral discs (IDs), and spinal canal (SC). ATM-Net\nadopts the Anatomy-aware Text Prompt Generator (ATPG) to adaptively convert\nimage annotations into anatomy-aware prompts in different views. These insights\nare further integrated with image features via the Holistic Anatomy-aware\nSemantic Fusion (HASF) module, building a comprehensive anatomical context. The\nChannel-wise Contrastive Anatomy-Aware Enhancement (CCAE) module further\nenhances class discrimination and refines segmentation through class-wise\nchannel-level multi-modal contrastive learning. Extensive experiments on the\nMRSpineSeg and SPIDER datasets demonstrate that ATM-Net significantly\noutperforms state-of-the-art methods, with consistent improvements regarding\nclass discrimination and segmentation details. For example, ATM-Net achieves\nDice of 79.39% and HD95 of 9.91 pixels on SPIDER, outperforming the competitive\nSpineParseNet by 8.31% and 4.14 pixels, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03601", "pdf": "https://arxiv.org/pdf/2504.03601", "abs": "https://arxiv.org/abs/2504.03601", "authors": ["Akshara Prabhakar", "Zuxin Liu", "Weiran Yao", "Jianguo Zhang", "Ming Zhu", "Shiyu Wang", "Zhiwei Liu", "Tulika Awalgaonkar", "Haolin Chen", "Thai Hoang", "Juan Carlos Niebles", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong"], "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages plus references and appendices", "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03490", "pdf": "https://arxiv.org/pdf/2504.03490", "abs": "https://arxiv.org/abs/2504.03490", "authors": ["Zihao He", "Shengchuan Zhang", "Runze Hu", "Yunhang Shen", "Yan Zhang"], "title": "BUFF: Bayesian Uncertainty Guided Diffusion Probabilistic Model for Single Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10; J.0"], "comment": "9 pages, 5 figures, AAAI 2025", "summary": "Super-resolution (SR) techniques are critical for enhancing image quality,\nparticularly in scenarios where high-resolution imagery is essential yet\nlimited by hardware constraints. Existing diffusion models for SR have relied\npredominantly on Gaussian models for noise generation, which often fall short\nwhen dealing with the complex and variable texture inherent in natural scenes.\nTo address these deficiencies, we introduce the Bayesian Uncertainty Guided\nDiffusion Probabilistic Model (BUFF). BUFF distinguishes itself by\nincorporating a Bayesian network to generate high-resolution uncertainty masks.\nThese masks guide the diffusion process, allowing for the adjustment of noise\nintensity in a manner that is both context-aware and adaptive. This novel\napproach not only enhances the fidelity of super-resolved images to their\noriginal high-resolution counterparts but also significantly mitigates\nartifacts and blurring in areas characterized by complex textures and fine\ndetails. The model demonstrates exceptional robustness against complex noise\npatterns and showcases superior adaptability in handling textures and edges\nwithin images. Empirical evidence, supported by visual results, illustrates the\nmodel's robustness, especially in challenging scenarios, and its effectiveness\nin addressing common SR issues such as blurring. Experimental evaluations\nconducted on the DIV2K dataset reveal that BUFF achieves a notable improvement,\nwith a +0.61 increase compared to baseline in SSIM on BSD100, surpassing\ntraditional diffusion approaches by an average additional +0.20dB PSNR gain.\nThese findings underscore the potential of Bayesian methods in enhancing\ndiffusion processes for SR, paving the way for future advancements in the\nfield.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03510", "pdf": "https://arxiv.org/pdf/2504.03510", "abs": "https://arxiv.org/abs/2504.03510", "authors": ["Tan Shu", "Li Shen"], "title": "FADConv: A Frequency-Aware Dynamic Convolution for Farmland Non-agriculturalization Identification and Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Cropland non-agriculturalization refers to the conversion of arable land into\nnon-agricultural uses such as forests, residential areas, and construction\nsites. This phenomenon not only directly leads to the loss of cropland\nresources but also poses systemic threats to food security and agricultural\nsustainability. Accurate identification of cropland and non-cropland areas is\ncrucial for detecting and addressing this issue. Traditional CNNs employ static\nconvolution layers, while dynamic convolution studies demonstrate that\nadaptively weighting multiple convolutional kernels through attention\nmechanisms can enhance accuracy. However, existing dynamic convolution methods\nrelying on Global Average Pooling (GAP) for attention weight allocation suffer\nfrom information loss, limiting segmentation precision. This paper proposes\nFrequency-Aware Dynamic Convolution (FADConv) and a Frequency Attention (FAT)\nmodule to address these limitations. Building upon the foundational structure\nof dynamic convolution, we designed FADConv by integrating 2D Discrete Cosine\nTransform (2D DCT) to capture frequency domain features and fuse them. FAT\nmodule generates high-quality attention weights that replace the traditional\nGAP method,making the combination between dynamic convolution kernels more\nreasonable.Experiments on the GID and Hi-CNA datasets demonstrate that FADConv\nsignificantly improves segmentation accuracy with minimal computational\noverhead. For instance, ResNet18 with FADConv achieves 1.9% and 2.7% increases\nin F1-score and IoU for cropland segmentation on GID, with only 58.87M\nadditional MAdds. Compared to other dynamic convolution approaches, FADConv\nexhibits superior performance in cropland segmentation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03624", "pdf": "https://arxiv.org/pdf/2504.03624", "abs": "https://arxiv.org/abs/2504.03624", "authors": ["NVIDIA", ":", "Aaron Blakeman", "Aarti Basant", "Abhinav Khattar", "Adithya Renduchintala", "Akhiad Bercovich", "Aleksander Ficek", "Alexis Bjorlin", "Ali Taghibakhshi", "Amala Sanjay Deshmukh", "Ameya Sunil Mahabaleshwarkar", "Andrew Tao", "Anna Shors", "Ashwath Aithal", "Ashwin Poojary", "Ayush Dattagupta", "Balaram Buddharaju", "Bobby Chen", "Boris Ginsburg", "Boxin Wang", "Brandon Norick", "Brian Butterfield", "Bryan Catanzaro", "Carlo del Mundo", "Chengyu Dong", "Christine Harvey", "Christopher Parisien", "Dan Su", "Daniel Korzekwa", "Danny Yin", "Daria Gitman", "David Mosallanezhad", "Deepak Narayanan", "Denys Fridman", "Dima Rekesh", "Ding Ma", "Dmytro Pykhtar", "Dong Ahn", "Duncan Riach", "Dusan Stosic", "Eileen Long", "Elad Segal", "Ellie Evans", "Eric Chung", "Erick Galinkin", "Evelina Bakhturina", "Ewa Dobrowolska", "Fei Jia", "Fuxiao Liu", "Gargi Prasad", "Gerald Shen", "Guilin Liu", "Guo Chen", "Haifeng Qian", "Helen Ngo", "Hongbin Liu", "Hui Li", "Igor Gitman", "Ilia Karmanov", "Ivan Moshkov", "Izik Golan", "Jan Kautz", "Jane Polak Scowcroft", "Jared Casper", "Jarno Seppanen", "Jason Lu", "Jason Sewall", "Jiaqi Zeng", "Jiaxuan You", "Jimmy Zhang", "Jing Zhang", "Jining Huang", "Jinze Xue", "Jocelyn Huang", "Joey Conway", "John Kamalu", "Jon Barker", "Jonathan Cohen", "Joseph Jennings", "Jupinder Parmar", "Karan Sapra", "Kari Briski", "Kateryna Chumachenko", "Katherine Luna", "Keshav Santhanam", "Kezhi Kong", "Kirthi Sivamani", "Krzysztof Pawelec", "Kumar Anik", "Kunlun Li", "Lawrence McAfee", "Leon Derczynski", "Lindsey Pavao", "Luis Vega", "Lukas Voegtle", "Maciej Bala", "Maer Rodrigues de Melo", "Makesh Narsimhan Sreedhar", "Marcin Chochowski", "Markus Kliegl", "Marta Stepniewska-Dziubinska", "Matthieu Le", "Matvei Novikov", "Mehrzad Samadi", "Michael Andersch", "Michael Evans", "Miguel Martinez", "Mike Chrzanowski", "Mike Ranzinger", "Mikolaj Blaz", "Misha Smelyanskiy", "Mohamed Fawzy", "Mohammad Shoeybi", "Mostofa Patwary", "Nayeon Lee", "Nima Tajbakhsh", "Ning Xu", "Oleg Rybakov", "Oleksii Kuchaiev", "Olivier Delalleau", "Osvald Nitski", "Parth Chadha", "Pasha Shamis", "Paulius Micikevicius", "Pavlo Molchanov", "Peter Dykas", "Philipp Fischer", "Pierre-Yves Aquilanti", "Piotr Bialecki", "Prasoon Varshney", "Pritam Gundecha", "Przemek Tredak", "Rabeeh Karimi", "Rahul Kandu", "Ran El-Yaniv", "Raviraj Joshi", "Roger Waleffe", "Ruoxi Zhang", "Sabrina Kavanaugh", "Sahil Jain", "Samuel Kriman", "Sangkug Lym", "Sanjeev Satheesh", "Saurav Muralidharan", "Sean Narenthiran", "Selvaraj Anandaraj", "Seonmyeong Bak", "Sergey Kashirsky", "Seungju Han", "Shantanu Acharya", "Shaona Ghosh", "Sharath Turuvekere Sreenivas", "Sharon Clay", "Shelby Thomas", "Shrimai Prabhumoye", "Shubham Pachori", "Shubham Toshniwal", "Shyamala Prayaga", "Siddhartha Jain", "Sirshak Das", "Slawek Kierat", "Somshubra Majumdar", "Song Han", "Soumye Singhal", "Sriharsha Niverty", "Stefania Alborghetti", "Suseella Panguluri", "Swetha Bhendigeri", "Syeda Nahida Akter", "Szymon Migacz", "Tal Shiri", "Terry Kong", "Timo Roman", "Tomer Ronen", "Trisha Saar", "Tugrul Konuk", "Tuomas Rintamaki", "Tyler Poon", "Ushnish De", "Vahid Noroozi", "Varun Singh", "Vijay Korthikanti", "Vitaly Kurin", "Wasi Uddin Ahmad", "Wei Du", "Wei Ping", "Wenliang Dai", "Wonmin Byeon", "Xiaowei Ren", "Yao Xu", "Yejin Choi", "Yian Zhang", "Ying Lin", "Yoshi Suhara", "Zhiding Yu", "Zhiqi Li", "Zhiyu Li", "Zhongbo Zhu", "Zhuolin Yang", "Zijia Chen"], "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time", "scaling"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03536", "pdf": "https://arxiv.org/pdf/2504.03536", "abs": "https://arxiv.org/abs/2504.03536", "authors": ["Boyuan Wang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Chaojun Ni", "Guan Huang", "Lihong Liu", "Xingang Wang"], "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration", "categories": ["cs.CV"], "comment": "Project Page: https://humandreamer-x.github.io/", "summary": "Single-image human reconstruction is vital for digital human modeling\napplications but remains an extremely challenging task. Current approaches rely\non generative models to synthesize multi-view images for subsequent 3D\nreconstruction and animation. However, directly generating multiple views from\na single human image suffers from geometric inconsistencies, resulting in\nissues like fragmented or blurred limbs in the reconstructed models. To tackle\nthese limitations, we introduce \\textbf{HumanDreamer-X}, a novel framework that\nintegrates multi-view human generation and reconstruction into a unified\npipeline, which significantly enhances the geometric consistency and visual\nfidelity of the reconstructed 3D models. In this framework, 3D Gaussian\nSplatting serves as an explicit 3D representation to provide initial geometry\nand appearance priority. Building upon this foundation, \\textbf{HumanFixer} is\ntrained to restore 3DGS renderings, which guarantee photorealistic results.\nFurthermore, we delve into the inherent challenges associated with attention\nmechanisms in multi-view human generation, and propose an attention modulation\nstrategy that effectively enhances geometric details identity consistency\nacross multi-view. Experimental results demonstrate that our approach markedly\nimproves generation and reconstruction PSNR quality metrics by 16.45% and\n12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing\ngeneralization capabilities on in-the-wild data and applicability to various\nhuman reconstruction backbone models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03563", "pdf": "https://arxiv.org/pdf/2504.03563", "abs": "https://arxiv.org/abs/2504.03563", "authors": ["Kaidong Li", "Tianxiao Zhang", "Kuan-Chuan Peng", "Guanghui Wang"], "title": "PF3Det: A Prompted Foundation Feature Assisted Visual LiDAR 3D Detector", "categories": ["cs.CV"], "comment": "This paper is accepted to the CVPR 2025 Workshop on Distillation of\n  Foundation Models for Autonomous Driving (WDFM-AD)", "summary": "3D object detection is crucial for autonomous driving, leveraging both LiDAR\npoint clouds for precise depth information and camera images for rich semantic\ninformation. Therefore, the multi-modal methods that combine both modalities\noffer more robust detection results. However, efficiently fusing LiDAR points\nand images remains challenging due to the domain gaps. In addition, the\nperformance of many models is limited by the amount of high quality labeled\ndata, which is expensive to create. The recent advances in foundation models,\nwhich use large-scale pre-training on different modalities, enable better\nmulti-modal fusion. Combining the prompt engineering techniques for efficient\ntraining, we propose the Prompted Foundational 3D Detector (PF3Det), which\nintegrates foundation model encoders and soft prompts to enhance LiDAR-camera\nfeature fusion. PF3Det achieves the state-of-the-art results under limited\ntraining data, improving NDS by 1.19% and mAP by 2.42% on the nuScenes dataset,\ndemonstrating its efficiency in 3D detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03602", "pdf": "https://arxiv.org/pdf/2504.03602", "abs": "https://arxiv.org/abs/2504.03602", "authors": ["Kai Lascheit", "Daniel Barath", "Marc Pollefeys", "Leonidas Guibas", "Francis Engelmann"], "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Registering human meshes to 3D point clouds is essential for applications\nsuch as augmented reality and human-robot interaction but often yields\nimprecise results due to noise and background clutter in real-world data. We\nintroduce a hybrid approach that incorporates body-part segmentation into the\nmesh fitting process, enhancing both human pose estimation and segmentation\naccuracy. Our method first assigns body part labels to individual points, which\nthen guide a two-step SMPL-X fitting: initial pose and orientation estimation\nusing body part centroids, followed by global refinement of the point cloud\nalignment. Additionally, we demonstrate that the fitted human mesh can refine\nbody part labels, leading to improved segmentation. Evaluations on the\ncluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that\nour approach significantly outperforms prior methods in both pose estimation\nand segmentation accuracy. Code and results are available on our project\nwebsite: https://segfit.github.io", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03607", "pdf": "https://arxiv.org/pdf/2504.03607", "abs": "https://arxiv.org/abs/2504.03607", "authors": ["Yuyang Hu", "Suhas Lohit", "Ulugbek S. Kamilov", "Tim K. Marks"], "title": "Multimodal Diffusion Bridge with Attention-Based SAR Fusion for Satellite Image Cloud Removal", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has achieved some success in addressing the challenge of cloud\nremoval in optical satellite images, by fusing with synthetic aperture radar\n(SAR) images. Recently, diffusion models have emerged as powerful tools for\ncloud removal, delivering higher-quality estimation by sampling from cloud-free\ndistributions, compared to earlier methods. However, diffusion models initiate\nsampling from pure Gaussian noise, which complicates the sampling trajectory\nand results in suboptimal performance. Also, current methods fall short in\neffectively fusing SAR and optical data. To address these limitations, we\npropose Diffusion Bridges for Cloud Removal, DB-CR, which directly bridges\nbetween the cloudy and cloud-free image distributions. In addition, we propose\na novel multimodal diffusion bridge architecture with a two-branch backbone for\nmultimodal image restoration, incorporating an efficient backbone and dedicated\ncross-modality fusion blocks to effectively extract and fuse features from\nsynthetic aperture radar (SAR) and optical images. By formulating cloud removal\nas a diffusion-bridge problem and leveraging this tailored architecture, DB-CR\nachieves high-fidelity results while being computationally efficient. We\nevaluated DB-CR on the SEN12MS-CR cloud-removal dataset, demonstrating that it\nachieves state-of-the-art results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03029", "pdf": "https://arxiv.org/pdf/2504.03029", "abs": "https://arxiv.org/abs/2504.03029", "authors": ["Nava Haghighi", "Sunny Yu", "James Landay", "Daniela Rosner"], "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and Assumptions in Large Language Models", "categories": ["cs.HC", "cs.CL"], "comment": "20 pages, 1 figure, 2 tables, CHI '25", "summary": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03621", "pdf": "https://arxiv.org/pdf/2504.03621", "abs": "https://arxiv.org/abs/2504.03621", "authors": ["Laziz Hamdi", "Amine Tamasna", "Pascal Boisson", "Thierry Paquet"], "title": "VISTA-OCR: Towards generative and interactive end to end OCR models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03160", "pdf": "https://arxiv.org/pdf/2504.03160", "abs": "https://arxiv.org/abs/2504.03160", "authors": ["Yuxiang Zheng", "Dayuan Fu", "Xiangkun Hu", "Xiaojie Cai", "Lyumanshan Ye", "Pengrui Lu", "Pengfei Liu"], "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["honesty"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02868", "pdf": "https://arxiv.org/pdf/2504.02868", "abs": "https://arxiv.org/abs/2504.02868", "authors": ["Ariadna Tohà-Dalmau", "Josep Rosinés-Fonoll", "Enrique Romero", "Ferran Mazzanti", "Ruben Martin-Pinardel", "Sonia Marias-Perez", "Carolina Bernal-Morales", "Rafael Castro-Dominguez", "Andrea Mendez", "Emilio Ortega", "Irene Vinagre", "Marga Gimenez", "Alfredo Vellido", "Javier Zarranz-Ventura"], "title": "Machine Learning Prediction of Cardiovascular Risk in Type 1 Diabetes Mellitus Using Radiomics Features from Multimodal Retinal Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "19 pages, 7 figures. Submitted to Ophthalmology Science, under second\n  review", "summary": "This study aimed to develop a machine learning (ML) algorithm capable of\ndetermining cardiovascular risk in multimodal retinal images from patients with\ntype 1 diabetes mellitus, distinguishing between moderate, high, and very\nhigh-risk levels. Radiomic features were extracted from fundus retinography,\noptical coherence tomography (OCT), and OCT angiography (OCTA) images. ML\nmodels were trained using these features either individually or combined with\nclinical data. A dataset of 597 eyes (359 individuals) was analyzed, and models\ntrained only with radiomic features achieved AUC values of (0.79 $\\pm$ 0.03)\nfor identifying moderate risk cases from high and very high-risk cases, and\n(0.73 $\\pm$ 0.07) for distinguishing between high and very high-risk cases. The\naddition of clinical variables improved all AUC values, reaching (0.99 $\\pm$\n0.01) for identifying moderate risk cases and (0.95 $\\pm$ 0.02) for\ndifferentiating between high and very high-risk cases. For very high CV risk,\nradiomics combined with OCT+OCTA metrics and ocular data achieved an AUC of\n(0.89 $\\pm$ 0.02) without systemic data input. These results demonstrate that\nradiomic features obtained from multimodal retinal images are useful for\ndiscriminating and classifying CV risk labels, highlighting the potential of\nthis oculomics approach for CV risk assessment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03360", "pdf": "https://arxiv.org/pdf/2504.03360", "abs": "https://arxiv.org/abs/2504.03360", "authors": ["Erik Johannes Husom", "Arda Goknil", "Merve Astekin", "Lwin Khin Shar", "Andre Kåsen", "Sagar Sen", "Benedikt Andreas Mithassel", "Ahmet Soylu"], "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "30 pages, 14 figures", "summary": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03129", "pdf": "https://arxiv.org/pdf/2504.03129", "abs": "https://arxiv.org/abs/2504.03129", "authors": ["Haozhan Tang", "Tianyi Zhang", "Oliver Kroemer", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Robots operating in unstructured environments often require accurate and\nconsistent object-level representations. This typically requires segmenting\nindividual objects from the robot's surroundings. While recent large models\nsuch as Segment Anything (SAM) offer strong performance in 2D image\nsegmentation. These advances do not translate directly to performance in the\nphysical 3D world, where they often over-segment objects and fail to produce\nconsistent mask correspondences across views. In this paper, we present\nGraphSeg, a framework for generating consistent 3D object segmentations from a\nsparse set of 2D images of the environment without any depth information.\nGraphSeg adds edges to graphs and constructs dual correspondence graphs: one\nfrom 2D pixel-level similarities and one from inferred 3D structure. We\nformulate segmentation as a problem of edge addition, then subsequent graph\ncontraction, which merges multiple 2D masks into unified object-level\nsegmentations. We can then leverage \\emph{3D foundation models} to produce\nsegmented 3D representations. GraphSeg achieves robust segmentation with\nsignificantly fewer images and greater accuracy than prior methods. We\ndemonstrate state-of-the-art performance on tabletop scenes and show that\nGraphSeg enables improved performance on downstream robotic manipulation tasks.\nCode available at https://github.com/tomtang502/graphseg.git.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03238", "pdf": "https://arxiv.org/pdf/2504.03238", "abs": "https://arxiv.org/abs/2504.03238", "authors": ["Akis Nousias", "Efklidis Katsaros", "Evangelos Syrmos", "Panagiotis Radoglou-Grammatikis", "Thomas Lagkas", "Vasileios Argyriou", "Ioannis Moscholios", "Evangelos Markakis", "Sotirios Goudos", "Panagiotis Sarigiannidis"], "title": "Malware Detection in Docker Containers: An Image is Worth a Thousand Logs", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Accepted at ICC-W", "summary": "Malware detection is increasingly challenged by evolving techniques like\nobfuscation and polymorphism, limiting the effectiveness of traditional\nmethods. Meanwhile, the widespread adoption of software containers has\nintroduced new security challenges, including the growing threat of malicious\nsoftware injection, where a container, once compromised, can serve as entry\npoint for further cyberattacks. In this work, we address these security issues\nby introducing a method to identify compromised containers through machine\nlearning analysis of their file systems. We cast the entire software containers\ninto large RGB images via their tarball representations, and propose to use\nestablished Convolutional Neural Network architectures on a streaming,\npatch-based manner. To support our experiments, we release the COSOCO\ndataset--the first of its kind--containing 3364 large-scale RGB images of\nbenign and compromised software containers at\nhttps://huggingface.co/datasets/k3ylabs/cosoco-image-dataset. Our method\ndetects more malware and achieves higher F1 and Recall scores than all\nindividual and ensembles of VirusTotal engines, demonstrating its effectiveness\nand setting a new standard for identifying malware-compromised software\ncontainers.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03589", "pdf": "https://arxiv.org/pdf/2504.03589", "abs": "https://arxiv.org/abs/2504.03589", "authors": ["Badhan Kumar Das", "Gengyan Zhao", "Han Liu", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "AdaViT: Adaptive Vision Transformer for Flexible Pretrain and Finetune with Variable 3D Medical Image Modalities", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Pretrain techniques, whether supervised or self-supervised, are widely used\nin deep learning to enhance model performance. In real-world clinical\nscenarios, different sets of magnetic resonance (MR) contrasts are often\nacquired for different subjects/cases, creating challenges for deep learning\nmodels assuming consistent input modalities among all the cases and between\npretrain and finetune. Existing methods struggle to maintain performance when\nthere is an input modality/contrast set mismatch with the pretrained model,\noften resulting in degraded accuracy. We propose an adaptive Vision Transformer\n(AdaViT) framework capable of handling variable set of input modalities for\neach case. We utilize a dynamic tokenizer to encode different input image\nmodalities to tokens and take advantage of the characteristics of the\ntransformer to build attention mechanism across variable length of tokens.\nThrough extensive experiments, we demonstrate that this architecture\neffectively transfers supervised pretrained models to new datasets with\ndifferent input modality/contrast sets, resulting in superior performance on\nzero-shot testing, few-shot finetuning, and backward transferring in brain\ninfarct and brain tumor segmentation tasks. Additionally, for self-supervised\npretrain, the proposed method is able to maximize the pretrain data and\nfacilitate transferring to diverse downstream tasks with variable sets of input\nmodalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
