{"id": "2504.04534", "pdf": "https://arxiv.org/pdf/2504.04534", "abs": "https://arxiv.org/abs/2504.04534", "authors": ["Anantharaman Janakiraman", "Behnaz Ghoraani"], "title": "An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text summarization is crucial for mitigating information overload across\ndomains like journalism, medicine, and business. This research evaluates\nsummarization performance across 17 large language models (OpenAI, Google,\nAnthropic, open-source) using a novel multi-dimensional framework. We assessed\nmodels on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed,\nSAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using\nmetrics for factual consistency, semantic similarity, lexical overlap, and\nhuman-like quality, while also considering efficiency factors. Our findings\nreveal significant performance differences, with specific models excelling in\nfactual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and\nprocessing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash).\nPerformance varies dramatically by dataset, with models struggling on technical\ndomains but performing well on conversational content. We identified a critical\ntension between factual consistency (best at 50 tokens) and perceived quality\n(best at 150 tokens). Our analysis provides evidence-based recommendations for\ndifferent use cases, from high-stakes applications requiring factual accuracy\nto resource-constrained environments needing efficient processing. This\ncomprehensive approach enhances evaluation methodology by integrating quality\nmetrics with operational considerations, incorporating trade-offs between\naccuracy, efficiency, and cost-effectiveness to guide model selection for\nspecific applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "summarization", "multi-dimensional"], "score": 6}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04275", "pdf": "https://arxiv.org/pdf/2504.04275", "abs": "https://arxiv.org/abs/2504.04275", "authors": ["TÃºlio Sousa de Gois", "Paloma Batista Cardoso"], "title": "negativas: a prototype for searching and classifying sentential negation in speech data", "categories": ["cs.CL"], "comment": null, "summary": "Negation is a universal feature of natural languages. In Brazilian\nPortuguese, the most commonly used negation particle is n\\~ao, which can scope\nover nouns or verbs. When it scopes over a verb, n\\~ao can occur in three\npositions: pre-verbal (NEG1), double negation (NEG2), or post-verbal (NEG3),\ne.g., n\\~ao gosto, n\\~ao gosto n\\~ao, gosto n\\~ao (\"I do not like it\"). From a\nvariationist perspective, these structures are different forms of expressing\nnegation. Pragmatically, they serve distinct communicative functions, such as\npoliteness and modal evaluation. Despite their grammatical acceptability, these\nforms differ in frequency. NEG1 dominates across Brazilian regions, while NEG2\nand NEG3 appear more rarely, suggesting its use is contextually restricted.\nThis low-frequency challenges research, often resulting in subjective,\nnon-generalizable interpretations of verbal negation with n\\~ao. To address\nthis, we developed negativas, a tool for automatically identifying NEG1, NEG2,\nand NEG3 in transcribed data. The tool's development involved four stages: i)\nanalyzing a dataset of 22 interviews from the Falares Sergipanos database,\nannotated by three linguists, ii) creating a code using natural language\nprocessing (NLP) techniques, iii) running the tool, iv) evaluating accuracy.\nInter-annotator consistency, measured using Fleiss' Kappa, was moderate (0.57).\nThe tool identified 3,338 instances of n\\~ao, classifying 2,085 as NEG1, NEG2,\nor NEG3, achieving a 93% success rate. However, negativas has limitations. NEG1\naccounted for 91.5% of identified structures, while NEG2 and NEG3 represented\n7.2% and 1.2%, respectively. The tool struggled with NEG2, sometimes\nmisclassifying instances as overlapping structures (NEG1/NEG2/NEG3).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "kappa", "accuracy"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04569", "pdf": "https://arxiv.org/pdf/2504.04569", "abs": "https://arxiv.org/abs/2504.04569", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations", "categories": ["cs.CL"], "comment": null, "summary": "In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "dialogue"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04713", "pdf": "https://arxiv.org/pdf/2504.04713", "abs": "https://arxiv.org/abs/2504.04713", "authors": ["Yifei Yu", "Qian-Wen Zhang", "Lingfeng Qiao", "Di Yin", "Fang Li", "Jie Wang", "Zengxi Chen", "Suncong Zheng", "Xiaolong Liang", "Xing Sun"], "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04550", "pdf": "https://arxiv.org/pdf/2504.04550", "abs": "https://arxiv.org/abs/2504.04550", "authors": ["Alkesh Patel", "Vibhav Chitalia", "Yinfei Yang"], "title": "Advancing Egocentric Video Question Answering with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages", "summary": "Egocentric Video Question Answering (QA) requires models to handle\nlong-horizon temporal reasoning, first-person perspectives, and specialized\nchallenges like frequent camera movement. This paper systematically evaluates\nboth proprietary and open-source Multimodal Large Language Models (MLLMs) on\nQaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four\npopular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct)\nare assessed using zero-shot and fine-tuned approaches for both OpenQA and\nCloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in\nQaEgo4D, enabling more reliable comparison. Our results show that fine-tuned\nVideo-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art\nperformance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for\nOpenQA) and +13% accuracy (for CloseQA). We also present a thorough error\nanalysis, indicating the model's difficulty in spatial reasoning and\nfine-grained object recognition - key areas for future improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03712", "pdf": "https://arxiv.org/pdf/2504.03712", "abs": "https://arxiv.org/abs/2504.03712", "authors": ["Jan Lewen", "Max Pargmann", "Jenia Jitsev", "Mehdi Cherti", "Robert Pitz-Paal", "Daniel Maldonado Quinto"], "title": "Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Concentrating Solar Power (CSP) plants are a key technology in the transition\ntoward sustainable energy. A critical factor for their safe and efficient\noperation is the distribution of concentrated solar flux on the receiver.\nHowever, flux distributions from individual heliostats are sensitive to surface\nimperfections. Measuring these surfaces across many heliostats remains\nimpractical in real-world deployments. As a result, control systems often\nassume idealized heliostat surfaces, leading to suboptimal performance and\npotential safety risks. To address this, inverse Deep Learning Raytracing\n(iDLR) has been introduced as a novel method for inferring heliostat surface\nprofiles from target images recorded during standard calibration procedures. In\nthis work, we present the first successful Sim-to-Real transfer of iDLR,\nenabling accurate surface predictions directly from real-world target images.\nWe evaluate our method on 63 heliostats under real operational conditions. iDLR\nsurface predictions achieve a median mean absolute error (MAE) of 0.17 mm and\nshow good agreement with deflectometry ground truth in 84% of cases. When used\nin raytracing simulations, it enables flux density predictions with a mean\naccuracy of 90% compared to deflectometry over our dataset, and outperforms the\ncommonly used ideal heliostat surface assumption by 26%. We tested this\napproach in a challenging double-extrapolation scenario-involving unseen sun\npositions and receiver projection-and found that iDLR maintains high predictive\naccuracy, highlighting its generalization capabilities. Our results demonstrate\nthat iDLR is a scalable, automated, and cost-effective solution for integrating\nrealistic heliostat surface models into digital twins. This opens the door to\nimproved flux control, more precise performance modeling, and ultimately,\nenhanced efficiency and safety in future CSP plants.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "agreement", "accuracy"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03846", "pdf": "https://arxiv.org/pdf/2504.03846", "abs": "https://arxiv.org/abs/2504.03846", "authors": ["Wei-Lin Chen", "Zhepei Wei", "Xinyu Zhu", "Shi Feng", "Yu Meng"], "title": "Do LLM Evaluators Prefer Themselves for a Reason?", "categories": ["cs.CL"], "comment": "Preprint. 31 pages", "summary": "Large language models (LLMs) are increasingly used as automatic evaluators in\napplications such as benchmarking, reward modeling, and self-refinement. Prior\nwork highlights a potential self-preference bias where LLMs favor their own\ngenerated responses, a tendency often intensifying with model size and\ncapability. This raises a critical question: Is self-preference detrimental, or\ndoes it simply reflect objectively superior outputs from more capable models?\nDisentangling these has been challenging due to the usage of subjective tasks\nin previous studies. To address this, we investigate self-preference using\nverifiable benchmarks (mathematical reasoning, factual knowledge, code\ngeneration) that allow objective ground-truth assessment. This enables us to\ndistinguish harmful self-preference (favoring objectively worse responses) from\nlegitimate self-preference (favoring genuinely superior ones). We conduct\nlarge-scale experiments under controlled evaluation conditions across diverse\nmodel families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our\nfindings reveal three key insights: (1) Better generators are better judges --\nLLM evaluators' accuracy strongly correlates with their task performance, and\nmuch of the self-preference in capable models is legitimate. (2) Harmful\nself-preference persists, particularly when evaluator models perform poorly as\ngenerators on specific task instances. Stronger models exhibit more pronounced\nharmful bias when they err, though such incorrect generations are less\nfrequent. (3) Inference-time scaling strategies, such as generating a long\nChain-of-Thought before evaluation, effectively reduce the harmful\nself-preference. These results provide a more nuanced understanding of\nLLM-based evaluation and practical insights for improving its reliability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04083", "pdf": "https://arxiv.org/pdf/2504.04083", "abs": "https://arxiv.org/abs/2504.04083", "authors": ["Aviv Brokman", "Xuguang Ai", "Yuhang Jiang", "Shashank Gupta", "Ramakanth Kavuluru"], "title": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Zero-shot methodology promises to cut down on costs of dataset\nannotation and domain expertise needed to make use of NLP. Generative large\nlanguage models trained to align with human goals have achieved high zero-shot\nperformance across a wide variety of tasks. As of yet, it is unclear how well\nthese models perform on biomedical relation extraction (RE). To address this\nknowledge gap, we explore patterns in the performance of OpenAI LLMs across a\ndiverse sampling of RE tasks.\n  Methods: We use OpenAI GPT-4-turbo and their reasoning model o1 to conduct\nend-to-end RE experiments on seven datasets. We use the JSON generation\ncapabilities of GPT models to generate structured output in two ways: (1) by\ndefining an explicit schema describing the structure of relations, and (2)\nusing a setting that infers the structure from the prompt language.\n  Results: Our work is the first to study and compare the performance of the\nGPT-4 and o1 for the end-to-end zero-shot biomedical RE task across a broad\narray of datasets. We found the zero-shot performances to be proximal to that\nof fine-tuned methods. The limitations of this approach are that it performs\npoorly on instances containing many relations and errs on the boundaries of\ntextual mentions.\n  Conclusion: Recent large language models exhibit promising zero-shot\ncapabilities in complex biomedical RE tasks, offering competitive performance\nwith reduced dataset curation and NLP modeling needs at the cost of increased\ncomputing, potentially increasing medical community accessibility. Addressing\nthe limitations we identify could further boost reliability. The code, data,\nand prompts for all our experiments are publicly available:\nhttps://github.com/bionlproc/ZeroShotRE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "reliability"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04377", "pdf": "https://arxiv.org/pdf/2504.04377", "abs": "https://arxiv.org/abs/2504.04377", "authors": ["Priyanshu Kumar", "Devansh Jain", "Akhila Yerukola", "Liwei Jiang", "Himanshu Beniwal", "Thomas Hartvigsen", "Maarten Sap"], "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages", "categories": ["cs.CL"], "comment": null, "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04891", "pdf": "https://arxiv.org/pdf/2504.04891", "abs": "https://arxiv.org/abs/2504.04891", "authors": ["Longdi Xian", "Jianzhang Ni", "Mingzhu Wang"], "title": "Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "agreement", "reliability"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04736", "pdf": "https://arxiv.org/pdf/2504.04736", "abs": "https://arxiv.org/abs/2504.04736", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Hao Zhou", "Irene Cai", "Christopher D. Manning"], "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "RLAIF"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05040", "pdf": "https://arxiv.org/pdf/2504.05040", "abs": "https://arxiv.org/abs/2504.05040", "authors": ["Haiwan Wei", "Yitian Yuan", "Xiaohan Lan", "Wei Ke", "Lin Ma"], "title": "InstructionBench: An Instructional Video Understanding Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Despite progress in video large language models (Video-LLMs), research on\ninstructional video understanding, crucial for enhancing access to\ninstructional content, remains insufficient. To address this, we introduce\nInstructionBench, an Instructional video understanding Benchmark, which\nchallenges models' advanced temporal reasoning within instructional videos\ncharacterized by their strict step-by-step flow. Employing GPT-4, we formulate\nQ\\&A pairs in open-ended and multiple-choice formats to assess both\nCoarse-Grained event-level and Fine-Grained object-level reasoning. Our\nfiltering strategies exclude questions answerable purely by common-sense\nknowledge, focusing on visual perception and analysis when evaluating Video-LLM\nmodels. The benchmark finally contains 5k questions across over 700 videos. We\nevaluate the latest Video-LLMs on our InstructionBench, finding that\nclosed-source models outperform open-source ones. However, even the best model,\nGPT-4o, achieves only 53.42\\% accuracy, indicating significant gaps in temporal\nreasoning. To advance the field, we also develop a comprehensive instructional\nvideo dataset with over 19k Q\\&A pairs from nearly 2.5k videos, using an\nautomated data generation framework, thereby enriching the community's research\nresources.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03739", "pdf": "https://arxiv.org/pdf/2504.03739", "abs": "https://arxiv.org/abs/2504.03739", "authors": ["Mingyan Liu"], "title": "A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models, such as GPT and BERT, have significantly improved\nperformance in tasks like text generation and summarization. However,\nhallucinations \"where models generate non-factual or misleading content\" are\nespecially problematic in smaller-scale architectures, limiting their\nreal-world applicability.In this paper, we propose a unified Virtual\nMixture-of-Experts (MoE) fusion strategy that enhances inference performance\nand mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing\nthe parameter count. Our method leverages multiple domain-specific expert\nprompts (with the number of experts being adjustable) to guide the model from\ndifferent perspectives. We apply a statistical outlier truncation strategy\nbased on the mean and standard deviation to filter out abnormally high\nprobability predictions, and we inject noise into the embedding space to\npromote output diversity. To clearly assess the contribution of each module, we\nadopt a fixed voting mechanism rather than a dynamic gating network, thereby\navoiding additional confounding factors. We provide detailed theoretical\nderivations from both statistical and ensemble learning perspectives to\ndemonstrate how our method reduces output variance and suppresses\nhallucinations. Extensive ablation experiments on dialogue generation tasks\nshow that our approach significantly improves inference accuracy and robustness\nin small models. Additionally, we discuss methods for evaluating the\northogonality of virtual experts and outline the potential for future work\ninvolving dynamic expert weight allocation using gating networks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization", "dialogue"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03790", "pdf": "https://arxiv.org/pdf/2504.03790", "abs": "https://arxiv.org/abs/2504.03790", "authors": ["GonÃ§alo Faria", "Noah A. Smith"], "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Increasing test-time computation has emerged as a promising direction for\nimproving language model performance, particularly in scenarios where model\nfinetuning is impractical or impossible due to computational constraints or\nprivate model weights. However, existing test-time search methods using a\nreward model (RM) often degrade in quality as compute scales, due to the\nover-optimization of what are inherently imperfect reward proxies. We introduce\nQAlign, a new test-time alignment approach. As we scale test-time compute,\nQAlign converges to sampling from the optimal aligned distribution for each\nindividual prompt. By adopting recent advances in Markov chain Monte Carlo for\ntext generation, our method enables better-aligned outputs without modifying\nthe underlying model or even requiring logit access. We demonstrate the\neffectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and\nGSM-Symbolic) using a task-specific RM, showing consistent improvements over\nexisting test-time compute methods like best-of-n and majority voting.\nFurthermore, when applied with more realistic RMs trained on the Tulu 3\npreference dataset, QAlign outperforms direct preference optimization (DPO),\nbest-of-n, majority voting, and weighted majority voting on a diverse range of\ndatasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical\nsolution to aligning language models at test time using additional computation\nwithout degradation, our approach expands the limits of the capability that can\nbe obtained from off-the-shelf language models without further training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scale", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03906", "pdf": "https://arxiv.org/pdf/2504.03906", "abs": "https://arxiv.org/abs/2504.03906", "authors": ["Abhilekh Borah", "Hasnat Md Abdullah", "Kangda Wei", "Ruihong Huang"], "title": "CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "The rise of Large Language Models (LLMs) has raised questions about their\nability to understand climate-related contexts. Though climate change dominates\nsocial media, analyzing its multimodal expressions is understudied, and current\ntools have failed to determine whether LLMs amplify credible solutions or\nspread unsubstantiated claims. To address this, we introduce CliME (Climate\nChange Multimodal Evaluation), a first-of-its-kind multimodal dataset,\ncomprising 2579 Twitter and Reddit posts. The benchmark features a diverse\ncollection of humorous memes and skeptical posts, capturing how these formats\ndistill complex issues into viral narratives that shape public opinion and\npolicy discussions. To systematically evaluate LLM performance, we present the\nClimate Alignment Quotient (CAQ), a novel metric comprising five distinct\ndimensions: Articulation, Evidence, Resonance, Transition, and Specificity.\nAdditionally, we propose three analytical lenses: Actionability, Criticality,\nand Justice, to guide the assessment of LLM-generated climate discourse using\nCAQ. Our findings, based on the CAQ metric, indicate that while most evaluated\nLLMs perform relatively well in Criticality and Justice, they consistently\nunderperform on the Actionability axis. Among the models evaluated, Claude 3.7\nSonnet achieves the highest overall performance. We publicly release our CliME\ndataset and code to foster further research in this domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04038", "pdf": "https://arxiv.org/pdf/2504.04038", "abs": "https://arxiv.org/abs/2504.04038", "authors": ["Kaung Lwin Thant", "Kwankamol Nongpong", "Ye Kyaw Thu", "Thura Aung", "Khaing Hsu Wai", "Thazin Myint Oo"], "title": "myNER: Contextualized Burmese Named Entity Recognition with Bidirectional LSTM and fastText Embeddings via Joint Training with POS Tagging", "categories": ["cs.CL", "I.2.7"], "comment": "7 pages, 2 figures, 5 tables, to be published in the proceedings of\n  IEEE ICCI-2025", "summary": "Named Entity Recognition (NER) involves identifying and categorizing named\nentities within textual data. Despite its significance, NER research has often\noverlooked low-resource languages like Myanmar (Burmese), primarily due to the\nlack of publicly available annotated datasets. To address this, we introduce\nmyNER, a novel word-level NER corpus featuring a 7-tag annotation scheme,\nenriched with Part-of-Speech (POS) tagging to provide additional syntactic\ninformation. Alongside the corpus, we conduct a comprehensive evaluation of NER\nmodels, including Conditional Random Fields (CRF), Bidirectional LSTM\n(BiLSTM)-CRF, and their combinations with fastText embeddings in different\nsettings. Our experiments reveal the effectiveness of contextualized word\nembeddings and the impact of joint training with POS tagging, demonstrating\nsignificant performance improvements across models. The traditional CRF\njoint-task model with fastText embeddings as a feature achieved the best\nresult, with a 0.9818 accuracy and 0.9811 weighted F1 score with 0.7429 macro\nF1 score. BiLSTM-CRF with fine-tuned fastText embeddings gets the best result\nof 0.9791 accuracy and 0.9776 weighted F1 score with 0.7395 macro F1 score.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04065", "pdf": "https://arxiv.org/pdf/2504.04065", "abs": "https://arxiv.org/abs/2504.04065", "authors": ["Jiaqi Deng", "Kaize Shi", "Zonghan Wu", "Huan Huo", "Dingxian Wang", "Guandong Xu"], "title": "UniRVQA: A Unified Framework for Retrieval-Augmented Vision Question Answering via Self-Reflective Joint Training", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions requiring external knowledge, such as web-sourced\nencyclopedia articles. Existing methods often use sequential and separate\nframeworks for the retriever and the generator with limited parametric\nknowledge sharing. However, since both retrieval and generation tasks require\naccurate understanding of contextual and external information, such separation\ncan potentially lead to suboptimal system performance. Another key challenge is\nthe integration of multimodal information. General-purpose multimodal\npre-trained models, while adept at multimodal representation learning, struggle\nwith fine-grained retrieval required for knowledge-intensive visual questions.\nRecent specialized pre-trained models mitigate the issue, but are\ncomputationally expensive. To bridge the gap, we propose a Unified\nRetrieval-Augmented VQA framework (UniRVQA). UniRVQA adapts general multimodal\npre-trained models for fine-grained knowledge-intensive tasks within a unified\nframework, enabling cross-task parametric knowledge sharing and the extension\nof existing multimodal representation learning capability. We further introduce\na reflective-answering mechanism that allows the model to explicitly evaluate\nand refine its knowledge boundary. Additionally, we integrate late interaction\ninto the retrieval-augmented generation joint training process to enhance\nfine-grained understanding of queries and documents. Our approach achieves\ncompetitive performance against state-of-the-art models, delivering a\nsignificant 4.7% improvement in answering accuracy, and brings an average 7.5%\nboost in base MLLMs' VQA performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04279", "pdf": "https://arxiv.org/pdf/2504.04279", "abs": "https://arxiv.org/abs/2504.04279", "authors": ["Hongchao Fang", "Can Qin", "Ran Xu", "Feng Liu", "Yixin Liu", "Lichao Sun", "Dongwon Lee", "Lifu Huang", "Wenpeng Yin"], "title": "Could AI Trace and Explain the Origins of AI-Generated Images and Text?", "categories": ["cs.CL"], "comment": null, "summary": "AI-generated content is becoming increasingly prevalent in the real world,\nleading to serious ethical and societal concerns. For instance, adversaries\nmight exploit large multimodal models (LMMs) to create images that violate\nethical or legal standards, while paper reviewers may misuse large language\nmodels (LLMs) to generate reviews without genuine intellectual effort. While\nprior work has explored detecting AI-generated images and texts, and\noccasionally tracing their source models, there is a lack of a systematic and\nfine-grained comparative study. Important dimensions--such as AI-generated\nimages vs. text, fully vs. partially AI-generated images, and general vs.\nmalicious use cases--remain underexplored. Furthermore, whether AI systems like\nGPT-4o can explain why certain forged content is attributed to specific\ngenerative models is still an open question, with no existing benchmark\naddressing this. To fill this gap, we introduce AI-FAKER, a comprehensive\nmultimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs,\ncovering both general and malicious use cases for AI-generated images and\ntexts. Our experiments reveal two key findings: (i) AI authorship detection\ndepends not only on the generated output but also on the model's original\ntraining intent; and (ii) GPT-4o provides highly consistent but less specific\nexplanations when analyzing content produced by OpenAI's own models, such as\nDALL-E and GPT-4o itself.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04373", "pdf": "https://arxiv.org/pdf/2504.04373", "abs": "https://arxiv.org/abs/2504.04373", "authors": ["Shenyang Liu", "Yang Gao", "Shaoyan Zhai", "Liqiang Wang"], "title": "StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Big Data (BigData)", "summary": "Prompt Recovery, reconstructing prompts from the outputs of large language\nmodels (LLMs), has grown in importance as LLMs become ubiquitous. Most users\naccess LLMs through APIs without internal model weights, relying only on\noutputs and logits, which complicates recovery. This paper explores a unique\nprompt recovery task focused on reconstructing prompts for style transfer and\nrephrasing, rather than typical question-answering. We introduce a dataset\ncreated with LLM assistance, ensuring quality through multiple techniques, and\ntest methods like zero-shot, few-shot, jailbreak, chain-of-thought,\nfine-tuning, and a novel canonical-prompt fallback for poor-performing cases.\nOur results show that one-shot and fine-tuning yield the best outcomes but\nhighlight flaws in traditional sentence similarity metrics for evaluating\nprompt recovery. Contributions include (1) a benchmark dataset, (2)\ncomprehensive experiments on prompt recovery strategies, and (3) identification\nof limitations in current evaluation metrics, all of which advance general\nprompt recovery research, where the structure of the input prompt is\nunrestricted.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04339", "pdf": "https://arxiv.org/pdf/2504.04339", "abs": "https://arxiv.org/abs/2504.04339", "authors": ["Peng Gao", "Yujian Lee", "Zailong Chen", "Hui zhang", "Xubo Liu", "Yiyang Hu", "Guquang Jing"], "title": "NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval", "categories": ["cs.CV"], "comment": "Has been accepted by ICASSP2025", "summary": "Composed Image Retrieval (CIR) seeks to find a target image using a\nmulti-modal query, which combines an image with modification text to pinpoint\nthe target. While recent CIR methods have shown promise, they mainly focus on\nexploring relationships between the query pairs (image and text) through data\naugmentation or model design. These methods often assume perfect alignment\nbetween queries and target images, an idealized scenario rarely encountered in\npractice. In reality, pairs are often partially or completely mismatched due to\nissues like inaccurate modification texts, low-quality target images, and\nannotation errors. Ignoring these mismatches leads to numerous False Positive\nPair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit\nand ultimately reducing its performance. To address this problem, we propose\nthe Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key\ncomponents: the Weight Compensation Block (WCB) and the Noise-pair Filter Block\n(NFB). The WCB coupled with diverse weight maps can ensure more stable token\nrepresentations of multi-modal queries and target images. Meanwhile, the NFB,\nin conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by\nevaluating loss distributions, and generates soft labels correspondingly,\nallowing for the design of the soft-label based Noise Contrastive Estimation\n(NCE) loss function. Consequently, the overall architecture helps to mitigate\nthe influence of mismatched and partially matched samples, with experimental\nresults demonstrating that NCL-CIR achieves exceptional performance on the\nbenchmark datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04348", "pdf": "https://arxiv.org/pdf/2504.04348", "abs": "https://arxiv.org/abs/2504.04348", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04471", "pdf": "https://arxiv.org/pdf/2504.04471", "abs": "https://arxiv.org/abs/2504.04471", "authors": ["Zhuo Zhi", "Qiangqiang Wu", "Minghe shen", "Wenbo Li", "Yinchuan Li", "Kun Shao", "Kaiwen Zhou"], "title": "VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT", "categories": ["cs.CV"], "comment": null, "summary": "Long video understanding has emerged as an increasingly important yet\nchallenging task in computer vision. Agent-based approaches are gaining\npopularity for processing long videos, as they can handle extended sequences\nand integrate various tools to capture fine-grained information. However,\nexisting methods still face several challenges: (1) they often rely solely on\nthe reasoning ability of large language models (LLMs) without dedicated\nmechanisms to enhance reasoning in long video scenarios; and (2) they remain\nvulnerable to errors or noise from external tools. To address these issues, we\npropose a specialized chain-of-thought (CoT) process tailored for long video\nanalysis. Our proposed CoT with plan-adjust mode enables the LLM to\nincrementally plan and adapt its information-gathering strategy. We further\nincorporate heuristic uncertainty estimation of both the LLM and external tools\nto guide the CoT process. This allows the LLM to assess the reliability of\nnewly collected information, refine its collection strategy, and make more\nrobust decisions when synthesizing final answers. Empirical experiments show\nthat our uncertainty-aware CoT effectively mitigates noise from external tools,\nleading to more reliable outputs. We implement our approach in a system called\nVideoAgent2, which also includes additional modules such as general context\nacquisition and specialized tool design. Evaluation on three dedicated long\nvideo benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms\nthe previous state-of-the-art agent-based method, VideoAgent, by an average of\n13.1% and achieves leading performance among all zero-shot approaches", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04737", "pdf": "https://arxiv.org/pdf/2504.04737", "abs": "https://arxiv.org/abs/2504.04737", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745", "abs": "https://arxiv.org/abs/2504.04745", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Submitted to XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\nmore evident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\nthe best-case scenario.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization", "dialogue"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04572", "pdf": "https://arxiv.org/pdf/2504.04572", "abs": "https://arxiv.org/abs/2504.04572", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Mohammed Bremoo", "Mohammed Khurd", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammad Almatrafi", "Tanveer Hussain"], "title": "Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric", "categories": ["cs.CV"], "comment": null, "summary": "Precise video retrieval requires multi-modal correlations to handle unseen\nvocabulary and scenes, becoming more complex for lengthy videos where models\nmust perform effectively without prior training on a specific dataset. We\nintroduce a unified framework that combines a visual matching stream and an\naural matching stream with a unique subtitles-based video segmentation\napproach. Additionally, the aural stream includes a complementary audio-based\ntwo-stage retrieval mechanism that enhances performance on long-duration\nvideos. Considering the complex nature of retrieval from lengthy videos and its\ncorresponding evaluation, we introduce a new retrieval evaluation method\nspecifically designed for long-video retrieval to support further research. We\nconducted experiments on the YouCook2 benchmark, showing promising retrieval\nperformance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04633", "pdf": "https://arxiv.org/pdf/2504.04633", "abs": "https://arxiv.org/abs/2504.04633", "authors": ["Yanshu Li", "Hongyang He", "Yi Cao", "Qisen Cheng", "Xiang Fu", "Ruixiang Tang"], "title": "M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint, 28 pages, 10 figures, 15 tables", "summary": "Multimodal in-context learning (ICL) is a vital capability for Large\nVision-Language Models (LVLMs), allowing task adaptation via contextual prompts\nwithout parameter retraining. However, its application is hindered by the\ntoken-intensive nature of inputs and the high complexity of cross-modal\nfew-shot learning, which limits the expressive power of representation methods.\nTo tackle these challenges, we propose \\textbf{M2IV}, a method that substitutes\nexplicit demonstrations with learnable \\textbf{I}n-context \\textbf{V}ectors\ndirectly integrated into LVLMs. By exploiting the complementary strengths of\nmulti-head attention (\\textbf{M}HA) and multi-layer perceptrons (\\textbf{M}LP),\nM2IV achieves robust cross-modal fidelity and fine-grained semantic\ndistillation through training. This significantly enhances performance across\ndiverse LVLMs and tasks and scales efficiently to many-shot scenarios,\nbypassing the context window limitations. We also introduce \\textbf{VLibrary},\na repository for storing and retrieving M2IV, enabling flexible LVLM steering\nfor tasks like cross-modal alignment, customized generation and safety\nimprovement. Experiments across seven benchmarks and three LVLMs show that M2IV\nsurpasses Vanilla ICL and prior representation engineering approaches, with an\naverage accuracy gain of \\textbf{3.74\\%} over ICL with the same shot count,\nalongside substantial efficiency advantages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104", "abs": "https://arxiv.org/abs/2504.05104", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04740", "pdf": "https://arxiv.org/pdf/2504.04740", "abs": "https://arxiv.org/abs/2504.04740", "authors": ["Samarth Mishra", "Kate Saenko", "Venkatesh Saligrama"], "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239", "abs": "https://arxiv.org/abs/2504.05239", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "title": "LLM-based Automated Grading with Human-in-the-Loop", "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "rubric"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262", "abs": "https://arxiv.org/abs/2504.05262", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05276", "pdf": "https://arxiv.org/pdf/2504.05276", "abs": "https://arxiv.org/abs/2504.05276", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04841", "pdf": "https://arxiv.org/pdf/2504.04841", "abs": "https://arxiv.org/abs/2504.04841", "authors": ["Sebastian Schmidt", "Julius KÃ¶rner", "Dominik Fuchsgruber", "Stefano Gasperini", "Federico Tombari", "Stephan GÃ¼nnemann"], "title": "Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In panoptic segmentation, individual instances must be separated within\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\nclasses, they struggle with novel categories and out-of-distribution (OOD)\ndata. This is particularly problematic in safety-critical applications, such as\nautonomous driving, where reliability in unseen scenarios is essential. We\naddress the gap between outstanding benchmark performance and reliability by\nproposing Prior2Former (P2F), the first approach for segmentation vision\ntransformers rooted in evidential learning. P2F extends the mask vision\ntransformer architecture by incorporating a Beta prior for computing model\nuncertainty in pixel-wise binary mask assignments. This design enables\nhigh-quality uncertainty estimation that effectively detects novel and OOD\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\npanoptic segmentation. Unlike most segmentation models addressing unknown\nclasses, P2F operates without access to OOD data samples or contrastive\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\nreal-world scenarios where such prior information is unavailable. Additionally,\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\nand OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It\nachieves the highest ranking in the OoDIS anomaly instance benchmark among\nmethods not using OOD data in any way.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "reliability"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04907", "pdf": "https://arxiv.org/pdf/2504.04907", "abs": "https://arxiv.org/abs/2504.04907", "authors": ["Hui Han", "Siyuan Li", "Jiaqi Chen", "Yiwen Yuan", "Yuling Wu", "Chak Tou Leong", "Hanwen Du", "Junchen Fu", "Youhua Li", "Jie Zhang", "Chi Zhang", "Li-jia Li", "Yongxin Ni"], "title": "Video-Bench: Human-Aligned Video Generation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR'25", "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04945", "pdf": "https://arxiv.org/pdf/2504.04945", "abs": "https://arxiv.org/abs/2504.04945", "authors": ["Rean Fernandes", "AndrÃ© Biedenkapp", "Frank Hutter", "Noor Awad"], "title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "comment": "COLM 2025 preprint, 9 pages, 3 figures, 16 appendix pages", "summary": "Legal reasoning tasks present unique challenges for large language models\n(LLMs) due to the complexity of domain-specific knowledge and reasoning\nprocesses. This paper investigates how effectively smaller language models\n(Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514\nMulti-state Bar Examination (MBE) questions to improve legal question answering\naccuracy. We evaluate these models on the 2022 MBE questions licensed from JD\nAdvising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our\nmethodology involves collecting approximately 200 questions per legal domain\nacross 7 domains. We distill the dataset using Llama 3 (70B) to transform\nexplanations into a structured IRAC (Issue, Rule, Application, Conclusion)\nformat as a guided reasoning process to see if it results in better performance\nover the non-distilled dataset. We compare the non-fine-tuned models against\ntheir supervised fine-tuned (SFT) counterparts, trained for different sample\nsizes per domain, to study the effect on accuracy and prompt adherence. We also\nanalyse option selection biases and their mitigation following SFT. In\naddition, we consolidate the performance across multiple variables: prompt type\n(few-shot vs zero-shot), answer ordering (chosen-option first vs\ngenerated-explanation first), response format (Numbered list vs Markdown vs\nJSON), and different decoding temperatures. Our findings show that\ndomain-specific SFT helps some model configurations achieve close to human\nbaseline performance, despite limited computational resources and a relatively\nsmall dataset. We release both the gathered SFT dataset and the family of\nSupervised Fine-tuned (SFT) adapters optimised for MBE performance. This\nestablishes a practical lower bound on resources needed towards achieving\neffective legal question answering in smaller LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04974", "pdf": "https://arxiv.org/pdf/2504.04974", "abs": "https://arxiv.org/abs/2504.04974", "authors": ["Ming Li", "Ruiyi Zhang", "Jian Chen", "Jiuxiang Gu", "Yufan Zhou", "Franck Dernoncourt", "Wanrong Zhu", "Tianyi Zhou", "Tong Sun"], "title": "Towards Visual Text Grounding of Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the existing evolution of Multimodal Large Language Models (MLLMs), a\nnon-neglectable limitation remains in their struggle with visual text\ngrounding, especially in text-rich images of documents. Document images, such\nas scanned forms and infographics, highlight critical challenges due to their\ncomplex layouts and textual content. However, current benchmarks do not fully\naddress these challenges, as they mostly focus on visual grounding on natural\nimages, rather than text-rich document images. Thus, to bridge this gap, we\nintroduce TRIG, a novel task with a newly designed instruction dataset for\nbenchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs\nin document question-answering. Specifically, we propose an OCR-LLM-human\ninteraction pipeline to create 800 manually annotated question-answer pairs as\na benchmark and a large-scale training set of 90$ synthetic data based on four\ndiverse datasets. A comprehensive evaluation of various MLLMs on our proposed\nbenchmark exposes substantial limitations in their grounding capability on\ntext-rich images. In addition, we propose two simple and effective TRIG methods\nbased on general instruction tuning and plug-and-play efficient embedding,\nrespectively. By finetuning MLLMs on our synthetic dataset, they promisingly\nimprove spatial reasoning and grounding capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05112", "pdf": "https://arxiv.org/pdf/2504.05112", "abs": "https://arxiv.org/abs/2504.05112", "authors": ["Ronghui Zhang", "Dakang Lyu", "Tengfei Li", "Yunfan Wu", "Ujjal Manandhar", "Benfei Wang", "Junzhou Chen", "Bolin Gao", "Danwei Wang", "Yiqiu Tan"], "title": "ABCDWaveNet: Advancing Robust Road Ponding Detection in Fog through Dynamic Frequency-Spatial Synergy", "categories": ["cs.CV"], "comment": null, "summary": "Road ponding presents a significant threat to vehicle safety, particularly in\nadverse fog conditions, where reliable detection remains a persistent challenge\nfor Advanced Driver Assistance Systems (ADAS). To address this, we propose\nABCDWaveNet, a novel deep learning framework leveraging Dynamic\nFrequency-Spatial Synergy for robust ponding detection in fog. The core of\nABCDWaveNet achieves this synergy by integrating dynamic convolution for\nadaptive feature extraction across varying visibilities with a wavelet-based\nmodule for synergistic frequency-spatial feature enhancement, significantly\nimproving robustness against fog interference. Building on this foundation,\nABCDWaveNet captures multi-scale structural and contextual information,\nsubsequently employing an Adaptive Attention Coupling Gate (AACG) to adaptively\nfuse global and local features for enhanced accuracy. To facilitate realistic\nevaluations under combined adverse conditions, we introduce the Foggy Low-Light\nPuddle dataset. Extensive experiments demonstrate that ABCDWaveNet establishes\nnew state-of-the-art performance, achieving significant Intersection over Union\n(IoU) gains of 3.51%, 1.75%, and 1.03% on the Foggy-Puddle, Puddle-1000, and\nour Foggy Low-Light Puddle datasets, respectively. Furthermore, its processing\nspeed of 25.48 FPS on an NVIDIA Jetson AGX Orin confirms its suitability for\nADAS deployment. These findings underscore the effectiveness of the proposed\nDynamic Frequency-Spatial Synergy within ABCDWaveNet, offering valuable\ninsights for developing proactive road safety solutions capable of operating\nreliably in challenging weather conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05305", "pdf": "https://arxiv.org/pdf/2504.05305", "abs": "https://arxiv.org/abs/2504.05305", "authors": ["Sangbeom Lim", "Junwan Kim", "Heeji Yoon", "Jaewoo Jung", "Seungryong Kim"], "title": "URECA: Unique Region Caption Anything", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://cvlab-kaist.github.io/URECA Code:\n  https://github.com/cvlab-kaist/URECA", "summary": "Region-level captioning aims to generate natural language descriptions for\nspecific image regions while highlighting their distinguishing features.\nHowever, existing methods struggle to produce unique captions across\nmulti-granularity, limiting their real-world applicability. To address the need\nfor detailed region-level understanding, we introduce URECA dataset, a\nlarge-scale dataset tailored for multi-granularity region captioning. Unlike\nprior datasets that focus primarily on salient objects, URECA dataset ensures a\nunique and consistent mapping between regions and captions by incorporating a\ndiverse set of objects, parts, and background elements. Central to this is a\nstage-wise data curation pipeline, where each stage incrementally refines\nregion selection and caption generation. By leveraging Multimodal Large\nLanguage Models (MLLMs) at each stage, our pipeline produces distinctive and\ncontextually grounded captions with improved accuracy and semantic diversity.\nBuilding upon this dataset, we present URECA, a novel captioning model designed\nto effectively encode multi-granularity regions. URECA maintains essential\nspatial properties such as position and shape through simple yet impactful\nmodifications to existing MLLMs, enabling fine-grained and semantically rich\nregion descriptions. Our approach introduces dynamic mask modeling and a\nhigh-resolution mask encoder to enhance caption uniqueness. Experiments show\nthat URECA achieves state-of-the-art performance on URECA dataset and\ngeneralizes well to existing region-level captioning benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03703", "pdf": "https://arxiv.org/pdf/2504.03703", "abs": "https://arxiv.org/abs/2504.03703", "authors": ["Mario Padilla Rodriguez", "Mohamed Nafea"], "title": "Hierarchical Attention Network for Interpretable ECG-based Heart Disease Classification", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "Work in progress. 7 pages, 4 figures", "summary": "Cardiovascular disease remains one of the leading causes of mortality\nworldwide, underscoring the need for accurate as well as interpretable\ndiagnostic machine learning tools. In this work, we investigate heart disease\nclassification using electrocardiogram (ECG) data from two widely-utilized\ndatasets: The MIT-BIH Arrhythmia and the PTB-XL datasets. We adapt a\nhierarchical attention network (HAN), originally developed for text\nclassification, into an ECG-based heart-disease classification task. Our\nadapted HAN incorporates two attention layers that focus on ECG data segments\nof varying sizes. We conduct a comparative analysis between our adapted HAN and\na more sophisticated state-of-the-art architecture, featuring a network with\nconvolution, attention, and transformer layers (CAT-Net). Our empirical\nevaluation encompasses multiple aspects including test accuracy (quantified by\n0-1 loss); model complexity (measured by the number of model parameters); and\ninterpretability (through attention map visualization). Our adapted HAN\ndemonstrates comparable test accuracy with significant reductions in model\ncomplexity and enhanced interpretability analysis: For the MIT-BIH dataset, our\nadapted HAN achieves 98.55\\% test accuracy compared to 99.14\\% for CAT-Net,\nwhile reducing the number of model parameters by a factor of 15.6. For the\nPTB-XL dataset, our adapted HAN achieves a 19.3-fold reduction in model\ncomplexity compared to CAT-Net, with only a 5\\% lower test accuracy. From an\ninterpretability perspective, the significantly simpler architecture and the\nhierarchical nature of our adapted HAN model facilitate a more straightforward\ninterpretability analysis based on visualizing attention weights. Building on\nthis advantage, we conduct an interpretability analysis of our HAN that\nhighlights the regions of the ECG signal most relevant to the model's\ndecisions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03782", "pdf": "https://arxiv.org/pdf/2504.03782", "abs": "https://arxiv.org/abs/2504.03782", "authors": ["Ramin Zarei Sabzevar", "Hamed Mohammadzadeh", "Tahmineh Tavakoli", "Ahad Harati"], "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04458", "pdf": "https://arxiv.org/pdf/2504.04458", "abs": "https://arxiv.org/abs/2504.04458", "authors": ["Bashir Alam", "Masa Cirkovic", "Mete Harun Akcay", "Md Kaf Shahrier", "Sebastien Lafond", "Hergys Rexha", "Kurt Benke", "Sepinoud Azimi", "Janan Arslan"], "title": "CALF: A Conditionally Adaptive Loss Function to Mitigate Class-Imbalanced Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Imbalanced datasets pose a considerable challenge in training deep learning\n(DL) models for medical diagnostics, particularly for segmentation tasks.\nImbalance may be associated with annotation quality limited annotated datasets,\nrare cases, or small-scale regions of interest (ROIs). These conditions\nadversely affect model training and performance, leading to segmentation\nboundaries which deviate from the true ROIs. Traditional loss functions, such\nas Binary Cross Entropy, replicate annotation biases and limit model\ngeneralization. We propose a novel, statistically driven, conditionally\nadaptive loss function (CALF) tailored to accommodate the conditions of\nimbalanced datasets in DL training. It employs a data-driven methodology by\nestimating imbalance severity using statistical methods of skewness and\nkurtosis, then applies an appropriate transformation to balance the training\ndataset while preserving data heterogeneity. This transformative approach\nintegrates a multifaceted process, encompassing preprocessing, dataset\nfiltering, and dynamic loss selection to achieve optimal outcomes. We benchmark\nour method against conventional loss functions using qualitative and\nquantitative evaluations. Experiments using large-scale open-source datasets\n(i.e., UPENN-GBM, UCSF, LGG, and BraTS) validate our approach, demonstrating\nsubstantial segmentation improvements. Code availability:\nhttps://anonymous.4open.science/r/MICCAI-Submission-43F9/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04532", "pdf": "https://arxiv.org/pdf/2504.04532", "abs": "https://arxiv.org/abs/2504.04532", "authors": ["Moinak Bhattacharya", "Saumya Gupta", "Annie Singh", "Chao Chen", "Gagandeep Singh", "Prateek Prasanna"], "title": "BrainMRDiff: A Diffusion Model for Anatomically Consistent Brain MRI Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate brain tumor diagnosis relies on the assessment of multiple Magnetic\nResonance Imaging (MRI) sequences. However, in clinical practice, the\nacquisition of certain sequences may be affected by factors like motion\nartifacts or contrast agent contraindications, leading to suboptimal outcome,\nsuch as poor image quality. This can then affect image interpretation by\nradiologists. Synthesizing high quality MRI sequences has thus become a\ncritical research focus. Though recent advancements in controllable generative\nAI have facilitated the synthesis of diagnostic quality MRI, ensuring\nanatomical accuracy remains a significant challenge. Preserving critical\nstructural relationships between different anatomical regions is essential, as\neven minor structural or topological inconsistencies can compromise diagnostic\nvalidity. In this work, we propose BrainMRDiff, a novel topology-preserving,\nanatomy-guided diffusion model for synthesizing brain MRI, leveraging brain and\ntumor anatomies as conditioning inputs. To achieve this, we introduce two key\nmodules: Tumor+Structure Aggregation (TSA) and Topology-Guided Anatomy\nPreservation (TGAP). TSA integrates diverse anatomical structures with tumor\ninformation, forming a comprehensive conditioning mechanism for the diffusion\nprocess. TGAP enforces topological consistency during reverse denoising\ndiffusion process; both these modules ensure that the generated image respects\nanatomical integrity. Experimental results demonstrate that BrainMRDiff\nsurpasses existing baselines, achieving performance improvements of 23.33% on\nthe BraTS-AG dataset and 33.33% on the BraTS-Met dataset. Code will be made\npublicly available soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04664", "pdf": "https://arxiv.org/pdf/2504.04664", "abs": "https://arxiv.org/abs/2504.04664", "authors": ["Md Bayazid Hossain", "Md Anwarul Islam Himel", "Md Abdur Rahim", "Shabbir Mahmood", "Abu Saleh Musa Miah", "Jungpil Shin"], "title": "Classification of ADHD and Healthy Children Using EEG Based Multi-Band Spatial Features Enhancement", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "Attention Deficit Hyperactivity Disorder (ADHD) is a common\nneurodevelopmental disorder in children, characterized by difficulties in\nattention, hyperactivity, and impulsivity. Early and accurate diagnosis of ADHD\nis critical for effective intervention and management. Electroencephalogram\n(EEG) signals have emerged as a non-invasive and efficient tool for ADHD\ndetection due to their high temporal resolution and ability to capture neural\ndynamics. In this study, we propose a method for classifying ADHD and healthy\nchildren using EEG data from the benchmark dataset. There were 61 children with\nADHD and 60 healthy children, both boys and girls, aged 7 to 12. The EEG\nsignals, recorded from 19 channels, were processed to extract Power Spectral\nDensity (PSD) and Spectral Entropy (SE) features across five frequency bands,\nresulting in a comprehensive 190-dimensional feature set. To evaluate the\nclassification performance, a Support Vector Machine (SVM) with the RBF kernel\ndemonstrated the best performance with a mean cross-validation accuracy of\n99.2\\% and a standard deviation of 0.0079, indicating high robustness and\nprecision. These results highlight the potential of spatial features in\nconjunction with machine learning for accurately classifying ADHD using EEG\ndata. This work contributes to developing non-invasive, data-driven tools for\nearly diagnosis and assessment of ADHD in children.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03786", "pdf": "https://arxiv.org/pdf/2504.03786", "abs": "https://arxiv.org/abs/2504.03786", "authors": ["Sifan Li", "Yujun Cai", "Bryan Hooi", "Nanyun Peng", "Yiwei Wang"], "title": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs", "categories": ["cs.CL"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03931", "pdf": "https://arxiv.org/pdf/2504.03931", "abs": "https://arxiv.org/abs/2504.03931", "authors": ["Zixuan Ke", "Yifei Ming", "Shafiq Joty"], "title": "Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial Proposal for NAACL2025", "summary": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "code generation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03979", "pdf": "https://arxiv.org/pdf/2504.03979", "abs": "https://arxiv.org/abs/2504.03979", "authors": ["Amit K Verma", "Zhisong Zhang", "Junwon Seo", "Robin Kuo", "Runbo Jiang", "Emma Strubell", "Anthony D Rollett"], "title": "Structured Extraction of Process Structure Properties Relationships in Materials Science", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.IR"], "comment": "16 pages, 3 figures, 13 table", "summary": "With the advent of large language models (LLMs), the vast unstructured text\nwithin millions of academic papers is increasingly accessible for materials\ndiscovery, although significant challenges remain. While LLMs offer promising\nfew- and zero-shot learning capabilities, particularly valuable in the\nmaterials domain where expert annotations are scarce, general-purpose LLMs\noften fail to address key materials-specific queries without further\nadaptation. To bridge this gap, fine-tuning LLMs on human-labeled data is\nessential for effective structured knowledge extraction. In this study, we\nintroduce a novel annotation schema designed to extract generic\nprocess-structure-properties relationships from scientific literature. We\ndemonstrate the utility of this approach using a dataset of 128 abstracts, with\nannotations drawn from two distinct domains: high-temperature materials (Domain\nI) and uncertainty quantification in simulating materials microstructure\n(Domain II). Initially, we developed a conditional random field (CRF) model\nbased on MatBERT, a domain-specific BERT variant, and evaluated its performance\non Domain I. Subsequently, we compared this model with a fine-tuned LLM (GPT-4o\nfrom OpenAI) under identical conditions. Our results indicate that fine-tuning\nLLMs can significantly improve entity extraction performance over the BERT-CRF\nbaseline on Domain I. However, when additional examples from Domain II were\nincorporated, the performance of the BERT-CRF model became comparable to that\nof the GPT-4o model. These findings underscore the potential of our schema for\nstructured knowledge extraction and highlight the complementary strengths of\nboth modeling approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03894", "pdf": "https://arxiv.org/pdf/2504.03894", "abs": "https://arxiv.org/abs/2504.03894", "authors": ["Haiqing Li", "Yuzhi Guo", "Feng Jiang", "Qifeng Zhou", "Hehuan Ma", "Junzhou Huang"], "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04010", "pdf": "https://arxiv.org/pdf/2504.04010", "abs": "https://arxiv.org/abs/2504.04010", "authors": ["Maksim Siniukov", "Di Chang", "Minh Tran", "Hongkun Gong", "Ashutosh Chaubey", "Mohammad Soleymani"], "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion", "categories": ["cs.CV", "cs.LG", "I.4.9"], "comment": "Project page: https://havent-invented.github.io/DiTaiListener", "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04012", "pdf": "https://arxiv.org/pdf/2504.04012", "abs": "https://arxiv.org/abs/2504.04012", "authors": ["Houzhang Fang", "Xiaolin Wang", "Zengyang Li", "Lu Wang", "Qingshan Li", "Yi Chang", "Luxin Yan"], "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025", "summary": "Infrared unmanned aerial vehicle (UAV) images captured using thermal\ndetectors are often affected by temperature dependent low-frequency\nnonuniformity, which significantly reduces the contrast of the images.\nDetecting UAV targets under nonuniform conditions is crucial in UAV\nsurveillance applications. Existing methods typically treat infrared\nnonuniformity correction (NUC) as a preprocessing step for detection, which\nleads to suboptimal performance. Balancing the two tasks while enhancing\ndetection beneficial information remains challenging. In this paper, we present\na detection-friendly union framework, termed UniCD, that simultaneously\naddresses both infrared NUC and UAV target detection tasks in an end-to-end\nmanner. We first model NUC as a small number of parameter estimation problem\njointly driven by priors and data to generate detection-conducive images. Then,\nwe incorporate a new auxiliary loss with target mask supervision into the\nbackbone of the infrared UAV target detection network to strengthen target\nfeatures while suppressing the background. To better balance correction and\ndetection, we introduce a detection-guided self-supervised loss to reduce\nfeature discrepancies between the two tasks, thereby enhancing detection\nrobustness to varying nonuniformity levels. Additionally, we construct a new\nbenchmark composed of 50,000 infrared images in various nonuniformity types,\nmulti-scale UAV targets and rich backgrounds with target annotations, called\nIRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust\nunion framework for NUC and UAV target detection while achieving real-time\nprocessing capabilities. Dataset can be available at\nhttps://github.com/IVPLaboratory/UniCD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04141", "pdf": "https://arxiv.org/pdf/2504.04141", "abs": "https://arxiv.org/abs/2504.04141", "authors": ["Yougang Lyu", "Shijie Ren", "Yue Feng", "Zihan Wang", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke"], "title": "Cognitive Debiasing Large Language Models for Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04025", "pdf": "https://arxiv.org/pdf/2504.04025", "abs": "https://arxiv.org/abs/2504.04025", "authors": ["Daniel Rivera", "Jacob Huddin", "Alexander Banerjee", "Rongzhen Zhang", "Brenda Mai", "Hanadi El Achi", "Jacob Armstrong", "Amer Wahed", "Andy Nguyen"], "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 6 figures, 1 table", "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04150", "pdf": "https://arxiv.org/pdf/2504.04150", "abs": "https://arxiv.org/abs/2504.04150", "authors": ["Yidong Wang"], "title": "Reasoning on Multiple Needles In A Haystack", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Needle In A Haystack (NIAH) task has been widely used to evaluate the\nlong-context question-answering capabilities of Large Language Models (LLMs).\nHowever, its reliance on simple retrieval limits its effectiveness. To address\nthis limitation, recent studies have introduced the Multiple Needles In A\nHaystack Reasoning (MNIAH-R) task, which incorporates supporting documents\n(Multiple needles) of multi-hop reasoning tasks into a distracting context\n(Haystack}). Despite this advancement, existing approaches still fail to\naddress the issue of models providing direct answers from internal knowledge,\nand they do not explain or mitigate the decline in accuracy as context length\nincreases. In this paper, we tackle the memory-based answering problem by\nfiltering out direct-answer questions, and we reveal that performance\ndegradation is primarily driven by the reduction in the length of the thinking\nprocess as the input length increases. Building on this insight, we decompose\nthe thinking process into retrieval and reasoning stages and introduce a\nreflection mechanism for multi-round extension. We also train a model using the\ngenerated iterative thinking process, which helps mitigate the performance\ndegradation. Furthermore, we demonstrate the application of this\nretrieval-reflection capability in mathematical reasoning scenarios, improving\nGPT-4o's performance on AIME2024.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04034", "pdf": "https://arxiv.org/pdf/2504.04034", "abs": "https://arxiv.org/abs/2504.04034", "authors": ["Dianshuo Li", "Li Chen", "Yunxiang Cao", "Kai Zhu", "Jun Cheng"], "title": "UCS: A Universal Model for Curvilinear Structure Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "Curvilinear structure segmentation (CSS) is vital in various domains,\nincluding medical imaging, landscape analysis, industrial surface inspection,\nand plant analysis. While existing methods achieve high performance within\nspecific domains, their generalizability is limited. On the other hand,\nlarge-scale models such as Segment Anything Model (SAM) exhibit strong\ngeneralization but are not optimized for curvilinear structures. Existing\nadaptations of SAM primarily focus on general object segmentation and lack\nspecialized design for CSS tasks. To bridge this gap, we propose the Universal\nCurvilinear structure Segmentation (\\textit{UCS}) model, which adapts SAM to\nCSS tasks while enhancing its generalization. \\textit{UCS} features a novel\nencoder architecture integrating a pretrained SAM encoder with two innovations:\na Sparse Adapter, strategically inserted to inherit the pre-trained SAM\nencoder's generalization capability while minimizing the number of fine-tuning\nparameters, and a Prompt Generation module, which leverages Fast Fourier\nTransform with a high-pass filter to generate curve-specific prompts.\nFurthermore, the \\textit{UCS} incorporates a mask decoder that eliminates\nreliance on manual interaction through a dual-compression module: a\nHierarchical Feature Compression module, which aggregates the outputs of the\nsampled encoder to enhance detail preservation, and a Guidance Feature\nCompression module, which extracts and compresses image-driven guidance\nfeatures. Evaluated on a comprehensive multi-domain dataset, including an\nin-house dataset covering eight natural curvilinear structures, \\textit{UCS}\ndemonstrates state-of-the-art generalization and open-set segmentation\nperformance across medical, engineering, natural, and plant imagery,\nestablishing a new benchmark for universal CSS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04045", "pdf": "https://arxiv.org/pdf/2504.04045", "abs": "https://arxiv.org/abs/2504.04045", "authors": ["Conghao Xiong", "Hao Chen", "Joseph J. Y. Sung"], "title": "A Survey of Pathology Foundation Model: Progress and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Computational pathology, analyzing whole slide images for automated cancer\ndiagnosis, relies on the multiple instance learning framework where performance\nheavily depends on the feature extractor and aggregator. Recent Pathology\nFoundation Models (PFMs), pretrained on large-scale histopathology data, have\nsignificantly enhanced capabilities of extractors and aggregators but lack\nsystematic analysis frameworks. This survey presents a hierarchical taxonomy\norganizing PFMs through a top-down philosophy that can be utilized to analyze\nFMs in any domain: model scope, model pretraining, and model design.\nAdditionally, we systematically categorize PFM evaluation tasks into\nslide-level, patch-level, multimodal, and biological tasks, providing\ncomprehensive benchmarking criteria. Our analysis identifies critical\nchallenges in both PFM development (pathology-specific methodology, end-to-end\npretraining, data-model scalability) and utilization (effective adaptation,\nmodel maintenance), paving the way for future directions in this promising\nfield. Resources referenced in this survey are available at\nhttps://github.com/BearCleverProud/AwesomeWSI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04155", "pdf": "https://arxiv.org/pdf/2504.04155", "abs": "https://arxiv.org/abs/2504.04155", "authors": ["Hengyu Luo", "Zihao Li", "Joseph Attieh", "Sawal Devkota", "Ona de Gibert", "Shaoxiong Ji", "Peiqin Lin", "Bhavani Sai Praneeth Varma Mantina", "Ananda Sreenidhi", "RaÃºl VÃ¡zquez", "Mengjie Wang", "Samea Yusofi", "JÃ¶rg Tiedemann"], "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are advancing at an unprecedented pace globally,\nwith regions increasingly adopting these models for applications in their\nprimary language. Evaluation of these models in diverse linguistic\nenvironments, especially in low-resource languages, has become a major\nchallenge for academia and industry. Existing evaluation frameworks are\ndisproportionately focused on English and a handful of high-resource languages,\nthereby overlooking the realistic performance of LLMs in multilingual and\nlower-resource scenarios. To address this gap, we introduce GlotEval, a\nlightweight framework designed for massively multilingual evaluation.\nSupporting seven key tasks (machine translation, text classification,\nsummarization, open-ended generation, reading comprehension, sequence labeling,\nand intrinsic evaluation), spanning over dozens to hundreds of languages,\nGlotEval highlights consistent multilingual benchmarking, language-specific\nprompt templates, and non-English-centric machine translation. This enables a\nprecise diagnosis of model strengths and weaknesses in diverse linguistic\ncontexts. A multilingual translation case study demonstrates GlotEval's\napplicability for multilingual and language-specific evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "summarization"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04051", "pdf": "https://arxiv.org/pdf/2504.04051", "abs": "https://arxiv.org/abs/2504.04051", "authors": ["Xuyang Guo", "Zekai Huang", "Jiayan Huo", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have driven significant progress in a variety of AI tasks,\nincluding text-to-video generation, where models like Video LDM and Stable\nVideo Diffusion can produce realistic, movie-level videos from textual\ninstructions. Despite these advances, current text-to-video models still face\nfundamental challenges in reliably following human commands, particularly in\nadhering to simple numerical constraints. In this work, we present\nT2VCountBench, a specialized benchmark aiming at evaluating the counting\ncapability of SOTA text-to-video models as of 2025. Our benchmark employs\nrigorous human evaluations to measure the number of generated objects and\ncovers a diverse range of generators, covering both open-source and commercial\nmodels. Extensive experiments reveal that all existing models struggle with\nbasic numerical tasks, almost always failing to generate videos with an object\ncount of 9 or fewer. Furthermore, our comprehensive ablation studies explore\nhow factors like video style, temporal dynamics, and multilingual inputs may\ninfluence counting performance. We also explore prompt refinement techniques\nand demonstrate that decomposing the task into smaller subtasks does not easily\nalleviate these limitations. Our findings highlight important challenges in\ncurrent text-to-video generation and provide insights for future research aimed\nat improving adherence to basic numerical constraints.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04085", "pdf": "https://arxiv.org/pdf/2504.04085", "abs": "https://arxiv.org/abs/2504.04085", "authors": ["Xiao-Hui Li", "Fei Yin", "Cheng-Lin Liu"], "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted by CVPR 2025", "summary": "Document image segmentation is crucial for document analysis and recognition\nbut remains challenging due to the diversity of document formats and\nsegmentation tasks. Existing methods often address these tasks separately,\nresulting in limited generalization and resource wastage. This paper introduces\nDocSAM, a transformer-based unified framework designed for various document\nimage segmentation tasks, such as document layout analysis, multi-granularity\ntext segmentation, and table structure recognition, by modelling these tasks as\na combination of instance and semantic segmentation. Specifically, DocSAM\nemploys Sentence-BERT to map category names from each dataset into semantic\nqueries that match the dimensionality of instance queries. These two sets of\nqueries interact through an attention mechanism and are cross-attended with\nimage features to predict instance and semantic segmentation masks. Instance\ncategories are predicted by computing the dot product between instance and\nsemantic queries, followed by softmax normalization of scores. Consequently,\nDocSAM can be jointly trained on heterogeneous datasets, enhancing robustness\nand generalization while reducing computational and storage resources.\nComprehensive evaluations show that DocSAM surpasses existing methods in\naccuracy, efficiency, and adaptability, highlighting its potential for\nadvancing document image understanding and segmentation across various\napplications. Codes are available at https://github.com/xhli-git/DocSAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04099", "pdf": "https://arxiv.org/pdf/2504.04099", "abs": "https://arxiv.org/abs/2504.04099", "authors": ["Chunzhao Xie", "Tongxuan Liu", "Lei Jiang", "Yuting Zeng", "jinrong Guo", "Yunheng Shen", "Weizhe Huang", "Jing Li", "Xiaohua Xu"], "title": "TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models have demonstrated remarkable performance across\nvarious tasks; however, the challenge of hallucinations constrains their\npractical applications. The hallucination problem arises from multiple factors,\nincluding the inherent hallucinations in language models, the limitations of\nvisual encoders in perception, and biases introduced by multimodal data.\nExtensive research has explored ways to mitigate hallucinations. For instance,\nOPERA prevents the model from overly focusing on \"anchor tokens\", thereby\nreducing hallucinations, whereas VCD mitigates hallucinations by employing a\ncontrastive decoding approach. In this paper, we investigate the correlation\nbetween the decay of attention to image tokens and the occurrence of\nhallucinations. Based on this finding, we propose Temporal Attention Real-time\nAccumulative Connection (TARAC), a novel training-free method that dynamically\naccumulates and updates LVLMs' attention on image tokens during generation. By\nenhancing the model's attention to image tokens, TARAC mitigates hallucinations\ncaused by the decay of attention on image tokens. We validate the effectiveness\nof TARAC across multiple models and datasets, demonstrating that our approach\nsubstantially mitigates hallucinations. In particular, TARAC reduces $C_S$ by\n25.2 and $C_I$ by 8.7 compared to VCD on the CHAIR benchmark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04124", "pdf": "https://arxiv.org/pdf/2504.04124", "abs": "https://arxiv.org/abs/2504.04124", "authors": ["Muhammad Ahmed Ullah Khan", "Abdul Hannan Khan", "Andreas Dengel"], "title": "EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Event cameras have higher temporal resolution, and require less storage and\nbandwidth compared to traditional RGB cameras. However, due to relatively\nlagging performance of event-based approaches, event cameras have not yet\nreplace traditional cameras in performance-critical applications like\nautonomous driving. Recent approaches in event-based object detection try to\nbridge this gap by employing computationally expensive transformer-based\nsolutions. However, due to their resource-intensive components, these solutions\nfail to exploit the sparsity and higher temporal resolution of event cameras\nefficiently. Moreover, these solutions are adopted from the vision domain,\nlacking specificity to the event cameras. In this work, we explore efficient\nand performant alternatives to recurrent vision transformer models and propose\na novel event-based object detection backbone. The proposed backbone employs a\nnovel Event Progression Extractor module, tailored specifically for event data,\nand uses Metaformer concept with convolution-based efficient components. We\nevaluate the resultant model on well-established traffic object detection\nbenchmarks and conduct cross-dataset evaluation to test its ability to\ngeneralize. The proposed model outperforms the state-of-the-art on Prophesee\nGen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF\nbecomes the fastest DNN-based architecture in the domain by outperforming most\nefficient event-based object detectors. Moreover, the proposed model shows\nbetter ability to generalize to unseen data and scales better with the\nabundance of data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04264", "pdf": "https://arxiv.org/pdf/2504.04264", "abs": "https://arxiv.org/abs/2504.04264", "authors": ["Mingyang Wang", "Heike Adel", "Lukas Lange", "Yihong Liu", "Ercong Nie", "Jannik StrÃ¶tgen", "Hinrich SchÃ¼tze"], "title": "Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual language models (MLMs) store factual knowledge across languages\nbut often struggle to provide consistent responses to semantically equivalent\nprompts in different languages. While previous studies point out this\ncross-lingual inconsistency issue, the underlying causes remain unexplored. In\nthis work, we use mechanistic interpretability methods to investigate\ncross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a\nlanguage-independent concept space through most layers, and only transition to\nlanguage-specific spaces in the final layers. Failures during the language\ntransition often result in incorrect predictions in the target language, even\nwhen the answers are correct in other languages. To mitigate this inconsistency\nissue, we propose a linear shortcut method that bypasses computations in the\nfinal layers, enhancing both prediction accuracy and cross-lingual consistency.\nOur findings shed light on the internal mechanisms of MLMs and provide a\nlightweight, effective strategy for producing more consistent factual outputs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04130", "pdf": "https://arxiv.org/pdf/2504.04130", "abs": "https://arxiv.org/abs/2504.04130", "authors": ["Andrei-Alexandru Preda", "Iulian-Marius TÄiatu", "Dumitru-Clementin Cercel"], "title": "Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images", "categories": ["cs.CV"], "comment": null, "summary": "In the field of deep learning, large architectures often obtain the best\nperformance for many tasks, but also require massive datasets. In the\nhistological domain, tissue images are expensive to obtain and constitute\nsensitive medical information, raising concerns about data scarcity and\nprivacy. Vision Transformers are state-of-the-art computer vision models that\nhave proven helpful in many tasks, including image classification. In this\nwork, we combine vision Transformers with generative adversarial networks to\ngenerate histopathological images related to colorectal cancer and test their\nquality by augmenting a training dataset, leading to improved classification\naccuracy. Then, we replicate this performance using the federated learning\ntechnique and a realistic Kubernetes setup with multiple nodes, simulating a\nscenario where the training dataset is split among several hospitals unable to\nshare their information directly due to privacy concerns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04158", "pdf": "https://arxiv.org/pdf/2504.04158", "abs": "https://arxiv.org/abs/2504.04158", "authors": ["Yunlong Lin", "Zixu Lin", "Haoyu Chen", "Panwang Pan", "Chenxin Li", "Sixiang Chen", "Yeying Jin", "Wenbo Li", "Xinghao Ding"], "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration", "categories": ["cs.CV"], "comment": "25 pages, 15 figures", "summary": "Vision-centric perception systems struggle with unpredictable and coupled\nweather degradations in the wild. Current solutions are often limited, as they\neither depend on specific degradation priors or suffer from significant domain\ngaps. To enable robust and autonomous operation in real-world conditions, we\npropose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to\nmanage multiple expert restoration models. To further enhance system\nrobustness, reduce hallucinations, and improve generalizability in real-world\nadverse weather, JarvisIR employs a novel two-stage framework consisting of\nsupervised fine-tuning and human feedback alignment. Specifically, to address\nthe lack of paired data in real-world scenarios, the human feedback alignment\nenables the VLM to be fine-tuned effectively on large-scale real-world data in\nan unsupervised manner. To support the training and evaluation of JarvisIR, we\nintroduce CleanBench, a comprehensive dataset consisting of high-quality and\nlarge-scale instruction-responses pairs, including 150K synthetic entries and\n80K real entries. Extensive experiments demonstrate that JarvisIR exhibits\nsuperior decision-making and restoration capabilities. Compared with existing\nmethods, it achieves a 50% improvement in the average of all perception metrics\non CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04185", "pdf": "https://arxiv.org/pdf/2504.04185", "abs": "https://arxiv.org/abs/2504.04185", "authors": ["Dong Liu", "Yuanchao Wu", "Bowen Tong", "Jiansong Deng"], "title": "SDEIT: Semantic-Driven Electrical Impedance Tomography", "categories": ["cs.CV"], "comment": null, "summary": "Regularization methods using prior knowledge are essential in solving\nill-posed inverse problems such as Electrical Impedance Tomography (EIT).\nHowever, designing effective regularization and integrating prior information\ninto EIT remains challenging due to the complexity and variability of\nanatomical structures. In this work, we introduce SDEIT, a novel\nsemantic-driven framework that integrates Stable Diffusion 3.5 into EIT,\nmarking the first use of large-scale text-to-image generation models in EIT.\nSDEIT employs natural language prompts as semantic priors to guide the\nreconstruction process. By coupling an implicit neural representation (INR)\nnetwork with a plug-and-play optimization scheme that leverages SD-generated\nimages as generative priors, SDEIT improves structural consistency and recovers\nfine details. Importantly, this method does not rely on paired training\ndatasets, increasing its adaptability to varied EIT scenarios. Extensive\nexperiments on both simulated and experimental data demonstrate that SDEIT\noutperforms state-of-the-art techniques, offering superior accuracy and\nrobustness. This work opens a new pathway for integrating multimodal priors\ninto ill-posed inverse problems like EIT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04314", "pdf": "https://arxiv.org/pdf/2504.04314", "abs": "https://arxiv.org/abs/2504.04314", "authors": ["Justin Miller", "Tristram Alexander"], "title": "Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.TH"], "comment": "12 pages, 4 figures, 2 tables", "summary": "The challenge of clustering short text data lies in balancing informativeness\nwith interpretability. Traditional evaluation metrics often overlook this\ntrade-off. Inspired by linguistic principles of communicative efficiency, this\npaper investigates the optimal number of clusters by quantifying the trade-off\nbetween informativeness and cognitive simplicity. We use large language models\n(LLMs) to generate cluster names and evaluate their effectiveness through\nsemantic density, information theory, and clustering accuracy. Our results show\nthat Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM,\nincreases semantic density compared to random assignment, effectively grouping\nsimilar bios. However, as clusters increase, interpretability declines, as\nmeasured by a generative LLM's ability to correctly assign bios based on\ncluster names. A logistic regression analysis confirms that classification\naccuracy depends on the semantic similarity between bios and their assigned\ncluster names, as well as their distinction from alternatives.\n  These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet\ninterpretable. We identify an optimal range of 16-22 clusters, paralleling\nlinguistic efficiency in lexical categorization. These insights inform both\ntheoretical models and practical applications, guiding future research toward\noptimising cluster interpretability and usefulness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04196", "pdf": "https://arxiv.org/pdf/2504.04196", "abs": "https://arxiv.org/abs/2504.04196", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages", "summary": "With the growing sizes of AI models like large language models (LLMs) and\nvision transformers, deploying them on devices with limited computational\nresources is a significant challenge particularly when addressing domain\ngeneralisation (DG) tasks. This paper introduces a novel grouped structural\npruning method for pre-trained vision transformers (ViT, BeiT, and DeiT),\nevaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency\ngraph analysis to identify and remove redundant groups of neurons, weights,\nfilters, or attention heads within transformers, using a range of selection\nmetrics. Grouped structural pruning is applied at pruning ratios of 50\\%, 75\\%\nand 95\\% and the models are then fine-tuned on selected distributions from DG\nbenchmarks to evaluate their overall performance in DG tasks. Results show\nsignificant improvements in inference speed and fine-tuning time with minimal\ntrade-offs in accuracy and DG task performance. For instance, on the PACS\nbenchmark, pruning ViT, BeiT, and DeiT models by 50\\% using the Hessian metric\nresulted in accuracy drops of only -2.94\\%, -1.42\\%, and -1.72\\%, respectively,\nwhile achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings\ndemonstrate the effectiveness of our approach in balancing model efficiency\nwith domain generalisation performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04336", "pdf": "https://arxiv.org/pdf/2504.04336", "abs": "https://arxiv.org/abs/2504.04336", "authors": ["Cong Sun", "Kurt Teichman", "Yiliang Zhou", "Brian Critelli", "David Nauheim", "Graham Keir", "Xindi Wang", "Judy Zhong", "Adam E Flanders", "George Shih", "Yifan Peng"], "title": "Generative Large Language Models Trained for Detecting Errors in Radiology Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this retrospective study, a dataset was constructed with two parts. The\nfirst part included 1,656 synthetic chest radiology reports generated by GPT-4\nusing specified prompts, with 828 being error-free synthetic reports and 828\ncontaining errors. The second part included 614 reports: 307 error-free reports\nbetween 2011 and 2016 from the MIMIC-CXR database and 307 corresponding\nsynthetic reports with errors generated by GPT-4 on the basis of these\nMIMIC-CXR reports and specified prompts. All errors were categorized into four\ntypes: negation, left/right, interval change, and transcription errors. Then,\nseveral models, including Llama-3, GPT-4, and BiomedBERT, were refined using\nzero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally,\nthe performance of these models was evaluated using the F1 score, 95\\%\nconfidence interval (CI) and paired-sample t-tests on our constructed dataset,\nwith the prediction results further assessed by radiologists. Using zero-shot\nprompting, the fine-tuned Llama-3-70B-Instruct model achieved the best\nperformance with the following F1 scores: 0.769 for negation errors, 0.772 for\nleft/right errors, 0.750 for interval change errors, 0.828 for transcription\nerrors, and 0.780 overall. In the real-world evaluation phase, two radiologists\nreviewed 200 randomly selected reports output by the model. Of these, 99 were\nconfirmed to contain errors detected by the models by both radiologists, and\n163 were confirmed to contain model-detected errors by at least one\nradiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology\nreports, greatly enhanced error detection in radiology reports.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04473", "pdf": "https://arxiv.org/pdf/2504.04473", "abs": "https://arxiv.org/abs/2504.04473", "authors": ["Archana Sahu", "Plaban Kumar Bhowmick"], "title": "Directed Graph-alignment Approach for Identification of Gaps in Short Answers", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 11 figures", "summary": "In this paper, we have presented a method for identifying missing items known\nas gaps in the student answers by comparing them against the corresponding\nmodel answer/reference answers, automatically. The gaps can be identified at\nword, phrase or sentence level. The identified gaps are useful in providing\nfeedback to the students for formative assessment. The problem of gap\nidentification has been modelled as an alignment of a pair of directed graphs\nrepresenting a student answer and the corresponding model answer for a given\nquestion. To validate the proposed approach, the gap annotated student answers\nconsidering answers from three widely known datasets in the short answer\ngrading domain, namely, University of North Texas (UNT), SciEntsBank, and\nBeetle have been developed and this gap annotated student answers' dataset is\navailable at: https://github.com/sahuarchana7/gaps-answers-dataset. Evaluation\nmetrics used in the traditional machine learning tasks have been adopted to\nevaluate the task of gap identification. Though performance of the proposed\napproach varies across the datasets and the types of the answers, overall the\nperformance is observed to be promising.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04435", "pdf": "https://arxiv.org/pdf/2504.04435", "abs": "https://arxiv.org/abs/2504.04435", "authors": ["Tatiana Merkulova", "Bharani Jayakumar"], "title": "Evaluation framework for Image Segmentation Algorithms", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for image\nsegmentation algorithms, encompassing naive methods, machine learning\napproaches, and deep learning techniques. We begin by introducing the\nfundamental concepts and importance of image segmentation, and the role of\ninteractive segmentation in enhancing accuracy. A detailed background theory\nsection explores various segmentation methods, including thresholding, edge\ndetection, region growing, feature extraction, random forests, support vector\nmachines, convolutional neural networks, U-Net, and Mask R-CNN. The\nimplementation and experimental setup are thoroughly described, highlighting\nthree primary approaches: algorithm assisting user, user assisting algorithm,\nand hybrid methods. Evaluation metrics such as Intersection over Union (IoU),\ncomputation time, and user interaction time are employed to measure\nperformance. A comparative analysis presents detailed results, emphasizing the\nstrengths, limitations, and trade-offs of each method. The paper concludes with\ninsights into the practical applicability of these approaches across various\nscenarios and outlines future work, focusing on expanding datasets, developing\nmore representative approaches, integrating real-time feedback, and exploring\nweakly supervised and self-supervised learning paradigms to enhance\nsegmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive\nSegmentation, Machine Learning, Deep Learning, Computer Vision", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04457", "pdf": "https://arxiv.org/pdf/2504.04457", "abs": "https://arxiv.org/abs/2504.04457", "authors": ["Alejandro Fontan", "Tobias Fischer", "Javier Civera", "Michael Milford"], "title": "VSLAM-LAB: A Comprehensive Framework for Visual SLAM Methods and Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Visual Simultaneous Localization and Mapping (VSLAM) research faces\nsignificant challenges due to fragmented toolchains, complex system\nconfigurations, and inconsistent evaluation methodologies. To address these\nissues, we present VSLAM-LAB, a unified framework designed to streamline the\ndevelopment, evaluation, and deployment of VSLAM systems. VSLAM-LAB simplifies\nthe entire workflow by enabling seamless compilation and configuration of VSLAM\nalgorithms, automated dataset downloading and preprocessing, and standardized\nexperiment design, execution, and evaluation--all accessible through a single\ncommand-line interface. The framework supports a wide range of VSLAM systems\nand datasets, offering broad compatibility and extendability while promoting\nreproducibility through consistent evaluation metrics and analysis tools. By\nreducing implementation complexity and minimizing configuration overhead,\nVSLAM-LAB empowers researchers to focus on advancing VSLAM methodologies and\naccelerates progress toward scalable, real-world solutions. We demonstrate the\nease with which user-relevant benchmarks can be created: here, we introduce\ndifficulty-level-based categories, but one could envision environment-specific\nor condition-specific categories.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04640", "pdf": "https://arxiv.org/pdf/2504.04640", "abs": "https://arxiv.org/abs/2504.04640", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "title": "Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference", "categories": ["cs.CL", "cs.AI"], "comment": "Under review for COLM 2025", "summary": "Understanding how people of various demographics think, feel, and express\nthemselves (collectively called group expression) is essential for social\nscience and underlies the assessment of bias in Large Language Models (LLMs).\nWhile LLMs can effectively summarize group expression when provided with\nempirical examples, coming up with generalizable theories of how a group's\nexpression manifests in real-world text is challenging. In this paper, we\ndefine a new task called Group Theorization, in which a system must write\ntheories that differentiate expression across demographic groups. We make\navailable a large dataset on this task, Splits!, constructed by splitting\nReddit posts by neutral topics (e.g. sports, cooking, and movies) and by\ndemographics (e.g. occupation, religion, and race). Finally, we suggest a\nsimple evaluation framework for assessing how effectively a method can generate\n'better' theories about group expression, backed by human validation. We\npublicly release the raw corpora and evaluation scripts for Splits! to help\nresearchers assess how methods infer--and potentially misrepresent--group\ndifferences in expression. We make Splits! and our evaluation module available\nat https://github.com/eyloncaplan/splits.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717", "abs": "https://arxiv.org/abs/2504.04717", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "After 136 days of meticulous preparation, we're thrilled to finally\n  share our comprehensive survey on llm multi-turn interactions with the\n  community!", "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04494", "pdf": "https://arxiv.org/pdf/2504.04494", "abs": "https://arxiv.org/abs/2504.04494", "authors": ["Marin BenÄeviÄ", "Robert Å ojo", "Irena GaliÄ"], "title": "Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation of skin color measurement\nmethods from dermatoscopic images using a synthetic dataset (S-SYNTH) with\ncontrolled ground-truth melanin content, lesion shapes, hair models, and 18\ndistinct lighting conditions. This allows for rigorous assessment of the\nrobustness and invariance to lighting conditions. We assess four classes of\nimage colorimetry approaches: segmentation-based, patch-based, color\nquantization, and neural networks. We use these methods to estimate the\nIndividual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic\nimages. Our results show that segmentation-based and color quantization methods\nyield robust, lighting-invariant estimates, whereas patch-based approaches\nexhibit significant lighting-dependent biases that require calibration.\nFurthermore, neural network models, particularly when combined with heavy\nblurring to reduce overfitting, can provide light-invariant Fitzpatrick\npredictions, although their generalization to real-world images remains\nunverified. We conclude with practical recommendations for designing fair and\nreliable skin color estimation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04495", "pdf": "https://arxiv.org/pdf/2504.04495", "abs": "https://arxiv.org/abs/2504.04495", "authors": ["Peng Wu", "Wanshun Su", "Guansong Pang", "Yujia Sun", "Qingsen Yan", "Peng Wang", "Yanning Zhang"], "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection", "categories": ["cs.CV", "I.4.9; I.5.4"], "comment": "11 pages, 4 figures, 6 tables", "summary": "With the increasing adoption of video anomaly detection in intelligent\nsurveillance domains, conventional visual-based detection approaches often\nstruggle with information insufficiency and high false-positive rates in\ncomplex environments. To address these limitations, we present a novel weakly\nsupervised framework that leverages audio-visual collaboration for robust video\nanomaly detection. Capitalizing on the exceptional cross-modal representation\nlearning capabilities of Contrastive Language-Image Pretraining (CLIP) across\nvisual, audio, and textual domains, our framework introduces two major\ninnovations: an efficient audio-visual fusion that enables adaptive cross-modal\nintegration through lightweight parametric adaptation while maintaining the\nfrozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances\ntext embeddings with key multimodal information based on the semantic\ncorrelation between audio-visual features and textual labels, significantly\nimproving CLIP's generalization for the video anomaly detection task. Moreover,\nto enhance robustness against modality deficiency during inference, we further\ndevelop an uncertainty-driven feature distillation module that synthesizes\naudio-visual representations from visual-only inputs. This module employs\nuncertainty modeling based on the diversity of audio-visual features to\ndynamically emphasize challenging features during the distillation process. Our\nframework demonstrates superior performance across multiple benchmarks, with\naudio integration significantly boosting anomaly detection accuracy in various\nscenarios. Notably, with unimodal data enhanced by uncertainty-driven\ndistillation, our approach consistently outperforms current unimodal VAD\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823", "abs": "https://arxiv.org/abs/2504.04823", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04540", "pdf": "https://arxiv.org/pdf/2504.04540", "abs": "https://arxiv.org/abs/2504.04540", "authors": ["Weichen Zhang", "Ruiying Peng", "Chen Gao", "Jianjie Fang", "Xin Zeng", "Kaiyuan Li", "Ziyou Wang", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Large Language Models (LLMs) leveraging spatial information in point\nclouds for 3D spatial reasoning attract great attention. Despite some promising\nresults, the role of point clouds in 3D spatial reasoning remains\nunder-explored. In this work, we comprehensively evaluate and analyze these\nmodels to answer the research question: \\textit{Does point cloud truly boost\nthe spatial reasoning capacities of 3D LLMs?} We first evaluate the spatial\nreasoning capacity of LLMs with different input modalities by replacing the\npoint cloud with the visual and text counterparts. We then propose a novel 3D\nQA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates\nmodels' understanding of binary spatial relationships. Our findings reveal\nseveral critical insights: 1) LLMs without point input could even achieve\ncompetitive performance even in a zero-shot manner; 2) existing 3D LLMs\nstruggle to comprehend the binary spatial relationships; 3) 3D LLMs exhibit\nlimitations in exploiting the structural coordinates in point clouds for\nfine-grained spatial reasoning. We think these conclusions can help the next\nstep of 3D LLMs and also offer insights for foundation models in other\nmodalities. We release datasets and reproducible codes in the anonymous project\npage: https://3d-llm.xyz.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04549", "pdf": "https://arxiv.org/pdf/2504.04549", "abs": "https://arxiv.org/abs/2504.04549", "authors": ["Han Yuan", "Lican Kang", "Yong Li"], "title": "Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "While deep learning has exhibited remarkable predictive capabilities in\nvarious medical image tasks, its inherent black-box nature has hindered its\nwidespread implementation in real-world healthcare settings. Our objective is\nto unveil the decision-making processes of deep learning models in the context\nof glaucoma classification by employing several Class Activation Map (CAM)\ntechniques to generate model focus regions and comparing them with clinical\ndomain knowledge of the anatomical area (optic cup, optic disk, and blood\nvessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny,\nand Swin Transformer-Tiny, were developed using binary diagnostic labels of\nglaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and\nLayer-CAM) were employed to highlight the model focus area. We applied the\npaired-sample t-test to compare the percentage of anatomies in the model focus\narea to the proportion of anatomies in the entire image. After that, Pearson's\nand Spearman's correlation tests were implemented to examine the relationship\nbetween model predictive ability and the percentage of anatomical structures in\nthe model focus area. On five public glaucoma datasets, all deep learning\nmodels consistently displayed statistically significantly higher percentages of\nanatomical structures in the focus area than the proportions of anatomical\nstructures in the entire image. Also, we validated the positive relationship\nbetween the percentage of anatomical structures in the focus area and model\npredictive performance. Our study provides evidence of the convergence of\ndecision logic between deep neural networks and human clinicians through\nrigorous statistical tests. We anticipate that it can help alleviate\nclinicians' concerns regarding the trustworthiness of deep learning in\nhealthcare. For reproducibility, the code and dataset have been released at\nGitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04915", "pdf": "https://arxiv.org/pdf/2504.04915", "abs": "https://arxiv.org/abs/2504.04915", "authors": ["Ran Xu", "Wenqi Shi", "Yuchen Zhuang", "Yue Yu", "Joyce C. Ho", "Haoyu Wang", "Carl Yang"], "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/", "summary": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04566", "pdf": "https://arxiv.org/pdf/2504.04566", "abs": "https://arxiv.org/abs/2504.04566", "authors": ["Maregu Assefa", "Muzammal Naseer", "Iyyakutti Iyappan Ganapathi", "Syed Sadaf Ali", "Mohamed L Seghier", "Naoufel Werghi"], "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Semi-supervised learning in medical image segmentation leverages unlabeled\ndata to reduce annotation burdens through consistency learning. However,\ncurrent methods struggle with class imbalance and high uncertainty from\npathology variations, leading to inaccurate segmentation in 3D medical images.\nTo address these challenges, we present DyCON, a Dynamic Uncertainty-aware\nConsistency and Contrastive Learning framework that enhances the generalization\nof consistency methods with two complementary losses: Uncertainty-aware\nConsistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL\nenforces global consistency by dynamically weighting the contribution of each\nvoxel to the consistency loss based on its uncertainty, preserving\nhigh-uncertainty regions instead of filtering them out. Initially, UnCL\nprioritizes learning from uncertain voxels with lower penalties, encouraging\nthe model to explore challenging regions. As training progress, the penalty\nshift towards confident voxels to refine predictions and ensure global\nconsistency. Meanwhile, FeCL enhances local feature discrimination in\nimbalanced regions by introducing dual focal mechanisms and adaptive confidence\nadjustments into the contrastive principle. These mechanisms jointly\nprioritizes hard positives and negatives while focusing on uncertain sample\npairs, effectively capturing subtle lesion variations under class imbalance.\nExtensive evaluations on four diverse medical image segmentation datasets\n(ISLES'22, BraTS'19, LA, Pancreas) show DyCON's superior performance against\nSOTA methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04953", "pdf": "https://arxiv.org/pdf/2504.04953", "abs": "https://arxiv.org/abs/2504.04953", "authors": ["JosÃ© Pombal", "Dongkeun Yoon", "Patrick Fernandes", "Ian Wu", "Seungone Kim", "Ricardo Rei", "Graham Neubig", "AndrÃ© F. T. Martins"], "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04582", "pdf": "https://arxiv.org/pdf/2504.04582", "abs": "https://arxiv.org/abs/2504.04582", "authors": ["Nicolo Resmini", "Eugenio Lomurno", "Cristian Sbrolli", "Matteo Matteucci"], "title": "Your Image Generator Is Your New Private Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative diffusion models have emerged as powerful tools to synthetically\nproduce training data, offering potential solutions to data scarcity and\nreducing labelling costs for downstream supervised deep learning applications.\nHowever, effectively leveraging text-conditioned image generation for building\nclassifier training sets requires addressing key issues: constructing\ninformative textual prompts, adapting generative models to specific domains,\nand ensuring robust performance. This paper proposes the Text-Conditioned\nKnowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines\ndynamic image captioning, parameter-efficient diffusion model fine-tuning, and\nGenerative Knowledge Distillation techniques to create synthetic datasets\ntailored for image classification. The pipeline is rigorously evaluated on ten\ndiverse image classification benchmarks. The results demonstrate that models\ntrained solely on TCKR-generated data achieve classification accuracies on par\nwith (and in several cases exceeding) models trained on real images.\nFurthermore, the evaluation reveals that these synthetic-data-trained models\nexhibit substantially enhanced privacy characteristics: their vulnerability to\nMembership Inference Attacks is significantly reduced, with the membership\ninference AUC lowered by 5.49 points on average compared to using real training\ndata, demonstrating a substantial improvement in the performance-privacy\ntrade-off. These findings indicate that high-fidelity synthetic data can\neffectively replace real data for training classifiers, yielding strong\nperformance whilst simultaneously providing improved privacy protection as a\nvaluable emergent property. The code and trained models are available in the\naccompanying open-source repository.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04631", "pdf": "https://arxiv.org/pdf/2504.04631", "abs": "https://arxiv.org/abs/2504.04631", "authors": ["Lei Wan", "Jianxin Zhao", "Andreas Wiedholz", "Manuel Bied", "Mateus Martinez de Lucena", "Abhishek Dinkar Jagtap", "Andreas Festag", "AntÃ´nio Augusto FrÃ¶hlich", "Hannan Ejaz Keen", "Alexey Vinel"], "title": "Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective", "categories": ["cs.CV"], "comment": "39 pages, 25 figures", "summary": "The effectiveness of autonomous vehicles relies on reliable perception\ncapabilities. Despite significant advancements in artificial intelligence and\nsensor fusion technologies, current single-vehicle perception systems continue\nto encounter limitations, notably visual occlusions and limited long-range\ndetection capabilities. Collaborative Perception (CP), enabled by\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has\nemerged as a promising solution to mitigate these issues and enhance the\nreliability of autonomous systems. Beyond advancements in communication, the\ncomputer vision community is increasingly focusing on improving vehicular\nperception through collaborative approaches. However, a systematic literature\nreview that thoroughly examines existing work and reduces subjective bias is\nstill lacking. Such a systematic approach helps identify research gaps,\nrecognize common trends across studies, and inform future research directions.\nIn response, this study follows the PRISMA 2020 guidelines and includes 106\npeer-reviewed articles. These publications are analyzed based on modalities,\ncollaboration schemes, and key perception tasks. Through a comparative\nanalysis, this review illustrates how different methods address practical\nissues such as pose errors, temporal latency, communication constraints, domain\nshifts, heterogeneity, and adversarial attacks. Furthermore, it critically\nexamines evaluation methodologies, highlighting a misalignment between current\nmetrics and CP's fundamental objectives. By delving into all relevant topics\nin-depth, this review offers valuable insights into challenges, opportunities,\nand risks, serving as a reference for advancing research in vehicular\ncollaborative perception.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04994", "pdf": "https://arxiv.org/pdf/2504.04994", "abs": "https://arxiv.org/abs/2504.04994", "authors": ["Ling Hu", "Yuemei Xu", "Xiaoyang Gu", "Letao Han"], "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05020", "pdf": "https://arxiv.org/pdf/2504.05020", "abs": "https://arxiv.org/abs/2504.05020", "authors": ["Charco Hui", "Yalu Wen"], "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04679", "pdf": "https://arxiv.org/pdf/2504.04679", "abs": "https://arxiv.org/abs/2504.04679", "authors": ["Wanzhou Liu", "Zhexiao Xiong", "Xinyu Li", "Nathan Jacobs"], "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10\n  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf", "summary": "Recent novel view synthesis (NVS) techniques, including Neural Radiance\nFields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene\nreconstruction with high-quality rendering and realistic detail recovery.\nEffectively removing occlusions while preserving scene details can further\nenhance the robustness and applicability of these techniques. However, existing\napproaches for object and occlusion removal predominantly rely on generative\npriors, which, despite filling the resulting holes, introduce new artifacts and\nblurriness. Moreover, existing benchmark datasets for evaluating occlusion\nremoval methods lack realistic complexity and viewpoint variations. To address\nthese issues, we introduce DeclutterSet, a novel dataset featuring diverse\nscenes with pronounced occlusions distributed across foreground, midground, and\nbackground, exhibiting substantial relative motion across viewpoints. We\nfurther introduce DeclutterNeRF, an occlusion removal method free from\ngenerative priors. DeclutterNeRF introduces joint multi-view optimization of\nlearnable camera parameters, occlusion annealing regularization, and employs an\nexplainable stochastic structural similarity loss, ensuring high-quality,\nartifact-free reconstructions from incomplete images. Experiments demonstrate\nthat DeclutterNeRF significantly outperforms state-of-the-art methods on our\nproposed DeclutterSet, establishing a strong baseline for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05185", "pdf": "https://arxiv.org/pdf/2504.05185", "abs": "https://arxiv.org/abs/2504.05185", "authors": ["Mehdi Fatemi", "Banafsheh Rafiee", "Mingjie Tang", "Kartik Talamadupula"], "title": "Concise Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228", "abs": "https://arxiv.org/abs/2504.05228", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "title": "NoveltyBench: Evaluating Creativity and Diversity in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize creativity alongside quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04756", "pdf": "https://arxiv.org/pdf/2504.04756", "abs": "https://arxiv.org/abs/2504.04756", "authors": ["Inhwan Bae", "Junoh Lee", "Hae-Gon Jeon"], "title": "Continuous Locomotive Crowd Behavior Generation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted at CVPR 2025. Project page:\n  https://ihbae.com/publication/crowdes/", "summary": "Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294", "abs": "https://arxiv.org/abs/2504.05294", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 5 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment", "reward hacking"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04780", "pdf": "https://arxiv.org/pdf/2504.04780", "abs": "https://arxiv.org/abs/2504.04780", "authors": ["Chenxi Zhao", "Daochang Wang", "Siqian Zhang", "Gangyao Kuang"], "title": "Bottom-Up Scattering Information Perception Network for SAR target recognition", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning methods based synthetic aperture radar (SAR) image target\nrecognition tasks have been widely studied currently. The existing deep methods\nare insufficient to perceive and mine the scattering information of SAR images,\nresulting in performance bottlenecks and poor robustness of the algorithms. To\nthis end, this paper proposes a novel bottom-up scattering information\nperception network for more interpretable target recognition by constructing\nthe proprietary interpretation network for SAR images. Firstly, the localized\nscattering perceptron is proposed to replace the backbone feature extractor\nbased on CNN networks to deeply mine the underlying scattering information of\nthe target. Then, an unsupervised scattering part feature extraction model is\nproposed to robustly characterize the target scattering part information and\nprovide fine-grained target representation. Finally, by aggregating the\nknowledge of target parts to form the complete target description, the\ninterpretability and discriminative ability of the model is improved. We\nperform experiments on the FAST-Vehicle dataset and the SAR-ACD dataset to\nvalidate the performance of the proposed method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04781", "pdf": "https://arxiv.org/pdf/2504.04781", "abs": "https://arxiv.org/abs/2504.04781", "authors": ["Chaoyi Wang", "Baoqing Li", "Xinhan Di"], "title": "OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "This work has been accepted to the Multimodal Algorithmic Reasoning\n  (MAR) Workshop at CVPR 2025", "summary": "Comprehending occluded objects are not well studied in existing large-scale\nvisual-language multi-modal models. Current state-of-the-art multi-modal large\nmodels struggles to provide satisfactory results in understanding occluded\nobjects through universal visual encoders and supervised learning strategies.\nTherefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language\nframework that integrates 3D-aware supervision and Chain-of-Thoughts guidance.\nParticularly, (1) we build a multi-modal large vision-language model framework\nwhich is consisted of a large multi-modal vision-language model and a 3D\nreconstruction expert model. (2) the corresponding multi-modal\nChain-of-Thoughts is learned through a combination of supervised and\nreinforcement training strategies, allowing the multi-modal vision-language\nmodel to enhance the recognition ability with learned multi-modal\nchain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts\nreasoning dataset, consisting of $110k$ samples of occluded objects held in\nhand, is built. In the evaluation, the proposed methods demonstrate decision\nscore improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70%\nfor two settings of a variety of state-of-the-art models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03748", "pdf": "https://arxiv.org/pdf/2504.03748", "abs": "https://arxiv.org/abs/2504.03748", "authors": ["Kaiyuan Hou", "Minghui Zhao", "Lilin Xu", "Yuang Fan", "Xiaofan Jiang"], "title": "TDBench: Benchmarking Vision-Language Models in Understanding Top-Down Images", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid emergence of Vision-Language Models (VLMs) has significantly\nadvanced multimodal understanding, enabling applications in scene comprehension\nand visual reasoning. While these models have been primarily evaluated and\ndeveloped for front-view image understanding, their capabilities in\ninterpreting top-down images have received limited attention, partly due to the\nscarcity of diverse top-down datasets and the challenges in collecting such\ndata. In contrast, top-down vision provides explicit spatial overviews and\nimproved contextual understanding of scenes, making it particularly valuable\nfor tasks like autonomous navigation, aerial imaging, and spatial planning. In\nthis work, we address this gap by introducing TDBench, a comprehensive\nbenchmark for VLMs in top-down image understanding. TDBench is constructed from\npublic top-down view datasets and high-quality simulated images, including\ndiverse real-world and synthetic scenarios. TDBench consists of visual\nquestion-answer pairs across ten evaluation dimensions of image understanding.\nMoreover, we conduct four case studies that commonly happen in real-world\nscenarios but are less explored. By revealing the strengths and limitations of\nexisting VLM through evaluation results, we hope TDBench to provide insights\nfor motivating future research. Project homepage:\nhttps://github.com/Columbia-ICSL/TDBench", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04801", "pdf": "https://arxiv.org/pdf/2504.04801", "abs": "https://arxiv.org/abs/2504.04801", "authors": ["Jinhong Wang", "Shuo Tong", "Jian liu", "Dongqi Tang", "Weiqiang Wang", "Wentong Li", "Hongxia Xu", "Danny Chen", "Jintai Chen", "Jian Wu"], "title": "OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable progress of multimodal large language models (MLLMs),\nthey continue to face challenges in achieving competitive performance on\nordinal regression (OR; a.k.a. ordinal classification). To address this issue,\nthis paper presents OrderChain, a novel and general prompting paradigm that\nimproves the ordinal understanding ability of MLLMs by specificity and\ncommonality modeling. Specifically, our OrderChain consists of a set of\ntask-aware prompts to facilitate the specificity modeling of diverse OR tasks\nand a new range optimization Chain-of-Thought (RO-CoT), which learns a\ncommonality way of thinking about OR tasks by uniformly decomposing them into\nmultiple small-range optimization subtasks. Further, we propose a category\nrecursive division (CRD) method to generate instruction candidate category\nprompts to support RO-CoT automatic optimization. Comprehensive experiments\nshow that a Large Language and Vision Assistant (LLaVA) model with our\nOrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g.,\nfrom 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and\nfrom 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably,\nLLaVA with our OrderChain also remarkably outperforms state-of-the-art methods\nby 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best\nknowledge, our OrderChain is the first work that augments MLLMs for OR tasks,\nand the effectiveness is witnessed across a spectrum of OR datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03814", "pdf": "https://arxiv.org/pdf/2504.03814", "abs": "https://arxiv.org/abs/2504.03814", "authors": ["Grgur KovaÄ", "JÃ©rÃ©my Perez", "RÃ©my Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04030", "pdf": "https://arxiv.org/pdf/2504.04030", "abs": "https://arxiv.org/abs/2504.04030", "authors": ["Wasi Uddin Ahmad", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Vahid Noroozi", "Somshubra Majumdar", "Boris Ginsburg"], "title": "OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have transformed software development by\nenabling code generation, automated debugging, and complex reasoning. However,\ntheir continued advancement is constrained by the scarcity of high-quality,\npublicly available supervised fine-tuning (SFT) datasets tailored for coding\ntasks. To bridge this gap, we introduce OpenCodeInstruct, the largest\nopen-access instruction tuning dataset, comprising 5 million diverse samples.\nEach sample includes a programming question, solution, test cases, execution\nfeedback, and LLM-generated quality assessments. We fine-tune various base\nmodels, including LLaMA and Qwen, across multiple scales (1B+, 3B+, and 7B+)\nusing our dataset. Comprehensive evaluations on popular benchmarks (HumanEval,\nMBPP, LiveCodeBench, and BigCodeBench) demonstrate substantial performance\nimprovements achieved by SFT with OpenCodeInstruct. We also present a detailed\nmethodology encompassing seed data curation, synthetic instruction and solution\ngeneration, and filtering.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04835", "pdf": "https://arxiv.org/pdf/2504.04835", "abs": "https://arxiv.org/abs/2504.04835", "authors": ["Shanshan Wang", "Haixiang Xu", "Hui Feng", "Xiaoqian Wang", "Pei Song", "Sijie Liu", "Jianhua He"], "title": "Inland Waterway Object Detection in Multi-environment: Dataset and Approach", "categories": ["cs.CV"], "comment": "37 pages,11 figures,5 tables", "summary": "The success of deep learning in intelligent ship visual perception relies\nheavily on rich image data. However, dedicated datasets for inland waterway\nvessels remain scarce, limiting the adaptability of visual perception systems\nin complex environments. Inland waterways, characterized by narrow channels,\nvariable weather, and urban interference, pose significant challenges to object\ndetection systems based on existing datasets. To address these issues, this\npaper introduces the Multi-environment Inland Waterway Vessel Dataset (MEIWVD),\ncomprising 32,478 high-quality images from diverse scenarios, including sunny,\nrainy, foggy, and artificial lighting conditions. MEIWVD covers common vessel\ntypes in the Yangtze River Basin, emphasizing diversity, sample independence,\nenvironmental complexity, and multi-scale characteristics, making it a robust\nbenchmark for vessel detection. Leveraging MEIWVD, this paper proposes a\nscene-guided image enhancement module to improve water surface images based on\nenvironmental conditions adaptively. Additionally, a parameter-limited dilated\nconvolution enhances the representation of vessel features, while a multi-scale\ndilated residual fusion method integrates multi-scale features for better\ndetection. Experiments show that MEIWVD provides a more rigorous benchmark for\nobject detection algorithms, and the proposed methods significantly improve\ndetector performance, especially in complex multi-environment scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04351", "pdf": "https://arxiv.org/pdf/2504.04351", "abs": "https://arxiv.org/abs/2504.04351", "authors": ["Jinyang Li", "Sangwon Hyun", "M. Ali Babar"], "title": "DDPT: Diffusion-Driven Prompt Tuning for Large Language Model Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICSE CAIN 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation. However, the quality of the generated code is heavily\ndependent on the structure and composition of the prompts used. Crafting\nhigh-quality prompts is a challenging task that requires significant knowledge\nand skills of prompt engineering. To advance the automation support for the\nprompt engineering for LLM-based code generation, we propose a novel solution\nDiffusion-Driven Prompt Tuning (DDPT) that learns how to generate optimal\nprompt embedding from Gaussian Noise to automate the prompt engineering for\ncode generation. We evaluate the feasibility of diffusion-based optimization\nand abstract the optimal prompt embedding as a directional vector toward the\noptimal embedding. We use the code generation loss given by the LLMs to help\nthe diffusion model capture the distribution of optimal prompt embedding during\ntraining. The trained diffusion model can build a path from the noise\ndistribution to the optimal distribution at the sampling phrase, the evaluation\nresult demonstrates that DDPT helps improve the prompt optimization for code\ngeneration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "code generation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04893", "pdf": "https://arxiv.org/pdf/2504.04893", "abs": "https://arxiv.org/abs/2504.04893", "authors": ["Justus Westerhoff", "Erblina Purellku", "Jakob Hackstein", "Leo Pinetzki", "Lorenz Hufe"], "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2", "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04596", "pdf": "https://arxiv.org/pdf/2504.04596", "abs": "https://arxiv.org/abs/2504.04596", "authors": ["Noga Ben Yoash", "Meni Brief", "Oded Ovadia", "Gil Shenderovitz", "Moshik Mishaeli", "Rachel Lemberg", "Eitam Sheetrit"], "title": "SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Benchmark available at:\n  https://huggingface.co/datasets/nogabenyoash/SecQue", "summary": "We introduce SECQUE, a comprehensive benchmark for evaluating large language\nmodels (LLMs) in financial analysis tasks. SECQUE comprises 565 expert-written\nquestions covering SEC filings analysis across four key categories: comparison\nanalysis, ratio calculation, risk assessment, and financial insight generation.\nTo assess model performance, we develop SECQUE-Judge, an evaluation mechanism\nleveraging multiple LLM-based judges, which demonstrates strong alignment with\nhuman evaluations. Additionally, we provide an extensive analysis of various\nmodels' performance on our benchmark. By making SECQUE publicly available, we\naim to facilitate further research and advancements in financial AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04935", "pdf": "https://arxiv.org/pdf/2504.04935", "abs": "https://arxiv.org/abs/2504.04935", "authors": ["Peng Liu", "Heng-Chao Li", "Sen Lei", "Nanqing Liu", "Bin Feng", "Xiao Wu"], "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crowd counting, which is a key computer vision task, has emerged as a\nfundamental technology in crowd analysis and public safety management. However,\nchallenges such as scale variations and complex backgrounds significantly\nimpact the accuracy of crowd counting. To mitigate these issues, this paper\nproposes a robust Transformer-based crowd counting network, termed RCCFormer,\nspecifically designed for background suppression and scale awareness. The\nproposed method incorporates a Multi-level Feature Fusion Module (MFFM), which\nmeticulously integrates features extracted at diverse stages of the backbone\narchitecture. It establishes a strong baseline capable of capturing intricate\nand comprehensive feature representations, surpassing traditional baselines.\nFurthermore, the introduced Detail-Embedded Attention Block (DEAB) captures\ncontextual information and local details through global self-attention and\nlocal attention along with a learnable manner for efficient fusion. This\nenhances the model's ability to focus on foreground regions while effectively\nmitigating background noise interference. Additionally, we develop an Adaptive\nScale-Aware Module (ASAM), with our novel Input-dependent Deformable\nConvolution (IDConv) as its fundamental building block. This module dynamically\nadapts to changes in head target shapes and scales, significantly improving the\nnetwork's capability to accommodate large-scale variations. The effectiveness\nof the proposed method is validated on the ShanghaiTech Part_A and Part_B,\nNWPU-Crowd, and QNRF datasets. The results demonstrate that our RCCFormer\nachieves excellent performance across all four datasets, showcasing\nstate-of-the-art outcomes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04988", "pdf": "https://arxiv.org/pdf/2504.04988", "abs": "https://arxiv.org/abs/2504.04988", "authors": ["Congcong Wen", "Yiting Lin", "Xiaokang Qu", "Nan Li", "Yong Liao", "Hui Lin", "Xiang Li"], "title": "RS-RAG: Bridging Remote Sensing Imagery and Comprehensive Knowledge with a Multi-Modal Dataset and Retrieval-Augmented Generation Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in VLMs has demonstrated impressive capabilities across a\nvariety of tasks in the natural image domain. Motivated by these advancements,\nthe remote sensing community has begun to adopt VLMs for remote sensing\nvision-language tasks, including scene understanding, image captioning, and\nvisual question answering. However, existing remote sensing VLMs typically rely\non closed-set scene understanding and focus on generic scene descriptions, yet\nlack the ability to incorporate external knowledge. This limitation hinders\ntheir capacity for semantic reasoning over complex or context-dependent queries\nthat involve domain-specific or world knowledge. To address these challenges,\nwe first introduced a multimodal Remote Sensing World Knowledge (RSWK) dataset,\nwhich comprises high-resolution satellite imagery and detailed textual\ndescriptions for 14,141 well-known landmarks from 175 countries, integrating\nboth remote sensing domain knowledge and broader world knowledge. Building upon\nthis dataset, we proposed a novel Remote Sensing Retrieval-Augmented Generation\n(RS-RAG) framework, which consists of two key components. The Multi-Modal\nKnowledge Vector Database Construction module encodes remote sensing imagery\nand associated textual knowledge into a unified vector space. The Knowledge\nRetrieval and Response Generation module retrieves and re-ranks relevant\nknowledge based on image and/or text queries, and incorporates the retrieved\ncontent into a knowledge-augmented prompt to guide the VLM in producing\ncontextually grounded responses. We validated the effectiveness of our approach\non three representative vision-language tasks, including image captioning,\nimage classification, and visual question answering, where RS-RAG significantly\noutperformed state-of-the-art baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05046", "pdf": "https://arxiv.org/pdf/2504.05046", "abs": "https://arxiv.org/abs/2504.05046", "authors": ["Shenghao Ren", "Yi Lu", "Jiayi Huang", "Jiayi Zhao", "He Zhang", "Tao Yu", "Qiu Shen", "Xun Cao"], "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Existing human Motion Capture (MoCap) methods mostly focus on the visual\nsimilarity while neglecting the physical plausibility. As a result, downstream\ntasks such as driving virtual human in 3D scene or humanoid robots in real\nworld suffer from issues such as timing drift and jitter, spatial problems like\nsliding and penetration, and poor global trajectory accuracy. In this paper, we\nrevisit human MoCap from the perspective of interaction between human body and\nphysical world by exploring the role of pressure. Firstly, we construct a\nlarge-scale human Motion capture dataset with Pressure, RGB and Optical sensors\n(named MotionPRO), which comprises 70 volunteers performing 400 types of\nmotion, encompassing a total of 12.4M pose frames. Secondly, we examine both\nthe necessity and effectiveness of the pressure signal through two challenging\ntasks: (1) pose and trajectory estimation based solely on pressure: We propose\na network that incorporates a small kernel decoder and a long-short-term\nattention module, and proof that pressure could provide accurate global\ntrajectory and plausible lower body pose. (2) pose and trajectory estimation by\nfusing pressure and RGB: We impose constraints on orthographic similarity along\nthe camera axis and whole-body contact along the vertical axis to enhance the\ncross-attention strategy to fuse pressure and RGB feature maps. Experiments\ndemonstrate that fusing pressure with RGB features not only significantly\nimproves performance in terms of objective metrics, but also plausibly drives\nvirtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that\nincorporating physical perception enables humanoid robots to perform more\nprecise and stable actions, which is highly beneficial for the development of\nembodied artificial intelligence. Project page is available at:\nhttps://nju-cite-mocaphumanoid.github.io/MotionPRO/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05220", "pdf": "https://arxiv.org/pdf/2504.05220", "abs": "https://arxiv.org/abs/2504.05220", "authors": ["Hengran Zhang", "Minghao Tang", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "12 pages, 4 figures", "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05258", "pdf": "https://arxiv.org/pdf/2504.05258", "abs": "https://arxiv.org/abs/2504.05258", "authors": ["AdriÃ¡n Bazaga", "Rexhina Blloshmi", "Bill Byrne", "AdriÃ  de Gispert"], "title": "Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating\ncoherent text, understanding context, and performing reasoning tasks. However,\nthey struggle with temporal reasoning, which requires processing time-related\ninformation such as event sequencing, durations, and inter-temporal\nrelationships. These capabilities are critical for applications including\nquestion answering, scheduling, and historical analysis. In this paper, we\nintroduce TISER, a novel framework that enhances the temporal reasoning\nabilities of LLMs through a multi-stage process that combines timeline\nconstruction with iterative self-reflection. Our approach leverages test-time\nscaling to extend the length of reasoning traces, enabling models to capture\ncomplex temporal dependencies more effectively. This strategy not only boosts\nreasoning accuracy but also improves the traceability of the inference process.\nExperimental results demonstrate state-of-the-art performance across multiple\nbenchmarks, including out-of-distribution test sets, and reveal that TISER\nenables smaller open-source models to surpass larger closed-weight models on\nchallenging temporal reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288", "abs": "https://arxiv.org/abs/2504.05288", "authors": ["Mingyang Fu", "Yuyang Peng", "Benlin Liu", "Yao Wan", "Dongping Chen"], "title": "LiveVQA: Live Visual Knowledge Seeking", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05148", "pdf": "https://arxiv.org/pdf/2504.05148", "abs": "https://arxiv.org/abs/2504.05148", "authors": ["Yasuhiro Yao", "Ryoichi Ishikawa", "Takeshi Oishi"], "title": "Stereo-LiDAR Fusion by Semi-Global Matching With Discrete Disparity-Matching Cost and Semidensification", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 8 figures, 7 tables", "summary": "We present a real-time, non-learning depth estimation method that fuses Light\nDetection and Ranging (LiDAR) data with stereo camera input. Our approach\ncomprises three key techniques: Semi-Global Matching (SGM) stereo with Discrete\nDisparity-matching Cost (DDC), semidensification of LiDAR disparity, and a\nconsistency check that combines stereo images and LiDAR data. Each of these\ncomponents is designed for parallelization on a GPU to realize real-time\nperformance. When it was evaluated on the KITTI dataset, the proposed method\nachieved an error rate of 2.79\\%, outperforming the previous state-of-the-art\nreal-time stereo-LiDAR fusion method, which had an error rate of 3.05\\%.\nFurthermore, we tested the proposed method in various scenarios, including\ndifferent LiDAR point densities, varying weather conditions, and indoor\nenvironments, to demonstrate its high adaptability. We believe that the\nreal-time and non-learning nature of our method makes it highly practical for\napplications in robotics and automation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05184", "pdf": "https://arxiv.org/pdf/2504.05184", "abs": "https://arxiv.org/abs/2504.05184", "authors": ["Rayan Merghani Ahmed", "Adnan Iltaf", "Bin Li", "Shoujun Zhou"], "title": "MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "The accurate segmentation of coronary Digital Subtraction Angiography (DSA)\nimages is essential for diagnosing and treating coronary artery diseases.\nDespite advances in deep learning-based segmentation, challenges such as low\ncontrast, noise, overlapping structures, high intra-class variance, and class\nimbalance limit precise vessel delineation. To overcome these limitations, we\npropose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture\nfor coronary DSA image segmentation. The framework combined Multi-Scale Dilated\nBottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),\nwhich not only enhances multi-scale feature extraction but also preserve\nfine-grained details, and improve contextual understanding. Furthermore, we\npropose a new Supervised Prototypical Contrastive Loss (SPCL), which combines\nsupervised and prototypical contrastive learning to minimize class imbalance\nand high intra-class variance by focusing on hard-to-classified background\nsamples. Experiments carried out on a private coronary DSA dataset demonstrate\nthat MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice\ncoefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average\nSurface Distance (ASD) and Average Contour Distance (ACD). The developed\nframework provides clinicians with precise vessel segmentation, enabling\naccurate identification of coronary stenosis and supporting informed diagnostic\nand therapeutic decisions. The code will be released at the following GitHub\nprofile link https://github.com/rayanmerghani/MSA-UNet3plus.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05238", "pdf": "https://arxiv.org/pdf/2504.05238", "abs": "https://arxiv.org/abs/2504.05238", "authors": ["Zhekai Zhou", "Guibo Luo", "Mingzhi Chen", "Zhenyu Weng", "Yuesheng Zhu"], "title": "Federated Learning for Medical Image Classification: A Comprehensive Benchmark", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "The federated learning paradigm is wellsuited for the field of medical image\nanalysis, as it can effectively cope with machine learning on isolated\nmulticenter data while protecting the privacy of participating parties.\nHowever, current research on optimization algorithms in federated learning\noften focuses on limited datasets and scenarios, primarily centered around\nnatural images, with insufficient comparative experiments in medical contexts.\nIn this work, we conduct a comprehensive evaluation of several state-of-the-art\nfederated learning algorithms in the context of medical imaging. We conduct a\nfair comparison of classification models trained using various federated\nlearning algorithms across multiple medical imaging datasets. Additionally, we\nevaluate system performance metrics, such as communication cost and\ncomputational efficiency, while considering different federated learning\narchitectures. Our findings show that medical imaging datasets pose substantial\nchallenges for current federated learning optimization algorithms. No single\nalgorithm consistently delivers optimal performance across all medical\nfederated learning scenarios, and many optimization algorithms may underperform\nwhen applied to these datasets. Our experiments provide a benchmark and\nguidance for future research and application of federated learning in medical\nimaging contexts. Furthermore, we propose an efficient and robust method that\ncombines generative techniques using denoising diffusion probabilistic models\nwith label smoothing to augment datasets, widely enhancing the performance of\nfederated learning on classification tasks across various medical imaging\ndatasets. Our code will be released on GitHub, offering a reliable and\ncomprehensive benchmark for future federated learning studies in medical\nimaging.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05249", "pdf": "https://arxiv.org/pdf/2504.05249", "abs": "https://arxiv.org/abs/2504.05249", "authors": ["Wenzhao Tang", "Weihang Li", "Xiucheng Liang", "Olaf Wysocki", "Filip Biljecki", "Christoph Holst", "Boris Jutzi"], "title": "Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for CVPRW '25", "summary": "Despite recent advancements in surface reconstruction, Level of Detail (LoD)\n3 building reconstruction remains an unresolved challenge. The main issue\npertains to the object-oriented modelling paradigm, which requires\ngeoreferencing, watertight geometry, facade semantics, and low-poly\nrepresentation -- Contrasting unstructured mesh-oriented models. In\nTexture2LoD3, we introduce a novel method leveraging the ubiquity of 3D\nbuilding model priors and panoramic street-level images, enabling the\nreconstruction of LoD3 building models. We observe that prior low-detail\nbuilding models can serve as valid planar targets for ortho-rectifying\nstreet-level panoramic images. Moreover, deploying segmentation on accurately\ntextured low-level building surfaces supports maintaining essential\ngeoreferencing, watertight geometry, and low-poly representation for LoD3\nreconstruction. In the absence of LoD3 validation data, we additionally\nintroduce the ReLoD3 dataset, on which we experimentally demonstrate that our\nmethod leads to improved facade segmentation accuracy by 11% and can replace\ncostly manual projections. We believe that Texture2LoD3 can scale the adoption\nof LoD3 models, opening applications in estimating building solar potential or\nenhancing autonomous driving simulations. The project website, code, and data\nare available here: https://wenzhaotang.github.io/Texture2LoD3/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05253", "pdf": "https://arxiv.org/pdf/2504.05253", "abs": "https://arxiv.org/abs/2504.05253", "authors": ["Ben Lonnqvist", "Elsa Scialom", "Abdulkadir Gokce", "Zehra Merchant", "Michael H. Herzog", "Martin Schrimpf"], "title": "Contour Integration Underlies Human-Like Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite the tremendous success of deep learning in computer vision, models\nstill fall behind humans in generalizing to new input distributions. Existing\nbenchmarks do not investigate the specific failure points of models by\nanalyzing performance under many controlled conditions. Our study\nsystematically dissects where and why models struggle with contour integration\n-- a hallmark of human vision -- by designing an experiment that tests object\nrecognition under various levels of object fragmentation. Humans (n=50) perform\nat high accuracy, even with few object contours present. This is in contrast to\nmodels which exhibit substantially lower sensitivity to increasing object\ncontours, with most of the over 1,000 models we tested barely performing above\nchance. Only at very large scales ($\\sim5B$ training dataset size) do models\nbegin to approach human performance. Importantly, humans exhibit an integration\nbias -- a preference towards recognizing objects made up of directional\nfragments over directionless fragments. We find that not only do models that\nshare this property perform better at our task, but that this bias also\nincreases with model training dataset size, and training models to exhibit\ncontour integration leads to high shape bias. Taken together, our results\nsuggest that contour integration is a hallmark of object vision that underlies\nobject recognition performance, and may be a mechanism learned from data at\nscale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05265", "pdf": "https://arxiv.org/pdf/2504.05265", "abs": "https://arxiv.org/abs/2504.05265", "authors": ["German Barquero", "Nadine Bertsch", "Manojkumar Marramreddy", "Carlos ChacÃ³n", "Filippo Arcadu", "Ferran Rigual", "Nicky Sijia He", "Cristina Palmero", "Sergio Escalera", "Yuting Ye", "Robin Kips"], "title": "From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models", "categories": ["cs.CV"], "comment": "Published in CVPR'25. Webpage: https://barquerogerman.github.io/RPM/", "summary": "In extended reality (XR), generating full-body motion of the users is\nimportant to understand their actions, drive their virtual avatars for social\ninteraction, and convey a realistic sense of presence. While prior works\nfocused on spatially sparse and always-on input signals from motion\ncontrollers, many XR applications opt for vision-based hand tracking for\nreduced user friction and better immersion. Compared to controllers, hand\ntracking signals are less accurate and can even be missing for an extended\nperiod of time. To handle such unreliable inputs, we present Rolling Prediction\nModel (RPM), an online and real-time approach that generates smooth full-body\nmotion from temporally and spatially sparse input signals. Our model generates\n1) accurate motion that matches the inputs (i.e., tracking mode) and 2)\nplausible motion when inputs are missing (i.e., synthesis mode). More\nimportantly, RPM generates seamless transitions from tracking to synthesis, and\nvice versa. To demonstrate the practical importance of handling noisy and\nmissing inputs, we present GORP, the first dataset of realistic sparse inputs\nfrom a commercial virtual reality (VR) headset with paired high quality body\nmotion ground truth. GORP provides >14 hours of VR gameplay data from 28 people\nusing motion controllers (spatially sparse) and hand tracking (spatially and\ntemporally sparse). We benchmark RPM against the state of the art on both\nsynthetic data and GORP to highlight how we can bridge the gap for real-world\napplications with a realistic dataset and by handling unreliable input signals.\nOur code, pretrained models, and GORP dataset are available in the project\nwebpage.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05271", "pdf": "https://arxiv.org/pdf/2504.05271", "abs": "https://arxiv.org/abs/2504.05271", "authors": ["Yusef Ahsini", "Marc Escoto", "J. Alberto Conejero"], "title": "AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data", "categories": ["cs.CV", "cs.LG"], "comment": "20 pages, 9 figures", "summary": "Anomalous diffusion occurs in a wide range of systems, including protein\ntransport within cells, animal movement in complex habitats, pollutant\ndispersion in groundwater, and nanoparticle motion in synthetic materials.\nAccurately estimating the anomalous diffusion exponent and the diffusion\ncoefficient from the particle trajectories is essential to distinguish between\nsub-diffusive, super-diffusive, or normal diffusion regimes. These estimates\nprovide a deeper insight into the underlying dynamics of the system,\nfacilitating the identification of particle behaviors and the detection of\nchanges in diffusion states. However, analyzing short and noisy video data,\nwhich often yield incomplete and heterogeneous trajectories, poses a\nsignificant challenge for traditional statistical approaches. We introduce a\ndata-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these\nissues. This approach not only infers the anomalous diffusion parameters with\nhigh accuracy but also identifies temporal transitions between different\nstates, even in the presence of noise and limited temporal resolution. Our\nmethodology demonstrated strong performance in the 2nd Anomalous Diffusion\n(AnDi) Challenge benchmark within the top submissions for video tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05288", "pdf": "https://arxiv.org/pdf/2504.05288", "abs": "https://arxiv.org/abs/2504.05288", "authors": ["Mingyang Fu", "Yuyang Peng", "Benlin Liu", "Yao Wan", "Dongping Chen"], "title": "LiveVQA: Live Visual Knowledge Seeking", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "We introduce LiveVQA, an automatically collected dataset of latest visual\nknowledge from the Internet with synthesized VQA problems. LiveVQA consists of\n3,602 single- and multi-hop visual questions from 6 news websites across 14\nnews categories, featuring high-quality image-text coherence and authentic\ninformation. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and\nQwen-2.5-VL family) demonstrates that stronger models perform better overall,\nwith advanced visual reasoning capabilities proving crucial for complex\nmulti-hop questions. Despite excellent performance on textual problems, models\nwith tools like search engines still show significant gaps when addressing\nvisual questions requiring latest visual knowledge, highlighting important\nareas for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05298", "pdf": "https://arxiv.org/pdf/2504.05298", "abs": "https://arxiv.org/abs/2504.05298", "authors": ["Karan Dalal", "Daniel Koceja", "Gashon Hussein", "Jiarui Xu", "Yue Zhao", "Youjin Song", "Shihao Han", "Ka Chun Cheung", "Jan Kautz", "Carlos Guestrin", "Tatsunori Hashimoto", "Sanmi Koyejo", "Yejin Choi", "Yu Sun", "Xiaolong Wang"], "title": "One-Minute Video Generation with Test-Time Training", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Transformers today still struggle to generate one-minute videos because\nself-attention layers are inefficient for long context. Alternatives such as\nMamba layers struggle with complex multi-scene stories because their hidden\nstates are less expressive. We experiment with Test-Time Training (TTT) layers,\nwhose hidden states themselves can be neural networks, therefore more\nexpressive. Adding TTT layers into a pre-trained Transformer enables it to\ngenerate one-minute videos from text storyboards. For proof of concept, we\ncurate a dataset based on Tom and Jerry cartoons. Compared to baselines such as\nMamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers\ngenerate much more coherent videos that tell complex stories, leading by 34 Elo\npoints in a human evaluation of 100 videos per method. Although promising,\nresults still contain artifacts, likely due to the limited capability of the\npre-trained 5B model. The efficiency of our implementation can also be\nimproved. We have only experimented with one-minute videos due to resource\nconstraints, but the approach can be extended to longer videos and more complex\nstories. Sample videos, code and annotations are available at:\nhttps://test-time-training.github.io/video-dit", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03756", "pdf": "https://arxiv.org/pdf/2504.03756", "abs": "https://arxiv.org/abs/2504.03756", "authors": ["Yu-Lin Kuo", "Yu-Chee Tseng", "Ting-Hui Chiang", "Yan-Ann Chen"], "title": "Semi-Self Representation Learning for Crowdsourced WiFi Trajectories", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by VTC2025-Spring", "summary": "WiFi fingerprint-based localization has been studied intensively. Point-based\nsolutions rely on position annotations of WiFi fingerprints. Trajectory-based\nsolutions, however, require end-position annotations of WiFi trajectories,\nwhere a WiFi trajectory is a multivariate time series of signal features. A\ntrajectory dataset is much larger than a pointwise dataset as the number of\npotential trajectories in a field may grow exponentially with respect to the\nsize of the field. This work presents a semi-self representation learning\nsolution, where a large dataset $C$ of crowdsourced unlabeled WiFi trajectories\ncan be automatically labeled by a much smaller dataset $\\tilde C$ of labeled\nWiFi trajectories. The size of $\\tilde C$ only needs to be proportional to the\nsize of the physical field, while the unlabeled $C$ could be much larger. This\nis made possible through a novel ``cut-and-flip'' augmentation scheme based on\nthe meet-in-the-middle paradigm. A two-stage learning consisting of trajectory\nembedding followed by endpoint embedding is proposed for the unlabeled $C$.\nThen the learned representations are labeled by $\\tilde C$ and connected to a\nneural-based localization network. The result, while delivering promising\naccuracy, significantly relieves the burden of human annotations for\ntrajectory-based localization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03758", "pdf": "https://arxiv.org/pdf/2504.03758", "abs": "https://arxiv.org/abs/2504.03758", "authors": ["Xuanwen Liang", "Jiayu Chen", "Eric Wai Ming Lee", "Wei Xie"], "title": "Improved visual-information-driven model for crowd simulation and its modular application", "categories": ["cs.CY", "cs.CV", "cs.GR"], "comment": null, "summary": "Data-driven crowd simulation models offer advantages in enhancing the\naccuracy and realism of simulations, and improving their generalizability is\nessential for promoting application. Current data-driven approaches are\nprimarily designed for a single scenario, with very few models validated across\nmore than two scenarios. It is still an open question to develop data-driven\ncrowd simulation models with strong generalizibility. We notice that the key to\naddressing this challenge lies in effectively and accurately capturing the core\ncommon influential features that govern pedestrians' navigation across diverse\nscenarios. Particularly, we believe that visual information is one of the most\ndominant influencing features. In light of this, this paper proposes a\ndata-driven model incorporating a refined visual information extraction method\nand exit cues to enhance generalizability. The proposed model is examined on\nfour common fundamental modules: bottleneck, corridor, corner and T-junction.\nThe evaluation results demonstrate that our model performs excellently across\nthese scenarios, aligning with pedestrian movement in real-world experiments,\nand significantly outperforms the classical knowledge-driven model.\nFurthermore, we introduce a modular approach to apply our proposed model in\ncomposite scenarios, and the results regarding trajectories and fundamental\ndiagrams indicate that our simulations closely match real-world patterns in the\ncomposite scenario. The research outcomes can provide inspiration for the\ndevelopment of data-driven crowd simulation models with high generalizability\nand advance the application of data-driven approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04066", "pdf": "https://arxiv.org/pdf/2504.04066", "abs": "https://arxiv.org/abs/2504.04066", "authors": ["Mengyuan Liu", "Yixiao Chen", "Anning Tian", "Xinmeng Wu", "Mozhi Shen", "Tianchou Gong", "Jeongkyu Lee"], "title": "Performance Analysis of Deep Learning Models for Femur Segmentation in MRI Scan", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Convolutional neural networks like U-Net excel in medical image segmentation,\nwhile attention mechanisms and KAN enhance feature extraction. Meta's SAM 2\nuses Vision Transformers for prompt-based segmentation without fine-tuning.\nHowever, biases in these models impact generalization with limited data. In\nthis study, we systematically evaluate and compare the performance of three\nCNN-based models, i.e., U-Net, Attention U-Net, and U-KAN, and one\ntransformer-based model, i.e., SAM 2 for segmenting femur bone structures in\nMRI scan. The dataset comprises 11,164 MRI scans with detailed annotations of\nfemoral regions. Performance is assessed using the Dice Similarity Coefficient,\nwhich ranges from 0.932 to 0.954. Attention U-Net achieves the highest overall\nscores, while U-KAN demonstrated superior performance in anatomical regions\nwith a smaller region of interest, leveraging its enhanced learning capacity to\nimprove segmentation accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04153", "pdf": "https://arxiv.org/pdf/2504.04153", "abs": "https://arxiv.org/abs/2504.04153", "authors": ["Yikai Wang", "Guangce Liu", "Xinzhou Wang", "Zilong Chen", "Jiafang Li", "Xin Liang", "Fuchun Sun", "Jun Zhu"], "title": "Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "Published in TPAMI 2025. Code: https://github.com/yikaiw/Vidu4D,\n  Project page: https://video4dgen.github.io", "summary": "The advancement of 4D (i.e., sequential 3D) generation opens up new\npossibilities for lifelike experiences in various applications, where users can\nexplore dynamic objects or characters from any viewpoint. Meanwhile, video\ngenerative models are receiving particular attention given their ability to\nproduce realistic and imaginative frames. These models are also observed to\nexhibit strong 3D consistency, indicating the potential to act as world\nsimulators. In this work, we present Video4DGen, a novel framework that excels\nin generating 4D representations from single or multiple generated videos as\nwell as generating 4D-guided videos. This framework is pivotal for creating\nhigh-fidelity virtual contents that maintain both spatial and temporal\ncoherence. The 4D outputs generated by Video4DGen are represented using our\nproposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping\nfunctions to transform Gaussian surfels (surface elements) from a static state\nto a dynamically warped state. We design warped-state geometric regularization\nand refinements on Gaussian surfels, to preserve the structural integrity and\nfine-grained appearance details. To perform 4D generation from multiple videos\nand capture representation across spatial, temporal, and pose dimensions, we\ndesign multi-video alignment, root pose optimization, and pose-guided frame\nsampling strategies. The leveraging of continuous warping fields also enables a\nprecise depiction of pose, motion, and deformation over per-video frames.\nFurther, to improve the overall fidelity from the observation of all camera\nposes, Video4DGen performs novel-view video generation guided by the 4D\ncontent, with the proposed confidence-filtered DGS to enhance the quality of\ngenerated sequences. With the ability of 4D and video generation, Video4DGen\noffers a powerful tool for applications in virtual reality, animation, and\nbeyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04260", "pdf": "https://arxiv.org/pdf/2504.04260", "abs": "https://arxiv.org/abs/2504.04260", "authors": ["Marimuthu Kalimuthu", "David HolzmÃ¼ller", "Mathias Niepert"], "title": "LOGLO-FNO: Efficient Learning of Local and Global Features in Fourier Neural Operators", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.geo-ph"], "comment": "Accepted for Oral Presentation at the ICLR 2025 Workshop on Machine\n  Learning Multiscale Processes (MLMP), Singapura", "summary": "Modeling high-frequency information is a critical challenge in scientific\nmachine learning. For instance, fully turbulent flow simulations of\nNavier-Stokes equations at Reynolds numbers 3500 and above can generate\nhigh-frequency signals due to swirling fluid motions caused by eddies and\nvortices. Faithfully modeling such signals using neural networks depends on\naccurately reconstructing moderate to high frequencies. However, it has been\nwell known that deep neural nets exhibit the so-called spectral bias toward\nlearning low-frequency components. Meanwhile, Fourier Neural Operators (FNOs)\nhave emerged as a popular class of data-driven models in recent years for\nsolving Partial Differential Equations (PDEs) and for surrogate modeling in\ngeneral. Although impressive results have been achieved on several PDE\nbenchmark problems, FNOs often perform poorly in learning non-dominant\nfrequencies characterized by local features. This limitation stems from the\nspectral bias inherent in neural networks and the explicit exclusion of\nhigh-frequency modes in FNOs and their variants. Therefore, to mitigate these\nissues and improve FNO's spectral learning capabilities to represent a broad\nrange of frequency components, we propose two key architectural enhancements:\n(i) a parallel branch performing local spectral convolutions (ii) a\nhigh-frequency propagation module. Moreover, we propose a novel\nfrequency-sensitive loss term based on radially binned spectral errors. This\nintroduction of a parallel branch for local convolutions reduces number of\ntrainable parameters by up to 50% while achieving the accuracy of baseline FNO\nthat relies solely on global convolutions. Experiments on three challenging PDE\nproblems in fluid mechanics and biological pattern formation, and the\nqualitative and spectral analysis of predictions show the effectiveness of our\nmethod over the state-of-the-art neural operator baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04338", "pdf": "https://arxiv.org/pdf/2504.04338", "abs": "https://arxiv.org/abs/2504.04338", "authors": ["Alexander Naumann", "Xunjiang Gu", "Tolga Dimlioglu", "Mariusz Bojarski", "Alperen Degirmenci", "Alexander Popov", "Devansh Bisla", "Marco Pavone", "Urs MÃ¼ller", "Boris Ivanovic"], "title": "Data Scaling Laws for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures, 4 tables, CVPR 2025 Workshop on Autonomous\n  Driving", "summary": "Autonomous vehicle (AV) stacks have traditionally relied on decomposed\napproaches, with separate modules handling perception, prediction, and\nplanning. However, this design introduces information loss during inter-module\ncommunication, increases computational overhead, and can lead to compounding\nerrors. To address these challenges, recent works have proposed architectures\nthat integrate all components into an end-to-end differentiable model, enabling\nholistic system optimization. This shift emphasizes data engineering over\nsoftware integration, offering the potential to enhance system performance by\nsimply scaling up training resources. In this work, we evaluate the performance\nof a simple end-to-end driving architecture on internal driving datasets\nranging in size from 16 to 8192 hours with both open-loop metrics and\nclosed-loop simulations. Specifically, we investigate how much additional\ntraining data is needed to achieve a target performance gain, e.g., a 5%\nimprovement in motion prediction accuracy. By understanding the relationship\nbetween model performance and training dataset size, we aim to provide insights\nfor data-driven decision-making in autonomous driving development.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04506", "pdf": "https://arxiv.org/pdf/2504.04506", "abs": "https://arxiv.org/abs/2504.04506", "authors": ["Netta Shafir", "Guy Hacohen", "Daphna Weinshall"], "title": "Active Learning with a Noisy Annotator", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Active Learning (AL) aims to reduce annotation costs by strategically\nselecting the most informative samples for labeling. However, most active\nlearning methods struggle in the low-budget regime where only a few labeled\nexamples are available. This issue becomes even more pronounced when annotators\nprovide noisy labels. A common AL approach for the low- and mid-budget regimes\nfocuses on maximizing the coverage of the labeled set across the entire\ndataset. We propose a novel framework called Noise-Aware Active Sampling (NAS)\nthat extends existing greedy, coverage-based active learning strategies to\nhandle noisy annotations. NAS identifies regions that remain uncovered due to\nthe selection of noisy representatives and enables resampling from these areas.\nWe introduce a simple yet effective noise filtering approach suitable for the\nlow-budget regime, which leverages the inner mechanism of NAS and can be\napplied for noise filtering before model training. On multiple computer vision\nbenchmarks, including CIFAR100 and ImageNet subsets, NAS significantly improves\nperformance for standard active learning methods across different noise types\nand rates.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04844", "pdf": "https://arxiv.org/pdf/2504.04844", "abs": "https://arxiv.org/abs/2504.04844", "authors": ["Zhicong Sun", "Jacqueline Lo", "Jinxing Hu"], "title": "Embracing Dynamics: Dynamics-aware 4D Gaussian Splatting SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "This paper is currently under reviewed for IROS 2025", "summary": "Simultaneous localization and mapping (SLAM) technology now has\nphotorealistic mapping capabilities thanks to the real-time high-fidelity\nrendering capability of 3D Gaussian splatting (3DGS). However, due to the\nstatic representation of scenes, current 3DGS-based SLAM encounters issues with\npose drift and failure to reconstruct accurate maps in dynamic environments. To\naddress this problem, we present D4DGS-SLAM, the first SLAM method based on\n4DGS map representation for dynamic environments. By incorporating the temporal\ndimension into scene representation, D4DGS-SLAM enables high-quality\nreconstruction of dynamic scenes. Utilizing the dynamics-aware InfoModule, we\ncan obtain the dynamics, visibility, and reliability of scene points, and\nfilter stable static points for tracking accordingly. When optimizing Gaussian\npoints, we apply different isotropic regularization terms to Gaussians with\nvarying dynamic characteristics. Experimental results on real-world dynamic\nscene datasets demonstrate that our method outperforms state-of-the-art\napproaches in both camera pose tracking and map quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "dimension"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05119", "pdf": "https://arxiv.org/pdf/2504.05119", "abs": "https://arxiv.org/abs/2504.05119", "authors": ["Jon GutiÃ©rrez Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "title": "Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "eess.IV"], "comment": null, "summary": "Machine learning-based embedded systems for safety-critical applications,\nsuch as aerospace and autonomous driving, must be robust to perturbations\ncaused by soft errors. As transistor geometries shrink and voltages decrease,\nmodern electronic devices become more susceptible to background radiation,\nincreasing the concern about failures produced by soft errors. The resilience\nof deep neural networks (DNNs) to these errors depends not only on target\ndevice technology but also on model structure and the numerical representation\nand arithmetic precision of their parameters. Compression techniques like\npruning and quantization, used to reduce memory footprint and computational\ncomplexity, alter both model structure and representation, affecting soft error\nrobustness. In this regard, although often overlooked, the choice of activation\nfunctions (AFs) impacts not only accuracy and trainability but also\ncompressibility and error resilience. This paper explores the use of bounded\nAFs to enhance robustness against parameter perturbations, while evaluating\ntheir effects on model accuracy, compressibility, and computational load with a\ntechnology-agnostic approach. We focus on encoder-decoder convolutional models\ndeveloped for semantic segmentation of hyperspectral images with application to\nautonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260\nSoM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03794", "pdf": "https://arxiv.org/pdf/2504.03794", "abs": "https://arxiv.org/abs/2504.03794", "authors": ["Liangwei Yang", "Yuhui Xu", "Juntao Tan", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Shelby Heinecke"], "title": "Entropy-Based Block Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03705", "pdf": "https://arxiv.org/pdf/2504.03705", "abs": "https://arxiv.org/abs/2504.03705", "authors": ["Luca Marini"], "title": "Semi-supervised learning for marine anomaly detection on board satellites", "categories": ["cs.CV"], "comment": "Master's project", "summary": "Aquatic bodies face numerous environmental threats caused by several marine\nanomalies. Marine debris can devastate habitats and endanger marine life\nthrough entanglement, while harmful algal blooms can produce toxins that\nnegatively affect marine ecosystems. Additionally, ships may discharge oil or\nengage in illegal and overfishing activities, causing further harm. These\nmarine anomalies can be identified by applying trained deep learning (DL)\nmodels on multispectral satellite imagery. Furthermore, the detection of other\nanomalies, such as clouds, could be beneficial in filtering out irrelevant\nimages. However, DL models often require a large volume of labeled data for\ntraining, which can be both costly and time-consuming, particularly for marine\nanomaly detection where expert annotation is needed. A potential solution is\nthe use of semi-supervised learning methods, which can also utilize unlabeled\ndata. In this project, we implement and study the performance of FixMatch for\nSemantic Segmentation, a semi-supervised algorithm for semantic segmentation.\nFirstly, we found that semi-supervised models perform best with a high\nconfidence threshold of 0.9 when there is a limited amount of labeled data.\nSecondly, we compare the performance of semi-supervised models with\nfully-supervised models under varying amounts of labeled data. Our findings\nsuggest that semi-supervised models outperform fully-supervised models with\nlimited labeled data, while fully-supervised models have a slightly better\nperformance with larger volumes of labeled data. We propose two hypotheses to\nexplain why fully-supervised models surpass semi-supervised ones when a high\nvolume of labeled data is used. All of our experiments were conducted using a\nU-Net model architecture with a limited number of parameters to ensure\ncompatibility with space-rated hardware.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03821", "pdf": "https://arxiv.org/pdf/2504.03821", "abs": "https://arxiv.org/abs/2504.03821", "authors": ["Andrew Kiruluta", "Andreas Lemos"], "title": "A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present a novel generative modeling framework,Wavelet-Fourier-Diffusion,\nwhich adapts the diffusion paradigm to hybrid frequency representations in\norder to synthesize high-quality, high-fidelity images with improved spatial\nlocalization. In contrast to conventional diffusion models that rely\nexclusively on additive noise in pixel space, our approach leverages a\nmulti-transform that combines wavelet sub-band decomposition with partial\nFourier steps. This strategy progressively degrades and then reconstructs\nimages in a hybrid spectral domain during the forward and reverse diffusion\nprocesses. By supplementing traditional Fourier-based analysis with the spatial\nlocalization capabilities of wavelets, our model can capture both global\nstructures and fine-grained features more effectively. We further extend the\napproach to conditional image generation by integrating embeddings or\nconditional features via cross-attention. Experimental evaluations on CIFAR-10,\nCelebA-HQ, and a conditional ImageNet subset illustrate that our method\nachieves competitive or superior performance relative to baseline diffusion\nmodels and state-of-the-art GANs, as measured by Fr\\'echet Inception Distance\n(FID) and Inception Score (IS). We also show how the hybrid frequency-based\nrepresentation improves control over global coherence and fine texture\nsynthesis, paving the way for new directions in multi-scale generative\nmodeling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03932", "pdf": "https://arxiv.org/pdf/2504.03932", "abs": "https://arxiv.org/abs/2504.03932", "authors": ["Dongsuk Jang", "Alan Li", "Arman Cohan"], "title": "YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization", "categories": ["cs.CL"], "comment": "Paper accepted at CL4HEALTH @ NAACL 2025: Annual Conference of the\n  Nations of the Americas Chapter of the Association for Computational\n  Linguistics", "summary": "Automated summarization of healthcare community question-answering forums is\nchallenging due to diverse perspectives presented across multiple user\nresponses to each question. The PerAnsSumm Shared Task was therefore proposed\nto tackle this challenge by identifying perspectives from different answers and\nthen generating a comprehensive answer to the question. In this study, we\naddress the PerAnsSumm Shared Task using two complementary paradigms: (i) a\ntraining-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct,\nand (ii) agentic approaches including zero- and few-shot prompting with\nfrontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA)\nframework that leverages a diverse set of LLMs by combining outputs from\nmulti-layer feedback aggregation. For perspective span\nidentification/classification, GPT-4o zero-shot achieves an overall score of\n0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a\n2-layer MoA configuration, we were able to improve LLaMA performance up by 28\npercent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains\nan overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our\n2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37.\nFurthermore, in few-shot setting, our results show that the\nsentence-transformer embedding-based exemplar selection provides more gain than\nmanually selected exemplars on LLaMA models, although the few-shot prompting is\nnot always helpful for GPT-4o. The YaleNLP team's approach ranked the overall\nsecond place in the shared task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03850", "pdf": "https://arxiv.org/pdf/2504.03850", "abs": "https://arxiv.org/abs/2504.03850", "authors": ["Ved Umrajkar", "Aakash Kumar Singh"], "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Tree-Ring Watermarking is a significant technique for authenticating\nAI-generated images. However, its effectiveness in rectified flow-based models\nremains unexplored, particularly given the inherent challenges of these models\nwith noise latent inversion. Through extensive experimentation, we evaluated\nand compared the detection and separability of watermarks between SD 2.1 and\nFLUX.1-dev models. By analyzing various text guidance configurations and\naugmentation attacks, we demonstrate how inversion limitations affect both\nwatermark recovery and the statistical separation between watermarked and\nunwatermarked images. Our findings provide valuable insights into the current\nlimitations of Tree-Ring Watermarking in the current SOTA models and highlight\nthe critical need for improved inversion methods to achieve reliable watermark\ndetection and separability. The official implementation, dataset release and\nall experimental results are available at this\n\\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03964", "pdf": "https://arxiv.org/pdf/2504.03964", "abs": "https://arxiv.org/abs/2504.03964", "authors": ["Simon A. Lee", "Anthony Wu", "Jeffrey N. Chiang"], "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)", "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03868", "pdf": "https://arxiv.org/pdf/2504.03868", "abs": "https://arxiv.org/abs/2504.03868", "authors": ["Ziming Liu", "Leichen Wang", "Ge Yang", "Xinrun Li", "Xingtao Hu", "Hao Sun", "Guangyu Gao"], "title": "Control Map Distribution using Map Query Bank for Online Map Generation", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require high-definition (HD) map that\ncontains detailed map information for planning and navigation. However,\npre-build HD map requires a large cost. Visual-based Online Map Generation\n(OMG) has become an alternative low-cost solution to build a local HD map.\nQuery-based BEV Transformer has been a base model for this task. This model\nlearns HD map predictions from an initial map queries distribution which is\nobtained by offline optimization on training set. Besides the quality of BEV\nfeature, the performance of this model also highly relies on the capacity of\ninitial map query distribution. However, this distribution is limited because\nthe limited query number. To make map predictions optimal on each test sample,\nit is essential to generate a suitable initial distribution for each specific\nscenario. This paper proposes to decompose the whole HD map distribution into a\nset of point representations, namely map query bank (MQBank). To build specific\nmap query initial distributions of different scenarios, low-cost standard\ndefinition map (SD map) data is introduced as a kind of prior knowledge.\nMoreover, each layer of map decoder network learns instance-level map query\nfeatures, which will lose detailed information of each point. However, BEV\nfeature map is a point-level dense feature. It is important to keep point-level\ninformation in map queries when interacting with BEV feature map. This can also\nbe solved with map query bank method. Final experiments show a new insight on\nSD map prior and a new record on OpenLaneV2 benchmark with 40.5%, 45.7% mAP on\nvehicle lane and pedestrian area.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03875", "pdf": "https://arxiv.org/pdf/2504.03875", "abs": "https://arxiv.org/abs/2504.03875", "authors": ["Wanhee Lee", "Klemen Kotar", "Rahul Mysore Venkatesh", "Jared Watrous", "Honglin Chen", "Khai Loong Aw", "Daniel L. K. Yamins"], "title": "3D Scene Understanding Through Local Random Access Sequence Modeling", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/projects/lras_3d/", "summary": "3D scene understanding from single images is a pivotal problem in computer\nvision with numerous downstream applications in graphics, augmented reality,\nand robotics. While diffusion-based modeling approaches have shown promise,\nthey often struggle to maintain object and scene consistency, especially in\ncomplex real-world scenarios. To address these limitations, we propose an\nautoregressive generative approach called Local Random Access Sequence (LRAS)\nmodeling, which uses local patch quantization and randomly ordered sequence\ngeneration. By utilizing optical flow as an intermediate representation for 3D\nscene editing, our experiments demonstrate that LRAS achieves state-of-the-art\nnovel view synthesis and 3D object manipulation capabilities. Furthermore, we\nshow that our framework naturally extends to self-supervised depth estimation\nthrough a simple modification of the sequence design. By achieving strong\nperformance on multiple 3D scene understanding tasks, LRAS provides a unified\nand effective framework for building the next generation of 3D vision models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03886", "pdf": "https://arxiv.org/pdf/2504.03886", "abs": "https://arxiv.org/abs/2504.03886", "authors": ["Jianhao Zheng", "Zihan Zhu", "Valentin Bieri", "Marc Pollefeys", "Songyou Peng", "Iro Armeni"], "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042", "abs": "https://arxiv.org/abs/2504.04042", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is a fundamental aspect of legal decision-making,\nenabling logical conclusions by connecting general legal principles with\nspecific case facts. Although existing large language models (LLMs) can\ngenerate responses to legal questions, they fail to perform explicit\nsyllogistic reasoning, often producing implicit and unstructured answers that\nlack explainability and trustworthiness. To address this limitation, we propose\nSyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic\nlegal reasoning. SyLeR integrates a tree-structured hierarchical retrieval\nmechanism to effectively combine relevant legal statutes and precedent cases,\nforming comprehensive major premises. This is followed by a two-stage\nfine-tuning process: supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning with a\nstructure-aware reward mechanism refines the ability of the model to generate\ndiverse logically sound and well-structured reasoning paths. We conducted\nextensive experiments across various dimensions, including in-domain and\ncross-domain user groups (legal laypersons and practitioners), multiple\nlanguages (Chinese and French), and different LLM backbones (legal-specific and\nopen-domain LLMs). The results show that SyLeR significantly improves response\naccuracy and consistently delivers explicit, explainable, and trustworthy legal\nreasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03948", "pdf": "https://arxiv.org/pdf/2504.03948", "abs": "https://arxiv.org/abs/2504.03948", "authors": ["Sanjoy Kundu", "Shanmukha Vellamchetti", "Sathyanarayanan N. Aakur"], "title": "ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition", "categories": ["cs.CV"], "comment": "17 pages, 6 figures, 3 tables. Under review", "summary": "Open-world egocentric activity recognition poses a fundamental challenge due\nto its unconstrained nature, requiring models to infer unseen activities from\nan expansive, partially observed search space. We introduce ProbRes, a\nProbabilistic Residual search framework based on jump-diffusion that\nefficiently navigates this space by balancing prior-guided exploration with\nlikelihood-driven exploitation. Our approach integrates structured commonsense\npriors to construct a semantically coherent search space, adaptively refines\npredictions using Vision-Language Models (VLMs) and employs a stochastic search\nmechanism to locate high-likelihood activity labels while minimizing exhaustive\nenumeration efficiently. We systematically evaluate ProbRes across multiple\nopenness levels (L0 - L3), demonstrating its adaptability to increasing search\nspace complexity. In addition to achieving state-of-the-art performance on\nbenchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we\nestablish a clear taxonomy for open-world recognition, delineating the\nchallenges and methodological advancements necessary for egocentric activity\nunderstanding. Our results highlight the importance of structured search\nstrategies, paving the way for scalable and efficient open-world activity\nrecognition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03953", "pdf": "https://arxiv.org/pdf/2504.03953", "abs": "https://arxiv.org/abs/2504.03953", "authors": ["Arash Sajjadi", "Mark Eramian"], "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68R10", "I.2.6; I.5.1; I.4.8"], "comment": "Submitted to arXiv. Code repository:\n  https://github.com/arashsajjadi/TGraphX |||\n  https://git.cs.usask.ca/arash/tgraphx", "summary": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04076", "pdf": "https://arxiv.org/pdf/2504.04076", "abs": "https://arxiv.org/abs/2504.04076", "authors": ["Bing Wang", "Bingrui Zhao", "Ximing Li", "Changchun Li", "Wanfu Gao", "Shengsheng Wang"], "title": "Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator", "categories": ["cs.CL", "cs.SI"], "comment": "11 pages, 5 figures. Accepted by SIGIR 2025. Code:\n  https://github.com/wangbing1416/CAMERED", "summary": "Over the past decade, social media platforms have been key in spreading\nrumors, leading to significant negative impacts. To counter this, the community\nhas developed various Rumor Detection (RD) algorithms to automatically identify\nthem using user comments as evidence. However, these RD methods often fail in\nthe early stages of rumor propagation when only limited user comments are\navailable, leading the community to focus on a more challenging topic named\nRumor Early Detection (RED). Typically, existing RED methods learn from limited\nsemantics in early comments. However, our preliminary experiment reveals that\nthe RED models always perform best when the number of training and test\ncomments is consistent and extensive. This inspires us to address the RED issue\nby generating more human-like comments to support this hypothesis. To implement\nthis idea, we tune a comment generator by simulating expert collaboration and\ncontroversy and propose a new RED framework named CAMERED. Specifically, we\nintegrate a mixture-of-expert structure into a generative language model and\npresent a novel routing network for expert collaboration. Additionally, we\nsynthesize a knowledgeable dataset and design an adversarial learning strategy\nto align the style of generated comments with real-world comments. We further\nintegrate generated and original comments with a mutual controversy fusion\nmodule. Experimental results show that CAMERED outperforms state-of-the-art RED\nbaseline models and generation methods, demonstrating its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04131", "pdf": "https://arxiv.org/pdf/2504.04131", "abs": "https://arxiv.org/abs/2504.04131", "authors": ["Michael J Bommarito", "Daniel Martin Katz", "Jillian Bommarito"], "title": "Precise Legal Sentence Boundary Detection for Retrieval at Scale: NUPunkt and CharBoundary", "categories": ["cs.CL"], "comment": "12 pages, 5 figures, 6 tables", "summary": "We present NUPunkt and CharBoundary, two sentence boundary detection\nlibraries optimized for high-precision, high-throughput processing of legal\ntext in large-scale applications such as due diligence, e-discovery, and legal\nresearch. These libraries address the critical challenges posed by legal\ndocuments containing specialized citations, abbreviations, and complex sentence\nstructures that confound general-purpose sentence boundary detectors.\n  Our experimental evaluation on five diverse legal datasets comprising over\n25,000 documents and 197,000 annotated sentence boundaries demonstrates that\nNUPunkt achieves 91.1% precision while processing 10 million characters per\nsecond with modest memory requirements (432 MB). CharBoundary models offer\nbalanced and adjustable precision-recall tradeoffs, with the large model\nachieving the highest F1 score (0.782) among all tested methods.\n  Notably, NUPunkt provides a 29-32% precision improvement over general-purpose\ntools while maintaining exceptional throughput, processing multi-million\ndocument collections in minutes rather than hours. Both libraries run\nefficiently on standard CPU hardware without requiring specialized\naccelerators. NUPunkt is implemented in pure Python with zero external\ndependencies, while CharBoundary relies only on scikit-learn and optional ONNX\nruntime integration for optimized performance. Both libraries are available\nunder the MIT license, can be installed via PyPI, and can be interactively\ntested at https://sentences.aleainstitute.ai/.\n  These libraries address critical precision issues in retrieval-augmented\ngeneration systems by preserving coherent legal concepts across sentences,\nwhere each percentage improvement in precision yields exponentially greater\nreductions in context fragmentation, creating cascading benefits throughout\nretrieval pipelines and significantly enhancing downstream reasoning quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04024", "pdf": "https://arxiv.org/pdf/2504.04024", "abs": "https://arxiv.org/abs/2504.04024", "authors": ["Yifan Li", "Wentao Bao", "Botao Ye", "Zhen Tan", "Tianlong Chen", "Huan Liu", "Yu Kong"], "title": "Window Token Concatenation for Efficient Visual Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "To effectively reduce the visual tokens in Visual Large Language Models\n(VLLMs), we propose a novel approach called Window Token Concatenation (WiCo).\nSpecifically, we employ a sliding window to concatenate spatially adjacent\nvisual tokens. However, directly concatenating these tokens may group diverse\ntokens into one, and thus obscure some fine details. To address this challenge,\nwe propose fine-tuning the last few layers of the vision encoder to adaptively\nadjust the visual tokens, encouraging that those within the same window exhibit\nsimilar features. To further enhance the performance on fine-grained visual\nunderstanding tasks, we introduce WiCo+, which decomposes the visual tokens in\nlater layers of the LLM. Such a design enjoys the merits of the large\nperception field of the LLM for fine-grained visual understanding while keeping\na small number of visual tokens for efficient inference. We perform extensive\nexperiments on both coarse- and fine-grained visual understanding tasks based\non LLaVA-1.5 and Shikra, showing better performance compared with existing\ntoken reduction projectors. The code is available:\nhttps://github.com/JackYFL/WiCo.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04029", "pdf": "https://arxiv.org/pdf/2504.04029", "abs": "https://arxiv.org/abs/2504.04029", "authors": ["Shintaro Shiba", "Yoshimitsu Aoki", "Guillermo Gallego"], "title": "Simultaneous Motion And Noise Estimation with Event Cameras", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 13 figures, 6 tables", "summary": "Event cameras are emerging vision sensors, whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras consider other tasks\nsuch as motion estimation separately (i.e., sequentially after denoising).\nHowever, motion is an intrinsic part of event data, since scene edges cannot be\nsensed without motion. This work proposes, to the best of our knowledge, the\nfirst method that simultaneously estimates motion in its various forms (e.g.,\nego-motion, optical flow) and noise. The method is flexible, as it allows\nreplacing the 1-step motion estimation of the widely-used Contrast Maximization\nframework with any other motion estimator, such as deep neural networks. The\nexperiments show that the proposed method achieves state-of-the-art results on\nthe E-MLB denoising benchmark and competitive results on the DND21 benchmark,\nwhile showing its efficacy on motion estimation and intensity reconstruction\ntasks. We believe that the proposed approach contributes to strengthening the\ntheory of event-data denoising, as well as impacting practical denoising\nuse-cases, as we release the code upon acceptance. Project page:\nhttps://github.com/tub-rip/ESMD", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04152", "pdf": "https://arxiv.org/pdf/2504.04152", "abs": "https://arxiv.org/abs/2504.04152", "authors": ["Zihao Li", "Shaoxiong Ji", "Hengyu Luo", "JÃ¶rg Tiedemann"], "title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant disparities in performance\nacross languages, primarily benefiting high-resource languages while\nmarginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as\na promising approach to address this imbalance, although the relative\neffectiveness of monolingual, bilingual, and code-augmented data strategies\nremains unclear. This study systematically evaluates 36 CPT configurations\ninvolving three multilingual base models, across 30+ languages categorized as\naltruistic, selfish, and stagnant, spanning various resource levels. Our\nfindings reveal three major insights: (1) Bilingual CPT improves multilingual\nclassification but often causes language mixing issues during generation. (2)\nIncluding programming code data during CPT consistently enhances multilingual\nclassification accuracy, particularly benefiting low-resource languages, but\nintroduces a trade-off by slightly degrading generation quality. (3) Contrary\nto prior work, we observe substantial deviations from language classifications\naccording to their impact on cross-lingual transfer: Languages classified as\naltruistic often negatively affect related languages, selfish languages show\nconditional and configuration-dependent behavior, and stagnant languages\ndemonstrate surprising adaptability under certain CPT conditions. These nuanced\ninteractions emphasize the complexity of multilingual representation learning,\nunderscoring the importance of systematic studies on generalizable language\nclassification to inform future multilingual CPT strategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04215", "pdf": "https://arxiv.org/pdf/2504.04215", "abs": "https://arxiv.org/abs/2504.04215", "authors": ["Vishnu Kabir Chhabra", "Mohammad Mahdi Khalili"], "title": "Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of large language models has spurred significant interest in\nmodel compression as a means to enhance their accessibility and practicality.\nWhile extensive research has explored model compression through the lens of\nsafety, findings suggest that safety-aligned models often lose elements of\ntrustworthiness post-compression. Simultaneously, the field of mechanistic\ninterpretability has gained traction, with notable discoveries, such as the\nidentification of a single direction in the residual stream mediating refusal\nbehaviors across diverse model architectures. In this work, we investigate the\nsafety of compressed models by examining the mechanisms of refusal, adopting a\nnovel interpretability-driven perspective to evaluate model safety.\nFurthermore, leveraging insights from our interpretability analysis, we propose\na lightweight, computationally efficient method to enhance the safety of\ncompressed models without compromising their performance or utility.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04216", "pdf": "https://arxiv.org/pdf/2504.04216", "abs": "https://arxiv.org/abs/2504.04216", "authors": ["Yuantao Zhang", "Zhankui Yang"], "title": "A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": "13 pages", "summary": "The rise of Large Language Models (LLMs) has brought about concerns regarding\ncopyright infringement and unethical practices in data and model usage. For\ninstance, slight modifications to existing LLMs may be used to falsely claim\nthe development of new models, leading to issues of model copying and\nviolations of ownership rights. This paper addresses these challenges by\nintroducing a novel metric for quantifying LLM similarity, which leverages\nperplexity curves and differences in Menger curvature. Comprehensive\nexperiments validate the performance of our methodology, demonstrating its\nsuperiority over baseline methods and its ability to generalize across diverse\nmodels and domains. Furthermore, we highlight the capability of our approach in\ndetecting model replication through simulations, emphasizing its potential to\npreserve the originality and integrity of LLMs. Code is available at\nhttps://github.com/zyttt-coder/LLM_similarity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04126", "pdf": "https://arxiv.org/pdf/2504.04126", "abs": "https://arxiv.org/abs/2504.04126", "authors": ["Zhenzhi Wang", "Yixuan Li", "Yanhong Zeng", "Yuwei Guo", "Dahua Lin", "Tianfan Xue", "Bo Dai"], "title": "Multi-identity Human Image Animation with Structural Video Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages", "summary": "Generating human videos from a single image while ensuring high visual\nquality and precise control is a challenging task, especially in complex\nscenarios involving multiple individuals and interactions with objects.\nExisting methods, while effective for single-human cases, often fail to handle\nthe intricacies of multi-identity interactions because they struggle to\nassociate the correct pairs of human appearance and pose condition and model\nthe distribution of 3D-aware dynamics. To address these limitations, we present\nStructural Video Diffusion, a novel framework designed for generating realistic\nmulti-human videos. Our approach introduces two core innovations:\nidentity-specific embeddings to maintain consistent appearances across\nindividuals and a structural learning mechanism that incorporates depth and\nsurface-normal cues to model human-object interactions. Additionally, we expand\nexisting human video dataset with 25K new videos featuring diverse multi-human\nand object interaction scenarios, providing a robust foundation for training.\nExperimental results demonstrate that Structural Video Diffusion achieves\nsuperior performance in generating lifelike, coherent videos for multiple\nsubjects with dynamic and rich interactions, advancing the state of\nhuman-centric video generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04292", "pdf": "https://arxiv.org/pdf/2504.04292", "abs": "https://arxiv.org/abs/2504.04292", "authors": ["Jie Yang", "Yiqiu Tang", "Yongjie Li", "Lihua Zhang", "Haoran Zhang"], "title": "Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring of Equity, Fixed Income, and Currency Markets", "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by IJCNN 2025", "summary": "Large language models (LLMs) have emerged as powerful tools in the field of\nfinance, particularly for risk management across different asset classes. In\nthis work, we introduce a Cross-Asset Risk Management framework that utilizes\nLLMs to facilitate real-time monitoring of equity, fixed income, and currency\nmarkets. This innovative approach enables dynamic risk assessment by\naggregating diverse data sources, ultimately enhancing decision-making\nprocesses. Our model effectively synthesizes and analyzes market signals to\nidentify potential risks and opportunities while providing a holistic view of\nasset classes. By employing advanced analytics, we leverage LLMs to interpret\nfinancial texts, news articles, and market reports, ensuring that risks are\ncontextualized within broader market narratives. Extensive backtesting and\nreal-time simulations validate the framework, showing increased accuracy in\npredicting market shifts compared to conventional methods. The focus on\nreal-time data integration enhances responsiveness, allowing financial\ninstitutions to manage risks adeptly under varying market conditions and\npromoting financial stability through the advanced application of LLMs in risk\nanalysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310", "abs": "https://arxiv.org/abs/2504.04310", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems-a pursuit currently limited\nby the absence of comprehensive benchmarks for systematic investigation. To\naddress this, we introduce CO-Bench, a benchmark suite featuring 36 real-world\nCO problems drawn from a broad range of domains and complexity levels. CO-Bench\nincludes structured problem formulations and curated data to support rigorous\ninvestigation of LLM agents. We evaluate multiple agent frameworks against\nestablished human-designed algorithms, revealing key strengths and limitations\nof current approaches and identifying promising directions for future research.\nCO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04190", "pdf": "https://arxiv.org/pdf/2504.04190", "abs": "https://arxiv.org/abs/2504.04190", "authors": ["Yuyang Zhang", "Baao Xie", "Hu Zhu", "Qi Wang", "Huanting Guo", "Xin Jin", "Wenjun Zeng"], "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D\nreconstruction, delivering both rapid rendering and high-quality results.\nHowever, existing 3DGS methods pose challenges in understanding underlying 3D\nsemantics, which hinders model controllability and interpretability. To address\nit, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to\ndiscover both coarse- and fine-grained 3D semantics via hierarchical\ndisentangled representation learning (DRL). Specifically, the model employs a\ndual-branch architecture, consisting of a point cloud initialization branch and\na triplane-Gaussian generation branch, to achieve coarse-grained\ndisentanglement by separating 3D geometry and visual appearance features.\nSubsequently, fine-grained semantic representations within each modality are\nfurther discovered through DRL-based encoder-adapters. To our knowledge, this\nis the first work to achieve unsupervised interpretable 3DGS. Evaluations\nindicate that our model achieves 3D disentanglement while preserving\nhigh-quality and rapid reconstruction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04335", "pdf": "https://arxiv.org/pdf/2504.04335", "abs": "https://arxiv.org/abs/2504.04335", "authors": ["Yuya Ogasa", "Yuki Arase"], "title": "Hallucination Detection using Multi-View Attention Features", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study tackles token-level hallucination detection in outputs of large\nlanguage models. Previous studies revealed that attention exhibits irregular\npatterns when hallucination occurs. Inspired by this, we extract features from\nthe attention matrix that provide complementary views of (a) the average\nattention each token receives, which helps identify whether certain tokens are\noverly influential or ignored, (b) the diversity of attention each token\nreceives, which reveals whether attention is biased toward specific subsets,\nand (c) the diversity of tokens a token attends to during generation, which\nindicates whether the model references a narrow or broad range of information.\nThese features are input to a Transformer-based classifier to conduct\ntoken-level classification to identify hallucinated spans. Experimental results\nindicate that the proposed method outperforms strong baselines on hallucination\ndetection with longer input contexts, i.e., data-to-text and summarization\ntasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04225", "pdf": "https://arxiv.org/pdf/2504.04225", "abs": "https://arxiv.org/abs/2504.04225", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images", "categories": ["cs.CV"], "comment": "31 pages", "summary": "Modern AI models excel in controlled settings but often fail in real-world\nscenarios where data distributions shift unpredictably - a challenge known as\ndomain generalisation (DG). This paper tackles this limitation by rigorously\nevaluating vision tramsformers, specifically the BEIT architecture which is a\nmodel pre-trained with masked image modelling (MIM), against synthetic\nout-of-distribution (OOD) benchmarks designed to mimic real-world noise and\nocclusions. We introduce a novel framework to generate OOD test cases by\nstrategically masking object regions in images using grid patterns (25\\%, 50\\%,\n75\\% occlusion) and leveraging cutting-edge zero-shot segmentation via Segment\nAnything and Grounding DINO to ensure precise object localisation. Experiments\nacross three benchmarks (PACS, Office-Home, DomainNet) demonstrate BEIT's known\nrobustness while maintaining 94\\% accuracy on PACS and 87\\% on Office-Home,\ndespite significant occlusions, outperforming CNNs and other vision\ntransformers by margins of up to 37\\%. Analysis of self-attention distances\nreveals that the BEIT dependence on global features correlates with its\nresilience. Furthermore, our synthetic benchmarks expose critical failure\nmodes: performance degrades sharply when occlusions disrupt object shapes e.g.\n68\\% drop for external grid masking vs. 22\\% for internal masking. This work\nprovides two key advances (1) a scalable method to generate OOD benchmarks\nusing controllable noise, and (2) empirical evidence that MIM and\nself-attention mechanism in vision transformers enhance DG by learning\ninvariant features. These insights bridge the gap between lab-trained models\nand real-world deployment that offer a blueprint for building AI systems that\ngeneralise reliably under uncertainty.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04252", "pdf": "https://arxiv.org/pdf/2504.04252", "abs": "https://arxiv.org/abs/2504.04252", "authors": ["Muhammad Osama Zeeshan", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Eric Grange"], "title": "Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Personalized facial expression recognition (FER) involves adapting a machine\nlearning model using samples from labeled sources and unlabeled target domains.\nGiven the challenges of recognizing subtle expressions with considerable\ninterpersonal variability, state-of-the-art unsupervised domain adaptation\n(UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain\ncorresponds to a specific subject, and improve model accuracy and robustness.\nHowever, when adapting to a specific target, the diverse nature of multiple\nsource domains translates to a large shift between source and target data.\nState-of-the-art MSDA methods for FER address this domain shift by considering\nall the sources to adapt to the target representations. Nevertheless, adapting\nto a target subject presents significant challenges due to large distributional\ndifferences between source and target domains, often resulting in negative\ntransfer. In addition, integrating all sources simultaneously increases\ncomputational costs and causes misalignment with the target. To address these\nissues, we propose a progressive MSDA approach that gradually introduces\ninformation from subjects based on their similarity to the target subject. This\nwill ensure that only the most relevant sources from the target are selected,\nwhich helps avoid the negative transfer caused by dissimilar sources. We first\nexploit the closest sources to reduce the distribution shift with the target\nand then move towards the furthest while only considering the most relevant\nsources based on the predetermined threshold. Furthermore, to mitigate\ncatastrophic forgetting caused by the incremental introduction of source\nsubjects, we implemented a density-based memory mechanism that preserves the\nmost relevant historical source samples for adaptation. Our experiments show\nthe effectiveness of our proposed method on pain datasets: Biovid and\nUNBC-McMaster.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04271", "pdf": "https://arxiv.org/pdf/2504.04271", "abs": "https://arxiv.org/abs/2504.04271", "authors": ["Mete Ahishali", "Anis Ur Rahman", "Einari Heinaro", "Samuli Junttila"], "title": "ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Information on standing dead trees is important for understanding forest\necosystem functioning and resilience but has been lacking over large geographic\nregions. Climate change has caused large-scale tree mortality events that can\nremain undetected due to limited data. In this study, we propose a novel method\nfor segmenting standing dead trees using aerial multispectral orthoimages.\nBecause access to annotated datasets has been a significant problem in forest\nremote sensing due to the need for forest expertise, we introduce a method for\ndomain transfer by leveraging domain adaptation to learn a transformation from\na source domain X to target domain Y. In this Image-to-Image translation task,\nwe aim to utilize available annotations in the target domain by pre-training a\nsegmentation network. When images from a new study site without annotations are\nintroduced (source domain X), these images are transformed into the target\ndomain. Then, transfer learning is applied by inferring the pre-trained network\non domain-adapted images. In addition to investigating the feasibility of\ncurrent domain adaptation approaches for this objective, we propose a novel\napproach called the Attention-guided Domain Adaptation Network (ADA-Net) with\nenhanced contrastive learning. Accordingly, the ADA-Net approach provides new\nstate-of-the-art domain adaptation performance levels outperforming existing\napproaches. We have evaluated the proposed approach using two datasets from\nFinland and the US. The USA images are converted to the Finland domain, and we\nshow that the synthetic USA2Finland dataset exhibits similar characteristics to\nthe Finland domain images. The software implementation is shared at\nhttps://github.com/meteahishali/ADA-Net. The data is publicly available at\nhttps://www.kaggle.com/datasets/meteahishali/aerial-imagery-for-standing-dead-tree-segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04323", "pdf": "https://arxiv.org/pdf/2504.04323", "abs": "https://arxiv.org/abs/2504.04323", "authors": ["Yiming Shi", "Shaoshuai Yang", "Xun Zhu", "Haoyu Wang", "Miao Li", "Ji Wu"], "title": "MedM-VL: What Makes a Good Medical LVLM?", "categories": ["cs.CV"], "comment": null, "summary": "Medical image analysis is a fundamental component. As deep learning\nprogresses, the focus has shifted from single-task applications, such as\nclassification and segmentation, to more complex multimodal tasks, including\nmedical visual question answering and report generation. Traditional shallow\nand task-specific models are increasingly limited in addressing the complexity\nand scalability required in clinical practice. The emergence of large language\nmodels (LLMs) has driven the development of medical Large Vision-Language\nModels (LVLMs), offering a unified solution for diverse vision-language tasks.\nIn this study, we investigate various architectural designs for medical LVLMs\nbased on the widely adopted LLaVA framework, which follows an\nencoder-connector-LLM paradigm. We construct two distinct models targeting 2D\nand 3D modalities, respectively. These models are designed to support both\ngeneral-purpose medical tasks and domain-specific fine-tuning, thereby serving\nas effective foundation models. To facilitate reproducibility and further\nresearch, we develop a modular and extensible codebase, MedM-VL, and release\ntwo LVLM variants: MedM-VL-2D for 2D medical image analysis and\nMedM-VL-CT-Chest for 3D CT-based applications. The code and models are\navailable at: https://github.com/MSIIP/MedM-VL", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04385", "pdf": "https://arxiv.org/pdf/2504.04385", "abs": "https://arxiv.org/abs/2504.04385", "authors": ["Xiaokai Wang", "Guiran Liu", "Binrong Zhu", "Jacky He", "Hongye Zheng", "Hanlu Zhang"], "title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "categories": ["cs.CL"], "comment": null, "summary": "This study proposes a medical entity extraction method based on Transformer\nto enhance the information extraction capability of medical literature.\nConsidering the professionalism and complexity of medical texts, we compare the\nperformance of different pre-trained language models (BERT, BioBERT,\nPubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental\nresults show that PubMedBERT achieves the best performance (F1-score = 88.8%),\nindicating that a language model pre-trained on biomedical literature is more\neffective in the medical domain. In addition, we analyze the impact of\ndifferent entity extraction methods (CRF, Span-based, Seq2Seq) and find that\nthe Span-based approach performs best in medical entity extraction tasks\n(F1-score = 88.6%). It demonstrates superior accuracy in identifying entity\nboundaries. In low-resource scenarios, we further explore the application of\nFew-shot Learning in medical entity extraction. Experimental results show that\neven with only 10-shot training samples, the model achieves an F1-score of\n79.1%, verifying the effectiveness of Few-shot Learning under limited data\nconditions. This study confirms that the combination of pre-trained language\nmodels and Few-shot Learning can enhance the accuracy of medical entity\nextraction. Future research can integrate knowledge graphs and active learning\nstrategies to improve the model's generalization and stability, providing a\nmore effective solution for medical NLP research. Keywords- Natural Language\nProcessing, medical named entity recognition, pre-trained language model,\nFew-shot Learning, information extraction, deep learning", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04340", "pdf": "https://arxiv.org/pdf/2504.04340", "abs": "https://arxiv.org/abs/2504.04340", "authors": ["Ying Zhao"], "title": "AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop on Harnessing Generative Models for\n  Synthetic Visual Datasets (SyntaGen)", "summary": "Anomaly generation is an effective way to mitigate data scarcity for anomaly\ndetection task. Most existing works shine at industrial anomaly generation with\nmultiple specialists or large generative models, rarely generalizing to\nanomalies in other applications. In this paper, we present AnomalyHybrid, a\ndomain-agnostic framework designed to generate authentic and diverse anomalies\nsimply by combining the reference and target images. AnomalyHybrid is a\nGenerative Adversarial Network(GAN)-based framework having two decoders that\nintegrate the appearance of reference image into the depth and edge structures\nof target image respectively. With the help of depth decoders, AnomalyHybrid\nachieves authentic generation especially for the anomalies with depth values\nchanging, such a s protrusion and dent. More, it relaxes the fine granularity\nstructural control of the edge decoder and brings more diversity. Without using\nannotations, AnomalyHybrid is easily trained with sets of color, depth and edge\nof same images having different augmentations. Extensive experiments carried on\nHeliconiusButterfly, MVTecAD and MVTec3D datasets demonstrate that\nAnomalyHybrid surpasses the GAN-based state-of-the-art on anomaly generation\nand its downstream anomaly classification, detection and segmentation tasks. On\nMVTecAD dataset, AnomalyHybrid achieves 2.06/0.32 IS/LPIPS for anomaly\ngeneration, 52.6 Acc for anomaly classification with ResNet34, 97.3/72.9 AP for\nimage/pixel-level anomaly detection with a simple UNet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04427", "pdf": "https://arxiv.org/pdf/2504.04427", "abs": "https://arxiv.org/abs/2504.04427", "authors": ["Shiyan Liu", "Rui Qu", "Yan Jin"], "title": "FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Generating consecutive images of lip movements that align with a given speech\nin audio-driven lip synthesis is a challenging task. While previous studies\nhave made strides in synchronization and visual quality, lip intelligibility\nand video fluency remain persistent challenges. This work proposes FluentLip, a\ntwo-stage approach for audio-driven lip synthesis, incorporating three featured\nstrategies. To improve lip synchronization and intelligibility, we integrate a\nphoneme extractor and encoder to generate a fusion of audio and phoneme\ninformation for multimodal learning. Additionally, we employ optical flow\nconsistency loss to ensure natural transitions between image frames.\nFurthermore, we incorporate a diffusion chain during the training of Generative\nAdversarial Networks (GANs) to improve both stability and efficiency. We\nevaluate our proposed FluentLip through extensive experiments, comparing it\nwith five state-of-the-art (SOTA) approaches across five metrics, including a\nproposed metric called Phoneme Error Rate (PER) that evaluates lip pose\nintelligibility and video fluency. The experimental results demonstrate that\nour FluentLip approach is highly competitive, achieving significant\nimprovements in smoothness and naturalness. In particular, it outperforms these\nSOTA approaches by approximately $\\textbf{16.3%}$ in Fr\\'echet Inception\nDistance (FID) and $\\textbf{35.2%}$ in PER.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04616", "pdf": "https://arxiv.org/pdf/2504.04616", "abs": "https://arxiv.org/abs/2504.04616", "authors": ["Qi Zhang", "Huitong Pan", "Zhijia Chen", "Longin Jan Latecki", "Cornelia Caragea", "Eduard Dragut"], "title": "DynClean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "Accepted to NAACL2025-Findings", "summary": "Distantly Supervised Named Entity Recognition (DS-NER) has attracted\nattention due to its scalability and ability to automatically generate labeled\ndata. However, distant annotation introduces many mislabeled instances,\nlimiting its performance. Most of the existing work attempt to solve this\nproblem by developing intricate models to learn from the noisy labels. An\nalternative approach is to attempt to clean the labeled data, thus increasing\nthe quality of distant labels. This approach has received little attention for\nNER. In this paper, we propose a training dynamics-based label cleaning\napproach, which leverages the behavior of a model as training progresses to\ncharacterize the distantly annotated samples. We also introduce an automatic\nthreshold estimation strategy to locate the errors in distant labels. Extensive\nexperimental results demonstrate that: (1) models trained on our cleaned DS-NER\ndatasets, which were refined by directly removing identified erroneous\nannotations, achieve significant improvements in F1-score, ranging from 3.18%\nto 8.95%; and (2) our method outperforms numerous advanced DS-NER approaches\nacross four datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04635", "pdf": "https://arxiv.org/pdf/2504.04635", "abs": "https://arxiv.org/abs/2504.04635", "authors": ["Patrick Queiroz Da Silva", "Hari Sethuraman", "Dheeraj Rajagopal", "Hannaneh Hajishirzi", "Sachin Kumar"], "title": "Steering off Course: Reliability Challenges in Steering Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Steering methods for language models (LMs) have gained traction as\nlightweight alternatives to fine-tuning, enabling targeted modifications to\nmodel activations. However, prior studies primarily report results on a few\nmodels, leaving critical gaps in understanding the robustness of these methods.\nIn this work, we systematically examine three prominent steering methods --\nDoLa, function vectors, and task vectors. In contrast to the original studies,\nwhich evaluated a handful of models, we test up to 36 models belonging to 14\nfamilies with sizes ranging from 1.5B to 70B parameters. Our experiments reveal\nsubstantial variability in the effectiveness of the steering approaches, with a\nlarge number of models showing no improvement and at times degradation in\nsteering performance. Our analysis demonstrate fundamental flaws in the\nassumptions underlying these methods, challenging their reliability as scalable\nsteering solutions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04698", "pdf": "https://arxiv.org/pdf/2504.04698", "abs": "https://arxiv.org/abs/2504.04698", "authors": ["Yuren Mao", "Yu Mi", "Peigen Liu", "Mengfei Zhang", "Hanqing Liu", "Yunjun Gao"], "title": "scAgent: Universal Single-Cell Annotation via a LLM Agent", "categories": ["cs.CL"], "comment": null, "summary": "Cell type annotation is critical for understanding cellular heterogeneity.\nBased on single-cell RNA-seq data and deep learning models, good progress has\nbeen made in annotating a fixed number of cell types within a specific tissue.\nHowever, universal cell annotation, which can generalize across tissues,\ndiscover novel cell types, and extend to novel cell types, remains less\nexplored. To fill this gap, this paper proposes scAgent, a universal cell\nannotation framework based on Large Language Models (LLMs). scAgent can\nidentify cell types and discover novel cell types in diverse tissues;\nfurthermore, it is data efficient to learn novel cell types. Experimental\nstudies in 160 cell types and 35 tissues demonstrate the superior performance\nof scAgent in general cell-type annotation, novel cell discovery, and\nextensibility to novel cell type.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04700", "pdf": "https://arxiv.org/pdf/2504.04700", "abs": "https://arxiv.org/abs/2504.04700", "authors": ["Hyunseo Shin", "Wonseok Hwang"], "title": "Causal Retrieval with Semantic Consideration", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the performance of conversational AI systems. To extend their\ncapabilities to knowledge-intensive domains such as biomedical and legal\nfields, where the accuracy is critical, LLMs are often combined with\ninformation retrieval (IR) systems to generate responses based on retrieved\ndocuments. However, for IR systems to effectively support such applications,\nthey must go beyond simple semantic matching and accurately capture diverse\nquery intents, including causal relationships. Existing IR models primarily\nfocus on retrieving documents based on surface-level semantic similarity,\noverlooking deeper relational structures such as causality. To address this, we\npropose CAWAI, a retrieval model that is trained with dual objectives: semantic\nand causal relations. Our extensive experiments demonstrate that CAWAI\noutperforms various models on diverse causal retrieval tasks especially under\nlarge-scale retrieval settings. We also show that CAWAI exhibits strong\nzero-shot generalization across scientific domain QA tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04715", "pdf": "https://arxiv.org/pdf/2504.04715", "abs": "https://arxiv.org/abs/2504.04715", "authors": ["Will Cai", "Tianneng Shi", "Xuandong Zhao", "Dawn Song"], "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04718", "pdf": "https://arxiv.org/pdf/2504.04718", "abs": "https://arxiv.org/abs/2504.04718", "authors": ["Minki Kang", "Jongwon Jeong", "Jaewoong Cho"], "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute", "self-verification"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04510", "pdf": "https://arxiv.org/pdf/2504.04510", "abs": "https://arxiv.org/abs/2504.04510", "authors": ["Shijian Wang", "Linxin Song", "Ryotaro Shimizu", "Masayuki Goto", "Hanqian Wu"], "title": "Attributed Synthetic Data Generation for Zero-shot Domain-specific Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot domain-specific image classification is challenging in classifying\nreal images without ground-truth in-domain training examples. Recent research\ninvolved knowledge from texts with a text-to-image model to generate in-domain\ntraining images in zero-shot scenarios. However, existing methods heavily rely\non simple prompt strategies, limiting the diversity of synthetic training\nimages, thus leading to inferior performance compared to real images. In this\npaper, we propose AttrSyn, which leverages large language models to generate\nattributed prompts. These prompts allow for the generation of more diverse\nattributed synthetic images. Experiments for zero-shot domain-specific image\nclassification on two fine-grained datasets show that training with synthetic\nimages generated by AttrSyn significantly outperforms CLIP's zero-shot\nclassification under most situations and consistently surpasses simple prompt\nstrategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04519", "pdf": "https://arxiv.org/pdf/2504.04519", "abs": "https://arxiv.org/abs/2504.04519", "authors": ["Junjie Jiang", "Zelin Wang", "Manqi Zhao", "Yin Li", "DongSheng Jiang"], "title": "SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04849", "pdf": "https://arxiv.org/pdf/2504.04849", "abs": "https://arxiv.org/abs/2504.04849", "authors": ["Sam Kirkham"], "title": "Discovering dynamical laws for speech gestures", "categories": ["cs.CL", "nlin.AO"], "comment": "Accepted for publication in 'Cognitive Science'", "summary": "A fundamental challenge in the cognitive sciences is discovering the dynamics\nthat govern behaviour. Take the example of spoken language, which is\ncharacterised by a highly variable and complex set of physical movements that\nmap onto the small set of cognitive units that comprise language. What are the\nfundamental dynamical principles behind the movements that structure speech\nproduction? In this study, we discover models in the form of symbolic equations\nthat govern articulatory gestures during speech. A sparse symbolic regression\nalgorithm is used to discover models from kinematic data on the tongue and\nlips. We explore these candidate models using analytical techniques and\nnumerical simulations, and find that a second-order linear model achieves high\nlevels of accuracy, but a nonlinear force is required to properly model\narticulatory dynamics in approximately one third of cases. This supports the\nproposal that an autonomous, nonlinear, second-order differential equation is a\nviable dynamical law for articulatory gestures in speech. We conclude by\nidentifying future opportunities and obstacles in data-driven model discovery\nand outline prospects for discovering the dynamical principles that govern\nlanguage, brain and behaviour.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04861", "pdf": "https://arxiv.org/pdf/2504.04861", "abs": "https://arxiv.org/abs/2504.04861", "authors": ["Hongtao Wang", "Renchi Yang", "Hewen Wang", "Haoran Zheng", "Jianliang Xu"], "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04963", "pdf": "https://arxiv.org/pdf/2504.04963", "abs": "https://arxiv.org/abs/2504.04963", "authors": ["Yuzhe Zhang", "Min Cen", "Hong Zhang"], "title": "Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "28pages, 3 figures. First submitted in Oct. 2023", "summary": "Distantly supervised named entity recognition (DS-NER) has been proposed to\nexploit the automatically labeled training data by external knowledge bases\ninstead of human annotations. However, it tends to suffer from a high false\nnegative rate due to the inherent incompleteness. To address this issue, we\npresent a novel approach called \\textbf{C}onstraint \\textbf{M}ulti-class\n\\textbf{P}ositive and \\textbf{U}nlabeled Learning (CMPU), which introduces a\nconstraint factor on the risk estimator of multiple positive classes. It\nsuggests that the constraint non-negative risk estimator is more robust against\noverfitting than previous PU learning methods with limited positive data. Solid\ntheoretical analysis on CMPU is provided to prove the validity of our approach.\nExtensive experiments on two benchmark datasets that were labeled using diverse\nexternal knowledge sources serve to demonstrate the superior performance of\nCMPU in comparison to existing DS-NER methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04966", "pdf": "https://arxiv.org/pdf/2504.04966", "abs": "https://arxiv.org/abs/2504.04966", "authors": ["Shion Fukuhata", "Yoshinobu Kano"], "title": "Few Dimensions are Enough: Fine-tuning BERT with Selected Dimensions Revealed Its Redundant Nature", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "When fine-tuning BERT models for specific tasks, it is common to select part\nof the final layer's output and input it into a newly created fully connected\nlayer. However, it remains unclear which part of the final layer should be\nselected and what information each dimension of the layers holds. In this\nstudy, we comprehensively investigated the effectiveness and redundancy of\ntoken vectors, layers, and dimensions through BERT fine-tuning on GLUE tasks.\nThe results showed that outputs other than the CLS vector in the final layer\ncontain equivalent information, most tasks require only 2-3 dimensions, and\nwhile the contribution of lower layers decreases, there is little difference\namong higher layers. We also evaluated the impact of freezing pre-trained\nlayers and conducted cross-fine-tuning, where fine-tuning is applied\nsequentially to different tasks. The findings suggest that hidden layers may\nchange significantly during fine-tuning, BERT has considerable redundancy,\nenabling it to handle multiple tasks simultaneously, and its number of\ndimensions may be excessive.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04976", "pdf": "https://arxiv.org/pdf/2504.04976", "abs": "https://arxiv.org/abs/2504.04976", "authors": ["Carlos PelÃ¡ez-GonzÃ¡lez", "AndrÃ©s Herrera-Poyatos", "Cristina Zuheros", "David Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera"], "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 5 figures", "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050", "abs": "https://arxiv.org/abs/2504.05050", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04676", "pdf": "https://arxiv.org/pdf/2504.04676", "abs": "https://arxiv.org/abs/2504.04676", "authors": ["Bo Li", "Jing Yun"], "title": "Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-view clustering can explore common semantics from multiple views and\nhas received increasing attention in recent years. However, current methods\nfocus on learning consistency in representation, neglecting the contribution of\neach view's complementarity aspect in representation learning. This limit poses\na significant challenge in multi-view representation learning. This paper\nproposes a novel multi-view clustering framework that introduces a disentangled\nvariational autoencoder that separates multi-view into shared and private\ninformation, i.e., consistency and complementarity information. We first learn\ninformative and consistent representations by maximizing mutual information\nacross different views through contrastive learning. This process will ignore\ncomplementary information. Then, we employ consistency inference constraints to\nexplicitly utilize complementary information when attempting to seek the\nconsistency of shared information across all views. Specifically, we perform a\nwithin-reconstruction using the private and shared information of each view and\na cross-reconstruction using the shared information of all views. The dual\nconsistency constraints are not only effective in improving the representation\nquality of data but also easy to extend to other scenarios, especially in\ncomplex multi-view scenes. This could be the first attempt to employ dual\nconsistent constraint in a unified MVC theoretical framework. During the\ntraining procedure, the consistency and complementarity features are jointly\noptimized. Extensive experiments show that our method outperforms baseline\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058", "abs": "https://arxiv.org/abs/2504.05058", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "title": "Not All Data Are Unlearned Equally", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05074", "pdf": "https://arxiv.org/pdf/2504.05074", "abs": "https://arxiv.org/abs/2504.05074", "authors": ["Venkat Srinivasan", "Vishaal Jatav", "Anushka Chandrababu", "Geetika Sharma"], "title": "On the Performance of an Explainable Language Model on PubMedQA", "categories": ["cs.CL"], "comment": "Working Paper", "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04687", "pdf": "https://arxiv.org/pdf/2504.04687", "abs": "https://arxiv.org/abs/2504.04687", "authors": ["Yicheng Leng", "Chaowei Fang", "Junye Chen", "Yixiang Fang", "Sheng Li", "Guanbin Li"], "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV", "I.2.10; I.4.4; I.4.5"], "comment": "To be published in AAAI 2025", "summary": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05081", "pdf": "https://arxiv.org/pdf/2504.05081", "abs": "https://arxiv.org/abs/2504.05081", "authors": ["Tianshi Zheng", "Yixiang Chen", "Chengxi Li", "Chunyang Li", "Qing Zong", "Haochen Shi", "Baixuan Xu", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning", "categories": ["cs.CL"], "comment": "30 pages, 12 tables, 6 figures", "summary": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04708", "pdf": "https://arxiv.org/pdf/2504.04708", "abs": "https://arxiv.org/abs/2504.04708", "authors": ["Minchul Kim", "Dingqiang Ye", "Yiyang Su", "Feng Liu", "Xiaoming Liu"], "title": "SapiensID: Foundation for Human Recognition", "categories": ["cs.CV"], "comment": "To appear in CVPR2025", "summary": "Existing human recognition systems often rely on separate, specialized models\nfor face and body analysis, limiting their effectiveness in real-world\nscenarios where pose, visibility, and context vary widely. This paper\nintroduces SapiensID, a unified model that bridges this gap, achieving robust\nperformance across diverse settings. SapiensID introduces (i) Retina Patch\n(RP), a dynamic patch generation scheme that adapts to subject scale and\nensures consistent tokenization of regions of interest, (ii) a masked\nrecognition model (MRM) that learns from variable token length, and (iii)\nSemantic Attention Head (SAH), an module that learns pose-invariant\nrepresentations by pooling features around key body parts. To facilitate\ntraining, we introduce WebBody4M, a large-scale dataset capturing diverse poses\nand scale variations. Extensive experiments demonstrate that SapiensID achieves\nstate-of-the-art results on various body ReID benchmarks, outperforming\nspecialized models in both short-term and long-term scenarios while remaining\ncompetitive with dedicated face recognition systems. Furthermore, SapiensID\nestablishes a strong baseline for the newly introduced challenge of Cross\nPose-Scale ReID, demonstrating its ability to generalize to complex, real-world\nconditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04716", "pdf": "https://arxiv.org/pdf/2504.04716", "abs": "https://arxiv.org/abs/2504.04716", "authors": ["Haoren Zhao", "Tianyi Chen", "Zhen Wang"], "title": "On the Robustness of GUI Grounding Models Against Image Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) grounding models are crucial for enabling\nintelligent agents to understand and interact with complex visual interfaces.\nHowever, these models face significant robustness challenges in real-world\nscenarios due to natural noise and adversarial perturbations, and their\nrobustness remains underexplored. In this study, we systematically evaluate the\nrobustness of state-of-the-art GUI grounding models, such as UGround, under\nthree conditions: natural noise, untargeted adversarial attacks, and targeted\nadversarial attacks. Our experiments, which were conducted across a wide range\nof GUI environments, including mobile, desktop, and web interfaces, have\nclearly demonstrated that GUI grounding models exhibit a high degree of\nsensitivity to adversarial perturbations and low-resolution conditions. These\nfindings provide valuable insights into the vulnerabilities of GUI grounding\nmodels and establish a strong benchmark for future research aimed at enhancing\ntheir robustness in practical applications. Our code is available at\nhttps://github.com/ZZZhr-1/Robust_GUI_Grounding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05122", "pdf": "https://arxiv.org/pdf/2504.05122", "abs": "https://arxiv.org/abs/2504.05122", "authors": ["Xinglin Lyu", "Wei Tang", "Yuang Li", "Xiaofeng Zhao", "Ming Zhu", "Junhui Li", "Yunfei Lu", "Min Zhang", "Daimeng Wei", "Hao Yang", "Min Zhang"], "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation", "categories": ["cs.CL"], "comment": null, "summary": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04722", "pdf": "https://arxiv.org/pdf/2504.04722", "abs": "https://arxiv.org/abs/2504.04722", "authors": ["Adnan Khan", "Alireza Choubineh", "Mai A. Shaaban", "Abbas Akkasi", "Majid Komeili"], "title": "TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment", "categories": ["cs.CV"], "comment": null, "summary": "Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss, as estimated by global\nprevalence data. However, traditional methods for creating these tactile\ngraphics are labor-intensive and struggle to meet demand. We introduce\nTactileNet, the first comprehensive dataset and AI-driven framework for\ngenerating tactile graphics using text-to-image Stable Diffusion (SD) models.\nBy integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes\nSD models to produce high-fidelity, guideline-compliant tactile graphics while\nreducing computational costs. Evaluations involving tactile experts show that\ngenerated graphics achieve 92.86% adherence to tactile standards and 100%\nalignment with natural images in posture and features. Our framework also\ndemonstrates scalability, generating 32,000 images (7,050 filtered for quality)\nacross 66 classes, with prompt editing enabling customizable outputs (e.g.,\nadding/removing details). Our work empowers designers to focus on refinement,\nsignificantly accelerating accessibility efforts. It underscores the\ntransformative potential of AI for social good, offering a scalable solution to\nbridge the accessibility gap in education and beyond.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04732", "pdf": "https://arxiv.org/pdf/2504.04732", "abs": "https://arxiv.org/abs/2504.04732", "authors": ["Zhenxing Ming", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "Inverse++: Vision-Centric 3D Semantic Occupancy Prediction Assisted with 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D semantic occupancy prediction aims to forecast detailed geometric and\nsemantic information of the surrounding environment for autonomous vehicles\n(AVs) using onboard surround-view cameras. Existing methods primarily focus on\nintricate inner structure module designs to improve model performance, such as\nefficient feature sampling and aggregation processes or intermediate feature\nrepresentation formats. In this paper, we explore multitask learning by\nintroducing an additional 3D supervision signal by incorporating an additional\n3D object detection auxiliary branch. This extra 3D supervision signal enhances\nthe model's overall performance by strengthening the capability of the\nintermediate features to capture small dynamic objects in the scene, and these\nsmall dynamic objects often include vulnerable road users, i.e. bicycles,\nmotorcycles, and pedestrians, whose detection is crucial for ensuring driving\nsafety in autonomous vehicles. Extensive experiments conducted on the nuScenes\ndatasets, including challenging rainy and nighttime scenarios, showcase that\nour approach attains state-of-the-art results, achieving an IoU score of 31.73%\nand a mIoU score of 20.91% and excels at detecting vulnerable road users (VRU).\nThe code will be made available at:https://github.com/DanielMing123/Inverse++", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05214", "pdf": "https://arxiv.org/pdf/2504.05214", "abs": "https://arxiv.org/abs/2504.05214", "authors": ["Sefika Efeoglu", "Adrian Paschke", "Sonja Schimmler"], "title": "Post-Training Language Models for Continual Relation Extraction", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04743", "pdf": "https://arxiv.org/pdf/2504.04743", "abs": "https://arxiv.org/abs/2504.04743", "authors": ["Xiongbo Lu", "Yaxiong Chen", "Shengwu Xiong"], "title": "AnyArtisticGlyph: Multilingual Controllable Artistic Glyph Generation", "categories": ["cs.CV"], "comment": null, "summary": "Artistic Glyph Image Generation (AGIG) differs from current\ncreativity-focused generation models by offering finely controllable\ndeterministic generation. It transfers the style of a reference image to a\nsource while preserving its content. Although advanced and promising, current\nmethods may reveal flaws when scrutinizing synthesized image details, often\nproducing blurred or incorrect textures, posing a significant challenge. Hence,\nwe introduce AnyArtisticGlyph, a diffusion-based, multilingual controllable\nartistic glyph generation model. It includes a font fusion and embedding\nmodule, which generates latent features for detailed structure creation, and a\nvision-text fusion and embedding module that uses the CLIP model to encode\nreferences and blends them with transformation caption embeddings for seamless\nglobal image generation. Moreover, we incorporate a coarse-grained\nfeature-level loss to enhance generation accuracy. Experiments show that it\nproduces natural, detailed artistic glyph images with state-of-the-art\nperformance. Our project will be open-sourced on\nhttps://github.com/jiean001/AnyArtisticGlyph to advance text generation\ntechnology.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05226", "pdf": "https://arxiv.org/pdf/2504.05226", "abs": "https://arxiv.org/abs/2504.05226", "authors": ["Jungyeul Park"], "title": "Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations", "categories": ["cs.CL"], "comment": null, "summary": "The development of lexicalized grammars, particularly Tree-Adjoining Grammar\n(TAG), has significantly advanced our understanding of syntax and semantics in\nnatural language processing (NLP). While existing syntactic resources like the\nPenn Treebank and Universal Dependencies offer extensive annotations for\nphrase-structure and dependency parsing, there is a lack of large-scale corpora\ngrounded in lexicalized grammar formalisms. To address this gap, we introduce\nTAGbank, a corpus of TAG derivations automatically extracted from existing\nsyntactic treebanks. This paper outlines a methodology for mapping\nphrase-structure annotations to TAG derivations, leveraging the generative\npower of TAG to support parsing, grammar induction, and semantic analysis. Our\napproach builds on the work of CCGbank, extending it to incorporate the unique\nstructural properties of TAG, including its transparent derivation trees and\nits ability to capture long-distance dependencies. We also discuss the\nchallenges involved in the extraction process, including ensuring consistency\nacross treebank schemes and dealing with language-specific syntactic\nidiosyncrasies. Finally, we propose the future extension of TAGbank to include\nmultilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks,\nto explore the cross-linguistic application of TAG's formalism. By providing a\nrobust, derivation-based resource, TAGbank aims to support a wide range of\ncomputational tasks and contribute to the theoretical understanding of TAG's\ngenerative capacity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04744", "pdf": "https://arxiv.org/pdf/2504.04744", "abs": "https://arxiv.org/abs/2504.04744", "authors": ["He Zhu", "Quyu Kong", "Kechun Xu", "Xunlong Xia", "Bing Deng", "Jieping Ye", "Rong Xiong", "Yue Wang"], "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025", "summary": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04753", "pdf": "https://arxiv.org/pdf/2504.04753", "abs": "https://arxiv.org/abs/2504.04753", "authors": ["Cheng Chen", "Jiacheng Wei", "Tianrun Chen", "Chi Zhang", "Xiaofeng Yang", "Shangzhan Zhang", "Bingchen Yang", "Chuan-Sheng Foo", "Guosheng Lin", "Qixing Huang", "Fayao Liu"], "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Creating CAD digital twins from the physical world is crucial for\nmanufacturing, design, and simulation. However, current methods typically rely\non costly 3D scanning with labor-intensive post-processing. To provide a\nuser-friendly design process, we explore the problem of reverse engineering\nfrom unconstrained real-world CAD images that can be easily captured by users\nof all experiences. However, the scarcity of real-world CAD data poses\nchallenges in directly training such models. To tackle these challenges, we\npropose CADCrafter, an image-to-parametric CAD model generation framework that\ntrains solely on synthetic textureless CAD data while testing on real-world\nimages. To bridge the significant representation disparity between images and\nparametric CAD models, we introduce a geometry encoder to accurately capture\ndiverse geometric features. Moreover, the texture-invariant properties of the\ngeometric features can also facilitate the generalization to real-world\nscenarios. Since compiling CAD parameter sequences into explicit CAD models is\na non-differentiable process, the network training inherently lacks explicit\ngeometric supervision. To impose geometric validity constraints, we employ\ndirect preference optimization (DPO) to fine-tune our model with the automatic\ncode checker feedback on CAD sequence quality. Furthermore, we collected a\nreal-world dataset, comprised of multi-view images and corresponding CAD\ncommand sequence pairs, to evaluate our method. Experimental results\ndemonstrate that our approach can robustly handle real unconstrained CAD\nimages, and even generalize to unseen general objects.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03714", "pdf": "https://arxiv.org/pdf/2504.03714", "abs": "https://arxiv.org/abs/2504.03714", "authors": ["Runpeng Dai", "Run Yang", "Fan Zhou", "Hongtu Zhu"], "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have become\nessential to general artificial intelligence, exhibiting remarkable\ncapabilities in task understanding and problem-solving. However, the real-world\nreliability of these models critically depends on their stability, which\nremains an underexplored area. Despite their widespread use, rigorous studies\nexamining the stability of LLMs under various perturbations are still lacking.\nIn this paper, we address this gap by proposing a novel stability measure for\nLLMs, inspired by statistical methods rooted in information geometry. Our\nmeasure possesses desirable invariance properties, making it well-suited for\nanalyzing model sensitivity to both parameter and input perturbations. To\nassess the effectiveness of our approach, we conduct extensive experiments on\nmodels ranging in size from 1.5B to 13B parameters. Our results demonstrate the\nutility of our measure in identifying salient parameters and detecting\nvulnerable regions in input images or critical dimensions in token embeddings.\nFurthermore, leveraging our stability framework, we enhance model robustness\nduring model merging, leading to improved performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04784", "pdf": "https://arxiv.org/pdf/2504.04784", "abs": "https://arxiv.org/abs/2504.04784", "authors": ["Hui Liu", "Bin Zou", "Suiyun Zhang", "Kecheng Chen", "Rui Liu", "Haoliang Li"], "title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Instruction-guided image editing enables users to specify modifications using\nnatural language, offering more flexibility and control. Among existing\nframeworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion\nmodels in scalability and performance. However, while real-world scenarios\noften require concurrent execution of multiple instructions, step-by-step\nediting suffers from accumulated errors and degraded quality, and integrating\nmultiple instructions with a single prompt usually results in incomplete edits\ndue to instruction conflicts. We propose Instruction Influence Disentanglement\n(IID), a novel framework enabling parallel execution of multiple instructions\nin a single denoising process, designed for DiT-based models. By analyzing\nself-attention mechanisms in DiTs, we identify distinctive attention patterns\nin multi-instruction settings and derive instruction-specific attention masks\nto disentangle each instruction's influence. These masks guide the editing\nprocess to ensure localized modifications while preserving consistency in\nnon-edited regions. Extensive experiments on open-source and custom datasets\ndemonstrate that IID reduces diffusion steps while improving fidelity and\ninstruction completion compared to existing baselines. The codes will be\npublicly released upon the acceptance of the paper.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04787", "pdf": "https://arxiv.org/pdf/2504.04787", "abs": "https://arxiv.org/abs/2504.04787", "authors": ["Mengxuan Wu", "Zekai Li", "Zhiyuan Liang", "Moyang Li", "Xuanlei Zhao", "Samir Khaki", "Zheng Zhu", "Xiaojiang Peng", "Konstantinos N. Plataniotis", "Kai Wang", "Wangbo Zhao", "Yang You"], "title": "Dynamic Vision Mamba", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03775", "pdf": "https://arxiv.org/pdf/2504.03775", "abs": "https://arxiv.org/abs/2504.03775", "authors": ["Weiqing Li", "Guochao Jiang", "Xiangyong Ding", "Zhangcheng Tao", "Chuzhan Hao", "Chenfeng Xu", "Yuewei Zhang", "Hao Wang"], "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": null, "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04804", "pdf": "https://arxiv.org/pdf/2504.04804", "abs": "https://arxiv.org/abs/2504.04804", "authors": ["Yuanpei Liu", "Kai Han"], "title": "DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at ICLR 2025", "summary": "In this paper, we tackle the problem of Generalized Category Discovery (GCD).\nGiven a dataset containing both labelled and unlabelled images, the objective\nis to categorize all images in the unlabelled subset, irrespective of whether\nthey are from known or unknown classes. In GCD, an inherent label bias exists\nbetween known and unknown classes due to the lack of ground-truth labels for\nthe latter. State-of-the-art methods in GCD leverage parametric classifiers\ntrained through self-distillation with soft labels, leaving the bias issue\nunattended. Besides, they treat all unlabelled samples uniformly, neglecting\nvariations in certainty levels and resulting in suboptimal learning. Moreover,\nthe explicit identification of semantic distribution shifts between known and\nunknown classes, a vital aspect for effective GCD, has been neglected. To\naddress these challenges, we introduce DebGCD, a \\underline{Deb}iased learning\nwith distribution guidance framework for \\underline{GCD}. Initially, DebGCD\nco-trains an auxiliary debiased classifier in the same feature space as the GCD\nclassifier, progressively enhancing the GCD features. Moreover, we introduce a\nsemantic distribution detector in a separate feature space to implicitly boost\nthe learning efficacy of GCD. Additionally, we employ a curriculum learning\nstrategy based on semantic distribution certainty to steer the debiased\nlearning at an optimized pace. Thorough evaluations on GCD benchmarks\ndemonstrate the consistent state-of-the-art performance of our framework,\nhighlighting its superiority. Project page: https://visual-ai.github.io/debgcd/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947", "abs": "https://arxiv.org/abs/2504.03947", "authors": ["Chris Samarinas", "Hamed Zamani"], "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04834", "pdf": "https://arxiv.org/pdf/2504.04834", "abs": "https://arxiv.org/abs/2504.04834", "authors": ["Pengju Sun", "Banglei Guan", "Zhenbao Yu", "Yang Shang", "Qifeng Yu", "Daniel Barath"], "title": "Learning Affine Correspondences by Integrating Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Affine correspondences have received significant attention due to their\nbenefits in tasks like image matching and pose estimation. Existing methods for\nextracting affine correspondences still have many limitations in terms of\nperformance; thus, exploring a new paradigm is crucial. In this paper, we\npresent a new pipeline designed for extracting accurate affine correspondences\nby integrating dense matching and geometric constraints. Specifically, a novel\nextraction framework is introduced, with the aid of dense matching and a novel\nkeypoint scale and orientation estimator. For this purpose, we propose loss\nfunctions based on geometric constraints, which can effectively improve\naccuracy by supervising neural networks to learn feature geometry. The\nexperimental show that the accuracy and robustness of our method outperform the\nexisting ones in image matching tasks. To further demonstrate the effectiveness\nof the proposed method, we applied it to relative pose estimation. Affine\ncorrespondences extracted by our method lead to more accurate poses than the\nbaselines on a range of real-world datasets. The code is available at\nhttps://github.com/stilcrad/DenseAffine.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04837", "pdf": "https://arxiv.org/pdf/2504.04837", "abs": "https://arxiv.org/abs/2504.04837", "authors": ["Zhi Zuo", "Chenyi Zhuang", "Zhiqiang Shen", "Pan Gao", "Jie Qin"], "title": "Uni4D: A Unified Self-Supervised Learning Framework for Point Cloud Videos", "categories": ["cs.CV"], "comment": "11 pages, 7 figures", "summary": "Point cloud video representation learning is primarily built upon the masking\nstrategy in a self-supervised manner. However, the progress is slow due to\nseveral significant challenges: (1) existing methods learn the motion\nparticularly with hand-crafted designs, leading to unsatisfactory motion\npatterns during pre-training which are non-transferable on fine-tuning\nscenarios. (2) previous Masked AutoEncoder (MAE) frameworks are limited in\nresolving the huge representation gap inherent in 4D data. In this study, we\nintroduce the first self-disentangled MAE for learning discriminative 4D\nrepresentations in the pre-training stage. To address the first challenge, we\npropose to model the motion representation in a latent space. The second issue\nis resolved by introducing the latent tokens along with the typical geometry\ntokens to disentangle high-level and low-level features during decoding.\nExtensive experiments on MSR-Action3D, NTU-RGBD, HOI4D, NvGesture, and SHREC'17\nverify this self-disentangled learning framework. We demonstrate that it can\nboost the fine-tuning performance on all 4D tasks, which we term Uni4D. Our\npre-trained model presents discriminative and meaningful 4D representations,\nparticularly benefits processing long videos, as Uni4D gets $+3.8\\%$\nsegmentation accuracy on HOI4D, significantly outperforming either\nself-supervised or fully-supervised methods after end-to-end fine-tuning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04277", "pdf": "https://arxiv.org/pdf/2504.04277", "abs": "https://arxiv.org/abs/2504.04277", "authors": ["Marios Kokkodis", "Richard Demsyn-Jones", "Vijay Raghavan"], "title": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "comment": null, "summary": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04842", "pdf": "https://arxiv.org/pdf/2504.04842", "abs": "https://arxiv.org/abs/2504.04842", "authors": ["Mengchao Wang", "Qiang Wang", "Fan Jiang", "Yaqi Fan", "Yunpeng Zhang", "Yonggang Qi", "Kun Zhao", "Mu Xu"], "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04869", "pdf": "https://arxiv.org/pdf/2504.04869", "abs": "https://arxiv.org/abs/2504.04869", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu"], "title": "Content-Aware Transformer for All-in-one Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration has witnessed significant advancements with the development\nof deep learning models. Although Transformer architectures have progressed\nconsiderably in recent years, challenges remain, particularly the limited\nreceptive field in window-based self-attention. In this work, we propose\nDSwinIR, a Deformable Sliding window Transformer for Image Restoration. DSwinIR\nintroduces a novel deformable sliding window self-attention that adaptively\nadjusts receptive fields based on image content, enabling the attention\nmechanism to focus on important regions and enhance feature extraction aligned\nwith salient features. Additionally, we introduce a central ensemble pattern to\nreduce the inclusion of irrelevant content within attention windows. In this\nway, the proposed DSwinIR model integrates the deformable sliding window\nTransformer and central ensemble pattern to amplify the strengths of both CNNs\nand Transformers while mitigating their limitations. Extensive experiments on\nvarious image restoration tasks demonstrate that DSwinIR achieves\nstate-of-the-art performance. For example, in image deraining, compared to\nDRSformer on the SPA dataset, DSwinIR achieves a 0.66 dB PSNR improvement. In\nall-in-one image restoration, compared to PromptIR, DSwinIR achieves over a\n0.66 dB and 1.04 dB improvement on three-task and five-task settings,\nrespectively. Pretrained models and code are available at our project\nhttps://github.com/Aitical/DSwinIR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04383", "pdf": "https://arxiv.org/pdf/2504.04383", "abs": "https://arxiv.org/abs/2504.04383", "authors": ["Ximing Lu", "Seungju Han", "David Acuna", "Hyunwoo Kim", "Jaehun Jung", "Shrimai Prabhumoye", "Niklas Muennighoff", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Code and data will be publicly released upon internal approval", "summary": "Large reasoning models exhibit remarkable reasoning capabilities via long,\nelaborate reasoning trajectories. Supervised fine-tuning on such reasoning\ntraces, also known as distillation, can be a cost-effective way to boost\nreasoning capabilities of student models. However, empirical observations\nreveal that these reasoning trajectories are often suboptimal, switching\nexcessively between different lines of thought, resulting in under-thinking,\nover-thinking, and even degenerate responses. We introduce Retro-Search, an\nMCTS-inspired search algorithm, for distilling higher quality reasoning paths\nfrom large reasoning models. Retro-Search retrospectively revises reasoning\npaths to discover better, yet shorter traces, which can then lead to student\nmodels with enhanced reasoning capabilities with shorter, thus faster\ninference. Our approach can enable two use cases: self-improvement, where\nmodels are fine-tuned on their own Retro-Search-ed thought traces, and\nweak-to-strong improvement, where a weaker model revises stronger model's\nthought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned\non its own Retro-Search-ed traces, reduces the average reasoning length by\n31.2% while improving performance by 7.7% across seven math benchmarks. For\nweak-to-strong improvement, we retrospectively revise R1-671B's traces from the\nOpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x\nsmaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance\ncomparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length\nand a 2.4% performance improvement compared to fine-tuning on the original\nOpenThoughts data. Our work counters recently emergent viewpoints that question\nthe relevance of search algorithms in the era of large reasoning models, by\ndemonstrating that there are still opportunities for algorithmic advancements,\neven for frontier models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["MCTS"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04903", "pdf": "https://arxiv.org/pdf/2504.04903", "abs": "https://arxiv.org/abs/2504.04903", "authors": ["Yuandong Pu", "Le Zhuo", "Kaiwen Zhu", "Liangbin Xie", "Wenlong Zhang", "Xiangyu Chen", "Pneg Gao", "Yu Qiao", "Chao Dong", "Yihao Liu"], "title": "Lumina-OmniLV: A Unified Multimodal Framework for General Low-Level Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Lunima-OmniLV (abbreviated as OmniLV), a universal multimodal\nmulti-task framework for low-level vision that addresses over 100 sub-tasks\nacross four major categories: image restoration, image enhancement,\nweak-semantic dense prediction, and stylization. OmniLV leverages both textual\nand visual prompts to offer flexible and user-friendly interactions. Built on\nDiffusion Transformer (DiT)-based generative priors, our framework supports\narbitrary resolutions -- achieving optimal performance at 1K resolution --\nwhile preserving fine-grained details and high fidelity. Through extensive\nexperiments, we demonstrate that separately encoding text and visual\ninstructions, combined with co-training using shallow feature control, is\nessential to mitigate task ambiguity and enhance multi-task generalization. Our\nfindings also reveal that integrating high-level generative tasks into\nlow-level vision models can compromise detail-sensitive restoration. These\ninsights pave the way for more robust and generalizable low-level vision\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04924", "pdf": "https://arxiv.org/pdf/2504.04924", "abs": "https://arxiv.org/abs/2504.04924", "authors": ["Changqing Su", "Yanqin Chen", "Zihan Lin", "Zhen Cheng", "You Zhou", "Bo Xiong", "Zhaofei Yu", "Tiejun Huang"], "title": "Inter-event Interval Microscopy for Event Cameras", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Event cameras, an innovative bio-inspired sensor, differ from traditional\ncameras by sensing changes in intensity rather than directly perceiving\nintensity and recording these variations as a continuous stream of \"events\".\nThe intensity reconstruction from these sparse events has long been a\nchallenging problem. Previous approaches mainly focused on transforming\nmotion-induced events into videos or achieving intensity imaging for static\nscenes by integrating modulation devices at the event camera acquisition end.\nIn this paper, for the first time, we achieve event-to-intensity conversion\nusing a static event camera for both static and dynamic scenes in fluorescence\nmicroscopy. Unlike conventional methods that primarily rely on event\nintegration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the\ntime interval between consecutive events at each pixel. With a fixed threshold\nin the event camera, the time interval can precisely represent the intensity.\nAt the hardware level, the proposed IEIM integrates a pulse light modulation\ndevice within a microscope equipped with an event camera, termed Pulse\nModulation-based Event-driven Fluorescence Microscopy.mAdditionally, we have\ncollected IEIMat dataset under various scenes including high dynamic range and\nhigh-speed scenarios. Experimental results on the IEIMat dataset demonstrate\nthat the proposed IEIM achieves superior spatial and temporal resolution, as\nwell as a higher dynamic range, with lower bandwidth compared to other methods.\nThe code and the IEIMat dataset will be made publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04699", "pdf": "https://arxiv.org/pdf/2504.04699", "abs": "https://arxiv.org/abs/2504.04699", "authors": ["Martin Weyssow", "Chengran Yang", "Junkai Chen", "Yikun Li", "Huihui Huang", "Ratnadira Widyasari", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "RLAIF", "AI feedback"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04927", "pdf": "https://arxiv.org/pdf/2504.04927", "abs": "https://arxiv.org/abs/2504.04927", "authors": ["Danial Amin", "Joni Salminen", "Farhan Ahmed", "Sonja M. H. Tervola", "Sankalp Sethi", "Bernard J. Jansen"], "title": "How Is Generative AI Used for Persona Development?: A Systematic Review of 52 Research Articles", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Although Generative AI (GenAI) has the potential for persona development,\nmany challenges must be addressed. This research systematically reviews 52\narticles from 2022-2024, with important findings. First, closed commercial\nmodels are frequently used in persona development, creating a monoculture\nSecond, GenAI is used in various stages of persona development (data\ncollection, segmentation, enrichment, and evaluation). Third, similar to other\nquantitative persona development techniques, there are major gaps in persona\nevaluation for AI generated personas. Fourth, human-AI collaboration models are\nunderdeveloped, despite human oversight being crucial for maintaining ethical\nstandards. These findings imply that realizing the full potential of\nAI-generated personas will require substantial efforts across academia and\nindustry. To that end, we provide a list of research avenues to inspire future\nwork.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05030", "pdf": "https://arxiv.org/pdf/2504.05030", "abs": "https://arxiv.org/abs/2504.05030", "authors": ["Wang Tang", "Fethiye Irmak Dogan", "Linbo Qing", "Hatice Gunes"], "title": "AsyReC: A Multimodal Graph-based Framework for Spatio-Temporal Asymmetric Dyadic Relationship Classification", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Dyadic social relationships, which refer to relationships between two\nindividuals who know each other through repeated interactions (or not), are\nshaped by shared spatial and temporal experiences. Current computational\nmethods for modeling these relationships face three major challenges: (1) the\nfailure to model asymmetric relationships, e.g., one individual may perceive\nthe other as a friend while the other perceives them as an acquaintance, (2)\nthe disruption of continuous interactions by discrete frame sampling, which\nsegments the temporal continuity of interaction in real-world scenarios, and\n(3) the limitation to consider periodic behavioral cues, such as rhythmic\nvocalizations or recurrent gestures, which are crucial for inferring the\nevolution of dyadic relationships. To address these challenges, we propose\nAsyReC, a multimodal graph-based framework for asymmetric dyadic relationship\nclassification, with three core innovations: (i) a triplet graph neural network\nwith node-edge dual attention that dynamically weights multimodal cues to\ncapture interaction asymmetries (addressing challenge 1); (ii) a clip-level\nrelationship learning architecture that preserves temporal continuity, enabling\nfine-grained modeling of real-world interaction dynamics (addressing challenge\n2); and (iii) a periodic temporal encoder that projects time indices onto\nsine/cosine waveforms to model recurrent behavioral patterns (addressing\nchallenge 3). Extensive experiments on two public datasets demonstrate\nstate-of-the-art performance, while ablation studies validate the critical role\nof asymmetric interaction modeling and periodic temporal encoding in improving\nthe robustness of dyadic relationship classification in real-world scenarios.\nOur code is publicly available at: https://github.com/tw-repository/AsyReC.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05062", "pdf": "https://arxiv.org/pdf/2504.05062", "abs": "https://arxiv.org/abs/2504.05062", "authors": ["Chenfeng Xu"], "title": "LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of deep learning, the field of change detection\n(CD) in remote sensing imagery has achieved remarkable progress. Existing\nchange detection methods primarily focus on achieving higher accuracy with\nincreased computational costs and parameter sizes, leaving development of\nlightweight methods for rapid real-world processing an underexplored challenge.\nTo address this challenge, we propose a Lightweight Difference Guiding Network\n(LDGNet), leveraging absolute difference image to guide optical remote sensing\nchange detection. First, to enhance the feature representation capability of\nthe lightweight backbone network, we propose the Difference Guiding Module\n(DGM), which leverages multi-scale features extracted from the absolute\ndifference image to progressively influence the original image encoder at each\nlayer, thereby reinforcing feature extraction. Second, we propose the\nDifference-Aware Dynamic Fusion (DADF) module with Visual State Space Model\n(VSSM) for lightweight long-range dependency modeling. The module first uses\nfeature absolute differences to guide VSSM's global contextual modeling of\nchange regions, then employs difference attention to dynamically fuse these\nlong-range features with feature differences, enhancing change semantics while\nsuppressing noise and background. Extensive experiments on multiple datasets\ndemonstrate that our method achieves comparable or superior performance to\ncurrent state-of-the-art (SOTA) methods requiring several times more\ncomputation, while maintaining only 3.43M parameters and 1.12G FLOPs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05076", "pdf": "https://arxiv.org/pdf/2504.05076", "abs": "https://arxiv.org/abs/2504.05076", "authors": ["Shuai Liu", "Qingyu Mao", "Chao Li", "Jiacong Chen", "Fanyang Meng", "Yonghong Tian", "Yongsheng Liang"], "title": "Content-Distortion High-Order Interaction for Blind Image Quality Assessment", "categories": ["cs.CV"], "comment": "19 pages (main text: 14 pages + appendix: 5 pages), 9 figures, 23\n  tables. In submission", "summary": "The content and distortion are widely recognized as the two primary factors\naffecting the visual quality of an image. While existing No-Reference Image\nQuality Assessment (NR-IQA) methods have modeled these factors, they fail to\ncapture the complex interactions between content and distortions. This\nshortfall impairs their ability to accurately perceive quality. To confront\nthis, we analyze the key properties required for interaction modeling and\npropose a robust NR-IQA approach termed CoDI-IQA (Content-Distortion high-order\nInteraction for NR-IQA), which aggregates local distortion and global content\nfeatures within a hierarchical interaction framework. Specifically, a\nProgressive Perception Interaction Module (PPIM) is proposed to explicitly\nsimulate how content and distortions independently and jointly influence image\nquality. By integrating internal interaction, coarse interaction, and fine\ninteraction, it achieves high-order interaction modeling that allows the model\nto properly represent the underlying interaction patterns. To ensure sufficient\ninteraction, multiple PPIMs are employed to hierarchically fuse multi-level\ncontent and distortion features at different granularities. We also tailor a\ntraining strategy suited for CoDI-IQA to maintain interaction stability.\nExtensive experiments demonstrate that the proposed method notably outperforms\nthe state-of-the-art methods in terms of prediction accuracy, data efficiency,\nand generalization ability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05135", "pdf": "https://arxiv.org/pdf/2504.05135", "abs": "https://arxiv.org/abs/2504.05135", "authors": ["Jiamei Xiong", "Xuefeng Yan", "Yongzhen Wang", "Wei Zhao", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "DA2Diff: Exploring Degradation-aware Adaptive Diffusion Priors for All-in-One Weather Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration under adverse weather conditions is a critical task for\nmany vision-based applications. Recent all-in-one frameworks that handle\nmultiple weather degradations within a unified model have shown potential.\nHowever, the diversity of degradation patterns across different weather\nconditions, as well as the complex and varied nature of real-world\ndegradations, pose significant challenges for multiple weather removal. To\naddress these challenges, we propose an innovative diffusion paradigm with\ndegradation-aware adaptive priors for all-in-one weather restoration, termed\nDA2Diff. It is a new exploration that applies CLIP to perceive\ndegradation-aware properties for better multi-weather restoration.\nSpecifically, we deploy a set of learnable prompts to capture degradation-aware\nrepresentations by the prompt-image similarity constraints in the CLIP space.\nBy aligning the snowy/hazy/rainy images with snow/haze/rain prompts, each\nprompt contributes to different weather degradation characteristics. The\nlearned prompts are then integrated into the diffusion model via the designed\nweather specific prompt guidance module, making it possible to restore multiple\nweather types. To further improve the adaptiveness to complex weather\ndegradations, we propose a dynamic expert selection modulator that employs a\ndynamic weather-aware router to flexibly dispatch varying numbers of\nrestoration experts for each weather-distorted image, allowing the diffusion\nmodel to restore diverse degradations adaptively. Experimental results\nsubstantiate the favorable performance of DA2Diff over state-of-the-arts in\nquantitative and qualitative evaluation. Source code will be available after\nacceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05170", "pdf": "https://arxiv.org/pdf/2504.05170", "abs": "https://arxiv.org/abs/2504.05170", "authors": ["Bonan Ding", "Jin Xie", "Jing Nie", "Jiale Cao"], "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2025", "summary": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05201", "pdf": "https://arxiv.org/pdf/2504.05201", "abs": "https://arxiv.org/abs/2504.05201", "authors": ["Jared Frazier", "Tejas Sudharshan Mathai", "Jianfei Liu", "Angshuman Paul", "Ronald M. Summers"], "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05207", "pdf": "https://arxiv.org/pdf/2504.05207", "abs": "https://arxiv.org/abs/2504.05207", "authors": ["Alexander Shieh", "Tejas Sudharshan Mathai", "Jianfei Liu", "Angshuman Paul", "Ronald M. Summers"], "title": "Correcting Class Imbalances with Self-Training for Improved Universal Lesion Detection and Tagging", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Universal lesion detection and tagging (ULDT) in CT studies is critical for\ntumor burden assessment and tracking the progression of lesion status\n(growth/shrinkage) over time. However, a lack of fully annotated data hinders\nthe development of effective ULDT approaches. Prior work used the DeepLesion\ndataset (4,427 patients, 10,594 studies, 32,120 CT slices, 32,735 lesions, 8\nbody part labels) for algorithmic development, but this dataset is not\ncompletely annotated and contains class imbalances. To address these issues, in\nthis work, we developed a self-training pipeline for ULDT. A VFNet model was\ntrained on a limited 11.5\\% subset of DeepLesion (bounding boxes + tags) to\ndetect and classify lesions in CT studies. Then, it identified and incorporated\nnovel lesion candidates from a larger unseen data subset into its training set,\nand self-trained itself over multiple rounds. Multiple self-training\nexperiments were conducted with different threshold policies to select\npredicted lesions with higher quality and cover the class imbalances. We\ndiscovered that direct self-training improved the sensitivities of\nover-represented lesion classes at the expense of under-represented classes.\nHowever, upsampling the lesions mined during self-training along with a\nvariable threshold policy yielded a 6.5\\% increase in sensitivity at 4 FP in\ncontrast to self-training without class balancing (72\\% vs 78.5\\%) and a 11.7\\%\nincrease compared to the same self-training policy without upsampling (66.8\\%\nvs 78.5\\%). Furthermore, we show that our results either improved or maintained\nthe sensitivity at 4FP for all 8 lesion classes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05219", "pdf": "https://arxiv.org/pdf/2504.05219", "abs": "https://arxiv.org/abs/2504.05219", "authors": ["Abdurrahim Yilmaz", "Serra Atilla Aydin", "Deniz Temur", "Furkan Yuceyalcin", "Berkin Deniz Kahya", "Rahmetullah Varol", "Ozay Gokoz", "Gulsum Gencoglan", "Huseyin Uvet", "Gonca Elcin"], "title": "An ensemble deep learning approach to detect tumors on Mohs micrographic surgery slides", "categories": ["cs.CV", "eess.IV"], "comment": "14 pages, 2 figures", "summary": "Mohs micrographic surgery (MMS) is the gold standard technique for removing\nhigh risk nonmelanoma skin cancer however, intraoperative histopathological\nexamination demands significant time, effort, and professionality. The\nobjective of this study is to develop a deep learning model to detect basal\ncell carcinoma (BCC) and artifacts on Mohs slides. A total of 731 Mohs slides\nfrom 51 patients with BCCs were used in this study, with 91 containing tumor\nand 640 without tumor which was defined as non-tumor. The dataset was employed\nto train U-Net based models that segment tumor and non-tumor regions on the\nslides. The segmented patches were classified as tumor, or non-tumor to produce\npredictions for whole slide images (WSIs). For the segmentation phase, the deep\nlearning model success was measured using a Dice score with 0.70 and 0.67\nvalue, area under the curve (AUC) score with 0.98 and 0.96 for tumor and\nnon-tumor, respectively. For the tumor classification, an AUC of 0.98 for\npatch-based detection, and AUC of 0.91 for slide-based detection was obtained\non the test dataset. We present an AI system that can detect tumors and\nnon-tumors in Mohs slides with high success. Deep learning can aid Mohs\nsurgeons and dermatopathologists in making more accurate decisions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05227", "pdf": "https://arxiv.org/pdf/2504.05227", "abs": "https://arxiv.org/abs/2504.05227", "authors": ["Julio Silva-RodrÃ­guez", "Jose Dolz", "Ismail Ben Ayed"], "title": "A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?", "categories": ["cs.CV"], "comment": "IPMI 2025. Code and weights: https://github.com/jusiro/DLILP", "summary": "Vision-language pre-training has recently gained popularity as it allows\nlearning rich feature representations using large-scale data sources. This\nparadigm has quickly made its way into the medical image analysis community. In\nparticular, there is an impressive amount of recent literature developing\nvision-language models for radiology. However, the available medical datasets\nwith image-text supervision are scarce, and medical concepts are fine-grained,\ninvolving expert knowledge that existing vision-language models struggle to\nencode. In this paper, we propose to take a prudent step back from the\nliterature and revisit supervised, unimodal pre-training, using fine-grained\nlabels instead. We conduct an extensive comparison demonstrating that unimodal\npre-training is highly competitive and better suited to integrating\nheterogeneous data sources. Our results also question the potential of recent\nvision-language models for open-vocabulary generalization, which have been\nevaluated using optimistic experimental settings. Finally, we study novel\nalternatives to better integrate fine-grained labels and noisy text\nsupervision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03654", "pdf": "https://arxiv.org/pdf/2504.03654", "abs": "https://arxiv.org/abs/2504.03654", "authors": ["Keondo Park", "You Rim Choi", "Inhoe Lee", "Hyung-Sin Kim"], "title": "PointSplit: Towards On-device 3D Object Detection with Heterogeneous Low-power Accelerators", "categories": ["cs.DC", "cs.AI", "cs.CV"], "comment": null, "summary": "Running deep learning models on resource-constrained edge devices has drawn\nsignificant attention due to its fast response, privacy preservation, and\nrobust operation regardless of Internet connectivity. While these devices\nalready cope with various intelligent tasks, the latest edge devices that are\nequipped with multiple types of low-power accelerators (i.e., both mobile GPU\nand NPU) can bring another opportunity; a task that used to be too heavy for an\nedge device in the single-accelerator world might become viable in the upcoming\nheterogeneous-accelerator world.To realize the potential in the context of 3D\nobject detection, we identify several technical challenges and propose\nPointSplit, a novel 3D object detection framework for multi-accelerator edge\ndevices that addresses the problems. Specifically, our PointSplit design\nincludes (1) 2D semantics-aware biased point sampling, (2) parallelized 3D\nfeature extraction, and (3) role-based group-wise quantization. We implement\nPointSplit on TensorFlow Lite and evaluate it on a customized hardware platform\ncomprising both mobile GPU and EdgeTPU. Experimental results on representative\nRGB-D datasets, SUN RGB-D and Scannet V2, demonstrate that PointSplit on a\nmulti-accelerator device is 24.7 times faster with similar accuracy compared to\nthe full-precision, 2D-3D fusion-based 3D detector on a GPU-only device.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03736", "pdf": "https://arxiv.org/pdf/2504.03736", "abs": "https://arxiv.org/abs/2504.03736", "authors": ["Teodor Chiaburu", "Felix BieÃmann", "Frank HauÃer"], "title": "Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages, 10 figures, accepted at WCXAI 2025 Istanbul", "summary": "Understanding uncertainty in Explainable AI (XAI) is crucial for building\ntrust and ensuring reliable decision-making in Machine Learning models. This\npaper introduces a unified framework for quantifying and interpreting\nUncertainty in XAI by defining a general explanation function $e_{\\theta}(x,\nf)$ that captures the propagation of uncertainty from key sources:\nperturbations in input data and model parameters. By using both analytical and\nempirical estimates of explanation variance, we provide a systematic means of\nassessing the impact uncertainty on explanations. We illustrate the approach\nusing a first-order uncertainty propagation as the analytical estimator. In a\ncomprehensive evaluation across heterogeneous datasets, we compare analytical\nand empirical estimates of uncertainty propagation and evaluate their\nrobustness. Extending previous work on inconsistencies in explanations, our\nexperiments identify XAI methods that do not reliably capture and propagate\nuncertainty. Our findings underscore the importance of uncertainty-aware\nexplanations in high-stakes applications and offer new insights into the\nlimitations of current XAI methods. The code for the experiments can be found\nin our repository at https://github.com/TeodorChiaburu/UXAI", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03783", "pdf": "https://arxiv.org/pdf/2504.03783", "abs": "https://arxiv.org/abs/2504.03783", "authors": ["Haoyuan Li", "Jindong Wang", "Mathias Funk", "Aaqib Saeed"], "title": "FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "comment": null, "summary": "Federated Active Learning (FAL) has emerged as a promising framework to\nleverage large quantities of unlabeled data across distributed clients while\npreserving data privacy. However, real-world deployments remain limited by high\nannotation costs and communication-intensive sampling processes, particularly\nin a cross-silo setting, when clients possess substantial local datasets. This\npaper addresses the crucial question: What is the best practice to reduce\ncommunication costs in human-in-the-loop learning with minimal annotator\neffort? Existing FAL methods typically rely on iterative annotation processes\nthat separate active sampling from federated updates, leading to multiple\nrounds of expensive communication and annotation. In response, we introduce\nFAST, a two-pass FAL framework that harnesses foundation models for weak\nlabeling in a preliminary pass, followed by a refinement pass focused\nexclusively on the most uncertain samples. By leveraging representation\nknowledge from foundation models and integrating refinement steps into a\nstreamlined workflow, FAST substantially reduces the overhead incurred by\niterative active sampling. Extensive experiments on diverse medical and natural\nimage benchmarks demonstrate that FAST outperforms existing FAL methods by an\naverage of 4.36% while reducing communication rounds eightfold under a limited\n5% labeling budget.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04749", "pdf": "https://arxiv.org/pdf/2504.04749", "abs": "https://arxiv.org/abs/2504.04749", "authors": ["Ahmad Hussein", "Mukesh Prasad", "Ali Braytee"], "title": "Vision Transformers with Autoencoders and Explainable AI for Cancer Patient Risk Stratification Using Whole Slide Imaging", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "10 pages", "summary": "Cancer remains one of the leading causes of mortality worldwide,\nnecessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has\nbecome an integral part of clinical workflows with advancements in digital\npathology. While various studies have utilized WSIs, their extracted features\nmay not fully capture the most relevant pathological information, and their\nlack of interpretability limits clinical adoption.\n  In this paper, we propose PATH-X, a framework that integrates Vision\nTransformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations)\nto enhance model explainability for patient stratification and risk prediction\nusing WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is\nselected from each WSI, and numerical feature embeddings are extracted using\nGoogle's pre-trained ViT. These features are then compressed via an autoencoder\nand used for unsupervised clustering and classification tasks. Kaplan-Meier\nsurvival analysis is applied to evaluate stratification into two and three risk\ngroups. SHAP is used to identify key contributing features, which are mapped\nonto histopathological slices to provide spatial context.\n  PATH-X demonstrates strong performance in breast and glioma cancers, where a\nsufficient number of WSIs enabled robust stratification. However, performance\nin lung cancer was limited due to data availability, emphasizing the need for\nlarger datasets to enhance model reliability and clinical applicability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04814", "pdf": "https://arxiv.org/pdf/2504.04814", "abs": "https://arxiv.org/abs/2504.04814", "authors": ["Nataliia Molchanova", "Pedro M. Gordaliza", "Alessandro Cagol", "Mario Ocampo--Pineda", "Po--Jui Lu", "Matthias Weigel", "Xinjie Chen", "Erin S. Beck", "Haris Tsagkas", "Daniel Reich", "Anna StÃ¶lting", "Pietro Maggi", "Delphine Ribes", "Adrien Depeursinge", "Cristina Granziera", "Henning MÃ¼ller", "Meritxell Bach Cuadra"], "title": "Explainability of AI Uncertainty: Application to Multiple Sclerosis Lesion Segmentation on MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Trustworthy artificial intelligence (AI) is essential in healthcare,\nparticularly for high-stakes tasks like medical image segmentation. Explainable\nAI and uncertainty quantification significantly enhance AI reliability by\naddressing key attributes such as robustness, usability, and explainability.\nDespite extensive technical advances in uncertainty quantification for medical\nimaging, understanding the clinical informativeness and interpretability of\nuncertainty remains limited. This study introduces a novel framework to explain\nthe potential sources of predictive uncertainty, specifically in cortical\nlesion segmentation in multiple sclerosis using deep ensembles. The proposed\nanalysis shifts the focus from the uncertainty-error relationship towards\nrelevant medical and engineering factors. Our findings reveal that\ninstance-wise uncertainty is strongly related to lesion size, shape, and\ncortical involvement. Expert rater feedback confirms that similar factors\nimpede annotator confidence. Evaluations conducted on two datasets (206\npatients, almost 2000 lesions) under both in-domain and distribution-shift\nconditions highlight the utility of the framework in different scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04831", "pdf": "https://arxiv.org/pdf/2504.04831", "abs": "https://arxiv.org/abs/2504.04831", "authors": ["Sanjeev Muralikrishnan", "Niladri Shekhar Dutt", "Niloy J. Mitra"], "title": "SMF: Template-free and Rig-free Animation Transfer using Kinetic Codes", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animation retargeting involves applying a sparse motion description (e.g.,\n2D/3D keypoint sequences) to a given character mesh to produce a semantically\nplausible and temporally coherent full-body motion. Existing approaches come\nwith a mix of restrictions - they require annotated training data, assume\naccess to template-based shape priors or artist-designed deformation rigs,\nsuffer from limited generalization to unseen motion and/or shapes, or exhibit\nmotion jitter. We propose Self-supervised Motion Fields (SMF) as a\nself-supervised framework that can be robustly trained with sparse motion\nrepresentations, without requiring dataset specific annotations, templates, or\nrigs. At the heart of our method are Kinetic Codes, a novel autoencoder-based\nsparse motion encoding, that exposes a semantically rich latent space\nsimplifying large-scale training. Our architecture comprises dedicated spatial\nand temporal gradient predictors, which are trained end-to-end. The resultant\nnetwork, regularized by the Kinetic Codes's latent space, has good\ngeneralization across shapes and motions. We evaluated our method on unseen\nmotion sampled from AMASS, D4D, Mixamo, and raw monocular video for animation\ntransfer on various characters with varying shapes and topology. We report a\nnew SoTA on the AMASS dataset in the context of generalization to unseen\nmotion. Project webpage at https://motionfields.github.io/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04939", "pdf": "https://arxiv.org/pdf/2504.04939", "abs": "https://arxiv.org/abs/2504.04939", "authors": ["Naoki Wake", "Atsushi Kanehira", "Kazuhiro Sasabuchi", "Jun Takamatsu", "Katsushi Ikeuchi"], "title": "A Taxonomy of Self-Handover", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "8 pages, 8 figures, 1 table, Last updated on April 7th, 2025", "summary": "Self-handover, transferring an object between one's own hands, is a common\nbut understudied bimanual action. While it facilitates seamless transitions in\ncomplex tasks, the strategies underlying its execution remain largely\nunexplored. Here, we introduce the first systematic taxonomy of self-handover,\nderived from manual annotation of over 12 hours of cooking activity performed\nby 21 participants. Our analysis reveals that self-handover is not merely a\npassive transition, but a highly coordinated action involving anticipatory\nadjustments by both hands. As a step toward automated analysis of human\nmanipulation, we further demonstrate the feasibility of classifying\nself-handover types using a state-of-the-art vision-language model. These\nfindings offer fresh insights into bimanual coordination, underscoring the role\nof self-handover in enabling smooth task transitions-an ability essential for\nadaptive dual-arm robotics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04956", "pdf": "https://arxiv.org/pdf/2504.04956", "abs": "https://arxiv.org/abs/2504.04956", "authors": ["Jihyun Lee", "Weipeng Xu", "Alexander Richard", "Shih-En Wei", "Shunsuke Saito", "Shaojie Bai", "Te-Li Wang", "Minhyuk Sung", "Tae-Kyun", "Kim", "Jason Saragih"], "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to CVPR 2025, project page:\n  https://jyunlee.github.io/projects/rewind/", "summary": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a\none-step diffusion model for real-time, high-fidelity human motion estimation\nfrom egocentric image inputs. While an existing method for egocentric\nwhole-body (i.e., body and hands) motion estimation is non-real-time and\nacausal due to diffusion-based iterative motion refinement to capture\ncorrelations between body and hand poses, REWIND operates in a fully causal and\nreal-time manner. To enable real-time inference, we introduce (1) cascaded\nbody-hand denoising diffusion, which effectively models the correlation between\negocentric body and hand motions in a fast, feed-forward manner, and (2)\ndiffusion distillation, which enables high-quality motion estimation with a\nsingle denoising step. Our denoising diffusion model is based on a modified\nTransformer architecture, designed to causally model output motions while\nenhancing generalizability to unseen motion lengths. Additionally, REWIND\noptionally supports identity-conditioned motion estimation when identity prior\nis available. To this end, we propose a novel identity conditioning method\nbased on a small set of pose exemplars of the target identity, which further\nenhances motion estimation quality. Through extensive experiments, we\ndemonstrate that REWIND significantly outperforms the existing baselines both\nwith and without exemplar-based identity conditioning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05033", "pdf": "https://arxiv.org/pdf/2504.05033", "abs": "https://arxiv.org/abs/2504.05033", "authors": ["Jay Kamat", "JÃºlia BorrÃ s", "Carme Torras"], "title": "CloSE: A Compact Shape- and Orientation-Agnostic Cloth State Representation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Cloth manipulation is a difficult problem mainly because of the non-rigid\nnature of cloth, which makes a good representation of deformation essential. We\npresent a new representation for the deformation-state of clothes. First, we\npropose the dGLI disk representation, based on topological indices computed for\nsegments on the edges of the cloth mesh border that are arranged on a circular\ngrid. The heat-map of the dGLI disk uncovers patterns that correspond to\nfeatures of the cloth state that are consistent for different shapes, sizes of\npositions of the cloth, like the corners and the fold locations. We then\nabstract these important features from the dGLI disk onto a circle, calling it\nthe Cloth StatE representation (CloSE). This representation is compact,\ncontinuous, and general for different shapes. Finally, we show the strengths of\nthis representation in two relevant applications: semantic labeling and high-\nand low-level planning. The code, the dataset and the video can be accessed\nfrom : https://jaykamat99.github.io/close-representation", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05196", "pdf": "https://arxiv.org/pdf/2504.05196", "abs": "https://arxiv.org/abs/2504.05196", "authors": ["Tejas Sudharshan Mathai", "Sungwon Lee", "Thomas C. Shen", "Zhiyong Lu", "Ronald M. Summers"], "title": "Universal Lymph Node Detection in Multiparametric MRI with Selective Augmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Robust localization of lymph nodes (LNs) in multiparametric MRI (mpMRI) is\ncritical for the assessment of lymphadenopathy. Radiologists routinely measure\nthe size of LN to distinguish benign from malignant nodes, which would require\nsubsequent cancer staging. Sizing is a cumbersome task compounded by the\ndiverse appearances of LNs in mpMRI, which renders their measurement difficult.\nFurthermore, smaller and potentially metastatic LNs could be missed during a\nbusy clinical day. To alleviate these imaging and workflow problems, we propose\na pipeline to universally detect both benign and metastatic nodes in the body\nfor their ensuing measurement. The recently proposed VFNet neural network was\nemployed to identify LN in T2 fat suppressed and diffusion weighted imaging\n(DWI) sequences acquired by various scanners with a variety of exam protocols.\nWe also use a selective augmentation technique known as Intra-Label LISA (ILL)\nto diversify the input data samples the model sees during training, such that\nit improves its robustness during the evaluation phase. We achieved a\nsensitivity of $\\sim$83\\% with ILL vs. $\\sim$80\\% without ILL at 4 FP/vol.\nCompared with current LN detection approaches evaluated on mpMRI, we show a\nsensitivity improvement of $\\sim$9\\% at 4 FP/vol.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05231", "pdf": "https://arxiv.org/pdf/2504.05231", "abs": "https://arxiv.org/abs/2504.05231", "authors": ["CÃ©sar Leblanc", "Lukas Picek", "Benjamin Deneu", "Pierre Bonnet", "Maximilien Servajean", "RÃ©mi Palard", "Alexis Joly"], "title": "Mapping biodiversity at very-high resolution in Europe", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper describes a cascading multimodal pipeline for high-resolution\nbiodiversity mapping across Europe, integrating species distribution modeling,\nbiodiversity indicators, and habitat classification. The proposed pipeline\nfirst predicts species compositions using a deep-SDM, a multimodal model\ntrained on remote sensing, climate time series, and species occurrence data at\n50x50m resolution. These predictions are then used to generate biodiversity\nindicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM\ndesigned for species-to-habitat mapping. With this approach, continental-scale\nspecies distribution maps, biodiversity indicator maps, and habitat maps are\nproduced, providing fine-grained ecological insights. Unlike traditional\nmethods, this framework enables joint modeling of interspecies dependencies,\nbias-aware training with heterogeneous presence-absence data, and large-scale\ninference from multi-source remote sensing inputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
