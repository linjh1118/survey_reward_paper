{"id": "2504.05535", "pdf": "https://arxiv.org/pdf/2504.05535", "abs": "https://arxiv.org/abs/2504.05535", "authors": ["M-A-P Team", "Siwei Wu", "Jincheng Ren", "Xinrun Du", "Shuyue Guo", "Xingwei Qu", "Yiming Liang", "Jie Liu", "Yunwen Li", "Tianyu Zheng", "Boyu Feng", "Huaqing Yuan", "Zenith Wang", "Jiaheng Liu", "Wenhao Huang", "Chenglin Cai", "Haoran Que", "Jian Yang", "Yuelin Bai", "Zekun Moore Wang", "Zhouliang Yu", "Qunshu Lin", "Ding Pan", "Yuchen Jiang", "Tiannan Wang", "Wangchunshu Zhou", "Shenzhi Wang", "Xingyuan Bu", "Minghao Liu", "Guoyin Wang", "Ge Zhang", "Chenghua Lin"], "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinese preference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of human preference datasets.\nTo address these challenges, we design an LLM-based Chinese preference dataset\nannotation pipeline with no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\nAlignBench \\citep{liu2024alignbenchbenchmarkingchinesealignment} show that that\nCOIG-P significantly outperforms other Chinese preference datasets, and it\nbrings significant performance improvements ranging from 2% to 12% for the\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\nand our experiments show that it is comparable to GPT-4o in identifying\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\ncodes and data are released in\nhttps://github.com/multimodal-art-projection/COIG-P.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "human preference", "annotation"], "score": 6}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05898", "pdf": "https://arxiv.org/pdf/2504.05898", "abs": "https://arxiv.org/abs/2504.05898", "authors": ["Peerat Limkonchotiwat", "Kanruethai Masuk", "Surapon Nonesung", "Chalermpun Mai-On", "Sarana Nutanong", "Wuttikorn Ponwitayarat", "Potsawee Manakul"], "title": "Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human Evaluation", "categories": ["cs.CL"], "comment": "Datasets and codes are available at\n  https://github.com/mrpeerat/Thai_local_benchmark", "summary": "Large language models show promising results in various NLP tasks. Despite\nthese successes, the robustness and consistency of LLMs in underrepresented\nlanguages remain largely unexplored, especially concerning local dialects.\nExisting benchmarks also focus on main dialects, neglecting LLMs' ability on\nlocal dialect texts. In this paper, we introduce a Thai local dialect benchmark\ncovering Northern (Lanna), Northeastern (Isan), and Southern (Dambro) Thai,\nevaluating LLMs on five NLP tasks: summarization, question answering,\ntranslation, conversation, and food-related tasks. Furthermore, we propose a\nhuman evaluation guideline and metric for Thai local dialects to assess\ngeneration fluency and dialect-specific accuracy. Results show that LLM\nperformance declines significantly in local Thai dialects compared to standard\nThai, with only proprietary models like GPT-4o and Gemini2 demonstrating some\nfluency", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy", "summarization", "question answering"], "score": 6}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05324", "pdf": "https://arxiv.org/pdf/2504.05324", "abs": "https://arxiv.org/abs/2504.05324", "authors": ["Chandana Sree Mala", "Gizem Gezici", "Fosca Giannotti"], "title": "Hybrid Retrieval for Hallucination Mitigation in Large Language Models: A Comparative Analysis", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in language comprehension and generation\nbut are prone to hallucinations, producing factually incorrect or unsupported\noutputs. Retrieval Augmented Generation (RAG) systems address this issue by\ngrounding LLM responses with external knowledge. This study evaluates the\nrelationship between retriever effectiveness and hallucination reduction in\nLLMs using three retrieval approaches: sparse retrieval based on BM25 keyword\nsearch, dense retrieval using semantic search with Sentence Transformers, and a\nproposed hybrid retrieval module. The hybrid module incorporates query\nexpansion and combines the results of sparse and dense retrievers through a\ndynamically weighted Reciprocal Rank Fusion score. Using the HaluBench dataset,\na benchmark for hallucinations in question answering tasks, we assess retrieval\nperformance with metrics such as mean average precision and normalised\ndiscounted cumulative gain, focusing on the relevance of the top three\nretrieved documents. Results show that the hybrid retriever achieves better\nrelevance scores, outperforming both sparse and dense retrievers. Further\nevaluation of LLM-generated answers against ground truth using metrics such as\naccuracy, hallucination rate, and rejection rate reveals that the hybrid\nretriever achieves the highest accuracy on fails, the lowest hallucination\nrate, and the lowest rejection rate. These findings highlight the hybrid\nretriever's ability to enhance retrieval relevance, reduce hallucination rates,\nand improve LLM reliability, emphasising the importance of advanced retrieval\ntechniques in mitigating hallucinations and improving response accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy", "question answering"], "score": 6}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05693", "pdf": "https://arxiv.org/pdf/2504.05693", "abs": "https://arxiv.org/abs/2504.05693", "authors": ["Aniket Deroy", "Subhankar Maity"], "title": "STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 6 figures", "summary": "Automatically assessing question quality is crucial for educators as it saves\ntime, ensures consistency, and provides immediate feedback for refining\nteaching materials. We propose a novel methodology called STRIVE (Structured\nThinking and Refinement with multiLLMs for Improving Verified Question\nEstimation) using a series of Large Language Models (LLMs) for automatic\nquestion evaluation. This approach aims to improve the accuracy and depth of\nquestion quality assessment, ultimately supporting diverse learners and\nenhancing educational practices. The method estimates question quality in an\nautomated manner by generating multiple evaluations based on the strengths and\nweaknesses of the provided question and then choosing the best solution\ngenerated by the LLM. Then the process is improved by iterative review and\nresponse with another LLM until the evaluation metric values converge. This\nsophisticated method of evaluating question quality improves the estimation of\nquestion quality by automating the task of question quality evaluation.\nCorrelation scores show that using this proposed method helps to improve\ncorrelation with human judgments compared to the baseline method. Error\nanalysis shows that metrics like relevance and appropriateness improve\nsignificantly relative to human judgments by using STRIVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06227", "pdf": "https://arxiv.org/pdf/2504.06227", "abs": "https://arxiv.org/abs/2504.06227", "authors": ["Krithi Shailya", "Shreya Rajpal", "Gokul S Krishnan", "Balaraman Ravindran"], "title": "LExT: Towards Evaluating Trustworthiness of Natural Language Explanations", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly integrated into\nhigh-stakes domains, there have been several approaches proposed toward\ngenerating natural language explanations. These explanations are crucial for\nenhancing the interpretability of a model, especially in sensitive domains like\nhealthcare, where transparency and reliability are key. In light of such\nexplanations being generated by LLMs and its known concerns, there is a growing\nneed for robust evaluation frameworks to assess model-generated explanations.\nNatural Language Generation metrics like BLEU and ROUGE capture syntactic and\nsemantic accuracies but overlook other crucial aspects such as factual\naccuracy, consistency, and faithfulness. To address this gap, we propose a\ngeneral framework for quantifying trustworthiness of natural language\nexplanations, balancing Plausibility and Faithfulness, to derive a\ncomprehensive Language Explanation Trustworthiness Score (LExT) (The code and\nset up to reproduce our experiments are publicly available at\nhttps://github.com/cerai-iitm/LExT). Applying our domain-agnostic framework to\nthe healthcare domain using public medical datasets, we evaluate six models,\nincluding domain-specific and general-purpose models. Our findings demonstrate\nsignificant differences in their ability to generate trustworthy explanations.\nOn comparing these explanations, we make interesting observations such as\ninconsistencies in Faithfulness demonstrated by general-purpose models and\ntheir tendency to outperform domain-specific fine-tuned models. This work\nfurther highlights the importance of using a tailored evaluation framework to\nassess natural language explanations in sensitive fields, providing a\nfoundation for improving the trustworthiness and transparency of language\nmodels in healthcare and beyond.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05783", "pdf": "https://arxiv.org/pdf/2504.05783", "abs": "https://arxiv.org/abs/2504.05783", "authors": ["Zijie Song", "Zhenzhen Hu", "Yixiao Ma", "Jia Li", "Richang Hong"], "title": "Video Flow as Time Series: Discovering Temporal Consistency and Variability for VideoQA", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Question Answering (VideoQA) is a complex video-language task that\ndemands a sophisticated understanding of both visual content and temporal\ndynamics. Traditional Transformer-style architectures, while effective in\nintegrating multimodal data, often simplify temporal dynamics through\npositional encoding and fail to capture non-linear interactions within video\nsequences. In this paper, we introduce the Temporal Trio Transformer (T3T), a\nnovel architecture that models time consistency and time variability. The T3T\nintegrates three key components: Temporal Smoothing (TS), Temporal Difference\n(TD), and Temporal Fusion (TF). The TS module employs Brownian Bridge for\ncapturing smooth, continuous temporal transitions, while the TD module\nidentifies and encodes significant temporal variations and abrupt changes\nwithin the video content. Subsequently, the TF module synthesizes these\ntemporal features with textual cues, facilitating a deeper contextual\nunderstanding and response accuracy. The efficacy of the T3T is demonstrated\nthrough extensive testing on multiple VideoQA benchmark datasets. Our results\nunderscore the importance of a nuanced approach to temporal modeling in\nimproving the accuracy and depth of video-based question answering.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05882", "pdf": "https://arxiv.org/pdf/2504.05882", "abs": "https://arxiv.org/abs/2504.05882", "authors": ["Luca Barco", "Giacomo Blanco", "Gaetano Chiriaco", "Alessia Intini", "Luigi La Riccia", "Vittorio Scolamiero", "Piero Boccardo", "Paolo Garza", "Fabrizio Dominici"], "title": "Turin3D: Evaluating Adaptation Strategies under Label Scarcity in Urban LiDAR Segmentation with Semi-Supervised Techniques", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPRW2025 - USM3D", "summary": "3D semantic segmentation plays a critical role in urban modelling, enabling\ndetailed understanding and mapping of city environments. In this paper, we\nintroduce Turin3D: a new aerial LiDAR dataset for point cloud semantic\nsegmentation covering an area of around 1.43 km2 in the city centre of Turin\nwith almost 70M points. We describe the data collection process and compare\nTurin3D with others previously proposed in the literature. We did not fully\nannotate the dataset due to the complexity and time-consuming nature of the\nprocess; however, a manual annotation process was performed on the validation\nand test sets, to enable a reliable evaluation of the proposed techniques. We\nfirst benchmark the performances of several point cloud semantic segmentation\nmodels, trained on the existing datasets, when tested on Turin3D, and then\nimprove their performances by applying a semi-supervised learning technique\nleveraging the unlabelled training set. The dataset will be publicly available\nto support research in outdoor point cloud segmentation, with particular\nrelevance for self-supervised and semi-supervised learning approaches given the\nabsence of ground truth annotations for the training set.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06185", "pdf": "https://arxiv.org/pdf/2504.06185", "abs": "https://arxiv.org/abs/2504.06185", "authors": ["Vanessa Borst", "Timo Dittus", "Tassilo Dege", "Astrid Schmieder", "Samuel Kounev"], "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care", "categories": ["cs.CV", "cs.AI"], "comment": "Main paper: 17 pages; supplementary material: 16 pages; paper\n  submitted to the application track of the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2025)", "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05422", "pdf": "https://arxiv.org/pdf/2504.05422", "abs": "https://arxiv.org/abs/2504.05422", "authors": ["Yue Yao", "Mohamed-Khalil Bouzidi", "Daniel Goehring", "Joerg Reichardt"], "title": "EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "As the prediction horizon increases, predicting the future evolution of\ntraffic scenes becomes increasingly difficult due to the multi-modal nature of\nagent motion. Most state-of-the-art (SotA) prediction models primarily focus on\nforecasting the most likely future. However, for the safe operation of\nautonomous vehicles, it is equally important to cover the distribution for\nplausible motion alternatives. To address this, we introduce EP-Diffuser, a\nnovel parameter-efficient diffusion-based generative model designed to capture\nthe distribution of possible traffic scene evolutions. Conditioned on road\nlayout and agent history, our model acts as a predictor and generates diverse,\nplausible scene continuations. We benchmark EP-Diffuser against two SotA models\nin terms of accuracy and plausibility of predictions on the Argoverse 2\ndataset. Despite its significantly smaller model size, our approach achieves\nboth highly accurate and plausible traffic scene predictions. We further\nevaluate model generalization ability in an out-of-distribution (OoD) test\nsetting using Waymo Open dataset and show superior robustness of our approach.\nThe code and model checkpoints can be found here:\nhttps://github.com/continental/EP-Diffuser.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05607", "pdf": "https://arxiv.org/pdf/2504.05607", "abs": "https://arxiv.org/abs/2504.05607", "authors": ["Qian-Wen Zhang", "Fang Li", "Jie Wang", "Lingfeng Qiao", "Yifei Yu", "Di Yin", "Xing Sun"], "title": "FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Extractive reading comprehension systems are designed to locate the correct\nanswer to a question within a given text. However, a persistent challenge lies\nin ensuring these models maintain high accuracy in answering questions while\nreliably recognizing unanswerable queries. Despite significant advances in\nlarge language models (LLMs) for reading comprehension, this issue remains\ncritical, particularly as the length of supported contexts continues to expand.\nTo address this challenge, we propose an innovative data augmentation\nmethodology grounded in a multi-agent collaborative framework. Unlike\ntraditional methods, such as the costly human annotation process required for\ndatasets like SQuAD 2.0, our method autonomously generates evidence-based\nquestion-answer pairs and systematically constructs unanswerable questions.\nUsing this methodology, we developed the FactGuard-Bench dataset, which\ncomprises 25,220 examples of both answerable and unanswerable question\nscenarios, with context lengths ranging from 8K to 128K. Experimental\nevaluations conducted on seven popular LLMs reveal that even the most advanced\nmodels achieve only 61.79% overall accuracy. Furthermore, we emphasize the\nimportance of a model's ability to reason about unanswerable questions to avoid\ngenerating plausible but incorrect answers. By implementing efficient data\nselection and generation within the multi-agent collaborative framework, our\nmethod significantly reduces the traditionally high costs associated with\nmanual annotation and provides valuable insights for the training and\noptimization of LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05575", "pdf": "https://arxiv.org/pdf/2504.05575", "abs": "https://arxiv.org/abs/2504.05575", "authors": ["Belal Alsinglawi", "Chris McCarthy", "Sara Webb", "Christopher Fluke", "Navid Toosy Saidy"], "title": "A Lightweight Large Vision-language Model for Multimodal Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures", "summary": "Medical Visual Question Answering (VQA) enhances clinical decision-making by\nenabling systems to interpret medical images and answer clinical queries.\nHowever, developing efficient, high-performance VQA models is challenging due\nto the complexity of medical imagery and diverse modalities. In this paper, we\nintroduce a lightweight, multimodal VQA model integrating BiomedCLIP for image\nfeature extraction and LLaMA-3 for text processing. Designed for medical VQA\ntasks, our model achieves state-of-the-art performance on the OmniMedVQA\ndataset. With approximately 8 billion parameters, it requires only two NVIDIA\n40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our\nresults show 73.4% accuracy for open-end questions, surpassing existing models\nand validating its potential for real-world medical applications. Key\ncontributions include a specialized multimodal VQA model, a resource-efficient\narchitecture, and strong performance in answering open-ended clinical\nquestions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05583", "pdf": "https://arxiv.org/pdf/2504.05583", "abs": "https://arxiv.org/abs/2504.05583", "authors": ["Jiahang Li", "Shibo Xue", "Yong Su"], "title": "Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification", "categories": ["cs.CV", "I.4.9; I.5.1"], "comment": "10 pages, 5 figures, 3 tables, URL:\n  https://szyyjl.github.io/eye_tracking_data.github.io/", "summary": "Inspired by human visual attention, deep neural networks have widely adopted\nattention mechanisms to learn locally discriminative attributes for challenging\nvisual classification tasks. However, existing approaches primarily emphasize\nthe representation of such features while neglecting their precise\nlocalization, which often leads to misclassification caused by shortcut biases.\nThis limitation becomes even more pronounced when models are evaluated on\ntransfer or out-of-distribution datasets. In contrast, humans are capable of\nleveraging prior object knowledge to quickly localize and compare fine-grained\nattributes, a capability that is especially crucial in complex and\nhigh-variance classification scenarios. Motivated by this, we introduce\nGaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence\ngaze encoder that models the precise sequential localization of human attention\non distinct local attributes. In parallel, a Vision Transformer (ViT) is\nemployed to learn the sequential representation of image content. Through\ncross-modal fusion, our framework integrates human gaze priors with\nmachine-derived visual sequences, effectively correcting inaccurate\nlocalization in image feature representations. Extensive qualitative and\nquantitative experiments demonstrate that gaze-guided cognitive cues\nsignificantly enhance classification accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05623", "pdf": "https://arxiv.org/pdf/2504.05623", "abs": "https://arxiv.org/abs/2504.05623", "authors": ["Mahmoud Afifi", "Luxi Zhao", "Abhijith Punnappurath", "Mohammed A. Abdelsalam", "Ran Zhang", "Michael S. Brown"], "title": "Time-Aware Auto White Balance in Mobile Photography", "categories": ["cs.CV"], "comment": null, "summary": "Cameras rely on auto white balance (AWB) to correct undesirable color casts\ncaused by scene illumination and the camera's spectral sensitivity. This is\ntypically achieved using an illuminant estimator that determines the global\ncolor cast solely from the color information in the camera's raw sensor image.\nMobile devices provide valuable additional metadata-such as capture timestamp\nand geolocation-that offers strong contextual clues to help narrow down the\npossible illumination solutions. This paper proposes a lightweight illuminant\nestimation method that incorporates such contextual metadata, along with\nadditional capture information and image colors, into a compact model (~5K\nparameters), achieving promising results, matching or surpassing larger models.\nTo validate our method, we introduce a dataset of 3,224 smartphone images with\ncontextual metadata collected at various times of day and under diverse\nlighting conditions. The dataset includes ground-truth illuminant colors,\ndetermined using a color chart, and user-preferred illuminants validated\nthrough a user study, providing a comprehensive benchmark for AWB evaluation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05673", "pdf": "https://arxiv.org/pdf/2504.05673", "abs": "https://arxiv.org/abs/2504.05673", "authors": ["Dongjun Qian", "Kai Su", "Yiming Tan", "Qishuai Diao", "Xian Wu", "Chang Liu", "Bingyue Peng", "Zehuan Yuan"], "title": "VC-LLM: Automated Advertisement Video Creation from Raw Footage using Multi-modal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "As short videos have risen in popularity, the role of video content in\nadvertising has become increasingly significant. Typically, advertisers record\na large amount of raw footage about the product and then create numerous\ndifferent short-form advertisement videos based on this raw footage. Creating\nsuch videos mainly involves editing raw footage and writing advertisement\nscripts, which requires a certain level of creative ability. It is usually\nchallenging to create many different video contents for the same product, and\nmanual efficiency is often low. In this paper, we present VC-LLM, a framework\npowered by Large Language Models for the automatic creation of high-quality\nshort-form advertisement videos. Our approach leverages high-resolution spatial\ninput and low-resolution temporal input to represent video clips more\neffectively, capturing both fine-grained visual details and broader temporal\ndynamics. In addition, during training, we incorporate supplementary\ninformation generated by rewriting the ground truth text, ensuring that all key\noutput information can be directly traced back to the input, thereby reducing\nmodel hallucinations. We also designed a benchmark to evaluate the quality of\nthe created videos. Experiments show that VC-LLM based on GPT-4o can produce\nvideos comparable to those created by humans. Furthermore, we collected\nnumerous high-quality short advertisement videos to create a pre-training\ndataset and manually cleaned a portion of the data to construct a high-quality\nfine-tuning dataset. Experiments indicate that, on the benchmark, the VC-LLM\nbased on fine-tuned LLM can produce videos with superior narrative logic\ncompared to those created by the VC-LLM based on GPT-4o.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05706", "pdf": "https://arxiv.org/pdf/2504.05706", "abs": "https://arxiv.org/abs/2504.05706", "authors": ["Fida Mohammad Thoker", "Letian Jiang", "Chen Zhao", "Piyush Bagad", "Hazel Doughty", "Bernard Ghanem", "Cees G. M. Snoek"], "title": "SEVERE++: Evaluating Benchmark Sensitivity in Generalization of Video Representation Learning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Continued advances in self-supervised learning have led to significant\nprogress in video representation learning, offering a scalable alternative to\nsupervised approaches by removing the need for manual annotations. Despite\nstrong performance on standard action recognition benchmarks, video\nself-supervised learning methods are largely evaluated under narrow protocols,\ntypically pretraining on Kinetics-400 and fine-tuning on similar datasets,\nlimiting our understanding of their generalization in real world scenarios. In\nthis work, we present a comprehensive evaluation of modern video\nself-supervised models, focusing on generalization across four key downstream\nfactors: domain shift, sample efficiency, action granularity, and task\ndiversity. Building on our prior work analyzing benchmark sensitivity in\nCNN-based contrastive learning, we extend the study to cover state-of-the-art\ntransformer-based video-only and video-text models. Specifically, we benchmark\n12 transformer-based methods (7 video-only, 5 video-text) and compare them to\n10 CNN-based methods, totaling over 1100 experiments across 8 datasets and 7\ndownstream tasks. Our analysis shows that, despite architectural advances,\ntransformer-based models remain sensitive to downstream conditions. No method\ngeneralizes consistently across all factors, video-only transformers perform\nbetter under domain shifts, CNNs outperform for fine-grained tasks, and\nvideo-text models often underperform despite large scale pretraining. We also\nfind that recent transformer models do not consistently outperform earlier\napproaches. Our findings provide a detailed view of the strengths and\nlimitations of current video SSL methods and offer a unified benchmark for\nevaluating generalization in video representation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06153", "pdf": "https://arxiv.org/pdf/2504.06153", "abs": "https://arxiv.org/abs/2504.06153", "authors": ["Akash Kumar", "Ashlesha Kumar", "Vibhav Vineet", "Yogesh S Rawat"], "title": "A Large-Scale Analysis on Contextual Self-Supervised Video Representation Learning", "categories": ["cs.CV"], "comment": "CVPR'25 Workshop: 6th Data-Efficient Workshop", "summary": "Self-supervised learning has emerged as a powerful paradigm for label-free\nmodel pretraining, particularly in the video domain, where manual annotation is\ncostly and time-intensive. However, existing self-supervised approaches employ\ndiverse experimental setups, making direct comparisons challenging due to the\nabsence of a standardized benchmark. In this work, we establish a unified\nbenchmark that enables fair comparisons across different methods. Additionally,\nwe systematically investigate five critical aspects of self-supervised learning\nin videos: (1) dataset size, (2) model complexity, (3) data distribution, (4)\ndata noise, and (5) feature representations. To facilitate this study, we\nevaluate six self-supervised learning methods across six network architectures,\nconducting extensive experiments on five benchmark datasets and assessing\nperformance on two distinct downstream tasks. Our analysis reveals key insights\ninto the interplay between pretraining strategies, dataset characteristics,\npretext tasks, and model architectures. Furthermore, we extend these findings\nto Video Foundation Models (ViFMs), demonstrating their relevance in\nlarge-scale video representation learning. Finally, leveraging these insights,\nwe propose a novel approach that significantly reduces training data\nrequirements while surpassing state-of-the-art methods that rely on 10% more\npretraining data. We believe this work will guide future research toward a\ndeeper understanding of self-supervised video representation learning and its\nbroader implications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05696", "pdf": "https://arxiv.org/pdf/2504.05696", "abs": "https://arxiv.org/abs/2504.05696", "authors": ["Sidhiq Mardianta", "Affandy", "Catur Supriyanto", "Catur Supriyanto", "Adi Wijaya"], "title": "Diabetic Retinopathy Detection Based on Convolutional Neural Networks with SMOTE and CLAHE Techniques Applied to Fundus Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC"], "comment": "6 pages, 6 figures, 2 tables", "summary": "Diabetic retinopathy (DR) is one of the major complications in diabetic\npatients' eyes, potentially leading to permanent blindness if not detected\ntimely. This study aims to evaluate the accuracy of artificial intelligence\n(AI) in diagnosing DR. The method employed is the Synthetic Minority\nOver-sampling Technique (SMOTE) algorithm, applied to identify DR and its\nseverity stages from fundus images using the public dataset \"APTOS 2019\nBlindness Detection.\" Literature was reviewed via ScienceDirect, ResearchGate,\nGoogle Scholar, and IEEE Xplore. Classification results using Convolutional\nNeural Network (CNN) showed the best performance for the binary classes normal\n(0) and DR (1) with an accuracy of 99.55%, precision of 99.54%, recall of\n99.54%, and F1-score of 99.54%. For the multiclass classification No_DR (0),\nMild (1), Moderate (2), Severe (3), Proliferate_DR (4), the accuracy was\n95.26%, precision 95.26%, recall 95.17%, and F1-score 95.23%. Evaluation using\nthe confusion matrix yielded results of 99.68% for binary classification and\n96.65% for multiclass. This study highlights the significant potential in\nenhancing the accuracy of DR diagnosis compared to traditional human analysis", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05420", "pdf": "https://arxiv.org/pdf/2504.05420", "abs": "https://arxiv.org/abs/2504.05420", "authors": ["Steven Koniaev", "Ori Ernst", "Jackie Chi Kit Cheung"], "title": "PreSumm: Predicting Summarization Performance Without Summarizing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite recent advancements in automatic summarization, state-of-the-art\nmodels do not summarize all documents equally well, raising the question: why?\nWhile prior research has extensively analyzed summarization models, little\nattention has been given to the role of document characteristics in influencing\nsummarization performance. In this work, we explore two key research questions.\nFirst, do documents exhibit consistent summarization quality across multiple\nsystems? If so, can we predict a document's summarization performance without\ngenerating a summary? We answer both questions affirmatively and introduce\nPreSumm, a novel task in which a system predicts summarization performance\nbased solely on the source document. Our analysis sheds light on common\nproperties of documents with low PreSumm scores, revealing that they often\nsuffer from coherence issues, complex content, or a lack of a clear main theme.\nIn addition, we demonstrate PreSumm's practical utility in two key\napplications: improving hybrid summarization workflows by identifying documents\nthat require manual summarization and enhancing dataset quality by filtering\noutliers and noisy documents. Overall, our findings highlight the critical role\nof document properties in summarization performance and offer insights into the\nlimitations of current systems that could serve as the basis for future\nimprovements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05400", "pdf": "https://arxiv.org/pdf/2504.05400", "abs": "https://arxiv.org/abs/2504.05400", "authors": ["Sihang Li", "Zeyu Jiang", "Grace Chen", "Chenyang Xu", "Siqi Tan", "Xue Wang", "Irving Fang", "Kristof Zyskowski", "Shannon P. McPherron", "Radu Iovita", "Chen Feng", "Jing Zhang"], "title": "GARF: Learning Generalizable 3D Reassembly for Real-World Fractures", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 11 figures. Project Page https://ai4ce.github.io/GARF/", "summary": "3D reassembly is a challenging spatial intelligence task with broad\napplications across scientific domains. While large-scale synthetic datasets\nhave fueled promising learning-based approaches, their generalizability to\ndifferent domains is limited. Critically, it remains uncertain whether models\ntrained on synthetic datasets can generalize to real-world fractures where\nbreakage patterns are more complex. To bridge this gap, we propose GARF, a\ngeneralizable 3D reassembly framework for real-world fractures. GARF leverages\nfracture-aware pretraining to learn fracture features from individual\nfragments, with flow matching enabling precise 6-DoF alignments. At inference\ntime, we introduce one-step preassembly, improving robustness to unseen objects\nand varying numbers of fractures. In collaboration with archaeologists,\npaleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset\nfor vision and learning communities, featuring real-world fracture types across\nceramics, bones, eggshells, and lithics. Comprehensive experiments have shown\nour approach consistently outperforms state-of-the-art methods on both\nsynthetic and real-world datasets, achieving 82.87\\% lower rotation error and\n25.15\\% higher part accuracy. This sheds light on training on synthetic data to\nadvance real-world 3D puzzle solving, demonstrating its strong generalization\nacross unseen object shapes and diverse fracture types.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05506", "pdf": "https://arxiv.org/pdf/2504.05506", "abs": "https://arxiv.org/abs/2504.05506", "authors": ["Ahmed Masry", "Mohammed Saidul Islam", "Mahir Ahmed", "Aayush Bajaj", "Firoz Kabir", "Aaryaman Kartha", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shadikur Rahman", "Mehrad Shahmohammadi", "Megh Thakkar", "Md Rizwan Parvez", "Enamul Hoque", "Shafiq Joty"], "title": "ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Charts are ubiquitous, as people often use them to analyze data, answer\nquestions, and discover critical insights. However, performing complex\nanalytical tasks with charts requires significant perceptual and cognitive\neffort. Chart Question Answering (CQA) systems automate this process by\nenabling models to interpret and reason with visual representations of data.\nHowever, existing benchmarks like ChartQA lack real-world diversity and have\nrecently shown performance saturation with modern large vision-language models\n(LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark\nthat includes 1,341 charts from 157 diverse sources, spanning various chart\ntypes, including infographics and dashboards, and featuring 1,948 questions in\nvarious types, such as multiple-choice, conversational, hypothetical, and\nunanswerable questions, to better reflect real-world challenges. Our\nevaluations with 21 models show a substantial performance drop for LVLMs on\nChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on\nChartQAPro, underscoring the complexity of chart reasoning. We complement our\nfindings with detailed error analyses and ablation studies, identifying key\nchallenges and opportunities for advancing LVLMs in chart understanding and\nreasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05523", "pdf": "https://arxiv.org/pdf/2504.05523", "abs": "https://arxiv.org/abs/2504.05523", "authors": ["Elisabeth Fittschen", "Sabrina Li", "Tom Lippincott", "Leshem Choshsem", "Craig Messner"], "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "testbed"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05457", "pdf": "https://arxiv.org/pdf/2504.05457", "abs": "https://arxiv.org/abs/2504.05457", "authors": ["Vésteinn Snæbjarnarson", "Kevin Du", "Niklas Stoehr", "Serge Belongie", "Ryan Cotterell", "Nico Lang", "Stella Frank"], "title": "Taxonomy-Aware Evaluation of Vision-Language Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "When a vision-language model (VLM) is prompted to identify an entity depicted\nin an image, it may answer 'I see a conifer,' rather than the specific label\n'norway spruce'. This raises two issues for evaluation: First, the\nunconstrained generated text needs to be mapped to the evaluation label space\n(i.e., 'conifer'). Second, a useful classification measure should give partial\ncredit to less-specific, but not incorrect, answers ('norway spruce' being a\ntype of 'conifer'). To meet these requirements, we propose a framework for\nevaluating unconstrained text predictions, such as those generated from a\nvision-language model, against a taxonomy. Specifically, we propose the use of\nhierarchical precision and recall measures to assess the level of correctness\nand specificity of predictions with regard to a taxonomy. Experimentally, we\nfirst show that existing text similarity measures do not capture taxonomic\nsimilarity well. We then develop and compare different methods to map textual\nVLM predictions onto a taxonomy. This allows us to compute hierarchical\nsimilarity measures between the generated text and the ground truth labels.\nFinally, we analyze modern VLMs on fine-grained visual classification tasks\nbased on our proposed taxonomic evaluation scheme.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05603", "pdf": "https://arxiv.org/pdf/2504.05603", "abs": "https://arxiv.org/abs/2504.05603", "authors": ["Naman Bhargava", "Mohammed I. Radaideh", "O Hwang Kwon", "Aditi Verma", "Majdi I. Radaideh"], "title": "On the Impact of Language Nuances on Sentiment Analysis with Large Language Models: Paraphrasing, Sarcasm, and Emojis", "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 10 Tables, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, including sentiment analysis. However, data\nquality--particularly when sourced from social media--can significantly impact\ntheir accuracy. This research explores how textual nuances, including emojis\nand sarcasm, affect sentiment analysis, with a particular focus on improving\ndata quality through text paraphrasing techniques. To address the lack of\nlabeled sarcasm data, the authors created a human-labeled dataset of 5929\ntweets that enabled the assessment of LLM in various sarcasm contexts. The\nresults show that when topic-specific datasets, such as those related to\nnuclear power, are used to finetune LLMs these models are not able to\ncomprehend accurate sentiment in presence of sarcasm due to less diverse text,\nrequiring external interventions like sarcasm removal to boost model accuracy.\nSarcasm removal led to up to 21% improvement in sentiment accuracy, as LLMs\ntrained on nuclear power-related content struggled with sarcastic tweets,\nachieving only 30% accuracy. In contrast, LLMs trained on general tweet\ndatasets, covering a broader range of topics, showed considerable improvements\nin predicting sentiment for sarcastic tweets (60% accuracy), indicating that\nincorporating general text data can enhance sarcasm detection. The study also\nutilized adversarial text augmentation, showing that creating synthetic text\nvariants by making minor changes significantly increased model robustness and\naccuracy for sarcastic tweets (approximately 85%). Additionally, text\nparaphrasing of tweets with fragmented language transformed around 40% of the\ntweets with low-confidence labels into high-confidence ones, improving LLMs\nsentiment analysis accuracy by 6%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05468", "pdf": "https://arxiv.org/pdf/2504.05468", "abs": "https://arxiv.org/abs/2504.05468", "authors": ["Thanos Delatolas", "Vicky Kalogeiton", "Dim P. Papadopoulos"], "title": "Studying Image Diffusion Features for Zero-Shot Video Object Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPRW2025", "summary": "This paper investigates the use of large-scale diffusion models for Zero-Shot\nVideo Object Segmentation (ZS-VOS) without fine-tuning on video data or\ntraining on any image segmentation data. While diffusion models have\ndemonstrated strong visual representations across various tasks, their direct\napplication to ZS-VOS remains underexplored. Our goal is to find the optimal\nfeature extraction process for ZS-VOS by identifying the most suitable time\nstep and layer from which to extract features. We further analyze the affinity\nof these features and observe a strong correlation with point correspondences.\nThrough extensive experiments on DAVIS-17 and MOSE, we find that diffusion\nmodels trained on ImageNet outperform those trained on larger, more diverse\ndatasets for ZS-VOS. Additionally, we highlight the importance of point\ncorrespondences in achieving high segmentation accuracy, and we yield\nstate-of-the-art results in ZS-VOS. Finally, our approach performs on par with\nmodels trained on expensive image segmentation datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05683", "pdf": "https://arxiv.org/pdf/2504.05683", "abs": "https://arxiv.org/abs/2504.05683", "authors": ["Subhankar Maity", "Aniket Deroy", "Sudeshna Sarkar"], "title": "Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs Ready for HR Spoken Interview Transcript Analysis?", "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 24 figures", "summary": "This research paper presents a comprehensive analysis of the performance of\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\ncomparison to expert human evaluators in providing scores, identifying errors,\nand offering feedback and improvement suggestions to candidates during mock HR\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\nsourced from real-world HR interview scenarios. Our findings reveal that\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\ncommendable performance and are capable of producing evaluations comparable to\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\nin providing scores comparable to human experts in terms of human evaluation\nmetrics, they frequently fail to identify errors and offer specific actionable\nadvice for candidate performance improvement in HR interviews. Our research\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\nconducive for automatic deployment in an HR interview assessment. Instead, our\nfindings advocate for a human-in-the-loop approach, to incorporate manual\nchecks for inconsistencies and provisions for improving feedback quality as a\nmore suitable strategy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05689", "pdf": "https://arxiv.org/pdf/2504.05689", "abs": "https://arxiv.org/abs/2504.05689", "authors": ["Xitao Li", "Haijun Wang", "Jiang Wu", "Ting Liu"], "title": "Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Conversational large language models (LLMs) have gained widespread attention\ndue to their instruction-following capabilities. To ensure conversational LLMs\nfollow instructions, role separators are employed to distinguish between\ndifferent participants in a conversation. However, incorporating role\nseparators introduces potential vulnerabilities. Misusing roles can lead to\nprompt injection attacks, which can easily misalign the model's behavior with\nthe user's intentions, raising significant security concerns. Although various\nprompt injection attacks have been proposed, recent research has largely\noverlooked the impact of role separators on safety. This highlights the\ncritical need to thoroughly understand the systemic weaknesses in dialogue\nsystems caused by role separators. This paper identifies modeling weaknesses\ncaused by role separators. Specifically, we observe a strong positional bias\nassociated with role separators, which is inherent in the format of dialogue\nmodeling and can be triggered by the insertion of role separators. We further\ndevelop the Separators Injection Attack (SIA), a new orthometric attack based\non role separators. The experiment results show that SIA is efficient and\nextensive in manipulating model behavior with an average gain of 18.2% for\nmanual methods and enhances the attack success rate to 100% with automatic\nmethods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "dialogue"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05541", "pdf": "https://arxiv.org/pdf/2504.05541", "abs": "https://arxiv.org/abs/2504.05541", "authors": ["Yunlong Tang", "Jing Bi", "Chao Huang", "Susan Liang", "Daiki Shimada", "Hang Hua", "Yunzhong Xiao", "Yizhi Song", "Pinxin Liu", "Mingqian Feng", "Junjia Guo", "Zhuo Liu", "Luchuan Song", "Ali Vosoughi", "Jinxi He", "Liu He", "Zeliang Zhang", "Jiebo Luo", "Chenliang Xu"], "title": "Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting", "categories": ["cs.CV"], "comment": null, "summary": "We present CAT-V (Caption AnyThing in Video), a training-free framework for\nfine-grained object-centric video captioning that enables detailed descriptions\nof user-selected objects through time. CAT-V integrates three key components: a\nSegmenter based on SAMURAI for precise object segmentation across frames, a\nTemporal Analyzer powered by TRACE-Uni for accurate event boundary detection\nand temporal analysis, and a Captioner using InternVL-2.5 for generating\ndetailed object-centric descriptions. Through spatiotemporal visual prompts and\nchain-of-thought reasoning, our framework generates detailed, temporally-aware\ndescriptions of objects' attributes, actions, statuses, interactions, and\nenvironmental contexts without requiring additional training data. CAT-V\nsupports flexible user interactions through various visual prompts (points,\nbounding boxes, and irregular regions) and maintains temporal sensitivity by\ntracking object states and interactions across different time segments. Our\napproach addresses limitations of existing video captioning methods, which\neither produce overly abstract descriptions or lack object-level precision,\nenabling fine-grained, object-specific descriptions while maintaining temporal\ncoherence and spatial accuracy. The GitHub repository for this project is\navailable at https://github.com/yunlong10/CAT-V", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05702", "pdf": "https://arxiv.org/pdf/2504.05702", "abs": "https://arxiv.org/abs/2504.05702", "authors": ["Jonathan Wright", "Mark Liberman", "Neville Ryant", "James Fiumara"], "title": "Evaluating Speech-to-Text Systems with PennSound", "categories": ["cs.CL"], "comment": null, "summary": "A random sample of nearly 10 hours of speech from PennSound, the world's\nlargest online collection of poetry readings and discussions, was used as a\nbenchmark to evaluate several commercial and open-source speech-to-text\nsystems. PennSound's wide variation in recording conditions and speech styles\nmakes it a good representative for many other untranscribed audio collections.\nReference transcripts were created by trained annotators, and system\ntranscripts were produced from AWS, Azure, Google, IBM, NeMo, Rev.ai, Whisper,\nand Whisper.cpp. Based on word error rate, Rev.ai was the top performer, and\nWhisper was the top open source performer (as long as hallucinations were\navoided). AWS had the best diarization error rates among three systems.\nHowever, WER and DER differences were slim, and various tradeoffs may motivate\nchoosing different systems for different end users. We also examine the issue\nof hallucinations in Whisper. Users of Whisper should be cautioned to be aware\nof runtime options, and whether the speed vs accuracy trade off is acceptable.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05736", "pdf": "https://arxiv.org/pdf/2504.05736", "abs": "https://arxiv.org/abs/2504.05736", "authors": ["Yida Cai", "Kun Liang", "Sanwoo Lee", "Qinghan Wang", "Yunfang Wu"], "title": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "In recent years, large language models (LLMs) achieve remarkable success\nacross a variety of tasks. However, their potential in the domain of Automated\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\nlanguage models to enhance their essay scoring capabilities. Specifically, we\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\nthe output of the ranking model, in the form of a candidate score set, with the\nessay content into the scoring model (Scorer) to produce the final score.\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\naverage QWK across all LLMs and datasets, and achieves the best performance on\nChinese essay scoring using the HSK dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05759", "pdf": "https://arxiv.org/pdf/2504.05759", "abs": "https://arxiv.org/abs/2504.05759", "authors": ["Nathanaël Beau", "Benoît Crabbé"], "title": "RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "As text and code resources have expanded, large-scale pre-trained models have\nshown promising capabilities in code generation tasks, typically employing\nsupervised fine-tuning with problem statement-program pairs. However,\nincreasing model size and data volume for performance gains also raises\ncomputational demands and risks of overfitting. Addressing these challenges, we\npresent RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO}\nfor sequence-to-sequence models, utilizing a large code database as an\nauxiliary scaling method. This approach, diverging from simply enlarging model\nand dataset sizes, allows RETROcode to leverage a vast code database for\nprediction, enhancing the model's efficiency by integrating extensive memory.\nOur findings indicate that RETROcode not only outperforms similar-sized\ntraditional architectures on test sets but also approaches the effectiveness of\nthe much larger Codex model, despite being trained from scratch on a\nsubstantially smaller dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05601", "pdf": "https://arxiv.org/pdf/2504.05601", "abs": "https://arxiv.org/abs/2504.05601", "authors": ["Zhenteng Li", "Sheng Lian", "Dengfeng Pan", "Youlin Wang", "Wei Liu"], "title": "AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in Unmanned Aerial Vehicle (UAV) images poses significant\nchallenges due to complex scale variations and class imbalance among objects.\nExisting methods often address these challenges separately, overlooking the\nintricate nature of UAV images and the potential synergy between them. In\nresponse, this paper proposes AD-Det, a novel framework employing a coherent\ncoarse-to-fine strategy that seamlessly integrates two pivotal components:\nAdaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste\n(DCC). ASOE utilizes a high-resolution feature map to identify and cluster\nregions containing small objects. These regions are subsequently enlarged and\nprocessed by a fine-grained detector. On the other hand, DCC conducts\nobject-level resampling by dynamically pasting tail classes around the cluster\ncenters obtained by ASOE, main-taining a dynamic memory bank for each tail\nclass. This approach enables AD-Det to not only extract regions with small\nobjects for precise detection but also dynamically perform reasonable\nresampling for tail-class objects. Consequently, AD-Det enhances the overall\ndetection performance by addressing the challenges of scale variations and\nclass imbalance in UAV images through a synergistic and adaptive framework. We\nextensively evaluate our approach on two public datasets, i.e., VisDrone and\nUAVDT, and demonstrate that AD-Det significantly outperforms existing\ncompetitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision\n(AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05767", "pdf": "https://arxiv.org/pdf/2504.05767", "abs": "https://arxiv.org/abs/2504.05767", "authors": ["Zhang Dong", "Mingbang Wang", "Songhang deng", "Le Dai", "Jiyuan Li", "Xingzu Liu", "Ruilin Nong"], "title": "Cross-Document Contextual Coreference Resolution in Knowledge Graphs", "categories": ["cs.CL", "cs.MA"], "comment": "ACL 2025 Submission Version", "summary": "Coreference resolution across multiple documents poses a significant\nchallenge in natural language processing, particularly within the domain of\nknowledge graphs. This study introduces an innovative method aimed at\nidentifying and resolving references to the same entities that appear across\ndiffering texts, thus enhancing the coherence and collaboration of information.\nOur method employs a dynamic linking mechanism that associates entities in the\nknowledge graph with their corresponding textual mentions. By utilizing\ncontextual embeddings along with graph-based inference strategies, we\neffectively capture the relationships and interactions among entities, thereby\nimproving the accuracy of coreference resolution. Rigorous evaluations on\nvarious benchmark datasets highlight notable advancements in our approach over\ntraditional methodologies. The results showcase how the contextual information\nderived from knowledge graphs enhances the understanding of complex\nrelationships across documents, leading to better entity linking and\ninformation extraction capabilities in applications driven by knowledge. Our\ntechnique demonstrates substantial improvements in both precision and recall,\nunderscoring its effectiveness in the area of cross-document coreference\nresolution.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05824", "pdf": "https://arxiv.org/pdf/2504.05824", "abs": "https://arxiv.org/abs/2504.05824", "authors": ["Zhang Dong", "Songhang deng", "Mingbang Wang", "Le Dai", "Jiyuan Li", "Xingzu Liu", "Ruilin Nong"], "title": "End-to-End Dialog Neural Coreference Resolution: Balancing Efficiency and Accuracy in Large-Scale Systems", "categories": ["cs.CL"], "comment": "submission of acl 2025", "summary": "Large-scale coreference resolution presents a significant challenge in\nnatural language processing, necessitating a balance between efficiency and\naccuracy. In response to this challenge, we introduce an End-to-End Neural\nCoreference Resolution system tailored for large-scale applications. Our system\nefficiently identifies and resolves coreference links in text, ensuring minimal\ncomputational overhead without compromising on performance. By utilizing\nadvanced neural network architectures, we incorporate various contextual\nembeddings and attention mechanisms, which enhance the quality of predictions\nfor coreference pairs. Furthermore, we apply optimization strategies to\naccelerate processing speeds, making the system suitable for real-world\ndeployment. Extensive evaluations conducted on benchmark datasets demonstrate\nthat our model achieves improved accuracy compared to existing approaches,\nwhile effectively maintaining rapid inference times. Rigorous testing confirms\nthe ability of our system to deliver precise coreference resolutions\nefficiently, thereby establishing a benchmark for future advancements in this\nfield.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05662", "pdf": "https://arxiv.org/pdf/2504.05662", "abs": "https://arxiv.org/abs/2504.05662", "authors": ["Shunsuke Sakai", "Tatsuhito Hasegawa"], "title": "Reconstruction-Free Anomaly Detection with Diffusion Models via Direct Latent Likelihood Evaluation", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/SkyShunsuke/InversionAD", "summary": "Diffusion models, with their robust distribution approximation capabilities,\nhave demonstrated excellent performance in anomaly detection. However,\nconventional reconstruction-based approaches rely on computing the\nreconstruction error between the original and denoised images, which requires\ncareful noise-strength tuning and over ten network evaluations per\ninput-leading to significantly slower detection speeds. To address these\nlimitations, we propose a novel diffusion-based anomaly detection method that\ncircumvents the need for resource-intensive reconstruction. Instead of\nreconstructing the input image, we directly infer its corresponding latent\nvariables and measure their density under the Gaussian prior distribution.\nRemarkably, the prior density proves effective as an anomaly score even when\nusing a short partial diffusion process of only 2-5 steps. We evaluate our\nmethod on the MVTecAD dataset, achieving an AUC of 0.991 at 15 FPS, thereby\nsetting a new state-of-the-art speed-AUC anomaly detection trade-off.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05954", "pdf": "https://arxiv.org/pdf/2504.05954", "abs": "https://arxiv.org/abs/2504.05954", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Unsupervised Location Mapping for Narrative Corpora", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work presents the task of unsupervised location mapping, which seeks to\nmap the trajectory of an individual narrative on a spatial map of locations in\nwhich a large set of narratives take place. Despite the fundamentality and\ngenerality of the task, very little work addressed the spatial mapping of\nnarrative texts. The task consists of two parts: (1) inducing a ``map'' with\nthe locations mentioned in a set of texts, and (2) extracting a trajectory from\na single narrative and positioning it on the map. Following recent advances in\nincreasing the context length of large language models, we propose a pipeline\nfor this task in a completely unsupervised manner without predefining the set\nof labels. We test our method on two different domains: (1) Holocaust\ntestimonies and (2) Lake District writing, namely multi-century literature on\ntravels in the English Lake District. We perform both intrinsic and extrinsic\nevaluations for the task, with encouraging results, thereby setting a benchmark\nand evaluation practices for the task, as well as highlighting challenges.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06011", "pdf": "https://arxiv.org/pdf/2504.06011", "abs": "https://arxiv.org/abs/2504.06011", "authors": ["Monojit Choudhury", "Shivam Chauhan", "Rocktim Jyoti Das", "Dhruv Sahnan", "Xudong Han", "Haonan Li", "Aaryamonvikram Singh", "Alok Anil Jadhav", "Utkarsh Agarwal", "Mukund Choudhary", "Debopriyo Banerjee", "Fajri Koto", "Junaid Bhat", "Awantika Shukla", "Samujjwal Ghosh", "Samta Kamboj", "Onkar Pandit", "Lalit Pradhan", "Rahul Pal", "Sunil Sahu", "Soundar Doraiswamy", "Parvez Mullah", "Ali El Filali", "Neha Sengupta", "Gokul Ramakrishnan", "Rituraj Joshi", "Gurpreet Gosal", "Avraham Sheinin", "Natalia Vassilieva", "Preslav Nakov"], "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi", "categories": ["cs.CL"], "comment": null, "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05679", "pdf": "https://arxiv.org/pdf/2504.05679", "abs": "https://arxiv.org/abs/2504.05679", "authors": ["Udayanga G. W. K. N. Gamage", "Xuanni Huo", "Luca Zanatta", "T Delbruck", "Cesar Cadena", "Matteo Fumagalli", "Silvia Tolu"], "title": "Event-based Civil Infrastructure Visual Defect Detection: ev-CIVIL Dataset and Benchmark", "categories": ["cs.CV"], "comment": "A journal paper which submitted to Sage SHM journa and it is under\n  review currently. consist of 25 pages. It has 19 figures and 5 tables.\n  Keywords Event-based vision, civil structural health monitoring, defect\n  detection, crack, spalling, DVS, dataset, YOLOv6, SSD, 2D event histograms", "summary": "Small Unmanned Aerial Vehicle (UAV) based visual inspections are a more\nefficient alternative to manual methods for examining civil structural defects,\noffering safe access to hazardous areas and significant cost savings by\nreducing labor requirements. However, traditional frame-based cameras, widely\nused in UAV-based inspections, often struggle to capture defects under low or\ndynamic lighting conditions. In contrast, Dynamic Vision Sensors (DVS), or\nevent-based cameras, excel in such scenarios by minimizing motion blur,\nenhancing power efficiency, and maintaining high-quality imaging across diverse\nlighting conditions without saturation or information loss. Despite these\nadvantages, existing research lacks studies exploring the feasibility of using\nDVS for detecting civil structural defects.Moreover, there is no dedicated\nevent-based dataset tailored for this purpose. Addressing this gap, this study\nintroduces the first event-based civil infrastructure defect detection dataset,\ncapturing defective surfaces as a spatio-temporal event stream using DVS.In\naddition to event-based data, the dataset includes grayscale intensity image\nframes captured simultaneously using an Active Pixel Sensor (APS). Both data\ntypes were collected using the DAVIS346 camera, which integrates DVS and APS\nsensors.The dataset focuses on two types of defects: cracks and spalling, and\nincludes data from both field and laboratory environments. The field dataset\ncomprises 318 recording sequences,documenting 458 distinct cracks and 121\ndistinct spalling instances.The laboratory dataset includes 362 recording\nsequences, covering 220 distinct cracks and 308 spalling instances.Four\nrealtime object detection models were evaluated on it to validate the dataset\neffectiveness.The results demonstrate the dataset robustness in enabling\naccurate defect detection and classification,even under challenging lighting\nconditions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06136", "pdf": "https://arxiv.org/pdf/2504.06136", "abs": "https://arxiv.org/abs/2504.06136", "authors": ["Movina Moses", "Mohab Elkaref", "James Barry", "Shinnosuke Tanaka", "Vishnudev Kuruvanthodi", "Nathan Herr", "Campbell D Watson", "Geeth De Mel"], "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and Evaluation Platform", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06160", "pdf": "https://arxiv.org/pdf/2504.06160", "abs": "https://arxiv.org/abs/2504.06160", "authors": ["Rijul Magu", "Arka Dutta", "Sean Kim", "Ashiqur R. KhudaBukhsh", "Munmun De Choudhury"], "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI", "J.4; K.4.1; K.4.2"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06166", "pdf": "https://arxiv.org/pdf/2504.06166", "abs": "https://arxiv.org/abs/2504.06166", "authors": ["Montgomery Gole", "Andriy Miranskyy"], "title": "Assessing how hyperparameters impact Large Language Models' sarcasm detection performance", "categories": ["cs.CL"], "comment": null, "summary": "Sarcasm detection is challenging for both humans and machines. This work\nexplores how model characteristics impact sarcasm detection in OpenAI's GPT,\nand Meta's Llama-2 models, given their strong natural language understanding,\nand popularity. We evaluate fine-tuned and zero-shot models across various\nsizes, releases, and hyperparameters. Experiments were conducted on the\npolitical and balanced (pol-bal) portion of the popular Self-Annotated Reddit\nCorpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically\nwith model size within a model family, while hyperparameter tuning also impacts\nperformance. In the fine-tuning scenario, full precision Llama-2-13b achieves\nstate-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to\naverage human performance. In the zero-shot setting, one GPT-4 model achieves\ncompetitive performance to prior attempts, yielding an accuracy of 0.70 and an\n$F_1$-score of 0.75. Furthermore, a model's performance may increase or decline\nwith each release, highlighting the need to reassess performance after each\nrelease.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05323", "pdf": "https://arxiv.org/pdf/2504.05323", "abs": "https://arxiv.org/abs/2504.05323", "authors": ["Mingjian Fu", "Hengsheng Chen", "Dongchun Jiang", "Yanchao Tan"], "title": "Multi-Perspective Attention Mechanism for Bias-Aware Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2"], "comment": "30 pages,10 figures,4 tables", "summary": "In the era of advancing information technology, recommender systems have\nemerged as crucial tools for dealing with information overload. However,\ntraditional recommender systems still have limitations in capturing the dynamic\nevolution of user behavior. To better understand and predict user behavior,\nespecially taking into account the complexity of temporal evolution, sequential\nrecommender systems have gradually become the focus of research. Currently,\nmany sequential recommendation algorithms ignore the amplification effects of\nprevalent biases, which leads to recommendation results being susceptible to\nthe Matthew Effect. Additionally, it will impose limitations on the recommender\nsystem's ability to deeply perceive and capture the dynamic shifts in user\npreferences, thereby diminishing the extent of its recommendation reach. To\naddress this issue effectively, we propose a recommendation system based on\nsequential information and attention mechanism called Multi-Perspective\nAttention Bias Sequential Recommendation (MABSRec). Firstly, we reconstruct\nuser sequences into three short types and utilize graph neural networks for\nitem weighting. Subsequently, an adaptive multi-bias perspective attention\nmodule is proposed to enhance the accuracy of recommendations. Experimental\nresults show that the MABSRec model exhibits significant advantages in all\nevaluation metrics, demonstrating its excellent performance in the sequence\nrecommendation task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05782", "pdf": "https://arxiv.org/pdf/2504.05782", "abs": "https://arxiv.org/abs/2504.05782", "authors": ["Pengfei Zhou", "Fanrui Zhang", "Xiaopeng Peng", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 8 figures", "summary": "Multimodal reasoning, which integrates language and visual cues into problem\nsolving and decision making, is a fundamental aspect of human intelligence and\na crucial step toward artificial general intelligence. However, the evaluation\nof multimodal reasoning capabilities in Multimodal Large Language Models\n(MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained\nby limited data size, narrow domain coverage, and unstructured knowledge\ndistribution. To close these gaps, we introduce MDK12-Bench, a\nmulti-disciplinary benchmark assessing the reasoning capabilities of MLLMs via\nreal-world K-12 examinations. Spanning six disciplines (math, physics,\nchemistry, biology, geography, and information science), our benchmark\ncomprises 140K reasoning instances across diverse difficulty levels from\nprimary school to 12th grade. It features 6,827 instance-level knowledge point\nannotations based on a well-organized knowledge structure, detailed answer\nexplanations, difficulty labels and cross-year partitions, providing a robust\nplatform for comprehensive evaluation. Additionally, we present a novel dynamic\nevaluation framework to mitigate data contamination issues by bootstrapping\nquestion forms, question types, and image styles during evaluation. Extensive\nexperiment on MDK12-Bench reveals the significant limitation of current MLLMs\nin multimodal reasoning. The findings on our benchmark provide insights into\nthe development of the next-generation models. Our data and codes are available\nat https://github.com/LanceZPF/MDK12.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05795", "pdf": "https://arxiv.org/pdf/2504.05795", "abs": "https://arxiv.org/abs/2504.05795", "authors": ["Hao Zhang", "Yanping Zha", "Qingwei Zhuang", "Zhenfeng Shao", "Jiayi Ma"], "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05520", "pdf": "https://arxiv.org/pdf/2504.05520", "abs": "https://arxiv.org/abs/2504.05520", "authors": ["Taiwei Shi", "Yiyang Wu", "Linxin Song", "Tianyi Zhou", "Jieyu Zhao"], "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning", "categories": ["cs.LG", "cs.CL"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the\nmathematical reasoning capabilities of large language models (LLMs), but it is\noften sample- and compute-inefficient, requiring extensive training. In this\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\nmethod that significantly improves both the efficiency and final accuracy of\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\ndifficulty of training problems based on the model's recent reward signals,\nensuring that the model consistently trains on tasks that are challenging but\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\nan optimal difficulty range, avoiding wasted computation on problems that are\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\nreward function or model architecture. Experiments on competition-level math\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\nsignificantly improves both training efficiency and reasoning performance. We\nevaluate AdaRFT across multiple data distributions and model sizes, showing\nthat it reduces the number of training steps by up to 2x and improves accuracy\nby a considerable margin, offering a more scalable and effective RFT framework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "PPO", "proximal policy optimization", "policy optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05830", "pdf": "https://arxiv.org/pdf/2504.05830", "abs": "https://arxiv.org/abs/2504.05830", "authors": ["Shiao Wang", "Xiao Wang", "Bo Jiang", "Lin Zhu", "Guoqi Li", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "Human Activity Recognition using RGB-Event based Sensors: A Multi-modal Heat Conduction Model and A Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Journal Extension of HARDVS (AAAI 2024)", "summary": "Human Activity Recognition (HAR) primarily relied on traditional RGB cameras\nto achieve high-performance activity recognition. However, the challenging\nfactors in real-world scenarios, such as insufficient lighting and rapid\nmovements, inevitably degrade the performance of RGB cameras. To address these\nchallenges, biologically inspired event cameras offer a promising solution to\novercome the limitations of traditional RGB cameras. In this work, we rethink\nhuman activity recognition by combining the RGB and event cameras. The first\ncontribution is the proposed large-scale multi-modal RGB-Event human activity\nrecognition benchmark dataset, termed HARDVS 2.0, which bridges the dataset\ngaps. It contains 300 categories of everyday real-world actions with a total of\n107,646 paired videos covering various challenging scenarios. Inspired by the\nphysics-informed heat conduction model, we propose a novel multi-modal heat\nconduction operation framework for effective activity recognition, termed\nMMHCO-HAR. More in detail, given the RGB frames and event streams, we first\nextract the feature embeddings using a stem network. Then, multi-modal Heat\nConduction blocks are designed to fuse the dual features, the key module of\nwhich is the multi-modal Heat Conduction Operation layer. We integrate RGB and\nevent embeddings through a multi-modal DCT-IDCT layer while adaptively\nincorporating the thermal conductivity coefficient via FVEs into this module.\nAfter that, we propose an adaptive fusion module based on a policy routing\nstrategy for high-performance classification. Comprehensive experiments\ndemonstrate that our method consistently performs well, validating its\neffectiveness and robustness. The source code and benchmark dataset will be\nreleased on https://github.com/Event-AHU/HARDVS/tree/HARDVSv2", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06196", "pdf": "https://arxiv.org/pdf/2504.06196", "abs": "https://arxiv.org/abs/2504.06196", "authors": ["Eric Wang", "Samuel Schmidgall", "Paul F. Jaeger", "Fan Zhang", "Rory Pilgrim", "Yossi Matias", "Joelle Barral", "David Fleet", "Shekoofeh Azizi"], "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06260", "pdf": "https://arxiv.org/pdf/2504.06260", "abs": "https://arxiv.org/abs/2504.06260", "authors": ["Nayantara Mudur", "Hao Cui", "Subhashini Venugopalan", "Paul Raccuglia", "Michael P. Brenner", "Peter Norgaard"], "title": "FEABench: Evaluating Language Models on Multiphysics Reasoning Ability", "categories": ["cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": "39 pages. Accepted at the NeurIPS 2024 Workshops on Mathematical\n  Reasoning and AI and Open-World Agents", "summary": "Building precise simulations of the real world and invoking numerical solvers\nto answer quantitative problems is an essential requirement in engineering and\nscience. We present FEABench, a benchmark to evaluate the ability of large\nlanguage models (LLMs) and LLM agents to simulate and solve physics,\nmathematics and engineering problems using finite element analysis (FEA). We\nintroduce a comprehensive evaluation scheme to investigate the ability of LLMs\nto solve these problems end-to-end by reasoning over natural language problem\ndescriptions and operating COMSOL Multiphysics$^\\circledR$, an FEA software, to\ncompute the answers. We additionally design a language model agent equipped\nwith the ability to interact with the software through its Application\nProgramming Interface (API), examine its outputs and use tools to improve its\nsolutions over multiple iterations. Our best performing strategy generates\nexecutable API calls 88% of the time. LLMs that can successfully interact with\nand operate FEA software to solve problems such as those in our benchmark would\npush the frontiers of automation in engineering. Acquiring this capability\nwould augment LLMs' reasoning skills with the precision of numerical solvers\nand advance the development of autonomous systems that can tackle complex\nproblems in the real world. The code is available at\nhttps://github.com/google/feabench", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05925", "pdf": "https://arxiv.org/pdf/2504.05925", "abs": "https://arxiv.org/abs/2504.05925", "authors": ["Hao Du", "Bo Wu", "Yan Lu", "Zhendong Mao"], "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "categories": ["cs.CV"], "comment": "CVPR 2025. The first two authors contributed equally", "summary": "Vision-language temporal alignment is a crucial capability for human dynamic\nrecognition and cognition in real-world scenarios. While existing research\nfocuses on capturing vision-language relevance, it faces limitations due to\nbiased temporal distributions, imprecise annotations, and insufficient\ncompositionally. To achieve fair evaluation and comprehensive exploration, our\nobjective is to investigate and evaluate the ability of models to achieve\nalignment from a temporal perspective, specifically focusing on their capacity\nto synchronize visual scenarios with linguistic context in a temporally\ncoherent manner. As a preliminary step, we present the statistical analysis of\nexisting benchmarks and reveal the existing challenges from a decomposed\nperspective. To this end, we introduce SVLTA, the Synthetic Vision-Language\nTemporal Alignment derived via a well-designed and feasible control generation\nmethod within a simulation environment. The approach considers commonsense\nknowledge, manipulable action, and constrained filtering, which generates\nreasonable, diverse, and balanced data distributions for diagnostic\nevaluations. Our experiments reveal diagnostic insights through the evaluations\nin temporal question answering, distributional shift sensitiveness, and\ntemporal alignment adaptation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06003", "pdf": "https://arxiv.org/pdf/2504.06003", "abs": "https://arxiv.org/abs/2504.06003", "authors": ["Can Zhang", "Gim Hee Lee"], "title": "econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "The primary focus of most recent works on open-vocabulary neural fields is\nextracting precise semantic features from the VLMs and then consolidating them\nefficiently into a multi-view consistent 3D neural fields representation.\nHowever, most existing works over-trusted SAM to regularize image-level CLIP\nwithout any further refinement. Moreover, several existing works improved\nefficiency by dimensionality reduction of semantic features from 2D VLMs before\nfusing with 3DGS semantic fields, which inevitably leads to multi-view\ninconsistency. In this work, we propose econSG for open-vocabulary semantic\nsegmentation with 3DGS. Our econSG consists of: 1) A Confidence-region Guided\nRegularization (CRR) that mutually refines SAM and CLIP to get the best of both\nworlds for precise semantic features with complete and precise boundaries. 2) A\nlow dimensional contextual space to enforce 3D multi-view consistency while\nimproving computational efficiency by fusing backprojected multi-view 2D\nfeatures and follow by dimensional reduction directly on the fused 3D features\ninstead of operating on each 2D view separately. Our econSG shows\nstate-of-the-art performance on four benchmark datasets compared to the\nexisting methods. Furthermore, we are also the most efficient training among\nall the methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06004", "pdf": "https://arxiv.org/pdf/2504.06004", "abs": "https://arxiv.org/abs/2504.06004", "authors": ["Mrityunjoy Gain", "Kitae Kim", "Avi Deb Raha", "Apurba Adhikary", "Eui-Nam Huh", "Zhu Han", "Choong Seon Hong"], "title": "FedFeat+: A Robust Federated Learning Framework Through Federated Aggregation and Differentially Private Feature-Based Classifier Retraining", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose the FedFeat+ framework, which distinctively\nseparates feature extraction from classification. We develop a two-tiered model\ntraining process: following local training, clients transmit their weights and\nsome features extracted from the feature extractor from the final local epochs\nto the server. The server aggregates these models using the FedAvg method and\nsubsequently retrains the global classifier utilizing the shared features. The\nclassifier retraining process enhances the model's understanding of the\nholistic view of the data distribution, ensuring better generalization across\ndiverse datasets. This improved generalization enables the classifier to\nadaptively influence the feature extractor during subsequent local training\nepochs. We establish a balance between enhancing model accuracy and\nsafeguarding individual privacy through the implementation of differential\nprivacy mechanisms. By incorporating noise into the feature vectors shared with\nthe server, we ensure that sensitive data remains confidential. We present a\ncomprehensive convergence analysis, along with theoretical reasoning regarding\nperformance enhancement and privacy preservation. We validate our approach\nthrough empirical evaluations conducted on benchmark datasets, including\nCIFAR-10, CIFAR-100, MNIST, and FMNIST, achieving high accuracy while adhering\nto stringent privacy guarantees. The experimental results demonstrate that the\nFedFeat+ framework, despite using only a lightweight two-layer CNN classifier,\noutperforms the FedAvg method in both IID and non-IID scenarios, achieving\naccuracy improvements ranging from 3.92 % to 12.34 % across CIFAR-10,\nCIFAR-100, and Fashion-MNIST datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06022", "pdf": "https://arxiv.org/pdf/2504.06022", "abs": "https://arxiv.org/abs/2504.06022", "authors": ["Luis Denninger", "Sina Mokhtarzadeh Azar", "Juergen Gall"], "title": "CamContextI2V: Context-aware Controllable Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recently, image-to-video (I2V) diffusion models have demonstrated impressive\nscene understanding and generative quality, incorporating image conditions to\nguide generation. However, these models primarily animate static images without\nextending beyond their provided context. Introducing additional constraints,\nsuch as camera trajectories, can enhance diversity but often degrades visual\nquality, limiting their applicability for tasks requiring faithful scene\nrepresentation. We propose CamContextI2V, an I2V model that integrates multiple\nimage conditions with 3D constraints alongside camera control to enrich both\nglobal semantics and fine-grained visual details. This enables more coherent\nand context-aware video generation. Moreover, we motivate the necessity of\ntemporal awareness for an effective context representation. Our comprehensive\nstudy on the RealEstate10K dataset demonstrates improvements in visual quality\nand camera controllability. We make our code and models publicly available at:\nhttps://github.com/LDenninger/CamContextI2V.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06039", "pdf": "https://arxiv.org/pdf/2504.06039", "abs": "https://arxiv.org/abs/2504.06039", "authors": ["Julia Werner", "Christoph Gerum", "Jorg Nick", "Maxime Le Floch", "Franz Brinkmann", "Jochen Hampe", "Oliver Bringmann"], "title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies", "categories": ["cs.CV"], "comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBS EMBC)", "summary": "Capsule endoscopy is a method to capture images of the gastrointestinal tract\nand screen for diseases which might remain hidden if investigated with standard\nendoscopes. Due to the limited size of a video capsule, embedding AI models\ndirectly into the capsule demands careful consideration of the model size and\nthus complicates anomaly detection in this field. Furthermore, the scarcity of\navailable data in this domain poses an ongoing challenge to achieving effective\nanomaly detection. Thus, this work introduces an ensemble strategy to address\nthis challenge in anomaly detection tasks in video capsule endoscopies,\nrequiring only a small number of individual neural networks during both the\ntraining and inference phases. Ensemble learning combines the predictions of\nmultiple independently trained neural networks. This has shown to be highly\neffective in enhancing both the accuracy and robustness of machine learning\nmodels. However, this comes at the cost of higher memory usage and increased\ncomputational effort, which quickly becomes prohibitive in many real-world\napplications. Instead of applying the same training algorithm to each\nindividual network, we propose using various loss functions, drawn from the\nanomaly detection field, to train each network. The methods are validated on\nthe two largest publicly available datasets for video capsule endoscopy images,\nthe Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on\nthe Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our\napproach outperforms current baselines with significantly fewer parameters\nacross all models, which is a crucial step towards incorporating artificial\nintelligence into capsule endoscopies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06088", "pdf": "https://arxiv.org/pdf/2504.06088", "abs": "https://arxiv.org/abs/2504.06088", "authors": ["Divyanshu Mishra", "Pramit Saha", "He Zhao", "Netzahualcoyotl Hernandez-Cruz", "Olga Patey", "Aris Papageorghiou", "J. Alison Noble"], "title": "MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted in AAAI 2025", "summary": "Accurate standard plane acquisition in fetal ultrasound (US) videos is\ncrucial for fetal growth assessment, anomaly detection, and adherence to\nclinical guidelines. However, manually selecting standard frames is\ntime-consuming and prone to intra- and inter-sonographer variability. Existing\nmethods primarily rely on image-based approaches that capture standard frames\nand then classify the input frames across different anatomies. This ignores the\ndynamic nature of video acquisition and its interpretation. To address these\nchallenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a\nvisual query-based video clip localization (VQ-VCL) method, to assist\nsonographers by enabling them to capture a quick US sweep. By then providing a\nvisual query of the anatomy they wish to analyze, MCAT returns the video clip\ncontaining the standard frames for that anatomy, facilitating thorough\nscreening for potential anomalies. We evaluate MCAT on two ultrasound video\ndatasets and a natural image VQ-VCL dataset based on Ego4D. Our model\noutperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound\ndatasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's\nefficiency and accuracy have significant potential implications for public\nhealth, especially in low- and middle-income countries (LMICs), where it may\nenhance prenatal care by streamlining standard plane acquisition, simplifying\nUS-based screening, diagnosis and allowing sonographers to examine more\npatients.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06263", "pdf": "https://arxiv.org/pdf/2504.06263", "abs": "https://arxiv.org/abs/2504.06263", "authors": ["Yiying Yang", "Wei Cheng", "Sijin Chen", "Xianfang Zeng", "Jiaxu Zhang", "Liao Wang", "Gang Yu", "Xingjun Ma", "Yu-Gang Jiang"], "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model", "categories": ["cs.CV"], "comment": "18 pages; Project Page: https://omnisvg.github.io/", "summary": "Scalable Vector Graphics (SVG) is an important image format widely adopted in\ngraphic design because of their resolution independence and editability. The\nstudy of generating high-quality SVG has continuously drawn attention from both\ndesigners and researchers in the AIGC community. However, existing methods\neither produces unstructured outputs with huge computational cost or is limited\nto generating monochrome icons of over-simplified structures. To produce\nhigh-quality and complex SVG, we propose OmniSVG, a unified framework that\nleverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal\nSVG generation. By parameterizing SVG commands and coordinates into discrete\ntokens, OmniSVG decouples structural logic from low-level geometry for\nefficient training while maintaining the expressiveness of complex SVG\nstructure. To further advance the development of SVG synthesis, we introduce\nMMSVG-2M, a multimodal dataset with two million richly annotated SVG assets,\nalong with a standardized evaluation protocol for conditional SVG generation\ntasks. Extensive experiments show that OmniSVG outperforms existing methods and\ndemonstrates its potential for integration into professional SVG design\nworkflows.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05803", "pdf": "https://arxiv.org/pdf/2504.05803", "abs": "https://arxiv.org/abs/2504.05803", "authors": ["Yihuan Huang", "Jiajun Liu", "Yanzhen Ren", "Wuyang Liu", "Juhua Tang"], "title": "SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve Phoneme-Viseme Alignment Ambiguity", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Speech-driven talking head synthesis tasks commonly use general acoustic\nfeatures (such as HuBERT and DeepSpeech) as guided speech features. However, we\ndiscovered that these features suffer from phoneme-viseme alignment ambiguity,\nwhich refers to the uncertainty and imprecision in matching phonemes (speech)\nwith visemes (lip). To address this issue, we propose the Speech Encoder for\nLip (SE4Lip) to encode lip features from speech directly, aligning speech and\nlip features in the joint embedding space by a cross-modal alignment framework.\nThe STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve\nthe fine-grained speech features. Experimental results show that SE4Lip\nachieves state-of-the-art performance in both NeRF and 3DGS rendering models.\nIts lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline\nand produces results close to the ground truth videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05888", "pdf": "https://arxiv.org/pdf/2504.05888", "abs": "https://arxiv.org/abs/2504.05888", "authors": ["Guillaume Gautier", "Alexandre Mercat", "Louis Fréneau", "Mikko Pitkänen", "Jarno Vanne"], "title": "UVG-VPC: Voxelized Point Cloud Dataset for Visual Volumetric Video-based Coding", "categories": ["cs.MM", "cs.CV"], "comment": "Point cloud compression;Geometry;Visualization;Three-dimensional\n  displays;Video sequences;Transform coding;Media;Open dataset;point\n  cloud;Visual Volumetric Video-based Coding (V3C);Video-based Point Cloud\n  Compression (V-PCC);Extended Reality (XR)", "summary": "Point cloud compression has become a crucial factor in immersive visual media\nprocessing and streaming. This paper presents a new open dataset called UVG-VPC\nfor the development, evaluation, and validation of MPEG Visual Volumetric\nVideo-based Coding (V3C) technology. The dataset is distributed under its own\nnon-commercial license. It consists of 12 point cloud test video sequences of\ndiverse characteristics with respect to the motion, RGB texture, 3D geometry,\nand surface occlusion of the points. Each sequence is 10 seconds long and\ncomprises 250 frames captured at 25 frames per second. The sequences are\nvoxelized with a geometry precision of 9 to 12 bits, and the voxel color\nattributes are represented as 8-bit RGB values. The dataset also includes\nassociated normals that make it more suitable for evaluating point cloud\ncompression solutions. The main objective of releasing the UVG-VPC dataset is\nto foster the development of V3C technologies and thereby shape the future in\nthis field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06084", "pdf": "https://arxiv.org/pdf/2504.06084", "abs": "https://arxiv.org/abs/2504.06084", "authors": ["Alexey Gavryushin", "Xi Wang", "Robert J. S. Malate", "Chenyu Yang", "Xiangyi Jia", "Shubh Goel", "Davide Liconti", "René Zurbrügg", "Robert K. Katzschmann", "Marc Pollefeys"], "title": "MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Large-scale egocentric video datasets capture diverse human activities across\na wide range of scenarios, offering rich and detailed insights into how humans\ninteract with objects, especially those that require fine-grained dexterous\ncontrol. Such complex, dexterous skills with precise controls are crucial for\nmany robotic manipulation tasks, yet are often insufficiently addressed by\ntraditional data-driven approaches to robotic manipulation. To address this\ngap, we leverage manipulation priors learned from large-scale egocentric video\ndatasets to improve policy learning for dexterous robotic manipulation tasks.\nWe present MAPLE, a novel method for dexterous robotic manipulation that\nexploits rich manipulation priors to enable efficient policy learning and\nbetter performance on diverse, complex manipulation tasks. Specifically, we\npredict hand-object contact points and detailed hand poses at the moment of\nhand-object contact and use the learned features to train policies for\ndownstream manipulation tasks. Experimental results demonstrate the\neffectiveness of MAPLE across existing simulation benchmarks, as well as a\nnewly designed set of challenging simulation tasks, which require fine-grained\nobject control and complex dexterous skills. The benefits of MAPLE are further\nhighlighted in real-world experiments using a dexterous robotic hand, whereas\nsimultaneous evaluation across both simulation and real-world experiments has\nremained underexplored in prior work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05325", "pdf": "https://arxiv.org/pdf/2504.05325", "abs": "https://arxiv.org/abs/2504.05325", "authors": ["Shiran Dudy", "Thulasi Tholeti", "Resmi Ramachandranpillai", "Muhammad Ali", "Toby Jia-Jun Li", "Ricardo Baeza-Yates"], "title": "Unequal Opportunities: Examining the Bias in Geographical Recommendations by Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have made them a popular\ninformation-seeking tool among end users. However, the statistical training\nmethods for LLMs have raised concerns about their representation of\nunder-represented topics, potentially leading to biases that could influence\nreal-world decisions and opportunities. These biases could have significant\neconomic, social, and cultural impacts as LLMs become more prevalent, whether\nthrough direct interactions--such as when users engage with chatbots or\nautomated assistants--or through their integration into third-party\napplications (as agents), where the models influence decision-making processes\nand functionalities behind the scenes. Our study examines the biases present in\nLLMs recommendations of U.S. cities and towns across three domains: relocation,\ntourism, and starting a business. We explore two key research questions: (i)\nHow similar LLMs responses are, and (ii) How this similarity might favor areas\nwith certain characteristics over others, introducing biases. We focus on the\nconsistency of LLMs responses and their tendency to over-represent or\nunder-represent specific locations. Our findings point to consistent\ndemographic biases in these recommendations, which could perpetuate a\n``rich-get-richer'' effect that widens existing economic disparities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05410", "pdf": "https://arxiv.org/pdf/2504.05410", "abs": "https://arxiv.org/abs/2504.05410", "authors": ["Benjamin Lipkin", "Benjamin LeBrun", "Jacob Hoover Vigly", "João Loula", "David R. MacIver", "Li Du", "Jason Eisner", "Ryan Cotterell", "Vikash Mansinghka", "Timothy J. O'Donnell", "Alexander K. Lew", "Tim Vieira"], "title": "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The dominant approach to generating from language models subject to some\nconstraint is locally constrained decoding (LCD), incrementally sampling tokens\nat each time step such that the constraint is never violated. Typically, this\nis achieved through token masking: looping over the vocabulary and excluding\nnon-conforming tokens. There are two important problems with this approach. (i)\nEvaluating the constraint on every token can be prohibitively expensive -- LM\nvocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global\ndistribution over strings, sampling tokens based only on local information,\neven if they lead down dead-end paths. This work introduces a new algorithm\nthat addresses both these problems. First, to avoid evaluating a constraint on\nthe full vocabulary at each step of generation, we propose an adaptive\nrejection sampling algorithm that typically requires orders of magnitude fewer\nconstraint evaluations. Second, we show how this algorithm can be extended to\nproduce low-variance, unbiased estimates of importance weights at a very small\nadditional cost -- estimates that can be soundly used within previously\nproposed sequential Monte Carlo algorithms to correct for the myopic behavior\nof local constraint enforcement. Through extensive empirical evaluation in\ntext-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON\ndomains, we show that our approach is superior to state-of-the-art baselines,\nsupporting a broader class of constraints and improving both runtime and\nperformance. Additional theoretical and empirical analyses show that our\nmethod's runtime efficiency is driven by its dynamic use of computation,\nscaling with the divergence between the unconstrained and constrained LM, and\nas a consequence, runtime improvements are greater for better models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05411", "pdf": "https://arxiv.org/pdf/2504.05411", "abs": "https://arxiv.org/abs/2504.05411", "authors": ["Lingzhi Shen", "Yunfei Long", "Xiaohao Cai", "Guanming Chen", "Imran Razzak", "Shoaib Jameel"], "title": "Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection automatically identifies an individual's personality\nfrom various data sources, such as social media texts. However, as the\nparameter scale of language models continues to grow, the computational cost\nbecomes increasingly difficult to manage. Fine-tuning also grows more complex,\nmaking it harder to justify the effort and reliably predict outcomes. We\nintroduce a novel parameter-efficient fine-tuning framework, PersLLM, to\naddress these challenges. In PersLLM, a large language model (LLM) extracts\nhigh-dimensional representations from raw data and stores them in a dynamic\nmemory layer. PersLLM then updates the downstream layers with a replaceable\noutput network, enabling flexible adaptation to various personality detection\nscenarios. By storing the features in the memory layer, we eliminate the need\nfor repeated complex computations by the LLM. Meanwhile, the lightweight output\nnetwork serves as a proxy for evaluating the overall effectiveness of the\nframework, improving the predictability of results. Experimental results on key\nbenchmark datasets like Kaggle and Pandora show that PersLLM significantly\nreduces computational cost while maintaining competitive performance and strong\nadaptability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05496", "pdf": "https://arxiv.org/pdf/2504.05496", "abs": "https://arxiv.org/abs/2504.05496", "authors": ["Atilla Kaan Alkan", "Shashwat Sourav", "Maja Jablonska", "Simone Astarita", "Rishabh Chakrabarty", "Nikhil Garuda", "Pranav Khetarpal", "Maciej Pióro", "Dimitrios Tanoglidis", "Kartheik G. Iyer", "Mugdha S. Polimera", "Michael J. Smith", "Tirthankar Ghosal", "Marc Huertas-Company", "Sandor Kruk", "Kevin Schawinski", "Ioana Ciucă"], "title": "A Survey on Hypothesis Generation for Scientific Discovery in the Era of Large Language Models", "categories": ["cs.CL", "68T50"], "comment": "9 pages (+2 pages of references), 2 figures", "summary": "Hypothesis generation is a fundamental step in scientific discovery, yet it\nis increasingly challenged by information overload and disciplinary\nfragmentation. Recent advances in Large Language Models (LLMs) have sparked\ngrowing interest in their potential to enhance and automate this process. This\npaper presents a comprehensive survey of hypothesis generation with LLMs by (i)\nreviewing existing methods, from simple prompting techniques to more complex\nframeworks, and proposing a taxonomy that categorizes these approaches; (ii)\nanalyzing techniques for improving hypothesis quality, such as novelty boosting\nand structured reasoning; (iii) providing an overview of evaluation strategies;\nand (iv) discussing key challenges and future directions, including multimodal\nintegration and human-AI collaboration. Our survey aims to serve as a reference\nfor researchers exploring LLMs for hypothesis generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05527", "pdf": "https://arxiv.org/pdf/2504.05527", "abs": "https://arxiv.org/abs/2504.05527", "authors": ["Despina Tomkou", "George Fatouros", "Andreas Andreou", "Georgios Makridis", "Fotis Liarokapis", "Dimitrios Dardanis", "Athanasios Kiourtis", "John Soldatos", "Dimosthenis Kyriazis"], "title": "Bridging Industrial Expertise and XR with LLM-Powered Conversational Agents", "categories": ["cs.CL", "cs.AI", "68T50, 68T40, 68U20, 68U35", "H.5.1; I.2.7; I.2.11; H.3.3; H.5.2; C.3"], "comment": "7 pages, 7 figures", "summary": "This paper introduces a novel integration of Retrieval-Augmented Generation\n(RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR)\ntechnologies to address knowledge transfer challenges in industrial\nenvironments. The proposed system embeds domain-specific industrial knowledge\ninto XR environments through a natural language interface, enabling hands-free,\ncontext-aware expert guidance for workers. We present the architecture of the\nproposed system consisting of an LLM Chat Engine with dynamic tool\norchestration and an XR application featuring voice-driven interaction.\nPerformance evaluation of various chunking strategies, embedding models, and\nvector databases reveals that semantic chunking, balanced embedding models, and\nefficient vector stores deliver optimal performance for industrial knowledge\nretrieval. The system's potential is demonstrated through early implementation\nin multiple industrial use cases, including robotic assembly, smart\ninfrastructure maintenance, and aerospace component servicing. Results indicate\npotential for enhancing training efficiency, remote assistance capabilities,\nand operational guidance in alignment with Industry 5.0's human-centric and\nresilient approach to industrial development.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05451", "pdf": "https://arxiv.org/pdf/2504.05451", "abs": "https://arxiv.org/abs/2504.05451", "authors": ["Arjun Somayazulu", "Efi Mavroudi", "Changan Chen", "Lorenzo Torresani", "Kristen Grauman"], "title": "Learning Activity View-invariance Under Extreme Viewpoint Changes via Curriculum Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional methods for view-invariant learning from video rely on controlled\nmulti-view settings with minimal scene clutter. However, they struggle with\nin-the-wild videos that exhibit extreme viewpoint differences and share little\nvisual content. We introduce a method for learning rich video representations\nin the presence of such severe view-occlusions. We first define a\ngeometry-based metric that ranks views at a fine-grained temporal scale by\ntheir likely occlusion level. Then, using those rankings, we formulate a\nknowledge distillation objective that preserves action-centric semantics with a\nnovel curriculum learning procedure that pairs incrementally more challenging\nviews over time, thereby allowing smooth adaptation to extreme viewpoint\ndifferences. We evaluate our approach on two tasks, outperforming SOTA models\non both temporal keystep grounding and fine-grained keystep recognition\nbenchmarks - particularly on views that exhibit severe occlusion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05571", "pdf": "https://arxiv.org/pdf/2504.05571", "abs": "https://arxiv.org/abs/2504.05571", "authors": ["Oded Ovadia", "Meni Brief", "Rachel Lemberg", "Eitam Sheetrit"], "title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) acquire vast knowledge during\npre-training, they often lack domain-specific, new, or niche information.\nContinual pre-training (CPT) attempts to address this gap but suffers from\ncatastrophic forgetting and inefficiencies in low-data regimes. We introduce\nKnowledge-Instruct, a novel approach to efficiently inject knowledge from\nlimited corpora through pure instruction-tuning. By generating\ninformation-dense synthetic instruction data, it effectively integrates new\nknowledge while preserving general reasoning and instruction-following\nabilities. Knowledge-Instruct demonstrates superior factual memorization,\nminimizes catastrophic forgetting, and remains scalable by leveraging synthetic\ndata from relatively small language models. Additionally, it enhances\ncontextual understanding, including complex multi-hop reasoning, facilitating\nintegration with retrieval systems. We validate its effectiveness across\ndiverse benchmarks, including Companies, a new dataset that we release to\nmeasure knowledge injection capabilities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05483", "pdf": "https://arxiv.org/pdf/2504.05483", "abs": "https://arxiv.org/abs/2504.05483", "authors": ["Mohammad Hossein Najafi", "Mohammad Morsali", "Mohammadreza Pashanejad", "Saman Soleimani Roudi", "Mohammad Norouzi", "Saeed Bagheri Shouraki"], "title": "Secure Diagnostics: Adversarial Robustness Meets Clinical Interpretability", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks for medical image classification often fail to\ngeneralize consistently in clinical practice due to violations of the i.i.d.\nassumption and opaque decision-making. This paper examines interpretability in\ndeep neural networks fine-tuned for fracture detection by evaluating model\nperformance against adversarial attack and comparing interpretability methods\nto fracture regions annotated by an orthopedic surgeon. Our findings prove that\nrobust models yield explanations more aligned with clinically meaningful areas,\nindicating that robustness encourages anatomically relevant feature\nprioritization. We emphasize the value of interpretability for facilitating\nhuman-AI collaboration, in which models serve as assistants under a\nhuman-in-the-loop paradigm: clinically plausible explanations foster trust,\nenable error correction, and discourage reliance on AI for high-stakes\ndecisions. This paper investigates robustness and interpretability as\ncomplementary benchmarks for bridging the gap between benchmark performance and\nsafe, actionable clinical deployment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05491", "pdf": "https://arxiv.org/pdf/2504.05491", "abs": "https://arxiv.org/abs/2504.05491", "authors": ["Sakib Reza", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Mohsen Moghaddam", "Octavia Camps"], "title": "REEF: Relevance-Aware and Efficient LLM Adapter for Video Understanding", "categories": ["cs.CV"], "comment": "Accepted at CVPRW'25", "summary": "Integrating vision models into large language models (LLMs) has sparked\nsignificant interest in creating vision-language foundation models, especially\nfor video understanding. Recent methods often utilize memory banks to handle\nuntrimmed videos for video-level understanding. However, they typically\ncompress visual memory using similarity-based greedy approaches, which can\noverlook the contextual importance of individual tokens. To address this, we\nintroduce an efficient LLM adapter designed for video-level understanding of\nuntrimmed videos that prioritizes the contextual relevance of spatio-temporal\ntokens. Our framework leverages scorer networks to selectively compress the\nvisual memory bank and filter spatial tokens based on relevance, using a\ndifferentiable Top-K operator for end-to-end training. Across three key\nvideo-level understanding tasks$\\unicode{x2013}$ untrimmed video\nclassification, video question answering, and video\ncaptioning$\\unicode{x2013}$our method achieves competitive or superior results\non four large-scale datasets while reducing computational overhead by up to\n34%. The code will be available soon on GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05632", "pdf": "https://arxiv.org/pdf/2504.05632", "abs": "https://arxiv.org/abs/2504.05632", "authors": ["Sanchit Kabra", "Akshita Jha", "Chandan Reddy"], "title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages", "summary": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05642", "pdf": "https://arxiv.org/pdf/2504.05642", "abs": "https://arxiv.org/abs/2504.05642", "authors": ["Subhankar Maity", "Aniket Deroy"], "title": "Leveraging Prompt-Tuning for Bengali Grammatical Error Explanation Using Large Language Models", "categories": ["cs.CL"], "comment": "9 pages, 2 figures", "summary": "We propose a novel three-step prompt-tuning method for Bengali Grammatical\nError Explanation (BGEE) using state-of-the-art large language models (LLMs)\nsuch as GPT-4, GPT-3.5 Turbo, and Llama-2-70b. Our approach involves\nidentifying and categorizing grammatical errors in Bengali sentences,\ngenerating corrected versions of the sentences, and providing natural language\nexplanations for each identified error. We evaluate the performance of our BGEE\nsystem using both automated evaluation metrics and human evaluation conducted\nby experienced Bengali language experts. Our proposed prompt-tuning approach\nshows that GPT-4, the best performing LLM, surpasses the baseline model in\nautomated evaluation metrics, with a 5.26% improvement in F1 score and a 6.95%\nimprovement in exact match. Furthermore, compared to the previous baseline,\nGPT-4 demonstrates a decrease of 25.51% in wrong error type and a decrease of\n26.27% in wrong error explanation. However, the results still lag behind the\nhuman baseline.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05747", "pdf": "https://arxiv.org/pdf/2504.05747", "abs": "https://arxiv.org/abs/2504.05747", "authors": ["Raymond Ng", "Thanh Ngan Nguyen", "Yuli Huang", "Ngee Chia Tai", "Wai Yi Leong", "Wei Qi Leong", "Xianbin Yong", "Jian Gang Ngui", "Yosephine Susanto", "Nicholas Cheng", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Adithya Venkatadri Hulagadri", "Kok Wai Teng", "Yeo Yeow Tong", "Bryan Siow", "Wei Yi Teo", "Wayne Lau", "Choon Meng Tan", "Brandon Ong", "Zhi Hao Ong", "Jann Railey Montalan", "Adwin Chan", "Sajeban Antonyrex", "Ren Lee", "Esther Choa", "David Ong Tat-Wee", "Bing Jie Darius Liu", "William Chandra Tjhi", "Erik Cambria", "Leslie Teo"], "title": "SEA-LION: Southeast Asian Languages in One Network", "categories": ["cs.CL"], "comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581", "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05764", "pdf": "https://arxiv.org/pdf/2504.05764", "abs": "https://arxiv.org/abs/2504.05764", "authors": ["Jiho Gwak", "Yuchul Jung"], "title": "Layer-Aware Embedding Fusion for LLMs in Text Classifications", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "11 pages, 3 figures, Preprint", "summary": "Embedding fusion has emerged as an effective approach for enhancing\nperformance across various NLP tasks. However, systematic guidelines for\nselecting optimal layers and developing effective fusion strategies for the\nintegration of LLMs remain underexplored. In this study, we propose a\nlayer-aware embedding selection method and investigate how to quantitatively\nevaluate different layers to identify the most important ones for downstream\nNLP tasks, showing that the critical layers vary depending on the dataset. We\nalso explore how combining embeddings from multiple LLMs, without requiring\nmodel fine-tuning, can improve performance. Experiments on four English text\nclassification datasets (SST-2, MR, R8, and R52) demonstrate that different\nlayers in LLMs exhibit varying degrees of representational strength for\nclassification, and that combining embeddings from different models can enhance\nperformance if the models exhibit complementary characteristics. Additionally,\nwe discuss resources overhead (memory and inference time) to provide a balanced\nperspective on the real world feasibility of embedding fusion. Future work will\nexplore multilingual and domain specific datasets, as well as techniques for\nautomating layer selection, to improve both performance and scalability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05613", "pdf": "https://arxiv.org/pdf/2504.05613", "abs": "https://arxiv.org/abs/2504.05613", "authors": ["Xiao Zhang", "Xiangyu Han", "Xiwen Lai", "Yao Sun", "Pei Zhang", "Konrad Kording"], "title": "Falcon: Fractional Alternating Cut with Overcoming Minima in Unsupervised Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Today's unsupervised image segmentation algorithms often segment\nsuboptimally. Modern graph-cut based approaches rely on high-dimensional\nattention maps from Transformer-based foundation models, typically employing a\nrelaxed Normalized Cut solved recursively via the Fiedler vector (the\neigenvector of the second smallest eigenvalue). Consequently, they still lag\nbehind supervised methods in both mask generation speed and segmentation\naccuracy. We present a regularized fractional alternating cut (Falcon), an\noptimization-based K-way Normalized Cut without relying on recursive\neigenvector computations, achieving substantially improved speed and accuracy.\nFalcon operates in two stages: (1) a fast K-way Normalized Cut solved by\nextending into a fractional quadratic transformation, with an alternating\niterative procedure and regularization to avoid local minima; and (2)\nrefinement of the resulting masks using complementary low-level information,\nproducing high-quality pixel-level segmentations. Experiments show that Falcon\nnot only surpasses existing state-of-the-art methods by an average of 2.5%\nacross six widely recognized benchmarks (reaching up to 4.3\\% improvement on\nCityscapes), but also reduces runtime by around 30% compared to prior\ngraph-based approaches. These findings demonstrate that the semantic\ninformation within foundation-model attention can be effectively harnessed by a\nhighly parallelizable graph cut framework. Consequently, Falcon can narrow the\ngap between unsupervised and supervised segmentation, enhancing scalability in\nreal-world applications and paving the way for dense prediction-based vision\npre-training in various downstream tasks. The code is released in\nhttps://github.com/KordingLab/Falcon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05640", "pdf": "https://arxiv.org/pdf/2504.05640", "abs": "https://arxiv.org/abs/2504.05640", "authors": ["Mingyang Zhu", "Yuqiu Liang", "Jiacheng Wang"], "title": "CTI-Unet: Cascaded Threshold Integration for Improved U-Net Segmentation of Pathology Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Chronic kidney disease (CKD) is a growing global health concern,\nnecessitating precise and efficient image analysis to aid diagnosis and\ntreatment planning. Automated segmentation of kidney pathology images plays a\ncentral role in facilitating clinical workflows, yet conventional segmentation\nmodels often require delicate threshold tuning. This paper proposes a novel\n\\textit{Cascaded Threshold-Integrated U-Net (CTI-Unet)} to overcome the\nlimitations of single-threshold segmentation. By sequentially integrating\nmultiple thresholded outputs, our approach can reconcile noise suppression with\nthe preservation of finer structural details. Experiments on the challenging\nKPIs2024 dataset demonstrate that CTI-Unet outperforms state-of-the-art\narchitectures such as nnU-Net, Swin-Unet, and CE-Net, offering a robust and\nflexible framework for kidney pathology image segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05644", "pdf": "https://arxiv.org/pdf/2504.05644", "abs": "https://arxiv.org/abs/2504.05644", "authors": ["Yan Zhang", "Zhong Ji", "Changxu Meng", "Yanwei Pang", "Jungong Han"], "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR),\nwhich aims at searching for the corresponding targets based on the given query.\nAmong these efforts, the application of Foundation Models (FMs), such as CLIP,\nto the domain of remote sensing has yielded encouraging outcomes. However,\nexisting FM based methodologies neglect the negative impact of weakly\ncorrelated sample pairs and fail to account for the key distinctions among\nremote sensing texts, leading to biased and superficial exploration of sample\npairs. To address these challenges, we propose an approach named iEBAKER (an\nImproved Eliminate Before Align strategy with Keyword Explicit Reasoning\nframework) for RSITR. Specifically, we propose an innovative Eliminate Before\nAlign (EBA) strategy to filter out the weakly correlated sample pairs, thereby\nmitigating their deviations from optimal embedding space during\nalignment.Further, two specific schemes are introduced from the perspective of\nwhether local similarity and global similarity affect each other. On this\nbasis, we introduce an alternative Sort After Reversed Retrieval (SAR)\nstrategy, aims at optimizing the similarity matrix via reverse retrieval.\nAdditionally, we incorporate a Keyword Explicit Reasoning (KER) module to\nfacilitate the beneficial impact of subtle key concept distinctions. Without\nbells and whistles, our approach enables a direct transition from FM to RSITR\ntask, eliminating the need for additional pretraining on remote sensing data.\nExtensive experiments conducted on three popular benchmark datasets demonstrate\nthat our proposed iEBAKER method surpasses the state-of-the-art models while\nrequiring less training data. Our source code will be released at\nhttps://github.com/zhangy0822/iEBAKER.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05855", "pdf": "https://arxiv.org/pdf/2504.05855", "abs": "https://arxiv.org/abs/2504.05855", "authors": ["Xingzu Liu", "Songhang deng", "Mingbang Wang", "Zhang Dong", "Le Dai", "Jiyuan Li", "Ruilin Nong"], "title": "Enhancing Coreference Resolution with Pretrained Language Models: Bridging the Gap Between Syntax and Semantics", "categories": ["cs.CL", "cs.AI"], "comment": "acl submission", "summary": "Large language models have made significant advancements in various natural\nlanguage processing tasks, including coreference resolution. However,\ntraditional methods often fall short in effectively distinguishing referential\nrelationships due to a lack of integration between syntactic and semantic\ninformation. This study introduces an innovative framework aimed at enhancing\ncoreference resolution by utilizing pretrained language models. Our approach\ncombines syntax parsing with semantic role labeling to accurately capture finer\ndistinctions in referential relationships. By employing state-of-the-art\npretrained models to gather contextual embeddings and applying an attention\nmechanism for fine-tuning, we improve the performance of coreference tasks.\nExperimental results across diverse datasets show that our method surpasses\nconventional coreference resolution systems, achieving notable accuracy in\ndisambiguating references. This development not only improves coreference\nresolution outcomes but also positively impacts other natural language\nprocessing tasks that depend on precise referential understanding.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05649", "pdf": "https://arxiv.org/pdf/2504.05649", "abs": "https://arxiv.org/abs/2504.05649", "authors": ["Yining Shi", "Kun Jiang", "Xin Zhao", "Kangan Qian", "Chuchu Xie", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "title": "POD: Predictive Object Detection with Single-Frame FMCW LiDAR Point Cloud", "categories": ["cs.CV"], "comment": null, "summary": "LiDAR-based 3D object detection is a fundamental task in the field of\nautonomous driving. This paper explores the unique advantage of Frequency\nModulated Continuous Wave (FMCW) LiDAR in autonomous perception. Given a single\nframe FMCW point cloud with radial velocity measurements, we expect that our\nobject detector can detect the short-term future locations of objects using\nonly the current frame sensor data and demonstrate a fast ability to respond to\nintermediate danger. To achieve this, we extend the standard object detection\ntask to a novel task named predictive object detection (POD), which aims to\npredict the short-term future location and dimensions of objects based solely\non current observations. Typically, a motion prediction task requires\nhistorical sensor information to process the temporal contexts of each object,\nwhile our detector's avoidance of multi-frame historical information enables a\nmuch faster response time to potential dangers. The core advantage of FMCW\nLiDAR lies in the radial velocity associated with every reflected point. We\npropose a novel POD framework, the core idea of which is to generate a virtual\nfuture point using a ray casting mechanism, create virtual two-frame point\nclouds with the current and virtual future frames, and encode these two-frame\nvoxel features with a sparse 4D encoder. Subsequently, the 4D voxel features\nare separated by temporal indices and remapped into two Bird's Eye View (BEV)\nfeatures: one decoded for standard current frame object detection and the other\nfor future predictive object detection. Extensive experiments on our in-house\ndataset demonstrate the state-of-the-art standard and predictive detection\nperformance of the proposed POD framework.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05914", "pdf": "https://arxiv.org/pdf/2504.05914", "abs": "https://arxiv.org/abs/2504.05914", "authors": ["Abhiram Reddy Yanampally"], "title": "High-Resource Translation:Turning Abundance into Accessibility", "categories": ["cs.CL"], "comment": "6 pages, 2 figures", "summary": "This paper presents a novel approach to constructing an English-to-Telugu\ntranslation model by leveraging transfer learning techniques and addressing the\nchallenges associated with low-resource languages. Utilizing the Bharat\nParallel Corpus Collection (BPCC) as the primary dataset, the model\nincorporates iterative backtranslation to generate synthetic parallel data,\neffectively augmenting the training dataset and enhancing the model's\ntranslation capabilities. The research focuses on a comprehensive strategy for\nimproving model performance through data augmentation, optimization of training\nparameters, and the effective use of pre-trained models. These methodologies\naim to create a robust translation system that can handle diverse sentence\nstructures and linguistic nuances in both English and Telugu. This work\nhighlights the significance of innovative data handling techniques and the\npotential of transfer learning in overcoming limitations posed by sparse\ndatasets in low-resource languages. The study contributes to the field of\nmachine translation and seeks to improve communication between English and\nTelugu speakers in practical contexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05995", "pdf": "https://arxiv.org/pdf/2504.05995", "abs": "https://arxiv.org/abs/2504.05995", "authors": ["Firoj Alam", "Md Arid Hasan", "Sahinur Rahman Laskar", "Mucahid Kutlu", "Shammur Absar Chowdhury"], "title": "NativQA Framework: Enabling LLMs with Native, Local, and Everyday Knowledge", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout cultural bias, fairness, and their applicability in diverse linguistic\nand underrepresented regional contexts. To enhance and benchmark the\ncapabilities of LLMs, there is a need to develop large-scale resources focused\non multilingual, local, and cultural contexts. In this study, we propose a\nframework, NativQA, that can seamlessly construct large-scale, culturally and\nregionally aligned QA datasets in native languages. The framework utilizes\nuser-defined seed queries and leverages search engines to collect\nlocation-specific, everyday information. It has been evaluated across 39\nlocations in 24 countries and in 7 languages, ranging from extremely\nlow-resource to high-resource languages, which resulted over 300K Question\nAnswer (QA) pairs. The developed resources can be used for LLM benchmarking and\nfurther fine-tuning. The framework has been made publicly available for the\ncommunity (https://gitlab.com/nativqa/nativqa-framework).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05677", "pdf": "https://arxiv.org/pdf/2504.05677", "abs": "https://arxiv.org/abs/2504.05677", "authors": ["Shunsuke Sakai", "Shunsuke Tsuge", "Tatsuhito Hasegawa"], "title": "Noisy Deep Ensemble: Accelerating Deep Ensemble Learning via Noise Injection", "categories": ["cs.CV"], "comment": null, "summary": "Neural network ensembles is a simple yet effective approach for enhancing\ngeneralization capabilities. The most common method involves independently\ntraining multiple neural networks initialized with different weights and then\naveraging their predictions during inference. However, this approach increases\ntraining time linearly with the number of ensemble members. To address this\nissue, we propose the novel ``\\textbf{Noisy Deep Ensemble}'' method,\nsignificantly reducing the training time required for neural network ensembles.\nIn this method, a \\textit{parent model} is trained until convergence, and then\nthe weights of the \\textit{parent model} are perturbed in various ways to\nconstruct multiple \\textit{child models}. This perturbation of the\n\\textit{parent model} weights facilitates the exploration of different local\nminima while significantly reducing the training time for each ensemble member.\nWe evaluated our method using diverse CNN architectures on CIFAR-10 and\nCIFAR-100 datasets, surpassing conventional efficient ensemble methods and\nachieving test accuracy comparable to standard ensembles. Code is available at\n\\href{https://github.com/TSTB-dev/NoisyDeepEnsemble}{https://github.com/TSTB-dev/NoisyDeepEnsemble}", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06037", "pdf": "https://arxiv.org/pdf/2504.06037", "abs": "https://arxiv.org/abs/2504.06037", "authors": ["Seunghyun Ji", "Soowon Lee"], "title": "Confidence Regularized Masked Language Modeling using Text Length", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 1 figure", "summary": "Masked language modeling, which is a task to predict a randomly masked word\nin the input text, is an efficient language representation learning method.\nMasked language modeling ignores various words which people can think of for\nfilling in the masked position and calculates the loss with a single word.\nEspecially when the input text is short, the entropy of the word distribution\nthat can fill in the masked position can be high. This may cause the model to\nbe overconfident in the single answer. To address this issue, we propose a\nnovel confidence regularizer that controls regularizing strength dynamically by\nthe input text length. Experiments with GLUE and SQuAD datasets showed that our\nmethod achieves better accuracy and lower expected calibration error.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05698", "pdf": "https://arxiv.org/pdf/2504.05698", "abs": "https://arxiv.org/abs/2504.05698", "authors": ["Wesley Khademi", "Li Fuxin"], "title": "Point-based Instance Completion with Scene Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Recent point-based object completion methods have demonstrated the ability to\naccurately recover the missing geometry of partially observed objects. However,\nthese approaches are not well-suited for completing objects within a scene, as\nthey do not consider known scene constraints (e.g., other observed surfaces) in\ntheir completions and further expect the partial input to be in a canonical\ncoordinate system, which does not hold for objects within scenes. While\ninstance scene completion methods have been proposed for completing objects\nwithin a scene, they lag behind point-based object completion methods in terms\nof object completion quality and still do not consider known scene constraints\nduring completion. To overcome these limitations, we propose a point\ncloud-based instance completion model that can robustly complete objects at\narbitrary scales and pose in the scene. To enable reasoning at the scene level,\nwe introduce a sparse set of scene constraints represented as point clouds and\nintegrate them into our completion model via a cross-attention mechanism. To\nevaluate the instance scene completion task on indoor scenes, we further build\na new dataset called ScanWCF, which contains labeled partial scans as well as\naligned ground truth scene completions that are watertight and collision-free.\nThrough several experiments, we demonstrate that our method achieves improved\nfidelity to partial scans, higher completion quality, and greater plausibility\nover existing state-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05746", "pdf": "https://arxiv.org/pdf/2504.05746", "abs": "https://arxiv.org/abs/2504.05746", "authors": ["Zhihua Xu", "Tianshui Chen", "Zhijing Yang", "Siyuan Peng", "Keze Wang", "Liang Lin"], "title": "Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation", "categories": ["cs.CV"], "comment": "Accepted at TMM 2025", "summary": "The paramount challenge in audio-driven One-shot Talking Head Animation\n(ADOS-THA) lies in capturing subtle imperceptible changes between adjacent\nvideo frames. Inherently, the temporal relationship of adjacent audio clips is\nhighly correlated with that of the corresponding adjacent video frames,\noffering supplementary information that can be pivotal for guiding and\nsupervising talking head animations. In this work, we propose to learn\naudio-visual correlations and integrate the correlations to help enhance\nfeature representation and regularize final generation by a novel Temporal\nAudio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first\nlearns an audio-visual temporal correlation metric, ensuring the temporal audio\nrelationships of adjacent clips are aligned with the temporal visual\nrelationships of corresponding adjacent video frames. Since the temporal audio\nrelationship contains aligned information about the visual frame, we first\nintegrate it to guide learning more representative features via a simple yet\neffective channel attention mechanism. During training, we also use the\nalignment correlations as an additional objective to supervise generating\nvisual frames. We conduct extensive experiments on several publicly available\nbenchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its\nsuperiority over existing leading algorithms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05751", "pdf": "https://arxiv.org/pdf/2504.05751", "abs": "https://arxiv.org/abs/2504.05751", "authors": ["Jiangsan Zhao", "Jakob Geipel", "Krzysztof Kusnierek", "Xuean Cui"], "title": "InvNeRF-Seg: Fine-Tuning a Pre-Trained NeRF for 3D Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have been widely adopted for reconstructing\nhigh quality 3D point clouds from 2D RGB images. However, the segmentation of\nthese reconstructed 3D scenes is more essential for downstream tasks such as\nobject counting, size estimation, and scene understanding. While segmentation\non raw 3D point clouds using deep learning requires labor intensive and\ntime-consuming manual annotation, directly training NeRF on binary masks also\nfails due to the absence of color and shading cues essential for geometry\nlearning. We propose Invariant NeRF for Segmentation (InvNeRFSeg), a two step,\nzero change fine tuning strategy for 3D segmentation. We first train a standard\nNeRF on RGB images and then fine tune it using 2D segmentation masks without\naltering either the model architecture or loss function. This approach produces\nhigher quality, cleaner segmented point clouds directly from the refined\nradiance field with minimal computational overhead or complexity. Field density\nanalysis reveals consistent semantic refinement: densities of object regions\nincrease while background densities are suppressed, ensuring clean and\ninterpretable segmentations. We demonstrate InvNeRFSegs superior performance\nover both SA3D and FruitNeRF on both synthetic fruit and real world soybean\ndatasets. This approach effectively extends 2D segmentation to high quality 3D\nsegmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05770", "pdf": "https://arxiv.org/pdf/2504.05770", "abs": "https://arxiv.org/abs/2504.05770", "authors": ["Inho Jake Park", "Jaehoon Jay Jeong", "Ho-Sang Jo"], "title": "A Lightweight Multi-Module Fusion Approach for Korean Character Recognition", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "comment": "12 pages, 5 figures, 5 tables", "summary": "Optical Character Recognition (OCR) is essential in applications such as\ndocument processing, license plate recognition, and intelligent surveillance.\nHowever, existing OCR models often underperform in real-world scenarios due to\nirregular text layouts, poor image quality, character variability, and high\ncomputational costs.\n  This paper introduces SDA-Net (Stroke-Sensitive Attention and Dynamic Context\nEncoding Network), a lightweight and efficient architecture designed for robust\nsingle-character recognition. SDA-Net incorporates: (1) a Dual Attention\nMechanism to enhance stroke-level and spatial feature extraction; (2) a Dynamic\nContext Encoding module that adaptively refines semantic information using a\nlearnable gating mechanism; (3) a U-Net-inspired Feature Fusion Strategy for\ncombining low-level and high-level features; and (4) a highly optimized\nlightweight backbone that reduces memory and computational demands.\n  Experimental results show that SDA-Net achieves state-of-the-art accuracy on\nchallenging OCR benchmarks, with significantly faster inference, making it\nwell-suited for deployment in real-time and edge-based OCR systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05317", "pdf": "https://arxiv.org/pdf/2504.05317", "abs": "https://arxiv.org/abs/2504.05317", "authors": ["Gorjan Radevski", "Kiril Gashteovski", "Shahbaz Syed", "Christopher Malon", "Sebastien Nicolas", "Chia-Chien Hung", "Timo Sztyler", "Verena Heußer", "Wiem Ben Rim", "Masafumi Enomoto", "Kunihiro Takeoka", "Masafumi Oyamada", "Goran Glavaš", "Carolin Lawrence"], "title": "On Synthesizing Data for Context Attribution in Question Answering", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Question Answering (QA) accounts for a significant portion of LLM usage \"in\nthe wild\". However, LLMs sometimes produce false or misleading responses, also\nknown as \"hallucinations\". Therefore, grounding the generated answers in\ncontextually provided information -- i.e., providing evidence for the generated\ntext -- is paramount for LLMs' trustworthiness. Providing this information is\nthe task of context attribution. In this paper, we systematically study\nLLM-based approaches for this task, namely we investigate (i) zero-shot\ninference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic\ndata generated by larger LLMs. Our key contribution is SynQA: a novel\ngenerative strategy for synthesizing context attribution data. Given selected\ncontext sentences, an LLM generates QA pairs that are supported by these\nsentences. This leverages LLMs' natural strengths in text generation while\nensuring clear attribution paths in the synthetic training data. We show that\nthe attribution data synthesized via SynQA is highly effective for fine-tuning\nsmall LMs for context attribution in different QA tasks and domains. Finally,\nwith a user study, we validate the usefulness of small LMs (fine-tuned on\nsynthetic data from SynQA) in context attribution for QA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05346", "pdf": "https://arxiv.org/pdf/2504.05346", "abs": "https://arxiv.org/abs/2504.05346", "authors": ["Ivan Ilin", "Peter Richtarik"], "title": "Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PF", "68T07, 68Q32"], "comment": "8 pages, 3 Figures, 3 Tables, 2 Algorithms, paper comes with Appendix", "summary": "This paper presents Thanos, a novel weight-pruning algorithm designed to\nreduce the memory footprint and enhance the computational efficiency of large\nlanguage models (LLMs) by removing redundant weights while maintaining\naccuracy. Thanos introduces a block-wise pruning strategy with adaptive masks\nthat dynamically adjust to weight importance, enabling flexible sparsity\npatterns and structured formats, such as $n:m$ sparsity, optimized for hardware\nacceleration. Experimental evaluations demonstrate that Thanos achieves\nstate-of-the-art performance in structured pruning and outperforms existing\nmethods in unstructured pruning. By providing an efficient and adaptable\napproach to model compression, Thanos offers a practical solution for deploying\nlarge models in resource-constrained environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05786", "pdf": "https://arxiv.org/pdf/2504.05786", "abs": "https://arxiv.org/abs/2504.05786", "authors": ["Jirong Zha", "Yuxuan Fan", "Xiao Yang", "Chen Gao", "Xinlei Chen"], "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures", "summary": "3D spatial understanding is essential in real-world applications such as\nrobotics, autonomous vehicles, virtual reality, and medical imaging. Recently,\nLarge Language Models (LLMs), having demonstrated remarkable success across\nvarious domains, have been leveraged to enhance 3D understanding tasks, showing\npotential to surpass traditional computer vision methods. In this survey, we\npresent a comprehensive review of methods integrating LLMs with 3D spatial\nunderstanding. We propose a taxonomy that categorizes existing methods into\nthree branches: image-based methods deriving 3D understanding from 2D visual\ndata, point cloud-based methods working directly with 3D representations, and\nhybrid modality-based methods combining multiple data streams. We\nsystematically review representative methods along these categories, covering\ndata representations, architectural modifications, and training strategies that\nbridge textual and 3D modalities. Finally, we discuss current limitations,\nincluding dataset scarcity and computational challenges, while highlighting\npromising research directions in spatial perception, multi-modal fusion, and\nreal-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05419", "pdf": "https://arxiv.org/pdf/2504.05419", "abs": "https://arxiv.org/abs/2504.05419", "authors": ["Anqi Zhang", "Yulin Chen", "Jane Pan", "Chen Zhao", "Aurojit Panda", "Jinyang Li", "He He"], "title": "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning models have achieved remarkable performance on tasks like math and\nlogical reasoning thanks to their ability to search during reasoning. However,\nthey still suffer from overthinking, often performing unnecessary reasoning\nsteps even after reaching the correct answer. This raises the question: can\nmodels evaluate the correctness of their intermediate answers during reasoning?\nIn this work, we study whether reasoning models encode information about answer\ncorrectness through probing the model's hidden states. The resulting probe can\nverify intermediate answers with high accuracy and produces highly calibrated\nscores. Additionally, we find models' hidden states encode correctness of\nfuture answers, enabling early prediction of the correctness before the\nintermediate answer is fully formulated. We then use the probe as a verifier to\ndecide whether to exit reasoning at intermediate answers during inference,\nreducing the number of inference tokens by 24\\% without compromising\nperformance. These findings confirm that reasoning models do encode a notion of\ncorrectness yet fail to exploit it, revealing substantial untapped potential to\nenhance their efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05789", "pdf": "https://arxiv.org/pdf/2504.05789", "abs": "https://arxiv.org/abs/2504.05789", "authors": ["Sarosij Bose", "Hannah Dela Cruz", "Arindam Dutta", "Elena Kokkoni", "Konstantinos Karydis", "Amit K. Roy-Chowdhury"], "title": "Leveraging Synthetic Adult Datasets for Unsupervised Infant Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ABAW@CVPR 2025", "summary": "Human pose estimation is a critical tool across a variety of healthcare\napplications. Despite significant progress in pose estimation algorithms\ntargeting adults, such developments for infants remain limited. Existing\nalgorithms for infant pose estimation, despite achieving commendable\nperformance, depend on fully supervised approaches that require large amounts\nof labeled data. These algorithms also struggle with poor generalizability\nunder distribution shifts. To address these challenges, we introduce SHIFT:\nLeveraging SyntHetic Adult Datasets for Unsupervised InFanT Pose Estimation,\nwhich leverages the pseudo-labeling-based Mean-Teacher framework to compensate\nfor the lack of labeled data and addresses distribution shifts by enforcing\nconsistency between the student and the teacher pseudo-labels. Additionally, to\npenalize implausible predictions obtained from the mean-teacher framework, we\nincorporate an infant manifold pose prior. To enhance SHIFT's self-occlusion\nperception ability, we propose a novel visibility consistency module for\nimproved alignment of the predicted poses with the original image. Extensive\nexperiments on multiple benchmarks show that SHIFT significantly outperforms\nexisting state-of-the-art unsupervised domain adaptation (UDA) pose estimation\nmethods by 5% and supervised infant pose estimation methods by a margin of 16%.\nThe project page is available at: https://sarosijbose.github.io/SHIFT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05518", "pdf": "https://arxiv.org/pdf/2504.05518", "abs": "https://arxiv.org/abs/2504.05518", "authors": ["Rem Yang", "Julian Dai", "Nikos Vasilakis", "Martin Rinard"], "title": "Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "We assess how the code reasoning abilities of large language models (LLMs)\ngeneralize to different kinds of programs. We present techniques for obtaining\nin- and out-of-distribution programs with different characteristics: code\nsampled from a domain-specific language, code automatically generated by an\nLLM, code collected from competitive programming contests, and mutated versions\nof these programs. We also present an experimental methodology for evaluating\nLLM generalization by comparing their performance on these programs. We perform\nan extensive evaluation across 10 state-of-the-art models from the past year,\nobtaining insights into their generalization capabilities over time and across\ndifferent classes of programs. Our results highlight that while earlier models\nexhibit behavior consistent with pattern matching, the latest models exhibit\nstrong generalization abilities on code reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05800", "pdf": "https://arxiv.org/pdf/2504.05800", "abs": "https://arxiv.org/abs/2504.05800", "authors": ["Jaskirat Singh", "Junshen Kevin Chen", "Jonas Kohler", "Michael Cohen"], "title": "Storybooth: Training-free Multi-Subject Consistency for Improved Visual Storytelling", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Training-free consistent text-to-image generation depicting the same subjects\nacross different images is a topic of widespread recent interest. Existing\nworks in this direction predominantly rely on cross-frame self-attention; which\nimproves subject-consistency by allowing tokens in each frame to pay attention\nto tokens in other frames during self-attention computation. While useful for\nsingle subjects, we find that it struggles when scaling to multiple characters.\nIn this work, we first analyze the reason for these limitations. Our\nexploration reveals that the primary-issue stems from self-attention-leakage,\nwhich is exacerbated when trying to ensure consistency across\nmultiple-characters. This happens when tokens from one subject pay attention to\nother characters, causing them to appear like each other (e.g., a dog appearing\nlike a duck). Motivated by these findings, we propose StoryBooth: a\ntraining-free approach for improving multi-character consistency. In\nparticular, we first leverage multi-modal chain-of-thought reasoning and\nregion-based generation to apriori localize the different subjects across the\ndesired story outputs. The final outputs are then generated using a modified\ndiffusion model which consists of two novel layers: 1) a bounded cross-frame\nself-attention layer for reducing inter-character attention leakage, and 2)\ntoken-merging layer for improving consistency of fine-grain subject details.\nThrough both qualitative and quantitative results we find that the proposed\napproach surpasses prior state-of-the-art, exhibiting improved consistency\nacross both multiple-characters and fine-grain subject details.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05605", "pdf": "https://arxiv.org/pdf/2504.05605", "abs": "https://arxiv.org/abs/2504.05605", "authors": ["Gejian Zhao", "Hanzhou Wu", "Xinpeng Zhang", "Athanasios V. Vasilakos"], "title": "ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai\n  University", "summary": "Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning\ntasks, but it also introduces new security issues. In this work, we present\nShadowCoT, a novel backdoor attack framework that targets the internal\nreasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks,\nShadowCoT directly manipulates the model's cognitive reasoning path, enabling\nit to hijack multi-step reasoning chains and produce logically coherent but\nadversarial outcomes. By conditioning on internal reasoning states, ShadowCoT\nlearns to recognize and selectively disrupt key reasoning steps, effectively\nmounting a self-reflective cognitive attack within the target model. Our\napproach introduces a lightweight yet effective multi-stage injection pipeline,\nwhich selectively rewires attention pathways and perturbs intermediate\nrepresentations with minimal parameter overhead (only 0.15% updated). ShadowCoT\nfurther leverages reinforcement learning and reasoning chain pollution (RCP) to\nautonomously synthesize stealthy adversarial CoTs that remain undetectable to\nadvanced defenses. Extensive experiments across diverse reasoning benchmarks\nand LLMs show that ShadowCoT consistently achieves high Attack Success Rate\n(94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance.\nThese results reveal an emergent class of cognition-level threats and highlight\nthe urgent need for defenses beyond shallow surface-level consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05810", "pdf": "https://arxiv.org/pdf/2504.05810", "abs": "https://arxiv.org/abs/2504.05810", "authors": ["Xinpeng Ding", "Kui Zhang", "Jinahua Han", "Lanqing Hong", "Hang Xu", "Xiaomeng Li"], "title": "PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning", "categories": ["cs.CV"], "comment": null, "summary": "Direct Preference Optimization (DPO) helps reduce hallucinations in Video\nMultimodal Large Language Models (VLLMs), but its reliance on offline\npreference data limits adaptability and fails to capture true video-response\nmisalignment. We propose Video Direct Preference Optimization (VDPO), an online\npreference learning framework that eliminates the need for preference\nannotation by leveraging video augmentations to generate rejected samples while\nkeeping responses fixed. However, selecting effective augmentations is\nnon-trivial, as some clips may be semantically identical to the original under\nspecific prompts, leading to false rejections and disrupting alignment. To\naddress this, we introduce Prompt-aware Multi-instance Learning VDPO\n(PaMi-VDPO), which selects augmentations based on prompt context. Instead of a\nsingle rejection, we construct a candidate set of augmented clips and apply a\nclose-to-far selection strategy, initially ensuring all clips are semantically\nrelevant while then prioritizing the most prompt-aware distinct clip. This\nallows the model to better capture meaningful visual differences, mitigating\nhallucinations, while avoiding false rejections, and improving alignment.\nPaMi-VDPOseamlessly integrates into existing VLLMs without additional\nparameters, GPT-4/human supervision. With only 10k SFT data, it improves the\nbase model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining\nstable performance on general video benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05731", "pdf": "https://arxiv.org/pdf/2504.05731", "abs": "https://arxiv.org/abs/2504.05731", "authors": ["Teng Shi", "Jun Xu", "Xiao Zhang", "Xiaoxue Zang", "Kai Zheng", "Yang Song", "Han Li"], "title": "Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Recently, the personalization of Large Language Models (LLMs) to generate\ncontent that aligns with individual user preferences has garnered widespread\nattention. Personalized Retrieval-Augmented Generation (RAG), which retrieves\nrelevant documents from the user's history to reflect their preferences and\nenhance LLM generation, is one commonly used approach for personalization.\nHowever, existing personalized RAG methods do not consider that the histories\nof similar users can also assist in personalized generation for the current\nuser, meaning that collaborative information between users can also benefit\npersonalized generation. Inspired by the application of collaborative filtering\nin recommender systems, we propose a method called CFRAG, which adapts\nCollaborative Filtering to RAG for personalized text generation. However, this\npresents two challenges: (1)~how to incorporate collaborative information\nwithout explicit user similarity labels? (2)~how to retrieve documents that\nsupport personalized LLM generation? For Challenge 1, we use contrastive\nlearning to train user embeddings to retrieve similar users and introduce\ncollaborative information. For Challenge 2, we design a personalized retriever\nand reranker to retrieve the top-$k$ documents from these users' histories. We\ntake into account the user's preference during retrieval and reranking. Then we\nleverage feedback from the LLM to fine-tune the personalized retriever and\nreranker, enabling them to retrieve documents that meet the personalized\ngeneration needs of the LLM. Experimental results on the Language Model\nPersonalization (LaMP) benchmark validate the effectiveness of CFRAG. Further\nanalysis confirms the importance of incorporating collaborative information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05908", "pdf": "https://arxiv.org/pdf/2504.05908", "abs": "https://arxiv.org/abs/2504.05908", "authors": ["Sriram Mandalika", "Lalitha V", "Athira Nambiar"], "title": "PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at The IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 - CVPRW", "summary": "Driving scene understanding is a critical real-world problem that involves\ninterpreting and associating various elements of a driving environment, such as\nvehicles, pedestrians, and traffic signals. Despite advancements in autonomous\ndriving, traditional pipelines rely on deterministic models that fail to\ncapture the probabilistic nature and inherent uncertainty of real-world\ndriving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware\nmodel for object interaction and Chain-of-Thought (CoT) reasoning in driving\nscenarios. In particular, our approach combines LiDAR-based 3D object detection\nwith multi-view RGB references to ensure interpretable and reliable scene\nunderstanding. Uncertainty and risk assessment, along with object interactions,\nare modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic\nreasoning under ambiguous conditions. Interpretable decisions are facilitated\nthrough CoT reasoning, leveraging object dynamics and contextual cues, while\nGrad-CAM visualizations highlight attention regions. Extensive evaluations on\nthe DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms\nstate-of-the-art CoT and risk-aware models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05966", "pdf": "https://arxiv.org/pdf/2504.05966", "abs": "https://arxiv.org/abs/2504.05966", "authors": ["Xiaolin Fan", "Yan Wang", "Yingying Zhang", "Mingkun Bao", "Bosen Jia", "Dong Lu", "Yifan Gu", "Jian Cheng", "Haogang Zhu"], "title": "AVP-AP: Self-supervised Automatic View Positioning in 3D cardiac CT via Atlas Prompting", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, published to TMI", "summary": "Automatic view positioning is crucial for cardiac computed tomography (CT)\nexaminations, including disease diagnosis and surgical planning. However, it is\nhighly challenging due to individual variability and large 3D search space.\nExisting work needs labor-intensive and time-consuming manual annotations to\ntrain view-specific models, which are limited to predicting only a fixed set of\nplanes. However, in real clinical scenarios, the challenge of positioning\nsemantic 2D slices with any orientation into varying coordinate space in\narbitrary 3D volume remains unsolved. We thus introduce a novel framework,\nAVP-AP, the first to use Atlas Prompting for self-supervised Automatic View\nPositioning in the 3D CT volume. Specifically, this paper first proposes an\natlas prompting method, which generates a 3D canonical atlas and trains a\nnetwork to map slices into their corresponding positions in the atlas space via\na self-supervised manner. Then, guided by atlas prompts corresponding to the\ngiven query images in a reference CT, we identify the coarse positions of\nslices in the target CT volume using rigid transformation between the 3D atlas\nand target CT volume, effectively reducing the search space. Finally, we refine\nthe coarse positions by maximizing the similarity between the predicted slices\nand the query images in the feature space of a given foundation model. Our\nframework is flexible and efficient compared to other methods, outperforming\nother methods by 19.8% average structural similarity (SSIM) in arbitrary view\npositioning and achieving 9% SSIM in two-chamber view compared to four\nradiologists. Meanwhile, experiments on a public dataset validate our\nframework's generalizability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05977", "pdf": "https://arxiv.org/pdf/2504.05977", "abs": "https://arxiv.org/abs/2504.05977", "authors": ["Jakob Lønborg Christensen", "Morten Rieger Hannemose", "Anders Bjorholm Dahl", "Vedrana Andersen Dahl"], "title": "Diffusion Based Ambiguous Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at SCIA25", "summary": "Medical image segmentation often involves inherent uncertainty due to\nvariations in expert annotations. Capturing this uncertainty is an important\ngoal and previous works have used various generative image models for the\npurpose of representing the full distribution of plausible expert ground\ntruths. In this work, we explore the design space of diffusion models for\ngenerative segmentation, investigating the impact of noise schedules,\nprediction types, and loss weightings. Notably, we find that making the noise\nschedule harder with input scaling significantly improves performance. We\nconclude that x- and v-prediction outperform epsilon-prediction, likely because\nthe diffusion process is in the discrete segmentation domain. Many loss\nweightings achieve similar performance as long as they give enough weight to\nthe end of the diffusion process. We base our experiments on the LIDC-IDRI lung\nlesion dataset and obtain state-of-the-art (SOTA) performance. Additionally, we\nintroduce a randomly cropped variant of the LIDC-IDRI dataset that is better\nsuited for uncertainty in image segmentation. Our model also achieves SOTA in\nthis harder setting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05979", "pdf": "https://arxiv.org/pdf/2504.05979", "abs": "https://arxiv.org/abs/2504.05979", "authors": ["Sixiang Chen", "Jinbin Bai", "Zhuoran Zhao", "Tian Ye", "Qingyu Shi", "Donghao Zhou", "Wenhao Chai", "Xin Lin", "Jianzong Wu", "Chao Tang", "Shilin Xu", "Tao Zhang", "Haobo Yuan", "Yikang Zhou", "Wei Chow", "Linfeng Li", "Xiangtai Li", "Lei Zhu", "Lu Qi"], "title": "An Empirical Study of GPT-4o Image Generation Capabilities", "categories": ["cs.CV"], "comment": null, "summary": "The landscape of image generation has rapidly evolved, from early GAN-based\napproaches to diffusion models and, most recently, to unified generative\narchitectures that seek to bridge understanding and generation tasks. Recent\nadvances, especially the GPT-4o, have demonstrated the feasibility of\nhigh-fidelity multimodal generation, their architectural design remains\nmysterious and unpublished. This prompts the question of whether image and text\ngeneration have already been successfully integrated into a unified framework\nfor those methods. In this work, we conduct an empirical study of GPT-4o's\nimage generation capabilities, benchmarking it against leading open-source and\ncommercial models. Our evaluation covers four main categories, including\ntext-to-image, image-to-image, image-to-3D, and image-to-X generation, with\nmore than 20 tasks. Our analysis highlights the strengths and limitations of\nGPT-4o under various settings, and situates it within the broader evolution of\ngenerative modeling. Through this investigation, we identify promising\ndirections for future unified generative models, emphasizing the role of\narchitectural design and data scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05992", "pdf": "https://arxiv.org/pdf/2504.05992", "abs": "https://arxiv.org/abs/2504.05992", "authors": ["Jie Yang", "Chang Su", "Yuhan Zhang", "Jianjun Zhu", "Jianli Wang"], "title": "Under-Sampled High-Dimensional Data Recovery via Symbiotic Multi-Prior Tensor Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The advancement of sensing technology has driven the widespread application\nof high-dimensional data. However, issues such as missing entries during\nacquisition and transmission negatively impact the accuracy of subsequent\ntasks. Tensor reconstruction aims to recover the underlying complete data from\nunder-sampled observed data by exploring prior information in high-dimensional\ndata. However, due to insufficient exploration, reconstruction methods still\nface challenges when sampling rate is extremely low. This work proposes a\ntensor reconstruction method integrating multiple priors to comprehensively\nexploit the inherent structure of the data. Specifically, the method combines\nlearnable tensor decomposition to enforce low-rank constraints of the\nreconstructed data, a pre-trained convolutional neural network for smoothing\nand denoising, and block-matching and 3D filtering regularization to enhance\nthe non-local similarity in the reconstructed data. An alternating direction\nmethod of the multipliers algorithm is designed to decompose the resulting\noptimization problem into three subproblems for efficient resolution. Extensive\nexperiments on color images, hyperspectral images, and grayscale videos\ndatasets demonstrate the superiority of our method in extreme cases as compared\nwith state-of-the-art methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06010", "pdf": "https://arxiv.org/pdf/2504.06010", "abs": "https://arxiv.org/abs/2504.06010", "authors": ["Stefanos-Iordanis Papadopoulos", "Christos Koutlis", "Symeon Papadopoulos", "Panagiotis C. Petrantonakis"], "title": "Latent Multimodal Reconstruction for Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Multimodal misinformation, such as miscaptioned images, where captions\nmisrepresent an image's origin, context, or meaning, poses a growing challenge\nin the digital age. To support fact-checkers, researchers have been focusing on\ncreating datasets and developing methods for multimodal misinformation\ndetection (MMD). Due to the scarcity of large-scale annotated MMD datasets,\nrecent studies leverage synthetic training data via out-of-context\nimage-caption pairs or named entity manipulations; altering names, dates, and\nlocations. However, these approaches often produce simplistic misinformation\nthat fails to reflect real-world complexity, limiting the robustness of\ndetection models trained on them. Meanwhile, despite recent advancements, Large\nVision-Language Models (LVLMs) remain underutilized for generating diverse,\nrealistic synthetic training data for MMD. To address this gap, we introduce\n\"MisCaption This!\", a training dataset comprising LVLM-generated miscaptioned\nimages. Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR),\na network trained to reconstruct the embeddings of truthful captions, providing\na strong auxiliary signal to the detection process. To optimize LAMAR, we\nexplore different training strategies (end-to-end training and large-scale\npre-training) and integration approaches (direct, mask, gate, and attention).\nExtensive experiments show that models trained on \"MisCaption This!\" generalize\nbetter on real-world misinformation, while LAMAR sets new state-of-the-art on\nboth NewsCLIPpings and VERITE benchmarks; highlighting the potential of\nLVLM-generated data and reconstruction-based approaches for advancing MMD. We\nrelease our code at:\nhttps://github.com/stevejpapad/miscaptioned-image-reconstruction", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06021", "pdf": "https://arxiv.org/pdf/2504.06021", "abs": "https://arxiv.org/abs/2504.06021", "authors": ["Dahyun Kang", "Ahmet Iscen", "Eunchan Jo", "Sua Choi", "Minsu Cho", "Cordelia Schmid"], "title": "Memory-Modular Classification: Learning to Generalize with Memory Replacement", "categories": ["cs.CV"], "comment": "Accepted to TMLR. Code available: https://github.com/dahyun-kang/mml", "summary": "We propose a novel memory-modular learner for image classification that\nseparates knowledge memorization from reasoning. Our model enables effective\ngeneralization to new classes by simply replacing the memory contents, without\nthe need for model retraining. Unlike traditional models that encode both world\nknowledge and task-specific skills into their weights during training, our\nmodel stores knowledge in the external memory of web-crawled image and text\ndata. At inference time, the model dynamically selects relevant content from\nthe memory based on the input image, allowing it to adapt to arbitrary classes\nby simply replacing the memory contents. The key differentiator that our\nlearner meta-learns to perform classification tasks with noisy web data from\nunseen classes, resulting in robust performance across various classification\nscenarios. Experimental results demonstrate the promising performance and\nversatility of our approach in handling diverse classification tasks, including\nzero-shot/few-shot classification of unseen classes, fine-grained\nclassification, and class-incremental classification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06099", "pdf": "https://arxiv.org/pdf/2504.06099", "abs": "https://arxiv.org/abs/2504.06099", "authors": ["Samuel Bielik", "Simon Bilik"], "title": "Towards Varroa destructor mite detection using a narrow spectra illumination", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper focuses on the development and modification of a beehive\nmonitoring device and Varroa destructor detection on the bees with the help of\nhyperspectral imagery while utilizing a U-net, semantic segmentation\narchitecture, and conventional computer vision methods. The main objectives\nwere to collect a dataset of bees and mites, and propose the computer vision\nmodel which can achieve the detection between bees and mites.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06120", "pdf": "https://arxiv.org/pdf/2504.06120", "abs": "https://arxiv.org/abs/2504.06120", "authors": ["Yuanpei Liu", "Zhenqi He", "Kai Han"], "title": "Hyperbolic Category Discovery", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at CVPR 2025", "summary": "Generalized Category Discovery (GCD) is an intriguing open-world problem that\nhas garnered increasing attention. Given a dataset that includes both labelled\nand unlabelled images, GCD aims to categorize all images in the unlabelled\nsubset, regardless of whether they belong to known or unknown classes. In GCD,\nthe common practice typically involves applying a spherical projection operator\nat the end of the self-supervised pretrained backbone, operating within\nEuclidean or spherical space. However, both of these spaces have been shown to\nbe suboptimal for encoding samples that possesses hierarchical structures. In\ncontrast, hyperbolic space exhibits exponential volume growth relative to\nradius, making it inherently strong at capturing the hierarchical structure of\nsamples from both seen and unseen categories. Therefore, we propose to tackle\nthe category discovery challenge in the hyperbolic space. We introduce HypCD, a\nsimple \\underline{Hyp}erbolic framework for learning hierarchy-aware\nrepresentations and classifiers for generalized \\underline{C}ategory\n\\underline{D}iscovery. HypCD first transforms the Euclidean embedding space of\nthe backbone network into hyperbolic space, facilitating subsequent\nrepresentation and classification learning by considering both hyperbolic\ndistance and the angle between samples. This approach is particularly helpful\nfor knowledge transfer from known to unknown categories in GCD. We thoroughly\nevaluate HypCD on public GCD benchmarks, by applying it to various baseline and\nstate-of-the-art methods, consistently achieving significant improvements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06121", "pdf": "https://arxiv.org/pdf/2504.06121", "abs": "https://arxiv.org/abs/2504.06121", "authors": ["Ronghui Zhang", "Yuhang Ma", "Tengfei Li", "Ziyu Lin", "Yueying Wu", "Junzhou Chen", "Lin Zhang", "Jia Hu", "Tony Z. Qiu", "Konghui Guo"], "title": "A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Lane detection is a critical component of Advanced Driver Assistance Systems\n(ADAS). Existing lane detection algorithms generally perform well under\nfavorable weather conditions. However, their performance degrades significantly\nin adverse conditions, such as fog, which increases the risk of traffic\naccidents. This challenge is compounded by the lack of specialized datasets and\nmethods designed for foggy environments. To address this, we introduce the\nFoggyLane dataset, captured in real-world foggy scenarios, and synthesize two\nadditional datasets, FoggyCULane and FoggyTusimple, from existing popular lane\ndetection datasets. Furthermore, we propose a robust Fog-Enhanced Network for\nlane detection, incorporating a Global Feature Fusion Module (GFFM) to capture\nglobal relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to\nmodel the structural and positional relationships of lane instances, and a\nLow-level Edge Enhanced Module (LEEM) to address missing edge details in foggy\nconditions. Comprehensive experiments demonstrate that our method achieves\nstate-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on\nFoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT\nacceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA\nJetson AGX Orin, confirming its real-time capabilities and robustness in foggy\nenvironments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06144", "pdf": "https://arxiv.org/pdf/2504.06144", "abs": "https://arxiv.org/abs/2504.06144", "authors": ["Jihun Park", "Jongmin Gim", "Kyoungmin Lee", "Minseok Oh", "Minwoo Choi", "Jaeyeul Kim", "Woo Chool Park", "Sunghoon Im"], "title": "A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model", "categories": ["cs.CV"], "comment": "17 pages, 15 figures", "summary": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06148", "pdf": "https://arxiv.org/pdf/2504.06148", "abs": "https://arxiv.org/abs/2504.06148", "authors": ["Xiangxi Zheng", "Linjie Li", "Zhengyuan Yang", "Ping Yu", "Alex Jinpeng Wang", "Rui Yan", "Yuan Yao", "Lijuan Wang"], "title": "V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have led to\nsignificant improvements across various multimodal benchmarks. However, as\nevaluations shift from static datasets to open-world, dynamic environments,\ncurrent game-based benchmarks remain inadequate because they lack\nvisual-centric tasks and fail to assess the diverse reasoning skills required\nfor real-world decision-making. To address this, we introduce Visual-centric\nMultiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework\ndesigned to assess visual reasoning capabilities of MLLMs. V-MAGE features five\ndiverse games with 30+ handcrafted levels, testing models on core visual skills\nsuch as positioning, trajectory tracking, timing, and visual memory, alongside\nhigher-level reasoning like long-term planning and deliberation. We use V-MAGE\nto evaluate leading MLLMs, revealing significant challenges in their visual\nperception and reasoning. In all game environments, the top-performing MLLMs,\nas determined by Elo rating comparisons, exhibit a substantial performance gap\ncompared to humans. Our findings highlight critical limitations, including\nvarious types of perceptual errors made by the models, and suggest potential\navenues for improvement from an agent-centric perspective, such as refining\nagent strategies and addressing perceptual inaccuracies. Code is available at\nhttps://github.com/CSU-JPG/V-MAGE.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06158", "pdf": "https://arxiv.org/pdf/2504.06158", "abs": "https://arxiv.org/abs/2504.06158", "authors": ["Saad Wazir", "Daeyoung Kim"], "title": "Rethinking the Nested U-Net Approach: Enhancing Biomarker Segmentation with Attention Mechanisms and Multiscale Feature Fusion", "categories": ["eess.IV", "cs.CV"], "comment": "Published in the Proceedings of the 2024 International Conference on\n  Medical Imaging and Computer-Aided Diagnosis (MICAD 2024), Lecture Notes in\n  Electrical Engineering (LNEE), Volume 1372, Springer Nature, Singapore", "summary": "Identifying biomarkers in medical images is vital for a wide range of biotech\napplications. However, recent Transformer and CNN based methods often struggle\nwith variations in morphology and staining, which limits their feature\nextraction capabilities. In medical image segmentation, where data samples are\noften limited, state-of-the-art (SOTA) methods improve accuracy by using\npre-trained encoders, while end-to-end approaches typically fall short due to\ndifficulties in transferring multiscale features effectively between encoders\nand decoders. To handle these challenges, we introduce a nested UNet\narchitecture that captures both local and global context through Multiscale\nFeature Fusion and Attention Mechanisms. This design improves feature\nintegration from encoders, highlights key channels and regions, and restores\nspatial details to enhance segmentation performance. Our method surpasses SOTA\napproaches, as evidenced by experiments across four datasets and detailed\nablation studies. Code: https://github.com/saadwazir/ReN-UNet", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06163", "pdf": "https://arxiv.org/pdf/2504.06163", "abs": "https://arxiv.org/abs/2504.06163", "authors": ["Artur Xarles", "Sergio Escalera", "Thomas B. Moeslund", "Albert Clapés"], "title": "Action Valuation in Sports: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Action Valuation (AV) has emerged as a key topic in Sports Analytics,\noffering valuable insights by assigning scores to individual actions based on\ntheir contribution to desired outcomes. Despite a few surveys addressing\nrelated concepts such as Player Valuation, there is no comprehensive review\ndedicated to an in-depth analysis of AV across different sports. In this\nsurvey, we introduce a taxonomy with nine dimensions related to the AV task,\nencompassing data, methodological approaches, evaluation techniques, and\npractical applications. Through this analysis, we aim to identify the essential\ncharacteristics of effective AV methods, highlight existing gaps in research,\nand propose future directions for advancing the field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06178", "pdf": "https://arxiv.org/pdf/2504.06178", "abs": "https://arxiv.org/abs/2504.06178", "authors": ["Yujia Hu", "Songhua Liu", "Xingyi Yang", "Xinchao Wang"], "title": "Flash Sculptor: Modular 3D Worlds from Objects", "categories": ["cs.CV"], "comment": null, "summary": "Existing text-to-3D and image-to-3D models often struggle with complex scenes\ninvolving multiple objects and intricate interactions. Although some recent\nattempts have explored such compositional scenarios, they still require an\nextensive process of optimizing the entire layout, which is highly cumbersome\nif not infeasible at all. To overcome these challenges, we propose Flash\nSculptor in this paper, a simple yet effective framework for compositional 3D\nscene/object reconstruction from a single image. At the heart of Flash Sculptor\nlies a divide-and-conquer strategy, which decouples compositional scene\nreconstruction into a sequence of sub-tasks, including handling the appearance,\nrotation, scale, and translation of each individual instance. Specifically, for\nrotation, we introduce a coarse-to-fine scheme that brings the best of both\nworlds--efficiency and accuracy--while for translation, we develop an\noutlier-removal-based algorithm that ensures robust and precise parameters in a\nsingle step, without any iterative optimization. Extensive experiments\ndemonstrate that Flash Sculptor achieves at least a 3 times speedup over\nexisting compositional 3D methods, while setting new benchmarks in\ncompositional 3D reconstruction performance. Codes are available at\nhttps://github.com/YujiaHu1109/Flash-Sculptor.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06232", "pdf": "https://arxiv.org/pdf/2504.06232", "abs": "https://arxiv.org/abs/2504.06232", "authors": ["Jiazi Bu", "Pengyang Ling", "Yujie Zhou", "Pan Zhang", "Tong Wu", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention\nrecently due to their remarkable ability to deliver flexible visual creations.\nStill, high-resolution image synthesis presents formidable challenges due to\nthe scarcity and complexity of high-resolution content. To this end, we present\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\npotential of pre-trained flow models. Specifically, HiFlow establishes a\nvirtual reference flow within the high-resolution space that effectively\ncaptures the characteristics of low-resolution flow information, offering\nguidance for high-resolution generation through three key aspects:\ninitialization alignment for low-frequency consistency, direction alignment for\nstructure preservation, and acceleration alignment for detail fidelity. By\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\nquality of high-resolution image synthesis of T2I models and demonstrates\nversatility across their personalized variants. Extensive experiments validate\nHiFlow's superiority in achieving superior high-resolution image quality over\ncurrent state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06237", "pdf": "https://arxiv.org/pdf/2504.06237", "abs": "https://arxiv.org/abs/2504.06237", "authors": ["Mina Bishay", "Graham Page", "Waleed Emad", "Mohammad Mavadati"], "title": "Monitoring Viewer Attention During Online Ads", "categories": ["cs.CV"], "comment": "Presented at the ECCV 2024 Workshops", "summary": "Nowadays, video ads spread through numerous online platforms, and are being\nwatched by millions of viewers worldwide. Big brands gauge the liking and\npurchase intent of their new ads, by analyzing the facial responses of viewers\nrecruited online to watch the ads from home or work. Although this approach\ncaptures naturalistic responses, it is susceptible to distractions inherent in\nthe participants' environments, such as a movie playing on TV, a colleague\nspeaking, or mobile notifications. Inattentive participants should get flagged\nand eliminated to avoid skewing the ad-testing process. In this paper we\nintroduce an architecture for monitoring viewer attention during online ads.\nLeveraging two behavior analysis toolkits; AFFDEX 2.0 and SmartEye SDK, we\nextract low-level facial features encompassing facial expressions, head pose,\nand gaze direction. These features are then combined to extract high-level\nfeatures that include estimated gaze on the screen plane, yawning, speaking,\netc -- this enables the identification of four primary distractors; off-screen\ngaze, drowsiness, speaking, and unattended screen. Our architecture tailors the\ngaze settings according to the device type (desktop or mobile). We validate our\narchitecture first on datasets annotated for specific distractors, and then on\na real-world ad testing dataset with various distractors. The proposed\narchitecture shows promising results in detecting distraction across both\ndesktop and mobile devices.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05316", "pdf": "https://arxiv.org/pdf/2504.05316", "abs": "https://arxiv.org/abs/2504.05316", "authors": ["Yinan Zhou", "Yaxiong Wang", "Haokun Lin", "Chen Ma", "Li Zhu", "Zhedong Zheng"], "title": "Scale Up Composed Image Retrieval Learning via Modification Text Generation", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "12 pages, 8 figures", "summary": "Composed Image Retrieval (CIR) aims to search an image of interest using a\ncombination of a reference image and modification text as the query. Despite\nrecent advancements, this task remains challenging due to limited training data\nand laborious triplet annotation processes. To address this issue, this paper\nproposes to synthesize the training triplets to augment the training resource\nfor the CIR problem. Specifically, we commence by training a modification text\ngenerator exploiting large-scale multimodal models and scale up the CIR\nlearning throughout both the pretraining and fine-tuning stages. During\npretraining, we leverage the trained generator to directly create Modification\nText-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For\nfine-tuning, we first synthesize reverse modification text to connect the\ntarget image back to the reference image. Subsequently, we devise a two-hop\nalignment strategy to incrementally close the semantic gap between the\nmultimodal pair and the target image. We initially learn an implicit prototype\nutilizing both the original triplet and its reversed version in a cycle manner,\nfollowed by combining the implicit prototype feature with the modification text\nto facilitate accurate alignment with the target image. Extensive experiments\nvalidate the efficacy of the generated triplets and confirm that our proposed\nmethodology attains competitive recall on both the CIRR and FashionIQ\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05342", "pdf": "https://arxiv.org/pdf/2504.05342", "abs": "https://arxiv.org/abs/2504.05342", "authors": ["Donato Crisostomi", "Alessandro Zirilli", "Antonio Andrea Gargiulo", "Maria Sofia Bucarelli", "Simone Scardapane", "Fabrizio Silvestri", "Iacopo Masi", "Emanuele Rodolà"], "title": "MASS: MoErging through Adaptive Subspace Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Model merging has recently emerged as a lightweight alternative to\nensembling, combining multiple fine-tuned models into a single set of\nparameters with no additional training overhead. Yet, existing merging methods\nfall short of matching the full accuracy of separately fine-tuned endpoints. We\npresent MASS (MoErging through Adaptive Subspace Selection), a new approach\nthat closes this gap by unifying multiple fine-tuned models while retaining\nnear state-of-the-art performance across tasks. Building on the low-rank\ndecomposition of per-task updates, MASS stores only the most salient singular\ncomponents for each task and merges them into a shared model. At inference\ntime, a non-parametric, data-free router identifies which subspace (or\ncombination thereof) best explains an input's intermediate features and\nactivates the corresponding task-specific block. This procedure is fully\ntraining-free and introduces only a two-pass inference overhead plus a ~2\nstorage factor compared to a single pretrained model, irrespective of the\nnumber of tasks. We evaluate MASS on CLIP-based image classification using\nViT-B-16, ViT-B-32 and ViT-L-14 for benchmarks of 8, 14 and 20 tasks\nrespectively, establishing a new state-of-the-art. Most notably, MASS recovers\nup to ~98% of the average accuracy of individual fine-tuned models, making it a\npractical alternative to ensembling at a fraction of the storage cost.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05576", "pdf": "https://arxiv.org/pdf/2504.05576", "abs": "https://arxiv.org/abs/2504.05576", "authors": ["Mingfei Chen", "Israel D. Gebru", "Ishwarya Ananthabhotla", "Christian Richardt", "Dejan Markovic", "Jake Sandakly", "Steven Krenn", "Todd Keebler", "Eli Shlizerman", "Alexander Richard"], "title": "SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "Highlight Accepted to CVPR 2025", "summary": "We introduce SoundVista, a method to generate the ambient sound of an\narbitrary scene at novel viewpoints. Given a pre-acquired recording of the\nscene from sparsely distributed microphones, SoundVista can synthesize the\nsound of that scene from an unseen target viewpoint. The method learns the\nunderlying acoustic transfer function that relates the signals acquired at the\ndistributed microphones to the signal at the target viewpoint, using a limited\nnumber of known recordings. Unlike existing works, our method does not require\nconstraints or prior knowledge of sound source details. Moreover, our method\nefficiently adapts to diverse room layouts, reference microphone configurations\nand unseen environments. To enable this, we introduce a visual-acoustic binding\nmodule that learns visual embeddings linked with local acoustic properties from\npanoramic RGB and depth data. We first leverage these embeddings to optimize\nthe placement of reference microphones in any given scene. During synthesis, we\nleverage multiple embeddings extracted from reference locations to get adaptive\nweights for their contribution, conditioned on target viewpoint. We benchmark\nthe task on both publicly available data and real-world settings. We\ndemonstrate significant improvements over existing methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05591", "pdf": "https://arxiv.org/pdf/2504.05591", "abs": "https://arxiv.org/abs/2504.05591", "authors": ["Peter D. Erickson", "Tejas Sudharshan Mathai", "Ronald M. Summers"], "title": "Class Imbalance Correction for Improved Universal Lesion Detection and Tagging in CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at MICCAI MILLAND Workshop 2022", "summary": "Radiologists routinely detect and size lesions in CT to stage cancer and\nassess tumor burden. To potentially aid their efforts, multiple lesion\ndetection algorithms have been developed with a large public dataset called\nDeepLesion (32,735 lesions, 32,120 CT slices, 10,594 studies, 4,427 patients, 8\nbody part labels). However, this dataset contains missing measurements and\nlesion tags, and exhibits a severe imbalance in the number of lesions per label\ncategory. In this work, we utilize a limited subset of DeepLesion (6\\%, 1331\nlesions, 1309 slices) containing lesion annotations and body part label tags to\ntrain a VFNet model to detect lesions and tag them. We address the class\nimbalance by conducting three experiments: 1) Balancing data by the body part\nlabels, 2) Balancing data by the number of lesions per patient, and 3)\nBalancing data by the lesion size. In contrast to a randomly sampled\n(unbalanced) data subset, our results indicated that balancing the body part\nlabels always increased sensitivity for lesions >= 1cm for classes with low\ndata quantities (Bone: 80\\% vs. 46\\%, Kidney: 77\\% vs. 61\\%, Soft Tissue: 70\\%\nvs. 60\\%, Pelvis: 83\\% vs. 76\\%). Similar trends were seen for three other\nmodels tested (FasterRCNN, RetinaNet, FoveaBox). Balancing data by lesion size\nalso helped the VFNet model improve recalls for all classes in contrast to an\nunbalanced dataset. We also provide a structured reporting guideline for a\n``Lesions'' subsection to be entered into the ``Findings'' section of a\nradiology report. To our knowledge, we are the first to report the class\nimbalance in DeepLesion, and have taken data-driven steps to address it in the\ncontext of joint lesion detection and tagging.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05604", "pdf": "https://arxiv.org/pdf/2504.05604", "abs": "https://arxiv.org/abs/2504.05604", "authors": ["Jihoon Kim", "Namwoo Kang"], "title": "PyTopo3D: A Python Framework for 3D SIMP-based Topology Optimization", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Three-dimensional topology optimization (TO) is a powerful technique in\nengineering design, but readily usable, open-source implementations remain\nlimited within the popular Python scientific environment. This paper introduces\nPyTopo3D, a software framework developed to address this gap. PyTopo3D provides\na feature-rich tool for 3D TO by implementing the well-established Solid\nIsotropic Material with Penalization (SIMP) method and an Optimality Criteria\n(OC) update scheme, adapted and significantly enhanced from the efficient\nMATLAB code by Liu and Tovar (2014). While building on proven methodology,\nPyTopo3D's primary contribution is its integration and extension within Python,\nleveraging sparse matrix operations, optional parallel solvers, and accelerated\nKD-Tree sensitivity filtering for performance. Crucially, it incorporates\nfunctionalities vital for practical engineering workflows, including the direct\nimport of complex design domains and non-design obstacles via STL files,\nintegrated 3D visualization of the optimization process, and direct STL export\nof optimized geometries for manufacturing or further analysis. PyTopo3D is\npresented as an accessible, performance-aware tool and citable reference\ndesigned to empower engineers, students, and researchers to more easily utilize\n3D TO within their existing Python-based workflows.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05618", "pdf": "https://arxiv.org/pdf/2504.05618", "abs": "https://arxiv.org/abs/2504.05618", "authors": ["Jiawei Duan", "Haibo Hu", "Qingqing Ye", "Xinyue Sun"], "title": "Technical Report: Full Version of Analyzing and Optimizing Perturbation of DP-SGD Geometrically", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB"], "comment": "This is the full version of our paper \"Analyzing and Optimizing\n  Perturbation of DP-SGD Geometrically\", which will appear in ICDE 2025 as a\n  regular research paper", "summary": "Differential privacy (DP) has become a prevalent privacy model in a wide\nrange of machine learning tasks, especially after the debut of DP-SGD. However,\nDP-SGD, which directly perturbs gradients in the training iterations, fails to\nmitigate the negative impacts of noise on gradient direction. As a result,\nDP-SGD is often inefficient. Although various solutions (e.g., clipping to\nreduce the sensitivity of gradients and amplifying privacy bounds to save\nprivacy budgets) are proposed to trade privacy for model efficiency, the root\ncause of its inefficiency is yet unveiled.\n  In this work, we first generalize DP-SGD and theoretically derive the impact\nof DP noise on the training process. Our analysis reveals that, in terms of a\nperturbed gradient, only the noise on direction has eminent impact on the model\nefficiency while that on magnitude can be mitigated by optimization techniques,\ni.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that\ntraditional DP introduces biased noise on the direction when adding unbiased\nnoise to the gradient itself. Overall, the perturbation of DP-SGD is actually\nsub-optimal from a geometric perspective. Motivated by this, we design a\ngeometric perturbation strategy GeoDP within the DP framework, which perturbs\nthe direction and the magnitude of a gradient, respectively. By directly\nreducing the noise on the direction, GeoDP mitigates the negative impact of DP\nnoise on model efficiency with the same DP guarantee. Extensive experiments on\ntwo public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three\nprevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the\neffectiveness and generality of our strategy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05651", "pdf": "https://arxiv.org/pdf/2504.05651", "abs": "https://arxiv.org/abs/2504.05651", "authors": ["Narine Kokhlikyan", "Bargav Jayaraman", "Florian Bordes", "Chuan Guo", "Kamalika Chaudhuri"], "title": "Measuring Déjà vu Memorization Efficiently", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent research has shown that representation learning models may\naccidentally memorize their training data. For example, the d\\'ej\\`a vu method\nshows that for certain representation learning models and training images, it\nis sometimes possible to correctly predict the foreground label given only the\nrepresentation of the background - better than through dataset-level\ncorrelations. However, their measurement method requires training two models -\none to estimate dataset-level correlations and the other to estimate\nmemorization. This multiple model setup becomes infeasible for large\nopen-source models. In this work, we propose alternative simple methods to\nestimate dataset-level correlations, and show that these can be used to\napproximate an off-the-shelf model's memorization ability without any\nretraining. This enables, for the first time, the measurement of memorization\nin pre-trained open-source image representation and vision-language\nrepresentation models. Our results show that different ways of measuring\nmemorization yield very similar aggregate results. We also find that\nopen-source models typically have lower aggregate memorization than similar\nmodels trained on a subset of the data. The code is available both for vision\nand vision language models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05684", "pdf": "https://arxiv.org/pdf/2504.05684", "abs": "https://arxiv.org/abs/2504.05684", "authors": ["Tri Ton", "Ji Woo Hong", "Chang D. Yoo"], "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "This paper introduces Timestep-Adaptive Representation Alignment with\nOnset-Aware Conditioning (TARO), a novel framework for high-fidelity and\ntemporally coherent video-to-audio synthesis. Built upon flow-based\ntransformers, which offer stable training and continuous transformations for\nenhanced synchronization and audio quality, TARO introduces two key\ninnovations: (1) Timestep-Adaptive Representation Alignment (TRA), which\ndynamically aligns latent representations by adjusting alignment strength based\non the noise schedule, ensuring smooth evolution and improved fidelity, and (2)\nOnset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp\nevent-driven markers of audio-relevant visual moments to enhance\nsynchronization with dynamic visual events. Extensive experiments on the\nVGGSound and Landscape datasets demonstrate that TARO outperforms prior\nmethods, achieving relatively 53\\% lower Frechet Distance (FD), 29% lower\nFrechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its\nsuperior audio quality and synchronization precision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05692", "pdf": "https://arxiv.org/pdf/2504.05692", "abs": "https://arxiv.org/abs/2504.05692", "authors": ["Songyan Zhang", "Yongtao Ge", "Jinyuan Tian", "Guangkai Xu", "Hao Chen", "Chen Lv", "Chunhua Shen"], "title": "POMATO: Marrying Pointmap Matching with Temporal Motion for Dynamic 3D Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "code: https://github.com/wyddmw/POMATO", "summary": "3D reconstruction in dynamic scenes primarily relies on the combination of\ngeometry estimation and matching modules where the latter task is pivotal for\ndistinguishing dynamic regions which can help to mitigate the interference\nintroduced by camera and object motion. Furthermore, the matching module\nexplicitly models object motion, enabling the tracking of specific targets and\nadvancing motion understanding in complex scenarios. Recently, the proposed\nrepresentation of pointmap in DUSt3R suggests a potential solution to unify\nboth geometry estimation and matching in 3D space, but it still struggles with\nambiguous matching in dynamic regions, which may hamper further improvement. In\nthis work, we present POMATO, a unified framework for dynamic 3D reconstruction\nby marrying pointmap matching with temporal motion. Specifically, our method\nfirst learns an explicit matching relationship by mapping RGB pixels from both\ndynamic and static regions across different views to 3D pointmaps within a\nunified coordinate system. Furthermore, we introduce a temporal motion module\nfor dynamic motions that ensures scale consistency across different frames and\nenhances performance in tasks requiring both precise geometry and reliable\nmatching, most notably 3D point tracking. We show the effectiveness of the\nproposed pointmap matching and temporal fusion paradigm by demonstrating the\nremarkable performance across multiple downstream tasks, including video depth\nestimation, 3D point tracking, and pose estimation. Code and models are\npublicly available at https://github.com/wyddmw/POMATO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05740", "pdf": "https://arxiv.org/pdf/2504.05740", "abs": "https://arxiv.org/abs/2504.05740", "authors": ["Jee Won Lee", "Hansol Lim", "Sooyeun Yang", "Jongseong Choi"], "title": "Micro-splatting: Maximizing Isotropic Constraints for Refined Optimization in 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting have achieved impressive\nscalability and real-time rendering for large-scale scenes but often fall short\nin capturing fine-grained details. Conventional approaches that rely on\nrelatively large covariance parameters tend to produce blurred representations,\nwhile directly reducing covariance sizes leads to sparsity. In this work, we\nintroduce Micro-splatting (Maximizing Isotropic Constraints for Refined\nOptimization in 3D Gaussian Splatting), a novel framework designed to overcome\nthese limitations. Our approach leverages a covariance regularization term to\npenalize excessively large Gaussians to ensure each splat remains compact and\nisotropic. This work implements an adaptive densification strategy that\ndynamically refines regions with high image gradients by lowering the splitting\nthreshold, followed by loss function enhancement. This strategy results in a\ndenser and more detailed gaussian means where needed, without sacrificing\nrendering efficiency. Quantitative evaluations using metrics such as L1, L2,\nPSNR, SSIM, and LPIPS, alongside qualitative comparisons demonstrate that our\nmethod significantly enhances fine-details in 3D reconstructions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05945", "pdf": "https://arxiv.org/pdf/2504.05945", "abs": "https://arxiv.org/abs/2504.05945", "authors": ["Kuntian Zhang", "Simin Yu", "Yaoshu Wang", "Makoto Onizuka", "Chuan Xiao"], "title": "CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Source codes are available at https://github.com/chuanxiao1983/CKGAN/", "summary": "In this paper, we propose CKGAN, a novel generative adversarial network (GAN)\nvariant based on an integral probability metrics framework with characteristic\nkernel (CKIPM). CKIPM, as a distance between two probability distributions, is\ndesigned to optimize the lowerbound of the maximum mean discrepancy (MMD) in a\nreproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN\nmitigates the notorious problem of mode collapse by mapping the generated\nimages back to random noise. To save the effort of selecting the kernel\nfunction manually, we propose a soft selection method to automatically learn a\ncharacteristic kernel function. The experimental evaluation conducted on a set\nof synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that\nCKGAN generally outperforms other MMD-based GANs. The results also show that at\nthe cost of moderately more training time, the automatically selected kernel\nfunction delivers very close performance to the best of manually fine-tuned one\non real image benchmarks and is able to improve the performances of other\nMMD-based GANs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06027", "pdf": "https://arxiv.org/pdf/2504.06027", "abs": "https://arxiv.org/abs/2504.06027", "authors": ["Xiaochen Wei", "Weiwei Guo", "Wenxian Yu", "Feiming Wei", "Dongying Li"], "title": "OSDM-MReg: Multimodal Image Registration based One Step Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Multimodal remote sensing image registration aligns images from different\nsensors for data fusion and analysis. However, current methods often fail to\nextract modality-invariant features when aligning image pairs with large\nnonlinear radiometric differences. To address this issues, we propose\nOSDM-MReg, a novel multimodal image registration framework based image-to-image\ntranslation to eliminate the gap of multimodal images. Firstly, we propose a\nnovel one-step unaligned target-guided conditional denoising diffusion\nprobabilistic models(UTGOS-CDDPM)to translate multimodal images into a unified\ndomain. In the inference stage, traditional conditional DDPM generate\ntranslated source image by a large number of iterations, which severely slows\ndown the image registration task. To address this issues, we use the unaligned\ntraget image as a condition to promote the generation of low-frequency features\nof the translated source image. Furthermore, during the training stage, we add\nthe inverse process of directly predicting the translated image to ensure that\nthe translated source image can be generated in one step during the testing\nstage. Additionally, to supervised the detail features of translated source\nimage, we propose a new perceptual loss that focuses on the high-frequency\nfeature differences between the translated and ground-truth images. Finally, a\nmultimodal multiscale image registration network (MM-Reg) fuse the multimodal\nfeature of the unimodal images and multimodal images by proposed multimodal\nfeature fusion strategy. Experiments demonstrate superior accuracy and\nefficiency across various multimodal registration tasks, particularly for\nSAR-optical image pairs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
