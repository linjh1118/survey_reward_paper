{"id": "2504.07080", "pdf": "https://arxiv.org/pdf/2504.07080", "abs": "https://arxiv.org/abs/2504.07080", "authors": ["Atharva Pandey", "Kshitij Dubey", "Rahul Sharma", "Amit Sharma"], "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency", "accuracy"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06785", "pdf": "https://arxiv.org/pdf/2504.06785", "abs": "https://arxiv.org/abs/2504.06785", "authors": ["Shuoshuo Xu", "Kai Zhao", "James Loney", "Zili Li", "Andrea Visentin"], "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "consistency", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07046", "pdf": "https://arxiv.org/pdf/2504.07046", "abs": "https://arxiv.org/abs/2504.07046", "authors": ["Jifang Wang", "Xue Yang", "Longyue Wang", "Zhenran Xu", "Yiyu Wang", "Yaowei Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress. GitHub:\n  https://github.com/HITsz-TMG/Agentic-CIGEval", "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "consistency", "reliability", "fine-grained"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07046", "pdf": "https://arxiv.org/pdf/2504.07046", "abs": "https://arxiv.org/abs/2504.07046", "authors": ["Jifang Wang", "Xue Yang", "Longyue Wang", "Zhenran Xu", "Yiyu Wang", "Yaowei Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress. GitHub:\n  https://github.com/HITsz-TMG/Agentic-CIGEval", "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "consistency", "reliability", "fine-grained"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06884", "pdf": "https://arxiv.org/pdf/2504.06884", "abs": "https://arxiv.org/abs/2504.06884", "authors": ["Wuyang Liu", "Yi Chai", "Yongpeng Yan", "Yanzhen Ren"], "title": "Audio-visual Event Localization on Portrait Mode Short Videos", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Audio-visual event localization (AVEL) plays a critical role in multimodal\nscene understanding. While existing datasets for AVEL predominantly comprise\nlandscape-oriented long videos with clean and simple audio context, short\nvideos have become the primary format of online video content due to the the\nproliferation of smartphones. Short videos are characterized by\nportrait-oriented framing and layered audio compositions (e.g., overlapping\nsound effects, voiceovers, and music), which brings unique challenges\nunaddressed by conventional methods. To this end, we introduce AVE-PM, the\nfirst AVEL dataset specifically designed for portrait mode short videos,\ncomprising 25,335 clips that span 86 fine-grained categories with frame-level\nannotations. Beyond dataset creation, our empirical analysis shows that\nstate-of-the-art AVEL methods suffer an average 18.66% performance drop during\ncross-mode evaluation. Further analysis reveals two key challenges of different\nvideo formats: 1) spatial bias from portrait-oriented framing introduces\ndistinct domain priors, and 2) noisy audio composition compromise the\nreliability of audio modality. To address these issues, we investigate optimal\npreprocessing recipes and the impact of background music for AVEL on portrait\nmode videos. Experiments show that these methods can still benefit from\ntailored preprocessing and specialized model design, thus achieving improved\nperformance. This work provides both a foundational benchmark and actionable\ninsights for advancing AVEL research in the era of mobile-centric video\ncontent. Dataset and code will be released.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "fine-grained"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06527", "pdf": "https://arxiv.org/pdf/2504.06527", "abs": "https://arxiv.org/abs/2504.06527", "authors": ["Xinyu Liu", "Xiaoguang Lin", "Xiang Liu", "Yong Yang", "Hongqian Wang", "Qilong Sun"], "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recording the open surgery process is essential for educational and medical\nevaluation purposes; however, traditional single-camera methods often face\nchallenges such as occlusions caused by the surgeon's head and body, as well as\nlimitations due to fixed camera angles, which reduce comprehensibility of the\nvideo content. This study addresses these limitations by employing a\nmulti-viewpoint camera recording system, capturing the surgical procedure from\nsix different angles to mitigate occlusions. We propose a fully supervised\nlearning-based time series prediction method to choose the best shot sequences\nfrom multiple simultaneously recorded video streams, ensuring optimal\nviewpoints at each moment. Our time series prediction model forecasts future\ncamera selections by extracting and fusing visual and semantic features from\nsurgical videos using pre-trained models. These features are processed by a\ntemporal prediction network with TimeBlocks to capture sequential dependencies.\nA linear embedding layer reduces dimensionality, and a Softmax classifier\nselects the optimal camera view based on the highest probability. In our\nexperiments, we created five groups of open thyroidectomy videos, each with\nsimultaneous recordings from six different angles. The results demonstrate that\nour method achieves competitive accuracy compared to traditional supervised\nmethods, even when predicting over longer time horizons. Furthermore, our\napproach outperforms state-of-the-art time series prediction techniques on our\ndataset. This manuscript makes a unique contribution by presenting an\ninnovative framework that advances surgical video analysis techniques, with\nsignificant implications for improving surgical education and patient safety.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "accuracy"], "score": 4}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07022", "pdf": "https://arxiv.org/pdf/2504.07022", "abs": "https://arxiv.org/abs/2504.07022", "authors": ["Chad Melton", "Alex Sorokine", "Steve Peterson"], "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety", "categories": ["cs.CL"], "comment": "14 pages, 3 Figures, 3 tables", "summary": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06766", "pdf": "https://arxiv.org/pdf/2504.06766", "abs": "https://arxiv.org/abs/2504.06766", "authors": ["Yuxin Wang", "Yiran Guo", "Yining Zheng", "Zhangyue Yin", "Shuo Chen", "Jie Yang", "Jiajun Chen", "Xuanjing Huang", "Xipeng Qiu"], "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06460", "pdf": "https://arxiv.org/pdf/2504.06460", "abs": "https://arxiv.org/abs/2504.06460", "authors": ["Sai Adith Senthil Kumar", "Hao Yan", "Saipavan Perepa", "Murong Yue", "Ziyu Yao"], "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06536", "pdf": "https://arxiv.org/pdf/2504.06536", "abs": "https://arxiv.org/abs/2504.06536", "authors": ["Happy Buzaaba", "Alexander Wettig", "David Ifeoluwa Adelani", "Christiane Fellbaum"], "title": "Lugha-Llama: Adapting Large Language Models for African Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06544", "pdf": "https://arxiv.org/pdf/2504.06544", "abs": "https://arxiv.org/abs/2504.06544", "authors": ["Weiwei Xing", "Yue Cheng", "Hongzhu Yi", "Xiaohui Gao", "Xiang Wei", "Xiaoyu Guo", "Yuming Zhang", "Xinyu Pang"], "title": "LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing", "categories": ["cs.CV"], "comment": "This paper has been accepted by AAAI 2025", "summary": "Classifiers often learn to be biased corresponding to the class-imbalanced\ndataset, especially under the semi-supervised learning (SSL) set. While\nprevious work tries to appropriately re-balance the classifiers by subtracting\na class-irrelevant image's logit, but lacks a firm theoretical basis. We\ntheoretically analyze why exploiting a baseline image can refine pseudo-labels\nand prove that the black image is the best choice. We also indicated that as\nthe training process deepens, the pseudo-labels before and after refinement\nbecome closer. Based on this observation, we propose a debiasing scheme dubbed\nLCGC, which Learning from Consistency Gradient Conflicting, by encouraging\nbiased class predictions during training. We intentionally update the\npseudo-labels whose gradient conflicts with the debiased logits, representing\nthe optimization direction offered by the over-imbalanced classifier\npredictions. Then, we debiased the predictions by subtracting the baseline\nimage logits during testing. Extensive experiments demonstrate that LCGC can\nsignificantly improve the prediction accuracy of existing CISSL models on\npublic benchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06606", "pdf": "https://arxiv.org/pdf/2504.06606", "abs": "https://arxiv.org/abs/2504.06606", "authors": ["Minghe Gao", "Xuqi Liu", "Zhongqi Yue", "Yang Wu", "Shuang Chen", "Juncheng Li", "Siliang Tang", "Fei Wu", "Tat-Seng Chua", "Yueting Zhuang"], "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07069", "pdf": "https://arxiv.org/pdf/2504.07069", "abs": "https://arxiv.org/abs/2504.07069", "authors": ["Bibek Paudel", "Alexander Lyzhov", "Preetam Joshi", "Puneet Anand"], "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06269", "pdf": "https://arxiv.org/pdf/2504.06269", "abs": "https://arxiv.org/abs/2504.06269", "authors": ["Yin Wu", "Zhengxuan Zhang", "Fuling Wang", "Yuyu Luo", "Hui Xiong", "Nan Tang"], "title": "EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "15 pages, 2 figures", "summary": "Misinformation continues to pose a significant challenge in today's\ninformation ecosystem, profoundly shaping public perception and behavior. Among\nits various manifestations, Out-of-Context (OOC) misinformation is particularly\nobscure, as it distorts meaning by pairing authentic images with misleading\ntextual narratives. Existing methods for detecting OOC misinformation\npredominantly rely on coarse-grained similarity metrics between image-text\npairs, which often fail to capture subtle inconsistencies or provide meaningful\nexplainability. While multi-modal large language models (MLLMs) demonstrate\nremarkable capabilities in visual reasoning and explanation generation, they\nhave not yet demonstrated the capacity to address complex, fine-grained, and\ncross-modal distinctions necessary for robust OOC detection. To overcome these\nlimitations, we introduce EXCLAIM, a retrieval-based framework designed to\nleverage external knowledge through multi-granularity index of multi-modal\nevents and entities. Our approach integrates multi-granularity contextual\nanalysis with a multi-agent reasoning architecture to systematically evaluate\nthe consistency and integrity of multi-modal news content. Comprehensive\nexperiments validate the effectiveness and resilience of EXCLAIM, demonstrating\nits ability to detect OOC misinformation with 4.3% higher accuracy compared to\nstate-of-the-art approaches, while offering explainable and actionable\ninsights.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06742", "pdf": "https://arxiv.org/pdf/2504.06742", "abs": "https://arxiv.org/abs/2504.06742", "authors": ["Alexandra Ertl", "Shuhan Xiao", "Stefan Denner", "Robin Peretzke", "David Zimmerer", "Peter Neher", "Fabian Isensee", "Klaus Maier-Hein"], "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection", "categories": ["cs.CV"], "comment": null, "summary": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability.This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06811", "pdf": "https://arxiv.org/pdf/2504.06811", "abs": "https://arxiv.org/abs/2504.06811", "authors": ["Abhinav Roy", "Bhavesh Gyanchandani", "Aditya Oza"], "title": "Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide, with early and accurate diagnosis playing a pivotal role in\nimproving patient outcomes. Automated detection of pulmonary nodules in\ncomputed tomography (CT) scans is a challenging task due to variability in\nnodule size, shape, texture, and location. Traditional Convolutional Neural\nNetworks (CNNs) have shown considerable promise in medical image analysis;\nhowever, their limited ability to capture fine-grained spatial-spectral\nvariations restricts their performance in complex diagnostic scenarios. In this\nstudy, we propose a novel hybrid deep learning architecture that incorporates\nChebyshev polynomial expansions into CNN layers to enhance expressive power and\nimprove the representation of underlying anatomical structures. The proposed\nChebyshev-CNN leverages the orthogonality and recursive properties of Chebyshev\npolynomials to extract high-frequency features and approximate complex\nnonlinear functions with greater fidelity. The model is trained and evaluated\non benchmark lung cancer imaging datasets, including LUNA16 and LIDC-IDRI,\nachieving superior performance in classifying pulmonary nodules as benign or\nmalignant. Quantitative results demonstrate significant improvements in\naccuracy, sensitivity, and specificity compared to traditional CNN-based\napproaches. This integration of polynomial-based spectral approximation within\ndeep learning provides a robust framework for enhancing automated medical\ndiagnostics and holds potential for broader applications in clinical decision\nsupport systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06836", "pdf": "https://arxiv.org/pdf/2504.06836", "abs": "https://arxiv.org/abs/2504.06836", "authors": ["Jakub Maciej Wiśniewski", "Anders Nymark Christensen", "Mary Le Ngo", "Martin Grønnebæk Tolsgaard", "Chun Kit Wong"], "title": "Determining Fetal Orientations From Blind Sweep Ultrasound Video", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Cognitive demands of fetal ultrasound examinations pose unique challenges\namong clinicians. With the goal of providing an assistive tool, we developed an\nautomated pipeline for predicting fetal orientation from ultrasound videos\nacquired following a simple blind sweep protocol. Leveraging on a pre-trained\nhead detection and segmentation model, this is achieved by first determining\nthe fetal presentation (cephalic or breech) with a template matching approach,\nfollowed by the fetal lie (facing left or right) by analyzing the spatial\ndistribution of segmented brain anatomies. Evaluation on a dataset of\nthird-trimester ultrasound scans demonstrated the promising accuracy of our\npipeline. This work distinguishes itself by introducing automated fetal lie\nprediction and by proposing an assistive paradigm that augments sonographer\nexpertise rather than replacing it. Future research will focus on enhancing\nacquisition efficiency, and exploring real-time clinical integration to improve\nworkflow and support for obstetric clinicians.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06925", "pdf": "https://arxiv.org/pdf/2504.06925", "abs": "https://arxiv.org/abs/2504.06925", "authors": ["Sergio Romero-Tapiador", "Ruben Tolosana", "Blanca Lacruz-Pleguezuelos", "Laura Judith Marcos Zambrano", "Guadalupe X. Bazán", "Isabel Espinosa-Salinas", "Julian Fierrez", "Javier Ortega-Garcia", "Enrique Carrillo de Santa Pau", "Aythami Morales"], "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IEEE/CVF Computer Vision and Pattern Recognition\n  Conference workshops 2025 (CVPRw) 10 pages, 4 figures, 2 tables", "summary": "Automatic dietary assessment based on food images remains a challenge,\nrequiring precise food detection, segmentation, and classification.\nVision-Language Models (VLMs) offer new possibilities by integrating visual and\ntextual reasoning. In this study, we evaluate six state-of-the-art VLMs\n(ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their\ncapabilities in food recognition at different levels. For the experimental\nframework, we introduce the FoodNExTDB, a unique food image database that\ncontains 9,263 expert-labeled images across 10 categories (e.g., \"protein\nsource\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g.,\n\"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by\nseven experts who manually annotated all images in the database. Also, we\npropose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts\nfor the inter-annotator variability. Results show that closed-source models\noutperform open-source ones, achieving over 90% EWR in recognizing food\nproducts in images containing a single product. Despite their potential,\ncurrent VLMs face challenges in fine-grained food recognition, particularly in\ndistinguishing subtle differences in cooking styles and visually similar food\nitems, which limits their reliability for automatic dietary assessment. The\nFoodNExTDB database is publicly available at\nhttps://github.com/AI4Food/FoodNExtDB.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06957", "pdf": "https://arxiv.org/pdf/2504.06957", "abs": "https://arxiv.org/abs/2504.06957", "authors": ["Marco Acerbis", "Nataša Sladoje", "Joakim Lindblad"], "title": "A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology", "categories": ["cs.CV"], "comment": "14 pages, 6 figures, SCIA2025", "summary": "Accurate and efficient cell detection is crucial in many biomedical image\nanalysis tasks. We evaluate the performance of several Deep Learning (DL)\nmethods for cell detection in Papanicolaou-stained cytological Whole Slide\nImages (WSIs), focusing on accuracy of predictions and computational\nefficiency. We examine recentoff-the-shelf algorithms as well as\ncustom-designed detectors, applying them to two datasets: the CNSeg Dataset and\nthe Oral Cancer (OC) Dataset. Our comparison includes well-established\nsegmentation methods such as StarDist, Cellpose, and the Segment Anything Model\n2 (SAM2), alongside centroid-based Fully Convolutional Regression Network\n(FCRN) approaches. We introduce a suitable evaluation metric to assess the\naccuracy of predictions based on the distance from ground truth positions. We\nalso explore the impact of dataset size and data augmentation techniques on\nmodel performance. Results show that centroid-based methods, particularly the\nImproved Fully Convolutional Regression Network (IFCRN) method, outperform\nsegmentation-based methods in terms of both detection accuracy and\ncomputational efficiency. This study highlights the potential of centroid-based\ndetectors as a preferred option for cell detection in resource-limited\nenvironments, offering faster processing times and lower GPU memory usage\nwithout compromising accuracy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06301", "pdf": "https://arxiv.org/pdf/2504.06301", "abs": "https://arxiv.org/abs/2504.06301", "authors": ["Mohsen Jenadeleh", "Jon Sneyers", "Panqi Jia", "Shima Mohammadi", "Joao Ascenso", "Dietmar Saupe"], "title": "Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 3 tables, submitted to QoMEX 2025", "summary": "Learning-based image compression methods have recently emerged as promising\nalternatives to traditional codecs, offering improved rate-distortion\nperformance and perceptual quality. JPEG AI represents the latest standardized\nframework in this domain, leveraging deep neural networks for high-fidelity\nimage reconstruction. In this study, we present a comprehensive subjective\nvisual quality assessment of JPEG AI-compressed images using the JPEG AIC-3\nmethodology, which quantifies perceptual differences in terms of Just\nNoticeable Difference (JND) units. We generated a dataset of 50 compressed\nimages with fine-grained distortion levels from five diverse sources. A\nlarge-scale crowdsourced experiment collected 96,200 triplet responses from 459\nparticipants. We reconstructed JND-based quality scales using a unified model\nbased on boosted and plain triplet comparisons. Additionally, we evaluated the\nalignment of multiple objective image quality metrics with human perception in\nthe high-fidelity range. The CVVDP metric achieved the overall highest\nperformance; however, most metrics including CVVDP were overly optimistic in\npredicting the quality of JPEG AI-compressed images. These findings emphasize\nthe necessity for rigorous subjective evaluations in the development and\nbenchmarking of modern image codecs, particularly in the high-fidelity range.\nAnother technical contribution is the introduction of the well-known\nMeng-Rosenthal-Rubin statistical test to the field of Quality of Experience\nresearch. This test can reliably assess the significance of difference in\nperformance of quality metrics in terms of correlation between metrics and\nground truth. The complete dataset, including all subjective scores, is\npublicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06866", "pdf": "https://arxiv.org/pdf/2504.06866", "abs": "https://arxiv.org/abs/2504.06866", "authors": ["Seunghyeok Back", "Joosoon Lee", "Kangmin Kim", "Heeseon Rho", "Geonhyup Lee", "Raeyoung Kang", "Sangbeom Lee", "Sangjun Noh", "Youngjin Lee", "Taeyeop Lee", "Kyoobin Lee"], "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06961", "pdf": "https://arxiv.org/pdf/2504.06961", "abs": "https://arxiv.org/abs/2504.06961", "authors": ["Yu Qi", "Yuanchen Ju", "Tianming Wei", "Chi Chu", "Lawson L. S. Wong", "Huazhe Xu"], "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025 (Conference on Computer Vision and Pattern\n  Recognition)", "summary": "3D assembly tasks, such as furniture assembly and component fitting, play a\ncrucial role in daily life and represent essential capabilities for future home\nrobots. Existing benchmarks and datasets predominantly focus on assembling\ngeometric fragments or factory parts, which fall short in addressing the\ncomplexities of everyday object interactions and assemblies. To bridge this\ngap, we present 2BY2, a large-scale annotated dataset for daily pairwise\nobjects assembly, covering 18 fine-grained tasks that reflect real-life\nscenarios, such as plugging into sockets, arranging flowers in vases, and\ninserting bread into toasters. 2BY2 dataset includes 1,034 instances and 517\npairwise objects with pose and symmetry annotations, requiring approaches that\nalign geometric shapes while accounting for functional and spatial\nrelationships between objects. Leveraging the 2BY2 dataset, we propose a\ntwo-step SE(3) pose estimation method with equivariant features for assembly\nconstraints. Compared to previous shape assembly methods, our approach achieves\nstate-of-the-art performance across all 18 tasks in the 2BY2 dataset.\nAdditionally, robot experiments further validate the reliability and\ngeneralization ability of our method for complex 3D assembly tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06292", "pdf": "https://arxiv.org/pdf/2504.06292", "abs": "https://arxiv.org/abs/2504.06292", "authors": ["Hongbin Liang", "Hezhe Qiao", "Wei Huang", "Qizhou Wang", "Mingsheng Shang", "Lin Chen"], "title": "Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in ICONIP2024", "summary": "Ensuring the safety of vulnerable road users through accurate prediction of\npedestrian crossing intention (PCI) plays a crucial role in the context of\nautonomous and assisted driving. Analyzing the set of observation video frames\nin ego-view has been widely used in most PCI prediction methods to forecast the\ncross intent. However, they struggle to capture the critical events related to\npedestrian behaviour along the temporal dimension due to the high redundancy of\nthe video frames, which results in the sub-optimal performance of PCI\nprediction. Our research addresses the challenge by introducing a novel\napproach called \\underline{T}emporal-\\underline{c}ontextual Event\n\\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module\n(TMM), which aims to manage the redundancy by clustering the observed video\nframes into multiple key temporal events. Then, the Contextual Attention Block\n(CAB) is employed to adaptively aggregate multiple event features along with\nvisual and non-visual data. By synthesizing the temporal feature extraction and\ncontextual attention on the key information across the critical events, TCL can\nlearn expressive representation for the PCI prediction. Extensive experiments\nare carried out on three widely adopted datasets, including PIE, JAAD-beh, and\nJAAD-all. The results show that TCL substantially surpasses the\nstate-of-the-art methods. Our code can be accessed at\nhttps://github.com/dadaguailhb/TCL.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "dimension"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06438", "pdf": "https://arxiv.org/pdf/2504.06438", "abs": "https://arxiv.org/abs/2504.06438", "authors": ["Yuehan Qin", "Shawn Li", "Yi Nian", "Xinyan Velocity Yu", "Yue Zhao", "Xuezhe Ma"], "title": "Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown substantial capacity for generating\nfluent, contextually appropriate responses. However, they can produce\nhallucinated outputs, especially when a user query includes one or more false\npremises-claims that contradict established facts. Such premises can mislead\nLLMs into offering fabricated or misleading details. Existing approaches\ninclude pretraining, fine-tuning, and inference-time techniques that often rely\non access to logits or address hallucinations after they occur. These methods\ntend to be computationally expensive, require extensive training data, or lack\nproactive mechanisms to prevent hallucination before generation, limiting their\nefficiency in real-time applications. We propose a retrieval-based framework\nthat identifies and addresses false premises before generation. Our method\nfirst transforms a user's query into a logical representation, then applies\nretrieval-augmented generation (RAG) to assess the validity of each premise\nusing factual sources. Finally, we incorporate the verification results into\nthe LLM's prompt to maintain factual consistency in the final output.\nExperiments show that this approach effectively reduces hallucinations,\nimproves factual accuracy, and does not require access to model logits or\nlarge-scale fine-tuning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06357", "pdf": "https://arxiv.org/pdf/2504.06357", "abs": "https://arxiv.org/abs/2504.06357", "authors": ["Vladimir Golovkin", "Nikolay Nemtsev", "Vasyl Shandyba", "Oleg Udin", "Nikita Kasatkin", "Pavel Kononov", "Anton Afanasiev", "Sergey Ulasen", "Andrei Boiarov"], "title": "From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for presentation at the CVPR 2025 CVsports Workshop", "summary": "Game State Reconstruction (GSR), a critical task in Sports Video\nUnderstanding, involves precise tracking and localization of all individuals on\nthe football field-players, goalkeepers, referees, and others - in real-world\ncoordinates. This capability enables coaches and analysts to derive actionable\ninsights into player movements, team formations, and game dynamics, ultimately\noptimizing training strategies and enhancing competitive advantage. Achieving\naccurate GSR using a single-camera setup is highly challenging due to frequent\ncamera movements, occlusions, and dynamic scene content. In this work, we\npresent a robust end-to-end pipeline for tracking players across an entire\nmatch using a single-camera setup. Our solution integrates a fine-tuned YOLOv5m\nfor object detection, a SegFormer-based camera parameter estimator, and a\nDeepSORT-based tracking framework enhanced with re-identification, orientation\nprediction, and jersey number recognition. By ensuring both spatial accuracy\nand temporal consistency, our method delivers state-of-the-art game state\nreconstruction, securing first place in the SoccerNet Game State Reconstruction\nChallenge 2024 and significantly outperforming competing methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06529", "pdf": "https://arxiv.org/pdf/2504.06529", "abs": "https://arxiv.org/abs/2504.06529", "authors": ["Khai Phan Tran", "Xue Li"], "title": "CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction", "categories": ["cs.CL"], "comment": "Published at ACIIDS 2024", "summary": "Document-level Relation Extraction (DocRE) involves identifying relations\nbetween entities across multiple sentences in a document. Evidence sentences,\ncrucial for precise entity pair relationships identification, enhance focus on\nessential text segments, improving DocRE performance. However, existing\nevidence retrieval systems often overlook the collaborative nature among\nsemantically similar entity pairs in the same document, hindering the\neffectiveness of the evidence retrieval task. To address this, we propose a\nnovel evidence retrieval framework, namely CDER. CDER employs an attentional\ngraph-based architecture to capture collaborative patterns and incorporates a\ndynamic sub-structure for additional robustness in evidence retrieval.\nExperimental results on the benchmark DocRE dataset show that CDER not only\nexcels in the evidence retrieval task but also enhances overall performance of\nexisting DocRE system.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06358", "pdf": "https://arxiv.org/pdf/2504.06358", "abs": "https://arxiv.org/abs/2504.06358", "authors": ["Yupeng Cheng", "Zi Pong Lim", "Sarthak Ketanbhai Modi", "Yon Shin Teo", "Yushi Cao", "Shang-Wei Lin"], "title": "Towards Calibration Enhanced Network by Inverse Adversarial Attack", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Test automation has become increasingly important as the complexity of both\ndesign and content in Human Machine Interface (HMI) software continues to grow.\nCurrent standard practice uses Optical Character Recognition (OCR) techniques\nto automatically extract textual information from HMI screens for validation.\nAt present, one of the key challenges faced during the automation of HMI screen\nvalidation is the noise handling for the OCR models. In this paper, we propose\nto utilize adversarial training techniques to enhance OCR models in HMI testing\nscenarios. More specifically, we design a new adversarial attack objective for\nOCR models to discover the decision boundaries in the context of HMI testing.\nWe then adopt adversarial training to optimize the decision boundaries towards\na more robust and accurate OCR model. In addition, we also built an HMI screen\ndataset based on real-world requirements and applied multiple types of\nperturbation onto the clean HMI dataset to provide a more complete coverage for\nthe potential scenarios. We conduct experiments to demonstrate how using\nadversarial training techniques yields more robust OCR models against various\nkinds of noises, while still maintaining high OCR model accuracy. Further\nexperiments even demonstrate that the adversarial training models exhibit a\ncertain degree of robustness against perturbations from other patterns.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06560", "pdf": "https://arxiv.org/pdf/2504.06560", "abs": "https://arxiv.org/abs/2504.06560", "authors": ["Lanrui Wang", "Mingyu Zheng", "Hongyin Tang", "Zheng Lin", "Yanan Cao", "Jingang Wang", "Xunliang Cai", "Weiping Wang"], "title": "NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Processing structured tabular data, particularly lengthy tables, constitutes\na fundamental yet challenging task for large language models (LLMs). However,\nexisting long-context benchmarks primarily focus on unstructured text,\nneglecting the challenges of long and complex structured tables. To address\nthis gap, we introduce NeedleInATable (NIAT), a novel task that treats each\ntable cell as a \"needle\" and requires the model to extract the target cell\nunder different queries. Evaluation results of mainstream LLMs on this\nbenchmark show they lack robust long-table comprehension, often relying on\nsuperficial correlations or shortcuts for complex table understanding tasks,\nrevealing significant limitations in processing intricate tabular data. To this\nend, we propose a data synthesis method to enhance models' long-table\ncomprehension capabilities. Experimental results show that our synthesized\ntraining data significantly enhances LLMs' performance on the NIAT task,\noutperforming both long-context LLMs and long-table agent methods. This work\nadvances the evaluation of LLMs' genuine long-structured table comprehension\ncapabilities and paves the way for progress in long-context and table\nunderstanding applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06564", "pdf": "https://arxiv.org/pdf/2504.06564", "abs": "https://arxiv.org/abs/2504.06564", "authors": ["Qingcheng Zeng", "Weihao Xuan", "Leyang Cui", "Rob Voigt"], "title": "Do Reasoning Models Show Better Verbalized Calibration?", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06821", "pdf": "https://arxiv.org/pdf/2504.06821", "abs": "https://arxiv.org/abs/2504.06821", "authors": ["Zora Zhiruo Wang", "Apurva Gandhi", "Graham Neubig", "Daniel Fried"], "title": "Inducing Programmatic Skills for Agentic Tasks", "categories": ["cs.CL"], "comment": null, "summary": "To succeed in common digital tasks such as web navigation, agents must carry\nout a variety of specialized tasks such as searching for products or planning a\ntravel route. To tackle these tasks, agents can bootstrap themselves by\nlearning task-specific skills online through interaction with the web\nenvironment. In this work, we demonstrate that programs are an effective\nrepresentation for skills. We propose agent skill induction (ASI), which allows\nagents to adapt themselves by inducing, verifying, and utilizing program-based\nskills on the fly. We start with an evaluation on the WebArena agent benchmark\nand show that ASI outperforms the static baseline agent and its text-skill\ncounterpart by 23.5% and 11.3% in success rate, mainly thanks to the\nprogrammatic verification guarantee during the induction phase. ASI also\nimproves efficiency by reducing 10.7-15.3% of the steps over baselines, by\ncomposing primitive actions (e.g., click) into higher-level skills (e.g.,\nsearch product). We then highlight the efficacy of ASI in remaining efficient\nand accurate under scaled-up web activities. Finally, we examine the\ngeneralizability of induced skills when transferring between websites, and find\nthat ASI can effectively reuse common skills, while also updating incompatible\nskills to versatile website changes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06580", "pdf": "https://arxiv.org/pdf/2504.06580", "abs": "https://arxiv.org/abs/2504.06580", "authors": ["Joochan Kim", "Minjoon Jung", "Byoung-Tak Zhang"], "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to SCSL @ ICLR 2025", "summary": "Action recognition models have achieved promising results in understanding\ninstructional videos. However, they often rely on dominant, dataset-specific\naction sequences rather than true video comprehension, a problem that we define\nas ordinal bias. To address this issue, we propose two effective video\nmanipulation methods: Action Masking, which masks frames of frequently\nco-occurring actions, and Sequence Shuffling, which randomizes the order of\naction segments. Through comprehensive experiments, we demonstrate that current\nmodels exhibit significant performance drops when confronted with nonstandard\naction sequences, underscoring their vulnerability to ordinal bias. Our\nfindings emphasize the importance of rethinking evaluation strategies and\ndeveloping models capable of generalizing beyond fixed action patterns in\ndiverse instructional videos.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06910", "pdf": "https://arxiv.org/pdf/2504.06910", "abs": "https://arxiv.org/abs/2504.06910", "authors": ["Sheng Lu", "Ilia Kuznetsov", "Iryna Gurevych"], "title": "Identifying Aspects in Peer Reviews", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06947", "pdf": "https://arxiv.org/pdf/2504.06947", "abs": "https://arxiv.org/abs/2504.06947", "authors": ["Natalia Loukachevitch", "Natalia Tkachenko", "Anna Lapanitsyna", "Mikhail Tikhomirov", "Nicolay Rusnachenko"], "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts", "categories": ["cs.CL", "I.2.7"], "comment": "RuOpinionNE-2024 represent a proceeding of RuSentNE-2023. It\n  contributes with extraction and evaluation of factual statements that support\n  the assigned sentiment", "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06638", "pdf": "https://arxiv.org/pdf/2504.06638", "abs": "https://arxiv.org/abs/2504.06638", "authors": ["Hu Cui", "Tessai Hayama"], "title": "HGMamba: Enhancing 3D Human Pose Estimation with a HyperGCN-Mamba Network", "categories": ["cs.CV"], "comment": "accepted by IJCNN2025", "summary": "3D human pose lifting is a promising research area that leverages estimated\nand ground-truth 2D human pose data for training. While existing approaches\nprimarily aim to enhance the performance of estimated 2D poses, they often\nstruggle when applied to ground-truth 2D pose data. We observe that achieving\naccurate 3D pose reconstruction from ground-truth 2D poses requires precise\nmodeling of local pose structures, alongside the ability to extract robust\nglobal spatio-temporal features. To address these challenges, we propose a\nnovel Hyper-GCN and Shuffle Mamba (HGMamba) block, which processes input data\nthrough two parallel streams: Hyper-GCN and Shuffle-Mamba. The Hyper-GCN stream\nmodels the human body structure as hypergraphs with varying levels of\ngranularity to effectively capture local joint dependencies. Meanwhile, the\nShuffle Mamba stream leverages a state space model to perform spatio-temporal\nscanning across all joints, enabling the establishment of global dependencies.\nBy adaptively fusing these two representations, HGMamba achieves strong global\nfeature modeling while excelling at local structure modeling. We stack multiple\nHGMamba blocks to create three variants of our model, allowing users to select\nthe most suitable configuration based on the desired speed-accuracy trade-off.\nExtensive evaluations on the Human3.6M and MPI-INF-3DHP benchmark datasets\ndemonstrate the effectiveness of our approach. HGMamba-B achieves\nstate-of-the-art results, with P1 errors of 38.65 mm and 14.33 mm on the\nrespective datasets. Code and models are available:\nhttps://github.com/HuCui2022/HGMamba", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06647", "pdf": "https://arxiv.org/pdf/2504.06647", "abs": "https://arxiv.org/abs/2504.06647", "authors": ["Nan Peng", "Xun Zhou", "Mingming Wang", "Guisong Chen", "Songming Chen"], "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction", "categories": ["cs.CV"], "comment": null, "summary": "Safety constitutes a foundational imperative for autonomous driving systems,\nnecessitating the maximal incorporation of accessible external prior\ninformation. This study establishes that temporal perception buffers and\ncost-efficient maps inherently form complementary prior sources for online\nvectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a\nunified prior-informed framework that systematically integrates two synergistic\ninformation sources: previous predictions and simulated outdated HD maps. The\nframework introduces two core innovations: a tile-indexed 3D vectorized global\nmap processor enabling efficient refreshment, storage, and retrieval of 3D\nvectorized priors; a tri-mode operational optimization paradigm ensuring\nconsistency across prior-free, map-absent, and map-prior scenarios while\nmitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap\nachieves state-of-the-art performance in map-free scenarios across established\nonline vectorized HD map construction benchmarks. When provided with simulated\noutdated HD maps, the framework exhibits robust capabilities in error-resilient\nprior fusion, empirically confirming the synergistic complementarity between\nprevious predictions and simulated outdated HD maps. Code will be available at\nhttps://github.com/pnnnnnnn/Uni-PrevPredMap.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "consistency"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06680", "pdf": "https://arxiv.org/pdf/2504.06680", "abs": "https://arxiv.org/abs/2504.06680", "authors": ["Christoph Balada", "Aida Romano-Martinez", "Vincent ten Cate", "Katharina Geschke", "Jonas Tesarz", "Paul Claßen", "Alexander K. Schuster", "Dativa Tibyampansha", "Karl-Patrik Kresoja", "Philipp S. Wild", "Sheraz Ahmed", "Andreas Dengel"], "title": "Deep Learning for Cardiovascular Risk Assessment: Proxy Features from Carotid Sonography as Predictors of Arterial Damage", "categories": ["cs.CV"], "comment": null, "summary": "In this study, hypertension is utilized as an indicator of individual\nvascular damage. This damage can be identified through machine learning\ntechniques, providing an early risk marker for potential major cardiovascular\nevents and offering valuable insights into the overall arterial condition of\nindividual patients. To this end, the VideoMAE deep learning model, originally\ndeveloped for video classification, was adapted by finetuning for application\nin the domain of ultrasound imaging. The model was trained and tested using a\ndataset comprising over 31,000 carotid sonography videos sourced from the\nGutenberg Health Study (15,010 participants), one of the largest prospective\npopulation health studies. This adaptation facilitates the classification of\nindividuals as hypertensive or non-hypertensive (75.7% validation accuracy),\nfunctioning as a proxy for detecting visual arterial damage. We demonstrate\nthat our machine learning model effectively captures visual features that\nprovide valuable insights into an individual's overall cardiovascular health.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06781", "pdf": "https://arxiv.org/pdf/2504.06781", "abs": "https://arxiv.org/abs/2504.06781", "authors": ["Reiji Saito", "Kazuhiro Hotta"], "title": "Domain Generalization through Attenuation of Domain-Specific Information", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Workshops", "summary": "In this paper, we propose a new evaluation metric called Domain Independence\n(DI) and Attenuation of Domain-Specific Information (ADSI) which is\nspecifically designed for domain-generalized semantic segmentation in\nautomotive images. DI measures the presence of domain-specific information: a\nlower DI value indicates strong domain dependence, while a higher DI value\nsuggests greater domain independence. This makes it roughly where\ndomain-specific information exists and up to which frequency range it is\npresent. As a result, it becomes possible to effectively suppress only the\nregions in the image that contain domain-specific information, enabling feature\nextraction independent of the domain. ADSI uses a Butterworth filter to remove\nthe low-frequency components of images that contain inherent domain-specific\ninformation such as sensor characteristics and lighting conditions. However,\nsince low-frequency components also contain important information such as\ncolor, we should not remove them completely. Thus, a scalar value (ranging from\n0 to 1) is multiplied by the low-frequency components to retain essential\ninformation. This helps the model learn more domain-independent features. In\nexperiments, GTA5 (synthetic dataset) was used as training images, and a\nreal-world dataset was used for evaluation, and the proposed method\noutperformed conventional approaches. Similarly, in experiments that the\nCityscapes (real-world dataset) was used for training and various environment\ndatasets such as rain and nighttime were used for evaluation, the proposed\nmethod demonstrated its robustness under nighttime conditions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06800", "pdf": "https://arxiv.org/pdf/2504.06800", "abs": "https://arxiv.org/abs/2504.06800", "authors": ["Danielle Cohen", "Hila Chefer", "Lior Wolf"], "title": "A Meaningful Perturbation Metric for Evaluating Explainability Methods", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable success, yet their\nwide adoption is often hindered by their opaque decision-making. To address\nthis, attribution methods have been proposed to assign relevance values to each\npart of the input. However, different methods often produce entirely different\nrelevance maps, necessitating the development of standardized metrics to\nevaluate them. Typically, such evaluation is performed through perturbation,\nwherein high- or low-relevance regions of the input image are manipulated to\nexamine the change in prediction. In this work, we introduce a novel approach,\nwhich harnesses image generation models to perform targeted perturbation.\nSpecifically, we focus on inpainting only the high-relevance pixels of an input\nimage to modify the model's predictions while preserving image fidelity. This\nis in contrast to existing approaches, which often produce out-of-distribution\nmodifications, leading to unreliable results. Through extensive experiments, we\ndemonstrate the effectiveness of our approach in generating meaningful rankings\nacross a wide range of models and attribution methods. Crucially, we establish\nthat the ranking produced by our metric exhibits significantly higher\ncorrelation with human preferences compared to existing approaches,\nunderscoring its potential for enhancing interpretability in DNNs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06801", "pdf": "https://arxiv.org/pdf/2504.06801", "abs": "https://arxiv.org/abs/2504.06801", "authors": ["Rishubh Parihar", "Srinjay Sarkar", "Sarthak Vora", "Jogendra Kundu", "R. Venkatesh Babu"], "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/monoplace3D", "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07086", "pdf": "https://arxiv.org/pdf/2504.07086", "abs": "https://arxiv.org/abs/2504.07086", "authors": ["Andreas Hochlehnert", "Hardik Bhatnagar", "Vishaal Udandarao", "Samuel Albanie", "Ameya Prabhu", "Matthias Bethge"], "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report", "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07097", "pdf": "https://arxiv.org/pdf/2504.07097", "abs": "https://arxiv.org/abs/2504.07097", "authors": ["Nikhil Shivakumar Nayak", "Krishnateja Killamsetty", "Ligong Han", "Abhishek Bhandwaldar", "Prateek Chanda", "Kai Xu", "Hao Wang", "Aldo Pareja", "Oleg Silkin", "Mustafa Eyceoz", "Akash Srivastava"], "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.PR", "stat.ML", "68T50", "I.2.0; G.3"], "comment": "25 pages, 13 figures, 6 tables", "summary": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06897", "pdf": "https://arxiv.org/pdf/2504.06897", "abs": "https://arxiv.org/abs/2504.06897", "authors": ["Jiawei Mao", "Yuhan Wang", "Yucheng Tang", "Daguang Xu", "Kang Wang", "Yang Yang", "Zongwei Zhou", "Yuyin Zhou"], "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web", "summary": "This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06908", "pdf": "https://arxiv.org/pdf/2504.06908", "abs": "https://arxiv.org/abs/2504.06908", "authors": ["Emmanuelle Bourigault", "Amir Jamaludin", "Abdullah Hamdi"], "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "preprint", "summary": "In medical imaging, the primary challenge is collecting large-scale labeled\ndata due to privacy concerns, logistics, and high labeling costs. In this work,\nwe present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset\nof body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D\nimages) and more than 1.37 billion 2D segmentation masks of 72 organs, all\nbased on the UK Biobank MRI dataset. We utilize automatic labeling, introduce\nan automated label cleaning pipeline with organ-specific filters, and manually\nannotate a subset of 300 MRIs with 11 abdominal classes to validate the quality\n(referred to as UKBOB-manual). This approach allows for scaling up the dataset\ncollection while maintaining confidence in the labels. We further confirm the\nvalidity of the labels by demonstrating zero-shot generalization of trained\nmodels on the filtered UKBOB to other small labeled datasets from similar\ndomains (e.g., abdominal MRI). To further mitigate the effect of noisy labels,\nwe propose a novel method called Entropy Test-time Adaptation (ETTA) to refine\nthe segmentation output. We use UKBOB to train a foundation model, Swin-BOB,\nfor 3D medical image segmentation based on the Swin-UNetr architecture,\nachieving state-of-the-art results in several benchmarks in 3D medical imaging,\nincluding the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the\nBTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained\nmodels and the code are available at https://emmanuelleb985.github.io/ukbob ,\nand the filtered labels will be made available with the UK Biobank.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time adaptation"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06950", "pdf": "https://arxiv.org/pdf/2504.06950", "abs": "https://arxiv.org/abs/2504.06950", "authors": ["Sachin Kumar Danisetty", "Alexandros Graikos", "Srikar Yellapragada", "Dimitris Samaras"], "title": "PathSegDiff: Pathology Segmentation using Diffusion model representations", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is crucial in many computational pathology pipelines,\nincluding accurate disease diagnosis, subtyping, outcome, and survivability\nprediction. The common approach for training a segmentation model relies on a\npre-trained feature extractor and a dataset of paired image and mask\nannotations. These are used to train a lightweight prediction model that\ntranslates features into per-pixel classes. The choice of the feature extractor\nis central to the performance of the final segmentation model, and recent\nliterature has focused on finding tasks to pre-train the feature extractor. In\nthis paper, we propose PathSegDiff, a novel approach for histopathology image\nsegmentation that leverages Latent Diffusion Models (LDMs) as pre-trained\nfeatured extractors. Our method utilizes a pathology-specific LDM, guided by a\nself-supervised encoder, to extract rich semantic information from H\\&E stained\nhistopathology images. We employ a simple, fully convolutional network to\nprocess the features extracted from the LDM and generate segmentation masks.\nOur experiments demonstrate significant improvements over traditional methods\non the BCSS and GlaS datasets, highlighting the effectiveness of\ndomain-specific diffusion pre-training in capturing intricate tissue structures\nand enhancing segmentation accuracy in histopathology images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06994", "pdf": "https://arxiv.org/pdf/2504.06994", "abs": "https://arxiv.org/abs/2504.06994", "authors": ["Omar Alama", "Avigyan Bhattacharya", "Haoyang He", "Seungchan Kim", "Yuheng Qiu", "Wenshan Wang", "Cherie Ho", "Nikhil Keetha", "Sebastian Scherer"], "title": "RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Open-set semantic mapping is crucial for open-world robots. Current mapping\napproaches either are limited by the depth range or only map beyond-range\nentities in constrained settings, where overall they fail to combine\nwithin-range and beyond-range observations. Furthermore, these methods make a\ntrade-off between fine-grained semantics and efficiency. We introduce\nRayFronts, a unified representation that enables both dense and beyond-range\nefficient semantic mapping. RayFronts encodes task-agnostic open-set semantics\nto both in-range voxels and beyond-range rays encoded at map boundaries,\nempowering the robot to reduce search volumes significantly and make informed\ndecisions both within & beyond sensory range, while running at 8.84 Hz on an\nOrin AGX. Benchmarking the within-range semantics shows that RayFronts's\nfine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation\nperformance while improving throughput by 16.5x. Traditionally, online mapping\nperformance is entangled with other system components, complicating evaluation.\nWe propose a planner-agnostic evaluation framework that captures the utility\nfor online beyond-range search and exploration, and show RayFronts reduces\nsearch volume 2.2x more efficiently than the closest online baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fernández Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemiński", "Jekaterina Novikova", "Luísa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovič", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Otávio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "María Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06356", "pdf": "https://arxiv.org/pdf/2504.06356", "abs": "https://arxiv.org/abs/2504.06356", "authors": ["Yifei Yuan", "Zahra Abbasiantaeb", "Yang Deng", "Mohammad Aliannejadi"], "title": "Query Understanding in LLM-based Conversational Information Seeking", "categories": ["cs.CL"], "comment": "WWW'25 Tutorial", "summary": "Query understanding in Conversational Information Seeking (CIS) involves\naccurately interpreting user intent through context-aware interactions. This\nincludes resolving ambiguities, refining queries, and adapting to evolving\ninformation needs. Large Language Models (LLMs) enhance this process by\ninterpreting nuanced language and adapting dynamically, improving the relevance\nand precision of search results in real-time. In this tutorial, we explore\nadvanced techniques to enhance query understanding in LLM-based CIS systems. We\ndelve into LLM-driven methods for developing robust evaluation metrics to\nassess query understanding quality in multi-turn interactions, strategies for\nbuilding more interactive systems, and applications like proactive query\nmanagement and query reformulation. We also discuss key challenges in\nintegrating LLMs for query understanding in conversational search systems and\noutline future research directions. Our goal is to deepen the audience's\nunderstanding of LLM-based conversational query understanding and inspire\ndiscussions to drive ongoing advancements in this field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06393", "pdf": "https://arxiv.org/pdf/2504.06393", "abs": "https://arxiv.org/abs/2504.06393", "authors": ["Rebecca M. M. Hicke", "Sil Hamilton", "David Mimno"], "title": "The Zero Body Problem: Probing LLM Use of Sensory Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sensory language expresses embodied experiences ranging from taste and sound\nto excitement and stomachache. This language is of interest to scholars from a\nwide range of domains including robotics, narratology, linguistics, and\ncognitive science. In this work, we explore whether language models, which are\nnot embodied, can approximate human use of embodied language. We extend an\nexisting corpus of parallel human and model responses to short story prompts\nwith an additional 18,000 stories generated by 18 popular models. We find that\nall models generate stories that differ significantly from human usage of\nsensory language, but the direction of these differences varies considerably\nbetween model families. Namely, Gemini models use significantly more sensory\nlanguage than humans along most axes whereas most models from the remaining\nfive families use significantly less. Linear probes run on five models suggest\nthat they are capable of identifying sensory language. However, we find\npreliminary evidence suggesting that instruction tuning may discourage usage of\nsensory language. Finally, to support further work, we release our expanded\nstory dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06397", "pdf": "https://arxiv.org/pdf/2504.06397", "abs": "https://arxiv.org/abs/2504.06397", "authors": ["Yufu Wang", "Yu Sun", "Priyanka Patel", "Kostas Daniilidis", "Michael J. Black", "Muhammed Kocabas"], "title": "PromptHMR: Promptable Human Mesh Recovery", "categories": ["cs.CV"], "comment": null, "summary": "Human pose and shape (HPS) estimation presents challenges in diverse\nscenarios such as crowded scenes, person-person interactions, and single-view\nreconstruction. Existing approaches lack mechanisms to incorporate auxiliary\n\"side information\" that could enhance reconstruction accuracy in such\nchallenging scenarios. Furthermore, the most accurate methods rely on cropped\nperson detections and cannot exploit scene context while methods that process\nthe whole image often fail to detect people and are less accurate than methods\nthat use crops. While recent language-based methods explore HPS reasoning\nthrough large language or vision-language models, their metric accuracy is well\nbelow the state of the art. In contrast, we present PromptHMR, a\ntransformer-based promptable method that reformulates HPS estimation through\nspatial and semantic prompts. Our method processes full images to maintain\nscene context and accepts multiple input modalities: spatial prompts like\nbounding boxes and masks, and semantic prompts like language descriptions or\ninteraction labels. PromptHMR demonstrates robust performance across\nchallenging scenarios: estimating people from bounding boxes as small as faces\nin crowded scenes, improving body shape estimation through language\ndescriptions, modeling person-person interactions, and producing temporally\ncoherent motions in videos. Experiments on benchmarks show that PromptHMR\nachieves state-of-the-art performance while offering flexible prompt-based\ncontrol over the HPS estimation process.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06432", "pdf": "https://arxiv.org/pdf/2504.06432", "abs": "https://arxiv.org/abs/2504.06432", "authors": ["Rupayan Mallick", "Sibo Dong", "Nataniel Ruiz", "Sarah Adel Bargal"], "title": "D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Applications of diffusion models for visual tasks have been quite noteworthy.\nThis paper targets making classification models more robust to occlusions for\nthe task of object recognition by proposing a pipeline that utilizes a frozen\ndiffusion model. Diffusion features have demonstrated success in image\ngeneration and image completion while understanding image context. Occlusion\ncan be posed as an image completion problem by deeming the pixels of the\noccluder to be `missing.' We hypothesize that such features can help\nhallucinate object visual features behind occluding objects, and hence we\npropose using them to enable models to become more occlusion robust. We design\nexperiments to include input-based augmentations as well as feature-based\naugmentations. Input-based augmentations involve finetuning on images where the\noccluder pixels are inpainted, and feature-based augmentations involve\naugmenting classification features with intermediate diffusion features. We\ndemonstrate that our proposed use of diffusion-based features results in models\nthat are more robust to partial object occlusions for both Transformers and\nConvNets on ImageNet with simulated occlusions. We also propose a dataset that\nencompasses real-world occlusions and demonstrate that our method is more\nrobust to partial object occlusions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06577", "pdf": "https://arxiv.org/pdf/2504.06577", "abs": "https://arxiv.org/abs/2504.06577", "authors": ["Pedro Cisneros-Velarde"], "title": "Bypassing Safety Guardrails in LLMs Using Humor", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we show it is possible to bypass the safety guardrails of\nlarge language models (LLMs) through a humorous prompt including the unsafe\nrequest. In particular, our method does not edit the unsafe request and follows\na fixed template -- it is simple to implement and does not need additional LLMs\nto craft prompts. Extensive experiments show the effectiveness of our method\nacross different LLMs. We also show that both removing and adding more humor to\nour method can reduce its effectiveness -- excessive humor possibly distracts\nthe LLM from fulfilling its unsafe request. Thus, we argue that LLM\njailbreaking occurs when there is a proper balance between focus on the unsafe\nrequest and presence of humor.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06486", "pdf": "https://arxiv.org/pdf/2504.06486", "abs": "https://arxiv.org/abs/2504.06486", "authors": ["Samuel Stevens", "S M Rayeed", "Jenna Kline"], "title": "Mind the Gap: Evaluating Vision Systems in Small Data Applications", "categories": ["cs.CV"], "comment": "4 pages (main text), 5 figures", "summary": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06600", "pdf": "https://arxiv.org/pdf/2504.06600", "abs": "https://arxiv.org/abs/2504.06600", "authors": ["William De Michele", "Abel Armas Cervantes", "Lea Frermann"], "title": "Automated Business Process Analysis: An LLM-Based Approach to Value Assessment", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06504", "pdf": "https://arxiv.org/pdf/2504.06504", "abs": "https://arxiv.org/abs/2504.06504", "authors": ["Xiaohang Yang", "Qing Wang", "Jiahao Yang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints", "categories": ["cs.CV"], "comment": "12 pages, 9 figures;", "summary": "Motion retargeting seeks to faithfully replicate the spatio-temporal motion\ncharacteristics of a source character onto a target character with a different\nbody shape. Apart from motion semantics preservation, ensuring geometric\nplausibility and maintaining temporal consistency are also crucial for\neffective motion retargeting. However, many existing methods prioritize either\ngeometric plausibility or temporal consistency. Neglecting geometric\nplausibility results in interpenetration while neglecting temporal consistency\nleads to motion jitter. In this paper, we propose a novel sequence-to-sequence\nmodel for seamless Spatial-Temporal aware motion Retargeting (STaR), with\npenetration and consistency constraints. STaR consists of two modules: (1) a\nspatial module that incorporates dense shape representation and a novel limb\npenetration constraint to ensure geometric plausibility while preserving motion\nsemantics, and (2) a temporal module that utilizes a temporal transformer and a\nnovel temporal consistency constraint to predict the entire motion sequence at\nonce while enforcing multi-level trajectory smoothness. The seamless\ncombination of the two modules helps us achieve a good balance between the\nsemantic, geometric, and temporal targets. Extensive experiments on the Mixamo\nand ScanRet datasets demonstrate that our method produces plausible and\ncoherent motions while significantly reducing interpenetration rates compared\nwith other approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06816", "pdf": "https://arxiv.org/pdf/2504.06816", "abs": "https://arxiv.org/abs/2504.06816", "authors": ["Karol Mikula", "Mariana Sarkociová Remešíková"], "title": "A Graph Diffusion Algorithm for Lexical Similarity Evaluation", "categories": ["cs.CL", "2020: 00A69, 05C90, 91F20"], "comment": "28 pages", "summary": "In this paper, we present an algorithm for evaluating lexical similarity\nbetween a given language and several reference language clusters. As an input,\nwe have a list of concepts and the corresponding translations in all considered\nlanguages. Moreover, each reference language is assigned to one of $c$ language\nclusters. For each of the concepts, the algorithm computes the distance between\neach pair of translations. Based on these distances, it constructs a weighted\ndirected graph, where every vertex represents a language. After, it solves a\ngraph diffusion equation with a Dirichlet boundary condition, where the unknown\nis a map from the vertex set to $\\mathbb{R}^c$. The resulting coordinates are\nvalues from the interval $[0,1]$ and they can be interpreted as probabilities\nof belonging to each of the clusters or as a lexical similarity distribution\nwith respect to the reference clusters. The distances between translations are\ncalculated using phonetic transcriptions and a modification of the\nDamerau-Levenshtein distance. The algorithm can be useful in analyzing\nrelationships between languages spoken in multilingual territories with a lot\nof mutual influences. We demonstrate this by presenting a case study regarding\nvarious European languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06843", "pdf": "https://arxiv.org/pdf/2504.06843", "abs": "https://arxiv.org/abs/2504.06843", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Ioannis Arapakis"], "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06607", "pdf": "https://arxiv.org/pdf/2504.06607", "abs": "https://arxiv.org/abs/2504.06607", "authors": ["Onkar Krishna", "Hiroki Ohashi"], "title": "Visually Similar Pair Alignment for Robust Cross-Domain Object Detection", "categories": ["cs.CV"], "comment": "15 pages, Journal paper submission", "summary": "Domain gaps between training data (source) and real-world environments\n(target) often degrade the performance of object detection models. Most\nexisting methods aim to bridge this gap by aligning features across source and\ntarget domains but often fail to account for visual differences, such as color\nor orientation, in alignment pairs. This limitation leads to less effective\ndomain adaptation, as the model struggles to manage both domain-specific shifts\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\nfor the first time, using a custom-built dataset, that aligning visually\nsimilar pairs significantly improves domain adaptation. Based on this insight,\nwe propose a novel memory-based system to enhance domain alignment. This system\nstores precomputed features of foreground objects and background areas from the\nsource domain, which are periodically updated during training. By retrieving\nvisually similar source features for alignment with target foreground and\nbackground features, the model effectively addresses domain-specific\ndifferences while reducing the impact of visual variations. Extensive\nexperiments across diverse domain shift scenarios validate our method's\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06917", "pdf": "https://arxiv.org/pdf/2504.06917", "abs": "https://arxiv.org/abs/2504.06917", "authors": ["Ming Liu", "Massimo Poesio"], "title": "Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains", "categories": ["cs.CL"], "comment": "32 pages, 15 figures", "summary": "With the growth of the Internet, buying habits have changed, and customers\nhave become more dependent on the online opinions of other customers to guide\ntheir purchases. Identifying fake reviews thus became an important area for\nNatural Language Processing (NLP) research. However, developing\nhigh-performance NLP models depends on the availability of large amounts of\ntraining data, which are often not available for low-resource languages or\ndomains. In this research, we used large language models to generate datasets\nto train fake review detectors. Our approach was used to generate fake reviews\nin different domains (book reviews, restaurant reviews, and hotel reviews) and\ndifferent languages (English and Chinese). Our results demonstrate that our\ndata augmentation techniques result in improved performance at fake review\ndetection for all domains and languages. The accuracy of our fake review\ndetection model can be improved by 0.3 percentage points on DeRev TEST, 10.9\npercentage points on Amazon TEST, 8.3 percentage points on Yelp TEST and 7.2\npercentage points on DianPing TEST using the augmented datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06629", "pdf": "https://arxiv.org/pdf/2504.06629", "abs": "https://arxiv.org/abs/2504.06629", "authors": ["MinKyu Lee", "Sangeek Hyun", "Woojin Jun", "Hyunjun Kim", "Jiwoo Chung", "Jae-Pil Heo"], "title": "Rethinking LayerNorm in Image Restoration Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This work investigates abnormal feature behaviors observed in image\nrestoration (IR) Transformers. Specifically, we identify two critical issues:\nfeature entropy becoming excessively small and feature magnitudes diverging up\nto a million-fold scale. We pinpoint the root cause to the per-token\nnormalization aspect of conventional LayerNorm, which disrupts essential\nspatial correlations and internal feature statistics. To address this, we\npropose a simple normalization strategy tailored for IR Transformers. Our\napproach applies normalization across the entire spatio-channel dimension,\neffectively preserving spatial correlations. Additionally, we introduce an\ninput-adaptive rescaling method that aligns feature statistics to the unique\nstatistical requirements of each input. Experimental results verify that this\ncombined strategy effectively resolves feature divergence, significantly\nenhancing both the stability and performance of IR Transformers across various\nIR tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06632", "pdf": "https://arxiv.org/pdf/2504.06632", "abs": "https://arxiv.org/abs/2504.06632", "authors": ["Yifan Gao", "Zihang Lin", "Chuanbin Liu", "Min Zhou", "Tiezheng Ge", "Bo Zheng", "Hongtao Xie"], "title": "PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page: https://poster-maker.github.io", "summary": "Product posters, which integrate subject, scene, and text, are crucial\npromotional tools for attracting customers. Creating such posters using modern\nimage generation methods is valuable, while the main challenge lies in\naccurately rendering text, especially for complex writing systems like Chinese,\nwhich contains over 10,000 individual characters. In this work, we identify the\nkey to precise text rendering as constructing a character-discriminative visual\nfeature as a control signal. Based on this insight, we propose a robust\ncharacter-wise representation as control and we develop TextRenderNet, which\nachieves a high text rendering accuracy of over 90%. Another challenge in\nposter generation is maintaining the fidelity of user-specific products. We\naddress this by introducing SceneGenNet, an inpainting-based model, and propose\nsubject fidelity feedback learning to further enhance fidelity. Based on\nTextRenderNet and SceneGenNet, we present PosterMaker, an end-to-end generation\nframework. To optimize PosterMaker efficiently, we implement a two-stage\ntraining strategy that decouples text rendering and background generation\nlearning. Experimental results show that PosterMaker outperforms existing\nbaselines by a remarkable margin, which demonstrates its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07024", "pdf": "https://arxiv.org/pdf/2504.07024", "abs": "https://arxiv.org/abs/2504.07024", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Data Augmentation and Hyperparameter Tuning for Low-Resource MFA", "categories": ["cs.CL"], "comment": null, "summary": "A continued issue for those working with computational tools and endangered\nand under-resourced languages is the lower accuracy of results for languages\nwith smaller amounts of data. We attempt to ameliorate this issue by using data\naugmentation methods to increase corpus size, comparing augmentation to\nhyperparameter tuning for multilingual forced alignment. Unlike text\naugmentation methods, audio augmentation does not lead to substantially\nincreased performance. Hyperparameter tuning, on the other hand, results in\nsubstantial improvement without (for this amount of data) infeasible additional\ntraining time. For languages with small to medium amounts of training data,\nthis is a workable alternative to adapting models from high-resource languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07053", "pdf": "https://arxiv.org/pdf/2504.07053", "abs": "https://arxiv.org/abs/2504.07053", "authors": ["Liang-Hsuan Tseng", "Yi-Chang Chen", "Kuan-Yi Lee", "Da-Shan Shiu", "Hung-yi Lee"], "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Preprint. Work in progress", "summary": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07070", "pdf": "https://arxiv.org/pdf/2504.07070", "abs": "https://arxiv.org/abs/2504.07070", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06666", "pdf": "https://arxiv.org/pdf/2504.06666", "abs": "https://arxiv.org/abs/2504.06666", "authors": ["Ruotian Peng", "Haiying He", "Yake Wei", "Yandong Wen", "Di Hu"], "title": "Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception", "categories": ["cs.CV"], "comment": null, "summary": "High-quality image captions play a crucial role in improving the performance\nof cross-modal applications such as text-to-image generation, text-to-video\ngeneration, and text-image retrieval. To generate long-form, high-quality\ncaptions, many recent studies have employed multimodal large language models\n(MLLMs). However, current MLLMs often produce captions that lack fine-grained\ndetails or suffer from hallucinations, a challenge that persists in both\nopen-source and closed-source models. Inspired by Feature-Integration theory,\nwhich suggests that attention must focus on specific regions to integrate\nvisual information effectively, we propose a \\textbf{divide-then-aggregate}\nstrategy. Our method first divides the image into semantic and spatial patches\nto extract fine-grained details, enhancing the model's local perception of the\nimage. These local details are then hierarchically aggregated to generate a\ncomprehensive global description. To address hallucinations and inconsistencies\nin the generated captions, we apply a semantic-level filtering process during\nhierarchical aggregation. This training-free pipeline can be applied to both\nopen-source models (LLaVA-1.5, LLaVA-1.6, Mini-Gemini) and closed-source models\n(Claude-3.5-Sonnet, GPT-4o, GLM-4V-Plus). Extensive experiments demonstrate\nthat our method generates more detailed, reliable captions, advancing\nmultimodal description generation without requiring model retraining. The\nsource code are available at https://github.com/GeWu-Lab/Patch-Matters", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07087", "pdf": "https://arxiv.org/pdf/2504.07087", "abs": "https://arxiv.org/abs/2504.07087", "authors": ["Elan Markowitz", "Krupa Galiya", "Greg Ver Steeg", "Aram Galstyan"], "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)", "summary": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06716", "pdf": "https://arxiv.org/pdf/2504.06716", "abs": "https://arxiv.org/abs/2504.06716", "authors": ["Anil Armagan", "Albert Saà-Garriga", "Bruno Manganelli", "Kyuwon Kim", "M. Kerim Yucel"], "title": "GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction", "categories": ["cs.CV"], "comment": "9 pages. In submission to an IEEE conference", "summary": "Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly\ndue to its ability to converge reasonably fast, faithfully represent the scene\nand render (novel) views in a fast fashion. However, it suffers from large\nstorage and memory requirements, and its training speed still lags behind the\nhash-grid based radiance field approaches (e.g. Instant-NGP), which makes it\nespecially difficult to deploy them in robotics scenarios, where 3D\nreconstruction is crucial for accurate operation. In this paper, we propose\nGSta that dynamically identifies Gaussians that have converged well during\ntraining, based on their positional and color gradient norms. By forcing such\nGaussians into a siesta and stopping their updates (freezing) during training,\nwe improve training speed with competitive accuracy compared to state of the\nart. We also propose an early stopping mechanism based on the PSNR values\ncomputed on a subset of training images. Combined with other improvements, such\nas integrating a learning rate scheduler, GSta achieves an improved Pareto\nfront in convergence speed, memory and storage requirements, while preserving\nquality. We also show that GSta can improve other methods and complement\northogonal approaches in efficiency improvement; once combined with Trick-GS,\nGSta achieves up to 5x faster training, 16x smaller disk size compared to\nvanilla GS, while having comparable accuracy and consuming only half the peak\nmemory. More visualisations are available at\nhttps://anilarmagan.github.io/SRUK-GSta.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06719", "pdf": "https://arxiv.org/pdf/2504.06719", "abs": "https://arxiv.org/abs/2504.06719", "authors": ["Pedro Hermosilla", "Christian Stippel", "Leon Sick"], "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025", "summary": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06020", "pdf": "https://arxiv.org/pdf/2504.06020", "abs": "https://arxiv.org/abs/2504.06020", "authors": ["Liyuan Mao", "Haoran Xu", "Amy Zhang", "Weinan Zhang", "Chenjia Bai"], "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work done during internships at Institute of Artificial Intelligence\n  (TeleAI), China Telecom", "summary": "A generalizable reward model is crucial in Reinforcement Learning from Human\nFeedback (RLHF) as it enables correctly evaluating unseen prompt-response\npairs. However, existing reward models lack this ability, as they are typically\ntrained by increasing the reward gap between chosen and rejected responses,\nwhile overlooking the prompts that the responses are conditioned on.\nConsequently, when the trained reward model is evaluated on prompt-response\npairs that lie outside the data distribution, neglecting the effect of prompts\nmay result in poor generalization of the reward model. To address this issue,\nwe decompose the reward value into two independent components: prompt-free\nreward and prompt-related reward. Prompt-free reward represents the evaluation\nthat is determined only by responses, while the prompt-related reward reflects\nthe reward that derives from both the prompt and the response. We extract these\ntwo components from an information-theoretic perspective, which requires no\nextra models. Subsequently, we propose a new reward learning algorithm by\nprioritizing data samples based on their prompt-free reward values. Through toy\nexamples, we demonstrate that the extracted prompt-free and prompt-related\nrewards effectively characterize two parts of the reward model. Further,\nstandard evaluations show that our method improves both the alignment\nperformance and the generalization capability of the reward model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06741", "pdf": "https://arxiv.org/pdf/2504.06741", "abs": "https://arxiv.org/abs/2504.06741", "authors": ["Constantin Ulrich", "Tassilo Wald", "Fabian Isensee", "Klaus H. Maier-Hein"], "title": "Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The segmentation of lesions in Moderate to Severe Traumatic Brain Injury\n(msTBI) presents a significant challenge in neuroimaging due to the diverse\ncharacteristics of these lesions, which vary in size, shape, and distribution\nacross brain regions and tissue types. This heterogeneity complicates\ntraditional image processing techniques, resulting in critical errors in tasks\nsuch as image registration and brain parcellation. To address these challenges,\nthe AIMS-TBI Segmentation Challenge 2024 aims to advance innovative\nsegmentation algorithms specifically designed for T1-weighted MRI data, the\nmost widely utilized imaging modality in clinical practice. Our proposed\nsolution leverages a large-scale multi-dataset supervised pretraining approach\ninspired by the MultiTalent method. We train a Resenc L network on a\ncomprehensive collection of datasets covering various anatomical and\npathological structures, which equips the model with a robust understanding of\nbrain anatomy and pathology. Following this, the model is fine-tuned on\nmsTBI-specific data to optimize its performance for the unique characteristics\nof T1-weighted MRI scans and outperforms the baseline without pretraining up to\n2 Dice points.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06752", "pdf": "https://arxiv.org/pdf/2504.06752", "abs": "https://arxiv.org/abs/2504.06752", "authors": ["Rishbuh Parihar", "Vaibhav Agrawal", "Sachidanand VS", "R. Venkatesh Babu"], "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/compasscontrol", "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware \\textbf{compass} tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06407", "pdf": "https://arxiv.org/pdf/2504.06407", "abs": "https://arxiv.org/abs/2504.06407", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Understanding Machine Unlearning Through the Lens of Mode Connectivity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Machine Unlearning aims to remove undesired information from trained models\nwithout requiring full retraining from scratch. Despite recent advancements,\ntheir underlying loss landscapes and optimization dynamics received less\nattention. In this paper, we investigate and analyze machine unlearning through\nthe lens of mode connectivity - the phenomenon where independently trained\nmodels can be connected by smooth low-loss paths in the parameter space. We\ndefine and study mode connectivity in unlearning across a range of overlooked\nconditions, including connections between different unlearning methods, models\ntrained with and without curriculum learning, and models optimized with\nfirst-order and secondorder techniques. Our findings show distinct patterns of\nfluctuation of different evaluation metrics along the curve, as well as the\nmechanistic (dis)similarity between unlearning methods. To the best of our\nknowledge, this is the first study on mode connectivity in the context of\nmachine unlearning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06514", "pdf": "https://arxiv.org/pdf/2504.06514", "abs": "https://arxiv.org/abs/2504.06514", "authors": ["Chenrui Fan", "Ming Li", "Lichao Sun", "Tianyi Zhou"], "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06575", "pdf": "https://arxiv.org/pdf/2504.06575", "abs": "https://arxiv.org/abs/2504.06575", "authors": ["Li An", "Yujian Liu", "Yepeng Liu", "Yang Zhang", "Yuheng Bu", "Shiyu Chang"], "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06838", "pdf": "https://arxiv.org/pdf/2504.06838", "abs": "https://arxiv.org/abs/2504.06838", "authors": ["Seonghwan Park", "Jaehyeon Jeong", "Yongjun Kim", "Jaeho Lee", "Namhoon Lee"], "title": "ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICLR 2025", "summary": "Recent studies have introduced various approaches for prompt-tuning black-box\nvision-language models, referred to as black-box prompt-tuning (BBPT). While\nBBPT has demonstrated considerable potential, it is often found that many\nexisting methods require an excessive number of queries (i.e., function\nevaluations), which poses a significant challenge in real-world scenarios where\nthe number of allowed queries is limited. To tackle this issue, we propose\nZeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that\nenables efficient and robust prompt optimization in a purely black-box setting.\nThe key idea of ZIP is to reduce the problem dimensionality and the variance of\nzeroth-order gradient estimates, such that the training is done fast with far\nless queries. We achieve this by re-parameterizing prompts in low-rank\nrepresentations and designing intrinsic-dimensional clipping of estimated\ngradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks\nand show that it achieves an average improvement of approximately 6% in\nfew-shot accuracy and 48% in query efficiency compared to the best-performing\nalternative BBPT methods, establishing a new state of the art. Our ablation\nanalysis further shows that the proposed clipping mechanism is robust and\nnearly optimal, without the need to manually select the clipping threshold,\nmatching the result of expensive hyperparameter search.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06841", "pdf": "https://arxiv.org/pdf/2504.06841", "abs": "https://arxiv.org/abs/2504.06841", "authors": ["Tom Simon", "William Mocaer", "Pierrick Tranouez", "Clement Chatelain", "Thierry Paquet"], "title": "Classifying the Unknown: In-Context Learning for Open-Vocabulary Text and Symbol Recognition", "categories": ["cs.CV"], "comment": "Submitted to ICDAR 2025", "summary": "We introduce Rosetta, a multimodal model that leverages Multimodal In-Context\nLearning (MICL) to classify sequences of novel script patterns in documents by\nleveraging minimal examples, thus eliminating the need for explicit retraining.\nTo enhance contextual learning, we designed a dataset generation process that\nensures varying degrees of contextual informativeness, improving the model's\nadaptability in leveraging context across different scenarios. A key strength\nof our method is the use of a Context-Aware Tokenizer (CAT), which enables\nopen-vocabulary classification. This allows the model to classify text and\nsymbol patterns across an unlimited range of classes, extending its\nclassification capabilities beyond the scope of its training alphabet of\npatterns. As a result, it unlocks applications such as the recognition of new\nalphabets and languages. Experiments on synthetic datasets demonstrate the\npotential of Rosetta to successfully classify Out-Of-Distribution visual\npatterns and diverse sets of alphabets and scripts, including but not limited\nto Chinese, Greek, Russian, French, Spanish, and Japanese.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089", "abs": "https://arxiv.org/abs/2504.07089", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "title": "OmniCaptioner: One Captioner to Rule Them All", "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06861", "pdf": "https://arxiv.org/pdf/2504.06861", "abs": "https://arxiv.org/abs/2504.06861", "authors": ["Diljeet Jagpal", "Xi Chen", "Vinay P. Namboodiri"], "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06881", "pdf": "https://arxiv.org/pdf/2504.06881", "abs": "https://arxiv.org/abs/2504.06881", "authors": ["Mingbo Li", "Liying Liu", "Ye Luo"], "title": "Compound and Parallel Modes of Tropical Convolutional Neural Networks", "categories": ["cs.CV", "cs.AI", "I.2.6"], "comment": "28 pages, 5 figures", "summary": "Convolutional neural networks have become increasingly deep and complex,\nleading to higher computational costs. While tropical convolutional neural\nnetworks (TCNNs) reduce multiplications, they underperform compared to standard\nCNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and\nparallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus\nkernels to replace traditional convolution kernels. This reduces\nmultiplications and balances efficiency with performance. Experiments on\nvarious datasets show that cTCNN and pTCNN match or exceed the performance of\nother CNN methods. Combining these with conventional CNNs in deeper\narchitectures also improves performance. We are further exploring simplified\nTCNN architectures that reduce parameters and multiplications with minimal\naccuracy loss, aiming for efficient and effective models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06920", "pdf": "https://arxiv.org/pdf/2504.06920", "abs": "https://arxiv.org/abs/2504.06920", "authors": ["Masquil Elías", "Marí Roger", "Ehret Thibaud", "Meinhardt-Llopis Enric", "Musé Pablo", "Facciolo Gabriele"], "title": "S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications", "categories": ["cs.CV"], "comment": "Accepted at Earthvision 2025 (CVPR Workshop)", "summary": "We introduce the S-EO dataset: a large-scale, high-resolution dataset,\ndesigned to advance geometry-aware shadow detection. Collected from diverse\npublic-domain sources, including challenge datasets and government providers\nsuch as USGS, our dataset comprises 702 georeferenced tiles across the USA,\neach covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3\npansharpened RGB images, panchromatic images, and a ground-truth DSM of the\narea obtained from LiDAR scans. For each image, we provide a shadow mask\nderived from geometry and sun position, a vegetation mask based on the NDVI\nindex, and a bundle-adjusted RPC model. With approximately 20,000 images, the\nS-EO dataset establishes a new public resource for shadow detection in remote\nsensing imagery and its applications to 3D reconstruction. To demonstrate the\ndataset's impact, we train and evaluate a shadow detector, showcasing its\nability to generalize, even to aerial images. Finally, we extend EO-NeRF - a\nstate-of-the-art NeRF approach for satellite imagery - to leverage our shadow\npredictions for improved 3D reconstructions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06962", "pdf": "https://arxiv.org/pdf/2504.06962", "abs": "https://arxiv.org/abs/2504.06962", "authors": ["Thomas Kerdreux", "Alexandre Tuel", "Quentin Febvre", "Alexis Mouche", "Bertrand Chapron"], "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR Workshop : The First Workshop on Foundation and\n  Large Vision Models in Remote Sensing", "summary": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of Nereus-SAR-1, the first model in the Nereus\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/nereus-sar-models/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06965", "pdf": "https://arxiv.org/pdf/2504.06965", "abs": "https://arxiv.org/abs/2504.06965", "authors": ["Teng Xiao", "Qi Hu", "Qingsong Yan", "Wei Liu", "Zhiwei Ye", "Fei Deng"], "title": "A Deep Single Image Rectification Approach for Pan-Tilt-Zoom Cameras", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Pan-Tilt-Zoom (PTZ) cameras with wide-angle lenses are widely used in\nsurveillance but often require image rectification due to their inherent\nnonlinear distortions. Current deep learning approaches typically struggle to\nmaintain fine-grained geometric details, resulting in inaccurate rectification.\nThis paper presents a Forward Distortion and Backward Warping Network\n(FDBW-Net), a novel framework for wide-angle image rectification. It begins by\nusing a forward distortion model to synthesize barrel-distorted images,\nreducing pixel redundancy and preventing blur. The network employs a pyramid\ncontext encoder with attention mechanisms to generate backward warping flows\ncontaining geometric details. Then, a multi-scale decoder is used to restore\ndistorted features and output rectified images. FDBW-Net's performance is\nvalidated on diverse datasets: public benchmarks, AirSim-rendered PTZ camera\nimagery, and real-scene PTZ camera datasets. It demonstrates that FDBW-Net\nachieves SOTA performance in distortion rectification, boosting the\nadaptability of PTZ cameras for practical visual applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06978", "pdf": "https://arxiv.org/pdf/2504.06978", "abs": "https://arxiv.org/abs/2504.06978", "authors": ["Daiwei Zhang", "Joaquin Gajardo", "Tomislav Medic", "Isinsu Katircioglu", "Mike Boss", "Norbert Kirchgessner", "Achim Walter", "Lukas Roth"], "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting", "categories": ["cs.CV"], "comment": "Copyright 2025 IEEE. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  version is published in the 2025 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW)", "summary": "Automated extraction of plant morphological traits is crucial for supporting\ncrop breeding and agricultural management through high-throughput field\nphenotyping (HTFP). Solutions based on multi-view RGB images are attractive due\nto their scalability and affordability, enabling volumetric measurements that\n2D approaches cannot directly capture. While advanced methods like Neural\nRadiance Fields (NeRFs) have shown promise, their application has been limited\nto counting or extracting traits from only a few plants or organs. Furthermore,\naccurately measuring complex structures like individual wheat heads-essential\nfor studying crop yields-remains particularly challenging due to occlusions and\nthe dense arrangement of crop canopies in field conditions. The recent\ndevelopment of 3D Gaussian Splatting (3DGS) offers a promising alternative for\nHTFP due to its high-quality reconstructions and explicit point-based\nrepresentation. In this paper, we present Wheat3DGS, a novel approach that\nleverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance\nsegmentation and morphological measurement of hundreds of wheat heads\nautomatically, representing the first application of 3DGS to HTFP. We validate\nthe accuracy of wheat head extraction against high-resolution laser scan data,\nobtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and\n40.2% for length, width, and volume. We provide additional comparisons to\nNeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating\nsuperior results. Our approach enables rapid, non-destructive measurements of\nkey yield-related traits at scale, with significant implications for\naccelerating crop breeding and improving our understanding of wheat\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06982", "pdf": "https://arxiv.org/pdf/2504.06982", "abs": "https://arxiv.org/abs/2504.06982", "authors": ["Yuhang Yang", "Fengqi Liu", "Yixing Lu", "Qin Zhao", "Pingyu Wu", "Wei Zhai", "Ran Yi", "Yang Cao", "Lizhuang Ma", "Zheng-Jun Zha", "Junting Dong"], "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets", "categories": ["cs.CV"], "comment": "project page:https://yyvhang.github.io/SIGMAN_3D/", "summary": "3D human digitization has long been a highly pursued yet challenging task.\nExisting methods aim to generate high-quality 3D digital humans from single or\nmultiple views, but remain primarily constrained by current paradigms and the\nscarcity of 3D human assets. Specifically, recent approaches fall into several\nparadigms: optimization-based and feed-forward (both single-view regression and\nmulti-view generation with reconstruction). However, they are limited by slow\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\nplanes to high-dimensional space due to occlusion and invisibility,\nrespectively. Furthermore, existing 3D human assets remain small-scale,\ninsufficient for large-scale training. To address these challenges, we propose\na latent space generation paradigm for 3D human digitization, which involves\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\nwith DiT-based conditional generation, we transform the ill-posed\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\nwhich also supports end-to-end inference. In addition, we employ the multi-view\noptimization approach combined with synthetic data to construct the HGS-1M\ndataset, which contains $1$ million 3D Gaussian assets to support the\nlarge-scale training. Experimental results demonstrate that our paradigm,\npowered by large-scale training, produces high-quality 3D human Gaussians with\nintricate textures, facial details, and loose clothing deformation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07078", "pdf": "https://arxiv.org/pdf/2504.07078", "abs": "https://arxiv.org/abs/2504.07078", "authors": ["Meien Li", "Mark Stamp"], "title": "Detecting AI-generated Artwork", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The high efficiency and quality of artwork generated by Artificial\nIntelligence (AI) has created new concerns and challenges for human artists. In\nparticular, recent improvements in generative AI have made it difficult for\npeople to distinguish between human-generated and AI-generated art. In this\nresearch, we consider the potential utility of various types of Machine\nLearning (ML) and Deep Learning (DL) models in distinguishing AI-generated\nartwork from human-generated artwork. We focus on three challenging artistic\nstyles, namely, baroque, cubism, and expressionism. The learning models we test\nare Logistic Regression (LR), Support Vector Machine (SVM), Multilayer\nPerceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental\nresults yield a multiclass accuracy of 0.8208 over six classes, and an\nimpressive accuracy of 0.9758 for the binary classification problem of\ndistinguishing AI-generated from human-generated art.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07083", "pdf": "https://arxiv.org/pdf/2504.07083", "abs": "https://arxiv.org/abs/2504.07083", "authors": ["Mengchen Zhang", "Tong Wu", "Jing Tan", "Ziwei Liu", "Gordon Wetzstein", "Dahua Lin"], "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography", "categories": ["cs.CV"], "comment": null, "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089", "abs": "https://arxiv.org/abs/2504.07089", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "title": "OmniCaptioner: One Captioner to Rule Them All", "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07093", "pdf": "https://arxiv.org/pdf/2504.07093", "abs": "https://arxiv.org/abs/2504.07093", "authors": ["Gene Chou", "Wenqi Xian", "Guandao Yang", "Mohamed Abdelfattah", "Bharath Hariharan", "Noah Snavely", "Ning Yu", "Paul Debevec"], "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution", "categories": ["cs.CV"], "comment": null, "summary": "A versatile video depth estimation model should (1) be accurate and\nconsistent across frames, (2) produce high-resolution depth maps, and (3)\nsupport real-time streaming. We propose FlashDepth, a method that satisfies all\nthree requirements, performing depth estimation on a 2044x1148 streaming video\nat 24 FPS. We show that, with careful modifications to pretrained single-image\ndepth models, these capabilities are enabled with relatively little data and\ntraining. We evaluate our approach across multiple unseen datasets against\nstate-of-the-art depth models, and find that ours outperforms them in terms of\nboundary sharpness and speed by a significant margin, while maintaining\ncompetitive accuracy. We hope our model will enable various applications that\nrequire high-resolution depth, such as video editing, and online\ndecision-making, such as robotics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06385", "pdf": "https://arxiv.org/pdf/2504.06385", "abs": "https://arxiv.org/abs/2504.06385", "authors": ["Paul Roetzer", "Florian Bernard"], "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching", "categories": ["cs.GR", "cs.CV"], "comment": "8 pages main paper", "summary": "Geometric consistency, i.e. the preservation of neighbourhoods, is a natural\nand strong prior in 3D shape matching. Geometrically consistent matchings are\ncrucial for many downstream applications, such as texture transfer or\nstatistical shape modelling. Yet, in practice, geometric consistency is often\noverlooked, or only achieved under severely limiting assumptions (e.g. a good\ninitialisation). In this work, we propose a novel formalism for computing\nglobally optimal and geometrically consistent matchings between 3D shapes which\nis scalable in practice. Our key idea is to represent the surface of the source\nshape as a collection of cyclic paths, which are then consistently matched to\nthe target shape. Mathematically, we construct a hyper product graph (between\nsource and target shape), and then cast 3D shape matching as a minimum-cost\ncirculation flow problem in this hyper graph, which yields global geometrically\nconsistent matchings between both shapes. We empirically show that our\nformalism is efficiently solvable and that it leads to high-quality results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06407", "pdf": "https://arxiv.org/pdf/2504.06407", "abs": "https://arxiv.org/abs/2504.06407", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Understanding Machine Unlearning Through the Lens of Mode Connectivity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Machine Unlearning aims to remove undesired information from trained models\nwithout requiring full retraining from scratch. Despite recent advancements,\ntheir underlying loss landscapes and optimization dynamics received less\nattention. In this paper, we investigate and analyze machine unlearning through\nthe lens of mode connectivity - the phenomenon where independently trained\nmodels can be connected by smooth low-loss paths in the parameter space. We\ndefine and study mode connectivity in unlearning across a range of overlooked\nconditions, including connections between different unlearning methods, models\ntrained with and without curriculum learning, and models optimized with\nfirst-order and secondorder techniques. Our findings show distinct patterns of\nfluctuation of different evaluation metrics along the curve, as well as the\nmechanistic (dis)similarity between unlearning methods. To the best of our\nknowledge, this is the first study on mode connectivity in the context of\nmachine unlearning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06479", "pdf": "https://arxiv.org/pdf/2504.06479", "abs": "https://arxiv.org/abs/2504.06479", "authors": ["Julian Nubert", "Turcan Tuna", "Jonas Frey", "Cesar Cadena", "Katherine J. Kuchenbecker", "Shehryar Khattak", "Marco Hutter"], "title": "Holistic Fusion: Task- and Setup-Agnostic Robot Localization and State Estimation with Factor Graphs", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": "21 pages, 25 figures, 9 tables, journal submission", "summary": "Seamless operation of mobile robots in challenging environments requires\nlow-latency local motion estimation (e.g., dynamic maneuvers) and accurate\nglobal localization (e.g., wayfinding). While most existing sensor-fusion\napproaches are designed for specific scenarios, this work introduces a flexible\nopen-source solution for task- and setup-agnostic multimodal sensor fusion that\nis distinguished by its generality and usability. Holistic Fusion formulates\nsensor fusion as a combined estimation problem of i) the local and global robot\nstate and ii) a (theoretically unlimited) number of dynamic context variables,\nincluding automatic alignment of reference frames; this formulation fits\ncountless real-world applications without any conceptual modifications. The\nproposed factor-graph solution enables the direct fusion of an arbitrary number\nof absolute, local, and landmark measurements expressed with respect to\ndifferent reference frames by explicitly including them as states in the\noptimization and modeling their evolution as random walks. Moreover, local\nsmoothness and consistency receive particular attention to prevent jumps in the\nrobot state belief. HF enables low-latency and smooth online state estimation\non typical robot hardware while simultaneously providing low-drift global\nlocalization at the IMU measurement rate. The efficacy of this released\nframework is demonstrated in five real-world scenarios on three robotic\nplatforms, each with distinct task requirements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06610", "pdf": "https://arxiv.org/pdf/2504.06610", "abs": "https://arxiv.org/abs/2504.06610", "authors": ["Sumeyye Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "title": "Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 4 figures, 1 table", "summary": "In this work, we propose a simple gloss-free, transformer-based sign language\nproduction (SLP) framework that directly maps spoken-language text to sign pose\nsequences. We first train a pose autoencoder that encodes sign poses into a\ncompact latent space using an articulator-based disentanglement strategy, where\nfeatures corresponding to the face, right hand, left hand, and body are modeled\nseparately to promote structured and interpretable representation learning.\nNext, a non-autoregressive transformer decoder is trained to predict these\nlatent representations from sentence-level text embeddings. To guide this\nprocess, we apply channel-aware regularization by aligning predicted latent\ndistributions with priors extracted from the ground-truth encodings using a\nKL-divergence loss. The contribution of each channel to the loss is weighted\naccording to its associated articulator region, enabling the model to account\nfor the relative importance of different articulators during training. Our\napproach does not rely on gloss supervision or pretrained models, and achieves\nstate-of-the-art results on the PHOENIX14T dataset using only a modest training\nset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06767", "pdf": "https://arxiv.org/pdf/2504.06767", "abs": "https://arxiv.org/abs/2504.06767", "authors": ["Paolo Angella", "Luca Balbi", "Fabrizio Ferrando", "Paolo Traverso", "Rosario Varriale", "Vito Paolo Pastore", "Matteo Santacesaria"], "title": "DIMA: DIffusing Motion Artifacts for unsupervised correction in brain MRI images", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 7 tables", "summary": "Motion artifacts remain a significant challenge in Magnetic Resonance Imaging\n(MRI), compromising diagnostic quality and potentially leading to misdiagnosis\nor repeated scans. Existing deep learning approaches for motion artifact\ncorrection typically require paired motion-free and motion-affected images for\ntraining, which are rarely available in clinical settings. To overcome this\nrequirement, we present DIMA (DIffusing Motion Artifacts), a novel framework\nthat leverages diffusion models to enable unsupervised motion artifact\ncorrection in brain MRI. Our two-phase approach first trains a diffusion model\non unpaired motion-affected images to learn the distribution of motion\nartifacts. This model then generates realistic motion artifacts on clean\nimages, creating paired datasets suitable for supervised training of correction\nnetworks. Unlike existing methods, DIMA operates without requiring k-space\nmanipulation or detailed knowledge of MRI sequence parameters, making it\nadaptable across different scanning protocols and hardware. Comprehensive\nevaluations across multiple datasets and anatomical planes demonstrate that our\nmethod achieves comparable performance to state-of-the-art supervised\napproaches while offering superior generalizability to real clinical data. DIMA\nrepresents a significant advancement in making motion artifact correction more\naccessible for routine clinical use, potentially reducing the need for repeat\nscans and improving diagnostic accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06921", "pdf": "https://arxiv.org/pdf/2504.06921", "abs": "https://arxiv.org/abs/2504.06921", "authors": ["Anisa V. Prasad", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Jianfei Liu", "Ronald M. Summers"], "title": "Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2025", "summary": "An accurate segmentation of the pancreas on CT is crucial to identify\npancreatic pathologies and extract imaging-based biomarkers. However, prior\nresearch on pancreas segmentation has primarily focused on modifying the\nsegmentation model architecture or utilizing pre- and post-processing\ntechniques. In this article, we investigate the utility of anatomical priors to\nenhance the segmentation performance of the pancreas. Two 3D full-resolution\nnnU-Net models were trained, one with 8 refined labels from the public PANORAMA\ndataset, and another that combined them with labels derived from the public\nTotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\\%\nincrease in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff\ndistance for pancreas segmentation ($p < .001$). Moreover, the pancreas was\nalways detected when anatomy priors were used, whereas there were 8 instances\nof failed detections without their use. The use of anatomy priors shows promise\nfor pancreas segmentation and subsequent derivation of imaging biomarkers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06924", "pdf": "https://arxiv.org/pdf/2504.06924", "abs": "https://arxiv.org/abs/2504.06924", "authors": ["Tejas Sudharshan Mathai", "Benjamin Hou", "Ronald M. Summers"], "title": "Longitudinal Assessment of Lung Lesion Burden in CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2025", "summary": "In the U.S., lung cancer is the second major cause of death. Early detection\nof suspicious lung nodules is crucial for patient treatment planning,\nmanagement, and improving outcomes. Many approaches for lung nodule\nsegmentation and volumetric analysis have been proposed, but few have looked at\nlongitudinal changes in total lung tumor burden. In this work, we trained two\n3D models (nnUNet) with and without anatomical priors to automatically segment\nlung lesions and quantified total lesion burden for each patient. The 3D model\nwithout priors significantly outperformed ($p < .001$) the model trained with\nanatomy priors. For detecting clinically significant lesions $>$ 1cm, a\nprecision of 71.3\\%, sensitivity of 68.4\\%, and F1-score of 69.8\\% was\nachieved. For segmentation, a Dice score of 77.1 $\\pm$ 20.3 and Hausdorff\ndistance error of 11.7 $\\pm$ 24.1 mm was obtained. The median lesion burden was\n6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and\nautomated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also\nevaluated with linear regression and Bland-Altman plots. The proposed approach\ncan produce a personalized evaluation of the total tumor burden for a patient\nand facilitate interval change tracking over time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
