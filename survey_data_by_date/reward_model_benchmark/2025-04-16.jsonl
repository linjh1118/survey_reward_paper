{"id": "2504.10496", "pdf": "https://arxiv.org/pdf/2504.10496", "abs": "https://arxiv.org/abs/2504.10496", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "title": "ArxivBench: Can LLMs Assist Researchers in Conducting Research?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable effectiveness in\ncompleting various tasks such as reasoning, translation, and question\nanswering. However the issue of factual incorrect content in LLM-generated\nresponses remains a persistent challenge. In this study, we evaluate both\nproprietary and open-source LLMs on their ability to respond with relevant\nresearch papers and accurate links to articles hosted on the arXiv platform,\nbased on high level prompts. To facilitate this evaluation, we introduce\narXivBench, a benchmark specifically designed to assess LLM performance across\neight major subject categories on arXiv and five subfields within computer\nscience, one of the most popular categories among them. Our findings reveal a\nconcerning accuracy of LLM-generated responses depending on the subject, with\nsome subjects experiencing significantly lower accuracy than others. Notably,\nClaude-3.5-Sonnet exhibits a substantial advantage in generating both relevant\nand accurate responses. And interestingly, most LLMs achieve a much higher\naccuracy in the Artificial Intelligence sub-field than other sub-fields. This\nbenchmark provides a standardized tool for evaluating the reliability of\nLLM-generated scientific responses, promoting more dependable use of LLMs in\nacademic and research environments. Our code is open-sourced at\nhttps://github.com/arxivBenchLLM/arXivBench and our dataset is available on\nhuggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10568", "pdf": "https://arxiv.org/pdf/2504.10568", "abs": "https://arxiv.org/abs/2504.10568", "authors": ["Aruna Gauba", "Irene Pi", "Yunze Man", "Ziqi Pang", "Vikram S. Adve", "Yu-Xiong Wang"], "title": "AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark", "categories": ["cs.CV"], "comment": "Project Website: https://agmmu.github.io/ Huggingface:\n  https://huggingface.co/datasets/AgMMU/AgMMU_v1/", "summary": "We curate a dataset AgMMU for evaluating and developing vision-language\nmodels (VLMs) to produce factually accurate answers for knowledge-intensive\nexpert domains. Our AgMMU concentrates on one of the most socially beneficial\ndomains, agriculture, which requires connecting detailed visual observation\nwith precise knowledge to diagnose, e.g., pest identification, management\ninstructions, etc. As a core uniqueness of our dataset, all facts, questions,\nand answers are extracted from 116,231 conversations between real-world users\nand authorized agricultural experts. After a three-step dataset curation\npipeline with GPT-4o, LLaMA models, and human verification, AgMMU features an\nevaluation set of 5,460 multiple-choice questions (MCQs) and open-ended\nquestions (OEQs). We also provide a development set that contains 205,399\npieces of agricultural knowledge information, including disease identification,\nsymptoms descriptions, management instructions, insect and pest identification,\nand species identification. As a multimodal factual dataset, it reveals that\nexisting VLMs face significant challenges with questions requiring both\ndetailed perception and factual knowledge. Moreover, open-source VLMs still\ndemonstrate a substantial performance gap compared to proprietary ones. To\nadvance knowledge-intensive VLMs, we conduct fine-tuning experiments using our\ndevelopment set, which improves LLaVA-1.5 evaluation accuracy by up to 3.1%. We\nhope that AgMMU can serve both as an evaluation benchmark dedicated to\nagriculture and a development suite for incorporating knowledge-intensive\nexpertise into general-purpose VLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792", "abs": "https://arxiv.org/abs/2504.10792", "authors": ["Jessica Lin", "Amir Zeldes"], "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "consistency", "summarization"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10757", "pdf": "https://arxiv.org/pdf/2504.10757", "abs": "https://arxiv.org/abs/2504.10757", "authors": ["Amirhosein Chahe", "Lifeng Zhou"], "title": "ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) show promise for autonomous driving but often\nlack transparent reasoning capabilities that are critical for safety. We\ninvestigate whether explicitly modeling reasoning during fine-tuning enhances\nVLM performance on driving decision tasks. Using GPT-4o, we generate structured\nreasoning chains for driving scenarios from the DriveLM benchmark with\ncategory-specific prompting strategies. We compare reasoning-based fine-tuning,\nanswer-only fine-tuning, and baseline instruction-tuned models across multiple\nsmall VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results\ndemonstrate that reasoning-based fine-tuning consistently outperforms\nalternatives, with Llama3.2-11B-reason achieving the highest performance.\nModels fine-tuned with reasoning show substantial improvements in accuracy and\ntext generation quality, suggesting explicit reasoning enhances internal\nrepresentations for driving decisions. These findings highlight the importance\nof transparent decision processes in safety-critical domains and offer a\npromising direction for developing more interpretable autonomous driving\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy", "question answering"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10776", "pdf": "https://arxiv.org/pdf/2504.10776", "abs": "https://arxiv.org/abs/2504.10776", "authors": ["Zhenyu Yu", "Hanqing Chen", "Mohd Yamani Idna Idris", "Pei Wang"], "title": "Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation", "categories": ["cs.CV"], "comment": null, "summary": "Precipitation plays a critical role in the Earth's hydrological cycle,\ndirectly affecting ecosystems, agriculture, and water resource management.\nAccurate precipitation estimation and prediction are crucial for understanding\nclimate dynamics, disaster preparedness, and environmental monitoring. In\nrecent years, artificial intelligence (AI) has gained increasing attention in\nquantitative remote sensing (QRS), enabling more advanced data analysis and\nimproving precipitation estimation accuracy. Although traditional methods have\nbeen widely used for precipitation estimation, they face limitations due to the\ndifficulty of data acquisition and the challenge of capturing complex feature\nrelationships. Furthermore, the lack of standardized multi-source satellite\ndatasets, and in most cases, the exclusive reliance on station data,\nsignificantly hinders the effective application of advanced AI models. To\naddress these challenges, we propose the Rainy dataset, a multi-source\nspatio-temporal dataset that integrates pure satellite data with station data,\nand propose Taper Loss, designed to fill the gap in tasks where only in-situ\ndata is available without area-wide support. The Rainy dataset supports five\nmain tasks: (1) satellite calibration, (2) precipitation event prediction, (3)\nprecipitation level prediction, (4) spatiotemporal prediction, and (5)\nprecipitation downscaling. For each task, we selected benchmark models and\nevaluation metrics to provide valuable references for researchers. Using\nprecipitation as an example, the Rainy dataset and Taper Loss demonstrate the\nseamless collaboration between QRS and computer vision, offering data support\nfor AI for Science in the field of QRS and providing valuable insights for\ninterdisciplinary collaboration and integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11337", "pdf": "https://arxiv.org/pdf/2504.11337", "abs": "https://arxiv.org/abs/2504.11337", "authors": ["Zhihao Xu", "Yongqi Tong", "Xin Zhang", "Jun Zhou", "Xiting Wang"], "title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Multi-objective preference alignment in language models often encounters a\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\nfrequently compromises others (e.g., harmlessness) due to the inherent\nconflicts between competing objectives. While prior work mainly focuses on\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\ntypes of data that can effectively mitigate these conflicts. Specifically, we\npropose the concept of Reward Consistency (RC), which identifies samples that\nalign with multiple preference objectives, thereby reducing conflicts during\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\nsamples inherently constrain performance degradation during multi-objective\noptimization. Building on these insights, we further develop Reward Consistency\nSampling, a framework that automatically constructs preference datasets that\neffectively mitigate conflicts during multi-objective alignment. Our generated\ndata achieves an average improvement of 13.37% in both the harmless rate and\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\nconsistently resolve conflicts in varying multi-objective scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "helpfulness", "harmlessness", "consistency"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11456", "pdf": "https://arxiv.org/pdf/2504.11456", "abs": "https://arxiv.org/abs/2504.11456", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10885", "pdf": "https://arxiv.org/pdf/2504.10885", "abs": "https://arxiv.org/abs/2504.10885", "authors": ["Zeyu Zhang", "Zijian Chen", "Zicheng Zhang", "Yuze Sun", "Yuan Tian", "Ziheng Jia", "Chunyi Li", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "title": "PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities\nacross a wide range of multimodal tasks, achieving ever-increasing performance\non various evaluation benchmarks. However, existing benchmarks are typically\nstatic and often overlap with pre-training datasets, leading to fixed\ncomplexity constraints and substantial data contamination issues. Meanwhile,\nmanually annotated datasets are labor-intensive, time-consuming, and subject to\nhuman bias and inconsistency, leading to reliability and reproducibility\nissues. To address these problems, we propose a fully dynamic multimodal\nevaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which\naims to generate fresh, diverse, and verifiable evaluation data automatically\nin puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw\nmaterial sampling module, a visual content generation module, and a puzzle rule\ndesign module, which ensures that each evaluation instance is primitive, highly\nrandomized, and uniquely solvable, enabling continual adaptation to the\nevolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a\ndynamic and scalable benchmark comprising 11,840 VQA samples. It features six\ncarefully designed puzzle tasks targeting three core LMM competencies, visual\nrecognition, logical reasoning, and context understanding. PuzzleBench differs\nfrom static benchmarks that quickly become outdated. It enables ongoing dataset\nrefreshing through OVPG and a rich set of open-ended puzzle designs, allowing\nseamless adaptation to the evolving capabilities of LMMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11218", "pdf": "https://arxiv.org/pdf/2504.11218", "abs": "https://arxiv.org/abs/2504.11218", "authors": ["Zeming wei", "Junyi Lin", "Yang Liu", "Weixing Chen", "Jingzhou Luo", "Guanbin Li", "Liang Lin"], "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "categories": ["cs.CV"], "comment": "The first large-scale 3D Gaussians Affordance Reasoning Benchmark", "summary": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10615", "pdf": "https://arxiv.org/pdf/2504.10615", "abs": "https://arxiv.org/abs/2504.10615", "authors": ["Thilo Hagendorff", "Sarah Fabi"], "title": "Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can perform reasoning computations both\ninternally within their latent space and externally by generating explicit\ntoken sequences like chains of thought. Significant progress in enhancing\nreasoning abilities has been made by scaling test-time compute. However,\nunderstanding and quantifying model-internal reasoning abilities - the\ninferential \"leaps\" models make between individual token predictions - remains\ncrucial. This study introduces a benchmark (n = 4,000 items) designed to\nquantify model-internal reasoning in different domains. We achieve this by\nhaving LLMs indicate the correct solution to reasoning problems not through\ndescriptive text, but by selecting a specific language of their initial\nresponse token that is different from English, the benchmark language. This not\nonly requires models to reason beyond their context window, but also to\noverrise their default tendency to respond in the same language as the prompt,\nthereby posing an additional cognitive strain. We evaluate a set of 18 LLMs,\nshowing significant performance variations, with GPT-4.5 achieving the highest\naccuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B\n(65.6%). Control experiments and difficulty scaling analyses suggest that while\nLLMs engage in internal reasoning, we cannot rule out heuristic exploitations\nunder certain conditions, marking an area for future investigation. Our\nexperiments demonstrate that LLMs can \"think\" via latent-space computations,\nrevealing model-internal inference strategies that need further understanding,\nespecially regarding safety-related concerns such as covert planning,\ngoal-seeking, or deception emerging without explicit token traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10823", "pdf": "https://arxiv.org/pdf/2504.10823", "abs": "https://arxiv.org/abs/2504.10823", "authors": ["Ayoung Lee", "Ryan Sungmo Kwon", "Peter Railton", "Lu Wang"], "title": "CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Navigating high-stakes dilemmas involving conflicting values is challenging\neven for humans, let alone for AI. Yet prior work in evaluating the reasoning\ncapabilities of large language models (LLMs) in such situations has been\nlimited to everyday scenarios. To close this gap, this work first introduces\nCLASH (Character perspective-based LLM Assessments in Situations with\nHigh-stakes), a meticulously curated dataset consisting of 345 high-impact\ndilemmas along with 3,795 individual perspectives of diverse values. In\nparticular, we design CLASH in a way to support the study of critical aspects\nof value-based decision-making processes which are missing from prior work,\nincluding understanding decision ambivalence and psychological discomfort as\nwell as capturing the temporal shifts of values in characters' perspectives. By\nbenchmarking 10 open and closed frontier models, we uncover several key\nfindings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet,\nachieve less than 50% accuracy in identifying situations where the decision\nshould be ambivalent, while they perform significantly better in clear-cut\nscenarios. (2) While LLMs reasonably predict psychological discomfort as marked\nby human, they inadequately comprehend perspectives involving value shifts,\nindicating a need for LLMs to reason over complex values. (3) Our experiments\nalso reveal a significant correlation between LLMs' value preferences and their\nsteerability towards a given value. (4) Finally, LLMs exhibit greater\nsteerability when engaged in value reasoning from a third-party perspective,\ncompared to a first-person setup, though certain value pairs benefit uniquely\nfrom the first-person framing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10861", "pdf": "https://arxiv.org/pdf/2504.10861", "abs": "https://arxiv.org/abs/2504.10861", "authors": ["Amanpreet Singh", "Joseph Chee Chang", "Chloe Anastasiades", "Dany Haddad", "Aakanksha Naik", "Amber Tanaka", "Angele Zamarron", "Cecile Nguyen", "Jena D. Hwang", "Jason Dunkleberger", "Matt Latzke", "Smita Rao", "Jaron Lochner", "Rob Evans", "Rodney Kinney", "Daniel S. Weld", "Doug Downey", "Sergey Feldman"], "title": "Ai2 Scholar QA: Organized Literature Synthesis with Attribution", "categories": ["cs.CL"], "comment": "7 pages", "summary": "Retrieval-augmented generation is increasingly effective in answering\nscientific questions from literature, but many state-of-the-art systems are\nexpensive and closed-source. We introduce Ai2 Scholar QA, a free online\nscientific question answering application. To facilitate research, we make our\nentire pipeline public: as a customizable open-source Python package and\ninteractive web app, along with paper indexes accessible through public APIs\nand downloadable datasets. We describe our system in detail and present\nexperiments analyzing its key design decisions. In an evaluation on a recent\nscientific QA benchmark, we find that Ai2 Scholar QA outperforms competing\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11169", "pdf": "https://arxiv.org/pdf/2504.11169", "abs": "https://arxiv.org/abs/2504.11169", "authors": ["Laura De Grazia", "Pol Pastells", "Mauro Vázquez Chas", "Desmond Elliott", "Danae Sánchez Villegas", "Mireia Farrús", "Mariona Taulé"], "title": "MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sexism is generally defined as prejudice and discrimination based on sex or\ngender, affecting every sector of society, from social institutions to\nrelationships and individual behavior. Social media platforms amplify the\nimpact of sexism by conveying discriminatory content not only through text but\nalso across multiple modalities, highlighting the critical need for a\nmultimodal approach to the analysis of sexism online. With the rise of social\nmedia platforms where users share short videos, sexism is increasingly\nspreading through video content. Automatically detecting sexism in videos is a\nchallenging task, as it requires analyzing the combination of verbal, audio,\nand visual elements to identify sexist content. In this study, (1) we introduce\nMuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of\n$\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose\nan innovative annotation framework for analyzing the contribution of textual\nand multimodal labels in the classification of sexist and non-sexist content;\nand (3) we evaluate a range of large language models (LLMs) and multimodal LLMs\non the task of sexism detection. We find that visual information plays a key\nrole in labeling sexist content for both humans and models. Models effectively\ndetect explicit sexism; however, they struggle with implicit cases, such as\nstereotypes, instances where annotators also show low agreement. This\nhighlights the inherent difficulty of the task, as identifying implicit sexism\ndepends on the social and cultural context.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "agreement"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11186", "pdf": "https://arxiv.org/pdf/2504.11186", "abs": "https://arxiv.org/abs/2504.11186", "authors": ["Minjie Zou", "Sahana Srinivasan", "Thaddaeus Wai Soon Lo", "Ke Zou", "Gabriel Dawei Yang", "Xuguang Ai", "Hyunjae Kim", "Maxwell Singer", "Fares Antaki", "Kelvin Li", "Robert Chang", "Marcus Tan", "David Ziyou Chen", "Dianbo Liu", "Qingyu Chen", "Yih Chung Tham"], "title": "Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items", "categories": ["cs.CL", "cs.AI"], "comment": "83 pages, 6 figures, 3 tables, 9 supplementary figures, 7\n  supplementary tables", "summary": "Recent advances in reasoning-focused large language models (LLMs) mark a\nshift from general LLMs toward models designed for complex decision-making, a\ncrucial aspect in medicine. However, their performance in specialized domains\nlike ophthalmology remains underexplored. This study comprehensively evaluated\nand compared the accuracy and reasoning capabilities of four newly developed\nreasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0\nFlash-Thinking. Each model was assessed using 5,888 multiple-choice\nophthalmology exam questions from the MedMCQA dataset in zero-shot setting.\nQuantitative evaluation included accuracy, Macro-F1, and five text-generation\nmetrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed\nagainst ground-truth reasonings. Average inference time was recorded for a\nsubset of 100 randomly selected questions. Additionally, two board-certified\nophthalmologists qualitatively assessed clarity, completeness, and reasoning\nstructure of responses to differential diagnosis questions.O1 (0.902) and\nDeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in\nMacro-F1 (0.900). The performance of models across the text-generation metrics\nvaried: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1\nand o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0\nFlash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and\no1 (0.176) led AlignScore. Inference time across the models varied, with\nDeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest\n(6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0\nFlash-Thinking tended to provide detailed and comprehensive intermediate\nreasoning, whereas o1 and o3-mini displayed concise and summarized\njustifications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10808", "pdf": "https://arxiv.org/pdf/2504.10808", "abs": "https://arxiv.org/abs/2504.10808", "authors": ["Md Rakibul Hasan", "Shafin Rahman", "Md Zakir Hossain", "Aneesh Krishna", "Tom Gedeon"], "title": "Tabular foundation model to detect empathy from visual cues", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Detecting empathy from video interactions is an emerging area of research.\nVideo datasets, however, are often released as extracted features (i.e.,\ntabular data) rather than raw footage due to privacy and ethical concerns.\nPrior research on such tabular datasets established tree-based classical\nmachine learning approaches as the best-performing models. Motivated by the\nrecent success of textual foundation models (i.e., large language models), we\nexplore the use of tabular foundation models in empathy detection from tabular\nvisual features. We experiment with two recent tabular foundation models $-$\nTabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups.\nOur experiments on a public human-robot interaction benchmark demonstrate a\nsignificant boost in cross-subject empathy detection accuracy over several\nstrong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow\n0.669$). In addition to performance improvement, we contribute novel insights\nand an evaluation setup to ensure generalisation on unseen subjects in this\npublic benchmark. As the practice of releasing video features as tabular\ndatasets is likely to persist due to privacy constraints, our findings will be\nwidely applicable to future empathy detection video datasets as well.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11277", "pdf": "https://arxiv.org/pdf/2504.11277", "abs": "https://arxiv.org/abs/2504.11277", "authors": ["Guocong Li", "Weize Liu", "Yihang Wu", "Ping Wang", "Shuaihan Huang", "Hongxia Xu", "Jian Wu"], "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering (QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information. We will\npublicly release our code upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11331", "pdf": "https://arxiv.org/pdf/2504.11331", "abs": "https://arxiv.org/abs/2504.11331", "authors": ["Hao Liu", "Lijun He", "Jiaxi Liang", "Zhihan Ren", "Fan Li"], "title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL", "cs.MM"], "comment": "submitted to ACM MM2025", "summary": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "aspect-based", "fine-grained"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11373", "pdf": "https://arxiv.org/pdf/2504.11373", "abs": "https://arxiv.org/abs/2504.11373", "authors": ["Wang Bill Zhu", "Tianqi Chen", "Ching Ying Lin", "Jade Law", "Mazen Jizzini", "Jorge J. Nieva", "Ruishan Liu", "Robin Jia"], "title": "Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Cancer patients are increasingly turning to large language models (LLMs) as a\nnew form of internet search for medical information, making it critical to\nassess how well these models handle complex, personalized questions. However,\ncurrent medical benchmarks focus on medical exams or consumer-searched\nquestions and do not evaluate LLMs on real patient questions with detailed\nclinical contexts. In this paper, we first evaluate LLMs on cancer-related\nquestions drawn from real patients, reviewed by three hematology oncology\nphysicians. While responses are generally accurate, with GPT-4-Turbo scoring\n4.13 out of 5, the models frequently fail to recognize or address false\npresuppositions in the questions-posing risks to safe medical decision-making.\nTo study this limitation systematically, we introduce Cancer-Myth, an\nexpert-verified adversarial dataset of 585 cancer-related questions with false\npresuppositions. On this benchmark, no frontier LLM -- including GPT-4o,\nGemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions\nmore than 30% of the time. Even advanced medical agentic methods do not prevent\nLLMs from ignoring false presuppositions. These findings expose a critical gap\nin the clinical reliability of LLMs and underscore the need for more robust\nsafeguards in medical AI systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10880", "pdf": "https://arxiv.org/pdf/2504.10880", "abs": "https://arxiv.org/abs/2504.10880", "authors": ["Aviral Chharia", "Tianyu Ren", "Tomotake Furuhata", "Kenji Shimada"], "title": "Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task", "categories": ["cs.CV"], "comment": "CVPR Workshop 2025; Project Website:\n  https://Safe-Construct.github.io/Safe-Construct", "summary": "Recognizing safety violations in construction environments is critical yet\nremains underexplored in computer vision. Existing models predominantly rely on\n2D object detection, which fails to capture the complexities of real-world\nviolations due to: (i) an oversimplified task formulation treating violation\nrecognition merely as object detection, (ii) inadequate validation under\nrealistic conditions, (iii) absence of standardized baselines, and (iv) limited\nscalability from the unavailability of synthetic dataset generators for diverse\nconstruction scenarios. To address these challenges, we introduce\nSafe-Construct, the first framework that reformulates violation recognition as\na 3D multi-view engagement task, leveraging scene-level worker-object context\nand 3D spatial understanding. We also propose the Synthetic Indoor Construction\nSite Generator (SICSG) to create diverse, scalable training data, overcoming\ndata limitations. Safe-Construct achieves a 7.6% improvement over\nstate-of-the-art methods across four violation types. We rigorously evaluate\nour approach in near-realistic settings, incorporating four violations, four\nworkers, 14 objects, and challenging conditions like occlusions (worker-object,\nworker-worker) and variable illumination (back-lighting, overexposure,\nsunlight). By integrating 3D multi-view spatial understanding and synthetic\ndata generation, Safe-Construct sets a new benchmark for scalable and robust\nsafety monitoring in high-risk industries. Project Website:\nhttps://Safe-Construct.github.io/Safe-Construct", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10889", "pdf": "https://arxiv.org/pdf/2504.10889", "abs": "https://arxiv.org/abs/2504.10889", "authors": ["Shripad Pate", "Aiman Farooq", "Suvrankar Dutta", "Musadiq Aadil Sheikh", "Atin Kumar", "Deepak Mishra"], "title": "Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model", "categories": ["cs.CV"], "comment": null, "summary": "Accurate rib fracture identification and classification are essential for\ntreatment planning. However, existing datasets often lack fine-grained\nannotations, particularly regarding rib fracture characterization, type, and\nprecise anatomical location on individual ribs. To address this, we introduce a\nnovel rib fracture annotation protocol tailored for fracture classification.\nFurther, we enhance fracture classification by leveraging cross-modal\nembeddings that bridge radiological images and clinical descriptions. Our\napproach employs hyperbolic embeddings to capture the hierarchical nature of\nfracture, mapping visual features and textual descriptions into a shared\nnon-Euclidean manifold. This framework enables more nuanced similarity\ncomputations between imaging characteristics and clinical descriptions,\naccounting for the inherent hierarchical relationships in fracture taxonomy.\nExperimental results demonstrate that our approach outperforms existing methods\nacross multiple classification tasks, with average recall improvements of 6% on\nthe AirRib dataset and 17.5% on the public RibFrac dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "fine-grained"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11014", "pdf": "https://arxiv.org/pdf/2504.11014", "abs": "https://arxiv.org/abs/2504.11014", "authors": ["Eunsoo Im", "Jung Kwon Lee", "Changhyun Jee"], "title": "GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*", "categories": ["cs.CV", "cs.AI"], "comment": "9pages, 1 supple", "summary": "The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11111", "pdf": "https://arxiv.org/pdf/2504.11111", "abs": "https://arxiv.org/abs/2504.11111", "authors": ["Yu Lin", "Jianghang Lin", "Kai Ye", "You Shen", "Yan Zhang", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Although fully-supervised oriented object detection has made significant\nprogress in multimodal remote sensing image understanding, it comes at the cost\nof labor-intensive annotation. Recent studies have explored weakly and\nsemi-supervised learning to alleviate this burden. However, these methods\noverlook the difficulties posed by dense annotations in complex remote sensing\nscenes. In this paper, we introduce a novel setting called sparsely annotated\noriented object detection (SAOOD), which only labels partial instances, and\npropose a solution to address its challenges. Specifically, we focus on two key\nissues in the setting: (1) sparse labeling leading to overfitting on limited\nforeground representations, and (2) unlabeled objects (false negatives)\nconfusing feature learning. To this end, we propose the S$^2$Teacher, a novel\nmethod that progressively mines pseudo-labels for unlabeled objects, from easy\nto hard, to enhance foreground representations. Additionally, it reweights the\nloss of unlabeled objects to mitigate their impact during training. Extensive\nexperiments demonstrate that S$^2$Teacher not only significantly improves\ndetector performance across different sparse annotation levels but also\nachieves near-fully-supervised performance on the DOTA dataset with only 10%\nannotation instances, effectively balancing detection accuracy with annotation\nefficiency. The code will be public.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11230", "pdf": "https://arxiv.org/pdf/2504.11230", "abs": "https://arxiv.org/abs/2504.11230", "authors": ["Jingshun Huang", "Haitao Lin", "Tianyu Wang", "Yanwei Fu", "Xiangyang Xue", "Yi Zhu"], "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image", "categories": ["cs.CV", "cs.RO"], "comment": "To appear in CVPR 2025 (Highlight)", "summary": "This paper tackles category-level pose estimation of articulated objects in\nrobotic manipulation tasks and introduces a new benchmark dataset. While recent\nmethods estimate part poses and sizes at the category level, they often rely on\ngeometric cues and complex multi-stage pipelines that first segment parts from\nthe point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation\nfor 6D poses. These approaches overlook dense semantic cues from RGB images,\nleading to suboptimal accuracy, particularly for objects with small parts. To\naddress these limitations, we propose a single-stage Network, CAP-Net, for\nestimating the 6D poses and sizes of Categorical Articulated Parts. This method\ncombines RGB-D features to generate instance segmentation and NPCS\nrepresentations for each part in an end-to-end manner. CAP-Net uses a unified\nnetwork to simultaneously predict point-wise class labels, centroid offsets,\nand NPCS maps. A clustering algorithm then groups points of the same predicted\nclass based on their estimated centroid distances to isolate each part.\nFinally, the NPCS region of each part is aligned with the point cloud to\nrecover its final pose and size. To bridge the sim-to-real domain gap, we\nintroduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date,\nfeaturing photorealistic RGB images and depth noise simulated from real\nsensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our\nmethod significantly outperforms the state-of-the-art approach. Real-world\ndeployments of our model in robotic tasks underscore its robustness and\nexceptional sim-to-real transfer capabilities, confirming its substantial\npractical utility. Our dataset, code and pre-trained models are available on\nthe project page.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11349", "pdf": "https://arxiv.org/pdf/2504.11349", "abs": "https://arxiv.org/abs/2504.11349", "authors": ["Yuezhe Yang", "Boyu Yang", "Yaqian Wang", "Yang He", "Xingbo Dong", "Zhe Jin"], "title": "Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.4.5"], "comment": "43 pages, 5 figures, submit to Medical Image Analysis", "summary": "The demand for high-quality medical imaging in clinical practice and assisted\ndiagnosis has made 3D reconstruction in radiological imaging a key research\nfocus. Artificial intelligence (AI) has emerged as a promising approach to\nenhancing reconstruction accuracy while reducing acquisition and processing\ntime, thereby minimizing patient radiation exposure and discomfort and\nultimately benefiting clinical diagnosis. This review explores state-of-the-art\nAI-based 3D reconstruction algorithms in radiological imaging, categorizing\nthem into explicit and implicit approaches based on their underlying\nprinciples. Explicit methods include point-based, volume-based, and Gaussian\nrepresentations, while implicit methods encompass implicit prior embedding and\nneural radiance fields. Additionally, we examine commonly used evaluation\nmetrics and benchmark datasets. Finally, we discuss the current state of\ndevelopment, key challenges, and future research directions in this evolving\nfield. Our project available on: https://github.com/Bean-Young/AI4Med.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10552", "pdf": "https://arxiv.org/pdf/2504.10552", "abs": "https://arxiv.org/abs/2504.10552", "authors": ["Arash Torabi Goodarzi", "Roman Kochnev", "Waleed Khalid", "Furui Qin", "Tolgay Atinc Uzun", "Yashkumar Sanjaybhai Dhameliya", "Yash Kanubhai Kathiriya", "Zofia Antonina Bentyn", "Dmitry Ignatov", "Radu Timofte"], "title": "LEMUR Neural Network Dataset: Towards Seamless AutoML", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DL"], "comment": null, "summary": "Neural networks are fundamental in artificial intelligence, driving progress\nin computer vision and natural language processing. High-quality datasets are\ncrucial for their development, and there is growing interest in datasets\ncomposed of neural networks themselves to support benchmarking, automated\nmachine learning (AutoML), and model analysis. We introduce LEMUR, an open\nsource dataset of neural network models with well-structured code for diverse\narchitectures across tasks such as object detection, image classification,\nsegmentation, and natural language processing. LEMUR is primarily designed to\nenable fine-tuning of large language models (LLMs) for AutoML tasks, providing\na rich source of structured model representations and associated performance\ndata. Leveraging Python and PyTorch, LEMUR enables seamless extension to new\ndatasets and models while maintaining consistency. It integrates an\nOptuna-powered framework for evaluation, hyperparameter optimization,\nstatistical analysis, and graphical insights. LEMUR provides an extension that\nenables models to run efficiently on edge devices, facilitating deployment in\nresource-constrained environments. Providing tools for model evaluation,\npreprocessing, and database management, LEMUR supports researchers and\npractitioners in developing, testing, and analyzing neural networks.\nAdditionally, it offers an API that delivers comprehensive information about\nneural network models and their complete performance statistics with a single\nrequest, which can be used in experiments with code-generating large language\nmodels. The LEMUR will be released as an open source project under the MIT\nlicense upon acceptance of the paper.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11022", "pdf": "https://arxiv.org/pdf/2504.11022", "abs": "https://arxiv.org/abs/2504.11022", "authors": ["Joana Reuss", "Jan Macdonald", "Simon Becker", "Konrad Schultka", "Lorenz Richter", "Marco Körner"], "title": "Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset", "categories": ["cs.LG", "cs.CV"], "comment": "19 pages, 7 figures, 12 tables", "summary": "Spatial imbalances in crop type data pose significant challenges for accurate\nclassification in remote sensing applications. Algorithms aiming at\ntransferring knowledge from data-rich to data-scarce tasks have thus surged in\npopularity. However, despite their effectiveness in previous evaluations, their\nperformance in challenging real-world applications is unclear and needs to be\nevaluated. This study benchmarks transfer learning and several meta-learning\nalgorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML),\nAlmost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the\nreal-world EuroCropsML time series dataset, which combines farmer-reported crop\ndata with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal.\nOur findings indicate that MAML-based meta-learning algorithms achieve slightly\nhigher accuracy compared to simpler transfer learning methods when applied to\ncrop type classification tasks in Estonia after pre-training on data from\nLatvia. However, this improvement comes at the cost of increased computational\ndemands and training time. Moreover, we find that the transfer of knowledge\nbetween geographically disparate regions, such as Estonia and Portugal, poses\nsignificant challenges to all investigated algorithms. These insights\nunderscore the trade-offs between accuracy and computational resource\nrequirements in selecting machine learning methods for real-world crop type\nclassification tasks and highlight the difficulties of transferring knowledge\nbetween different regions of the Earth. To facilitate future research in this\ndomain, we present the first comprehensive benchmark for evaluating transfer\nand meta-learning methods for crop type classification under real-world\nconditions. The corresponding code is publicly available at\nhttps://github.com/dida-do/eurocrops-meta-learning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257", "abs": "https://arxiv.org/abs/2504.11257", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation.In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://colmon46.github.io/i2e-bench-leaderboard/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10660", "pdf": "https://arxiv.org/pdf/2504.10660", "abs": "https://arxiv.org/abs/2504.10660", "authors": ["Paul Rosu"], "title": "LITERA: An LLM Based Approach to Latin-to-English Translation", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL Findings", "summary": "This paper introduces an LLM-based Latin-to-English translation platform\ndesigned to address the challenges of translating Latin texts. We named the\nmodel LITERA, which stands for Latin Interpretation and Translations into\nEnglish for Research Assistance. Through a multi-layered translation process\nutilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an\nunprecedented level of accuracy, showcased by greatly improved BLEU scores,\nparticularly in classical Latin, along with improved BLEURT scores. The\ndevelopment of LITERA involved close collaboration with Duke University's\nClassical Studies Department, which was instrumental in creating a small,\nhigh-quality parallel Latin-English dataset. This paper details the\narchitecture, fine-tuning methodology, and prompting strategies used in LITERA,\nemphasizing its ability to produce literal translations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10679", "pdf": "https://arxiv.org/pdf/2504.10679", "abs": "https://arxiv.org/abs/2504.10679", "authors": ["F. A. Rizvi", "T. Navojith", "A. M. N. H. Adhikari", "W. P. U. Senevirathna", "Dharshana Kasthurirathna", "Lakmini Abeywardhana"], "title": "Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 Pages, 2 figures, 7 Tables", "summary": "Brand reputation in the banking sector is maintained through insightful\nanalysis of customer opinion on code-mixed and multilingual content.\nConventional NLP models misclassify or ignore code-mixed text, when mix with\nlow resource languages such as Sinhala-English and fail to capture\ndomain-specific knowledge. This study introduces a hybrid NLP method to improve\nkeyword extraction, content filtering, and aspect-based classification of\nbanking content. Keyword extraction in English is performed with a hybrid\napproach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT\nembeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%.\nCode-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa\nmodel integrated with a domain-specific Sinhala financial vocabulary, and it\nresults in an accuracy of 87.4%. To ensure data quality, irrelevant comment\nfiltering was performed using several models, with the BERT-base-uncased model\nachieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better\nthan GPT-4o, SVM, and keyword-based filtering. Aspect classification followed\nthe same pattern, with the BERT-base-uncased model achieving 87.4% for English\nand XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based\napproaches. These findings confirm that fine-tuned transformer models\noutperform traditional methods in multilingual financial text analysis. The\npresent framework offers an accurate and scalable solution for brand reputation\nmonitoring in code-mixed and low-resource banking environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "aspect-based"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10642", "pdf": "https://arxiv.org/pdf/2504.10642", "abs": "https://arxiv.org/abs/2504.10642", "authors": ["Tan-Hanh Pham", "Chris Ngo", "Trong-Duong Bui", "Minh Luu Quang", "Tan-Huong Pham", "Truong-Son Hy"], "title": "SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging", "categories": ["cs.CV"], "comment": "CVPR Multimodal Algorithmic Reasoning Workshop 2025 - SilVarMed", "summary": "Medical Visual Language Models have shown great potential in various\nhealthcare applications, including medical image captioning and diagnostic\nassistance. However, most existing models rely on text-based instructions,\nlimiting their usability in real-world clinical environments especially in\nscenarios such as surgery, text-based interaction is often impractical for\nphysicians. In addition, current medical image analysis models typically lack\ncomprehensive reasoning behind their predictions, which reduces their\nreliability for clinical decision-making. Given that medical diagnosis errors\ncan have life-changing consequences, there is a critical need for interpretable\nand rational medical assistance. To address these challenges, we introduce an\nend-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image\nassistant that integrates speech interaction with VLMs, pioneering the task of\nvoice-based communication for medical image analysis. In addition, we focus on\nthe interpretation of the reasoning behind each prediction of medical\nabnormalities with a proposed reasoning dataset. Through extensive experiments,\nwe demonstrate a proof-of-concept study for reasoning-driven medical image\ninterpretation with end-to-end speech interaction. We believe this work will\nadvance the field of medical AI by fostering more transparent, interactive, and\nclinically viable diagnostic support systems. Our code and dataset are publicly\navailable at SiVar-Med.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11042", "pdf": "https://arxiv.org/pdf/2504.11042", "abs": "https://arxiv.org/abs/2504.11042", "authors": ["Sukannya Purkayastha", "Zhuang Li", "Anne Lauscher", "Lizhen Qu", "Iryna Gurevych"], "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "categories": ["cs.CL"], "comment": "29 pages, 18 Figures, 15 Tables", "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10804", "pdf": "https://arxiv.org/pdf/2504.10804", "abs": "https://arxiv.org/abs/2504.10804", "authors": ["Jiani Liu", "Zhiyuan Wang", "Zeliang Zhang", "Chao Huang", "Susan Liang", "Yunlong Tang", "Chenliang Xu"], "title": "The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability", "categories": ["cs.CV"], "comment": "Work in progress. 10 pages. 4 figures", "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nrange of applications, including many safety-critical tasks. However, their\nunique architectural properties raise new challenges and opportunities in\nadversarial robustness. In particular, we observe that adversarial examples\ncrafted on ViTs exhibit higher transferability compared to those crafted on\nCNNs, suggesting that ViTs contain structural characteristics favorable for\ntransferable attacks. In this work, we investigate the role of computational\nredundancy in ViTs and its impact on adversarial transferability. Unlike prior\nstudies that aim to reduce computation for efficiency, we propose to exploit\nthis redundancy to improve the quality and transferability of adversarial\nexamples. Through a detailed analysis, we identify two forms of redundancy,\nincluding the data-level and model-level, that can be harnessed to amplify\nattack effectiveness. Building on this insight, we design a suite of\ntechniques, including attention sparsity manipulation, attention head\npermutation, clean token regularization, ghost MoE diversification, and\ntest-time adversarial training. Extensive experiments on the ImageNet-1k\ndataset validate the effectiveness of our approach, showing that our methods\nsignificantly outperform existing baselines in both transferability and\ngenerality across diverse model architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10810", "pdf": "https://arxiv.org/pdf/2504.10810", "abs": "https://arxiv.org/abs/2504.10810", "authors": ["Anmol Singhal Navya Singhal"], "title": "PatrolVision: Automated License Plate Recognition in the wild", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in IEEE Southeast Con 2025. To be published in IEEEXplore", "summary": "Adoption of AI driven techniques in public services remains low due to\nchallenges related to accuracy and speed of information at population scale.\nComputer vision techniques for traffic monitoring have not gained much\npopularity despite their relative strength in areas such as autonomous driving.\nDespite large number of academic methods for Automatic License Plate\nRecognition (ALPR) systems, very few provide an end to end solution for\npatrolling in the city. This paper presents a novel prototype for a low power\nGPU based patrolling system to be deployed in an urban environment on\nsurveillance vehicles for automated vehicle detection, recognition and\ntracking. In this work, we propose a complete ALPR system for Singapore license\nplates having both single and double line creating our own YOLO based network.\nWe focus on unconstrained capture scenarios as would be the case in real world\napplication, where the license plate (LP) might be considerably distorted due\nto oblique views. In this work, we first detect the license plate from the full\nimage using RFB-Net and rectify multiple distorted license plates in a single\nimage. After that, the detected license plate image is fed to our network for\ncharacter recognition. We evaluate the performance of our proposed system on a\nnewly built dataset covering more than 16,000 images. The system was able to\ncorrectly detect license plates with 86\\% precision and recognize characters of\na license plate in 67\\% of the test set, and 89\\% accuracy with one incorrect\ncharacter (partial match). We also test latency of our system and achieve 64FPS\non Tesla P4 GPU", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11369", "pdf": "https://arxiv.org/pdf/2504.11369", "abs": "https://arxiv.org/abs/2504.11369", "authors": ["Lucio La Cava", "Andrea Tagarelli"], "title": "OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": "Under review with ARR", "summary": "Open Large Language Models (OLLMs) are increasingly leveraged in generative\nAI applications, posing new challenges for detecting their outputs. We propose\nOpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate\nmachine-generated text detectors on the Turing Test and Authorship Attribution\nproblems. OpenTuringBench focuses on a representative set of OLLMs, and\nfeatures a number of challenging evaluation tasks, including\nhuman/machine-manipulated texts, out-of-domain texts, and texts from previously\nunseen models. We also provide OTBDetector, a contrastive learning framework to\ndetect and attribute OLLM-based machine-generated texts. Results highlight the\nrelevance and varying degrees of difficulty of the OpenTuringBench tasks, with\nour detector achieving remarkable capabilities across the various tasks and\noutperforming most existing detectors. Resources are available on the\nOpenTuringBench Hugging Face repository at\nhttps://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10834", "pdf": "https://arxiv.org/pdf/2504.10834", "abs": "https://arxiv.org/abs/2504.10834", "authors": ["Sihang Chen", "Lijun Yun", "Ze Liu", "JianFeng Zhu", "Jie Chen", "Hui Wang", "Yueping Nie"], "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation", "categories": ["cs.CV"], "comment": "26 pages, 69 figures", "summary": "Deep learning techniques have achieved remarkable success in the semantic\nsegmentation of remote sensing images and in land-use change detection.\nNevertheless, their real-time deployment on edge platforms remains constrained\nby decoder complexity. Herein, we introduce LightFormer, a lightweight decoder\nfor time-critical tasks that involve unstructured targets, such as disaster\nassessment, unmanned aerial vehicle search-and-rescue, and cultural heritage\nmonitoring. LightFormer employs a feature-fusion and refinement module built on\nchannel processing and a learnable gating mechanism to aggregate multi-scale,\nmulti-range information efficiently, which drastically curtails model\ncomplexity. Furthermore, we propose a spatial information selection module\n(SISM) that integrates long-range attention with a detail preservation branch\nto capture spatial dependencies across multiple scales, thereby substantially\nimproving the recognition of unstructured targets in complex scenes. On the\nISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9%\nvs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters,\nthus achieving an excellent accuracy-efficiency trade-off. Consistent results\non LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its\nrobustness and superior perception of unstructured objects. These findings\nhighlight LightFormer as a practical solution for remote sensing applications\nwhere both computational economy and high-precision segmentation are\nimperative.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11381", "pdf": "https://arxiv.org/pdf/2504.11381", "abs": "https://arxiv.org/abs/2504.11381", "authors": ["Juan Diego Rodriguez", "Wenxuan Ding", "Katrin Erk", "Greg Durrett"], "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "question answering"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11409", "pdf": "https://arxiv.org/pdf/2504.11409", "abs": "https://arxiv.org/abs/2504.11409", "authors": ["Ali Taghibakhshi", "Sharath Turuvekere Sreenivas", "Saurav Muralidharan", "Marcin Chochowski", "Yashaswi Karnati", "Raviraj Joshi", "Ameya Sunil Mahabaleshwarkar", "Zijia Chen", "Yoshi Suhara", "Oluwatobi Olabiyi", "Daniel Korzekwa", "Mostofa Patwary", "Mohammad Shoeybi", "Jan Kautz", "Bryan Catanzaro", "Ashwath Aithal", "Nima Tajbakhsh", "Pavlo Molchanov"], "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dimension"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10854", "pdf": "https://arxiv.org/pdf/2504.10854", "abs": "https://arxiv.org/abs/2504.10854", "authors": ["Hanning Chen", "Yang Ni", "Wenjun Huang", "Hyunwoo Oh", "Yezi Liu", "Tamoghno Das", "Mohsen Imani"], "title": "LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision Language Models (LVLMs) have been widely adopted to guide vision\nfoundation models in performing reasoning segmentation tasks, achieving\nimpressive performance. However, the substantial computational overhead\nassociated with LVLMs presents a new challenge. The primary source of this\ncomputational cost arises from processing hundreds of image tokens. Therefore,\nan effective strategy to mitigate such overhead is to reduce the number of\nimage tokens, a process known as image token pruning. Previous studies on image\ntoken pruning for LVLMs have primarily focused on high level visual\nunderstanding tasks, such as visual question answering and image captioning. In\ncontrast, guiding vision foundation models to generate accurate visual masks\nbased on textual queries demands precise semantic and spatial reasoning\ncapabilities. Consequently, pruning methods must carefully control individual\nimage tokens throughout the LVLM reasoning process. Our empirical analysis\nreveals that existing methods struggle to adequately balance reductions in\ncomputational overhead with the necessity to maintain high segmentation\naccuracy. In this work, we propose LVLM_CSP, a novel training free visual token\npruning method specifically designed for LVLM based reasoning segmentation\ntasks. LVLM_CSP consists of three stages: clustering, scattering, and pruning.\nInitially, the LVLM performs coarse-grained visual reasoning using a subset of\nselected image tokens. Next, fine grained reasoning is conducted, and finally,\nmost visual tokens are pruned in the last stage. Extensive experiments\ndemonstrate that LVLM_CSP achieves a 65% reduction in image token inference\nFLOPs with virtually no accuracy degradation, and a 70% reduction with only a\nminor 1% drop in accuracy on the 7B LVLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10888", "pdf": "https://arxiv.org/pdf/2504.10888", "abs": "https://arxiv.org/abs/2504.10888", "authors": ["Jiahuan Long", "Wen Yao", "Tingsong Jiang", "Chao Ma"], "title": "CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial patches are widely used to evaluate the robustness of object\ndetection systems in real-world scenarios. These patches were initially\ndesigned to deceive single-modal detectors (e.g., visible or infrared) and have\nrecently been extended to target visible-infrared dual-modal detectors.\nHowever, existing dual-modal adversarial patch attacks have limited attack\neffectiveness across diverse physical scenarios. To address this, we propose\nCDUPatch, a universal cross-modal patch attack against visible-infrared object\ndetectors across scales, views, and scenarios. Specifically, we observe that\ncolor variations lead to different levels of thermal absorption, resulting in\ntemperature differences in infrared imaging. Leveraging this property, we\npropose an RGB-to-infrared adapter that maps RGB patches to infrared patches,\nenabling unified optimization of cross-modal patches. By learning an optimal\ncolor distribution on the adversarial patch, we can manipulate its thermal\nresponse and generate an adversarial infrared texture. Additionally, we\nintroduce a multi-scale clipping strategy and construct a new visible-infrared\ndataset, MSDrone, which contains aerial vehicle images in varying scales and\nperspectives. These data augmentation strategies enhance the robustness of our\npatch in real-world conditions. Experiments on four benchmark datasets (e.g.,\nDroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms\nexisting patch attacks in the digital domain. Extensive physical tests further\nconfirm strong transferability across scales, views, and scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10514", "pdf": "https://arxiv.org/pdf/2504.10514", "abs": "https://arxiv.org/abs/2504.10514", "authors": ["Yijun Liang", "Ming Li", "Chenrui Fan", "Ziyue Li", "Dang Nguyen", "Kwesi Cobbina", "Shweta Bhardwaj", "Jiuhai Chen", "Fuxiao Liu", "Tianyi Zhou"], "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "33 pages, including references and appendix. Code is available at\n  https://github.com/tianyi-lab/ColorBench", "summary": "Color plays an important role in human perception and usually provides\ncritical clues in visual reasoning. However, it is unclear whether and how\nvision-language models (VLMs) can perceive, understand, and leverage color as\nhumans. This paper introduces ColorBench, an innovative benchmark meticulously\ncrafted to assess the capabilities of VLMs in color understanding, including\ncolor perception, reasoning, and robustness. By curating a suite of diverse\ntest scenarios, with grounding in real applications, ColorBench evaluates how\nthese models perceive colors, infer meanings from color-based cues, and\nmaintain consistent performance under varying color transformations. Through an\nextensive evaluation of 32 VLMs with varying language models and vision\nencoders, our paper reveals some undiscovered findings: (i) The scaling law\n(larger models are better) still holds on ColorBench, while the language model\nplays a more important role than the vision encoder. (ii) However, the\nperformance gaps across models are relatively small, indicating that color\nunderstanding has been largely neglected by existing VLMs. (iii) CoT reasoning\nimproves color understanding accuracies and robustness, though they are\nvision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on\nColorBench but they can also mislead models in some tasks. These findings\nhighlight the critical limitations of current VLMs and underscore the need to\nenhance color comprehension. Our ColorBenchcan serve as a foundational tool for\nadvancing the study of human-level color understanding of multimodal AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scaling law"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10905", "pdf": "https://arxiv.org/pdf/2504.10905", "abs": "https://arxiv.org/abs/2504.10905", "authors": ["Yukang Lin", "Yan Hong", "Zunnan Xu", "Xindi Li", "Chao Xu", "Chuanbiao Song", "Ronghui Li", "Haoxing Chen", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang", "Xiu Li"], "title": "InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation", "categories": ["cs.CV", "cs.HC"], "comment": "under preview", "summary": "Recent video generation research has focused heavily on isolated actions,\nleaving interactive motions-such as hand-face interactions-largely unexamined.\nThese interactions are essential for emerging biometric authentication systems,\nwhich rely on interactive motion-based anti-spoofing approaches. From a\nsecurity perspective, there is a growing need for large-scale, high-quality\ninteractive videos to train and strengthen authentication models. In this work,\nwe introduce a novel paradigm for animating realistic hand-face interactions.\nOur approach simultaneously learns spatio-temporal contact dynamics and\nbiomechanically plausible deformation effects, enabling natural interactions\nwhere hand movements induce anatomically accurate facial deformations while\nmaintaining collision-free contact. To facilitate this research, we present\nInterHF, a large-scale hand-face interaction dataset featuring 18 interaction\npatterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a\nregion-aware diffusion model designed specifically for interaction animation.\nInterAnimate leverages learnable spatial and temporal latents to effectively\ncapture dynamic interaction priors and integrates a region-aware interaction\nmechanism that injects these priors into the denoising process. To the best of\nour knowledge, this work represents the first large-scale effort to\nsystematically study human hand-face interactions. Qualitative and quantitative\nresults show InterAnimate produces highly realistic animations, setting a new\nbenchmark. Code and data will be made public to advance research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10929", "pdf": "https://arxiv.org/pdf/2504.10929", "abs": "https://arxiv.org/abs/2504.10929", "authors": ["Chang Yu", "Yisi Luo", "Kai Ye", "Xile Zhao", "Deyu Meng"], "title": "Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters", "categories": ["cs.CV"], "comment": null, "summary": "Implicit neural representation (INR) has emerged as a powerful paradigm for\nvisual data representation. However, classical INR methods represent data in\nthe original space mixed with different frequency components, and several\nfeature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank\n$R$) need manual configurations. In this work, we propose a self-evolving\ncross-frequency INR using the Haar wavelet transform (termed CF-INR), which\ndecouples data into four frequency components and employs INRs in the wavelet\nspace. CF-INR allows the characterization of different frequency components\nseparately, thus enabling higher accuracy for data representation. To more\nprecisely characterize cross-frequency components, we propose a cross-frequency\ntensor decomposition paradigm for CF-INR with self-evolving parameters, which\nautomatically updates the rank parameter $R$ and the frequency parameter\n$\\omega$ for each frequency component through self-evolving optimization. This\nself-evolution paradigm eliminates the laborious manual tuning of these\nparameters, and learns a customized cross-frequency feature encoding\nconfiguration for each dataset. We evaluate CF-INR on a variety of visual data\nrepresentation and recovery tasks, including image regression, inpainting,\ndenoising, and cloud removal. Extensive experiments demonstrate that CF-INR\noutperforms state-of-the-art methods in each case.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10972", "pdf": "https://arxiv.org/pdf/2504.10972", "abs": "https://arxiv.org/abs/2504.10972", "authors": ["Yihang Liu", "Lianghua He", "Ying Wen", "Longzhen Yang", "Hongzhou Chen"], "title": "AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images", "categories": ["cs.CV"], "comment": null, "summary": "Current self-supervised methods, such as contrastive learning, predominantly\nfocus on global discrimination, neglecting the critical fine-grained anatomical\ndetails required for accurate radiographic analysis. To address this challenge,\nwe propose an Anatomy-driven self-supervised framework for enhancing\nFine-grained Representation in radiographic image analysis (AFiRe). The core\nidea of AFiRe is to align the anatomical consistency with the unique\ntoken-processing characteristics of Vision Transformer. Specifically, AFiRe\nsynergistically performs two self-supervised schemes: (i) Token-wise\nanatomy-guided contrastive learning, which aligns image tokens based on\nstructural and categorical consistency, thereby enhancing fine-grained\nspatial-anatomical discrimination; (ii) Pixel-level anomaly-removal\nrestoration, which particularly focuses on local anomalies, thereby refining\nthe learned discrimination with detailed geometrical information. Additionally,\nwe propose Synthetic Lesion Mask to enhance anatomical diversity while\npreserving intra-consistency, which is typically corrupted by traditional data\naugmentations, such as Cropping and Affine transformations. Experimental\nresults show that AFiRe: (i) provides robust anatomical discrimination,\nachieving more cohesive feature clusters compared to state-of-the-art\ncontrastive learning methods; (ii) demonstrates superior generalization,\nsurpassing 7 radiography-specific self-supervised methods in multi-label\nclassification tasks with limited labeling; and (iii) integrates fine-grained\ninformation, enabling precise anomaly detection using only image-level\nannotations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10979", "pdf": "https://arxiv.org/pdf/2504.10979", "abs": "https://arxiv.org/abs/2504.10979", "authors": ["Pancheng Zhao", "Deng-Ping Fan", "Shupeng Cheng", "Salman Khan", "Fahad Shahbaz Khan", "David Clifton", "Peng Xu", "Jufeng Yang"], "title": "Deep Learning in Concealed Dense Prediction", "categories": ["cs.CV"], "comment": "Technique Report", "summary": "Deep learning is developing rapidly and handling common computer vision tasks\nwell. It is time to pay attention to more complex vision tasks, as model size,\nknowledge, and reasoning capabilities continue to improve. In this paper, we\nintroduce and review a family of complex tasks, termed Concealed Dense\nPrediction (CDP), which has great value in agriculture, industry, etc. CDP's\nintrinsic trait is that the targets are concealed in their surroundings, thus\nfully perceiving them requires fine-grained representations, prior knowledge,\nauxiliary reasoning, etc. The contributions of this review are three-fold: (i)\nWe introduce the scope, characteristics, and challenges specific to CDP tasks\nand emphasize their essential differences from generic vision tasks. (ii) We\ndevelop a taxonomy based on concealment counteracting to summarize deep\nlearning efforts in CDP through experiments on three tasks. We compare 25\nstate-of-the-art methods across 12 widely used concealed datasets. (iii) We\ndiscuss the potential applications of CDP in the large model era and summarize\n6 potential research directions. We offer perspectives for the future\ndevelopment of CDP by constructing a large-scale multimodal instruction\nfine-tuning dataset, CvpINST, and a concealed visual perception agent,\nCvpAgent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11239", "pdf": "https://arxiv.org/pdf/2504.11239", "abs": "https://arxiv.org/abs/2504.11239", "authors": ["Chang Yang", "Ruiyu Wang", "Junzhe Jiang", "Qi Jiang", "Qinggang Zhang", "Yanchen Deng", "Shuxin Li", "Shuyue Hu", "Bo Li", "Florian T. Pokorny", "Xiao Huang", "Xinrun Wang"], "title": "Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "Preliminary work, 10 pages for main text", "summary": "Reasoning is the fundamental capability of large language models (LLMs). Due\nto the rapid progress of LLMs, there are two main issues of current benchmarks:\ni) these benchmarks can be crushed in a short time (less than 1 year), and ii)\nthese benchmarks may be easily hacked. To handle these issues, we propose the\never-scalingness for building the benchmarks which are uncrushable, unhackable,\nauto-verifiable and general. This paper presents Nondeterministic\nPolynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark\nfor LLMs. Specifically, the NPPC has three main modules: i) npgym, which\nprovides a unified interface of 25 well-known NP-complete problems and can\ngenerate any number of instances with any levels of complexities, ii) npsolver:\nwhich provides a unified interface to evaluate the problem instances with both\nonline and offline models via APIs and local deployments, respectively, and\niii) npeval: which provides the comprehensive and ready-to-use tools to analyze\nthe performances of LLMs over different problems, the number of tokens, the aha\nmoments, the reasoning errors and the solution errors. Extensive experiments\nover widely-used LLMs demonstrate: i) NPPC can successfully decrease the\nperformances of advanced LLMs' performances to below 10%, demonstrating that\nNPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the\nmost powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and\no1/o3-mini in most NP-complete problems considered, and iii) the numbers of\ntokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and\nDeepSeek-R1, are observed first to increase and then decrease when the problem\ninstances become more and more difficult. We believe that NPPC is the first\never-scaling reasoning benchmark, serving as the uncrushable and unhackable\ntestbed for LLMs toward artificial general intelligence (AGI).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "o1"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11015", "pdf": "https://arxiv.org/pdf/2504.11015", "abs": "https://arxiv.org/abs/2504.11015", "authors": ["Chenyang Zhu", "Xing Zhang", "Yuyang Sun", "Ching-Chun Chang", "Isao Echizen"], "title": "AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in image generation, particularly diffusion models, have\nsignificantly lowered the barrier for creating sophisticated forgeries, making\nimage manipulation detection and localization (IMDL) increasingly challenging.\nWhile prior work in IMDL has focused largely on natural images, the anime\ndomain remains underexplored-despite its growing vulnerability to AI-generated\nforgeries. Misrepresentations of AI-generated images as hand-drawn artwork,\ncopyright violations, and inappropriate content modifications pose serious\nthreats to the anime community and industry. To address this gap, we propose\nAnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive\nannotations. It comprises over two million images including real, partially\nmanipulated, and fully AI-generated samples. Experiments indicate that models\ntrained on existing IMDL datasets of natural images perform poorly when applied\nto anime images, highlighting a clear domain gap between anime and natural\nimages. To better handle IMDL tasks in anime domain, we further propose\nAniXplore, a novel model tailored to the visual characteristics of anime\nimagery. Extensive evaluations demonstrate that AniXplore achieves superior\nperformance compared to existing methods. Dataset and code can be found in\nhttps://flytweety.github.io/AnimeDL2M/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11019", "pdf": "https://arxiv.org/pdf/2504.11019", "abs": "https://arxiv.org/abs/2504.11019", "authors": ["Hyejin Lee", "Seokjun Hong", "Jeonghoon Song", "Haechan Cho", "Zhixiong Jin", "Byeonghun Kim", "Joobin Jin", "Jaegyun Im", "Byeongjoon Noh", "Hwasoo Yeo"], "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen", "categories": ["cs.CV", "I.2.10; I.4.8; H.2.8; J.7"], "comment": "30 pages, 15 figures", "summary": "Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11441", "pdf": "https://arxiv.org/pdf/2504.11441", "abs": "https://arxiv.org/abs/2504.11441", "authors": ["Elizabeth Fons", "Rachneet Kaur", "Zhen Zeng", "Soham Palande", "Tucker Balch", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "TADACap: Time-series Adaptive Domain-Aware Captioning", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICAIF 2024", "summary": "While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11050", "pdf": "https://arxiv.org/pdf/2504.11050", "abs": "https://arxiv.org/abs/2504.11050", "authors": ["Yunshuang Yuan", "Monika Sester"], "title": "Leveraging LLMs and attention-mechanism for automatic annotation of historical maps", "categories": ["cs.CV"], "comment": null, "summary": "Historical maps are essential resources that provide insights into the\ngeographical landscapes of the past. They serve as valuable tools for\nresearchers across disciplines such as history, geography, and urban studies,\nfacilitating the reconstruction of historical environments and the analysis of\nspatial transformations over time. However, when constrained to analogue or\nscanned formats, their interpretation is limited to humans and therefore not\nscalable. Recent advancements in machine learning, particularly in computer\nvision and large language models (LLMs), have opened new avenues for automating\nthe recognition and classification of features and objects in historical maps.\nIn this paper, we propose a novel distillation method that leverages LLMs and\nattention mechanisms for the automatic annotation of historical maps. LLMs are\nemployed to generate coarse classification labels for low-resolution historical\nimage patches, while attention mechanisms are utilized to refine these labels\nto higher resolutions. Experimental results demonstrate that the refined labels\nachieve a high recall of more than 90%. Additionally, the intersection over\nunion (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with\nprecision scores of 87.1% and 79.5%, respectively, indicate that most labels\nare well-aligned with ground-truth annotations. Notably, these results were\nachieved without the use of fine-grained manual labels during training,\nunderscoring the potential of our approach for efficient and scalable\nhistorical map analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11101", "pdf": "https://arxiv.org/pdf/2504.11101", "abs": "https://arxiv.org/abs/2504.11101", "authors": ["Yulong Zhang", "Tianyi Liang", "Xinyue Huang", "Erfei Cui", "Xu Guo", "Pei Chu", "Chenhui Li", "Ru Zhang", "Wenhai Wang", "Gongshen Liu"], "title": "Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0\\% accuracy gains on\nmathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs\nwhile maintaining overall performance. Notably, the entire process requires\nneither training nor supervision while maintaining plug-and-play functionality\nthroughout.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11154", "pdf": "https://arxiv.org/pdf/2504.11154", "abs": "https://arxiv.org/abs/2504.11154", "authors": ["Kaan Aydin", "Joelle Hanna", "Damian Borth"], "title": "SAR-to-RGB Translation with Latent Diffusion for Earth Observation", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 3 figures", "summary": "Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide\ncomplementary remote sensing (RS) data, but S2 images are often unavailable due\nto cloud cover or data gaps. To address this, we propose a diffusion model\n(DM)-based approach for SAR-to-RGB translation, generating synthetic optical\nimages from SAR inputs. We explore three different setups: two using Standard\nDiffusion, which reconstruct S2 images by adding and removing noise (one\nwithout and one with class conditioning), and one using Cold Diffusion, which\nblends S2 with S1 before removing the SAR signal. We evaluate the generated\nimages in downstream tasks, including land cover classification and cloud\nremoval. While generated images may not perfectly replicate real S2 data, they\nstill provide valuable information. Our results show that class conditioning\nimproves classification accuracy, while cloud removal performance remains\ncompetitive despite our approach not being optimized for it. Interestingly,\ndespite exhibiting lower perceptual quality, the Cold Diffusion setup performs\nwell in land cover classification, suggesting that traditional quantitative\nevaluation metrics may not fully reflect the practical utility of generated\nimages. Our findings highlight the potential of DMs for SAR-to-RGB translation\nin RS applications where RGB images are missing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11164", "pdf": "https://arxiv.org/pdf/2504.11164", "abs": "https://arxiv.org/abs/2504.11164", "authors": ["Chenming Li", "Chengxu Liu", "Yuanting Fan", "Xiao Jin", "Xingsong Hou", "Xueming Qian"], "title": "TSAL: Few-shot Text Segmentation Based on Attribute Learning", "categories": ["cs.CV"], "comment": null, "summary": "Recently supervised learning rapidly develops in scene text segmentation.\nHowever, the lack of high-quality datasets and the high cost of pixel\nannotation greatly limit the development of them. Considering the\nwell-performed few-shot learning methods for downstream tasks, we investigate\nthe application of the few-shot learning method to scene text segmentation. We\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\nfor segmentation. To fully utilize the semantic and texture information in the\nimage, a visual-guided branch is proposed to separately extract text and\nbackground features. To reduce data dependency and improve text detection\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\ntemplates to capture various text attributes. To enable adaptive prompts\ncapture distinctive text features and complex background distribution, we\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\ndifferent attributes with visual features and prompt prototypes, AFA enables\nadaptive prompts to capture both general and distinctive attribute information.\nTSAL can capture the unique attributes of text and achieve precise segmentation\nusing only few images. Experiments demonstrate that our method achieves SOTA\nperformance on multiple text segmentation datasets under few-shot settings and\nshow great potential in text-related domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11165", "pdf": "https://arxiv.org/pdf/2504.11165", "abs": "https://arxiv.org/abs/2504.11165", "authors": ["Linlin Xiao", "Zhang Tiancong", "Yutong Jia", "Xinyu Nie", "Mengyao Wang", "Xiaohang Shao"], "title": "YOLO-RS: Remote Sensing Enhanced Crop Detection Methods", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of remote sensing technology, crop classification\nand health detection based on deep learning have gradually become a research\nhotspot. However, the existing target detection methods show poor performance\nwhen dealing with small targets in remote sensing images, especially in the\ncase of complex background and image mixing, which is difficult to meet the\npractical application requirementsite. To address this problem, a novel target\ndetection model YOLO-RS is proposed in this paper. The model is based on the\nlatest Yolov11 which significantly enhances the detection of small targets by\nintroducing the Context Anchor Attention (CAA) mechanism and an efficient\nmulti-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional\nfeature fusion strategy in the feature fusion process, which effectively\nenhances the model's performance in the detection of small targets. Small\ntarget detection. Meanwhile, the ACmix module at the end of the model backbone\nnetwork solves the category imbalance problem by adaptively adjusting the\ncontrast and sample mixing, thus enhancing the detection accuracy in complex\nscenes. In the experiments on the PDT remote sensing crop health detection\ndataset and the CWC crop classification dataset, YOLO-RS improves both the\nrecall and the mean average precision (mAP) by about 2-3\\% or so compared with\nthe existing state-of-the-art methods, while the F1-score is also significantly\nimproved. Moreover, the computational complexity of the model only increases by\nabout 5.2 GFLOPs, indicating its significant advantages in both performance and\nefficiency. The experimental results validate the effectiveness and application\npotential of YOLO-RS in the task of detecting small targets in remote sensing\nimages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11171", "pdf": "https://arxiv.org/pdf/2504.11171", "abs": "https://arxiv.org/abs/2504.11171", "authors": ["Johannes Jakubik", "Felix Yang", "Benedikt Blumenstiel", "Erik Scheurer", "Rocco Sedona", "Stefano Maurogiovanni", "Jente Bosmans", "Nikolaos Dionelis", "Valerio Marsocci", "Niklas Kopp", "Rahul Ramachandran", "Paolo Fraccaro", "Thomas Brunschwiler", "Gabriele Cavallaro", "Juan Bernabe-Moreno", "Nicolas Longépé"], "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code is open-sourced under a permissive license.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11172", "pdf": "https://arxiv.org/pdf/2504.11172", "abs": "https://arxiv.org/abs/2504.11172", "authors": ["Benedikt Blumenstiel", "Paolo Fraccaro", "Valerio Marsocci", "Johannes Jakubik", "Stefano Maurogiovanni", "Mikolaj Czerkawski", "Rocco Sedona", "Gabriele Cavallaro", "Thomas Brunschwiler", "Juan Bernabe-Moreno", "Nicolas Longépé"], "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9 million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training and fostering robust cross-modal correlation learning.\nWe provide detailed data processing steps, comprehensive statistics, and\nempirical evidence demonstrating improved model performance when pre-trained on\nTerraMesh. The dataset will be made publicly available with a permissive\nlicense.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11262", "pdf": "https://arxiv.org/pdf/2504.11262", "abs": "https://arxiv.org/abs/2504.11262", "authors": ["Xiaoxiao Ma", "Junxiong Tong"], "title": "Enhanced Small Target Detection via Multi-Modal Fusion and Attention Mechanisms: A YOLOv5 Approach", "categories": ["cs.CV"], "comment": "Accepted by ATC 2024", "summary": "With the rapid development of information technology, modern warfare\nincreasingly relies on intelligence, making small target detection critical in\nmilitary applications. The growing demand for efficient, real-time detection\nhas created challenges in identifying small targets in complex environments due\nto interference. To address this, we propose a small target detection method\nbased on multi-modal image fusion and attention mechanisms. This method\nleverages YOLOv5, integrating infrared and visible light data along with a\nconvolutional attention module to enhance detection performance. The process\nbegins with multi-modal dataset registration using feature point matching,\nensuring accurate network training. By combining infrared and visible light\nfeatures with attention mechanisms, the model improves detection accuracy and\nrobustness. Experimental results on anti-UAV and Visdrone datasets demonstrate\nthe effectiveness and practicality of our approach, achieving superior\ndetection results for small and dim targets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11305", "pdf": "https://arxiv.org/pdf/2504.11305", "abs": "https://arxiv.org/abs/2504.11305", "authors": ["Jincheng Kang", "Yi Cen", "Yigang Cen", "Ke Wang", "Yuhan Liu"], "title": "CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 11 figures", "summary": "Wood defect detection is critical for ensuring quality control in the wood\nprocessing industry. However, current industrial applications face two major\nchallenges: traditional methods are costly, subjective, and labor-intensive,\nwhile mainstream deep learning models often struggle to balance detection\naccuracy and computational efficiency for edge deployment. To address these\nissues, this study proposes CFIS-YOLO, a lightweight object detection model\noptimized for edge devices. The model introduces an enhanced C2f structure, a\ndynamic feature recombination module, and a novel loss function that\nincorporates auxiliary bounding boxes and angular constraints. These\ninnovations improve multi-scale feature fusion and small object localization\nwhile significantly reducing computational overhead. Evaluated on a public wood\ndefect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of\n77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON\nBM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to\n17.3\\% of the original implementation, and incurs only a 0.5 percentage point\ndrop in mAP. These results demonstrate that CFIS-YOLO is a practical and\neffective solution for real-world wood defect detection in resource-constrained\nenvironments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11306", "pdf": "https://arxiv.org/pdf/2504.11306", "abs": "https://arxiv.org/abs/2504.11306", "authors": ["Trinnhallen Brisley", "Aryan Gandhi", "Joseph Magen"], "title": "Context-Aware Palmprint Recognition via a Relative Similarity Metric", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new approach to matching mechanism for palmprint recognition by\nintroducing a Relative Similarity Metric (RSM) that enhances the robustness and\ndiscriminability of existing matching frameworks. While conventional systems\nrely on direct pairwise similarity measures, such as cosine or Euclidean\ndistances, these metrics fail to capture how a pairwise similarity compares\nwithin the context of the entire dataset. Our method addresses this by\nevaluating the relative consistency of similarity scores across up to all\nidentities, allowing for better suppression of false positives and negatives.\nApplied atop the CCNet architecture, our method achieves a new state-of-the-art\n0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous\nmethods and demonstrating the efficacy of incorporating relational structure\ninto the palmprint matching process.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11310", "pdf": "https://arxiv.org/pdf/2504.11310", "abs": "https://arxiv.org/abs/2504.11310", "authors": ["Dayong Liu", "Qingrui Zhang", "Zeyang Meng"], "title": "Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection", "categories": ["cs.CV"], "comment": "in Chinese language", "summary": "In multi-target tracking and detection tasks, it is necessary to continuously\ntrack multiple targets, such as vehicles, pedestrians, etc. To achieve this\ngoal, the system must be able to continuously acquire and process image frames\ncontaining these targets. These consecutive frame images enable the algorithm\nto update the position and state of the target in real-time in each frame of\nthe image. How to accurately associate the detected target with the target in\nthe previous or next frame to form a stable trajectory is a complex problem.\nTherefore, a multi object tracking and detection method for intelligent driving\nvehicles based on YOLOv5 and point cloud 3D projection is proposed. Using\nRetinex algorithm to enhance the image of the environment in front of the\nvehicle, remove light interference in the image, and build an intelligent\ndetection model based on YOLOv5 network structure. The enhanced image is input\ninto the model, and multiple targets in front of the vehicle are identified\nthrough feature extraction and target localization. By combining point cloud 3D\nprojection technology, the correlation between the position changes of adjacent\nframe images in the projection coordinate system can be inferred. By\nsequentially projecting the multi-target recognition results of multiple\nconsecutive frame images into the 3D laser point cloud environment, effective\ntracking of the motion trajectories of all targets in front of the vehicle can\nbe achieved. The experimental results show that the application of this method\nfor intelligent driving vehicle front multi-target tracking and detection\nyields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its\nsuperior tracking and detection performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11346", "pdf": "https://arxiv.org/pdf/2504.11346", "abs": "https://arxiv.org/abs/2504.11346", "authors": ["Yu Gao", "Lixue Gong", "Qiushan Guo", "Xiaoxia Hou", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xuanda Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Xin Xia", "Xuefeng Xiao", "Zhonghua Zhai", "Xinyu Zhang", "Qi Zhang", "Yuwei Zhang", "Shijia Zhao", "Jianchao Yang", "Weilin Huang"], "title": "Seedream 3.0 Technical Report", "categories": ["cs.CV"], "comment": "Seedream 3.0 Technical Report", "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11347", "pdf": "https://arxiv.org/pdf/2504.11347", "abs": "https://arxiv.org/abs/2504.11347", "authors": ["Soyoung Yoo", "Namwoo Kang"], "title": "DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation", "categories": ["cs.CV", "physics.app-ph", "68T07"], "comment": "28 pages, 18 figures. Not yet submitted to a journal or conference", "summary": "Data-driven design is emerging as a powerful strategy to accelerate\nengineering innovation. However, its application to vehicle wheel design\nremains limited due to the lack of large-scale, high-quality datasets that\ninclude 3D geometry and physical performance metrics. To address this gap, this\nstudy proposes a synthetic design-performance dataset generation framework\nusing generative AI. The proposed framework first generates 2D rendered images\nusing Stable Diffusion, and then reconstructs the 3D geometry through 2.5D\ndepth estimation. Structural simulations are subsequently performed to extract\nengineering performance data. To further expand the design and performance\nspace, topology optimization is applied, enabling the generation of a more\ndiverse set of wheel designs. The final dataset, named DeepWheel, consists of\nover 6,000 photo-realistic images and 900 structurally analyzed 3D models. This\nmulti-modal dataset serves as a valuable resource for surrogate model training,\ndata-driven inverse design, and design space exploration. The proposed\nmethodology is also applicable to other complex design domains. The dataset is\nreleased under the Creative Commons Attribution-NonCommercial 4.0\nInternational(CC BY-NC 4.0) and is available on the\nhttps://www.smartdesignlab.org/datasets", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11368", "pdf": "https://arxiv.org/pdf/2504.11368", "abs": "https://arxiv.org/abs/2504.11368", "authors": ["Jingkun Chen", "Haoran Duan", "Xiao Zhang", "Boyan Gao", "Tao Tan", "Vicente Grau", "Jungong Han"], "title": "From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation", "categories": ["cs.CV", "68T45", "I.2.10; I.4.8"], "comment": "10 pages, 5 figures", "summary": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11415", "pdf": "https://arxiv.org/pdf/2504.11415", "abs": "https://arxiv.org/abs/2504.11415", "authors": ["Nikolette Pedersen", "Regitze Sydendal", "Andreas Wulff", "Ralf Raumanns", "Eike Petersen", "Veronika Cheplygina"], "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages (excluding appendix), 2 figures (excluding appendix),\n  submitted to MIUA 2025 conference (response pending)", "summary": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11441", "pdf": "https://arxiv.org/pdf/2504.11441", "abs": "https://arxiv.org/abs/2504.11441", "authors": ["Elizabeth Fons", "Rachneet Kaur", "Zhen Zeng", "Soham Palande", "Tucker Balch", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "TADACap: Time-series Adaptive Domain-Aware Captioning", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICAIF 2024", "summary": "While image captioning has gained significant attention, the potential of\ncaptioning time-series images, prevalent in areas like finance and healthcare,\nremains largely untapped. Existing time-series captioning methods typically\noffer generic, domain-agnostic descriptions of time-series shapes and struggle\nto adapt to new domains without substantial retraining. To address these\nlimitations, we introduce TADACap, a retrieval-based framework to generate\ndomain-aware captions for time-series images, capable of adapting to new\ndomains without retraining. Building on TADACap, we propose a novel retrieval\nstrategy that retrieves diverse image-caption pairs from a target domain\ndatabase, namely TADACap-diverse. We benchmarked TADACap-diverse against\nstate-of-the-art methods and ablation variants. TADACap-diverse demonstrates\ncomparable semantic accuracy while requiring significantly less annotation\neffort.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2410.10291", "pdf": "https://arxiv.org/pdf/2410.10291", "abs": "https://arxiv.org/abs/2410.10291", "authors": ["Xiangru Zhu", "Penglei Sun", "Yaoxian Song", "Yanghua Xiao", "Zhixu Li", "Chengyu Wang", "Jun Huang", "Bei Yang", "Xiaoxiao Xu"], "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by ICLR 2025", "summary": "Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10857", "pdf": "https://arxiv.org/pdf/2504.10857", "abs": "https://arxiv.org/abs/2504.10857", "authors": ["Shun Iwase", "Zubair Irshad", "Katherine Liu", "Vitor Guizilini", "Robert Lee", "Takuya Ikeda", "Ayako Amma", "Koichi Nishiwaki", "Kris Kitani", "Rares Ambrus", "Sergey Zakharov"], "title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping", "categories": ["cs.RO", "cs.CV"], "comment": "Published at CVPR 2025, Webpage: https://sh8.io/#/zerograsp", "summary": "Robotic grasping is a cornerstone capability of embodied systems. Many\nmethods directly output grasps from partial information without modeling the\ngeometry of the scene, leading to suboptimal motion and even collisions. To\naddress these issues, we introduce ZeroGrasp, a novel framework that\nsimultaneously performs 3D reconstruction and grasp pose prediction in near\nreal-time. A key insight of our method is that occlusion reasoning and modeling\nthe spatial relationships between objects is beneficial for both accurate\nreconstruction and grasping. We couple our method with a novel large-scale\nsynthetic dataset, which comprises 1M photo-realistic images, high-resolution\n3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K\nobjects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the\nGraspNet-1B benchmark as well as through real-world robot experiments.\nZeroGrasp achieves state-of-the-art performance and generalizes to novel\nreal-world objects by leveraging synthetic data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11438", "pdf": "https://arxiv.org/pdf/2504.11438", "abs": "https://arxiv.org/abs/2504.11438", "authors": ["Lewis Clifton", "Xin Tian", "Duangdao Palasuwan", "Phandee Watanaboonyongcharoen", "Ponlapat Rojnuckarin", "Nantheera Anantrasirichai"], "title": "Mamba-Based Ensemble learning for White Blood Cell Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "White blood cell (WBC) classification assists in assessing immune health and\ndiagnosing various diseases, yet manual classification is labor-intensive and\nprone to inconsistencies. Recent advancements in deep learning have shown\npromise over traditional methods; however, challenges such as data imbalance\nand the computational demands of modern technologies, such as Transformer-based\nmodels which do not scale well with input size, limit their practical\napplication. This paper introduces a novel framework that leverages Mamba\nmodels integrated with ensemble learning to improve WBC classification. Mamba\nmodels, known for their linear complexity, provide a scalable alternative to\nTransformer-based approaches, making them suitable for deployment in\nresource-constrained environments. Additionally, we introduce a new WBC\ndataset, Chula-WBC-8, for benchmarking. Our approach not only validates the\neffectiveness of Mamba models in this domain but also demonstrates their\npotential to significantly enhance classification efficiency without\ncompromising accuracy. The source code can be found at\nhttps://github.com/LewisClifton/Mamba-WBC-Classification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10647", "pdf": "https://arxiv.org/pdf/2504.10647", "abs": "https://arxiv.org/abs/2504.10647", "authors": ["Nafis Sadeq", "Xin Xu", "Zhouhang Xie", "Julian McAuley", "Byungkyu Kang", "Prarit Lamba", "Xiang Gao"], "title": "Improving In-Context Learning with Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Language models rely on semantic priors to perform in-context learning, which\nleads to poor performance on tasks involving inductive reasoning.\nInstruction-tuning methods based on imitation learning can superficially\nenhance the in-context learning performance of language models, but they often\nfail to improve the model's understanding of the underlying rules that connect\ninputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning\ndistillation technique designed to improve the inductive reasoning capabilities\nof language models. Through a careful combination of data augmentation,\nfiltering, supervised fine-tuning, and alignment, ReDis achieves significant\nperformance improvements across a diverse range of tasks, including 1D-ARC,\nList Function, ACRE, and MiniSCAN. Experiments on three language model\nbackbones show that ReDis outperforms equivalent few-shot prompting baselines\nacross all tasks and even surpasses the teacher model, GPT-4o, in some cases.\nReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%,\n2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within\na similar hypothesis search space. The code, dataset, and model checkpoints\nwill be made available at\nhttps://github.com/NafisSadeq/reasoning-distillation.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10567", "pdf": "https://arxiv.org/pdf/2504.10567", "abs": "https://arxiv.org/abs/2504.10567", "authors": ["Yushu Wu", "Yanyu Li", "Ivan Skorokhodov", "Anil Kag", "Willi Menapace", "Sharath Girish", "Aliaksandr Siarohin", "Yanzhi Wang", "Sergey Tulyakov"], "title": "H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "8 pages, 4 figures, 6 tables", "summary": "Autoencoder (AE) is the key to the success of latent diffusion models for\nimage and video generation, reducing the denoising resolution and improving\nefficiency. However, the power of AE has long been underexplored in terms of\nnetwork design, compression ratio, and training strategy. In this work, we\nsystematically examine the architecture design choices and optimize the\ncomputation distribution to obtain a series of efficient and high-compression\nvideo AEs that can decode in real time on mobile devices. We also unify the\ndesign of plain Autoencoder and image-conditioned I2V VAE, achieving\nmultifunctionality in a single network. In addition, we find that the widely\nadopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no\nsignificant improvements when training AEs at scale. We propose a novel latent\nconsistency loss that does not require complicated discriminator design or\nhyperparameter tuning, but provides stable improvements in reconstruction\nquality. Our AE achieves an ultra-high compression ratio and real-time decoding\nspeed on mobile while outperforming prior art in terms of reconstruction\nmetrics by a large margin. We finally validate our AE by training a DiT on its\nlatent space and demonstrate fast, high-quality text-to-video generation\ncapability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10681", "pdf": "https://arxiv.org/pdf/2504.10681", "abs": "https://arxiv.org/abs/2504.10681", "authors": ["Soham Shah", "Kumar Shridhar", "Surojit Chatterjee", "Souvik Sen"], "title": "EMAFusion: A Self-Optimizing System for Seamless LLM Selection and Integration", "categories": ["cs.CL"], "comment": null, "summary": "While recent advances in large language models (LLMs) have significantly\nenhanced performance across diverse natural language tasks, the high\ncomputational and financial costs associated with their deployment remain\nsubstantial barriers. Existing routing strategies partially alleviate this\nchallenge by assigning queries to cheaper or specialized models, but they\nfrequently rely on extensive labeled data or fragile task-specific heuristics.\nConversely, fusion techniques aggregate multiple LLM outputs to boost accuracy\nand robustness, yet they often exacerbate cost and may reinforce shared biases.\n  We introduce EMAFusion, a new framework that self-optimizes for seamless LLM\nselection and reliable execution for a given query. Specifically, EMAFusion\nintegrates a taxonomy-based router for familiar query types, a learned router\nfor ambiguous inputs, and a cascading approach that progressively escalates\nfrom cheaper to more expensive models based on multi-judge confidence\nevaluations. Through extensive evaluations, we find EMAFusion outperforms the\nbest individual models by over 2.6 percentage points (94.3% vs. 91.7%), while\nbeing 4X cheaper than the average cost. EMAFusion further achieves a remarkable\n17.1 percentage point improvement over models like GPT-4 at less than 1/20th\nthe cost. Our combined routing approach delivers 94.3% accuracy compared to\ntaxonomy-based (88.1%) and learned model predictor-based (91.7%) methods alone,\ndemonstrating the effectiveness of our unified strategy. Finally, EMAFusion\nsupports flexible cost-accuracy trade-offs, allowing users to balance their\nbudgetary constraints and performance needs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10635", "pdf": "https://arxiv.org/pdf/2504.10635", "abs": "https://arxiv.org/abs/2504.10635", "authors": ["Chunzhuo Wang", "Zhewen Xue", "T. Sunil Kumar", "Guido Camps", "Hans Hallez", "Bart Vanrumste"], "title": "Skeleton-Based Intake Gesture Detection With Spatial-Temporal Graph Convolutional Networks", "categories": ["cs.CV"], "comment": "The manuscript has been accepted in 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (IEEE EMBC\n  2025)", "summary": "Overweight and obesity have emerged as widespread societal challenges,\nfrequently linked to unhealthy eating patterns. A promising approach to enhance\ndietary monitoring in everyday life involves automated detection of food intake\ngestures. This study introduces a skeleton based approach using a model that\ncombines a dilated spatial-temporal graph convolutional network (ST-GCN) with a\nbidirectional long-short-term memory (BiLSTM) framework, as called\nST-GCN-BiLSTM, to detect intake gestures. The skeleton-based method provides\nkey benefits, including environmental robustness, reduced data dependency, and\nenhanced privacy preservation. Two datasets were employed for model validation.\nThe OREBA dataset, which consists of laboratory-recorded videos, achieved\nsegmental F1-scores of 86.18% and 74.84% for identifying eating and drinking\ngestures. Additionally, a self-collected dataset using smartphone recordings in\nmore adaptable experimental conditions was evaluated with the model trained on\nOREBA, yielding F1-scores of 85.40% and 67.80% for detecting eating and\ndrinking gestures. The results not only confirm the feasibility of utilizing\nskeleton data for intake gesture detection but also highlight the robustness of\nthe proposed approach in cross-dataset validation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10724", "pdf": "https://arxiv.org/pdf/2504.10724", "abs": "https://arxiv.org/abs/2504.10724", "authors": ["Avinash Kumar", "Shashank Nag", "Jason Clemons", "Lizy John", "Poulami Das"], "title": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Deploying large language models (LLMs) presents critical challenges due to\nthe inherent trade-offs associated with key performance metrics, such as\nlatency, accuracy, and throughput. Typically, gains in one metric is\naccompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently\nnavigate this trade-off space by skipping some of the later model layers when\nit confidently finds an output token early, thus reducing latency without\nimpacting accuracy. However, as the early exits taken depend on the task and\nare unknown apriori to request processing, EE-LLMs conservatively load the\nentire model, limiting resource savings and throughput. Also, current\nframeworks statically select a model for a user task, limiting our ability to\nadapt to changing nature of the input queries.\n  We propose HELIOS to address these challenges. First, HELIOS shortlists a set\nof candidate LLMs, evaluates them using a subset of prompts, gathering\ntelemetry data in real-time. Second, HELIOS uses the early exit data from these\nevaluations to greedily load the selected model only up to a limited number of\nlayers. This approach yields memory savings which enables us to process more\nrequests at the same time, thereby improving throughput. Third, HELIOS monitors\nand periodically reassesses the performance of the candidate LLMs and if\nneeded, switches to another model that can service incoming queries more\nefficiently (such as using fewer layers without lowering accuracy). Our\nevaluations show that HELIOS achieves 1.48$\\times$ throughput, 1.10$\\times$\nenergy-efficiency, 1.39$\\times$ lower response time, and 3.7$\\times$\nimprovements in inference batch sizes compared to the baseline, when optimizing\nfor the respective service level objectives.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10768", "pdf": "https://arxiv.org/pdf/2504.10768", "abs": "https://arxiv.org/abs/2504.10768", "authors": ["Ralf Schmälzle", "Sue Lim", "Yuetong Du", "Gary Bente"], "title": "The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "This paper examines the thin-slicing approach - the ability to make accurate\njudgments based on minimal information - in the context of scientific\npresentations. Drawing on research from nonverbal communication and personality\npsychology, we show that brief excerpts (thin slices) reliably predict overall\npresentation quality. Using a novel corpus of over one hundred real-life\nscience talks, we employ Large Language Models (LLMs) to evaluate transcripts\nof full presentations and their thin slices. By correlating LLM-based\nevaluations of short excerpts with full-talk assessments, we determine how much\ninformation is needed for accurate predictions. Our results demonstrate that\nLLM-based evaluations align closely with human ratings, proving their validity,\nreliability, and efficiency. Critically, even very short excerpts (less than 10\npercent of a talk) strongly predict overall evaluations. This suggests that the\nfirst moments of a presentation convey relevant information that is used in\nquality evaluations and can shape lasting impressions. The findings are robust\nacross different LLMs and prompting strategies. This work extends thin-slicing\nresearch to public speaking and connects theories of impression formation to\nLLMs and current research on AI communication. We discuss implications for\ncommunication and social cognition research on message reception. Lastly, we\nsuggest an LLM-based thin-slicing framework as a scalable feedback tool to\nenhance human communication.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10797", "pdf": "https://arxiv.org/pdf/2504.10797", "abs": "https://arxiv.org/abs/2504.10797", "authors": ["Annabella Sakunkoo", "Jonathan Sakunkoo"], "title": "Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies", "categories": ["cs.CL", "cs.AI", "cs.HC", "H.5; J.4"], "comment": null, "summary": "Across cultures, names tell a lot about their bearers as they carry deep\npersonal and cultural significance. Names also serve as powerful signals of\ngender, race, and status in the social hierarchy - a pecking order in which\nindividual positions shape others' expectations on their perceived competence\nand worth. With the widespread adoption of LLMs and as names are often an input\nfor LLMs, it is crucial to evaluate whether LLMs may sort people into status\npositions based on first and last names and, if so, whether it is in an unfair,\nbiased fashion. While prior work has primarily investigated biases in first\nnames, little attention has been paid to last names and even less to the\ncombined effects of first and last names. In this study, we conduct a\nlarge-scale analysis of name variations across 5 ethnicities to examine how AI\nexhibits name biases. Our study investigates three key characteristics of\ninequality and finds that LLMs reflect and reinforce status hierarchies based\non names that signal gender and ethnicity as they encode differential\nexpectations of competence, leadership, and economic potential. Contrary to the\ncommon assumption that AI tends to favor Whites, we show that East and, in some\ncontexts, South Asian names receive higher rankings. We also disaggregate\nAsians, a population projected to be the largest immigrant group in the U.S. by\n2055. Our results challenge the monolithic Asian model minority assumption,\nillustrating a more complex and stratified model of bias. Gender moderates\nbiases, with girls facing unfair disadvantages in certain racial groups.\nAdditionally, spanning cultural categories by adopting Western first names\nimproves AI-perceived status for East and Southeast Asian students,\nparticularly for girls. Our findings underscore the importance of\nintersectional and more nuanced understandings of race, gender, and mixed\nidentities in the evaluation of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10686", "pdf": "https://arxiv.org/pdf/2504.10686", "abs": "https://arxiv.org/abs/2504.10686", "authors": ["Bin Ren", "Hang Guo", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yawei Li", "Yao Zhang", "Xinning Chai", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Li Song", "Hongyuan Yu", "Pufan Xu", "Cheng Wan", "Zhijuan Huang", "Peng Guo", "Shuyuan Cui", "Chenjun Li", "Xuehai Hu", "Pan Pan", "Xin Zhang", "Heng Zhang", "Qing Luo", "Linyan Jiang", "Haibo Lei", "Qifang Gao", "Yaqing Li", "Weihua Luo", "Tsing Li", "Qing Wang", "Yi Liu", "Yang Wang", "Hongyu An", "Liou Zhang", "Shijie Zhao", "Lianhong Song", "Long Sun", "Jinshan Pan", "Jiangxin Dong", "Jinhui Tang", "Jing Wei", "Mengyang Wang", "Ruilong Guo", "Qian Wang", "Qingliang Liu", "Yang Cheng", "Davinci", "Enxuan Gu", "Pinxin Liu", "Yongsheng Yu", "Hang Hua", "Yunlong Tang", "Shihao Wang", "Yukun Yang", "Zhiyu Zhang", "Yukun Yang", "Jiyu Wu", "Jiancheng Huang", "Yifan Liu", "Yi Huang", "Shifeng Chen", "Rui Chen", "Yi Feng", "Mingxi Li", "Cailu Wan", "Xiangji Wu", "Zibin Liu", "Jinyang Zhong", "Kihwan Yoon", "Ganzorig Gankhuyag", "Shengyun Zhong", "Mingyang Wu", "Renjie Li", "Yushen Zuo", "Zhengzhong Tu", "Zongang Gao", "Guannan Chen", "Yuan Tian", "Wenhui Chen", "Weijun Yuan", "Zhan Li", "Yihang Chen", "Yifan Deng", "Ruting Deng", "Yilin Zhang", "Huan Zheng", "Yanyan Wei", "Wenxuan Zhao", "Suiyi Zhao", "Fei Wang", "Kun Li", "Yinggan Tang", "Mengjie Su", "Jae-hyeon Lee", "Dong-Hyeop Son", "Ui-Jin Choi", "Tiancheng Shao", "Yuqing Zhang", "Mengcheng Ma", "Donggeun Ko", "Youngsang Kwak", "Jiun Lee", "Jaehwa Kwak", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jing Hu", "Hui Deng", "Xuan Zhang", "Lin Zhu", "Qinrui Fan", "Weijian Deng", "Junnan Wu", "Wenqin Deng", "Yuquan Liu", "Zhaohong Xu", "Jameer Babu Pinjari", "Kuldeep Purohit", "Zeyu Xiao", "Zhuoyuan Li", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Sachin Chaudhary", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Wei-Chen Shen", "I-Hsiang Chen", "Yunzhe Xu", "Chen Zhao", "Zhizhou Chen", "Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Marcos V. Conde", "Simone Bianco", "Luca Cogo", "Gianmarco Corti"], "title": "The Tenth NTIRE 2025 Efficient Super-Resolution Challenge Report", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025 NTIRE Workshop, Efficient Super-Resolution\n  Challenge Report. 50 pages", "summary": "This paper presents a comprehensive review of the NTIRE 2025 Challenge on\nSingle-Image Efficient Super-Resolution (ESR). The challenge aimed to advance\nthe development of deep models that optimize key computational metrics, i.e.,\nruntime, parameters, and FLOPs, while achieving a PSNR of at least 26.90 dB on\nthe $\\operatorname{DIV2K\\_LSDIR\\_valid}$ dataset and 26.99 dB on the\n$\\operatorname{DIV2K\\_LSDIR\\_test}$ dataset. A robust participation saw\n\\textbf{244} registered entrants, with \\textbf{43} teams submitting valid\nentries. This report meticulously analyzes these methods and results,\nemphasizing groundbreaking advancements in state-of-the-art single-image ESR\ntechniques. The analysis highlights innovative approaches and establishes\nbenchmarks for future research in the field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10727", "pdf": "https://arxiv.org/pdf/2504.10727", "abs": "https://arxiv.org/abs/2504.10727", "authors": ["Darryl Hannan", "John Cooper", "Dylan White", "Timothy Doster", "Henry Kvinge", "Yijing Watkins"], "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization", "categories": ["cs.CV"], "comment": "26 pages, CVPR MORSE Workshop 2025", "summary": "Multimodal large language models (MLLMs) have altered the landscape of\ncomputer vision, obtaining impressive results across a wide range of tasks,\nespecially in zero-shot settings. Unfortunately, their strong performance does\nnot always transfer to out-of-distribution domains, such as earth observation\n(EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks,\nsuch as image captioning and scene understanding, while failing at tasks that\nrequire more fine-grained spatial reasoning, such as object localization.\nHowever, MLLMs are advancing rapidly and insights quickly become out-dated. In\nthis work, we analyze more recent MLLMs that have been explicitly trained to\ninclude fine-grained spatial reasoning capabilities, benchmarking them on EO\nobject localization tasks. We demonstrate that these models are performant in\ncertain settings, making them well suited for zero-shot scenarios.\nAdditionally, we provide a detailed discussion focused on prompt selection,\nground sample distance (GSD) optimization, and analyzing failure cases. We hope\nthat this work will prove valuable as others evaluate whether an MLLM is well\nsuited for a given EO localization task and how to optimize it.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10982", "pdf": "https://arxiv.org/pdf/2504.10982", "abs": "https://arxiv.org/abs/2504.10982", "authors": ["Yingjian Chen", "Feiyang Li", "Xingyu Song", "Tianxiao Li", "Issey Sudeka", "Irene Li"], "title": "Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Large language models (LLMs) perform well in medical QA, but their\neffectiveness in Japanese contexts is limited due to privacy constraints that\nprevent the use of commercial models like GPT-4 in clinical settings. As a\nresult, recent efforts focus on instruction-tuning open-source LLMs, though the\npotential of combining them with retrieval-augmented generation (RAG) remains\nunderexplored. To bridge this gap, we are the first to explore a knowledge\ngraph-based (KG) RAG framework for Japanese medical QA small-scale open-source\nLLMs. Experimental results show that KG-based RAG has only a limited impact on\nJapanese medical QA using small-scale open-source LLMs. Further case studies\nreveal that the effectiveness of the RAG is sensitive to the quality and\nrelevance of the external retrieved content. These findings offer valuable\ninsights into the challenges and potential of applying RAG in Japanese medical\nQA, while also serving as a reference for other low-resource languages.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10746", "pdf": "https://arxiv.org/pdf/2504.10746", "abs": "https://arxiv.org/abs/2504.10746", "authors": ["Xiulong Liu", "Anurag Kumar", "Paul Calamia", "Sebastia V. Amengual", "Calvin Murdock", "Ishwarya Ananthabhotla", "Philip Robinson", "Eli Shlizerman", "Vamsi Krishna Ithapu", "Ruohan Gao"], "title": "Hearing Anywhere in Any Environment", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "In mixed reality applications, a realistic acoustic experience in spatial\nenvironments is as crucial as the visual experience for achieving true\nimmersion. Despite recent advances in neural approaches for Room Impulse\nResponse (RIR) estimation, most existing methods are limited to the single\nenvironment on which they are trained, lacking the ability to generalize to new\nrooms with different geometries and surface materials. We aim to develop a\nunified model capable of reconstructing the spatial acoustic experience of any\nenvironment with minimum additional measurements. To this end, we present xRIR,\na framework for cross-room RIR prediction. The core of our generalizable\napproach lies in combining a geometric feature extractor, which captures\nspatial context from panorama depth images, with a RIR encoder that extracts\ndetailed acoustic features from only a few reference RIR samples. To evaluate\nour method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity\nsimulation of over 300,000 RIRs from 260 rooms. Experiments show that our\nmethod strongly outperforms a series of baselines. Furthermore, we successfully\nperform sim-to-real transfer by evaluating our model on four real-world\nenvironments, demonstrating the generalizability of our approach and the\nrealism of our dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11001", "pdf": "https://arxiv.org/pdf/2504.11001", "abs": "https://arxiv.org/abs/2504.11001", "authors": ["Alan Dao", "Thinh Le"], "title": "ReZero: Enhancing LLM search ability by trying one-more-time", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10764", "pdf": "https://arxiv.org/pdf/2504.10764", "abs": "https://arxiv.org/abs/2504.10764", "authors": ["Jostan Brown", "Cindy Grimm", "Joseph R. Davidson"], "title": "SeeTree -- A modular, open-source system for tree detection and orchard localization", "categories": ["cs.CV", "cs.RO"], "comment": "26 pages, 12 figures", "summary": "Accurate localization is an important functional requirement for precision\norchard management. However, there are few off-the-shelf commercial solutions\navailable to growers. In this paper, we present SeeTree, a modular, open source\nembedded system for tree trunk detection and orchard localization that is\ndeployable on any vehicle. Building on our prior work on vision-based in-row\nlocalization using particle filters, SeeTree includes several new capabilities.\nFirst, it provides capacity for full orchard localization including out-of-row\nheadland turning. Second, it includes the flexibility to integrate either\nvisual, GNSS, or wheel odometry in the motion model. During field experiments\nin a commercial orchard, the system converged to the correct location 99% of\nthe time over 800 trials, even when starting with large uncertainty in the\ninitial particle locations. When turning out of row, the system correctly\ntracked 99% of the turns (860 trials representing 43 unique row changes). To\nhelp support adoption and future research and development, we make our dataset,\ndesign files, and source code freely available to the community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11082", "pdf": "https://arxiv.org/pdf/2504.11082", "abs": "https://arxiv.org/abs/2504.11082", "authors": ["Efthymios Georgiou", "Vassilis Katsouros", "Yannis Avrithis", "Alexandros Potamianos"], "title": "DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "While multimodal fusion has been extensively studied in Multimodal Sentiment\nAnalysis (MSA), the role of fusion depth and multimodal capacity allocation\nremains underexplored. In this work, we position fusion depth, scalability, and\ndedicated multimodal capacity as primary factors for effective fusion. We\nintroduce DeepMLF, a novel multimodal language model (LM) with learnable tokens\ntailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a\npretrained decoder LM augmented with multimodal information across its layers.\nWe append learnable tokens to the LM that: 1) capture modality interactions in\na controlled fashion and 2) preserve independent information flow for each\nmodality. These fusion tokens gather linguistic information via causal\nself-attention in LM Blocks and integrate with audiovisual information through\ncross-attention MM Blocks. Serving as dedicated multimodal capacity, this\ndesign enables progressive fusion across multiple layers, providing depth in\nthe fusion process. Our training recipe combines modality-specific losses and\nlanguage modelling loss, with the decoder LM tasked to predict ground truth\npolarity. Across three MSA benchmarks with varying dataset characteristics,\nDeepMLF achieves state-of-the-art performance. Our results confirm that deeper\nfusion leads to better performance, with optimal fusion depths (5-7) exceeding\nthose of existing approaches. Additionally, our analysis on the number of\nfusion tokens reveals that small token sets ($\\sim$20) achieve optimal\nperformance. We examine the importance of representation learning order (fusion\ncurriculum) through audiovisual encoder initialization experiments. Our\nablation studies demonstrate the superiority of the proposed fusion design and\ngating while providing a holistic examination of DeepMLF's scalability to LLMs,\nand the impact of each training objective and embedding regularization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10765", "pdf": "https://arxiv.org/pdf/2504.10765", "abs": "https://arxiv.org/abs/2504.10765", "authors": ["Jeremy Klotz", "Shree K. Nayar"], "title": "Minimal Sensing for Orienting a Solar Panel", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "A solar panel harvests the most energy when pointing in the direction that\nmaximizes the total illumination (irradiance) falling on it. Given an arbitrary\norientation of a panel and an arbitrary environmental illumination, we address\nthe problem of finding the direction of maximum total irradiance. We develop a\nminimal sensing approach where measurements from just four photodetectors are\nused to iteratively vary the tilt of the panel to maximize the irradiance. Many\nenvironments produce irradiance functions with multiple local maxima. As a\nresult, simply measuring the gradient of the irradiance function and applying\ngradient ascent will not work. We show that a larger, optimized tilt between\nthe detectors and the panel is equivalent to blurring the irradiance function.\nThis has the effect of eliminating local maxima and turning the irradiance\nfunction into a unimodal one, whose maximum can be found using gradient ascent.\nWe show that there is a close relationship between our approach and scale space\ntheory. We have collected a large dataset of high-dynamic range lighting\nenvironments in New York City, called \\textit{UrbanSky}. We used this dataset\nto conduct simulations to verify the robustness of our approach. Finally, we\nhave built a portable solar panel with four compact detectors and an actuator\nto conduct experiments in various real-world settings: direct sunlight, cloudy\nsky, urban settings with occlusions and shadows, and complex indoor lighting.\nIn all cases, we show significant improvements in harvested energy compared to\nstandard approaches for controlling the orientation of a solar panel.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11183", "pdf": "https://arxiv.org/pdf/2504.11183", "abs": "https://arxiv.org/abs/2504.11183", "authors": ["Ej Zhou", "Weiming Lu"], "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting", "categories": ["cs.CL"], "comment": null, "summary": "Social bias in language models can potentially exacerbate social\ninequalities. Despite it having garnered wide attention, most research focuses\non English data. In a low-resource scenario, the models often perform worse due\nto insufficient training data. This study aims to leverage high-resource\nlanguage corpora to evaluate bias and experiment with debiasing methods in\nlow-resource languages. We evaluated the performance of recent multilingual\nmodels in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}),\nRussian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and\nanalyzed four bias dimensions: \\textit{gender}, \\textit{religion},\n\\textit{nationality}, and \\textit{race-color}. By constructing multilingual\nbias evaluation datasets, this study allows fair comparisons between models\nacross languages. We have further investigated three debiasing\nmethods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that\ndebiasing methods from high-resource languages can be effectively transferred\nto low-resource ones, providing actionable insights for fairness research in\nmultilingual NLP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10809", "pdf": "https://arxiv.org/pdf/2504.10809", "abs": "https://arxiv.org/abs/2504.10809", "authors": ["Christophe Bolduc", "Yannick Hold-Geoffroy", "Zhixin Shu", "Jean-François Lalonde"], "title": "GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR", "categories": ["cs.CV"], "comment": null, "summary": "We present GaSLight, a method that generates spatially-varying lighting from\nregular images. Our method proposes using HDR Gaussian Splats as light source\nrepresentation, marking the first time regular images can serve as light\nsources in a 3D renderer. Our two-stage process first enhances the dynamic\nrange of images plausibly and accurately by leveraging the priors embedded in\ndiffusion models. Next, we employ Gaussian Splats to model 3D lighting,\nachieving spatially variant lighting. Our approach yields state-of-the-art\nresults on HDR estimations and their applications in illuminating virtual\nobjects and scenes. To facilitate the benchmarking of images as light sources,\nwe introduce a novel dataset of calibrated and unsaturated HDR to evaluate\nimages as light sources. We assess our method using a combination of this novel\ndataset and an existing dataset from the literature. The code to reproduce our\nmethod will be available upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10825", "pdf": "https://arxiv.org/pdf/2504.10825", "abs": "https://arxiv.org/abs/2504.10825", "authors": ["Dianbing Xi", "Jiepeng Wang", "Yuanzhi Liang", "Xi Qiu", "Yuchi Huo", "Rui Wang", "Chi Zhang", "Xuelong Li"], "title": "OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding", "categories": ["cs.CV"], "comment": "Our project page: https://tele-ai.github.io/OmniVDiff/", "summary": "In this paper, we propose a novel framework for controllable video diffusion,\nOmniVDiff, aiming to synthesize and comprehend multiple video visual content in\na single diffusion model. To achieve this, OmniVDiff treats all video visual\nmodalities in the color space to learn a joint distribution, while employing an\nadaptive control strategy that dynamically adjusts the role of each visual\nmodality during the diffusion process, either as a generation modality or a\nconditioning modality. This allows flexible manipulation of each modality's\nrole, enabling support for a wide range of tasks. Consequently, our model\nsupports three key functionalities: (1) Text-conditioned video generation:\nmulti-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are\ngenerated based on the text conditions in one diffusion process; (2) Video\nunderstanding: OmniVDiff can estimate the depth, canny map, and semantic\nsegmentation across the input rgb frames while ensuring coherence with the rgb\ninput; and (3) X-conditioned video generation: OmniVDiff generates videos\nconditioned on fine-grained attributes (e.g., depth maps or segmentation maps).\nBy integrating these diverse tasks into a unified video diffusion framework,\nOmniVDiff enhances the flexibility and scalability for controllable video\ndiffusion, making it an effective tool for a variety of downstream\napplications, such as video-to-video translation. Extensive experiments\ndemonstrate the effectiveness of our approach, highlighting its potential for\nvarious video-related applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10852", "pdf": "https://arxiv.org/pdf/2504.10852", "abs": "https://arxiv.org/abs/2504.10852", "authors": ["Pengxiao Han", "Changkun Ye", "Jinguang Tong", "Cuicui Jiang", "Jie Hong", "Li Fang", "Xuesong Li"], "title": "Enhancing Features in Long-tailed Data Using Large Vision Mode", "categories": ["cs.CV"], "comment": null, "summary": "Language-based foundation models, such as large language models (LLMs) or\nlarge vision-language models (LVLMs), have been widely studied in long-tailed\nrecognition. However, the need for linguistic data is not applicable to all\npractical tasks. In this study, we aim to explore using large vision models\n(LVMs) or visual foundation models (VFMs) to enhance long-tailed data features\nwithout any language information. Specifically, we extract features from the\nLVM and fuse them with features in the baseline network's map and latent space\nto obtain the augmented features. Moreover, we design several prototype-based\nlosses in the latent space to further exploit the potential of the augmented\nfeatures. In the experimental section, we validate our approach on two\nbenchmark datasets: ImageNet-LT and iNaturalist2018.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11426", "pdf": "https://arxiv.org/pdf/2504.11426", "abs": "https://arxiv.org/abs/2504.11426", "authors": ["Xue Zhang", "Songming Zhang", "Yunlong Liang", "Fandong Meng", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328", "summary": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11431", "pdf": "https://arxiv.org/pdf/2504.11431", "abs": "https://arxiv.org/abs/2504.11431", "authors": ["Maria Teleki", "Xiangjue Dong", "Haoran Liu", "James Caverlee"], "title": "Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": "To appear in ICWSM 2025", "summary": "Masculine defaults are widely recognized as a significant type of gender\nbias, but they are often unseen as they are under-researched. Masculine\ndefaults involve three key parts: (i) the cultural context, (ii) the masculine\ncharacteristics or behaviors, and (iii) the reward for, or simply acceptance\nof, those masculine characteristics or behaviors. In this work, we study\ndiscourse-based masculine defaults, and propose a twofold framework for (i) the\nlarge-scale discovery and analysis of gendered discourse words in spoken\ncontent via our Gendered Discourse Correlation Framework (GDCF); and (ii) the\nmeasurement of the gender bias associated with these gendered discourse words\nin LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus\nour study on podcasts, a popular and growing form of social media, analyzing\n15,117 podcast episodes. We analyze correlations between gender and discourse\nwords -- discovered via LDA and BERTopic -- to automatically form gendered\ndiscourse word lists. We then study the prevalence of these gendered discourse\nwords in domain-specific contexts, and find that gendered discourse-based\nmasculine defaults exist in the domains of business, technology/politics, and\nvideo games. Next, we study the representation of these gendered discourse\nwords from a state-of-the-art LLM embedding model from OpenAI, and find that\nthe masculine discourse words have a more stable and robust representation than\nthe feminine discourse words, which may result in better system performance on\ndownstream tasks for men. Hence, men are rewarded for their discourse patterns\nwith better system performance by one of the state-of-the-art language models\n-- and this embedding disparity is a representational harm and a masculine\ndefault.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11442", "pdf": "https://arxiv.org/pdf/2504.11442", "abs": "https://arxiv.org/abs/2504.11442", "authors": ["Leon Guertler", "Bobby Cheng", "Simon Yu", "Bo Liu", "Leshem Choshen", "Cheston Tan"], "title": "TextArena", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "work in progress; 5 pages, 3 figures", "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10490", "pdf": "https://arxiv.org/pdf/2504.10490", "abs": "https://arxiv.org/abs/2504.10490", "authors": ["Gabriel Bo", "Marc Bernardino", "Justin Gu"], "title": "GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA", "categories": ["cs.LG", "cs.CL"], "comment": "10 pages, 11 figures. This submission cites arXiv:2404.19756.\n  Supplementary materials and additional information are available at\n  arXiv:2404.19756", "summary": "We explore the potential of integrating learnable and interpretable\nmodules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based\nrepresentations--within a pre-trained GPT-2 model to enhance multi-task\nlearning accuracy. Motivated by the recent surge in using KAN and graph\nattention (GAT) architectures in chain-of-thought (CoT) models and debates over\ntheir benefits compared to simpler architectures like MLPs, we begin by\nenhancing a standard self-attention transformer using Low-Rank Adaptation\n(LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This\napproach yields significant improvements. To further boost interpretability and\nricher representations, we develop two variants that attempt to improve the\nstandard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However,\nsystematic evaluations reveal that neither variant outperforms the optimized\nLoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set,\n99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On\nsonnet generation, we get a CHRF score of 42.097. These findings highlight that\nefficient parameter adaptation via LoRA remains the most effective strategy for\nour tasks: sentiment analysis, paraphrase detection, and sonnet generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10883", "pdf": "https://arxiv.org/pdf/2504.10883", "abs": "https://arxiv.org/abs/2504.10883", "authors": ["Karan Jain", "Mohammad Nayeem Teli"], "title": "Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have recently gained state of the art performance on many\nimage generation tasks. However, most models require significant computational\nresources to achieve this. This becomes apparent in the application of medical\nimage synthesis due to the 3D nature of medical datasets like CT-scans, MRIs,\nelectron microscope, etc. In this paper we propose a novel architecture for a\nsingle GPU memory-efficient training for diffusion models for high dimensional\nmedical datasets. The proposed model is built by using an invertible UNet\narchitecture with invertible attention modules. This leads to the following two\ncontributions: 1. denoising diffusion models and thus enabling memory usage to\nbe independent of the dimensionality of the dataset, and 2. reducing the energy\nusage during training. While this new model can be applied to a multitude of\nimage generation tasks, we showcase its memory-efficiency on the 3D BraTS2020\ndataset leading to up to 15\\% decrease in peak memory consumption during\ntraining with comparable results to SOTA while maintaining the image quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10519", "pdf": "https://arxiv.org/pdf/2504.10519", "abs": "https://arxiv.org/abs/2504.10519", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "title": "Toward Super Agent System with Hybrid AI Routers", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10766", "pdf": "https://arxiv.org/pdf/2504.10766", "abs": "https://arxiv.org/abs/2504.10766", "authors": ["Ming Li", "Yanhong Li", "Ziyue Li", "Tianyi Zhou"], "title": "How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As the post-training of large language models (LLMs) advances from\ninstruction-following to complex reasoning tasks, understanding how different\ndata affect finetuning dynamics remains largely unexplored. In this paper, we\npresent a spectral analysis of layer-wise gradients induced by low/high-quality\ninstruction and reasoning data for LLM post-training. Our analysis reveals that\nwidely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and\nReward, can be explained and unified by spectral properties computed from\ngradients' singular value decomposition (SVD). Specifically, higher-quality\ndata are usually associated with lower nuclear norms and higher effective\nranks. Notably, effective rank exhibits better robustness and resolution than\nnuclear norm in capturing subtle quality differences. For example, reasoning\ndata achieves substantially higher effective ranks than instruction data,\nimplying richer gradient structures on more complex tasks. Our experiments also\nhighlight that models within the same family share similar gradient patterns\nregardless of their sizes, whereas different model families diverge\nsignificantly. Providing a unified view on the effects of data quality across\ninstruction and reasoning data, this work illuminates the interplay between\ndata quality and training stability, shedding novel insights into developing\nbetter data exploration strategies for post-training.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11190", "pdf": "https://arxiv.org/pdf/2504.11190", "abs": "https://arxiv.org/abs/2504.11190", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "Enhancing multimodal analogical reasoning with Logic Augmented Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models have demonstrated their capabilities\nacross a variety of tasks. However, automatically extracting implicit knowledge\nfrom natural language remains a significant challenge, as machines lack active\nexperience with the physical world. Given this scenario, semantic knowledge\ngraphs can serve as conceptual spaces that guide the automated text generation\nreasoning process to achieve more efficient and explainable results. In this\npaper, we apply a logic-augmented generation (LAG) framework that leverages the\nexplicit representation of a text through a semantic knowledge graph and\napplies it in combination with prompt heuristics to elicit implicit analogical\nconnections. This method generates extended knowledge graph triples\nrepresenting implicit meaning, enabling systems to reason on unlabeled\nmultimodal data regardless of the domain. We validate our work through three\nmetaphor detection and understanding tasks across four datasets, as they\nrequire deep analogical reasoning capabilities. The results show that this\nintegrated approach surpasses current baselines, performs better than humans in\nunderstanding visual metaphors, and enables more explainable reasoning\nprocesses, though still has inherent limitations in metaphor understanding,\nespecially for domain-specific metaphors. Furthermore, we propose a thorough\nerror analysis, discussing issues with metaphorical annotations and current\nevaluation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11243", "pdf": "https://arxiv.org/pdf/2504.11243", "abs": "https://arxiv.org/abs/2504.11243", "authors": ["Balahari Vignesh Balu", "Florian Geissler", "Francesco Carella", "Joao-Vitor Zacchi", "Josef Jiru", "Nuria Mata", "Reinhard Stolle"], "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "We study the automated derivation of safety requirements in a self-driving\nvehicle use case, leveraging LLMs in combination with agent-based\nretrieval-augmented generation. Conventional approaches that utilise\npre-trained LLMs to assist in safety analyses typically lack domain-specific\nknowledge. Existing RAG approaches address this issue, yet their performance\ndeteriorates when handling complex queries and it becomes increasingly harder\nto retrieve the most relevant information. This is particularly relevant for\nsafety-relevant applications. In this paper, we propose the use of agent-based\nRAG to derive safety requirements and show that the retrieved information is\nmore relevant to the queries. We implement an agent-based approach on a\ndocument pool of automotive standards and the Apollo case study, as a\nrepresentative example of an automated driving perception system. Our solution\nis tested on a data set of safety requirement questions and answers, extracted\nfrom the Apollo data. Evaluating a set of selected RAG metrics, we present and\ndiscuss advantages of a agent-based approach compared to default RAG methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11008", "pdf": "https://arxiv.org/pdf/2504.11008", "abs": "https://arxiv.org/abs/2504.11008", "authors": ["Qinyue Tong", "Ziqian Lu", "Jun Liu", "Yangming Zheng", "Zheming Lu"], "title": "MediSee: Reasoning-based Pixel-level Perception in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Despite remarkable advancements in pixel-level medical image perception,\nexisting methods are either limited to specific tasks or heavily rely on\naccurate bounding boxes or text labels as input prompts. However, the medical\nknowledge required for input is a huge obstacle for general public, which\ngreatly reduces the universality of these methods. Compared with these\ndomain-specialized auxiliary information, general users tend to rely on oral\nqueries that require logical reasoning. In this paper, we introduce a novel\nmedical vision task: Medical Reasoning Segmentation and Detection (MedSD),\nwhich aims to comprehend implicit queries about medical images and generate the\ncorresponding segmentation mask and bounding box for the target object. To\naccomplish this task, we first introduce a Multi-perspective, Logic-driven\nMedical Reasoning Segmentation and Detection (MLMR-SD) dataset, which\nencompasses a substantial collection of medical entity targets along with their\ncorresponding reasoning. Furthermore, we propose MediSee, an effective baseline\nmodel designed for medical reasoning segmentation and detection. The\nexperimental results indicate that the proposed method can effectively address\nMedSD with implicit colloquial queries and outperform traditional medical\nreferring segmentation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11364", "pdf": "https://arxiv.org/pdf/2504.11364", "abs": "https://arxiv.org/abs/2504.11364", "authors": ["Tianwei Ni", "Allen Nie", "Sapana Chaudhary", "Yao Liu", "Huzefa Rangwala", "Rasool Fakoor"], "title": "Teaching Large Language Models to Reason through Learning and Forgetting", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Leveraging inference-time search in large language models has proven\neffective in further enhancing a trained model's capability to solve complex\nmathematical and reasoning problems. However, this approach significantly\nincreases computational costs and inference time, as the model must generate\nand evaluate multiple candidate solutions to identify a viable reasoning path.\nTo address this, we propose an effective approach that integrates search\ncapabilities directly into the model by fine-tuning it using both successful\n(learning) and failed reasoning paths (forgetting) derived from diverse search\nmethods. While fine-tuning the model with these data might seem\nstraightforward, we identify a critical issue: the model's search capability\ntends to degrade rapidly if fine-tuning is performed naively. We show that this\ndegradation can be substantially mitigated by employing a smaller learning\nrate. Extensive experiments on the challenging Game-of-24 and Countdown\nmathematical reasoning benchmarks show that our approach not only outperforms\nboth standard fine-tuning and inference-time search baselines but also\nsignificantly reduces inference time by 180$\\times$.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11367", "pdf": "https://arxiv.org/pdf/2504.11367", "abs": "https://arxiv.org/abs/2504.11367", "authors": ["Rui Tang", "Ziyun Yong", "Shuyu Jiang", "Xingshu Chen", "Yaofang Liu", "Yi-Cheng Zhang", "Gui-Quan Sun", "Wei Wang"], "title": "Network Alignment", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "Complex networks are frequently employed to model physical or virtual complex\nsystems. When certain entities exist across multiple systems simultaneously,\nunveiling their corresponding relationships across the networks becomes\ncrucial. This problem, known as network alignment, holds significant\nimportance. It enhances our understanding of complex system structures and\nbehaviours, facilitates the validation and extension of theoretical physics\nresearch about studying complex systems, and fosters diverse practical\napplications across various fields. However, due to variations in the\nstructure, characteristics, and properties of complex networks across different\nfields, the study of network alignment is often isolated within each domain,\nwith even the terminologies and concepts lacking uniformity. This review\ncomprehensively summarizes the latest advancements in network alignment\nresearch, focusing on analyzing network alignment characteristics and progress\nin various domains such as social network analysis, bioinformatics,\ncomputational linguistics and privacy protection. It provides a detailed\nanalysis of various methods' implementation principles, processes, and\nperformance differences, including structure consistency-based methods, network\nembedding-based methods, and graph neural network-based (GNN-based) methods.\nAdditionally, the methods for network alignment under different conditions,\nsuch as in attributed networks, heterogeneous networks, directed networks, and\ndynamic networks, are presented. Furthermore, the challenges and the open\nissues for future studies are also discussed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11024", "pdf": "https://arxiv.org/pdf/2504.11024", "abs": "https://arxiv.org/abs/2504.11024", "authors": ["Andrea Simonelli", "Norman Müller", "Peter Kontschieder"], "title": "Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing availability of digital 3D environments, whether through\nimage-based 3D reconstruction, generation, or scans obtained by robots, is\ndriving innovation across various applications. These come with a significant\ndemand for 3D interaction, such as 3D Interactive Segmentation, which is useful\nfor tasks like object selection and manipulation. Additionally, there is a\npersistent need for solutions that are efficient, precise, and performing well\nacross diverse settings, particularly in unseen environments and with\nunfamiliar objects. In this work, we introduce a 3D interactive segmentation\nmethod that consistently surpasses previous state-of-the-art techniques on both\nin-domain and out-of-domain datasets. Our simple approach integrates a\nvoxel-based sparse encoder with a lightweight transformer-based decoder that\nimplements implicit click fusion, achieving superior performance and maximizing\nefficiency. Our method demonstrates substantial improvements on benchmark\ndatasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on\nunseen geometric distributions such as the ones obtained by Gaussian Splatting.\nThe project web-page is available at https://simonelli-andrea.github.io/easy3d.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11038", "pdf": "https://arxiv.org/pdf/2504.11038", "abs": "https://arxiv.org/abs/2504.11038", "authors": ["Yudong Zhang", "Ruobing Xie", "Jiansheng Chen", "Xingwu Sun", "Zhanhui Kang", "Yu Wang"], "title": "QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by NAACL 2025 main", "summary": "In typical multimodal tasks, such as Visual Question Answering (VQA),\nadversarial attacks targeting a specific image and question can lead large\nvision-language models (LVLMs) to provide incorrect answers. However, it is\ncommon for a single image to be associated with multiple questions, and LVLMs\nmay still answer other questions correctly even for an adversarial image\nattacked by a specific question. To address this, we introduce the\nquery-agnostic visual attack (QAVA), which aims to create robust adversarial\nexamples that generate incorrect responses to unspecified and unknown\nquestions. Compared to traditional adversarial attacks focused on specific\nimages and questions, QAVA significantly enhances the effectiveness and\nefficiency of attacks on images when the question is unknown, achieving\nperformance comparable to attacks on known target questions. Our research\nbroadens the scope of visual adversarial attacks on LVLMs in practical\nsettings, uncovering previously overlooked vulnerabilities, particularly in the\ncontext of visual adversarial threats. The code is available at\nhttps://github.com/btzyd/qava.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11055", "pdf": "https://arxiv.org/pdf/2504.11055", "abs": "https://arxiv.org/abs/2504.11055", "authors": ["Alireza Salehi", "Mohammadreza Salehi", "Reshad Hosseini", "Cees G. M. Snoek", "Makoto Yamada", "Mohammad Sabokrou"], "title": "Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly Detection (AD) involves identifying deviations from normal data\ndistributions and is critical in fields such as medical diagnostics and\nindustrial defect detection. Traditional AD methods typically require the\navailability of normal training samples; however, this assumption is not always\nfeasible, as collecting such data can be impractical. Additionally, these\nmethods often struggle to generalize across different domains. Recent\nadvancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot\ngeneralization capabilities of CLIP but still face a performance gap between\nimage-level and pixel-level anomaly detection. To address this gap, we propose\na novel approach that conditions the prompts of the text encoder based on image\ncontext extracted from the vision encoder. Also, to capture fine-grained\nvariations more effectively, we have modified the CLIP vision encoder and\naltered the extraction of dense features. These changes ensure that the\nfeatures retain richer spatial and structural information for both normal and\nanomalous prompts. Our method achieves state-of-the-art performance, improving\nperformance by 2% to 29% across different metrics on 14 datasets. This\ndemonstrates its effectiveness in both image-level and pixel-level anomaly\ndetection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11063", "pdf": "https://arxiv.org/pdf/2504.11063", "abs": "https://arxiv.org/abs/2504.11063", "authors": ["Pedro Diaz-Garcia", "Felix Escalona", "Miguel Cazorla"], "title": "UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques", "categories": ["cs.CV"], "comment": null, "summary": "The purpose of this paper is to explore the use of underwater image\nenhancement techniques to improve keypoint detection and matching. By applying\nadvanced deep learning models, including generative adversarial networks and\nconvolutional neural networks, we aim to find the best method which improves\nthe accuracy of keypoint detection and the robustness of matching algorithms.\nWe evaluate the performance of these techniques on various underwater datasets,\ndemonstrating significant improvements over traditional methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11066", "pdf": "https://arxiv.org/pdf/2504.11066", "abs": "https://arxiv.org/abs/2504.11066", "authors": ["Marco Micheletto", "Giulia Orrù", "Luca Ghiani", "Gian Luca Marcialis"], "title": "Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Presentation Attack Detection (PAD) systems are usually designed\nindependently of the fingerprint verification system. While this can be\nacceptable for use cases where specific user templates are not predetermined,\nit represents a missed opportunity to enhance security in scenarios where\nintegrating PAD with the fingerprint verification system could significantly\nleverage users' templates, which are the real target of a potential\npresentation attack. This does not mean that a PAD should be specifically\ndesigned for such users; that would imply the availability of many enrolled\nusers' PAI and, consequently, complexity, time, and cost increase. On the\ncontrary, we propose to equip a basic PAD, designed according to the state of\nthe art, with an innovative add-on module called the Closeness Binary Code (CC)\nmodule. The term \"closeness\" refers to a peculiar property of the bona\nfide-related features: in an Euclidean feature space, genuine fingerprints tend\nto cluster in a specific pattern. First, samples from the same finger are close\nto each other, then samples from other fingers of the same user and finally,\nsamples from fingers of other users. This property is statistically verified in\nour previous publication, and further confirmed in this paper. It is\nindependent of the user population and the feature set class, which can be\nhandcrafted or deep network-based (embeddings). Therefore, the add-on can be\ndesigned without the need for the targeted user samples; moreover, it exploits\nher/his samples' \"closeness\" property during the verification stage. Extensive\nexperiments on benchmark datasets and state-of-the-art PAD methods confirm the\nbenefits of the proposed add-on, which can be easily coupled with the main PAD\nmodule integrated into the fingerprint verification system.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11080", "pdf": "https://arxiv.org/pdf/2504.11080", "abs": "https://arxiv.org/abs/2504.11080", "authors": ["Elman Ghazaei", "Erchan Aptoula"], "title": "Change State Space Models for Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite their frequent use for change detection, both ConvNets and Vision\ntransformers (ViT) exhibit well-known limitations, namely the former struggle\nto model long-range dependencies while the latter are computationally\ninefficient, rendering them challenging to train on large-scale datasets.\nVision Mamba, an architecture based on State Space Models has emerged as an\nalternative addressing the aforementioned deficiencies and has been already\napplied to remote sensing change detection, though mostly as a feature\nextracting backbone. In this article the Change State Space Model is\nintroduced, that has been specifically designed for change detection by\nfocusing on the relevant changes between bi-temporal images, effectively\nfiltering out irrelevant information. By concentrating solely on the changed\nfeatures, the number of network parameters is reduced, enhancing significantly\ncomputational efficiency while maintaining high detection performance and\nrobustness against input degradation. The proposed model has been evaluated via\nthree benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based\ncounterparts at a fraction of their computational complexity. The\nimplementation will be made available at https://github.com/Elman295/CSSM upon\nacceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11134", "pdf": "https://arxiv.org/pdf/2504.11134", "abs": "https://arxiv.org/abs/2504.11134", "authors": ["Gustav Hanning", "Gabrielle Flood", "Viktor Larsson"], "title": "Visual Re-Ranking with Non-Visual Side Information", "categories": ["cs.CV"], "comment": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025", "summary": "The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11143", "pdf": "https://arxiv.org/pdf/2504.11143", "abs": "https://arxiv.org/abs/2504.11143", "authors": ["Xiang Wang", "Shiwei Zhang", "Hangjie Yuan", "Yujie Wei", "Yingya Zhang", "Changxin Gao", "Yuehuan Wang", "Nong Sang"], "title": "Taming Consistency Distillation for Accelerated Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in human image animation have been propelled by video\ndiffusion models, yet their reliance on numerous iterative denoising steps\nresults in high inference costs and slow speeds. An intuitive solution involves\nadopting consistency models, which serve as an effective acceleration paradigm\nthrough consistency distillation. However, simply employing this strategy in\nhuman image animation often leads to quality decline, including visual\nblurring, motion degradation, and facial distortion, particularly in dynamic\nregions. In this paper, we propose the DanceLCM approach complemented by\nseveral enhancements to improve visual quality and motion continuity at\nlow-step regime: (1) segmented consistency distillation with an auxiliary\nlight-weight head to incorporate supervision from real video latents,\nmitigating cumulative errors resulting from single full-trajectory generation;\n(2) a motion-focused loss to centre on motion regions, and explicit injection\nof facial fidelity features to improve face authenticity. Extensive qualitative\nand quantitative experiments demonstrate that DanceLCM achieves results\ncomparable to state-of-the-art video diffusion models with a mere 2-4 inference\nsteps, significantly reducing the inference burden without compromising video\nquality. The code and models will be made publicly available.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11150", "pdf": "https://arxiv.org/pdf/2504.11150", "abs": "https://arxiv.org/abs/2504.11150", "authors": ["Mahir Gulzar", "Yar Muhammad", "Naveed Muhammad"], "title": "GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Predicting future trajectories of surrounding vehicles heavily relies on what\ncontextual information is given to a motion prediction model. The context\nitself can be static (lanes, regulatory elements, etc) or dynamic (traffic\nparticipants). This paper presents a lane graph-based motion prediction model\nthat first predicts graph-based goal proposals and later fuses them with cross\nattention over multiple contextual elements. We follow the famous\nencoder-interactor-decoder architecture where the encoder encodes scene context\nusing lightweight Gated Recurrent Units, the interactor applies cross-context\nattention over encoded scene features and graph goal proposals, and the decoder\nregresses multimodal trajectories via Laplacian Mixture Density Network from\nthe aggregated encodings. Using cross-attention over graph-based goal proposals\ngives robust trajectory estimates since the model learns to attend to future\ngoal-relevant scene elements for the intended agent. We evaluate our work on\nnuScenes motion prediction dataset, achieving state-of-the-art results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11199", "pdf": "https://arxiv.org/pdf/2504.11199", "abs": "https://arxiv.org/abs/2504.11199", "authors": ["Min Jung Lee", "Dayoung Gong", "Minsu Cho"], "title": "Video Summarization with Large Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The exponential increase in video content poses significant challenges in\nterms of efficient navigation, search, and retrieval, thus requiring advanced\nvideo summarization techniques. Existing video summarization methods, which\nheavily rely on visual features and temporal dynamics, often fail to capture\nthe semantics of video content, resulting in incomplete or incoherent\nsummaries. To tackle the challenge, we propose a new video summarization\nframework that leverages the capabilities of recent Large Language Models\n(LLMs), expecting that the knowledge learned from massive data enables LLMs to\nevaluate video frames in a manner that better aligns with diverse semantics and\nhuman judgments, effectively addressing the inherent subjectivity in defining\nkeyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates\nvideo frames into a sequence of captions using a Muti-modal Large Language\nModel (M-LLM) and then assesses the importance of each frame using an LLM,\nbased on the captions in its local context. These local importance scores are\nrefined through a global attention mechanism in the entire context of video\ncaptions, ensuring that our summaries effectively reflect both the details and\nthe overarching narrative. Our experimental results demonstrate the superiority\nof the proposed method over existing ones in standard benchmarks, highlighting\nthe potential of LLMs in the processing of multimedia content.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11232", "pdf": "https://arxiv.org/pdf/2504.11232", "abs": "https://arxiv.org/abs/2504.11232", "authors": ["Elisa Ancarani", "Julie Tores", "Lucile Sassatelli", "Rémy Sun", "Hui-Yin Wu", "Frédéric Precioso"], "title": "Leveraging multimodal explanatory annotations for video interpretation with Modality Specific Dataset", "categories": ["cs.CV", "cs.MM"], "comment": "6 pages, 8 Figures", "summary": "We examine the impact of concept-informed supervision on multimodal video\ninterpretation models using MOByGaze, a dataset containing human-annotated\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\nwhich consist of data subsets categorized by the modality (visual, textual, or\naudio) of annotated concepts. Models trained on CMSDs outperform those using\ntraditional legacy training in both early and late fusion approaches. Notably,\nthis approach enables late fusion models to achieve performance close to that\nof early fusion models. These findings underscore the importance of\nmodality-specific annotations in developing robust, self-explainable video\nmodels and contribute to advancing interpretable multimodal learning in complex\nvideo analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11268", "pdf": "https://arxiv.org/pdf/2504.11268", "abs": "https://arxiv.org/abs/2504.11268", "authors": ["Juan Garcia Giraldo", "Nikolaos Dimitriadis", "Ke Wang", "Pascal Frossard"], "title": "Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "Model merging is a flexible and computationally tractable approach to merge\nsingle-task checkpoints into a multi-task model. Prior work has solely focused\non constrained multi-task settings where there is a one-to-one mapping between\na sample and a task, overlooking the paradigm where multiple tasks may operate\non the same sample, e.g., scene understanding. In this paper, we focus on the\nmulti-task setting with single-input-multiple-outputs (SIMO) and show that it\nqualitatively differs from the single-input-single-output model merging\nsettings studied in the literature due to the existence of task-specific\ndecoders and diverse loss objectives. We identify that existing model merging\nmethods lead to significant performance degradation, primarily due to\nrepresentation misalignment between the merged encoder and task-specific\ndecoders. We propose two simple and efficient fixes for the SIMO setting to\nre-align the feature representation after merging. Compared to joint\nfine-tuning, our approach is computationally effective and flexible, and sheds\nlight into identifying task relationships in an offline manner. Experiments on\nNYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task\narithmetic suffices to enable multi-task capabilities; however, the\nrepresentations generated by the merged encoder has to be re-aligned with the\ntask-specific heads; (2) the proposed architecture rivals traditional\nmulti-task learning in performance but requires fewer samples and training\nsteps by leveraging the existence of task-specific models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11271", "pdf": "https://arxiv.org/pdf/2504.11271", "abs": "https://arxiv.org/abs/2504.11271", "authors": ["Xinning Chai", "Yao Zhang", "Yuxuan Zhang", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Li Song"], "title": "Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) have been widely used in efficient image\nsuper-resolution. However, for CNN-based methods, performance gains often\nrequire deeper networks and larger feature maps, which increase complexity and\ninference costs. Inspired by LoRA's success in fine-tuning large language\nmodels, we explore its application to lightweight models and propose\nDistillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which\nimproves model performance without increasing architectural complexity or\ninference costs. Specifically, we integrate ConvLoRA into the efficient SR\nnetwork SPAN by replacing the SPAB module with the proposed SConvLB module and\nincorporating ConvLoRA layers into both the pixel shuffle block and its\npreceding convolutional layer. DSCLoRA leverages low-rank decomposition for\nparameter updates and employs a spatial feature affinity-based knowledge\ndistillation strategy to transfer second-order statistical information from\nteacher models (pre-trained SPAN) to student models (ours). This method\npreserves the core knowledge of lightweight models and facilitates optimal\nsolution discovery under certain conditions. Experiments on benchmark datasets\nshow that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its\nefficiency and competitive image quality. Notably, DSCLoRA ranked first in the\nOverall Performance Track of the NTIRE 2025 Efficient Super-Resolution\nChallenge. Our code and models are made publicly available at\nhttps://github.com/Yaozzz666/DSCF-SR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11307", "pdf": "https://arxiv.org/pdf/2504.11307", "abs": "https://arxiv.org/abs/2504.11307", "authors": ["Sonia Laguna", "Lin Zhang", "Can Deniz Bezek", "Monika Farkas", "Dieter Schweizer", "Rahel A. Kubik-Huch", "Orcun Goksel"], "title": "Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks", "categories": ["cs.CV"], "comment": "Published at the International Journal of Computer Assisted Radiology\n  and Surgery. Presented at the 16th International Conference on Information\n  Processing in Computer-Assisted Interventions 2025", "summary": "Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its\nimaging can provide a promising biomarker for diagnosis. Reconstructing SoS\nimages from ultrasound acquisitions can be cast as a limited-angle\ncomputed-tomography problem, with Variational Networks being a promising\nmodel-based deep learning solution. Some acquired data frames may, however, get\ncorrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows,\nwhich in turn negatively affects the resulting SoS reconstructions. We propose\nto use the uncertainty in SoS reconstructions to attribute trust to each\nindividual acquired frame. Given multiple acquisitions, we then use an\nuncertainty based automatic selection among these retrospectively, to improve\ndiagnostic decisions. We investigate uncertainty estimation based on Monte\nCarlo Dropout and Bayesian Variational Inference. We assess our automatic frame\nselection method for differential diagnosis of breast cancer, distinguishing\nbetween benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions\nclassified as BI-RADS~4, which represents suspicious cases for probable\nmalignancy. The most trustworthy frame among four acquisitions of each lesion\nwas identified using uncertainty based criteria. Selecting a frame informed by\nuncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout\nand Bayesian Variational Inference, respectively, superior to any\nuncertainty-uninformed baselines with the best one achieving 64%. A novel use\nof uncertainty estimation is proposed for selecting one of multiple data\nacquisitions for further processing and decision making.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11326", "pdf": "https://arxiv.org/pdf/2504.11326", "abs": "https://arxiv.org/abs/2504.11326", "authors": ["Henghui Ding", "Chang Liu", "Nikhila Ravi", "Shuting He", "Yunchao Wei", "Song Bai", "Philip Torr", "Kehuan Song", "Xinglin Xie", "Kexin Zhang", "Licheng Jiao", "Lingling Li", "Shuyuan Yang", "Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma", "Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Che", "Wei Zhan", "Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu", "Haobo Yuan", "Xiangtai Li", "Tao Zhang", "Lu Qi", "Ming-Hsuan Yang"], "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild", "categories": ["cs.CV"], "comment": "Workshop Page: https://pvuw.github.io/. arXiv admin note: text\n  overlap with arXiv:2504.00476, arXiv:2504.05178", "summary": "This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11379", "pdf": "https://arxiv.org/pdf/2504.11379", "abs": "https://arxiv.org/abs/2504.11379", "authors": ["Liu Yang", "Huiyu Duan", "Yucheng Zhu", "Xiaohong Liu", "Lu Liu", "Zitong Xu", "Guangji Ma", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "title": "Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model", "categories": ["cs.CV"], "comment": "10 pages", "summary": "$360^{\\circ}$ omnidirectional images (ODIs) have gained considerable\nattention recently, and are widely used in various virtual reality (VR) and\naugmented reality (AR) applications. However, capturing such images is\nexpensive and requires specialized equipment, making ODI synthesis increasingly\nimportant. While common 2D image generation and editing methods are rapidly\nadvancing, these models struggle to deliver satisfactory results when\ngenerating or editing ODIs due to the unique format and broad 360$^{\\circ}$\nField-of-View (FoV) of ODIs. To bridge this gap, we construct\n\\textbf{\\textit{Any2Omni}}, the first comprehensive ODI generation-editing\ndataset comprises 60,000+ training data covering diverse input conditions and\nup to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an\n\\textbf{\\underline{Omni}} model for \\textbf{\\underline{Omni}}-directional image\ngeneration and editing (\\textbf{\\textit{Omni$^2$}}), with the capability of\nhandling various ODI generation and editing tasks under diverse input\nconditions using one model. Extensive experiments demonstrate the superiority\nand effectiveness of the proposed Omni$^2$ model for both the ODI generation\nand editing tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11416", "pdf": "https://arxiv.org/pdf/2504.11416", "abs": "https://arxiv.org/abs/2504.11416", "authors": ["Panagiotis Agrafiotis", "Begüm Demir"], "title": "Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "summary": "Accurate, detailed, and high-frequent bathymetry is crucial for shallow\nseabed areas facing intense climatological and anthropogenic pressures. Current\nmethods utilizing airborne or satellite optical imagery to derive bathymetry\nprimarily rely on either SfM-MVS with refraction correction or Spectrally\nDerived Bathymetry (SDB). However, SDB methods often require extensive manual\nfieldwork or costly reference data, while SfM-MVS approaches face challenges\neven after refraction correction. These include depth data gaps and noise in\nenvironments with homogeneous visual textures, which hinder the creation of\naccurate and complete Digital Surface Models (DSMs) of the seabed. To address\nthese challenges, this work introduces a methodology that combines the\nhigh-fidelity 3D reconstruction capabilities of the SfM-MVS methods with\nstate-of-the-art refraction correction techniques, along with the spectral\nanalysis capabilities of a new deep learning-based method for bathymetry\nprediction. This integration enables a synergistic approach where SfM-MVS\nderived DSMs with data gaps are used as training data to generate complete\nbathymetric maps. In this context, we propose Swin-BathyUNet that combines\nU-Net with Swin Transformer self-attention layers and a cross-attention\nmechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve\nbathymetric accuracy by capturing long-range spatial relationships and can also\nfunction as a standalone solution for standard SDB with various training depth\ndata, independent of the SfM-MVS output. Experimental results in two completely\ndifferent test sites in the Mediterranean and Baltic Seas demonstrate the\neffectiveness of the proposed approach through extensive experiments that\ndemonstrate improvements in bathymetric accuracy, detail, coverage, and noise\nreduction in the predicted DSM. The code is available at\nhttps://github.com/pagraf/Swin-BathyUNet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11418", "pdf": "https://arxiv.org/pdf/2504.11418", "abs": "https://arxiv.org/abs/2504.11418", "authors": ["Tibor Kubík", "Oldřich Kodym", "Petr Šilling", "Kateřina Trávníčková", "Tomáš Mojžiš", "Jan Matula"], "title": "Leveraging Point Transformers for Detecting Anatomical Landmarks in Digital Dentistry", "categories": ["cs.CV"], "comment": "10 pages + references, 3 figures, MICCAI2024 3DTeethland Challenge\n  submission", "summary": "The increasing availability of intraoral scanning devices has heightened\ntheir importance in modern clinical orthodontics. Clinicians utilize advanced\nComputer-Aided Design techniques to create patient-specific treatment plans\nthat include laboriously identifying crucial landmarks such as cusps,\nmesial-distal locations, facial axis points, and tooth-gingiva boundaries.\nDetecting such landmarks automatically presents challenges, including limited\ndataset sizes, significant anatomical variability among subjects, and the\ngeometric nature of the data. We present our experiments from the 3DTeethLand\nGrand Challenge at MICCAI 2024. Our method leverages recent advancements in\npoint cloud learning through transformer architectures. We designed a Point\nTransformer v3 inspired module to capture meaningful geometric and anatomical\nfeatures, which are processed by a lightweight decoder to predict per-point\ndistances, further processed by graph-based non-minima suppression. We report\npromising results and discuss insights on learned feature interpretability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11427", "pdf": "https://arxiv.org/pdf/2504.11427", "abs": "https://arxiv.org/abs/2504.11427", "authors": ["Yanrui Bin", "Wenbo Hu", "Haoyuan Wang", "Xinya Chen", "Bing Wang"], "title": "NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, Project Page: https://normalcrafter.github.io/", "summary": "Surface normal estimation serves as a cornerstone for a spectrum of computer\nvision applications. While numerous efforts have been devoted to static image\nscenarios, ensuring temporal coherence in video-based normal estimation remains\na formidable challenge. Instead of merely augmenting existing methods with\ntemporal components, we present NormalCrafter to leverage the inherent temporal\npriors of video diffusion models. To secure high-fidelity normal estimation\nacross sequences, we propose Semantic Feature Regularization (SFR), which\naligns diffusion features with semantic cues, encouraging the model to\nconcentrate on the intrinsic semantics of the scene. Moreover, we introduce a\ntwo-stage training protocol that leverages both latent and pixel space learning\nto preserve spatial accuracy while maintaining long temporal context. Extensive\nevaluations demonstrate the efficacy of our method, showcasing a superior\nperformance in generating temporally consistent normal sequences with intricate\ndetails from diverse videos.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11434", "pdf": "https://arxiv.org/pdf/2504.11434", "abs": "https://arxiv.org/abs/2504.11434", "authors": ["Yifan Ding", "Xixi Liu", "Jonas Unger", "Gabriel Eilertsen"], "title": "Enhancing Out-of-Distribution Detection with Extended Logit Normalization", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for the safe deployment of\nmachine learning models. Recent advances have explored improved classification\nlosses and representation learning strategies to enhance OOD detection.\nHowever, these methods are often tailored to specific post-hoc detection\ntechniques, limiting their generalizability. In this work, we identify a\ncritical issue in Logit Normalization (LogitNorm), which inhibits its\neffectiveness in improving certain post-hoc OOD detection methods. To address\nthis, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel\nhyperparameter-free formulation that significantly benefits a wide range of\npost-hoc detection methods. By incorporating feature distance-awareness to\nLogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and\nin-distribution (ID) confidence calibration than its predecessor. Extensive\nexperiments across standard benchmarks demonstrate that our approach\noutperforms state-of-the-art training-time methods in OOD detection while\nmaintaining strong ID classification accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11447", "pdf": "https://arxiv.org/pdf/2504.11447", "abs": "https://arxiv.org/abs/2504.11447", "authors": ["An Zhaol", "Shengyuan Zhang", "Ling Yang", "Zejian Li", "Jiale Wu", "Haoran Xu", "AnYang Wei", "Perry Pengyun GU Lingyun Sun"], "title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", "categories": ["cs.CV"], "comment": "Our code is public available on\n  https://github.com/happyw1nd/DistillationDPO", "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "policy optimization", "preference", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11451", "pdf": "https://arxiv.org/pdf/2504.11451", "abs": "https://arxiv.org/abs/2504.11451", "authors": ["Minghua Liu", "Mikaela Angelina Uy", "Donglai Xiang", "Hao Su", "Sanja Fidler", "Nicholas Sharp", "Jun Gao"], "title": "PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond", "categories": ["cs.CV"], "comment": "https://research.nvidia.com/labs/toronto-ai/partfield-release/", "summary": "We propose PartField, a feedforward approach for learning part-based 3D\nfeatures, which captures the general concept of parts and their hierarchy\nwithout relying on predefined templates or text-based names, and can be applied\nto open-world 3D shapes across various modalities. PartField requires only a 3D\nfeedforward pass at inference time, significantly improving runtime and\nrobustness compared to prior approaches. Our model is trained by distilling 2D\nand 3D part proposals from a mix of labeled datasets and image segmentations on\nlarge unsupervised datasets, via a contrastive learning formulation. It\nproduces a continuous feature field which can be clustered to yield a\nhierarchical part decomposition. Comparisons show that PartField is up to 20%\nmore accurate and often orders of magnitude faster than other recent\nclass-agnostic part-segmentation methods. Beyond single-shape part\ndecomposition, consistency in the learned field emerges across shapes, enabling\ntasks such as co-segmentation and correspondence, which we demonstrate in\nseveral applications of these general-purpose, hierarchical, and consistent 3D\nfeature fields. Check our Webpage!\nhttps://research.nvidia.com/labs/toronto-ai/partfield-release/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11457", "pdf": "https://arxiv.org/pdf/2504.11457", "abs": "https://arxiv.org/abs/2504.11457", "authors": ["Ziqi Pang", "Xin Xu", "Yu-Xiong Wang"], "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10493", "pdf": "https://arxiv.org/pdf/2504.10493", "abs": "https://arxiv.org/abs/2504.10493", "authors": ["K. A. Muthukumar", "Dhruva Nandi", "Priya Ranjan", "Krithika Ramachandran", "Shiny PJ", "Anirban Ghosh", "Ashwini M", "Aiswaryah Radhakrishnan", "V. E. Dhandapani", "Rajiv Janardhanan"], "title": "Integrating electrocardiogram and fundus images for early detection of cardiovascular diseases", "categories": ["eess.IV", "cs.CV"], "comment": "EMD, Fundus image, CNN, CVD prediction", "summary": "Cardiovascular diseases (CVD) are a predominant health concern globally,\nemphasizing the need for advanced diagnostic techniques. In our research, we\npresent an avant-garde methodology that synergistically integrates ECG readings\nand retinal fundus images to facilitate the early disease tagging as well as\ntriaging of the CVDs in the order of disease priority. Recognizing the\nintricate vascular network of the retina as a reflection of the cardiovascular\nsystem, alongwith the dynamic cardiac insights from ECG, we sought to provide a\nholistic diagnostic perspective. Initially, a Fast Fourier Transform (FFT) was\napplied to both the ECG and fundus images, transforming the data into the\nfrequency domain. Subsequently, the Earth Mover's Distance (EMD) was computed\nfor the frequency-domain features of both modalities. These EMD values were\nthen concatenated, forming a comprehensive feature set that was fed into a\nNeural Network classifier. This approach, leveraging the FFT's spectral\ninsights and EMD's capability to capture nuanced data differences, offers a\nrobust representation for CVD classification. Preliminary tests yielded a\ncommendable accuracy of 84 percent, underscoring the potential of this combined\ndiagnostic strategy. As we continue our research, we anticipate refining and\nvalidating the model further to enhance its clinical applicability in resource\nlimited healthcare ecosystems prevalent across the Indian sub-continent and\nalso the world at large.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10526", "pdf": "https://arxiv.org/pdf/2504.10526", "abs": "https://arxiv.org/abs/2504.10526", "authors": ["Mingyang Zhu", "Yinting Liu", "Mingyu Li", "Jiacheng Wang"], "title": "PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current methods for pathology image segmentation typically treat 2D slices\nindependently, ignoring valuable cross-slice information. We present\nPathSeqSAM, a novel approach that treats 2D pathology slices as sequential\nvideo frames using SAM2's memory mechanisms. Our method introduces a\ndistance-aware attention mechanism that accounts for variable physical\ndistances between slices and employs LoRA for domain adaptation. Evaluated on\nthe KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM\ndemonstrates improved segmentation quality, particularly in challenging cases\nthat benefit from cross-slice context. We have publicly released our code at\nhttps://github.com/JackyyyWang/PathSeqSAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10584", "pdf": "https://arxiv.org/pdf/2504.10584", "abs": "https://arxiv.org/abs/2504.10584", "authors": ["Roni H. Goldshmid", "John O. Dabiri", "John E. Sader"], "title": "Visual anemometry of natural vegetation from their leaf motion", "categories": ["physics.flu-dyn", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "High-resolution, near-ground wind-speed data are critical for improving the\naccuracy of weather predictions and climate models,$^{1-3}$ supporting wildfire\ncontrol efforts,$^{4-7}$ and ensuring the safe passage of airplanes during\ntakeoff and landing maneouvers.$^{8,9}$ Quantitative wind speed anemometry\ngenerally employs on-site instrumentation for accurate single-position data or\nsophisticated remote techniques such as Doppler radar for quantitative field\nmeasurements. It is widely recognized that the wind-induced motion of\nvegetation depends in a complex manner on their structure and mechanical\nproperties, obviating their use in quantitative anemometry.$^{10-14}$ We\nanalyze measurements on a host of different vegetation showing that leaf motion\ncan be decoupled from the leaf's branch and support structure, at\nlow-to-moderate wind speed, $U_{wind}$. This wind speed range is characterized\nby a leaf Reynolds number, enabling the development of a remote, quantitative\nanemometry method based on the formula,\n$U_{wind}\\approx740\\sqrt{{\\mu}U_{leaf}/{\\rho}D}$, that relies only on the leaf\nsize $D$, its measured fluctuating (RMS) speed $U_{leaf}$, the air viscosity\n$\\mu$, and its mass density $\\rho$. This formula is corroborated by a\nfirst-principles model and validated using a host of laboratory and field tests\non diverse vegetation types, ranging from oak, olive, and magnolia trees\nthrough to camphor and bullgrass. The findings of this study open the door to a\nnew paradigm in anemometry, using natural vegetation to enable remote and rapid\nquantitative field measurements at global locations with minimal cost.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10833", "pdf": "https://arxiv.org/pdf/2504.10833", "abs": "https://arxiv.org/abs/2504.10833", "authors": ["Shubham Kumar", "Dwip Dalal", "Narendra Ahuja"], "title": "Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a\npromising tool for generating semantic explanations of the decision-making\nprocesses in deep neural networks, having applications in both model\nimprovement and understanding. It is vital that the explanation is accurate, or\nfaithful, to the model, yet we identify several limitations of prior\nfaithfulness metrics that inhibit an accurate evaluation; most notably, prior\nmetrics involve only the set of concepts present, ignoring how they may be\nspatially distributed. We address these limitations with Surrogate Faithfulness\n(SF), an evaluation method that introduces a spatially-aware surrogate and two\nnovel faithfulness metrics. Using SF, we produce Optimally Faithful (OF)\nexplanations, where concepts are found that maximize faithfulness. Our\nexperiments show that (1) adding spatial-awareness to prior U-CBEMs increases\nfaithfulness in all cases; (2) OF produces significantly more faithful\nexplanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's\nlearned concepts generalize well to out-of-domain data and are more robust to\nadversarial examples, where prior U-CBEMs struggle.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10916", "pdf": "https://arxiv.org/pdf/2504.10916", "abs": "https://arxiv.org/abs/2504.10916", "authors": ["Zhenyu Yang", "Haiming Zhu", "Rihui Zhang", "Haipeng Zhang", "Jianliang Wang", "Chunhao Wang", "Minbin Chen", "Fang-Fang Yin"], "title": "Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification", "categories": ["physics.med-ph", "cs.CV"], "comment": "27 pages, 3 figures", "summary": "Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11195", "pdf": "https://arxiv.org/pdf/2504.11195", "abs": "https://arxiv.org/abs/2504.11195", "authors": ["Lijun Sheng", "Jian Liang", "Zilei Wang", "Ran He"], "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "CVPR 2025", "summary": "Vision-language models (VLMs), such as CLIP, have gained significant\npopularity as foundation models, with numerous fine-tuning methods developed to\nenhance performance on downstream tasks. However, due to their inherent\nvulnerability and the common practice of selecting from a limited set of\nopen-source models, VLMs suffer from a higher risk of adversarial attacks than\ntraditional vision models. Existing defense techniques typically rely on\nadversarial fine-tuning during training, which requires labeled data and lacks\nof flexibility for downstream tasks. To address these limitations, we propose\nrobust test-time prompt tuning (R-TPT), which mitigates the impact of\nadversarial attacks during the inference stage. We first reformulate the\nclassic marginal entropy objective by eliminating the term that introduces\nconflicts under adversarial conditions, retaining only the pointwise entropy\nminimization. Furthermore, we introduce a plug-and-play reliability-based\nweighted ensembling strategy, which aggregates useful information from reliable\naugmented views to strengthen the defense. R-TPT enhances defense against\nadversarial attacks without requiring labeled training data while offering high\nflexibility for inference tasks. Extensive experiments on widely used\nbenchmarks with various attacks demonstrate the effectiveness of R-TPT. The\ncode is available in https://github.com/TomSheng21/R-TPT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11247", "pdf": "https://arxiv.org/pdf/2504.11247", "abs": "https://arxiv.org/abs/2504.11247", "authors": ["Fikrican Özgür", "René Zurbrügg", "Suryansh Kumar"], "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "10 pages, 9 figures, 6 tables", "summary": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11286", "pdf": "https://arxiv.org/pdf/2504.11286", "abs": "https://arxiv.org/abs/2504.11286", "authors": ["Pengcheng Zheng", "Kecheng Chen", "Jiaxin Huang", "Bohao Chen", "Ju Liu", "Yazhou Ren", "Xiaorong Pu"], "title": "Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image restoration tasks aim to recover high-quality images from\ndegraded observations, exhibiting emergent desires in many clinical scenarios,\nsuch as low-dose CT image denoising, MRI super-resolution, and MRI artifact\nremoval. Despite the success achieved by existing deep learning-based\nrestoration methods with sophisticated modules, they struggle with rendering\ncomputationally-efficient reconstruction results. Moreover, they usually ignore\nthe reliability of the restoration results, which is much more urgent in\nmedical systems. To alleviate these issues, we present LRformer, a Lightweight\nTransformer-based method via Reliability-guided learning in the frequency\ndomain. Specifically, inspired by the uncertainty quantification in Bayesian\nneural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer\n(RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling\noperations to generate sufficiently-reliable priors by performing multiple\ninferences on the foundational medical image segmentation model, MedSAM.\nAdditionally, instead of directly incorporating the priors in the spatial\ndomain, we decompose the cross-attention (CA) mechanism into real symmetric and\nimaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in\nthe design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging\nthe conjugated symmetric property of FFT, GFCA reduces the computational\ncomplexity of naive CA by nearly half. Extensive experimental results in\nvarious tasks demonstrate the superiority of the proposed LRformer in both\neffectiveness and efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
