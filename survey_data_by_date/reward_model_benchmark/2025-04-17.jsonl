{"id": "2504.11582", "pdf": "https://arxiv.org/pdf/2504.11582", "abs": "https://arxiv.org/abs/2504.11582", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "categories": ["cs.CL"], "comment": "38 pages, 7 figures", "summary": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11777", "pdf": "https://arxiv.org/pdf/2504.11777", "abs": "https://arxiv.org/abs/2504.11777", "authors": ["Yongpei Ma", "Pengyu Wang", "Adam Dunn", "Usman Naseem", "Jinman Kim"], "title": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets", "categories": ["cs.CV", "cs.LG"], "comment": "The first two listed authors contributed equally to this work", "summary": "Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "consistency", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11705", "pdf": "https://arxiv.org/pdf/2504.11705", "abs": "https://arxiv.org/abs/2504.11705", "authors": ["Adriano D'Alessandro", "Ali Mahdavi-Amiri", "Ghassan Hamarneh"], "title": "Learning What NOT to Count", "categories": ["cs.CV"], "comment": null, "summary": "Few/zero-shot object counting methods reduce the need for extensive\nannotations but often struggle to distinguish between fine-grained categories,\nespecially when multiple similar objects appear in the same scene. To address\nthis limitation, we propose an annotation-free approach that enables the\nseamless integration of new fine-grained categories into existing few/zero-shot\ncounting models. By leveraging latent generative models, we synthesize\nhigh-quality, category-specific crowded scenes, providing a rich training\nsource for adapting to new categories without manual labeling. Our approach\nintroduces an attention prediction network that identifies fine-grained\ncategory boundaries trained using only synthetic pseudo-annotated data. At\ninference, these fine-grained attention estimates refine the output of existing\nfew/zero-shot counting networks. To benchmark our method, we further introduce\nthe FGTC dataset, a taxonomy-specific fine-grained object counting dataset for\nnatural images. Our method substantially enhances pre-trained state-of-the-art\nmodels on fine-grained taxon counting tasks, while using only synthetic data.\nCode and data to be released upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "fine-grained"], "score": 4}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12018", "pdf": "https://arxiv.org/pdf/2504.12018", "abs": "https://arxiv.org/abs/2504.12018", "authors": ["Xinli Yue", "JianHui Sun", "Junda Lu", "Liangchao Yao", "Fan Xia", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Yuetang Deng"], "title": "Instruction-augmented Multimodal Alignment for Image-Text and Element Matching", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop", "summary": "With the rapid advancement of text-to-image (T2I) generation models,\nassessing the semantic alignment between generated images and text descriptions\nhas become a significant research challenge. Current methods, including those\nbased on Visual Question Answering (VQA), still struggle with fine-grained\nassessments and precise quantification of image-text alignment. This paper\npresents an improved evaluation method named Instruction-augmented Multimodal\nAlignment for Image-Text and Element Matching (iMatch), which evaluates\nimage-text semantic alignment by fine-tuning multimodal large language models.\nWe introduce four innovative augmentation strategies: First, the QAlign\nstrategy creates a precise probabilistic mapping to convert discrete scores\nfrom multimodal large language models into continuous matching scores. Second,\na validation set augmentation strategy uses pseudo-labels from model\npredictions to expand training data, boosting the model's generalization\nperformance. Third, an element augmentation strategy integrates element\ncategory labels to refine the model's understanding of image-text matching.\nFourth, an image augmentation strategy employs techniques like random lighting\nto increase the model's robustness. Additionally, we propose prompt type\naugmentation and score perturbation strategies to further enhance the accuracy\nof element assessments. Our experimental results show that the iMatch method\nsignificantly surpasses existing methods, confirming its effectiveness and\npractical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025\nText to Image Generation Model Quality Assessment - Track 1 Image-Text\nAlignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12256", "pdf": "https://arxiv.org/pdf/2504.12256", "abs": "https://arxiv.org/abs/2504.12256", "authors": ["Andreas Plesner", "Turlan Kuzhagaliyev", "Roger Wattenhofer"], "title": "FLIP Reasoning Challenge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025", "summary": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "testbed", "accuracy"], "score": 4}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11536", "pdf": "https://arxiv.org/pdf/2504.11536", "abs": "https://arxiv.org/abs/2504.11536", "authors": ["Jiazhan Feng", "Shijue Huang", "Xingwei Qu", "Ge Zhang", "Yujia Qin", "Baoquan Zhong", "Chengquan Jiang", "Jinxin Chi", "Wanjun Zhong"], "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11477", "pdf": "https://arxiv.org/pdf/2504.11477", "abs": "https://arxiv.org/abs/2504.11477", "authors": ["Yunkai Zhang", "Shiyin Wei", "Yong Huang", "Yawu Su", "Shanshan Lu", "Hui Li"], "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing computer vision(CV)-based structural damage identification models\ndemonstrate notable accuracy in categorizing and localizing damage. However,\nthese models present several critical limitations that hinder their practical\napplication in civil engineering(CE). Primarily, their ability to recognize\ndamage types remains constrained, preventing comprehensive analysis of the\nhighly varied and complex conditions encountered in real-world CE structures.\nSecond, these models lack linguistic capabilities, rendering them unable to\narticulate structural damage characteristics through natural language\ndescriptions. With the continuous advancement of artificial intelligence(AI),\nlarge multi-modal models(LMMs) have emerged as a transformative solution,\nenabling the unified encoding and alignment of textual and visual data. These\nmodels can autonomously generate detailed descriptive narratives of structural\ndamage while demonstrating robust generalization across diverse scenarios and\ntasks. This study introduces SDIGLM, an innovative LMM for structural damage\nidentification, developed based on the open-source VisualGLM-6B architecture.\nTo address the challenge of adapting LMMs to the intricate and varied operating\nconditions in CE, this work integrates a U-Net-based semantic segmentation\nmodule to generate defect segmentation maps as visual Chain of Thought(CoT).\nAdditionally, a multi-round dialogue fine-tuning dataset is constructed to\nenhance logical reasoning, complemented by a language CoT formed through prompt\nengineering. By leveraging this multi-modal CoT, SDIGLM surpasses\ngeneral-purpose LMMs in structural damage identification, achieving an accuracy\nof 95.24% across various infrastructure types. Moreover, the model effectively\ndescribes damage characteristics such as hole size, crack direction, and\ncorrosion severity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "dialogue"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11900", "pdf": "https://arxiv.org/pdf/2504.11900", "abs": "https://arxiv.org/abs/2504.11900", "authors": ["Kabir Ahuja", "Melanie Sclar", "Yulia Tsvetkov"], "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "summarization"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11986", "pdf": "https://arxiv.org/pdf/2504.11986", "abs": "https://arxiv.org/abs/2504.11986", "authors": ["Jose Manuel Guevara-Vela"], "title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "factuality", "accuracy"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11838", "pdf": "https://arxiv.org/pdf/2504.11838", "abs": "https://arxiv.org/abs/2504.11838", "authors": ["Bianca Lamm", "Janis Keuper"], "title": "A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification", "categories": ["cs.CV"], "comment": null, "summary": "Despite the rapid evolution of learning and computer vision algorithms,\nFine-Grained Classification (FGC) still poses an open problem in many\npractically relevant applications. In the retail domain, for example, the\nidentification of fast changing and visually highly similar products and their\nproperties are key to automated price-monitoring and product recommendation.\nThis paper presents a novel Visual RAG pipeline that combines the Retrieval\nAugmented Generation (RAG) approach and Vision Language Models (VLMs) for\nfew-shot FGC. This Visual RAG pipeline extracts product and promotion data in\nadvertisement leaflets from various retailers and simultaneously predicts\nfine-grained product ids along with price and discount information. Compared to\nprevious approaches, the key characteristic of the Visual RAG pipeline is that\nit allows the prediction of novel products without re-training, simply by\nadding a few class samples to the RAG database. Comparing several VLM back-ends\nlike GPT-4o [23], GPT-4o-mini [24], and Gemini 2.0 Flash [10], our approach\nachieves 86.8% accuracy on a diverse dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11856", "pdf": "https://arxiv.org/pdf/2504.11856", "abs": "https://arxiv.org/abs/2504.11856", "authors": ["Zhenhuan Zhou", "Yuchen Zhang", "Along He", "Peng Wang", "Xueshuo Xie", "Tao Li"], "title": "Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation", "categories": ["cs.CV"], "comment": "12 pages, Initial submission time 25 December 2024, Now Under Review", "summary": "Root canal (RC) treatment is a highly delicate and technically complex\nprocedure in clinical practice, heavily influenced by the clinicians'\nexperience and subjective judgment. Deep learning has made significant\nadvancements in the field of computer-aided diagnosis (CAD) because it can\nprovide more objective and accurate diagnostic results. However, its\napplication in RC treatment is still relatively rare, mainly due to the lack of\npublic datasets in this field. To address this issue, in this paper, we\nestablished a First Molar Root Canal segmentation dataset called FMRC-2025.\nAdditionally, to alleviate the workload of manual annotation for dentists and\nfully leverage the unlabeled data, we designed a Cross-Frequency Collaborative\ntraining semi-supervised learning (SSL) Network called CFC-Net. It consists of\ntwo components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which\nintroduces two specialized students (SS) and one comprehensive teacher (CT) for\ncollaborative multi-frequency training. The CT and SS are trained on different\nfrequency components while fully integrating multi-frequency knowledge through\ncross and full frequency consistency supervisions. (2) Uncertainty-guided\nCross-Frequency Mix (UCF-Mix) mechanism enables the network to generate\nhigh-confidence pseudo-labels while learning to integrate multi-frequency\ninformation and maintaining the structural integrity of the targets. Extensive\nexperiments on FMRC-2025 and three public dental datasets demonstrate that\nCFC-MT is effective for RC segmentation and can also exhibit strong\ngeneralizability on other dental segmentation tasks, outperforming\nstate-of-the-art SSL medical image segmentation methods. Codes and dataset will\nbe released.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11741", "pdf": "https://arxiv.org/pdf/2504.11741", "abs": "https://arxiv.org/abs/2504.11741", "authors": ["Yiyou Sun", "Georgia Zhou", "Hao Wang", "Dacheng Li", "Nouha Dziri", "Dawn Song"], "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11942", "pdf": "https://arxiv.org/pdf/2504.11942", "abs": "https://arxiv.org/abs/2504.11942", "authors": ["Nada Shahin", "Leila Ismail"], "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "comment": null, "summary": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11966", "pdf": "https://arxiv.org/pdf/2504.11966", "abs": "https://arxiv.org/abs/2504.11966", "authors": ["Linjuan Fan", "Di Wen", "Kunyu Peng", "Kailun Yang", "Jiaming Zhang", "Ruiping Liu", "Yufan Chen", "Junwei Zheng", "Jiamin Wu", "Xudong Han", "Rainer Stiefelhagen"], "title": "Exploring Video-Based Driver Activity Recognition under Noisy Labels", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "comment": "The source code is available at\n  https://github.com/ilonafan/DAR-noisy-labels", "summary": "As an open research topic in the field of deep learning, learning with noisy\nlabels has attracted much attention and grown rapidly over the past ten years.\nLearning with label noise is crucial for driver distraction behavior\nrecognition, as real-world video data often contains mislabeled samples,\nimpacting model reliability and performance. However, label noise learning is\nbarely explored in the driver activity recognition field. In this paper, we\npropose the first label noise learning approach for the driver activity\nrecognition task. Based on the cluster assumption, we initially enable the\nmodel to learn clustering-friendly low-dimensional representations from given\nvideos and assign the resultant embeddings into clusters. We subsequently\nperform co-refinement within each cluster to smooth the classifier outputs.\nFurthermore, we propose a flexible sample selection strategy that combines two\nselection criteria without relying on any hyperparameters to filter clean\nsamples from the training dataset. We also incorporate a self-adaptive\nparameter into the sample selection process to enforce balancing across\nclasses. A comprehensive variety of experiments on the public Drive&Act dataset\nfor all granularity levels demonstrates the superior performance of our method\nin comparison with other label-denoising methods derived from the image\nclassification field. The source code is available at\nhttps://github.com/ilonafan/DAR-noisy-labels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "criteria"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11942", "pdf": "https://arxiv.org/pdf/2504.11942", "abs": "https://arxiv.org/abs/2504.11942", "authors": ["Nada Shahin", "Leila Ismail"], "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "comment": null, "summary": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11472", "pdf": "https://arxiv.org/pdf/2504.11472", "abs": "https://arxiv.org/abs/2504.11472", "authors": ["Kebin Contreras", "Brayan Monroy", "Jorge Bacca"], "title": "High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Object detection precision is crucial for ensuring the safety and efficacy of\nautonomous driving systems. The quality of acquired images directly influences\nthe ability of autonomous driving systems to correctly recognize and respond to\nother vehicles, pedestrians, and obstacles in real-time. However, real\nenvironments present extreme variations in lighting, causing saturation\nproblems and resulting in the loss of crucial details for detection.\nTraditionally, High Dynamic Range (HDR) images have been preferred for their\nability to capture a broad spectrum of light intensities, but the need for\nmultiple captures to construct HDR images is inefficient for real-time\napplications in autonomous vehicles. To address these issues, this work\nintroduces the use of modulo sensors for robust object detection. The modulo\nsensor allows pixels to `reset/wrap' upon reaching saturation level by\nacquiring an irradiance encoding image which can then be recovered using\nunwrapping algorithms. The applied reconstruction techniques enable HDR\nrecovery of color intensity and image details, ensuring better visual quality\neven under extreme lighting conditions at the cost of extra time. Experiments\nwith the YOLOv10 model demonstrate that images processed using modulo images\nachieve performance comparable to HDR images and significantly surpass\nsaturated images in terms of object detection accuracy. Moreover, the proposed\nmodulo imaging step combined with HDR image reconstruction is shorter than the\ntime required for conventional HDR image acquisition.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11482", "pdf": "https://arxiv.org/pdf/2504.11482", "abs": "https://arxiv.org/abs/2504.11482", "authors": ["Vidya Sudevan", "Fakhreddine Zayer", "Rizwana Kausar", "Sajid Javed", "Hamad Karki", "Giulia De Masi", "Jorge Dias"], "title": "snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing", "categories": ["cs.CV", "cs.AI", "cs.PF", "cs.RO", "eess.IV"], "comment": null, "summary": "Underwater image dehazing is critical for vision-based marine operations\nbecause light scattering and absorption can severely reduce visibility. This\npaper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)\nspecifically designed for underwater dehazing. By leveraging the temporal\ndynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image\nsequences while maintaining low power consumption. Static underwater images are\nfirst converted into time-dependent sequences by repeatedly inputting the same\nimage over user-defined timesteps. These RGB sequences are then transformed\ninto LAB color space representations and processed concurrently. The\narchitecture features three key modules: (i) a K estimator that extracts\nfeatures from multiple color space representations; (ii) a Background Light\nEstimator that jointly infers the background light component from the RGB-LAB\nimages; and (iii) a soft image reconstruction module that produces haze-free,\nvisibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a\nsurrogate gradient-based backpropagation through time (BPTT) strategy alongside\na novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ\nachieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it\nyields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million\nnetwork parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the\nalgorithm significantly outperforms existing state-of-the-art methods in terms\nof efficiency. These features make snnTrans-DHZ highly suitable for deployment\nin underwater robotics, marine exploration, and environmental monitoring.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11500", "pdf": "https://arxiv.org/pdf/2504.11500", "abs": "https://arxiv.org/abs/2504.11500", "authors": ["Kaicong Huang", "Talha Azfar", "Jack Reilly", "Ruimin Ke"], "title": "TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Transit Origin-Destination (OD) data are essential for transit planning,\nparticularly in route optimization and demand-responsive paratransit systems.\nTraditional methods, such as manual surveys, are costly and inefficient, while\nBluetooth and WiFi-based approaches require passengers to carry specific\ndevices, limiting data coverage. On the other hand, most transit vehicles are\nequipped with onboard cameras for surveillance, offering an opportunity to\nrepurpose them for edge-based OD data collection through visual person\nre-identification (ReID). However, such approaches face significant challenges,\nincluding severe occlusion and viewpoint variations in transit environments,\nwhich greatly reduce matching accuracy and hinder their adoption. Moreover,\ndesigning effective algorithms that can operate efficiently on edge devices\nremains an open challenge. To address these challenges, we propose TransitReID,\na novel framework for individual-level transit OD data collection. TransitReID\nconsists of two key components: (1) An occlusion-robust ReID algorithm\nfeaturing a variational autoencoder guided region-attention mechanism that\nadaptively focuses on visible body regions through reconstruction\nloss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic\nMatching (HSDM) mechanism specifically designed for efficient and robust\ntransit OD matching which balances storage, speed, and accuracy. Additionally,\na multi-threaded design supports near real-time operation on edge devices,\nwhich also ensuring privacy protection. We also introduce a ReID dataset\ntailored for complex bus environments to address the lack of relevant training\ndata. Experimental results demonstrate that TransitReID achieves\nstate-of-the-art performance in ReID tasks, with an accuracy of approximately\n90\\% in bus route simulations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11662", "pdf": "https://arxiv.org/pdf/2504.11662", "abs": "https://arxiv.org/abs/2504.11662", "authors": ["Marcos Mendes", "Gon√ßalo Perna", "Pedro Rito", "Duarte Raposo", "Susana Sargento"], "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing", "categories": ["cs.CV", "68T45"], "comment": "30th ITS World Congress, Dubai, UAE, 16-20 September 2024", "summary": "The World Health Organization suggests that road traffic crashes cost\napproximately 518 billion dollars globally each year, which accounts for 3% of\nthe gross domestic product for most countries. Most fatal road accidents in\nurban areas involve Vulnerable Road Users (VRUs). Smart cities environments\npresent innovative approaches to combat accidents involving cutting-edge\ntechnologies, that include advanced sensors, extensive datasets, Machine\nLearning (ML) models, communication systems, and edge computing. This paper\nproposes a strategy and an implementation of a system for road monitoring and\nsafety for smart cities, based on Computer Vision (CV) and edge computing.\nPromising results were obtained by implementing vision algorithms and tracking\nusing surveillance cameras, that are part of a Smart City testbed, the Aveiro\nTech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars,\npedestrians, and bicycles, while predicting the road state, the distance\nbetween moving objects, and inferring on collision events to prevent\ncollisions, in near real-time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed", "safety"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11934", "pdf": "https://arxiv.org/pdf/2504.11934", "abs": "https://arxiv.org/abs/2504.11934", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025", "summary": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952", "abs": "https://arxiv.org/abs/2504.11952", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Hamza Farooq"], "title": "Robust and Fine-Grained Detection of AI Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Feb ARR Submission", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11972", "pdf": "https://arxiv.org/pdf/2504.11972", "abs": "https://arxiv.org/abs/2504.11972", "authors": ["Xanh Ho", "Jiahao Huang", "Florian Boudin", "Akiko Aizawa"], "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA", "categories": ["cs.CL"], "comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa", "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "question answering"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11707", "pdf": "https://arxiv.org/pdf/2504.11707", "abs": "https://arxiv.org/abs/2504.11707", "authors": ["Muhammad Shahid Muneer", "Simon S. Woo"], "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Short Paper The Web Conference", "summary": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12098", "pdf": "https://arxiv.org/pdf/2504.12098", "abs": "https://arxiv.org/abs/2504.12098", "authors": ["Adil Bahaj", "Hamed Rahimi", "Mohamed Chetouani", "Mounir Ghogho"], "title": "Gauging Overprecision in LLMs: An Empirical Study", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11798", "pdf": "https://arxiv.org/pdf/2504.11798", "abs": "https://arxiv.org/abs/2504.11798", "authors": ["Chao Yuan", "Tianyi Zhang", "Guanglin Niu"], "title": "Neighbor-Based Feature and Index Enhancement for Person Re-Identification", "categories": ["cs.CV"], "comment": "Comment: This paper has been accepted for publication in the 2025\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW)", "summary": "Person re-identification (Re-ID) aims to match the same pedestrian in a large\ngallery with different cameras and views. Enhancing the robustness of the\nextracted feature representations is a main challenge in Re-ID. Existing\nmethods usually improve feature representation by improving model architecture,\nbut most methods ignore the potential contextual information, which limits the\neffectiveness of feature representation and retrieval performance. Neighborhood\ninformation, especially the potential information of multi-order neighborhoods,\ncan effectively enrich feature expression and improve retrieval accuracy, but\nthis has not been fully explored in existing research. Therefore, we propose a\nnovel model DMON-ARO that leverages latent neighborhood information to enhance\nboth feature representation and index performance. Our approach is built on two\ncomplementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and\nAsymmetric Relationship Optimization (ARO). The DMON module dynamically\naggregates multi-order neighbor relationships, allowing it to capture richer\ncontextual information and enhance feature representation through adaptive\nneighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing\nquery-to-gallery relationships, improving the index accuracy. Extensive\nexperiments on three benchmark datasets demonstrate that our approach achieves\nperformance improvements against baseline models, which illustrate the\neffectiveness of our model. Specifically, our model demonstrates improvements\nin Rank-1 accuracy and mAP. Moreover, this method can also be directly extended\nto other re-identification tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11571", "pdf": "https://arxiv.org/pdf/2504.11571", "abs": "https://arxiv.org/abs/2504.11571", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "categories": ["cs.AI", "cs.CL"], "comment": "41 pages, 11 figures", "summary": "Large Language Model (LLM)-powered agents have unlocked new possibilities for\nautomating human tasks. While prior work has focused on well-defined tasks with\nspecified goals, the capabilities of agents in creative design tasks with\nopen-ended goals remain underexplored. We introduce GraphicBench, a new\nplanning benchmark for graphic design that covers 1,079 user queries and input\nimages across four design types. We further present GraphicTown, an LLM agent\nframework with three design experts and 46 actions (tools) to choose from for\nexecuting each step of the planned workflows in web environments. Experiments\nwith six LLMs demonstrate their ability to generate workflows that integrate\nboth explicit design constraints from user queries and implicit commonsense\nconstraints. However, these workflows often do not lead to successful execution\noutcomes, primarily due to challenges in: (1) reasoning about spatial\nrelationships, (2) coordinating global dependencies across experts, and (3)\nretrieving the most appropriate action per step. We envision GraphicBench as a\nchallenging yet valuable testbed for advancing LLM-agent planning and execution\nin creative design tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "testbed"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11922", "pdf": "https://arxiv.org/pdf/2504.11922", "abs": "https://arxiv.org/abs/2504.11922", "authors": ["Lvpan Cai", "Haowei Wang", "Jiayi Ji", "YanShu ZhouMen", "Yiwei Ma", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach", "categories": ["cs.CV"], "comment": null, "summary": "The rise of AI-generated image editing tools has made localized forgeries\nincreasingly realistic, posing challenges for visual content integrity.\nAlthough recent efforts have explored localized AIGC detection, existing\ndatasets predominantly focus on object-level forgeries while overlooking\nbroader scene edits in regions such as sky or ground. To address these\nlimitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000\nlocally forged images with diverse scene-aware annotations, which are based on\nsemantic calibration to ensure high-quality samples. BR-Gen is constructed\nthrough a fully automated Perception-Creation-Evaluation pipeline to ensure\nsemantic coherence and visual realism. In addition, we further propose\n\\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that\nenhances the detection of localized forgeries by amplifying forgery-related\nfeatures across the entire image. NFA-ViT mines heterogeneous regions in\nimages, \\emph{i.e.}, potential edited areas, by noise fingerprints.\nSubsequently, attention mechanism is introduced to compel the interaction\nbetween normal and abnormal features, thereby propagating the generalization\ntraces throughout the entire image, allowing subtle forgeries to influence a\nbroader context and improving overall detection robustness. Extensive\nexperiments demonstrate that BR-Gen constructs entirely new scenarios that are\nnot covered by existing methods. Take a step further, NFA-ViT outperforms\nexisting methods on BR-Gen and generalizes well across current benchmarks. All\ndata and codes are available at https://github.com/clpbc/BR-Gen.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11995", "pdf": "https://arxiv.org/pdf/2504.11995", "abs": "https://arxiv.org/abs/2504.11995", "authors": ["Rahima Khanam", "Muhammad Hussain"], "title": "A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions", "categories": ["cs.CV"], "comment": null, "summary": "The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12021", "pdf": "https://arxiv.org/pdf/2504.12021", "abs": "https://arxiv.org/abs/2504.12021", "authors": ["Mohamad Dalal", "Artur Xarles", "Anthony Cioppa", "Silvio Giancola", "Marc Van Droogenbroeck", "Bernard Ghanem", "Albert Clap√©s", "Sergio Escalera", "Thomas B. Moeslund"], "title": "Action Anticipation from SoccerNet Football Video Broadcasts", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "15 pages, 14 figures. To be published in the CVSports CVPR workshop", "summary": "Artificial intelligence has revolutionized the way we analyze sports videos,\nwhether to understand the actions of games in long untrimmed videos or to\nanticipate the player's motion in future frames. Despite these efforts, little\nattention has been given to anticipating game actions before they occur. In\nthis work, we introduce the task of action anticipation for football broadcast\nvideos, which consists in predicting future actions in unobserved future\nframes, within a five- or ten-second anticipation window. To benchmark this\ntask, we release a new dataset, namely the SoccerNet Ball Action Anticipation\ndataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a\nFootball Action ANticipation TRAnsformer (FAANTRA), a baseline method that\nadapts FUTR, a state-of-the-art action anticipation model, to predict\nball-related actions. To evaluate action anticipation, we introduce new\nmetrics, including mAP@$\\delta$, which evaluates the temporal precision of\npredicted future actions, as well as mAP@$\\infty$, which evaluates their\noccurrence within the anticipation window. We also conduct extensive ablation\nstudies to examine the impact of various task settings, input configurations,\nand model architectures. Experimental results highlight both the feasibility\nand challenges of action anticipation in football videos, providing valuable\ninsights into the design of predictive models for sports analytics. By\nforecasting actions before they unfold, our work will enable applications in\nautomated broadcasting, tactical analysis, and player decision-making. Our\ndataset and code are publicly available at\nhttps://github.com/MohamadDalal/FAANTRA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12027", "pdf": "https://arxiv.org/pdf/2504.12027", "abs": "https://arxiv.org/abs/2504.12027", "authors": ["Bingyan Liu", "Chengyu Wang", "Tongtong Su", "Huan Ten", "Jun Huang", "Kailing Guo", "Kui Jia"], "title": "Understanding Attention Mechanism in Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12039", "pdf": "https://arxiv.org/pdf/2504.12039", "abs": "https://arxiv.org/abs/2504.12039", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12048", "pdf": "https://arxiv.org/pdf/2504.12048", "abs": "https://arxiv.org/abs/2504.12048", "authors": ["Zirui Pan", "Xin Wang", "Yipeng Zhang", "Hong Chen", "Kwan Man Cheng", "Yaofei Wu", "Wenwu Zhu"], "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM", "categories": ["cs.CV"], "comment": "AAAI 2025 Poster", "summary": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12078", "pdf": "https://arxiv.org/pdf/2504.12078", "abs": "https://arxiv.org/abs/2504.12078", "authors": ["Trina De", "Adrian Urbanski", "Artur Yakimovich"], "title": "Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects", "categories": ["cs.CV", "q-bio.QM", "J.3; I.4"], "comment": "12 pages, 8 figures", "summary": "Biomedical images often contain objects known to be spatially correlated or\nnested due to their inherent properties, leading to semantic relations.\nExamples include cell nuclei being nested within eukaryotic cells and colonies\ngrowing exclusively within their culture dishes. While these semantic relations\nbear key importance, detection tasks are often formulated independently,\nrequiring multi-shot analysis pipelines. Importantly, spatial correlation could\nconstitute a fundamental prior facilitating learning of more meaningful\nrepresentations for tasks like instance segmentation. This knowledge has, thus\nfar, not been utilised by the biomedical computer vision community. We argue\nthat the instance segmentation of two or more categories of objects can be\nachieved in parallel. We achieve this via two architectures HydraStarDist (HSD)\nand the novel (HSD-WBR) based on the widely-used StarDist (SD), to take\nadvantage of the star-convexity of our target objects. HSD and HSD-WBR are\nconstructed to be capable of incorporating their interactions as constraints\ninto account. HSD implicitly incorporates spatial correlation priors based on\nobject interaction through a joint encoder. HSD-WBR further enforces the prior\nin a regularisation layer with the penalty we proposed named Within Boundary\nRegularisation Penalty (WBR). Both architectures achieve nested instance\nsegmentation in a single shot. We demonstrate their competitiveness based on\n$IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate\n(JTPR) compared to their baseline SD and Cellpose. Our approach can be further\nmodified to capture partial-inclusion/-exclusion in multi-object interactions\nin fluorescent or brightfield microscopy or digital imaging. Finally, our\nstrategy suggests gains by making this learning single-shot and computationally\nefficient.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "criteria"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12080", "pdf": "https://arxiv.org/pdf/2504.12080", "abs": "https://arxiv.org/abs/2504.12080", "authors": ["Mengshi Qi", "Pengfei Zhu", "Xiangtai Li", "Xiaoyang Bi", "Lu Qi", "Huadong Ma", "Ming-Hsuan Yang"], "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12103", "pdf": "https://arxiv.org/pdf/2504.12103", "abs": "https://arxiv.org/abs/2504.12103", "authors": ["Tao Wen", "Jiepeng Wang", "Yabo Chen", "Shugong Xu", "Chi Zhang", "Xuelong Li"], "title": "Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image", "categories": ["cs.CV"], "comment": "Our project page: https://tele-ai.github.io/MetricSolver/", "summary": "Accurate and generalizable metric depth estimation is crucial for various\ncomputer vision applications but remains challenging due to the diverse depth\nscales encountered in indoor and outdoor environments. In this paper, we\nintroduce Metric-Solver, a novel sliding anchor-based metric depth estimation\nmethod that dynamically adapts to varying scene scales. Our approach leverages\nan anchor-based representation, where a reference depth serves as an anchor to\nseparate and normalize the scene depth into two components: scaled near-field\ndepth and tapered far-field depth. The anchor acts as a normalization factor,\nenabling the near-field depth to be normalized within a consistent range while\nmapping far-field depth smoothly toward zero. Through this approach, any depth\nfrom zero to infinity in the scene can be represented within a unified\nrepresentation, effectively eliminating the need to manually account for scene\nscale variations. More importantly, for the same scene, the anchor can slide\nalong the depth axis, dynamically adjusting to different depth scales. A\nsmaller anchor provides higher resolution in the near-field, improving depth\nprecision for closer objects while a larger anchor improves depth estimation in\nfar regions. This adaptability enables the model to handle depth predictions at\nvarying distances and ensure strong generalization across datasets. Our design\nenables a unified and adaptive depth representation across diverse\nenvironments. Extensive experiments demonstrate that Metric-Solver outperforms\nexisting methods in both accuracy and cross-dataset generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12132", "pdf": "https://arxiv.org/pdf/2504.12132", "abs": "https://arxiv.org/abs/2504.12132", "authors": ["Linhao Qu", "Shiman Li", "Xiaoyuan Luo", "Shaolei Liu", "Qinhao Guo", "Manning Wang", "Zhijian Song"], "title": "Weakly Semi-supervised Whole Slide Image Classification by Two-level Cross Consistency Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Computer-aided Whole Slide Image (WSI) classification has the potential to\nenhance the accuracy and efficiency of clinical pathological diagnosis. It is\ncommonly formulated as a Multiple Instance Learning (MIL) problem, where each\nWSI is treated as a bag and the small patches extracted from the WSI are\nconsidered instances within that bag. However, obtaining labels for a large\nnumber of bags is a costly and time-consuming process, particularly when\nutilizing existing WSIs for new classification tasks. This limitation renders\nmost existing WSI classification methods ineffective. To address this issue, we\npropose a novel WSI classification problem setting, more aligned with clinical\npractice, termed Weakly Semi-supervised Whole slide image Classification\n(WSWC). In WSWC, a small number of bags are labeled, while a significant number\nof bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the\nabsence of patch labels, distinguishes it from typical semi-supervised image\nclassification problems, making existing algorithms for natural images\nunsuitable for directly solving the WSWC problem. In this paper, we present a\nconcise and efficient framework, named CroCo, to tackle the WSWC problem\nthrough two-level Cross Consistency supervision. CroCo comprises two\nheterogeneous classifier branches capable of performing both instance\nclassification and bag classification. The fundamental idea is to establish\ncross-consistency supervision at both the bag-level and instance-level between\nthe two branches during training. Extensive experiments conducted on four\ndatasets demonstrate that CroCo achieves superior bag classification and\ninstance classification performance compared to other comparative methods when\nlimited WSIs with bag labels are available. To the best of our knowledge, this\npaper presents for the first time the WSWC problem and gives a successful\nresolution.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12157", "pdf": "https://arxiv.org/pdf/2504.12157", "abs": "https://arxiv.org/abs/2504.12157", "authors": ["Xiaojun Ye", "Chun Wang", "Yiren Song", "Sheng Zhou", "Liangcheng Li", "Jiajun Bu"], "title": "FocusedAD: Character-centric Movie Audio Description", "categories": ["cs.CV", "I.2.10"], "comment": "Code and Demo link: https://github.com/Thorin215/FocusedAD", "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12165", "pdf": "https://arxiv.org/pdf/2504.12165", "abs": "https://arxiv.org/abs/2504.12165", "authors": ["Yike Liu", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "CodingHomo: Bootstrapping Deep Homography With Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "Homography estimation is a fundamental task in computer vision with\napplications in diverse fields. Recent advances in deep learning have improved\nhomography estimation, particularly with unsupervised learning approaches,\noffering increased robustness and generalizability. However, accurately\npredicting homography, especially in complex motions, remains a challenge. In\nresponse, this work introduces a novel method leveraging video coding,\nparticularly by harnessing inherent motion vectors (MVs) present in videos. We\npresent CodingHomo, an unsupervised framework for homography estimation. Our\nframework features a Mask-Guided Fusion (MGF) module that identifies and\nutilizes beneficial features among the MVs, thereby enhancing the accuracy of\nhomography prediction. Additionally, the Mask-Guided Homography Estimation\n(MGHE) module is presented for eliminating undesired features in the\ncoarse-to-fine homography refinement process. CodingHomo outperforms existing\nstate-of-the-art unsupervised methods, delivering good robustness and\ngeneralizability. The code and dataset are available at:\n\\href{github}{https://github.com/liuyike422/CodingHomo", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12167", "pdf": "https://arxiv.org/pdf/2504.12167", "abs": "https://arxiv.org/abs/2504.12167", "authors": ["Yuan Luo", "Rudolf Hoffmann", "Yan Xia", "Olaf Wysocki", "Benedikt Schwab", "Thomas H. Kolbe", "Daniel Cremers"], "title": "RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning", "categories": ["cs.CV", "cs.LG"], "comment": "The paper accepted for CVPRW '25 (PBVS 2025 - the Perception Beyond\n  the Visible Spectrum)", "summary": "Semantic 3D city models are worldwide easy-accessible, providing accurate,\nobject-oriented, and semantic-rich 3D priors. To date, their potential to\nmitigate the noise impact on radar object detection remains under-explored. In\nthis paper, we first introduce a unique dataset, RadarCity, comprising 54K\nsynchronized radar-image pairs and semantic 3D city models. Moreover, we\npropose a novel neural network, RADLER, leveraging the effectiveness of\ncontrastive self-supervised learning (SSL) and semantic 3D city models to\nenhance radar object detection of pedestrians, cyclists, and cars.\nSpecifically, we first obtain the robust radar features via a SSL network in\nthe radar-image pretext task. We then use a simple yet effective feature fusion\nstrategy to incorporate semantic-depth features from semantic 3D city models.\nHaving prior 3D information as guidance, RADLER obtains more fine-grained\ndetails to enhance radar object detection. We extensively evaluate RADLER on\nthe collected RadarCity dataset and demonstrate average improvements of 5.46%\nin mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over\nprevious radar object detection methods. We believe this work will foster\nfurther research on semantic-guided and map-supported radar object detection.\nOur project page is publicly available\nathttps://gpp-communication.github.io/RADLER .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12215", "pdf": "https://arxiv.org/pdf/2504.12215", "abs": "https://arxiv.org/abs/2504.12215", "authors": ["Ilkin Sevgi Isler", "David Mohaisen", "Curtis Lisle", "Damla Turgut", "Ulas Bagci"], "title": "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 2 figures, to appear in IEEE ADSCA 2025", "summary": "Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12240", "pdf": "https://arxiv.org/pdf/2504.12240", "abs": "https://arxiv.org/abs/2504.12240", "authors": ["Junhao Zhuang", "Lingen Li", "Xuan Ju", "Zhaoyang Zhang", "Chun Yuan", "Ying Shan"], "title": "Cobra: Efficient Line Art COlorization with BRoAder References", "categories": ["cs.CV"], "comment": "Project page with code: https://zhuang2002.github.io/Cobra/", "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12284", "pdf": "https://arxiv.org/pdf/2504.12284", "abs": "https://arxiv.org/abs/2504.12284", "authors": ["Aditya Prakash", "Benjamin Lundell", "Dmitry Andreychuk", "David Forsyth", "Saurabh Gupta", "Harpreet Sawhney"], "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact", "summary": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11493", "pdf": "https://arxiv.org/pdf/2504.11493", "abs": "https://arxiv.org/abs/2504.11493", "authors": ["Azizul Zahid", "Jie Fan", "Farong Wang", "Ashton Dy", "Sai Swaminathan", "Fei Liu"], "title": "Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "ICRA'25 Workshop: Human-Centered Robot Learning in the Era of Big\n  Data and Large Models", "summary": "Understanding action correspondence between humans and robots is essential\nfor evaluating alignment in decision-making, particularly in human-robot\ncollaboration and imitation learning within unstructured environments. We\npropose a multimodal demonstration learning framework that explicitly models\nhuman demonstrations from RGB video with robot demonstrations in voxelized\nRGB-D space. Focusing on the \"pick and place\" task from the RH20T dataset, we\nutilize data from 5 users across 10 diverse scenes. Our approach combines\nResNet-based visual encoding for human intention modeling and a Perceiver\nTransformer for voxel-based robot action prediction. After 2000 training\nepochs, the human model reaches 71.67% accuracy, and the robot model achieves\n71.8% accuracy, demonstrating the framework's potential for aligning complex,\nmultimodal human and robot behaviors in manipulation tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11509", "pdf": "https://arxiv.org/pdf/2504.11509", "abs": "https://arxiv.org/abs/2504.11509", "authors": ["Wenyi Zhang", "Ju Jia", "Xiaojun Jia", "Yihao Huang", "Xinfeng Li", "Cong Wu", "Lina Wang"], "title": "PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12249", "pdf": "https://arxiv.org/pdf/2504.12249", "abs": "https://arxiv.org/abs/2504.12249", "authors": ["Zhijin He", "Alan B. McMillan"], "title": "Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The application of artificial intelligence (AI) in medical imaging has\nrevolutionized diagnostic practices, enabling advanced analysis and\ninterpretation of radiological data. This study presents a comprehensive\nevaluation of radiomics-based and deep learning-based approaches for disease\ndetection in chest radiography, focusing on COVID-19, lung opacity, and viral\npneumonia. While deep learning models, particularly convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), learn directly from image data,\nradiomics-based models extract and analyze quantitative features, potentially\nproviding advantages in data-limited scenarios. This study systematically\ncompares the diagnostic accuracy and robustness of various AI models, including\nDecision Trees, Gradient Boosting, Random Forests, Support Vector Machines\n(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against\nstate-of-the-art computer vision deep learning architectures. Performance\nmetrics across varying sample sizes reveal insights into each model's efficacy,\nhighlighting the contexts in which specific AI approaches may offer enhanced\ndiagnostic capabilities. The results aim to inform the integration of AI-driven\ndiagnostic tools in clinical practice, particularly in automated and\nhigh-throughput environments where timely, reliable diagnosis is critical. This\ncomparative study addresses an essential gap, establishing guidance for the\nselection of AI models based on clinical and operational needs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11460", "pdf": "https://arxiv.org/pdf/2504.11460", "abs": "https://arxiv.org/abs/2504.11460", "authors": ["Tobias Hallmen", "Robin-Nico Kampa", "Fabian Deuser", "Norbert Oswald", "Elisabeth Andr√©"], "title": "Semantic Matters: Multimodal Features for Affective Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this study, we present our methodology for two tasks: the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry\nIntensity (EMI) Estimation Challenge, both conducted as part of the 8th\nWorkshop and Competition on Affective & Behavior Analysis in-the-wild. Building\non previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast\ndataset to extract various audio features, capturing both linguistic and\nparalinguistic information. Our approach incorporates a\nvalence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like\nencoder, and a vision transformer (ViT) with predictions subsequently processed\nthrough a long short-term memory (LSTM) architecture for temporal modeling. In\nthis iteration, we integrate the textual and visual modality into our analysis,\nrecognizing that semantic content provides valuable contextual cues and\nunderscoring that the meaning of speech often conveys more critical insights\nthan its acoustic counterpart alone. Fusing in the vision modality helps in\nsome cases to interpret the textual modality more precisely. This combined\napproach yields significant performance improvements over baseline methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11468", "pdf": "https://arxiv.org/pdf/2504.11468", "abs": "https://arxiv.org/abs/2504.11468", "authors": ["Hardy Chen", "Haoqin Tu", "Fali Wang", "Hui Liu", "Xianfeng Tang", "Xinya Du", "Yuyin Zhou", "Cihang Xie"], "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This work revisits the dominant supervised fine-tuning (SFT) then\nreinforcement learning (RL) paradigm for training Large Vision-Language Models\n(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent\nRL by inducing ``pseudo reasoning paths'' imitated from expert models. While\nthese paths may resemble the native reasoning paths of RL models, they often\ninvolve prolonged, hesitant, less informative steps, and incorrect reasoning.\nTo systematically study this effect, we introduce VLAA-Thinking, a new\nmultimodal dataset designed to support reasoning in LVLMs. Constructed via a\nsix-step pipeline involving captioning, reasoning distillation, answer rewrite\nand verification, VLAA-Thinking comprises high-quality, step-by-step visual\nreasoning traces for SFT, along with a more challenging RL split from the same\ndata source. Using this dataset, we conduct extensive experiments comparing\nSFT, RL and their combinations. Results show that while SFT helps models learn\nreasoning formats, it often locks aligned models into imitative, rigid\nreasoning modes that impede further learning. In contrast, building on the\nGroup Relative Policy Optimization (GRPO) with a novel mixed reward module\nintegrating both perception and cognition signals, our RL approach fosters more\ngenuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on\nQwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard\n(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)\namong 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope\nour findings provide valuable insights in developing reasoning-capable LVLMs\nand can inform future research in this area.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11473", "pdf": "https://arxiv.org/pdf/2504.11473", "abs": "https://arxiv.org/abs/2504.11473", "authors": ["Warren Zhu", "Aida Ramezani", "Yang Xu"], "title": "Visual moral inference and communication", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Humans can make moral inferences from multiple sources of input. In contrast,\nautomated moral inference in artificial intelligence typically relies on\nlanguage models with textual input. However, morality is conveyed through\nmodalities beyond language. We present a computational framework that supports\nmoral inference from natural images, demonstrated in two related tasks: 1)\ninferring human moral judgment toward visual images and 2) analyzing patterns\nin moral content communicated via images from public news. We find that models\nbased on text alone cannot capture the fine-grained human moral judgment toward\nvisual stimuli, but language-vision fusion models offer better precision in\nvisual moral inference. Furthermore, applications of our framework to news data\nreveal implicit biases in news categories and geopolitical discussions. Our\nwork creates avenues for automating visual moral inference and discovering\npatterns of visual moral communication in public media.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11478", "pdf": "https://arxiv.org/pdf/2504.11478", "abs": "https://arxiv.org/abs/2504.11478", "authors": ["Hao Kang", "Stathi Fotiadis", "Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Min Jin Chong", "Xin Lu"], "title": "Flux Already Knows - Activating Subject-Driven Image Generation without Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a simple yet effective zero-shot framework for subject-driven\nimage generation using a vanilla Flux model. By framing the task as grid-based\nimage completion and simply replicating the subject image(s) in a mosaic\nlayout, we activate strong identity-preserving capabilities without any\nadditional data, training, or inference-time fine-tuning. This \"free lunch\"\napproach is further strengthened by a novel cascade attention design and meta\nprompting technique, boosting fidelity and versatility. Experimental results\nshow that our method outperforms baselines across multiple key metrics in\nbenchmarks and human preference studies, with trade-offs in certain aspects.\nAdditionally, it supports diverse edits, including logo insertion, virtual\ntry-on, and subject replacement or insertion. These results demonstrate that a\npre-trained foundational text-to-image model can enable high-quality,\nresource-efficient subject-driven generation, opening new possibilities for\nlightweight customization in downstream applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11626", "pdf": "https://arxiv.org/pdf/2504.11626", "abs": "https://arxiv.org/abs/2504.11626", "authors": ["Ozan ƒ∞rsoy", "Pengxiang Cheng", "Jennifer L. Chen", "Daniel Preo≈£iuc-Pietro", "Shiyue Zhang", "Duccio Pappadopulo"], "title": "Improving Instruct Models for Free: A Study on Partial Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Author ordering chosen at random", "summary": "Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model being overly conversational or verbose. This, in turn, can\nlead to degradation of in-context few-shot learning performance. In this work,\nwe study the performance trajectory between base and instruct models by scaling\ndown the strength of instruction-tuning via the partial adaption method. We\nshow that, across several model families and model sizes, reducing the strength\nof instruction-tuning results in material improvement on a few-shot in-context\nlearning benchmark covering a variety of classic natural language tasks. This\ncomes at the cost of losing some degree of instruction following ability as\nmeasured by AlpacaEval. Our study shines light on the potential trade-off\nbetween in-context learning and instruction following abilities that is worth\nconsidering in practice.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11637", "pdf": "https://arxiv.org/pdf/2504.11637", "abs": "https://arxiv.org/abs/2504.11637", "authors": ["Yiming Xiao", "Ali Mostafavi"], "title": "DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization", "categories": ["cs.CV"], "comment": "23 pages, 6 figures", "summary": "Natural disasters increasingly threaten communities worldwide, creating an\nurgent need for rapid, reliable building damage assessment to guide emergency\nresponse and recovery efforts. Current methods typically classify damage in\nbinary (damaged/undamaged) or ordinal severity terms, limiting their practical\nutility. In fact, the determination of damage typology is crucial for response\nand recovery efforts. To address this important gap, this paper introduces\nDamageCAT, a novel framework that provides typology-based categorical damage\ndescriptions rather than simple severity ratings. Accordingly, this study\npresents two key contributions: (1) the BD-TypoSAT dataset containing satellite\nimage triplets (pre-disaster, post-disaster, and damage masks) from Hurricane\nIda with four damage categories (partial roof damage, total roof damage,\npartial structural collapse, and total structural collapse), and (2) a\nhierarchical U-Net-based transformer architecture that effectively processes\npre-post disaster image pairs to identify and categorize building damage.\nDespite significant class imbalances in the training data, our model achieved\nrobust performance with overall metrics of 0.7921 Intersection over Union (IoU)\nand 0.8835 F1 scores across all categories. The model's capability to recognize\nintricate damage typology in less common categories is especially remarkable.\nThe DamageCAT framework advances automated damage assessment by providing\nactionable, typological information that better supports disaster response\ndecision-making and resource allocation compared to traditional severity-based\napproaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829", "abs": "https://arxiv.org/abs/2504.11829", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "title": "D√©j√† Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11669", "pdf": "https://arxiv.org/pdf/2504.11669", "abs": "https://arxiv.org/abs/2504.11669", "authors": ["Amirhossein Dadashzadeh", "Parsa Esmati", "Majid Mirmehdi"], "title": "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Source-Free Unsupervised Video Domain Adaptation (SFUVDA)\nleverage vision-language models to enhance pseudo-label generation. However,\nchallenges such as noisy pseudo-labels and over-confident predictions limit\ntheir effectiveness in adapting well across domains. We propose Co-STAR, a\nnovel framework that integrates curriculum learning with collaborative\nself-training between a source-trained teacher and a contrastive\nvision-language model (CLIP). Our curriculum learning approach employs a\nreliability-based weight function that measures bidirectional prediction\nalignment between the teacher and CLIP, balancing between confident and\nuncertain predictions. This function preserves uncertainty for difficult\nsamples, while prioritizing reliable pseudo-labels when the predictions from\nboth models closely align. To further improve adaptation, we propose Adaptive\nCurriculum Regularization, which modifies the learning priority of samples in a\nprobabilistic, adaptive manner based on their confidence scores and prediction\nstability, mitigating overfitting to noisy and over-confident samples.\nExtensive experiments across multiple video domain adaptation benchmarks\ndemonstrate that Co-STAR consistently outperforms state-of-the-art SFUVDA\nmethods. Code is available at: https://github.com/Plrbear/Co-Star", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11686", "pdf": "https://arxiv.org/pdf/2504.11686", "abs": "https://arxiv.org/abs/2504.11686", "authors": ["Yiran He", "Yun Cao", "Bowen Yang", "Zeyu Zhang"], "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 11 figures, 13IHMMSec2025", "summary": "The rapid development of generative AI facilitates content creation and makes\nimage manipulation easier and more difficult to detect. While multimodal Large\nLanguage Models (LLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating AI-generated Content (AIGC) and struggle to\ncomprehend local forgery details. In this work, we investigate the application\nof multimodal LLMs in forgery detection. We propose a framework capable of\nevaluating image authenticity, localizing tampered regions, providing evidence,\nand tracing generation methods based on semantic tampering clues. Our method\ndemonstrates that the potential of LLMs in forgery analysis can be effectively\nunlocked through meticulous prompt engineering and the application of few-shot\nlearning techniques. We conduct qualitative and quantitative experiments and\nshow that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in\nLaMa, which is competitive with state-of-the-art AIGC detection methods. We\nfurther discuss the limitations of multimodal LLMs in such tasks and propose\npotential improvements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11732", "pdf": "https://arxiv.org/pdf/2504.11732", "abs": "https://arxiv.org/abs/2504.11732", "authors": ["Jilan Xu", "Yifei Huang", "Baoqi Pei", "Junlin Hou", "Qingqiu Li", "Guo Chen", "Yuejie Zhang", "Rui Feng", "Weidi Xie"], "title": "EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Generating videos in the first-person perspective has broad application\nprospects in the field of augmented reality and embodied intelligence. In this\nwork, we explore the cross-view video prediction task, where given an\nexo-centric video, the first frame of the corresponding ego-centric video, and\ntextual instructions, the goal is to generate futur frames of the ego-centric\nvideo. Inspired by the notion that hand-object interactions (HOI) in\nego-centric videos represent the primary intentions and actions of the current\nactor, we present EgoExo-Gen that explicitly models the hand-object dynamics\nfor cross-view video prediction. EgoExo-Gen consists of two stages. First, we\ndesign a cross-view HOI mask prediction model that anticipates the HOI masks in\nfuture ego-frames by modeling the spatio-temporal ego-exo correspondence. Next,\nwe employ a video diffusion model to predict future ego-frames using the first\nego-frame and textual instructions, while incorporating the HOI masks as\nstructural guidance to enhance prediction quality. To facilitate training, we\ndevelop an automated pipeline to generate pseudo HOI masks for both ego- and\nexo-videos by exploiting vision foundation models. Extensive experiments\ndemonstrate that our proposed EgoExo-Gen achieves better prediction performance\ncompared to previous video prediction models on the Ego-Exo4D and H2O benchmark\ndatasets, with the HOI masks significantly improving the generation of hands\nand interactive objects in the ego-centric videos.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12052", "pdf": "https://arxiv.org/pdf/2504.12052", "abs": "https://arxiv.org/abs/2504.12052", "authors": ["Fran√ßois Haguinet", "Jeffery L Painter", "Gregory E Powell", "Andrea Callegaro", "Andrew Bate"], "title": "Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": "30 pages, 7 figures, 5 supplementary figures", "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12082", "pdf": "https://arxiv.org/pdf/2504.12082", "abs": "https://arxiv.org/abs/2504.12082", "authors": ["Yumin Kim", "Hwanhee Lee"], "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11754", "pdf": "https://arxiv.org/pdf/2504.11754", "abs": "https://arxiv.org/abs/2504.11754", "authors": ["Zihui Zhang", "Yafei Yang", "Hongtao Wen", "Bo Yang"], "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS", "summary": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12108", "pdf": "https://arxiv.org/pdf/2504.12108", "abs": "https://arxiv.org/abs/2504.12108", "authors": ["Shizhan Cai", "Liang Ding", "Dacheng Tao"], "title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11773", "pdf": "https://arxiv.org/pdf/2504.11773", "abs": "https://arxiv.org/abs/2504.11773", "authors": ["Yiran Wang", "Jiaqi Li", "Chaoyi Hong", "Ruibo Li", "Liusheng Sun", "Xiao Song", "Zhe Wang", "Zhiguo Cao", "Guosheng Lin"], "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Oral Presentation)", "summary": "Radar-Camera depth estimation aims to predict dense and accurate metric depth\nby fusing input images and Radar data. Model efficiency is crucial for this\ntask in pursuit of real-time processing on autonomous vehicles and robotic\nplatforms. However, due to the sparsity of Radar returns, the prevailing\nmethods adopt multi-stage frameworks with intermediate quasi-dense depth, which\nare time-consuming and not robust. To address these challenges, we propose\nTacoDepth, an efficient and accurate Radar-Camera depth estimation model with\none-stage fusion. Specifically, the graph-based Radar structure extractor and\nthe pyramid-based Radar fusion module are designed to capture and integrate the\ngraph structures of Radar point clouds, delivering superior model efficiency\nand robustness without relying on the intermediate depth results. Moreover,\nTacoDepth can be flexible for different inference modes, providing a better\nbalance of speed and accuracy. Extensive experiments are conducted to\ndemonstrate the efficacy of our method. Compared with the previous\nstate-of-the-art approach, TacoDepth improves depth accuracy and processing\nspeed by 12.8% and 91.8%. Our work provides a new perspective on efficient\nRadar-Camera depth estimation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12172", "pdf": "https://arxiv.org/pdf/2504.12172", "abs": "https://arxiv.org/abs/2504.12172", "authors": ["Maged S. Al-Shaibani", "Zaid Alyafeai", "Irfan Ahmad"], "title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11779", "pdf": "https://arxiv.org/pdf/2504.11779", "abs": "https://arxiv.org/abs/2504.11779", "authors": ["Qishun Wang", "Zhengzheng Tu", "Chenglong Li", "Bo Jiang"], "title": "Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of\ntraditional RGB-based VOD in challenging lighting conditions, making it more\npractical and effective in many applications.\n  However, similar to most RGBT fusion tasks, it still mainly relies on\nmanually aligned multimodal image pairs.\n  In this paper, we propose a novel Multimodal Spatio-temporal Graph learning\nNetwork (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust\ngraph representation learning model.\n  Specifically, we first design an Adaptive Partitioning Layer (APL) to\nestimate the corresponding regions of the Thermal image within the RGB image\n(high-resolution), achieving a preliminary inexact alignment.\n  Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which\nemploys a sparse information passing mechanism on the estimated inexact\nalignment to achieve reliable information interaction between different\nmodalities.\n  Moreover, to fully exploit the temporal cues for RGBT VOD problem, we\nintroduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal\nSparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM\naims to filter out some redundant information between adjacent frames by\nemploying the sparse aggregation mechanism on the temporal graph. Meanwhile,\nTSB is dedicated to achieving the complementary learning of local spatial\nrelationships.\n  Extensive comparative experiments conducted on both the aligned dataset\nVT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness\nand superiority of our proposed method. Our project will be made available on\nour website for free public access.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12180", "pdf": "https://arxiv.org/pdf/2504.12180", "abs": "https://arxiv.org/abs/2504.12180", "authors": ["Jaime E. Cuellar", "Oscar Moreno-Martinez", "Paula Sofia Torres-Rodriguez", "Jaime Andres Pavlich-Mariscal", "Andres Felipe Mican-Castiblanco", "Juan Guillermo Torres-Hurtado"], "title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification", "categories": ["cs.CL", "cs.AI"], "comment": "in Spanish language", "summary": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11781", "pdf": "https://arxiv.org/pdf/2504.11781", "abs": "https://arxiv.org/abs/2504.11781", "authors": ["Guanchun Wang", "Xiangrong Zhang", "Yifei Zhang", "Zelin Peng", "Tianyang Zhang", "Xu Tang", "Licheng Jiao"], "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figures", "summary": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11786", "pdf": "https://arxiv.org/pdf/2504.11786", "abs": "https://arxiv.org/abs/2504.11786", "authors": ["Sang-Jun Park", "Keun-Soo Heo", "Dong-Hee Shin", "Young-Han Son", "Ji-Hye Oh", "Tae-Eui Kam"], "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation", "categories": ["cs.CV"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "The automatic generation of radiology reports has emerged as a promising\nsolution to reduce a time-consuming task and accurately capture critical\ndisease-relevant findings in X-ray images. Previous approaches for radiology\nreport generation have shown impressive performance. However, there remains\nsignificant potential to improve accuracy by ensuring that retrieved reports\ncontain disease-relevant findings similar to those in the X-ray images and by\nrefining generated reports. In this study, we propose a Disease-aware\nimage-text Alignment and self-correcting Re-alignment for Trustworthy radiology\nreport generation (DART) framework. In the first stage, we generate initial\nreports based on image-to-text retrieval with disease-matching, embedding both\nimages and texts in a shared embedding space through contrastive learning. This\napproach ensures the retrieval of reports with similar disease-relevant\nfindings that closely align with the input X-ray images. In the second stage,\nwe further enhance the initial reports by introducing a self-correction module\nthat re-aligns them with the X-ray images. Our proposed framework achieves\nstate-of-the-art results on two widely used benchmarks, surpassing previous\napproaches in both report generation and clinical efficacy metrics, thereby\nenhancing the trustworthiness of radiology reports.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11820", "pdf": "https://arxiv.org/pdf/2504.11820", "abs": "https://arxiv.org/abs/2504.11820", "authors": ["Delong Suzhang", "Meng Yang"], "title": "Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12285", "pdf": "https://arxiv.org/pdf/2504.12285", "abs": "https://arxiv.org/abs/2504.12285", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "title": "BitNet b1.58 2B4T Technical Report", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11460", "pdf": "https://arxiv.org/pdf/2504.11460", "abs": "https://arxiv.org/abs/2504.11460", "authors": ["Tobias Hallmen", "Robin-Nico Kampa", "Fabian Deuser", "Norbert Oswald", "Elisabeth Andr√©"], "title": "Semantic Matters: Multimodal Features for Affective Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this study, we present our methodology for two tasks: the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry\nIntensity (EMI) Estimation Challenge, both conducted as part of the 8th\nWorkshop and Competition on Affective & Behavior Analysis in-the-wild. Building\non previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast\ndataset to extract various audio features, capturing both linguistic and\nparalinguistic information. Our approach incorporates a\nvalence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like\nencoder, and a vision transformer (ViT) with predictions subsequently processed\nthrough a long short-term memory (LSTM) architecture for temporal modeling. In\nthis iteration, we integrate the textual and visual modality into our analysis,\nrecognizing that semantic content provides valuable contextual cues and\nunderscoring that the meaning of speech often conveys more critical insights\nthan its acoustic counterpart alone. Fusing in the vision modality helps in\nsome cases to interpret the textual modality more precisely. This combined\napproach yields significant performance improvements over baseline methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11858", "pdf": "https://arxiv.org/pdf/2504.11858", "abs": "https://arxiv.org/abs/2504.11858", "authors": ["Jo√´l Mathys", "Andreas Plesner", "Jorel Elmiger", "Roger Wattenhofer"], "title": "Synthetic Data for Blood Vessel Network Extraction", "categories": ["cs.CV"], "comment": "Presented at SynthData Workshop at ICLR 2025", "summary": "Blood vessel networks in the brain play a crucial role in stroke research,\nwhere understanding their topology is essential for analyzing blood flow\ndynamics. However, extracting detailed topological vessel network information\nfrom microscopy data remains a significant challenge, mainly due to the\nscarcity of labeled training data and the need for high topological accuracy.\nThis work combines synthetic data generation with deep learning to\nautomatically extract vessel networks as graphs from volumetric microscopy\ndata. To combat data scarcity, we introduce a comprehensive pipeline for\ngenerating large-scale synthetic datasets that mirror the characteristics of\nreal vessel networks. Our three-stage approach progresses from abstract graph\ngeneration through vessel mask creation to realistic medical image synthesis,\nincorporating biological constraints and imaging artifacts at each stage. Using\nthis synthetic data, we develop a two-stage deep learning pipeline of 3D\nU-Net-based models for node detection and edge prediction. Fine-tuning on real\nmicroscopy data shows promising adaptation, improving edge prediction F1 scores\nfrom 0.496 to 0.626 by training on merely 5 manually labeled samples. These\nresults suggest that automated vessel network extraction is becoming\npractically feasible, opening new possibilities for large-scale vascular\nanalysis in stroke research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11524", "pdf": "https://arxiv.org/pdf/2504.11524", "abs": "https://arxiv.org/abs/2504.11524", "authors": ["Haokun Liu", "Sicong Huang", "Jingyu Hu", "Yangqiaoyu Zhou", "Chenhao Tan"], "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, website link:\n  https://chicagohai.github.io/HypoBench/", "summary": "There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11896", "pdf": "https://arxiv.org/pdf/2504.11896", "abs": "https://arxiv.org/abs/2504.11896", "authors": ["Xingxing Yang", "Jie Chen", "Zaifeng Yang"], "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11889", "pdf": "https://arxiv.org/pdf/2504.11889", "abs": "https://arxiv.org/abs/2504.11889", "authors": ["Donghee Han", "Hwanjun Song", "Mun Yong Yi"], "title": "Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11914", "pdf": "https://arxiv.org/pdf/2504.11914", "abs": "https://arxiv.org/abs/2504.11914", "authors": ["Yuhao Chao", "Jie Liu", "Jie Tang", "Gangshan Wu"], "title": "AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Industrial Anomaly Detection (IAD) poses a formidable challenge due to the\nscarcity of defective samples, making it imperative to deploy models capable of\nrobust generalization to detect unseen anomalies effectively. Traditional\napproaches, often constrained by hand-crafted features or domain-specific\nexpert models, struggle to address this limitation, underscoring the need for a\nparadigm shift. We introduce AnomalyR1, a pioneering framework that leverages\nVLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional\ngeneralization and interpretability, to revolutionize IAD. By integrating MLLM\nwith Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned\nOutcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution\nthat autonomously processes inputs of image and domain knowledge, reasons\nthrough analysis, and generates precise anomaly localizations and masks. Based\non the latest multimodal IAD benchmark, our compact 3-billion-parameter model\noutperforms existing methods, establishing state-of-the-art results. As MLLM\ncapabilities continue to advance, this study is the first to deliver an\nend-to-end VLM-based IAD solution that demonstrates the transformative\npotential of ROAM-enhanced GRPO, positioning our framework as a forward-looking\ncornerstone for next-generation intelligent anomaly detection systems in\nindustrial applications with limited defective data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12137", "pdf": "https://arxiv.org/pdf/2504.12137", "abs": "https://arxiv.org/abs/2504.12137", "authors": ["Laura Fieback", "Nishilkumar Balar", "Jakob Spiegelberg", "Hanno Gottschalk"], "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11946", "pdf": "https://arxiv.org/pdf/2504.11946", "abs": "https://arxiv.org/abs/2504.11946", "authors": ["Haoyang Wang", "Liming Liu", "Peiheng Wang", "Junlin Hao", "Jiangkai Wu", "Xinggong Zhang"], "title": "R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Mesh reconstruction from multi-view images is a fundamental problem in\ncomputer vision, but its performance degrades significantly under sparse-view\nconditions, especially in unseen regions where no ground-truth observations are\navailable. While recent advances in diffusion models have demonstrated strong\ncapabilities in synthesizing novel views from limited inputs, their outputs\noften suffer from visual artifacts and lack 3D consistency, posing challenges\nfor reliable mesh optimization. In this paper, we propose a novel framework\nthat leverages diffusion models to enhance sparse-view mesh reconstruction in a\nprincipled and reliable manner. To address the instability of diffusion\noutputs, we propose a Consensus Diffusion Module that filters unreliable\ngenerations via interquartile range (IQR) analysis and performs variance-aware\nimage fusion to produce robust pseudo-supervision. Building on this, we design\nan online reinforcement learning strategy based on the Upper Confidence Bound\n(UCB) to adaptively select the most informative viewpoints for enhancement,\nguided by diffusion loss. Finally, the fused images are used to jointly\nsupervise a NeRF-based model alongside sparse-view ground truth, ensuring\nconsistency across both geometry and appearance. Extensive experiments\ndemonstrate that our method achieves significant improvements in both geometric\nquality and rendering quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11949", "pdf": "https://arxiv.org/pdf/2504.11949", "abs": "https://arxiv.org/abs/2504.11949", "authors": ["Jie Wang", "Chen Ye Gan", "Caoqi Wei", "Jiangtao Wen", "Yuxing Han"], "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12020", "pdf": "https://arxiv.org/pdf/2504.12020", "abs": "https://arxiv.org/abs/2504.12020", "authors": ["Shiwei Gan", "Yafeng Yin", "Zhiwei Jiang", "Hongkai Wen", "Lei Xie", "Sanglu Lu"], "title": "MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (T-PAMI). This is a regular paper\n  submission", "summary": "Recent advances in sign language research have benefited from CNN-based\nbackbones, which are primarily transferred from traditional computer vision\ntasks (\\eg object identification, image recognition). However, these CNN-based\nbackbones usually excel at extracting features like contours and texture, but\nmay struggle with capturing sign-related features. In fact, sign language tasks\nrequire focusing on sign-related regions, including the collaboration between\ndifferent regions (\\eg left hand region and right hand region) and the\neffective content in a single region. To capture such region-related features,\nwe introduce MixSignGraph, which represents sign sequences as a group of mixed\ngraphs and designs the following three graph modules for feature extraction,\n\\ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and\nHierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the\ncorrelation of intra-frame cross-region features within one frame, \\ie focusing\non spatial features. The TSG module tracks the interaction of inter-frame\ncross-region features among adjacent frames, \\ie focusing on temporal features.\nThe HSG module aggregates the same-region features from different-granularity\nfeature maps of a frame, \\ie focusing on hierarchical features. In addition, to\nfurther improve the performance of sign language tasks without gloss\nannotations, we propose a simple yet counter-intuitive Text-driven CTC\nPre-training (TCP) method, which generates pseudo gloss labels from text labels\nfor model pre-training. Extensive experiments conducted on current five public\nsign language datasets demonstrate the superior performance of the proposed\nmodel. Notably, our model surpasses the SOTA models on multiple sign language\ntasks across several datasets, without relying on any additional cues.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12029", "pdf": "https://arxiv.org/pdf/2504.12029", "abs": "https://arxiv.org/abs/2504.12029", "authors": ["Bingjie Gao", "Bo Zhang", "Li Niu"], "title": "Object Placement for Anything", "categories": ["cs.CV"], "comment": "accepted by ICME 2025", "summary": "Object placement aims to determine the appropriate placement (\\emph{e.g.},\nlocation and size) of a foreground object when placing it on the background\nimage. Most previous works are limited by small-scale labeled dataset, which\nhinders the real-world application of object placement. In this work, we devise\na semi-supervised framework which can exploit large-scale unlabeled dataset to\npromote the generalization ability of discriminative object placement models.\nThe discriminative models predict the rationality label for each foreground\nplacement given a foreground-background pair. To better leverage the labeled\ndata, under the semi-supervised framework, we further propose to transfer the\nknowledge of rationality variation, \\emph{i.e.}, whether the change of\nforeground placement would result in the change of rationality label, from\nlabeled data to unlabeled data. Extensive experiments demonstrate that our\nframework can effectively enhance the generalization ability of discriminative\nobject placement models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12045", "pdf": "https://arxiv.org/pdf/2504.12045", "abs": "https://arxiv.org/abs/2504.12045", "authors": ["Jonas Myhre Schi√∏tt", "Viktor Sebastian Petersen", "Dimitrios P. Papadopoulos"], "title": "pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild", "categories": ["cs.CV", "cs.LG", "I.2.1; I.4.6"], "comment": "15 pages, 7 figures, to be published in SCIA 2025", "summary": "Computer vision models have seen increased usage in sports, and reinforcement\nlearning (RL) is famous for beating humans in strategic games such as Chess and\nGo. In this paper, we are interested in building upon these advances and\nexamining the game of classic 8-ball pool. We introduce pix2pockets, a\nfoundation for an RL-assisted pool coach. Given a single image of a pool table,\nwe first aim to detect the table and the balls and then propose the optimal\nshot suggestion. For the first task, we build a dataset with 195 diverse images\nwhere we manually annotate all balls and table dots, leading to 5748 object\nsegmentation masks. For the second task, we build a standardized RL environment\nthat allows easy development and benchmarking of any RL algorithm. Our object\ndetection model yields an AP50 of 91.2 while our ball location pipeline obtains\nan error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set\na baseline for the shot suggestion task and we show that all of them fail to\npocket all balls without making a foul move. We also present a simple baseline\nthat achieves a per-shot success rate of 94.7% and clears a full game in a\nsingle turn 30% of the time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12083", "pdf": "https://arxiv.org/pdf/2504.12083", "abs": "https://arxiv.org/abs/2504.12083", "authors": ["Pritam Sarkar", "Ali Etemad"], "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12088", "pdf": "https://arxiv.org/pdf/2504.12088", "abs": "https://arxiv.org/abs/2504.12088", "authors": ["Mirza Samad Ahmed Baig", "Syeda Anshrah Gillani", "Abdul Akbar Khan", "Shahid Munir Shah"], "title": "AttentionDrop: A Novel Regularization Method for Transformer Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "26 pages", "summary": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12100", "pdf": "https://arxiv.org/pdf/2504.12100", "abs": "https://arxiv.org/abs/2504.12100", "authors": ["Kaifeng Gao", "Siqi Chen", "Hanwang Zhang", "Jun Xiao", "Yueting Zhuang", "Qianru Sun"], "title": "Generalized Visual Relation Detection with Diffusion Models", "categories": ["cs.CV"], "comment": "Under review at IEEE TCSVT. The Appendix is provided additionally", "summary": "Visual relation detection (VRD) aims to identify relationships (or\ninteractions) between object pairs in an image. Although recent VRD models have\nachieved impressive performance, they are all restricted to pre-defined\nrelation categories, while failing to consider the semantic ambiguity\ncharacteristic of visual relations. Unlike objects, the appearance of visual\nrelations is always subtle and can be described by multiple predicate words\nfrom different perspectives, e.g., ``ride'' can be depicted as ``race'' and\n``sit on'', from the sports and spatial position views, respectively. To this\nend, we propose to model visual relations as continuous embeddings, and design\ndiffusion models to achieve generalized VRD in a conditional generative manner,\ntermed Diff-VRD. We model the diffusion process in a latent space and generate\nall possible relations in the image as an embedding sequence. During the\ngeneration, the visual and text embeddings of subject-object pairs serve as\nconditional signals and are injected via cross-attention. After the generation,\nwe design a subsequent matching stage to assign the relation words to\nsubject-object pairs by considering their semantic similarities. Benefiting\nfrom the diffusion-based generative process, our Diff-VRD is able to generate\nvisual relations beyond the pre-defined category labels of datasets. To\nproperly evaluate this generalized VRD task, we introduce two evaluation\nmetrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image\ncaptioning. Extensive experiments in both human-object interaction (HOI)\ndetection and scene graph generation (SGG) benchmarks attest to the superiority\nand effectiveness of Diff-VRD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12104", "pdf": "https://arxiv.org/pdf/2504.12104", "abs": "https://arxiv.org/abs/2504.12104", "authors": ["Shuo Li", "Fang Liu", "Zehua Hao", "Xinyi Wang", "Lingling Li", "Xu Liu", "Puhua Chen", "Wenping Ma"], "title": "Logits DeConfusion with CLIP for Few-Shot Learning", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "With its powerful visual-language alignment capability, CLIP performs well in\nzero-shot and few-shot learning tasks. However, we found in experiments that\nCLIP's logits suffer from serious inter-class confusion problems in downstream\ntasks, and the ambiguity between categories seriously affects the accuracy. To\naddress this challenge, we propose a novel method called Logits DeConfusion,\nwhich effectively learns and eliminates inter-class confusion in logits by\ncombining our Multi-level Adapter Fusion (MAF) module with our Inter-Class\nDeconfusion (ICD) module. Our MAF extracts features from different levels and\nfuses them uniformly to enhance feature representation. Our ICD learnably\neliminates inter-class confusion in logits with a residual structure.\nExperimental results show that our method can significantly improve the\nclassification performance and alleviate the inter-class confusion problem. The\ncode is available at https://github.com/LiShuo1001/LDC.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12112", "pdf": "https://arxiv.org/pdf/2504.12112", "abs": "https://arxiv.org/abs/2504.12112", "authors": ["Zhenyu Yu", "Mohd Yamani Inda Idris", "Pei Wang"], "title": "A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Remote sensing imagery is essential for environmental monitoring,\nagricultural management, and disaster response. However, data loss due to cloud\ncover, sensor failures, or incomplete acquisition-especially in high-resolution\nand high-frequency tasks-severely limits satellite imagery's effectiveness.\nTraditional interpolation methods struggle with large missing areas and complex\nstructures. Remote sensing imagery consists of multiple bands, each with\ndistinct meanings, and ensuring consistency across bands is critical to avoid\nanomalies in the combined images. This paper proposes SatelliteMaker, a\ndiffusion-based method that reconstructs missing data across varying levels of\ndata loss while maintaining spatial, spectral, and temporal consistency. We\nalso propose Digital Elevation Model (DEM) as a conditioning input and use\ntailored prompts to generate realistic images, making diffusion models\napplicable to quantitative remote sensing tasks. Additionally, we propose a\nVGG-Adapter module based on Distribution Loss, which reduces distribution\ndiscrepancy and ensures style consistency. Extensive experiments show that\nSatelliteMaker achieves state-of-the-art performance across multiple tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12137", "pdf": "https://arxiv.org/pdf/2504.12137", "abs": "https://arxiv.org/abs/2504.12137", "authors": ["Laura Fieback", "Nishilkumar Balar", "Jakob Spiegelberg", "Hanno Gottschalk"], "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12169", "pdf": "https://arxiv.org/pdf/2504.12169", "abs": "https://arxiv.org/abs/2504.12169", "authors": ["Joanne Lin", "Crispian Morris", "Ruirui Lin", "Fan Zhang", "David Bull", "Nantheera Anantrasirichai"], "title": "Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video Pipeline", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Low-light conditions pose significant challenges for both human and machine\nannotation. This in turn has led to a lack of research into machine\nunderstanding for low-light images and (in particular) videos. A common\napproach is to apply annotations obtained from high quality datasets to\nsynthetically created low light versions. In addition, these approaches are\noften limited through the use of unrealistic noise models. In this paper, we\npropose a new Degradation Estimation Network (DEN), which synthetically\ngenerates realistic standard RGB (sRGB) noise without the requirement for\ncamera metadata. This is achieved by estimating the parameters of\nphysics-informed noise distributions, trained in a self-supervised manner. This\nzero-shot approach allows our method to generate synthetic noisy content with a\ndiverse range of realistic noise characteristics, unlike other methods which\nfocus on recreating the noise characteristics of the training data. We evaluate\nour proposed synthetic pipeline using various methods trained on its synthetic\ndata for typical low-light tasks including synthetic noise replication, video\nenhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\%\nLPIPS, and 62\\% AP$_{50-95}$, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12186", "pdf": "https://arxiv.org/pdf/2504.12186", "abs": "https://arxiv.org/abs/2504.12186", "authors": ["Alejandro Newell", "Peiyun Hu", "Lahav Lipson", "Stephan R. Richter", "Vladlen Koltun"], "title": "CoMotion: Concurrent Multi-person 3D Motion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025, for code and weights go to\n  https://github.com/apple/ml-comotion", "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12197", "pdf": "https://arxiv.org/pdf/2504.12197", "abs": "https://arxiv.org/abs/2504.12197", "authors": ["Mahdi Alehdaghi", "Rajarshi Bhattacharya", "Pourya Shamsolmoali", "Rafael M. O. Cruz", "Maguelonne Heritier", "Eric Granger"], "title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has provided considerable advancements for multimedia systems,\nyet the interpretability of deep models remains a challenge. State-of-the-art\npost-hoc explainability methods, such as GradCAM, provide visual interpretation\nbased on heatmaps but lack conceptual clarity. Prototype-based approaches, like\nProtoPNet and PIPNet, offer a more structured explanation but rely on fixed\npatches, limiting their robustness and semantic consistency.\n  To address these limitations, a part-prototypical concept mining network\n(PCMNet) is proposed that dynamically learns interpretable prototypes from\nmeaningful regions. PCMNet clusters prototypes into concept groups, creating\nsemantically grounded explanations without requiring additional annotations.\nThrough a joint process of unsupervised part discovery and concept activation\nvector extraction, PCMNet effectively captures discriminative concepts and\nmakes interpretable classification decisions.\n  Our extensive experiments comparing PCMNet against state-of-the-art methods\non multiple datasets show that it can provide a high level of interpretability,\nstability, and robustness under clean and occluded scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12222", "pdf": "https://arxiv.org/pdf/2504.12222", "abs": "https://arxiv.org/abs/2504.12222", "authors": ["Yike Liu", "Jianhui Zhang", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "Coding-Prior Guided Diffusion Network for Video Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "While recent video deblurring methods have advanced significantly, they often\noverlook two valuable prior information: (1) motion vectors (MVs) and coding\nresiduals (CRs) from video codecs, which provide efficient inter-frame\nalignment cues, and (2) the rich real-world knowledge embedded in pre-trained\ndiffusion generative models. We present CPGDNet, a novel two-stage framework\nthat effectively leverages both coding priors and generative diffusion priors\nfor high-quality deblurring. First, our coding-prior feature propagation (CPFP)\nmodule utilizes MVs for efficient frame alignment and CRs to generate attention\nmasks, addressing motion inaccuracies and texture variations. Second, a\ncoding-prior controlled generation (CPC) module network integrates coding\npriors into a pretrained diffusion model, guiding it to enhance critical\nregions and synthesize realistic details. Experiments demonstrate our method\nachieves state-of-the-art perceptual quality with up to 30% improvement in IQA\nmetrics. Both the code and the codingprior-augmented dataset will be\nopen-sourced.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12245", "pdf": "https://arxiv.org/pdf/2504.12245", "abs": "https://arxiv.org/abs/2504.12245", "authors": ["Xia Wang", "Haiyang Sun", "Tiantian Cao", "Yueying Sun", "Min Feng"], "title": "SIDME: Self-supervised Image Demoir√©ing via Masked Encoder-Decoder Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "21 pages, 13 figures", "summary": "Moir\\'e patterns, resulting from aliasing between object light signals and\ncamera sampling frequencies, often degrade image quality during capture.\nTraditional demoir\\'eing methods have generally treated images as a whole for\nprocessing and training, neglecting the unique signal characteristics of\ndifferent color channels. Moreover, the randomness and variability of moir\\'e\npattern generation pose challenges to the robustness of existing methods when\napplied to real-world data. To address these issues, this paper presents SIDME\n(Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction),\na novel model designed to generate high-quality visual images by effectively\nprocessing moir\\'e patterns. SIDME combines a masked encoder-decoder\narchitecture with self-supervised learning, allowing the model to reconstruct\nimages using the inherent properties of camera sampling frequencies. A key\ninnovation is the random masked image reconstructor, which utilizes an\nencoder-decoder structure to handle the reconstruction task. Furthermore, since\nthe green channel in camera sampling has a higher sampling frequency compared\nto red and blue channels, a specialized self-supervised loss function is\ndesigned to improve the training efficiency and effectiveness. To ensure the\ngeneralization ability of the model, a self-supervised moir\\'e image generation\nmethod has been developed to produce a dataset that closely mimics real-world\nconditions. Extensive experiments demonstrate that SIDME outperforms existing\nmethods in processing real moir\\'e pattern data, showing its superior\ngeneralization performance and robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12264", "pdf": "https://arxiv.org/pdf/2504.12264", "abs": "https://arxiv.org/abs/2504.12264", "authors": ["Ayca Takmaz", "Cristiano Saltori", "Neehar Peri", "Tim Meinhardt", "Riccardo de Lutio", "Laura Leal-Taix√©", "Aljo≈°a O≈°ep"], "title": "Towards Learning to Complete Anything in Lidar", "categories": ["cs.CV"], "comment": null, "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.12292", "pdf": "https://arxiv.org/pdf/2504.12292", "abs": "https://arxiv.org/abs/2504.12292", "authors": ["Liam Schoneveld", "Zhe Chen", "Davide Davoli", "Jiapeng Tang", "Saimon Terazawa", "Ko Nishino", "Matthias Nie√üner"], "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/", "summary": "Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11469", "pdf": "https://arxiv.org/pdf/2504.11469", "abs": "https://arxiv.org/abs/2504.11469", "authors": ["Guillaume Garret", "Antoine Vacavant", "Carole Frindel"], "title": "Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Open access version of an article submitted to Medical Image\n  Understanding and Analysis (MIUA) 2025", "summary": "Deep learning models have achieved impressive performance in medical image\nsegmentation, yet their black-box nature limits clinical adoption. In vascular\napplications, trustworthy segmentation should rely on both local image cues and\nglobal anatomical structures, such as vessel connectivity or branching.\nHowever, the extent to which models leverage such global context remains\nunclear. We present a novel explainability pipeline for 3D vessel segmentation,\ncombining gradient-based attribution with graph-guided point selection and a\nblob-based analysis of Saliency maps. Using vascular graphs extracted from\nground truth, we define anatomically meaningful points of interest (POIs) and\nassess the contribution of input voxels via Saliency maps. These are analyzed\nat both global and local scales using a custom blob detector. Applied to IRCAD\nand Bullitt datasets, our analysis shows that model decisions are dominated by\nhighly localized attribution blobs centered near POIs. Attribution features\nshow little correlation with vessel-level properties such as thickness,\ntubularity, or connectivity -- suggesting limited use of global anatomical\nreasoning. Our results underline the importance of structured explainability\ntools and highlight the current limitations of segmentation models in capturing\nglobal vascular context.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11474", "pdf": "https://arxiv.org/pdf/2504.11474", "abs": "https://arxiv.org/abs/2504.11474", "authors": ["Byunggun Kim", "Younghun Kwon"], "title": "Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of\nthe common mental diseases discovered not only in children but also in adults.\nIn this context, we propose a ADHD diagnosis transformer model that can\neffectively simultaneously find important brain spatiotemporal biomarkers from\nresting-state functional magnetic resonance (rs-fMRI). This model not only\nlearns spatiotemporal individual features but also learns the correlation with\nfull attention structures specialized in ADHD diagnosis. In particular, it\nfocuses on learning local blood oxygenation level dependent (BOLD) signals and\ndistinguishing important regions of interest (ROI) in the brain. Specifically,\nthe three proposed methods for ADHD diagnosis transformer are as follows.\nFirst, we design a CNN-based embedding block to obtain more expressive\nembedding features in brain region attention. It is reconstructed based on the\npreviously CNN-based ADHD diagnosis models for the transformer. Next, for\nindividual spatiotemporal feature attention, we change the attention method to\nlocal temporal attention and ROI-rank based masking. For the temporal features\nof fMRI, the local temporal attention enables to learn local BOLD signal\nfeatures with only simple window masking. For the spatial feature of fMRI,\nROI-rank based masking can distinguish ROIs with high correlation in ROI\nrelationships based on attention scores, thereby providing a more specific\nbiomarker for ADHD diagnosis. The experiment was conducted with various types\nof transformer models. To evaluate these models, we collected the data from 939\nindividuals from all sites provided by the ADHD-200 competition. Through this,\nthe spatiotemporal enhanced transformer for ADHD diagnosis outperforms the\nperformance of other different types of transformer variants. (77.78ACC\n76.60SPE 79.22SEN 79.30AUC)", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11698", "pdf": "https://arxiv.org/pdf/2504.11698", "abs": "https://arxiv.org/abs/2504.11698", "authors": ["Xingwu Ji", "Haochen Niu", "Dexin Duan", "Rendong Ying", "Fei Wen", "Peilin Liu"], "title": "An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World", "categories": ["cs.RO", "cs.CV"], "comment": "11 pages, 14 figures", "summary": "Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11923", "pdf": "https://arxiv.org/pdf/2504.11923", "abs": "https://arxiv.org/abs/2504.11923", "authors": ["Zeyu Dai", "Shengcai Liu", "Rui He", "Jiahao Wu", "Ning Lu", "Wenqi Fan", "Qing Li", "Ke Tang"], "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
{"id": "2504.11992", "pdf": "https://arxiv.org/pdf/2504.11992", "abs": "https://arxiv.org/abs/2504.11992", "authors": ["Pascal Schlachter", "Jonathan Fuss", "Bin Yang"], "title": "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025)", "summary": "A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-17.jsonl"}
