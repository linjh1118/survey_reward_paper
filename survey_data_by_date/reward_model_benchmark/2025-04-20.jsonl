{"id": "2504.12696", "pdf": "https://arxiv.org/pdf/2504.12696", "abs": "https://arxiv.org/abs/2504.12696", "authors": ["Naibang Wang", "Deyong Shang", "Yan Gong", "Xiaoxi Hu", "Ziying Song", "Lei Yang", "Yuhan Huang", "Xiaoyu Wang", "Jianli Lu"], "title": "Collaborative Perception Datasets for Autonomous Driving: A Review", "categories": ["cs.CV"], "comment": "18pages, 7figures, journal", "summary": "Collaborative perception has attracted growing interest from academia and\nindustry due to its potential to enhance perception accuracy, safety, and\nrobustness in autonomous driving through multi-agent information fusion. With\nthe advancement of Vehicle-to-Everything (V2X) communication, numerous\ncollaborative perception datasets have emerged, varying in cooperation\nparadigms, sensor configurations, data sources, and application scenarios.\nHowever, the absence of systematic summarization and comparative analysis\nhinders effective resource utilization and standardization of model evaluation.\nAs the first comprehensive review focused on collaborative perception datasets,\nthis work reviews and compares existing resources from a multi-dimensional\nperspective. We categorize datasets based on cooperation paradigms, examine\ntheir data sources and scenarios, and analyze sensor modalities and supported\ntasks. A detailed comparative analysis is conducted across multiple dimensions.\nWe also outline key challenges and future directions, including dataset\nscalability, diversity, domain adaptation, standardization, privacy, and the\nintegration of large language models. To support ongoing research, we provide a\ncontinuously updated online repository of collaborative perception datasets and\nrelated literature:\nhttps://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "accuracy", "summarization", "multi-dimensional"], "score": 6}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068", "abs": "https://arxiv.org/abs/2504.13068", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "agreement", "kappa", "accuracy"], "score": 5}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867", "abs": "https://arxiv.org/abs/2504.12867", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "reliability", "fine-grained"], "score": 5}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12312", "pdf": "https://arxiv.org/pdf/2504.12312", "abs": "https://arxiv.org/abs/2504.12312", "authors": ["Zihao Xu", "Junchen Ding", "Yiling Lou", "Kun Zhang", "Dong Gong", "Yuekang Li"], "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12314", "pdf": "https://arxiv.org/pdf/2504.12314", "abs": "https://arxiv.org/abs/2504.12314", "authors": ["Hao Li", "Liuzhenghao Lv", "He Cao", "Zijing Liu", "Zhiyuan Yan", "Yu Wang", "Yonghong Tian", "Yu Li", "Li Yuan"], "title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large language models are increasingly used in scientific domains, especially\nfor molecular understanding and analysis. However, existing models are affected\nby hallucination issues, resulting in errors in drug design and utilization. In\nthis paper, we first analyze the sources of hallucination in LLMs for molecular\ncomprehension tasks, specifically the knowledge shortcut phenomenon observed in\nthe PubChem dataset. To evaluate hallucination in molecular comprehension tasks\nwith computational efficiency, we introduce \\textbf{Mol-Hallu}, a novel\nfree-form evaluation metric that quantifies the degree of hallucination based\non the scientific entailment relationship between generated text and actual\nmolecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze\nthe extent of hallucination in various LLMs performing molecular comprehension\ntasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is\nproposed to alleviate molecular hallucinations, Experiments show the\neffectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our\nfindings provide critical insights into mitigating hallucination and improving\nthe reliability of LLMs in scientific applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12513", "pdf": "https://arxiv.org/pdf/2504.12513", "abs": "https://arxiv.org/abs/2504.12513", "authors": ["Chaitanya Patel", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "AdaVid: Adaptive Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025. Project Page: https://chaitanya100100.github.io/AdaVid/", "summary": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "dimension"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12599", "pdf": "https://arxiv.org/pdf/2504.12599", "abs": "https://arxiv.org/abs/2504.12599", "authors": ["Wenxin Chen", "Mengxue Qu", "Weitai Kang", "Yan Yan", "Yao Zhao", "Yunchao Wei"], "title": "3DResT: A Strong Baseline for Semi-Supervised 3D Referring Expression Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D Referring Expression Segmentation (3D-RES) typically requires extensive\ninstance-level annotations, which are time-consuming and costly.\nSemi-supervised learning (SSL) mitigates this by using limited labeled data\nalongside abundant unlabeled data, improving performance while reducing\nannotation costs. SSL uses a teacher-student paradigm where teacher generates\nhigh-confidence-filtered pseudo-labels to guide student. However, in the\ncontext of 3D-RES, where each label corresponds to a single mask and labeled\ndata is scarce, existing SSL methods treat high-quality pseudo-labels merely as\nauxiliary supervision, which limits the model's learning potential. The\nreliance on high-confidence thresholds for filtering often results in\npotentially valuable pseudo-labels being discarded, restricting the model's\nability to leverage the abundant unlabeled data. Therefore, we identify two\ncritical challenges in semi-supervised 3D-RES, namely, inefficient utilization\nof high-quality pseudo-labels and wastage of useful information from\nlow-quality pseudo-labels. In this paper, we introduce the first\nsemi-supervised learning framework for 3D-RES, presenting a robust baseline\nmethod named 3DResT. To address these challenges, we propose two novel designs\ncalled Teacher-Student Consistency-Based Sampling (TSCS) and Quality-Driven\nDynamic Weighting (QDW). TSCS aids in the selection of high-quality\npseudo-labels, integrating them into the labeled dataset to strengthen the\nlabeled supervision signals. QDW preserves low-quality pseudo-labels by\ndynamically assigning them lower weights, allowing for the effective extraction\nof useful information rather than discarding them. Extensive experiments\nconducted on the widely used benchmark demonstrate the effectiveness of our\nmethod. Notably, with only 1% labeled data, 3DResT achieves an mIoU improvement\nof 8.34 points compared to the fully supervised method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "consistency"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12333", "pdf": "https://arxiv.org/pdf/2504.12333", "abs": "https://arxiv.org/abs/2504.12333", "authors": ["Andr√©s Isaza-Giraldo", "Paulo Bala", "Lucas Pereira"], "title": "Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "2nd HEAL Workshop at CHI Conference on Human Factors in Computing\n  Systems. April 26, 2025. Yokohama, Japan", "summary": "The evaluation of open-ended responses in serious games presents a unique\nchallenge, as correctness is often subjective. Large Language Models (LLMs) are\nincreasingly being explored as evaluators in such contexts, yet their accuracy\nand consistency remain uncertain, particularly for smaller models intended for\nlocal execution. This study investigates the reliability of five small-scale\nLLMs when assessing player responses in \\textit{En-join}, a game that simulates\ndecision-making within energy communities. By leveraging traditional binary\nclassification metrics (including accuracy, true positive rate, and true\nnegative rate), we systematically compare these models across different\nevaluation scenarios. Our results highlight the strengths and limitations of\neach model, revealing trade-offs between sensitivity, specificity, and overall\nperformance. We demonstrate that while some models excel at identifying correct\nresponses, others struggle with false positives or inconsistent evaluations.\nThe findings highlight the need for context-aware evaluation frameworks and\ncareful model selection when deploying LLMs as evaluators. This work\ncontributes to the broader discourse on the trustworthiness of AI-driven\nassessment tools, offering insights into how different LLM architectures handle\nsubjective evaluation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12704", "pdf": "https://arxiv.org/pdf/2504.12704", "abs": "https://arxiv.org/abs/2504.12704", "authors": ["Qianqian Sun", "Jixiang Luo", "Dell Zhang", "Xuelong Li"], "title": "SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex Instruction Understanding", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advancements in image editing have utilized large-scale multimodal\nmodels to enable intuitive, natural instruction-driven interactions. However,\nconventional methods still face significant challenges, particularly in spatial\nreasoning, precise region segmentation, and maintaining semantic consistency,\nespecially in complex scenes. To overcome these challenges, we introduce\nSmartFreeEdit, a novel end-to-end framework that integrates a multimodal large\nlanguage model (MLLM) with a hypergraph-enhanced inpainting architecture,\nenabling precise, mask-free image editing guided exclusively by natural\nlanguage instructions. The key innovations of SmartFreeEdit include:(1)the\nintroduction of region aware tokens and a mask embedding paradigm that enhance\nthe spatial understanding of complex scenes;(2) a reasoning segmentation\npipeline designed to optimize the generation of editing masks based on natural\nlanguage instructions;and (3) a hypergraph-augmented inpainting module that\nensures the preservation of both structural integrity and semantic coherence\nduring complex edits, overcoming the limitations of local-based image\ngeneration. Extensive experiments on the Reason-Edit benchmark demonstrate that\nSmartFreeEdit surpasses current state-of-the-art methods across multiple\nevaluation metrics, including segmentation accuracy, instruction adherence, and\nvisual quality preservation, while addressing the issue of local information\nfocus and improving global consistency in the edited image. Our project will be\navailable at https://github.com/smileformylove/SmartFreeEdit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12553", "pdf": "https://arxiv.org/pdf/2504.12553", "abs": "https://arxiv.org/abs/2504.12553", "authors": ["Zahra Pourbahman", "Fatemeh Rajabi", "Mohammadhossein Sadeghi", "Omid Ghahroodi", "Somaye Bakhshaei", "Arash Amini", "Reza Kazemi", "Mahdieh Soleymani Baghshah"], "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12408", "pdf": "https://arxiv.org/pdf/2504.12408", "abs": "https://arxiv.org/abs/2504.12408", "authors": ["Negar Arabzadeh", "Charles L. A . Clarke"], "title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "reliability", "kappa"], "score": 4}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12351", "pdf": "https://arxiv.org/pdf/2504.12351", "abs": "https://arxiv.org/abs/2504.12351", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Vedrana Ivezic", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey Arnold"], "title": "Prototype-Guided Diffusion for Digital Pathology: Achieving Foundation Model Performance with Minimal Clinical Data", "categories": ["cs.GR", "cs.AI", "eess.IV", "q-bio.TO"], "comment": null, "summary": "Foundation models in digital pathology use massive datasets to learn useful\ncompact feature representations of complex histology images. However, there is\nlimited transparency into what drives the correlation between dataset size and\nperformance, raising the question of whether simply adding more data to\nincrease performance is always necessary. In this study, we propose a\nprototype-guided diffusion model to generate high-fidelity synthetic pathology\ndata at scale, enabling large-scale self-supervised learning and reducing\nreliance on real patient samples while preserving downstream performance. Using\nguidance from histological prototypes during sampling, our approach ensures\nbiologically and diagnostically meaningful variations in the generated data. We\ndemonstrate that self-supervised features trained on our synthetic dataset\nachieve competitive performance despite using ~60x-760x less data than models\ntrained on large real-world datasets. Notably, models trained using our\nsynthetic data showed statistically comparable or better performance across\nmultiple evaluation metrics and tasks, even when compared to models trained on\norders of magnitude larger datasets. Our hybrid approach, combining synthetic\nand real data, further enhanced performance, achieving top results in several\nevaluations. These findings underscore the potential of generative AI to create\ncompelling training data for digital pathology, significantly reducing the\nreliance on extensive clinical datasets and highlighting the efficiency of our\napproach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12684", "pdf": "https://arxiv.org/pdf/2504.12684", "abs": "https://arxiv.org/abs/2504.12684", "authors": ["Junyi Cao", "Evangelos Kalogerakis"], "title": "SOPHY: Generating Simulation-Ready Objects with Physical Materials", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://xjay18.github.io/SOPHY", "summary": "We present SOPHY, a generative model for 3D physics-aware shape synthesis.\nUnlike existing 3D generative models that focus solely on static geometry or 4D\nmodels that produce physics-agnostic animations, our approach jointly\nsynthesizes shape, texture, and material properties related to physics-grounded\ndynamics, making the generated objects ready for simulations and interactive,\ndynamic environments. To train our model, we introduce a dataset of 3D objects\nannotated with detailed physical material attributes, along with an annotation\npipeline for efficient material annotation. Our method enables applications\nsuch as text-driven generation of interactive, physics-aware 3D objects and\nsingle-image reconstruction of physically plausible shapes. Furthermore, our\nexperiments demonstrate that jointly modeling shape and material properties\nenhances the realism and fidelity of generated shapes, improving performance on\ngenerative geometry evaluation metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12368", "pdf": "https://arxiv.org/pdf/2504.12368", "abs": "https://arxiv.org/abs/2504.12368", "authors": ["Babak Ghassemi", "Cassio Fraga-Dantas", "Raffaele Gaetano", "Dino Ienco", "Omid Ghorbanzadeh", "Emma Izquierdo-Verdiguier", "Francesco Vuolo"], "title": "Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Land use and land cover mapping from Earth Observation (EO) data is a\ncritical tool for sustainable land and resource management. While advanced\nmachine learning and deep learning algorithms excel at analyzing EO imagery\ndata, they often overlook crucial geospatial metadata information that could\nenhance scalability and accuracy across regional, continental, and global\nscales. To address this limitation, we propose BRIDGE-LC (Bi-level\nRepresentation Integration for Disentangled GEospatial Land Cover), a novel\ndeep learning framework that integrates multi-scale geospatial information into\nthe land cover classification process. By simultaneously leveraging\nfine-grained (latitude/longitude) and coarse-grained (biogeographical region)\nspatial information, our lightweight multi-layer perceptron architecture learns\nfrom both during training but only requires fine-grained information for\ninference, allowing it to disentangle region-specific from region-agnostic land\ncover features while maintaining computational efficiency. To assess the\nquality of our framework, we use an open-access in-situ dataset and adopt\nseveral competing classification approaches commonly considered for large-scale\nland cover mapping. We evaluated all approaches through two scenarios: an\nextrapolation scenario in which training data encompasses samples from all\nbiogeographical regions, and a leave-one-region-out scenario where one region\nis excluded from training. We also explore the spatial representation learned\nby our model, highlighting a connection between its internal manifold and the\ngeographical information used during training. Our results demonstrate that\nintegrating geospatial information improves land cover mapping performance,\nwith the most substantial gains achieved by jointly leveraging both fine- and\ncoarse-grained spatial information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12395", "pdf": "https://arxiv.org/pdf/2504.12395", "abs": "https://arxiv.org/abs/2504.12395", "authors": ["Jiale Tao", "Yanbing Zhang", "Qixun Wang", "Yiji Cheng", "Haofan Wang", "Xu Bai", "Zhengguang Zhou", "Ruihuang Li", "Linqing Wang", "Chunyu Wang", "Qin Lin", "Qinglin Lu"], "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework", "categories": ["cs.CV"], "comment": "Tech Report. Code is available at\n  https://github.com/Tencent/InstantCharacter", "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12574", "pdf": "https://arxiv.org/pdf/2504.12574", "abs": "https://arxiv.org/abs/2504.12574", "authors": ["Zhenyu Yu", "Mohd Yamani Inda Idris", "Pei Wang"], "title": "Prompt-Driven and Training-Free Forgetting Approach and Dataset for Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The widespread adoption of diffusion models in image generation has increased\nthe demand for privacy-compliant unlearning. However, due to the\nhigh-dimensional nature and complex feature representations of diffusion\nmodels, achieving selective unlearning remains challenging, as existing methods\nstruggle to remove sensitive information while preserving the consistency of\nnon-sensitive regions. To address this, we propose an Automatic Dataset\nCreation Framework based on prompt-based layered editing and training-free\nlocal feature removal, constructing the ForgetMe dataset and introducing the\nEntangled evaluation metric. The Entangled metric quantifies unlearning\neffectiveness by assessing the similarity and consistency between the target\nand background regions and supports both paired (Entangled-D) and unpaired\n(Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe\ndataset encompasses a diverse set of real and synthetic scenarios, including\nCUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We\napply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on\nthis dataset and validate the effectiveness of both the ForgetMe dataset and\nthe Entangled metric, establishing them as benchmarks for selective unlearning.\nOur work provides a scalable and adaptable solution for advancing\nprivacy-preserving generative AI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12652", "pdf": "https://arxiv.org/pdf/2504.12652", "abs": "https://arxiv.org/abs/2504.12652", "authors": ["Md. Sanaullah Chowdhury Lameya Sabrin"], "title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12342", "pdf": "https://arxiv.org/pdf/2504.12342", "abs": "https://arxiv.org/abs/2504.12342", "authors": ["Hanmeng Zhong", "Linqing Chen", "Weilei Wang", "Wentao Wu"], "title": "Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Recently, the application of the retrieval-augmented Large Language Models\n(LLMs) in specific domains has gained significant attention, especially in\nbiopharmaceuticals. However, in this context, there is no benchmark\nspecifically designed for biopharmaceuticals to evaluate LLMs. In this paper,\nwe introduce the Biopharmaceuticals Retrieval-Augmented Generation Evaluation\n(BRAGE) , the first benchmark tailored for evaluating LLMs' Query and Reference\nUnderstanding Capability (QRUC) in the biopharmaceutical domain, available in\nEnglish, French, German and Chinese. In addition, Traditional\nQuestion-Answering (QA) metrics like accuracy and exact match fall short in the\nopen-ended retrieval-augmented QA scenarios. To address this, we propose a\ncitation-based classification method to evaluate the QRUC of LLMs to understand\nthe relationship between queries and references. We apply this method to\nevaluate the mainstream LLMs on BRAGE. Experimental results show that there is\na significant gap in the biopharmaceutical QRUC of mainstream LLMs, and their\nQRUC needs to be improved.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12350", "pdf": "https://arxiv.org/pdf/2504.12350", "abs": "https://arxiv.org/abs/2504.12350", "authors": ["Jing Wang", "Jeremy C Weiss"], "title": "A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Timing of clinical events is central to characterization of patient\ntrajectories, enabling analyses such as process tracing, forecasting, and\ncausal reasoning. However, structured electronic health records capture few\ndata elements critical to these tasks, while clinical reports lack temporal\nlocalization of events in structured form. We present a system that transforms\ncase reports into textual time series-structured pairs of textual events and\ntimestamps. We contrast manual and large language model (LLM) annotations\n(n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access\n(PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93).\nWe find that the LLM models have moderate event recall(O1-preview: 0.80) but\nhigh temporal concordance among identified events (O1-preview: 0.95). By\nestablishing the task, annotation, and assessment systems, and by demonstrating\nhigh concordance, this work may serve as a benchmark for leveraging the PMOA\ncorpus for temporal analytics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "agreement"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12749", "pdf": "https://arxiv.org/pdf/2504.12749", "abs": "https://arxiv.org/abs/2504.12749", "authors": ["Weijia Li", "Guanglei Chu", "Jiong Chen", "Guo-Sen Xie", "Caifeng Shan", "Fang Zhao"], "title": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in industrial anomaly detection have highlighted the need for\ndeeper logical anomaly analysis, where unexpected relationships among objects,\ncounts, and spatial configurations must be identified and explained. Existing\napproaches often rely on large-scale external reasoning modules or elaborate\npipeline designs, hindering practical deployment and interpretability. To\naddress these limitations, we introduce a new task, Reasoning Logical Anomaly\nDetection (RLAD), which extends traditional anomaly detection by incorporating\nlogical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny\nmultimodal language model built on Qwen2.5-VL 3B. Our approach leverages a\ntwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for\nfine-grained visual understanding, followed by Group Relative Policy\nOptimization (GRPO) to refine logical anomaly detection and enforce coherent,\nhuman-readable reasoning. Crucially, reward signals are derived from both the\ndetection accuracy and the structural quality of the outputs, obviating the\nneed for building chain of thought (CoT) reasoning data. Experiments on the\nMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,\nmatches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further\nexcels in producing concise and interpretable rationales. This unified design\nreduces reliance on large models and complex pipelines, while offering\ntransparent and interpretable insights into logical anomaly detection. Code and\ndata will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12560", "pdf": "https://arxiv.org/pdf/2504.12560", "abs": "https://arxiv.org/abs/2504.12560", "authors": ["Elahe Khatibi", "Ziyu Wang", "Amir M. Rahmani"], "title": "CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced large\nlanguage models (LLMs) in knowledge-intensive tasks by incorporating external\nknowledge retrieval. However, existing RAG frameworks primarily rely on\nsemantic similarity and correlation-driven retrieval, limiting their ability to\ndistinguish true causal relationships from spurious associations. This results\nin responses that may be factually grounded but fail to establish\ncause-and-effect mechanisms, leading to incomplete or misleading insights. To\naddress this issue, we introduce Causal Dynamic Feedback for Adaptive\nRetrieval-Augmented Generation (CDF-RAG), a framework designed to improve\ncausal consistency, factual accuracy, and explainability in generative\nreasoning. CDF-RAG iteratively refines queries, retrieves structured causal\ngraphs, and enables multi-hop causal reasoning across interconnected knowledge\nsources. Additionally, it validates responses against causal pathways, ensuring\nlogically coherent and factually grounded outputs. We evaluate CDF-RAG on four\ndiverse datasets, demonstrating its ability to improve response accuracy and\ncausal correctness over existing RAG-based methods. Our code is publicly\navailable at https://github.com/ elakhatibi/CDF-RAG.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12597", "pdf": "https://arxiv.org/pdf/2504.12597", "abs": "https://arxiv.org/abs/2504.12597", "authors": ["Liangyu Xu", "Yingxiu Zhao", "Jingyun Wang", "Yingyao Wang", "Bu Pi", "Chen Wang", "Mingliang Zhang", "Jihao Gu", "Xiang Li", "Xiaoyong Zhu", "Jun Song", "Bo Zheng"], "title": "GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning", "categories": ["cs.CL"], "comment": "10 pages, 8 figures", "summary": "Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12992", "pdf": "https://arxiv.org/pdf/2504.12992", "abs": "https://arxiv.org/abs/2504.12992", "authors": ["Devina Anduyan", "Nyza Cabillo", "Navy Gultiano", "Mark Phil Pacot"], "title": "Enhancing Cocoa Pod Disease Classification via Transfer Learning and Ensemble Methods: Toward Robust Predictive Modeling", "categories": ["cs.CV"], "comment": null, "summary": "This study presents an ensemble-based approach for cocoa pod disease\nclassification by integrating transfer learning with three ensemble learning\nstrategies: Bagging, Boosting, and Stacking. Pre-trained convolutional neural\nnetworks, including VGG16, VGG19, ResNet50, ResNet101, InceptionV3, and\nXception, were fine-tuned and employed as base learners to detect three disease\ncategories: Black Pod Rot, Pod Borer, and Healthy. A balanced dataset of 6,000\ncocoa pod images was curated and augmented to ensure robustness against\nvariations in lighting, orientation, and disease severity. The performance of\neach ensemble method was evaluated using accuracy, precision, recall, and\nF1-score. Experimental results show that Bagging consistently achieved superior\nclassification performance with a test accuracy of 100%, outperforming Boosting\n(97%) and Stacking (92%). The findings confirm that combining transfer learning\nwith ensemble techniques improves model generalization and reliability, making\nit a promising direction for precision agriculture and automated crop disease\nmanagement.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12673", "pdf": "https://arxiv.org/pdf/2504.12673", "abs": "https://arxiv.org/abs/2504.12673", "authors": ["Singon Kim", "Gunho Jung", "Seong-Whan Lee"], "title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12737", "pdf": "https://arxiv.org/pdf/2504.12737", "abs": "https://arxiv.org/abs/2504.12737", "authors": ["Chenghao Fan", "Zhenyi Lu", "Jie Tian"], "title": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model", "categories": ["cs.CL"], "comment": "Chinese-Vicuna Technique Report", "summary": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["Vicuna", "dialogue", "code generation"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13054", "pdf": "https://arxiv.org/pdf/2504.13054", "abs": "https://arxiv.org/abs/2504.13054", "authors": ["Yichao Feng", "Shuai Zhao", "Yueqiu Li", "Luwei Xiao", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "summarization", "aspect-based"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13176", "pdf": "https://arxiv.org/pdf/2504.13176", "abs": "https://arxiv.org/abs/2504.13176", "authors": ["Fei Shen", "Jian Yu", "Cong Wang", "Xin Jiang", "Xiaoyu Du", "Jinhui Tang"], "title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion Design", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12558", "pdf": "https://arxiv.org/pdf/2504.12558", "abs": "https://arxiv.org/abs/2504.12558", "authors": ["Negar Arabzadeh", "Charles L. A. Clarke"], "title": "Benchmarking LLM-based Relevance Judgment Methods", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12684", "pdf": "https://arxiv.org/pdf/2504.12684", "abs": "https://arxiv.org/abs/2504.12684", "authors": ["Junyi Cao", "Evangelos Kalogerakis"], "title": "SOPHY: Generating Simulation-Ready Objects with Physical Materials", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://xjay18.github.io/SOPHY", "summary": "We present SOPHY, a generative model for 3D physics-aware shape synthesis.\nUnlike existing 3D generative models that focus solely on static geometry or 4D\nmodels that produce physics-agnostic animations, our approach jointly\nsynthesizes shape, texture, and material properties related to physics-grounded\ndynamics, making the generated objects ready for simulations and interactive,\ndynamic environments. To train our model, we introduce a dataset of 3D objects\nannotated with detailed physical material attributes, along with an annotation\npipeline for efficient material annotation. Our method enables applications\nsuch as text-driven generation of interactive, physics-aware 3D objects and\nsingle-image reconstruction of physically plausible shapes. Furthermore, our\nexperiments demonstrate that jointly modeling shape and material properties\nenhances the realism and fidelity of generated shapes, improving performance on\ngenerative geometry evaluation metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12451", "pdf": "https://arxiv.org/pdf/2504.12451", "abs": "https://arxiv.org/abs/2504.12451", "authors": ["Jia-Peng Zhang", "Cheng-Feng Pu", "Meng-Hao Guo", "Yan-Pei Cao", "Shi-Min Hu"], "title": "One Model to Rig Them All: Diverse Skeleton Rigging with UniRig", "categories": ["cs.GR"], "comment": "18 pages", "summary": "The rapid evolution of 3D content creation, encompassing both AI-powered\nmethods and traditional workflows, is driving an unprecedented demand for\nautomated rigging solutions that can keep pace with the increasing complexity\nand diversity of 3D models. We introduce UniRig, a novel, unified framework for\nautomatic skeletal rigging that leverages the power of large autoregressive\nmodels and a bone-point cross-attention mechanism to generate both high-quality\nskeletons and skinning weights. Unlike previous methods that struggle with\ncomplex or non-standard topologies, UniRig accurately predicts topologically\nvalid skeleton structures thanks to a new Skeleton Tree Tokenization method\nthat efficiently encodes hierarchical relationships within the skeleton. To\ntrain and evaluate UniRig, we present Rig-XL, a new large-scale dataset of over\n14,000 rigged 3D models spanning a wide range of categories. UniRig\nsignificantly outperforms state-of-the-art academic and commercial methods,\nachieving a 215% improvement in rigging accuracy and a 194% improvement in\nmotion accuracy on challenging datasets. Our method works seamlessly across\ndiverse object categories, from detailed anime characters to complex organic\nand inorganic structures, demonstrating its versatility and robustness. By\nautomating the tedious and time-consuming rigging process, UniRig has the\npotential to speed up animation pipelines with unprecedented ease and\nefficiency. Project Page: https://zjp-shadow.github.io/works/UniRig/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12442", "pdf": "https://arxiv.org/pdf/2504.12442", "abs": "https://arxiv.org/abs/2504.12442", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "3D-PointZshotS: Geometry-Aware 3D Point Cloud Zero-Shot Semantic Segmentation Narrowing the Visual-Semantic Gap", "categories": ["cs.CV"], "comment": null, "summary": "Existing zero-shot 3D point cloud segmentation methods often struggle with\nlimited transferability from seen classes to unseen classes and from semantic\nto visual space. To alleviate this, we introduce 3D-PointZshotS, a\ngeometry-aware zero-shot segmentation framework that enhances both feature\ngeneration and alignment using latent geometric prototypes (LGPs).\nSpecifically, we integrate LGPs into a generator via a cross-attention\nmechanism, enriching semantic features with fine-grained geometric details. To\nfurther enhance stability and generalization, we introduce a self-consistency\nloss, which enforces feature robustness against point-wise perturbations.\nAdditionally, we re-represent visual and semantic features in a shared space,\nbridging the semantic-visual gap and facilitating knowledge transfer to unseen\nclasses. Experiments on three real-world datasets, namely ScanNet,\nSemanticKITTI, and S3DIS, demonstrate that our method achieves superior\nperformance over four baselines in terms of harmonic mIoU. The code is\navailable at \\href{https://github.com/LexieYang/3D-PointZshotS}{Github}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12321", "pdf": "https://arxiv.org/pdf/2504.12321", "abs": "https://arxiv.org/abs/2504.12321", "authors": ["Charlotte Siska", "Anush Sankaran"], "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12326", "pdf": "https://arxiv.org/pdf/2504.12326", "abs": "https://arxiv.org/abs/2504.12326", "authors": ["Shahriar Noroozizadeh", "Jeremy C. Weiss"], "title": "Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical case reports and discharge summaries may be the most complete and\naccurate summarization of patient encounters, yet they are finalized, i.e.,\ntimestamped after the encounter. Complementary data structured streams become\navailable sooner but suffer from incompleteness. To train models and algorithms\non more complete and temporally fine-grained data, we construct a pipeline to\nphenotype, extract, and annotate time-localized findings within case reports\nusing large language models. We apply our pipeline to generate an open-access\ntextual time series corpus for Sepsis-3 comprising 2,139 case reports from the\nPubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA\nand timeline annotations from I2B2/MIMIC-IV and compare the results to\nphysician-expert annotations. We show high recovery rates of clinical findings\n(event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and\nstrong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B\nInstruct--0.932). Our work characterizes the ability of LLMs to time-localize\nclinical findings in text, illustrating the limitations of LLM use for temporal\nreconstruction and providing several potential avenues of improvement via\nmultimodal integration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12329", "pdf": "https://arxiv.org/pdf/2504.12329", "abs": "https://arxiv.org/abs/2504.12329", "authors": ["Wang Yang", "Xiang Yue", "Vipin Chaudhary", "Xiaotian Han"], "title": "Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances leverage post-training to enhance model reasoning\nperformance, which typically requires costly training pipelines and still\nsuffers from inefficient, overly lengthy outputs. We introduce Speculative\nThinking, a training-free framework that enables large reasoning models to\nguide smaller ones during inference at the reasoning level, distinct from\nspeculative decoding, which operates at the token level. Our approach is based\non two observations: (1) reasoning-supportive tokens such as \"wait\" frequently\nappear after structural delimiters like \"\\n\\n\", serving as signals for\nreflection or continuation; and (2) larger models exhibit stronger control over\nreflective behavior, reducing unnecessary backtracking while improving\nreasoning quality. By strategically delegating reflective steps to a more\ncapable model, our method significantly boosts the reasoning accuracy of\nreasoning models while shortening their output. With the assistance of the 32B\nreasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to\n89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average\noutput length is reduced from 5439 tokens to 4583 tokens, representing a 15.7%\ndecrease. Moreover, when applied to a non-reasoning model\n(Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8%\non the same benchmark, achieving a relative improvement of 7.8%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "reasoning model"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12330", "pdf": "https://arxiv.org/pdf/2504.12330", "abs": "https://arxiv.org/abs/2504.12330", "authors": ["Pei Liu", "Xin Liu", "Ruoyu Yao", "Junming Liu", "Siyuan Meng", "Ding Wang", "Jun Ma"], "title": "HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) augments Large Language Models\n(LLMs) with external knowledge, conventional single-agent RAG remains\nfundamentally limited in resolving complex queries demanding coordinated\nreasoning across heterogeneous data ecosystems. We present HM-RAG, a novel\nHierarchical Multi-agent Multimodal RAG framework that pioneers collaborative\nintelligence for dynamic knowledge synthesis across structured, unstructured,\nand graph-based data. The framework is composed of three-tiered architecture\nwith specialized agents: a Decomposition Agent that dissects complex queries\ninto contextually coherent sub-tasks via semantic-aware query rewriting and\nschema-guided context augmentation; Multi-source Retrieval Agents that carry\nout parallel, modality-specific retrieval using plug-and-play modules designed\nfor vector, graph, and web-based databases; and a Decision Agent that uses\nconsistency voting to integrate multi-source answers and resolve discrepancies\nin retrieval results through Expert Model Refinement. This architecture attains\ncomprehensive query understanding by combining textual, graph-relational, and\nweb-derived evidence, resulting in a remarkable 12.95% improvement in answer\naccuracy and a 3.56% boost in question classification accuracy over baseline\nRAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG\nestablishes state-of-the-art results in zero-shot settings on both datasets.\nIts modular architecture ensures seamless integration of new data modalities\nwhile maintaining strict data governance, marking a significant advancement in\naddressing the critical challenges of multimodal reasoning and knowledge\nsynthesis in RAG systems. Code is available at\nhttps://github.com/ocean-luna/HMRAG.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12606", "pdf": "https://arxiv.org/pdf/2504.12606", "abs": "https://arxiv.org/abs/2504.12606", "authors": ["Changsheng Lv", "Mengshi Qi", "Zijian Fu", "Huadong Ma"], "title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12643", "pdf": "https://arxiv.org/pdf/2504.12643", "abs": "https://arxiv.org/abs/2504.12643", "authors": ["Hang Ji", "Tao Ni", "Xufeng Huang", "Tao Luo", "Xin Zhan", "Junbo Chen"], "title": "RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating Enhanced Rotary Position Embedding", "categories": ["cs.CV"], "comment": null, "summary": "This technical report introduces a targeted improvement to the StreamPETR\nframework, specifically aimed at enhancing velocity estimation, a critical\nfactor influencing the overall NuScenes Detection Score. While StreamPETR\nexhibits strong 3D bounding box detection performance as reflected by its high\nmean Average Precision our analysis identified velocity estimation as a\nsubstantial bottleneck when evaluated on the NuScenes dataset. To overcome this\nlimitation, we propose a customized positional embedding strategy tailored to\nenhance temporal modeling capabilities. Experimental evaluations conducted on\nthe NuScenes test set demonstrate that our improved approach achieves a\nstate-of-the-art NDS of 70.86% using the ViT-L backbone, setting a new\nbenchmark for camera-only 3D object detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12334", "pdf": "https://arxiv.org/pdf/2504.12334", "abs": "https://arxiv.org/abs/2504.12334", "authors": ["Zongxian Yang", "Jiayu Qian", "Zhi-An Huang", "Kay Chen Tan"], "title": "QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Large language models (LLMs) face significant challenges in specialized\nbiomedical tasks due to the inherent complexity of medical reasoning and the\nsensitive nature of clinical data. Existing LLMs often struggle with intricate\nmedical terminology and the need for accurate clinical insights, leading to\nperformance reduction when quantized for resource-constrained deployment. To\naddress these issues, we propose Quantized Medical Tree of Thought (QM-ToT), a\npath-based reasoning framework. QM-ToT leverages a Tree of Thought (ToT)\nreasoning approach to decompose complex medical problems into manageable\nsubtasks, coupled with evaluator assessment layers. This framework facilitates\nsubstantial performance improvements in INT4-quantized models on the\nchallenging MedQAUSMLE dataset. Specifically, we demonstrate a remarkable\naccuracy increase from 34% to 50% for the LLaMA2-70b model and from 58.77% to\n69.49% for LLaMA-3.1-8b. Besides, we also proposed an effect data distillation\nmethod based on ToT. Compared to the traditional distillation method, we\nachieved an improvement of 86. 27% while using only 3.9% of the data.This work,\nfor the first time, showcases the potential of ToT to significantly enhance\nperformance on complex biomedical tasks, establishing a crucial foundation for\nfuture advances in deploying high-performing quantized LLM in resource-limited\nmedical settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12345", "pdf": "https://arxiv.org/pdf/2504.12345", "abs": "https://arxiv.org/abs/2504.12345", "authors": ["Yutong Xia", "Ao Qu", "Yunhan Zheng", "Yihong Tang", "Dingyi Zhuang", "Yuxuan Liang", "Cathy Wu", "Roger Zimmermann", "Jinhua Zhao"], "title": "Reimagining Urban Science: Scaling Causal Inference with Large Language Models", "categories": ["cs.CL", "cs.CY", "cs.MA"], "comment": null, "summary": "Urban causal research is essential for understanding the complex dynamics of\ncities and informing evidence-based policies. However, it is challenged by the\ninefficiency and bias of hypothesis generation, barriers to multimodal data\ncomplexity, and the methodological fragility of causal experimentation. Recent\nadvances in large language models (LLMs) present an opportunity to rethink how\nurban causal analysis is conducted. This Perspective examines current urban\ncausal research by analyzing taxonomies that categorize research topics, data\nsources, and methodological approaches to identify structural gaps. We then\nintroduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four\ndistinct modular agents responsible for hypothesis generation, data\nengineering, experiment design and execution, and results interpretation with\npolicy recommendations. We propose evaluation criteria for rigor and\ntransparency and reflect on implications for human-AI collaboration, equity,\nand accountability. We call for a new research agenda that embraces\nAI-augmented workflows not as replacements for human expertise but as tools to\nbroaden participation, improve reproducibility, and unlock more inclusive forms\nof urban causal reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12711", "pdf": "https://arxiv.org/pdf/2504.12711", "abs": "https://arxiv.org/abs/2504.12711", "authors": ["Xin Li", "Yeying Jin", "Xin Jin", "Zongwei Wu", "Bingchen Li", "Yufei Wang", "Wenhan Yang", "Yu Li", "Zhibo Chen", "Bihan Wen", "Robby T. Tan", "Radu Timofte", "Qiyu Rong", "Hongyuan Jing", "Mengmeng Zhang", "Jinglong Li", "Xiangyu Lu", "Yi Ren", "Yuting Liu", "Meng Zhang", "Xiang Chen", "Qiyuan Guan", "Jiangxin Dong", "Jinshan Pan", "Conglin Gou", "Qirui Yang", "Fangpu Zhang", "Yunlong Lin", "Sixiang Chen", "Guoxi Huang", "Ruirui Lin", "Yan Zhang", "Jingyu Yang", "Huanjing Yue", "Jiyuan Chen", "Qiaosi Yi", "Hongjun Wang", "Chenxi Xie", "Shuai Li", "Yuhui Wu", "Kaiyi Ma", "Jiakui Hu", "Juncheng Li", "Liwen Pan", "Guangwei Gao", "Wenjie Li", "Zhenyu Jin", "Heng Guo", "Zhanyu Ma", "Yubo Wang", "Jinghua Wang", "Wangzhi Xing", "Anjusree Karnavar", "Diqi Chen", "Mohammad Aminul Islam", "Hao Yang", "Ruikun Zhang", "Liyuan Pan", "Qianhao Luo", "XinCao", "Han Zhou", "Yan Min", "Wei Dong", "Jun Chen", "Taoyi Wu", "Weijia Dou", "Yu Wang", "Shengjie Zhao", "Yongcheng Huang", "Xingyu Han", "Anyan Huang", "Hongtao Wu", "Hong Wang", "Yefeng Zheng", "Abhijeet Kumar", "Aman Kumar", "Marcos V. Conde", "Paula Garrido", "Daniel Feijoo", "Juan C. Benito", "Guanglu Dong", "Xin Lin", "Siyuan Liu", "Tianheng Zheng", "Jiayu Zhong", "Shouyi Wang", "Xiangtai Li", "Lanqing Guo", "Lu Qi", "Chao Ren", "Shuaibo Wang", "Shilong Zhang", "Wanyu Zhou", "Yunze Wu", "Qinzhong Tan", "Jieyuan Pei", "Zhuoxuan Li", "Jiayu Wang", "Haoyu Bian", "Haoran Sun", "Subhajit Paul", "Ni Tang", "Junhao Huang", "Zihan Cheng", "Hongyun Zhu", "Yuehan Wu", "Kaixin Deng", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhizun Luo", "Zeyu Xiao", "Zhuoyuan Li", "Nguyen Pham Hoang Le", "An Dinh Thien", "Son T. Luu", "Kiet Van Nguyen", "Ronghua Xu", "Xianmin Tian", "Weijian Zhou", "Jiacheng Zhang", "Yuqian Chen", "Yihang Duan", "Yujie Wu", "Suresh Raikwar", "Arsh Garg", "Kritika", "Jianhua Zheng", "Xiaoshan Ma", "Ruolin Zhao", "Yongyu Yang", "Yongsheng Liang", "Guiming Huang", "Qiang Li", "Hongbin Zhang", "Xiangyu Zheng", "A. N. Rajagopalan"], "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Challenge Report of CVPR NTIRE 2025; 26 pages; Methods from 32 teams", "summary": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12355", "pdf": "https://arxiv.org/pdf/2504.12355", "abs": "https://arxiv.org/abs/2504.12355", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "ldar Batyrshin", "Grigori Sidorov"], "title": "Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "Drug overdose remains a critical global health issue, often driven by misuse\nof opioids, painkillers, and psychiatric medications. Traditional research\nmethods face limitations, whereas social media offers real-time insights into\nself-reported substance use and overdose symptoms. This study proposes an\nAI-driven NLP framework trained on annotated social media data to detect\ncommonly used drugs and associated overdose symptoms. Using a hybrid annotation\nstrategy with LLMs and human annotators, we applied traditional ML models,\nneural networks, and advanced transformer-based models. Our framework achieved\n98% accuracy in multi-class and 97% in multi-label classification,\noutperforming baseline models by up to 8%. These findings highlight the\npotential of AI for supporting public health surveillance and personalized\nintervention strategies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12459", "pdf": "https://arxiv.org/pdf/2504.12459", "abs": "https://arxiv.org/abs/2504.12459", "authors": ["Jack Merullo", "Noah A. Smith", "Sarah Wiegreffe", "Yanai Elazar"], "title": "On Linear Representations and Pretraining Data Frequency in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "Pretraining data has a direct impact on the behaviors and quality of language\nmodels (LMs), but we only understand the most basic principles of this\nrelationship. While most work focuses on pretraining data's effect on\ndownstream task behavior, we investigate its relationship to LM\nrepresentations. Previous work has discovered that, in language models, some\nconcepts are encoded `linearly' in the representations, but what factors cause\nthese representations to form? We study the connection between pretraining data\nfrequency and models' linear representations of factual relations. We find\nevidence that the formation of linear representations is strongly connected to\npretraining term frequencies; specifically for subject-relation-object fact\ntriplets, both subject-object co-occurrence frequency and in-context learning\naccuracy for the relation are highly correlated with linear representations.\nThis is the case across all phases of pretraining. In OLMo-7B and GPT-J, we\ndiscover that a linear representation consistently (but not exclusively) forms\nwhen the subjects and objects within a relation co-occur at least 1k and 2k\ntimes, respectively, regardless of when these occurrences happen during\npretraining. Finally, we train a regression model on measurements of linear\nrepresentation quality in fully-trained LMs that can predict how often a term\nwas seen in pretraining. Our model achieves low error even on inputs from a\ndifferent model with a different pretraining dataset, providing a new method\nfor estimating properties of the otherwise-unknown training data of closed-data\nmodels. We conclude that the strength of linear representations in LMs contains\nsignal about the models' pretraining corpora that may provide new avenues for\ncontrolling and improving model behavior: particularly, manipulating the\nmodels' training data to meet specific frequency thresholds.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12799", "pdf": "https://arxiv.org/pdf/2504.12799", "abs": "https://arxiv.org/abs/2504.12799", "authors": ["Mingwei Li", "Pu Pang", "Hehe Fan", "Hua Huang", "Yi Yang"], "title": "TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors", "categories": ["cs.CV"], "comment": "Project page: https://longxiang-ai.github.io/TSGS/", "summary": "Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12807", "pdf": "https://arxiv.org/pdf/2504.12807", "abs": "https://arxiv.org/abs/2504.12807", "authors": ["Ach Khozaimi", "Isnani Darti", "Syaiful Anam", "Wuryansari Muharini Kusumawinahyu"], "title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12523", "pdf": "https://arxiv.org/pdf/2504.12523", "abs": "https://arxiv.org/abs/2504.12523", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Memorization vs. Reasoning: Updating LLMs with New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Large language models (LLMs) encode vast amounts of pre-trained knowledge in\ntheir parameters, but updating them as real-world information evolves remains a\nchallenge. Existing methodologies and benchmarks primarily target entity\nsubstitutions, failing to capture the full breadth of complex real-world\ndynamics. In this paper, we introduce Knowledge Update Playground (KUP), an\nautomatic pipeline for simulating realistic knowledge updates reflected in an\nevidence corpora. KUP's evaluation framework includes direct and indirect\nprobes to both test memorization of updated facts and reasoning over them, for\nany update learning methods. Next, we present a lightweight method called\nmemory conditioned training (MCT), which conditions tokens in the update corpus\non self-generated \"memory\" tokens during training. Our strategy encourages LLMs\nto surface and reason over newly memorized knowledge at inference. Our results\non two strong LLMs show that (1) KUP benchmark is highly challenging, with the\nbest CPT models achieving $<2\\%$ in indirect probing setting (reasoning) and\n(2) MCT training significantly outperforms prior continued pre-training (CPT)\nbaselines, improving direct probing (memorization) results by up to $25.4\\%$.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12959", "pdf": "https://arxiv.org/pdf/2504.12959", "abs": "https://arxiv.org/abs/2504.12959", "authors": ["Dubing Chen", "Huan Zheng", "Jin Fang", "Xingping Dong", "Xianfei Li", "Wenlong Liao", "Tao He", "Pai Peng", "Jianbing Shen"], "title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We present GDFusion, a temporal fusion method for vision-based 3D semantic\noccupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects\nof temporal fusion within the VisionOcc framework, focusing on both temporal\ncues and fusion strategies. It systematically examines the entire VisionOcc\npipeline, identifying three fundamental yet previously overlooked temporal\ncues: scene-level consistency, motion calibration, and geometric\ncomplementation. These cues capture diverse facets of temporal evolution and\nmake distinct contributions across various modules in the VisionOcc framework.\nTo effectively fuse temporal signals across heterogeneous representations, we\npropose a novel fusion strategy by reinterpreting the formulation of vanilla\nRNNs. This reinterpretation leverages gradient descent on features to unify the\nintegration of diverse temporal information, seamlessly embedding the proposed\ntemporal cues into the network. Extensive experiments on nuScenes demonstrate\nthat GDFusion significantly outperforms established baselines. Notably, on\nOcc3D benchmark, it achieves 1.4\\%-4.8\\% mIoU improvements and reduces memory\nconsumption by 27\\%-72\\%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12637", "pdf": "https://arxiv.org/pdf/2504.12637", "abs": "https://arxiv.org/abs/2504.12637", "authors": ["Linda He", "Jue Wang", "Maurice Weber", "Shang Zhu", "Ben Athiwaratkun", "Ce Zhang"], "title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 5 figures", "summary": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12687", "pdf": "https://arxiv.org/pdf/2504.12687", "abs": "https://arxiv.org/abs/2504.12687", "authors": ["Weijie Lv", "Xuan Xia", "Sheng-Jun Huang"], "title": "Data-efficient LLM Fine-tuning for Code Generation", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2408.02193", "summary": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13026", "pdf": "https://arxiv.org/pdf/2504.13026", "abs": "https://arxiv.org/abs/2504.13026", "authors": ["Yide Liu", "Haijiang Sun", "Xiaowen Zhang", "Qiaoyuan Liu", "Zhouchang Chen", "Chongzhuo Xiao"], "title": "TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for Remote Sensing Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Remote Sensing Image Super-Resolution (RSISR) reconstructs high-resolution\n(HR) remote sensing images from low-resolution inputs to support fine-grained\nground object interpretation. Existing methods face three key challenges: (1)\nDifficulty in extracting multi-scale features from spatially heterogeneous RS\nscenes, (2) Limited prior information causing semantic inconsistency in\nreconstructions, and (3) Trade-off imbalance between geometric accuracy and\nvisual quality. To address these issues, we propose the Texture Transfer\nResidual Denoising Dual Diffusion Model (TTRD3) with three innovations: First,\na Multi-scale Feature Aggregation Block (MFAB) employing parallel heterogeneous\nconvolutional kernels for multi-scale feature extraction. Second, a Sparse\nTexture Transfer Guidance (STTG) module that transfers HR texture priors from\nreference images of similar scenes. Third, a Residual Denoising Dual Diffusion\nModel (RDDM) framework combining residual diffusion for deterministic\nreconstruction and noise diffusion for diverse generation. Experiments on\nmulti-source RS datasets demonstrate TTRD3's superiority over state-of-the-art\nmethods, achieving 1.43% LPIPS improvement and 3.67% FID enhancement compared\nto best-performing baselines. Code/model: https://github.com/LED-666/TTRD3.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12845", "pdf": "https://arxiv.org/pdf/2504.12845", "abs": "https://arxiv.org/abs/2504.12845", "authors": ["Amey Hengle", "Prasoon Bajpai", "Soham Dan", "Tanmoy Chakraborty"], "title": "Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks", "categories": ["cs.CL"], "comment": "33 Pages in Total - 23 (Main Manuscript) + 10 (Appendix)", "summary": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12882", "pdf": "https://arxiv.org/pdf/2504.12882", "abs": "https://arxiv.org/abs/2504.12882", "authors": ["Patrick Giedemann", "Pius von D√§niken", "Jan Deriu", "Alvaro Rodrigo", "Anselmo Pe√±as", "Mark Cieliebak"], "title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos", "categories": ["cs.CL"], "comment": null, "summary": "The growing influence of video content as a medium for communication and\nmisinformation underscores the urgent need for effective tools to analyze\nclaims in multilingual and multi-topic settings. Existing efforts in\nmisinformation detection largely focus on written text, leaving a significant\ngap in addressing the complexity of spoken text in video transcripts. We\nintroduce ViClaim, a dataset of 1,798 annotated video transcripts across three\nlanguages (English, German, Spanish) and six topics. Each sentence in the\ntranscripts is labeled with three claim-related categories: fact-check-worthy,\nfact-non-check-worthy, or opinion. We developed a custom annotation tool to\nfacilitate the highly complex annotation process. Experiments with\nstate-of-the-art multilingual language models demonstrate strong performance in\ncross-validation (macro F1 up to 0.896) but reveal challenges in generalization\nto unseen topics, particularly for distinct domains. Our findings highlight the\ncomplexity of claim detection in video transcripts. ViClaim offers a robust\nfoundation for advancing misinformation detection in video-based communication,\naddressing a critical gap in multimodal analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13077", "pdf": "https://arxiv.org/pdf/2504.13077", "abs": "https://arxiv.org/abs/2504.13077", "authors": ["Prasanna Reddy Pulakurthi", "Majid Rabbani", "Celso M. de Melo", "Sohail A. Dianat", "Raghuveer M. Rao"], "title": "Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts of Labeled Data", "categories": ["cs.CV"], "comment": "9 pages, 2 figures, Accepted to SPIE DSC 2025 Conference: Synthetic\n  Data for Artificial Intelligence and Machine Learning: Tools, Techniques, and\n  Applications III", "summary": "This paper introduces a novel dual-region augmentation approach designed to\nreduce reliance on large-scale labeled datasets while improving model\nrobustness and adaptability across diverse computer vision tasks, including\nsource-free domain adaptation (SFDA) and person re-identification (ReID). Our\nmethod performs targeted data transformations by applying random noise\nperturbations to foreground objects and spatially shuffling background patches.\nThis effectively increases the diversity of the training data, improving model\nrobustness and generalization. Evaluations on the PACS dataset for SFDA\ndemonstrate that our augmentation strategy consistently outperforms existing\nmethods, achieving significant accuracy improvements in both single-target and\nmulti-target adaptation settings. By augmenting training data through\nstructured transformations, our method enables model generalization across\ndomains, providing a scalable solution for reducing reliance on manually\nannotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID\ndatasets validate the effectiveness of our approach for person ReID, surpassing\ntraditional augmentation techniques.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13092", "pdf": "https://arxiv.org/pdf/2504.13092", "abs": "https://arxiv.org/abs/2504.13092", "authors": ["Yihua Shao", "Haojin He", "Sijie Li", "Siyu Chen", "Xinwei Long", "Fanhu Zeng", "Yuxuan Fan", "Muyang Zhang", "Ziyang Yan", "Ao Ma", "Xiaochen Wang", "Hao Tang", "Yan Wang", "Shuyan Li"], "title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12915", "pdf": "https://arxiv.org/pdf/2504.12915", "abs": "https://arxiv.org/abs/2504.12915", "authors": ["Ebrahim Norouzi", "Sven Hertling", "Harald Sack"], "title": "ConExion: Concept Extraction with Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12972", "pdf": "https://arxiv.org/pdf/2504.12972", "abs": "https://arxiv.org/abs/2504.12972", "authors": ["Adithya Pratapa", "Teruko Mitamura"], "title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13122", "pdf": "https://arxiv.org/pdf/2504.13122", "abs": "https://arxiv.org/abs/2504.13122", "authors": ["Haojian Huang", "Haodong Chen", "Shengqiong Wu", "Meng Luo", "Jinlan Fu", "Xinya Du", "Hanwang Zhang", "Hao Fei"], "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "categories": ["cs.CV", "cs.LG"], "comment": "Code and Data: https://github.com/HaroldChen19/VistaDPO", "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12976", "pdf": "https://arxiv.org/pdf/2504.12976", "abs": "https://arxiv.org/abs/2504.12976", "authors": ["Charles O'Neill", "Tirthankar Ghosal", "Roberta RƒÉileanu", "Mike Walmsley", "Thang Bui", "Kevin Schawinski", "Ioana CiucƒÉ"], "title": "Sparks of Science: Hypothesis Generation Using Structured Paper Data", "categories": ["cs.CL"], "comment": "9 pages, 2 figures. Comments welcome", "summary": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13123", "pdf": "https://arxiv.org/pdf/2504.13123", "abs": "https://arxiv.org/abs/2504.13123", "authors": ["Xinsong Zhang", "Yarong Zeng", "Xinting Huang", "Hu Hu", "Runquan Xie", "Han Hu", "Zhanhui Kang"], "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12982", "pdf": "https://arxiv.org/pdf/2504.12982", "abs": "https://arxiv.org/abs/2504.12982", "authors": ["Jiatai Wang", "Zhiwei Xu", "Di Jin", "Xuewen Yang", "Tao Li"], "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13129", "pdf": "https://arxiv.org/pdf/2504.13129", "abs": "https://arxiv.org/abs/2504.13129", "authors": ["Jialuo Li", "Wenhao Chai", "Xingyu Fu", "Haiyang Xu", "Saining Xie"], "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web", "summary": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13143", "pdf": "https://arxiv.org/pdf/2504.13143", "abs": "https://arxiv.org/abs/2504.13143", "authors": ["Siwei Yang", "Mude Hui", "Bingchen Zhao", "Yuyin Zhou", "Nataniel Ruiz", "Cihang Xie"], "title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://ucsc-vlaa.github.io/Complex-Edit/, Dataset:\n  https://huggingface.co/datasets/UCSC-VLAA/Complex-Edit", "summary": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13079", "pdf": "https://arxiv.org/pdf/2504.13079", "abs": "https://arxiv.org/abs/2504.13079", "authors": ["Han Wang", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "Retrieval-Augmented Generation with Conflicting Evidence", "categories": ["cs.CL", "cs.AI"], "comment": "Our data and code is available at:\n  https://github.com/HanNight/RAMDocs", "summary": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "factuality"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13157", "pdf": "https://arxiv.org/pdf/2504.13157", "abs": "https://arxiv.org/abs/2504.13157", "authors": ["Khiem Vuong", "Anurag Ghosh", "Deva Ramanan", "Srinivasa Narasimhan", "Shubham Tulsiani"], "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis", "categories": ["cs.CV"], "comment": "Appearing in CVPR 2025. Project page:\n  https://aerial-megadepth.github.io", "summary": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13169", "pdf": "https://arxiv.org/pdf/2504.13169", "abs": "https://arxiv.org/abs/2504.13169", "authors": ["Tsung-Han Wu", "Heekyung Lee", "Jiaxin Ge", "Joseph E. Gonzalez", "Trevor Darrell", "David M. Chan"], "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling", "categories": ["cs.CV"], "comment": "Preprint. Project Page: https://reverse-vlm.github.io", "summary": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "self-verification"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12309", "pdf": "https://arxiv.org/pdf/2504.12309", "abs": "https://arxiv.org/abs/2504.12309", "authors": ["Yi-De Lin", "Guan-Ze Liao"], "title": "Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "From 2000 to 2015, the UN's Millennium Development Goals guided global\npriorities. The subsequent Sustainable Development Goals (SDGs) adopted a more\ndynamic approach, with annual indicator updates. As 2030 nears and progress\nlags, innovative acceleration strategies are critical. This study develops an\nAI-powered knowledge graph system to analyze SDG interconnections, discover\npotential new goals, and visualize them online. Using official SDG texts,\nElsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot\non 269 talks from 2023 applies AI-speculative design, large language models,\nand retrieval-augmented generation. Key findings include: (1) Heatmap analysis\nreveals strong associations between Goal 10 and Goal 16, and minimal coverage\nof Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new\ncentral nodes, showing how richer data supports divergent thinking and goal\nclarity. (3) Six potential new goals are proposed, centered on equity,\nresilience, and technology-driven inclusion. This speculative-AI framework\noffers fresh insights for policymakers and lays groundwork for future\nmultimodal and cross-system SDG applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13180", "pdf": "https://arxiv.org/pdf/2504.13180", "abs": "https://arxiv.org/abs/2504.13180", "authors": ["Jang Hyun Cho", "Andrea Madotto", "Effrosyni Mavroudi", "Triantafyllos Afouras", "Tushar Nagarajan", "Muhammad Maaz", "Yale Song", "Tengyu Ma", "Shuming Hu", "Suyog Jain", "Miguel Martin", "Huiyu Wang", "Hanoona Rasheed", "Peize Sun", "Po-Yao Huang", "Daniel Bolya", "Nikhila Ravi", "Shashank Jain", "Tammy Stark", "Shane Moon", "Babak Damavandi", "Vivian Lee", "Andrew Westbury", "Salman Khan", "Philipp Kr√§henb√ºhl", "Piotr Doll√°r", "Lorenzo Torresani", "Kristen Grauman", "Christoph Feichtenhofer"], "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Technical report", "summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12545", "pdf": "https://arxiv.org/pdf/2504.12545", "abs": "https://arxiv.org/abs/2504.12545", "authors": ["Benign John Ihugba", "Afsana Nasrin", "Ling Wu", "Lin Li", "Lijun Qian", "Xishuang Dong"], "title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12562", "pdf": "https://arxiv.org/pdf/2504.12562", "abs": "https://arxiv.org/abs/2504.12562", "authors": ["Haidar Khan", "Hisham A. Alyahya", "Yazeed Alnumay", "M Saiful Bari", "B√ºlent Yener"], "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12492", "pdf": "https://arxiv.org/pdf/2504.12492", "abs": "https://arxiv.org/abs/2504.12492", "authors": ["Vasco Xu", "Chenfeng Gao", "Henry Hoffmann", "Karan Ahuja"], "title": "MobilePoser: Real-Time Full-Body Pose Estimation and 3D Human Translation from IMUs in Mobile Consumer Devices", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "There has been a continued trend towards minimizing instrumentation for\nfull-body motion capture, going from specialized rooms and equipment, to arrays\nof worn sensors and recently sparse inertial pose capture methods. However, as\nthese techniques migrate towards lower-fidelity IMUs on ubiquitous commodity\ndevices, like phones, watches, and earbuds, challenges arise including\ncompromised online performance, temporal consistency, and loss of global\ntranslation due to sensor noise and drift. Addressing these challenges, we\nintroduce MobilePoser, a real-time system for full-body pose and global\ntranslation estimation using any available subset of IMUs already present in\nthese consumer devices. MobilePoser employs a multi-stage deep neural network\nfor kinematic pose estimation followed by a physics-based motion optimizer,\nachieving state-of-the-art accuracy while remaining lightweight. We conclude\nwith a series of demonstrative applications to illustrate the unique potential\nof MobilePoser across a variety of fields, such as health and wellness, gaming,\nand indoor navigation to name a few.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12588", "pdf": "https://arxiv.org/pdf/2504.12588", "abs": "https://arxiv.org/abs/2504.12588", "authors": ["Liheng Ma", "Soumyasundar Pal", "Yingxue Zhang", "Philip H. S. Torr", "Mark Coates"], "title": "Simplifying Graph Transformers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12511", "pdf": "https://arxiv.org/pdf/2504.12511", "abs": "https://arxiv.org/abs/2504.12511", "authors": ["Shravan Chaudhari", "Trilokya Akula", "Yoon Kim", "Tom Blake"], "title": "Multimodal LLM Augmented Reasoning for Interpretable Visual Perception Analysis", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "In this paper, we advance the study of AI-augmented reasoning in the context\nof Human-Computer Interaction (HCI), psychology and cognitive science, focusing\non the critical task of visual perception. Specifically, we investigate the\napplicability of Multimodal Large Language Models (MLLMs) in this domain. To\nthis end, we leverage established principles and explanations from psychology\nand cognitive science related to complexity in human visual perception. We use\nthem as guiding principles for the MLLMs to compare and interprete visual\ncontent. Our study aims to benchmark MLLMs across various explainability\nprinciples relevant to visual perception. Unlike recent approaches that\nprimarily employ advanced deep learning models to predict complexity metrics\nfrom visual content, our work does not seek to develop a mere new predictive\nmodel. Instead, we propose a novel annotation-free analytical framework to\nassess utility of MLLMs as cognitive assistants for HCI tasks, using visual\nperception as a case study. The primary goal is to pave the way for principled\nstudy in quantifying and evaluating the interpretability of MLLMs for\napplications in improving human reasoning capability and uncovering biases in\nexisting perception datasets annotated by humans.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12879", "pdf": "https://arxiv.org/pdf/2504.12879", "abs": "https://arxiv.org/abs/2504.12879", "authors": ["Grigory Kovalev", "Mikhail Tikhomirov", "Evgeny Kozhevnikov", "Max Kornilov", "Natalia Loukachevitch"], "title": "Building Russian Benchmark for Evaluation of Information Retrieval Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12718", "pdf": "https://arxiv.org/pdf/2504.12718", "abs": "https://arxiv.org/abs/2504.12718", "authors": ["Walid Rehamnia", "Alexandra Getmanskaya", "Evgeniy Vasilyev", "Vadim Turlapov"], "title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"], "comment": "32 pages, 15 figures, 3 tables, 42 references", "summary": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13038", "pdf": "https://arxiv.org/pdf/2504.13038", "abs": "https://arxiv.org/abs/2504.13038", "authors": ["Leo Lepp√§nen", "Lili Aunimo", "Arto Hellas", "Jukka K. Nurminen", "Linda Mannila"], "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses", "categories": ["cs.CY", "cs.CL", "K.3.1; I.2.7"], "comment": "10 pages, 4 figures", "summary": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13059", "pdf": "https://arxiv.org/pdf/2504.13059", "abs": "https://arxiv.org/abs/2504.13059", "authors": ["Yao Mu", "Tianxing Chen", "Zanxin Chen", "Shijia Peng", "Zhiqian Lan", "Zeyu Gao", "Zhixuan Liang", "Qiaojun Yu", "Yude Zou", "Mingkun Xu", "Lunkai Lin", "Zhiqiang Xie", "Mingyu Ding", "Ping Luo"], "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/", "summary": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13128", "pdf": "https://arxiv.org/pdf/2504.13128", "abs": "https://arxiv.org/abs/2504.13128", "authors": ["Nandan Thakur", "Jimmy Lin", "Sam Havens", "Michael Carbin", "Omar Khattab", "Andrew Drozdov"], "title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12826", "pdf": "https://arxiv.org/pdf/2504.12826", "abs": "https://arxiv.org/abs/2504.12826", "authors": ["Pengxuan Yang", "Yupeng Zheng", "Qichao Zhang", "Kefei Zhu", "Zebin Xing", "Qiao Lin", "Yun-Fu Liu", "Zhiguo Su", "Dongbin Zhao"], "title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving aims to produce planning trajectories from raw\nsensors directly. Currently, most approaches integrate perception, prediction,\nand planning modules into a fully differentiable network, promising great\nscalability. However, these methods typically rely on deterministic modeling of\nonline maps in the perception module for guiding or constraining vehicle\nplanning, which may incorporate erroneous perception information and further\ncompromise planning safety. To address this issue, we delve into the importance\nof online map uncertainty for enhancing autonomous driving safety and propose a\nnovel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty\nof the online map in the perception module. It then leverages the uncertainty\nto guide motion prediction and planning modules to produce multi-modal\ntrajectories. Finally, to achieve safer autonomous driving, UncAD proposes an\nuncertainty-collision-aware planning selection strategy according to the online\nmap uncertainty to evaluate and select the best trajectory. In this study, we\nincorporate UncAD into various state-of-the-art (SOTA) end-to-end methods.\nExperiments on the nuScenes dataset show that integrating UncAD, with only a\n1.9% increase in parameters, can reduce collision rates by up to 26% and\ndrivable area conflict rate by up to 42%. Codes, pre-trained models, and demo\nvideos can be accessed at https://github.com/pengxuanyang/UncAD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13151", "pdf": "https://arxiv.org/pdf/2504.13151", "abs": "https://arxiv.org/abs/2504.13151", "authors": ["Aaron Mueller", "Atticus Geiger", "Sarah Wiegreffe", "Dana Arad", "Iv√°n Arcuschin", "Adam Belfki", "Yik Siu Chan", "Jaden Fiotto-Kaufman", "Tal Haklay", "Michael Hanna", "Jing Huang", "Rohan Gupta", "Yaniv Nikankin", "Hadas Orgad", "Nikhil Prakash", "Anja Reusch", "Aruna Sankaranarayanan", "Shun Shao", "Alessandro Stolfo", "Martin Tutek", "Amir Zur", "David Bau", "Yonatan Belinkov"], "title": "MIB: A Mechanistic Interpretability Benchmark", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13172", "pdf": "https://arxiv.org/pdf/2504.13172", "abs": "https://arxiv.org/abs/2504.13172", "authors": ["Haoxuan Li", "Yi Bin", "Yunshan Ma", "Guoqing Wang", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs", "categories": ["cs.IR", "cs.CL", "cs.MM"], "comment": null, "summary": "Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12364", "pdf": "https://arxiv.org/pdf/2504.12364", "abs": "https://arxiv.org/abs/2504.12364", "authors": ["Tianhui Song", "Weixin Feng", "Shuai Wang", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Limin Wang"], "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based Model Merging", "categories": ["cs.CV"], "comment": null, "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12308", "pdf": "https://arxiv.org/pdf/2504.12308", "abs": "https://arxiv.org/abs/2504.12308", "authors": ["Devansh Singh", "Sundaraparipurnan Narayanan"], "title": "Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": null, "summary": "Privacy Masking is a critical concept under data privacy involving\nanonymization and de-anonymization of personally identifiable information\n(PII). Privacy masking techniques rely on Named Entity Recognition (NER)\napproaches under NLP support in identifying and classifying named entities in\neach text. NER approaches, however, have several limitations including (a)\ncontent sensitivity including ambiguous, polysemic, context dependent or domain\nspecific content, (b) phrasing variabilities including nicknames and alias,\ninformal expressions, alternative representations, emerging expressions,\nevolving naming conventions and (c) formats or syntax variations, typos,\nmisspellings. However, there are a couple of PII datasets that have been widely\nused by researchers and the open-source community to train models on PII\ndetection or masking. These datasets have been used to train models including\nPiiranha and Starpii, which have been downloaded over 300k and 580k times on\nHuggingFace. We examine the quality of the PII masking by these models given\nthe limitations of the datasets and of the NER approaches. We curate a dataset\nof 17K unique, semi-synthetic sentences containing 16 types of PII by compiling\ninformation from across multiple jurisdictions including India, U.K and U.S. We\ngenerate sentences (using language models) containing these PII at five\ndifferent NER detection feature dimensions - (1) Basic Entity Recognition, (2)\nContextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4)\nEvolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER)\nand 1 in adversarial context. We present the results and exhibit the privacy\nexposure caused by such model use (considering the extent of lifetime downloads\nof these models). We conclude by highlighting the gaps in measuring performance\nof the models and the need for contextual disclosure in model cards for such\nmodels.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12776", "pdf": "https://arxiv.org/pdf/2504.12776", "abs": "https://arxiv.org/abs/2504.12776", "authors": ["Markus Wallinger", "Annika Bonerath", "Wouter Meulemans", "Martin N√∂llenburg", "Spehen Kobourov", "Alexander Wolff"], "title": "StorySets: Ordering Curves and Dimensions for Visualizing Uncertain Sets and Multi-Dimensional Discrete Data", "categories": ["cs.GR"], "comment": null, "summary": "We propose a method for visualizing uncertain set systems, which differs from\nprevious set visualization approaches that are based on certainty (an element\neither belongs to a set or not). Our method is inspired by storyline\nvisualizations and parallel coordinate plots: (a) each element is represented\nby a vertical glyph, subdivided into bins that represent different levels of\nuncertainty; (b) each set is represented by an x-monotone curve that traverses\nelement glyphs through the bins representing the level of uncertainty of their\nmembership. Our implementation also includes optimizations to reduce visual\ncomplexity captured by the number of turns for the set curves and the number of\ncrossings. Although several of the natural underlying optimization problems are\nNP-hard in theory (e.g., optimal element order, optimal set order), in\npractice, we can compute near-optimal solutions with respect to curve crossings\nwith the help of a new exact algorithm for optimally ordering set curves within\neach element's bins. With these optimizations, the proposed method makes it\neasy to see set containment (the smaller set's curve is strictly below the\nlarger set's curve). A brief design-space exploration using uncertain\nset-membership data, as well as multi-dimensional discrete data, shows the\nflexibility of the proposed approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311", "abs": "https://arxiv.org/abs/2504.12311", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12788", "pdf": "https://arxiv.org/pdf/2504.12788", "abs": "https://arxiv.org/abs/2504.12788", "authors": ["Xiao Han", "Runze Tian", "Yifei Tong", "Fenggen Yu", "Dingyao Liu", "Yan Zhang"], "title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Drag-driven editing has become popular among designers for its ability to\nmodify complex geometric structures through simple and intuitive manipulation,\nallowing users to adjust and reshape content with minimal technical skill. This\ndrag operation has been incorporated into numerous methods to facilitate the\nediting of 2D images and 3D meshes in design. However, few studies have\nexplored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)\nrepresentation, as deforming 3DGS while preserving shape coherence and visual\ncontinuity remains challenging. In this paper, we introduce ARAP-GS, a\ndrag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)\ndeformation. Unlike previous 3DGS editing methods, we are the first to apply\nARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven\ngeometric transformations. To preserve scene appearance after deformation, we\nincorporate an advanced diffusion prior for image super-resolution within our\niterative optimization process. This approach enhances visual quality while\nmaintaining multi-view consistency in the edited results. Experiments show that\nARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its\neffectiveness and superiority for drag-driven 3DGS editing. Additionally, our\nmethod is highly efficient, requiring only 10 to 20 minutes to edit a scene on\na single RTX 3090 GPU.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12369", "pdf": "https://arxiv.org/pdf/2504.12369", "abs": "https://arxiv.org/abs/2504.12369", "authors": ["Zeqi Xiao", "Yushi Lan", "Yifan Zhou", "Wenqi Ouyang", "Shuai Yang", "Yanhong Zeng", "Xingang Pan"], "title": "WORLDMEM: Long-term Consistent World Simulation with Memory", "categories": ["cs.CV"], "comment": "Project page at https://xizaoqu.github.io/worldmem/", "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12313", "pdf": "https://arxiv.org/pdf/2504.12313", "abs": "https://arxiv.org/abs/2504.12313", "authors": ["Xiaoyan Zhao", "Yang Deng", "Wenjie Wang", "Hongzhan lin", "Hong Cheng", "Rui Zhang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Conversational Recommender Systems (CRSs) engage users in multi-turn\ninteractions to deliver personalized recommendations. The emergence of large\nlanguage models (LLMs) further enhances these systems by enabling more natural\nand dynamic user interactions. However, a key challenge remains in\nunderstanding how personality traits shape conversational recommendation\noutcomes. Psychological evidence highlights the influence of personality traits\non user interaction behaviors. To address this, we introduce an LLM-based\npersonality-aware user simulation for CRSs (PerCRS). The user agent induces\ncustomizable personality traits and preferences, while the system agent\npossesses the persuasion capability to simulate realistic interaction in CRSs.\nWe incorporate multi-aspect evaluation to ensure robustness and conduct\nextensive analysis from both user and system perspectives. Experimental results\ndemonstrate that state-of-the-art LLMs can effectively generate diverse user\nresponses aligned with specified personality traits, thereby prompting CRSs to\ndynamically adjust their recommendation strategies. Our experimental analysis\noffers empirical insights into the impact of personality traits on the outcomes\nof conversational recommender systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12811", "pdf": "https://arxiv.org/pdf/2504.12811", "abs": "https://arxiv.org/abs/2504.12811", "authors": ["Michael Steiner", "Thomas K√∂hler", "Lukas Radl", "Felix Windisch", "Dieter Schmalstieg", "Markus Steinberger"], "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12856", "pdf": "https://arxiv.org/pdf/2504.12856", "abs": "https://arxiv.org/abs/2504.12856", "authors": ["Yifeng Cheng", "Juan Du"], "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO", "I.5.4"], "comment": null, "summary": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12456", "pdf": "https://arxiv.org/pdf/2504.12456", "abs": "https://arxiv.org/abs/2504.12456", "authors": ["Huantao Ren", "Minmin Yang", "Senem Velipasalar"], "title": "DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks have achieved significant success in 3D point cloud\nclassification while relying on large-scale, annotated point cloud datasets,\nwhich are labor-intensive to build. Compared to capturing data with LiDAR\nsensors and then performing annotation, it is relatively easier to sample point\nclouds from CAD models. Yet, data sampled from CAD models is regular, and does\nnot suffer from occlusion and missing points, which are very common for LiDAR\ndata, creating a large domain shift. Therefore, it is critical to develop\nmethods that can generalize well across different point cloud domains. %In this\npaper, we focus on the 3D point cloud domain generalization problem. Existing\n3D domain generalization methods employ point-based backbones to extract point\ncloud features. Yet, by analyzing point utilization of point-based methods and\nobserving the geometry of point clouds from different domains, we have found\nthat a large number of point features are discarded by point-based methods\nthrough the max-pooling operation. This is a significant waste especially\nconsidering the fact that domain generalization is more challenging than\nsupervised learning, and point clouds are already affected by missing points\nand occlusion to begin with. To address these issues, we propose a novel method\nfor 3D point cloud domain generalization, which can generalize to unseen\ndomains of point clouds. Our proposed method employs multiple 2D projections of\na 3D point cloud to alleviate the issue of missing points and involves a simple\nyet effective convolution-based model to extract features. The experiments,\nperformed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the\neffectiveness of our proposed method, which outperforms different baselines,\nand can transfer well from synthetic domain to real-world domain.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13022", "pdf": "https://arxiv.org/pdf/2504.13022", "abs": "https://arxiv.org/abs/2504.13022", "authors": ["Xiangrui Liu", "Xinju Wu", "Shiqi Wang", "Zhu Li", "Sam Kwong"], "title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation", "categories": ["cs.GR", "cs.CV"], "comment": "Submitted to a journal", "summary": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12320", "pdf": "https://arxiv.org/pdf/2504.12320", "abs": "https://arxiv.org/abs/2504.12320", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "19 pages + Appendix, 13 figure", "summary": "Following the widespread adoption of ChatGPT in early 2023, numerous studies\nreported that large language models (LLMs) can match or even surpass human\nperformance in creative tasks. However, it remains unclear whether LLMs have\nbecome more creative over time, and how consistent their creative output is. In\nthis study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,\nGrok, Mistral, and DeepSeek -- across two validated creativity assessments: the\nDivergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary\nto expectations, we found no evidence of increased creative performance over\nthe past 18-24 months, with GPT-4 performing worse than in previous studies.\nFor the more widely used AUT, all models performed on average better than the\naverage human, with GPT-4o and o3-mini performing best. However, only 0.28% of\nLLM-generated responses reached the top 10% of human creativity benchmarks.\nBeyond inter-model differences, we document substantial intra-model\nvariability: the same LLM, given the same prompt, can produce outputs ranging\nfrom below-average to original. This variability has important implications for\nboth creativity research and practical applications. Ignoring such variability\nrisks misjudging the creative potential of LLMs, either inflating or\nunderestimating their capabilities. The choice of prompts affected LLMs\ndifferently. Our findings underscore the need for more nuanced evaluation\nframeworks and highlight the importance of model selection, prompt design, and\nrepeated assessment when using Generative AI (GenAI) tools in creative\ncontexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12515", "pdf": "https://arxiv.org/pdf/2504.12515", "abs": "https://arxiv.org/abs/2504.12515", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "Event Quality Score (EQS): Assessing the Realism of Simulated Event Camera Streams via Distances in Latent Space", "categories": ["cs.CV"], "comment": "Accepted at 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Fifth International Workshop on Event-Based\n  Vision", "summary": "Event cameras promise a paradigm shift in vision sensing with their low\nlatency, high dynamic range, and asynchronous nature of events. Unfortunately,\nthe scarcity of high-quality labeled datasets hinders their widespread adoption\nin deep learning-driven computer vision. To mitigate this, several simulators\nhave been proposed to generate synthetic event data for training models for\ndetection and estimation tasks. However, the fundamentally different sensor\ndesign of event cameras compared to traditional frame-based cameras poses a\nchallenge for accurate simulation. As a result, most simulated data fail to\nmimic data captured by real event cameras. Inspired by existing work on using\ndeep features for image comparison, we introduce event quality score (EQS), a\nquality metric that utilizes activations of the RVT architecture. Through\nsim-to-real experiments on the DSEC driving dataset, it is shown that a higher\nEQS implies improved generalization to real-world data after training on\nsimulated events. Thus, optimizing for EQS can lead to developing more\nrealistic event camera simulators, effectively reducing the simulation gap. EQS\nis available at https://github.com/eventbasedvision/EQS.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12542", "pdf": "https://arxiv.org/pdf/2504.12542", "abs": "https://arxiv.org/abs/2504.12542", "authors": ["Kooshan Amini", "Yuhao Liu", "Jamie Ellen Padgett", "Guha Balakrishnan", "Ashok Veeraraghavan"], "title": "Post-Hurricane Debris Segmentation Using Fine-Tuned Foundational Vision Models", "categories": ["cs.CV"], "comment": "12 pages, 8 figures", "summary": "Timely and accurate detection of hurricane debris is critical for effective\ndisaster response and community resilience. While post-disaster aerial imagery\nis readily available, robust debris segmentation solutions applicable across\nmultiple disaster regions remain limited. Developing a generalized solution is\nchallenging due to varying environmental and imaging conditions that alter\ndebris' visual signatures across different regions, further compounded by the\nscarcity of training data. This study addresses these challenges by fine-tuning\npre-trained foundational vision models, achieving robust performance with a\nrelatively small, high-quality dataset. Specifically, this work introduces an\nopen-source dataset comprising approximately 1,200 manually annotated aerial\nRGB images from Hurricanes Ian, Ida, and Ike. To mitigate human biases and\nenhance data quality, labels from multiple annotators are strategically\naggregated and visual prompt engineering is employed. The resulting fine-tuned\nmodel, named fCLIPSeg, achieves a Dice score of 0.70 on data from Hurricane Ida\n-- a disaster event entirely excluded during training -- with virtually no\nfalse positives in debris-free areas. This work presents the first\nevent-agnostic debris segmentation model requiring only standard RGB imagery\nduring deployment, making it well-suited for rapid, large-scale post-disaster\nimpact assessments and recovery planning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12552", "pdf": "https://arxiv.org/pdf/2504.12552", "abs": "https://arxiv.org/abs/2504.12552", "authors": ["Alejandra Perez", "Han Zhang", "Yu-Chun Ku", "Lalithkumar Seenivasan", "Roger Soberanis", "Jose L. Porras", "Richard Day", "Jeff Jopling", "Peter Najjar", "Mathias Unberath"], "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324", "abs": "https://arxiv.org/abs/2504.12324", "authors": ["Mengying Yuan", "Wangzi Xuan", "Fei Li"], "title": "Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in both natural\nlanguage processing and information retrieval. While NLI has developed many\nsub-directions such as sentence-level NLI, document-level NLI and cross-lingual\nNLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In\nthis paper, we propose a novel paradigm for CDCL-NLI that extends traditional\nNLI capabilities to multi-document, multilingual scenarios. To support this\ntask, we construct a high-quality CDCL-NLI dataset including 1,110 instances\nand spanning 26 languages. To build a baseline for this task, we also propose\nan innovative method that integrates RST-enhanced graph fusion and\ninterpretability prediction. Our method employs RST (Rhetorical Structure\nTheory) on RGAT (Relation-aware Graph Attention Network) for cross-document\ncontext modeling, coupled with a structure-aware semantic alignment mechanism\nbased on lexical chains for cross-lingual understanding. For NLI\ninterpretability, we develop an EDU-level attribution framework that generates\nextractive explanations. Extensive experiments demonstrate our approach's\nsuperior performance, achieving significant improvements over both traditional\nNLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our\nwork sheds light on the study of NLI and will bring research interest on\ncross-document cross-lingual context understanding, semantic retrieval and\ninterpretability inference. Our dataset and code are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer\nreview}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12556", "pdf": "https://arxiv.org/pdf/2504.12556", "abs": "https://arxiv.org/abs/2504.12556", "authors": ["Xinyu Zhao", "Jun Liu", "Faqiang Wang", "Li Cui", "Yuping Duan"], "title": "Contour Field based Elliptical Shape Prior for the Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "The elliptical shape prior information plays a vital role in improving the\naccuracy of image segmentation for specific tasks in medical and natural\nimages. Existing deep learning-based segmentation methods, including the\nSegment Anything Model (SAM), often struggle to produce segmentation results\nwith elliptical shapes efficiently. This paper proposes a new approach to\nintegrate the prior of elliptical shapes into the deep learning-based SAM image\nsegmentation techniques using variational methods. The proposed method\nestablishes a parameterized elliptical contour field, which constrains the\nsegmentation results to align with predefined elliptical contours. Utilizing\nthe dual algorithm, the model seamlessly integrates image features with\nelliptical priors and spatial regularization priors, thereby greatly enhancing\nsegmentation accuracy. By decomposing SAM into four mathematical sub-problems,\nwe integrate the variational ellipse prior to design a new SAM network\nstructure, ensuring that the segmentation output of SAM consists of elliptical\nregions. Experimental results on some specific image datasets demonstrate an\nimprovement over the original SAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12325", "pdf": "https://arxiv.org/pdf/2504.12325", "abs": "https://arxiv.org/abs/2504.12325", "authors": ["Haiqi Zhang", "Zhengyuan Zhu", "Zeyu Zhang", "Chengkai Li"], "title": "LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": null, "summary": "With the vast expansion of content on social media platforms, analyzing and\ncomprehending online discourse has become increasingly complex. This paper\nintroduces LLMTaxo, a novel framework leveraging large language models for the\nautomated construction of taxonomy of factual claims from social media by\ngenerating topics from multi-level granularities. This approach aids\nstakeholders in more effectively navigating the social media landscapes. We\nimplement this framework with different models across three distinct datasets\nand introduce specially designed taxonomy evaluation metrics for a\ncomprehensive assessment. With the evaluations from both human evaluators and\nGPT-4, the results indicate that LLMTaxo effectively categorizes factual claims\nfrom social media, and reveals that certain models perform better on specific\ndatasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12573", "pdf": "https://arxiv.org/pdf/2504.12573", "abs": "https://arxiv.org/abs/2504.12573", "authors": ["Yuning Zhou", "Henry Badgery", "Matthew Read", "James Bailey", "Catherine Davey"], "title": "Parsimonious Dataset Construction for Laparoscopic Cholecystectomy Structure Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "IEEE EMBS ISC Australia 2022", "summary": "Labeling has always been expensive in the medical context, which has hindered\nrelated deep learning application. Our work introduces active learning in\nsurgical video frame selection to construct a high-quality, affordable\nLaparoscopic Cholecystectomy dataset for semantic segmentation. Active learning\nallows the Deep Neural Networks (DNNs) learning pipeline to include the dataset\nconstruction workflow, which means DNNs trained by existing dataset will\nidentify the most informative data from the newly collected data. At the same\ntime, DNNs' performance and generalization ability improve over time when the\nnewly selected and annotated data are included in the training data. We\nassessed different data informativeness measurements and found the deep\nfeatures distances select the most informative data in this task. Our\nexperiments show that with half of the data selected by active learning, the\nDNNs achieve almost the same performance with 0.4349 mean Intersection over\nUnion (mIoU) compared to the same DNNs trained on the full dataset (0.4374\nmIoU) on the critical anatomies and surgical instruments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12576", "pdf": "https://arxiv.org/pdf/2504.12576", "abs": "https://arxiv.org/abs/2504.12576", "authors": ["Wentao Wu", "Xiao Wang", "Chenglong Li", "Bo Jiang", "Jin Tang", "Bin Luo", "Qi Liu"], "title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12328", "pdf": "https://arxiv.org/pdf/2504.12328", "abs": "https://arxiv.org/abs/2504.12328", "authors": ["Jialun Zhong", "Wei Shen", "Yanzeng Li", "Songyang Gao", "Hua Lu", "Yicheng Chen", "Yang Zhang", "Wei Zhou", "Jinjie Gu", "Lei Zou"], "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12605", "pdf": "https://arxiv.org/pdf/2504.12605", "abs": "https://arxiv.org/abs/2504.12605", "authors": ["Xin Su", "Chen Wu", "Yu Zhang", "Chen Lyu", "Zhuoran Zheng"], "title": "AdaQual-Diff: Diffusion-Based Image Restoration via Adaptive Quality Prompting", "categories": ["cs.CV"], "comment": null, "summary": "Restoring images afflicted by complex real-world degradations remains\nchallenging, as conventional methods often fail to adapt to the unique mixture\nand severity of artifacts present. This stems from a reliance on indirect cues\nwhich poorly capture the true perceptual quality deficit. To address this\nfundamental limitation, we introduce AdaQual-Diff, a diffusion-based framework\nthat integrates perceptual quality assessment directly into the generative\nrestoration process. Our approach establishes a mathematical relationship\nbetween regional quality scores from DeQAScore and optimal guidance complexity,\nimplemented through an Adaptive Quality Prompting mechanism. This mechanism\nsystematically modulates prompt structure according to measured degradation\nseverity: regions with lower perceptual quality receive computationally\nintensive, structurally complex prompts with precise restoration directives,\nwhile higher quality regions receive minimal prompts focused on preservation\nrather than intervention. The technical core of our method lies in the dynamic\nallocation of computational resources proportional to degradation severity,\ncreating a spatially-varying guidance field that directs the diffusion process\nwith mathematical precision. By combining this quality-guided approach with\ncontent-specific conditioning, our framework achieves fine-grained control over\nregional restoration intensity without requiring additional parameters or\ninference iterations. Experimental results demonstrate that AdaQual-Diff\nachieves visually superior restorations across diverse synthetic and real-world\ndatasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12331", "pdf": "https://arxiv.org/pdf/2504.12331", "abs": "https://arxiv.org/abs/2504.12331", "authors": ["Xiangju Li", "Dong Yang", "Xiaogang Zhu", "Faliang Huang", "Peng Zhang", "Zhongying Zhao"], "title": "Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Span-level emotion-cause-category triplet extraction represents a novel and\ncomplex challenge within emotion cause analysis. This task involves identifying\nemotion spans, cause spans, and their associated emotion categories within the\ntext to form structured triplets. While prior research has predominantly\nconcentrated on clause-level emotion-cause pair extraction and span-level\nemotion-cause detection, these methods often confront challenges originating\nfrom redundant information retrieval and difficulty in accurately determining\nemotion categories, particularly when emotions are expressed implicitly or\nambiguously. To overcome these challenges, this study explores a fine-grained\napproach to span-level emotion-cause-category triplet extraction and introduces\nan innovative framework that leverages instruction tuning and data augmentation\ntechniques based on large language models. The proposed method employs\ntask-specific triplet extraction instructions and utilizes low-rank adaptation\nto fine-tune large language models, eliminating the necessity for intricate\ntask-specific architectures. Furthermore, a prompt-based data augmentation\nstrategy is developed to address data scarcity by guiding large language models\nin generating high-quality synthetic training data. Extensive experimental\nevaluations demonstrate that the proposed approach significantly outperforms\nexisting baseline methods, achieving at least a 12.8% improvement in span-level\nemotion-cause-category triplet extraction metrics. The results demonstrate the\nmethod's effectiveness and robustness, offering a promising avenue for\nadvancing research in emotion cause analysis. The source code is available at\nhttps://github.com/zxgnlp/InstruDa-LLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12619", "pdf": "https://arxiv.org/pdf/2504.12619", "abs": "https://arxiv.org/abs/2504.12619", "authors": ["Yun-Cheng Li", "Sen Lei", "Yi-Tao Zhao", "Heng-Chao Li", "Jun Li", "Antonio Plaza"], "title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping", "categories": ["cs.CV"], "comment": null, "summary": "Building change detection remains challenging for urban development, disaster\nassessment, and military reconnaissance. While foundation models like Segment\nAnything Model (SAM) show strong segmentation capabilities, SAM is limited in\nthe task of building change detection due to domain gap issues. Existing\nadapter-based fine-tuning approaches face challenges with imbalanced building\ndistribution, resulting in poor detection of subtle changes and inaccurate edge\nextraction. Additionally, bi-temporal misalignment in change detection,\ntypically addressed by optical flow, remains vulnerable to background noises.\nThis affects the detection of building changes and compromises both detection\naccuracy and edge recognition. To tackle these challenges, we propose a new\nSAM-Based Network with Distribution-Aware Fourier Adaptation and\nEdge-Constrained Warping (FAEWNet) for building change detection. FAEWNet\nutilizes the SAM encoder to extract rich visual features from remote sensing\nimages. To guide SAM in focusing on specific ground objects in remote sensing\nscenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate\ntask-oriented changed information. This adapter not only effectively addresses\nthe domain gap issue, but also pays attention to the distribution of changed\nbuildings. Furthermore, to mitigate noise interference and misalignment in\nheight offset estimation, we design a novel flow module that refines building\nedge extraction and enhances the perception of changed buildings. Our\nstate-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets\nhighlight the effectiveness of FAEWNet. The code is available at\nhttps://github.com/SUPERMAN123000/FAEWNet.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12332", "pdf": "https://arxiv.org/pdf/2504.12332", "abs": "https://arxiv.org/abs/2504.12332", "authors": ["Mingrui Zan", "Yunquan Zhang", "Boyang Zhang", "Fangming Liu", "Daning Cheng"], "title": "Can the capability of Large Language Models be described by human ability? A Meta Study", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Users of Large Language Models (LLMs) often perceive these models as\nintelligent entities with human-like capabilities. However, the extent to which\nLLMs' capabilities truly approximate human abilities remains a topic of debate.\nIn this paper, to characterize the capabilities of LLMs in relation to human\ncapabilities, we collected performance data from over 80 models across 37\nevaluation benchmarks. The evaluation benchmarks are categorized into 6 primary\nabilities and 11 sub-abilities in human aspect. Then, we then clustered the\nperformance rankings into several categories and compared these clustering\nresults with classifications based on human ability aspects. Our findings lead\nto the following conclusions: 1. We have confirmed that certain capabilities of\nLLMs with fewer than 10 billion parameters can indeed be described using human\nability metrics; 2. While some abilities are considered interrelated in humans,\nthey appear nearly uncorrelated in LLMs; 3. The capabilities possessed by LLMs\nvary significantly with the parameter scale of the model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12335", "pdf": "https://arxiv.org/pdf/2504.12335", "abs": "https://arxiv.org/abs/2504.12335", "authors": ["Alden Dima", "James Foulds", "Shimei Pan", "Philip Feldman"], "title": "You've Changed: Detecting Modification of Black-Box Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are often provided as a service via an API,\nmaking it challenging for developers to detect changes in their behavior. We\npresent an approach to monitor LLMs for changes by comparing the distributions\nof linguistic and psycholinguistic features of generated text. Our method uses\na statistical test to determine whether the distributions of features from two\nsamples of text are equivalent, allowing developers to identify when an LLM has\nchanged. We demonstrate the effectiveness of our approach using five OpenAI\ncompletion models and Meta's Llama 3 70B chat model. Our results show that\nsimple text features coupled with a statistical test can distinguish between\nlanguage models. We also explore the use of our approach to detect prompt\ninjection attacks. Our work enables frequent LLM change monitoring and avoids\ncomputationally expensive benchmark evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12667", "pdf": "https://arxiv.org/pdf/2504.12667", "abs": "https://arxiv.org/abs/2504.12667", "authors": ["Lin Liu", "Ziying Song", "Hongyu Pan", "Lei Yang", "Caiyan Jia"], "title": "Two Tasks, One Goal: Uniting Motion and Planning for Excellent End To End Autonomous Driving Performance", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end autonomous driving has made impressive progress in recent years.\nFormer end-to-end autonomous driving approaches often decouple planning and\nmotion tasks, treating them as separate modules. This separation overlooks the\npotential benefits that planning can gain from learning out-of-distribution\ndata encountered in motion tasks. However, unifying these tasks poses\nsignificant challenges, such as constructing shared contextual representations\nand handling the unobservability of other vehicles' states. To address these\nchallenges, we propose TTOG, a novel two-stage trajectory generation framework.\nIn the first stage, a diverse set of trajectory candidates is generated, while\nthe second stage focuses on refining these candidates through vehicle state\ninformation. To mitigate the issue of unavailable surrounding vehicle states,\nTTOG employs a self-vehicle data-trained state estimator, subsequently extended\nto other vehicles. Furthermore, we introduce ECSA (equivariant context-sharing\nscene adapter) to enhance the generalization of scene representations across\ndifferent agents. Experimental results demonstrate that TTOG achieves\nstate-of-the-art performance across both planning and motion tasks. Notably, on\nthe challenging open-loop nuScenes dataset, TTOG reduces the L2 distance by\n36.06\\%. Furthermore, on the closed-loop Bench2Drive dataset, our approach\nachieves a 22\\% improvement in the driving score (DS), significantly\noutperforming existing baselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12676", "pdf": "https://arxiv.org/pdf/2504.12676", "abs": "https://arxiv.org/abs/2504.12676", "authors": ["Yu Song", "Tatsuaki Goh", "Yinhao Li", "Jiahua Dong", "Shunsuke Miyashima", "Yutaro Iwamoto", "Yohei Kondo", "Keiji Nakajima", "Yen-wei Chen"], "title": "Accurate Tracking of Arabidopsis Root Cortex Cell Nuclei in 3D Time-Lapse Microscopy Images Based on Genetic Algorithm", "categories": ["cs.CV"], "comment": null, "summary": "Arabidopsis is a widely used model plant to gain basic knowledge on plant\nphysiology and development. Live imaging is an important technique to visualize\nand quantify elemental processes in plant development. To uncover novel\ntheories underlying plant growth and cell division, accurate cell tracking on\nlive imaging is of utmost importance. The commonly used cell tracking software,\nTrackMate, adopts tracking-by-detection fashion, which applies Laplacian of\nGaussian (LoG) for blob detection, and Linear Assignment Problem (LAP) tracker\nfor tracking. However, they do not perform sufficiently when cells are densely\narranged. To alleviate the problems mentioned above, we propose an accurate\ntracking method based on Genetic algorithm (GA) using knowledge of Arabidopsis\nroot cellular patterns and spatial relationship among volumes. Our method can\nbe described as a coarse-to-fine method, in which we first conducted relatively\neasy line-level tracking of cell nuclei, then performed complicated nuclear\ntracking based on known linear arrangement of cell files and their spatial\nrelationship between nuclei. Our method has been evaluated on a long-time live\nimaging dataset of Arabidopsis root tips, and with minor manual rectification,\nit accurately tracks nuclei. To the best of our knowledge, this research\nrepresents the first successful attempt to address a long-standing problem in\nthe field of time-lapse microscopy in the root meristem by proposing an\naccurate tracking method for Arabidopsis root nuclei.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12338", "pdf": "https://arxiv.org/pdf/2504.12338", "abs": "https://arxiv.org/abs/2504.12338", "authors": ["David Anderson", "Michaela Anderson", "Margret Bjarnadottir", "Stephen Mahar", "Shriyan Reyya"], "title": "Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions", "categories": ["cs.CL", "cs.LG", "I.2.0"], "comment": "Paper and Online Supplement combined into one PDF. 26 pages. 2\n  figures", "summary": "There is a long history of building predictive models in healthcare using\ntabular data from electronic medical records. However, these models fail to\nextract the information found in unstructured clinical notes, which document\ndiagnosis, treatment, progress, medications, and care plans. In this study, we\ninvestigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical\nquestions about patients, when given access to the patient's discharge summary,\ncan support patient-level mortality prediction. Using data from 14,011\nfirst-time admissions to the Coronary Care or Cardiovascular Intensive Care\nUnits in the MIMIC-IV Note dataset, we implement a transparent framework that\nuses GPT responses as input features in logistic regression models. Our\nfindings demonstrate that GPT-based models alone can outperform models trained\non standard tabular data, and that combining both sources of information yields\neven greater predictive power, increasing AUC by an average of 5.1 percentage\npoints and increasing positive predictive value by 29.9 percent for the\nhighest-risk decile. These results highlight the value of integrating large\nlanguage models (LLMs) into clinical prediction tasks and underscore the\nbroader potential for using LLMs in any domain where unstructured text data\nremains an underutilized resource.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12679", "pdf": "https://arxiv.org/pdf/2504.12679", "abs": "https://arxiv.org/abs/2504.12679", "authors": ["Bofei Zhang", "Zirui Shang", "Zhi Gao", "Wang Zhang", "Rui Xie", "Xiaojian Ma", "Tao Yuan", "Xinxiao Wu", "Song-Chun Zhu", "Qing Li"], "title": "TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials", "categories": ["cs.CV"], "comment": null, "summary": "Building Graphical User Interface (GUI) agents is a promising research\ndirection, which simulates human interaction with computers or mobile phones to\nperform diverse GUI tasks. However, a major challenge in developing generalized\nGUI agents is the lack of sufficient trajectory data across various operating\nsystems and applications, mainly due to the high cost of manual annotations. In\nthis paper, we propose the TongUI framework that builds generalized GUI agents\nby learning from rich multimodal web tutorials. Concretely, we crawl and\nprocess online GUI tutorials (such as videos and articles) into GUI agent\ntrajectory data, through which we produce the GUI-Net dataset containing 143K\ntrajectory data across five operating systems and more than 200 applications.\nWe develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,\nwhich show remarkable performance improvements on commonly used grounding and\nnavigation benchmarks, outperforming baseline agents about 10\\% on multiple\nbenchmarks, showing the effectiveness of the GUI-Net dataset and underscoring\nthe significance of our TongUI framework. We will fully open-source the code,\nthe GUI-Net dataset, and the trained models soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12339", "pdf": "https://arxiv.org/pdf/2504.12339", "abs": "https://arxiv.org/abs/2504.12339", "authors": ["Yaodong Song", "Hongjie Chen", "Jie Lian", "Yuxin Zhang", "Guangmin Xia", "Zehan Li", "Genliang Zhao", "Jian Kang", "Yongxiang Li", "Jie Li"], "title": "GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-k\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12689", "pdf": "https://arxiv.org/pdf/2504.12689", "abs": "https://arxiv.org/abs/2504.12689", "authors": ["Qishan Wang", "Shuyong Gao", "Junjie Hu", "Jiawen Yu", "Xuan Tong", "You Li", "Wenqiang Zhang"], "title": "HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset", "categories": ["cs.CV"], "comment": "Accepted to IEEE ICME 2025", "summary": "Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving\nincreasing attention due to their relatively low deployment costs and improved\ntraining efficiency. However, the real-world effectiveness of MUAD methods is\nquestioned due to limitations in current Industrial Anomaly Detection (IAD)\ndatasets. These datasets contain numerous classes that are unlikely to be\nproduced by the same factory and fail to cover multiple structures or\nappearances. Additionally, the defects do not reflect real-world\ncharacteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial\nAnomaly Detection (HSS-IAD) dataset, which contains 8,580 images of\nmetallic-like industrial parts and precise anomaly annotations. These parts\nexhibit variations in structure and appearance, with subtle defects that\nclosely resemble the base materials. We also provide foreground images for\nsynthetic anomaly generation. Finally, we evaluate popular IAD methods on this\ndataset under multi-class and class-separated settings, demonstrating its\npotential to bridge the gap between existing datasets and real factory\nconditions. The dataset is available at\nhttps://github.com/Qiqigeww/HSS-IAD-Dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12341", "pdf": "https://arxiv.org/pdf/2504.12341", "abs": "https://arxiv.org/abs/2504.12341", "authors": ["Linqing Chen", "Weilei Wang", "Yubin Xia", "Wentao Wu", "Peng Xu", "Zilong Bai", "Jie Fang", "Chaobo Xu", "Ran Hu", "Licong Xu", "Haoran Hua", "Jing Sun", "Hanmeng Zhong", "Jin Liu", "Tian Qiu", "Haowen Liu", "Meng Hu", "Xiuwen Li", "Fei Gao", "Yong Gu", "Tao Shi", "Chaochao Wang", "Jianping Lu", "Cheng Sun", "Yixin Wang", "Shengjie Yang", "Yuancheng Li", "Lu Jin", "Lisha Zhang", "Fu Bian", "Zhongkai Ye", "Lidong Pei", "Changyang Tu"], "title": "Streamlining Biomedical Research with Specialized LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel system that integrates state-of-the-art,\ndomain-specific large language models with advanced information retrieval\ntechniques to deliver comprehensive and context-aware responses. Our approach\nfacilitates seamless interaction among diverse components, enabling\ncross-validation of outputs to produce accurate, high-quality responses\nenriched with relevant data, images, tables, and other modalities. We\ndemonstrate the system's capability to enhance response precision by leveraging\na robust question-answering model, significantly improving the quality of\ndialogue generation. The system provides an accessible platform for real-time,\nhigh-fidelity interactions, allowing users to benefit from efficient\nhuman-computer interaction, precise retrieval, and simultaneous access to a\nwide range of literature and data. This dramatically improves the research\nefficiency of professionals in the biomedical and pharmaceutical domains and\nfacilitates faster, more informed decision-making throughout the R\\&D process.\nFurthermore, the system proposed in this paper is available at\nhttps://synapse-chat.patsnap.com.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12699", "pdf": "https://arxiv.org/pdf/2504.12699", "abs": "https://arxiv.org/abs/2504.12699", "authors": ["Jingjing Liu", "Zhiyong Wang", "Xinyu Fan", "Amirhossein Dadashzadeh", "Honghai Liu", "Majid Mirmehdi"], "title": "Unsupervised Cross-Domain 3D Human Pose Estimation via Pseudo-Label-Guided Global Transforms", "categories": ["cs.CV"], "comment": "11 pages, 6 figures, including appendix. This work has been submitted\n  to the IEEE for possible publication", "summary": "Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12344", "pdf": "https://arxiv.org/pdf/2504.12344", "abs": "https://arxiv.org/abs/2504.12344", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "title": "Propaganda via AI? A Study on Semantic Backdoors in Large Language Models", "categories": ["cs.CL"], "comment": "18 pages, 1 figure", "summary": "Large language models (LLMs) demonstrate remarkable performance across myriad\nlanguage tasks, yet they remain vulnerable to backdoor attacks, where\nadversaries implant hidden triggers that systematically manipulate model\noutputs. Traditional defenses focus on explicit token-level anomalies and\ntherefore overlook semantic backdoors-covert triggers embedded at the\nconceptual level (e.g., ideological stances or cultural references) that rely\non meaning-based cues rather than lexical oddities. We first show, in a\ncontrolled finetuning setting, that such semantic backdoors can be implanted\nwith only a small poisoned corpus, establishing their practical feasibility. We\nthen formalize the notion of semantic backdoors in LLMs and introduce a\nblack-box detection framework, RAVEN (short for \"Response Anomaly Vigilance for\nuncovering semantic backdoors\"), which combines semantic entropy with\ncross-model consistency analysis. The framework probes multiple models with\nstructured topic-perspective prompts, clusters the sampled responses via\nbidirectional entailment, and flags anomalously uniform outputs; cross-model\ncomparison isolates model-specific anomalies from corpus-wide biases. Empirical\nevaluations across diverse LLM families (GPT-4o, Llama, DeepSeek, Mistral)\nuncover previously undetected semantic backdoors, providing the first\nproof-of-concept evidence of these hidden vulnerabilities and underscoring the\nurgent need for concept-level auditing of deployed language models. We\nopen-source our code and data at https://github.com/NayMyatMin/RAVEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12709", "pdf": "https://arxiv.org/pdf/2504.12709", "abs": "https://arxiv.org/abs/2504.12709", "authors": ["Shumin Wang", "Zhuoran Yang", "Lidian Wang", "Zhipeng Tang", "Heng Li", "Lehan Pan", "Sha Zhang", "Jie Peng", "Jianmin Ji", "Yanyong Zhang"], "title": "Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The significant achievements of pre-trained models leveraging large volumes\nof data in the field of NLP and 2D vision inspire us to explore the potential\nof extensive data pre-training for 3D perception in autonomous driving. Toward\nthis goal, this paper proposes to utilize massive unlabeled data from\nheterogeneous datasets to pre-train 3D perception models. We introduce a\nself-supervised pre-training framework that learns effective 3D representations\nfrom scratch on unlabeled data, combined with a prompt adapter based domain\nadaptation strategy to reduce dataset bias. The approach significantly improves\nmodel performance on downstream tasks such as 3D object detection, BEV\nsegmentation, 3D object tracking, and occupancy prediction, and shows steady\nperformance increase as the training data volume scales up, demonstrating the\npotential of continually benefit 3D perception models for autonomous driving.\nWe will release the source code to inspire further investigations in the\ncommunity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12347", "pdf": "https://arxiv.org/pdf/2504.12347", "abs": "https://arxiv.org/abs/2504.12347", "authors": ["Mika Set√§l√§", "Pieta Sikstr√∂m", "Ville Heilala", "Tommi K√§rkk√§inen"], "title": "Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination", "categories": ["cs.CL", "cs.AI", "cs.CY", "K.3; I.2"], "comment": null, "summary": "Large language models (LLMs) have shown increasing promise in educational\nsettings, yet their mathematical reasoning has been considered evolving. This\nstudy evaluates the mathematical capabilities of various LLMs using the Finnish\nmatriculation examination, a high-stakes digital test for upper secondary\neducation. Initial tests yielded moderate performance corresponding to\nmid-range grades, but later evaluations demonstrated substantial improvements\nas the language models evolved. Remarkably, some models achieved near-perfect\nor perfect scores, matching top student performance and qualifying for\nuniversity admission. Our findings highlight the rapid advances in the\nmathematical proficiency of LLMs and illustrate their potential to also support\neducational assessments at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12739", "pdf": "https://arxiv.org/pdf/2504.12739", "abs": "https://arxiv.org/abs/2504.12739", "authors": ["Runyi Hu", "Jie Zhang", "Shiqian Zhao", "Nils Lukas", "Jiwei Li", "Qing Guo", "Han Qiu", "Tianwei Zhang"], "title": "Mask Image Watermarking", "categories": ["cs.CV"], "comment": "23 pages, 18 figures", "summary": "We present MaskMark, a simple, efficient and flexible framework for image\nwatermarking. MaskMark has two variants: MaskMark-D, which supports global\nwatermark embedding, watermark localization, and local watermark extraction for\napplications such as tamper detection, and MaskMark-ED, which focuses on local\nwatermark embedding and extraction with enhanced robustness in small regions,\nenabling localized image protection. Built upon the classical Encoder-\nDistortion-Decoder training paradigm, MaskMark-D introduces a simple masking\nmechanism during the decoding stage to support both global and local watermark\nextraction. A mask is applied to the watermarked image before extraction,\nallowing the decoder to focus on selected regions and learn local extraction. A\nlocalization module is also integrated into the decoder to identify watermark\nregions during inference, reducing interference from irrelevant content and\nimproving accuracy. MaskMark-ED extends this design by incorporating the mask\ninto the encoding stage as well, guiding the encoder to embed the watermark in\ndesignated local regions for enhanced robustness. Comprehensive experiments\nshow that MaskMark achieves state-of-the-art performance in global watermark\nextraction, local watermark extraction, watermark localization, and\nmulti-watermark embedding. It outperforms all existing baselines, including the\nrecent leading model WAM for local watermarking, while preserving high visual\nquality of the watermarked images. MaskMark is also flexible, by adjusting the\ndistortion layer, it can adapt to different robustness requirements with just a\nfew steps of fine-tuning. Moreover, our approach is efficient and easy to\noptimize, requiring only 20 hours on a single A6000 GPU with just 1/15 the\ncomputational cost of WAM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12747", "pdf": "https://arxiv.org/pdf/2504.12747", "abs": "https://arxiv.org/abs/2504.12747", "authors": ["Guanyu Wang", "Kailong Wang", "Yihao Huang", "Mingyi Zhou", "Zhang Qing cnwatcher", "Geguang Pu", "Li Li"], "title": "Privacy Protection Against Personalized Text-to-Image Synthesis via Cross-image Consistency Constraints", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of diffusion models and personalization techniques has\nmade it possible to recreate individual portraits from just a few publicly\navailable images. While such capabilities empower various creative\napplications, they also introduce serious privacy concerns, as adversaries can\nexploit them to generate highly realistic impersonations. To counter these\nthreats, anti-personalization methods have been proposed, which add adversarial\nperturbations to published images to disrupt the training of personalization\nmodels. However, existing approaches largely overlook the intrinsic multi-image\nnature of personalization and instead adopt a naive strategy of applying\nperturbations independently, as commonly done in single-image settings. This\nneglects the opportunity to leverage inter-image relationships for stronger\nprivacy protection. Therefore, we advocate for a group-level perspective on\nprivacy protection against personalization. Specifically, we introduce\nCross-image Anti-Personalization (CAP), a novel framework that enhances\nresistance to personalization by enforcing style consistency across perturbed\nimages. Furthermore, we develop a dynamic ratio adjustment strategy that\nadaptively balances the impact of the consistency loss throughout the attack\niterations. Extensive experiments on the classical CelebHQ and VGGFace2\nbenchmarks show that CAP substantially improves existing methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12360", "pdf": "https://arxiv.org/pdf/2504.12360", "abs": "https://arxiv.org/abs/2504.12360", "authors": ["Mieczys≈Çaw A. K≈Çopotek", "S≈Çawomir T. Wierzcho≈Ñ", "Bart≈Çomiej Starosta", "Dariusz Czerski", "Piotr Borkowski"], "title": "A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version", "categories": ["cs.CL", "cs.AI"], "comment": "1 figure, 17 pages, this is an extended version of a paper accepted\n  for the 25th International Conference on Computational Science (ICCS), 7-9\n  July 2025", "summary": "This paper investigates the problem of Graph Spectral Clustering with\nnegative similarities, resulting from document embeddings different from the\ntraditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for\ncombinatorial Laplacians and normalized Laplacians are discussed. An\nexperimental investigation shows the advantages and disadvantages of 6\ndifferent solutions proposed in the literature and in this research. The\nresearch demonstrates that GloVe embeddings frequently cause failures of\nnormalized Laplacian based GSC due to negative similarities. Furthermore,\napplication of methods curing similarity negativity leads to accuracy\nimprovement for both combinatorial and normalized Laplacian based GSC. It also\nleads to applicability for GloVe embeddings of explanation methods developed\noriginally bythe authors for Term Vector Space embeddings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12753", "pdf": "https://arxiv.org/pdf/2504.12753", "abs": "https://arxiv.org/abs/2504.12753", "authors": ["Siyu Chen", "Ting Han", "Changshe Zhang", "Xin Luo", "Meiliu Wu", "Guorong Cai", "Jinhe Su"], "title": "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Models (VFMs) have delivered remarkable performance in\nDomain Generalized Semantic Segmentation (DGSS). However, recent methods often\noverlook the fact that visual cues are susceptible, whereas the underlying\ngeometry remains stable, rendering depth information more robust. In this\npaper, we investigate the potential of integrating depth information with\nfeatures from VFMs, to improve the geometric consistency within an image and\nboost the generalization performance of VFMs. We propose a novel fine-tuning\nDGSS framework, named DepthForge, which integrates the visual cues from frozen\nDINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of\nthe VFMs, we incorporate depth-aware learnable tokens to continuously decouple\ndomain-invariant visual and spatial information, thereby enhancing depth\nawareness and attention of the VFMs. Finally, we develop a depth refinement\ndecoder and integrate it into the model architecture to adaptively refine\nmulti-layer VFM features and depth-aware learnable tokens. Extensive\nexperiments are conducted based on various DGSS settings and five different\ndatsets as unseen target domains. The qualitative and quantitative results\ndemonstrate that our method significantly outperforms alternative approaches\nwith stronger performance, steadier visual-spatial attention, and superior\ngeneralization ability. In particular, DepthForge exhibits outstanding\nperformance under extreme conditions (e.g., night and snow). Code is available\nat https://github.com/anonymouse-xzrptkvyqc/DepthForge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12474", "pdf": "https://arxiv.org/pdf/2504.12474", "abs": "https://arxiv.org/abs/2504.12474", "authors": ["Azadeh Beiranvand", "Seyed Mehdi Vahidipour"], "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12491", "pdf": "https://arxiv.org/pdf/2504.12491", "abs": "https://arxiv.org/abs/2504.12491", "authors": ["Hansi Zeng", "Kai Hui", "Honglei Zhuang", "Zhen Qin", "Zhenrui Yue", "Hamed Zamani", "Dana Alon"], "title": "Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?", "categories": ["cs.CL"], "comment": null, "summary": "While metrics available during pre-training, such as perplexity, correlate\nwell with model performance at scaling-laws studies, their predictive\ncapacities at a fixed model size remain unclear, hindering effective model\nselection and development. To address this gap, we formulate the task of\nselecting pre-training checkpoints to maximize downstream fine-tuning\nperformance as a pairwise classification problem: predicting which of two LLMs,\ndiffering in their pre-training, will perform better after supervised\nfine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants\nwith systematically varied pre-training configurations, e.g., objectives or\ndata, and evaluate them on diverse downstream tasks after SFT. We first conduct\na study and demonstrate that the conventional perplexity is a misleading\nindicator. As such, we introduce novel unsupervised and supervised proxy\nmetrics derived from pre-training that successfully reduce the relative\nperformance prediction error rate by over 50%. Despite the inherent complexity\nof this task, we demonstrate the practical utility of our proposed proxies in\nspecific scenarios, paving the way for more efficient design of pre-training\nschemes optimized for various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12494", "pdf": "https://arxiv.org/pdf/2504.12494", "abs": "https://arxiv.org/abs/2504.12494", "authors": ["Jianlin Shi", "Qiwei Gan", "Elizabeth Hanchrow", "Annie Bowles", "John Stanley", "Adam P. Bress", "Jordana B. Cohen", "Patrick R. Alba"], "title": "Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification", "categories": ["cs.CL"], "comment": "This manuscript has been submitted to AMIA 2025 annual symposium\n  (https://amia.org/education-events/amia-2025-annual-symposium)", "summary": "Clinical natural language processing (NLP) is increasingly in demand in both\nclinical research and operational practice. However, most of the\nstate-of-the-art solutions are transformers-based and require high\ncomputational resources, limiting their accessibility. We propose a hybrid NLP\nframework that integrates rule-based filtering, a Support Vector Machine (SVM)\nclassifier, and a BERT-based model to improve efficiency while maintaining\naccuracy. We applied this framework in a dementia identification case study\ninvolving 4.9 million veterans with incident hypertension, analyzing 2.1\nbillion clinical notes. At the patient level, our method achieved a precision\nof 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP\napproach identified over three times as many dementia cases as structured data\nmethods. All processing was completed in approximately two weeks using a single\nmachine with dual A40 GPUs. This study demonstrates the feasibility of hybrid\nNLP solutions for large-scale clinical text analysis, making state-of-the-art\nmethods more accessible to healthcare organizations with limited computational\nresources.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12809", "pdf": "https://arxiv.org/pdf/2504.12809", "abs": "https://arxiv.org/abs/2504.12809", "authors": ["Inzamamul Alam", "Md Tanvir Islam", "Simon S. Woo"], "title": "Saliency-Aware Diffusion Reconstruction for Effective Invisible Watermark Removal", "categories": ["cs.CV", "cs.MM", "I.4.5; I.5.4"], "comment": "Accepted at The Web Conference 2025", "summary": "As digital content becomes increasingly ubiquitous, the need for robust\nwatermark removal techniques has grown due to the inadequacy of existing\nembedding techniques, which lack robustness. This paper introduces a novel\nSaliency-Aware Diffusion Reconstruction (SADRE) framework for watermark\nelimination on the web, combining adaptive noise injection, region-specific\nperturbations, and advanced diffusion-based reconstruction. SADRE disrupts\nembedded watermarks by injecting targeted noise into latent representations\nguided by saliency masks although preserving essential image features. A\nreverse diffusion process ensures high-fidelity image restoration, leveraging\nadaptive noise levels determined by watermark strength. Our framework is\ntheoretically grounded with stability guarantees and achieves robust watermark\nremoval across diverse scenarios. Empirical evaluations on state-of-the-art\n(SOTA) watermarking techniques demonstrate SADRE's superiority in balancing\nwatermark disruption and image quality. SADRE sets a new benchmark for\nwatermark elimination, offering a flexible and reliable solution for real-world\nweb content. Code is available\non~\\href{https://github.com/inzamamulDU/SADRE}{\\textbf{https://github.com/inzamamulDU/SADRE}}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12516", "pdf": "https://arxiv.org/pdf/2504.12516", "abs": "https://arxiv.org/abs/2504.12516", "authors": ["Jason Wei", "Zhiqing Sun", "Spencer Papay", "Scott McKinney", "Jeffrey Han", "Isa Fulford", "Hyung Won Chung", "Alex Tachard Passos", "William Fedus", "Amelia Glaese"], "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents", "categories": ["cs.CL"], "comment": null, "summary": "We present BrowseComp, a simple yet challenging benchmark for measuring the\nability for agents to browse the web. BrowseComp comprises 1,266 questions that\nrequire persistently navigating the internet in search of hard-to-find,\nentangled information. Despite the difficulty of the questions, BrowseComp is\nsimple and easy-to-use, as predicted answers are short and easily verifiable\nagainst reference answers. BrowseComp for browsing agents can be seen as\nanalogous to how programming competitions are an incomplete but useful\nbenchmark for coding agents. While BrowseComp sidesteps challenges of a true\nuser query distribution, like generating long answers or resolving ambiguity,\nit measures the important core capability of exercising persistence and\ncreativity in finding information. BrowseComp can be found at\nhttps://github.com/openai/simple-evals.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12833", "pdf": "https://arxiv.org/pdf/2504.12833", "abs": "https://arxiv.org/abs/2504.12833", "authors": ["Elior Benarous", "Yilun Du", "Heng Yang"], "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "RLAIF", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12868", "pdf": "https://arxiv.org/pdf/2504.12868", "abs": "https://arxiv.org/abs/2504.12868", "authors": ["Agnieszka Anna Tomaka", "Leszek Luchowski", "Micha≈Ç Tarnawski", "Dariusz Pojda"], "title": "Computer-Aided Design of Personalized Occlusal Positioning Splints Using Multimodal 3D Data", "categories": ["cs.CV"], "comment": null, "summary": "Contemporary digital technology has a pivotal role in the design of\ncustomized medical appliances, including occlusal splints used in the treatment\nof stomatognathic system dysfunctions. We present an approach to computer-aided\ndesign and precision assessment of positioning occlusal splints, bridging\nclinical concepts with current digital dental practice. In our model, a 3D\nsplint is generated based on a transformation matrix that represents the\ntherapeutic change in mandibular position, defined by a specialist using a\nvirtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans\nand plaster model digitisation. The paper introduces a novel method for\ngenerating splints that accurately reproduce occlusal conditions in the\ntherapeutic position, including a mechanism for resolving surface conflicts\nthrough virtual embossing. We demonstrate how transformation matrices can be\nacquired through clinical tools and intraoral devices, and evaluate the\naccuracy of the designed and printed splints using profile and surface\ndeviation analysis. The proposed method enables reproducible, patient-specific\nsplint fabrication and opens new possibilities in diagnostics, multimodal image\nregistration and quantification of occlusal discrepancies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12869", "pdf": "https://arxiv.org/pdf/2504.12869", "abs": "https://arxiv.org/abs/2504.12869", "authors": ["Xi Tong", "Xing Luo", "Jiangxin Yang", "Yanpeng Cao"], "title": "SC3EF: A Joint Self-Correlation and Cross-Correspondence Estimation Framework for Visible and Thermal Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Multispectral imaging plays a critical role in a range of intelligent\ntransportation applications, including advanced driver assistance systems\n(ADAS), traffic monitoring, and night vision. However, accurate visible and\nthermal (RGB-T) image registration poses a significant challenge due to the\nconsiderable modality differences. In this paper, we present a novel joint\nSelf-Correlation and Cross-Correspondence Estimation Framework (SC3EF),\nleveraging both local representative features and global contextual cues to\neffectively generate RGB-T correspondences. For this purpose, we design a\nconvolution-transformer-based pipeline to extract local representative features\nand encode global correlations of intra-modality for inter-modality\ncorrespondence estimation between unaligned visible and thermal images. After\nmerging the local and global correspondence estimation results, we further\nemploy a hierarchical optical flow estimation decoder to progressively refine\nthe estimated dense correspondence maps. Extensive experiments demonstrate the\neffectiveness of our proposed method, outperforming the current\nstate-of-the-art (SOTA) methods on representative RGB-T datasets. Furthermore,\nit also shows competitive generalization capabilities across challenging\nscenarios, including large parallax, severe occlusions, adverse weather, and\nother cross-modal datasets (e.g., RGB-N and RGB-D).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12970", "pdf": "https://arxiv.org/pdf/2504.12970", "abs": "https://arxiv.org/abs/2504.12970", "authors": ["Long Qian", "Bingke Zhu", "Yingying Chen", "Ming Tang", "Jinqiao Wang"], "title": "MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven Bi-Level Optimization for Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection is a crucial task in computer vision, yet collecting\nreal-world defect images is inherently difficult due to the rarity and\nunpredictability of anomalies. Consequently, researchers have turned to\nsynthetic methods for training data augmentation. However, existing synthetic\nstrategies (e.g., naive cut-and-paste or inpainting) overlook the underlying\nphysical causes of defects, leading to inconsistent, low-fidelity anomalies\nthat hamper model generalization to real-world complexities. In this thesis, we\nintroduced a novel pipeline that generates synthetic anomalies through\nMath-Physics model guidance, refines them via a Coarse-to-Fine approach and\nemploys a bi-level optimization strategy with a Synthesis Quality\nEstimator(SQE). By incorporating physical modeling of cracks, corrosion, and\ndeformation, our method produces realistic defect masks, which are subsequently\nenhanced in two phases. The first stage (npcF) enforces a PDE-based consistency\nto achieve a globally coherent anomaly structure, while the second stage\n(npcF++) further improves local fidelity using wavelet transforms and boundary\nsynergy blocks. Additionally, we leverage SQE-driven weighting, ensuring that\nhigh-quality synthetic samples receive greater emphasis during training. To\nvalidate our approach, we conducted comprehensive experiments on three widely\nadopted industrial anomaly detection benchmarks: MVTec AD, VisA, and BTAD.\nAcross these datasets, the proposed pipeline achieves state-of-the-art (SOTA)\nresults in both image-AUROC and pixel-AUROC, confirming the effectiveness of\nour MaPhC2F and BiSQAD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663", "abs": "https://arxiv.org/abs/2504.12663", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12997", "pdf": "https://arxiv.org/pdf/2504.12997", "abs": "https://arxiv.org/abs/2504.12997", "authors": ["Jiancheng Zhao", "Xiang Ji", "Zhuoxiao Li", "Zunian Wan", "Weihang Ran", "Mingze Ma", "Muyao Niu", "Yifan Zhan", "Cheng-Ching Tseng", "Yinqiang Zheng"], "title": "All-in-One Transferring Image Compression from Human Perception to Multi-Machine Perception", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. To address these limitations,\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\nwithin a single model. Our method introduces a shared adaptor to learn general\nsemantic features and task-specific adaptors to preserve task-level\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\nour method achieves strong performance across multiple tasks while maintaining\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\ntransferring.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13015", "pdf": "https://arxiv.org/pdf/2504.13015", "abs": "https://arxiv.org/abs/2504.13015", "authors": ["Guoqing Zhang", "Jingyun Yang", "Yang Li"], "title": "Hierarchical Feature Learning for Medical Point Clouds via State Space Model", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "Deep learning-based point cloud modeling has been widely investigated as an\nindispensable component of general shape analysis. Recently, transformer and\nstate space model (SSM) have shown promising capacities in point cloud\nlearning. However, limited research has been conducted on medical point clouds,\nwhich have great potential in disease diagnosis and treatment. This paper\npresents an SSM-based hierarchical feature learning framework for medical point\ncloud understanding. Specifically, we down-sample input into multiple levels\nthrough the farthest point sampling. At each level, we perform a series of\nk-nearest neighbor (KNN) queries to aggregate multi-scale structural\ninformation. To assist SSM in processing point clouds, we introduce\ncoordinate-order and inside-out scanning strategies for efficient serialization\nof irregular points. Point features are calculated progressively from short\nneighbor sequences and long point sequences through vanilla and group Point SSM\nblocks, to capture both local patterns and long-range dependencies. To evaluate\nthe proposed method, we build a large-scale medical point cloud dataset named\nMedPointS for anatomy classification, completion, and segmentation. Extensive\nexperiments conducted on MedPointS demonstrate that our method achieves\nsuperior performance across all tasks. The dataset is available at\nhttps://flemme-docs.readthedocs.io/en/latest/medpoints.html. Code is merged to\na public medical imaging platform: https://github.com/wlsdzyzl/flemme.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12691", "pdf": "https://arxiv.org/pdf/2504.12691", "abs": "https://arxiv.org/abs/2504.12691", "authors": ["Yiyou Sun", "Yu Gai", "Lijie Chen", "Abhilasha Ravichander", "Yejin Choi", "Dawn Song"], "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13024", "pdf": "https://arxiv.org/pdf/2504.13024", "abs": "https://arxiv.org/abs/2504.13024", "authors": ["Daniel Gonzalez-Alvarado", "Fabio Schlindwein", "Jonas Cassel", "Laura Steingruber", "Stefania Petra", "Christoph Schn√∂rr"], "title": "Riemannian Patch Assignment Gradient Flows", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces patch assignment flows for metric data labeling on\ngraphs. Labelings are determined by regularizing initial local labelings\nthrough the dynamic interaction of both labels and label assignments across the\ngraph, entirely encoded by a dictionary of competing labeled patches and\nmediated by patch assignment variables. Maximal consistency of patch\nassignments is achieved by geometric numerical integration of a Riemannian\nascent flow, as critical point of a Lagrangian action functional. Experiments\nillustrate properties of the approach, including uncertainty quantification of\nlabel assignments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12723", "pdf": "https://arxiv.org/pdf/2504.12723", "abs": "https://arxiv.org/abs/2504.12723", "authors": ["James Hale", "Sushrita Rakshit", "Kushal Chawla", "Jeanne M. Brett", "Jonathan Gratch"], "title": "KODIS: A Multicultural Dispute Resolution Dialogue Corpus", "categories": ["cs.CL"], "comment": null, "summary": "We present KODIS, a dyadic dispute resolution corpus containing thousands of\ndialogues from over 75 countries. Motivated by a theoretical model of culture\nand conflict, participants engage in a typical customer service dispute\ndesigned by experts to evoke strong emotions and conflict. The corpus contains\na rich set of dispositional, process, and outcome measures. The initial\nanalysis supports theories of how anger expressions lead to escalatory spirals\nand highlights cultural differences in emotional expression. We make this\ncorpus and data collection framework available to the community.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13035", "pdf": "https://arxiv.org/pdf/2504.13035", "abs": "https://arxiv.org/abs/2504.13035", "authors": ["WonJun Moon", "Cheol-Ho Cho", "Woojin Jun", "Minho Shim", "Taeoh Kim", "Inwoong Lee", "Dongyoon Wee", "Jae-Pil Heo"], "title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13042", "pdf": "https://arxiv.org/pdf/2504.13042", "abs": "https://arxiv.org/abs/2504.13042", "authors": ["Dachun Kai", "Yueyi Zhang", "Jin Wang", "Zeyu Xiao", "Zhiwei Xiong", "Xiaoyan Sun"], "title": "Event-Enhanced Blurry Video Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/", "summary": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12773", "pdf": "https://arxiv.org/pdf/2504.12773", "abs": "https://arxiv.org/abs/2504.12773", "authors": ["Yicheng Pan", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Jun Du", "Jianshu Zhang", "Quan Liu", "Jianqing Gao", "Feng Ma"], "title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 5 figures", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12805", "pdf": "https://arxiv.org/pdf/2504.12805", "abs": "https://arxiv.org/abs/2504.12805", "authors": ["Takaya Arita", "Wenxian Zheng", "Reiji Suzuki", "Fuminori Akiba"], "title": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "30 pages, 13 figures, 1 table", "summary": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13065", "pdf": "https://arxiv.org/pdf/2504.13065", "abs": "https://arxiv.org/abs/2504.13065", "authors": ["Yang Yue", "Yulin Wang", "Haojun Jiang", "Pan Liu", "Shiji Song", "Gao Huang"], "title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Echocardiography is crucial for cardiovascular disease detection but relies\nheavily on experienced sonographers. Echocardiography probe guidance systems,\nwhich provide real-time movement instructions for acquiring standard plane\nimages, offer a promising solution for AI-assisted or fully autonomous\nscanning. However, developing effective machine learning models for this task\nremains challenging, as they must grasp heart anatomy and the intricate\ninterplay between probe motion and visual signals. To address this, we present\nEchoWorld, a motion-aware world modeling framework for probe guidance that\nencodes anatomical knowledge and motion-induced visual dynamics, while\neffectively leveraging past visual-motion sequences to enhance guidance\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\nprinciples, where the model predicts masked anatomical regions and simulates\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\neffectively integrates historical visual-motion data, enabling precise and\nadaptive probe guidance. Trained on more than one million ultrasound images\nfrom over 200 routine scans, EchoWorld effectively captures key\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\nour method significantly reduces guidance errors compared to existing visual\nbackbones and guidance frameworks, excelling in both single-frame and\nsequential evaluation protocols. Code is available at\nhttps://github.com/LeapLabTHU/EchoWorld.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13074", "pdf": "https://arxiv.org/pdf/2504.13074", "abs": "https://arxiv.org/abs/2504.13074", "authors": ["Guibin Chen", "Dixuan Lin", "Jiangping Yang", "Chunze Lin", "Juncheng Zhu", "Mingyuan Fan", "Hao Zhang", "Sheng Chen", "Zheng Chen", "Chengchen Ma", "Weiming Xiong", "Wei Wang", "Nuo Pang", "Kang Kang", "Zhiheng Xu", "Yuzhe Jin", "Yupeng Liang", "Yubing Song", "Peng Zhao", "Boyuan Xu", "Di Qiu", "Debang Li", "Zhengcong Fei", "Yang Li", "Yahui Zhou"], "title": "SkyReels-V2: Infinite-length Film Generative Model", "categories": ["cs.CV"], "comment": "31 pages,10 figures", "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12891", "pdf": "https://arxiv.org/pdf/2504.12891", "abs": "https://arxiv.org/abs/2504.12891", "authors": ["Vicent Briva-Iglesias"], "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12898", "pdf": "https://arxiv.org/pdf/2504.12898", "abs": "https://arxiv.org/abs/2504.12898", "authors": ["Zhouhao Sun", "Xiao Ding", "Li Du", "Yunpeng Xu", "Yixuan Ma", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13078", "pdf": "https://arxiv.org/pdf/2504.13078", "abs": "https://arxiv.org/abs/2504.13078", "authors": ["Riza Velioglu", "Petra Bevandic", "Robin Chan", "Barbara Hammer"], "title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12911", "pdf": "https://arxiv.org/pdf/2504.12911", "abs": "https://arxiv.org/abs/2504.12911", "authors": ["Chengyi Ju", "Weijie Shi", "Chengzhong Liu", "Jiaming Ji", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "Benchmarking Multi-National Value Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13099", "pdf": "https://arxiv.org/pdf/2504.13099", "abs": "https://arxiv.org/abs/2504.13099", "authors": ["Ranjan Sapkota", "Rahul Harsha Cheppally", "Ajay Sharda", "Manoj Karkee"], "title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity", "categories": ["cs.CV"], "comment": null, "summary": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12951", "pdf": "https://arxiv.org/pdf/2504.12951", "abs": "https://arxiv.org/abs/2504.12951", "authors": ["Nearchos Potamitis", "Akhil Arora"], "title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 16 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2405.06691", "summary": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120", "abs": "https://arxiv.org/abs/2504.13120", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12996", "pdf": "https://arxiv.org/pdf/2504.12996", "abs": "https://arxiv.org/abs/2504.12996", "authors": ["Saransh Agrawal", "Kuan-Hao Huang"], "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, In Proceedings of The 19th International Workshop on\n  Semantic Evaluation (SemEval), 2025", "summary": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13140", "pdf": "https://arxiv.org/pdf/2504.13140", "abs": "https://arxiv.org/abs/2504.13140", "authors": ["Jongseo Lee", "Wooil Lee", "Gyeong-Moon Park", "Seong Tae Kim", "Jinwoo Choi"], "title": "PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition", "categories": ["cs.CV"], "comment": "This paper is accepted by CVPRW 2025", "summary": "Human action recognition (HAR) has achieved impressive results with deep\nlearning models, but their decision-making process remains opaque due to their\nblack-box nature. Ensuring interpretability is crucial, especially for\nreal-world applications requiring transparency and accountability. Existing\nvideo XAI methods primarily rely on feature attribution or static textual\nconcepts, both of which struggle to capture motion dynamics and temporal\ndependencies essential for action understanding. To address these challenges,\nwe propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR),\na novel concept bottleneck framework that introduces human pose sequences as\nmotion-aware, structured concepts for video action recognition. Unlike methods\nbased on pixel-level features or static textual descriptions, PCBEAR leverages\nhuman skeleton poses, which focus solely on body movements, providing robust\nand interpretable explanations of motion dynamics. We define two types of\npose-based concepts: static pose concepts for spatial configurations at\nindividual frames, and dynamic pose concepts for motion patterns across\nmultiple frames. To construct these concepts, PCBEAR applies clustering to\nvideo pose sequences, allowing for automatic discovery of meaningful concepts\nwithout manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500,\nshowing that it achieves high classification performance while offering\ninterpretable, motion-driven explanations. Our method provides both strong\npredictive performance and human-understandable insights into the model's\nreasoning process, enabling test-time interventions for debugging and improving\nmodel behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13023", "pdf": "https://arxiv.org/pdf/2504.13023", "abs": "https://arxiv.org/abs/2504.13023", "authors": ["Sangwook Kim", "Soonyoung Lee", "Jongseong Jang"], "title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13152", "pdf": "https://arxiv.org/pdf/2504.13152", "abs": "https://arxiv.org/abs/2504.13152", "authors": ["Haiwen Feng", "Junyi Zhang", "Qianqian Wang", "Yufei Ye", "Pengcheng Yu", "Michael J. Black", "Trevor Darrell", "Angjoo Kanazawa"], "title": "St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World", "categories": ["cs.CV"], "comment": "Project page: https://St4RTrack.github.io/", "summary": "Dynamic 3D reconstruction and point tracking in videos are typically treated\nas separate tasks, despite their deep connection. We propose St4RTrack, a\nfeed-forward framework that simultaneously reconstructs and tracks dynamic\nvideo content in a world coordinate frame from RGB inputs. This is achieved by\npredicting two appropriately defined pointmaps for a pair of frames captured at\ndifferent moments. Specifically, we predict both pointmaps at the same moment,\nin the same world, capturing both static and dynamic scene geometry while\nmaintaining 3D correspondences. Chaining these predictions through the video\nsequence with respect to a reference frame naturally computes long-range\ncorrespondences, effectively combining 3D reconstruction with 3D tracking.\nUnlike prior methods that rely heavily on 4D ground truth supervision, we\nemploy a novel adaptation scheme based on a reprojection loss. We establish a\nnew extensive benchmark for world-frame reconstruction and tracking,\ndemonstrating the effectiveness and efficiency of our unified, data-driven\nframework. Our code, model, and benchmark will be released.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13153", "pdf": "https://arxiv.org/pdf/2504.13153", "abs": "https://arxiv.org/abs/2504.13153", "authors": ["Shaohui Dai", "Yansong Qu", "Zheyan Li", "Xinyang Li", "Shengchuan Zhang", "Liujuan Cao"], "title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting with Superpoint Graphs", "categories": ["cs.CV"], "comment": null, "summary": "Bridging natural language and 3D geometry is a crucial step toward flexible,\nlanguage-driven scene understanding. While recent advances in 3D Gaussian\nSplatting (3DGS) have enabled fast and high-quality scene reconstruction,\nresearch has also explored incorporating open-vocabulary understanding into\n3DGS. However, most existing methods require iterative optimization over\nper-view 2D semantic feature maps, which not only results in inefficiencies but\nalso leads to inconsistent 3D semantics across views. To address these\nlimitations, we introduce a training-free framework that constructs a\nsuperpoint graph directly from Gaussian primitives. The superpoint graph\npartitions the scene into spatially compact and semantically coherent regions,\nforming view-consistent 3D entities and providing a structured foundation for\nopen-vocabulary understanding. Based on the graph structure, we design an\nefficient reprojection strategy that lifts 2D semantic features onto the\nsuperpoints, avoiding costly multi-view iterative training. The resulting\nrepresentation ensures strong 3D semantic coherence and naturally supports\nhierarchical understanding, enabling both coarse- and fine-grained\nopen-vocabulary perception within a unified semantic field. Extensive\nexperiments demonstrate that our method achieves state-of-the-art\nopen-vocabulary segmentation performance, with semantic field reconstruction\ncompleted over $30\\times$ faster. Our code will be available at\nhttps://github.com/Atrovast/THGS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13125", "pdf": "https://arxiv.org/pdf/2504.13125", "abs": "https://arxiv.org/abs/2504.13125", "authors": ["Varun Rao", "Youran Sun", "Mahendra Kumar", "Tejas Mutneja", "Agastya Mukherjee", "Haizhao Yang"], "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134", "abs": "https://arxiv.org/abs/2504.13134", "authors": ["Anamika Lochab", "Ruqi Zhang"], "title": "Energy-Based Reward Models for Robust Language Model Alignment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "alignment", "reward hacking"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13139", "pdf": "https://arxiv.org/pdf/2504.13139", "abs": "https://arxiv.org/abs/2504.13139", "authors": ["Jo√£o Loula", "Benjamin LeBrun", "Li Du", "Ben Lipkin", "Clemente Pasti", "Gabriel Grand", "Tianyu Liu", "Yahya Emara", "Marjorie Freedman", "Jason Eisner", "Ryan Cotterel", "Vikash Mansinghka", "Alexander K. Lew", "Tim Vieira", "Timothy J. O'Donnell"], "title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, 4 figures", "summary": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13167", "pdf": "https://arxiv.org/pdf/2504.13167", "abs": "https://arxiv.org/abs/2504.13167", "authors": ["Zetong Zhang", "Manuel kaufmann", "Lixin Xue", "Jie Song", "Martin R. Oswald"], "title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos", "categories": ["cs.CV", "I.4.5"], "comment": "Accepted at CVPR 2025", "summary": "Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13161", "pdf": "https://arxiv.org/pdf/2504.13161", "abs": "https://arxiv.org/abs/2504.13161", "authors": ["Shizhe Diao", "Yu Yang", "Yonggan Fu", "Xin Dong", "Dan Su", "Markus Kliegl", "Zijia Chen", "Peter Belcak", "Yoshi Suhara", "Hongxu Yin", "Mostofa Patwary", "Yingyan", "Lin", "Jan Kautz", "Pavlo Molchanov"], "title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12319", "pdf": "https://arxiv.org/pdf/2504.12319", "abs": "https://arxiv.org/abs/2504.12319", "authors": ["Duc Tuyen TA", "Wajdi Ben Saad", "Ji Young Oh"], "title": "Specialized text classification: an approach to classifying Open Banking transactions", "categories": ["cs.IR", "cs.AI", "cs.CL", "q-fin.CP"], "comment": null, "summary": "With the introduction of the PSD2 regulation in the EU which established the\nOpen Banking framework, a new window of opportunities has opened for banks and\nfintechs to explore and enrich Bank transaction descriptions with the aim of\nbuilding a better understanding of customer behavior, while using this\nunderstanding to prevent fraud, reduce risks and offer more competitive and\ntailored services.\n  And although the usage of natural language processing models and techniques\nhas seen an incredible progress in various applications and domains over the\npast few years, custom applications based on domain-specific text corpus remain\nunaddressed especially in the banking sector.\n  In this paper, we introduce a language-based Open Banking transaction\nclassification system with a focus on the french market and french language\ntext. The system encompasses data collection, labeling, preprocessing,\nmodeling, and evaluation stages. Unlike previous studies that focus on general\nclassification approaches, this system is specifically tailored to address the\nchallenges posed by training a language model with a specialized text corpus\n(Banking data in the French context). By incorporating language-specific\ntechniques and domain knowledge, the proposed system demonstrates enhanced\nperformance and efficiency compared to generic approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13181", "pdf": "https://arxiv.org/pdf/2504.13181", "abs": "https://arxiv.org/abs/2504.13181", "authors": ["Daniel Bolya", "Po-Yao Huang", "Peize Sun", "Jang Hyun Cho", "Andrea Madotto", "Chen Wei", "Tengyu Ma", "Jiale Zhi", "Jathushan Rajasegaran", "Hanoona Rasheed", "Junke Wang", "Marco Monteiro", "Hu Xu", "Shiyu Dong", "Nikhila Ravi", "Daniel Li", "Piotr Doll√°r", "Christoph Feichtenhofer"], "title": "Perception Encoder: The best visual embeddings are not at the output of the network", "categories": ["cs.CV"], "comment": "Initial Submission", "summary": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12356", "pdf": "https://arxiv.org/pdf/2504.12356", "abs": "https://arxiv.org/abs/2504.12356", "authors": ["Sidun Liu", "Wenyu Li", "Peng Qiao", "Yong Dou"], "title": "Regist3R: Incremental Registration with Stereo Foundation Model", "categories": ["eess.IV", "cs.CV"], "comment": "19 pages", "summary": "Multi-view 3D reconstruction has remained an essential yet challenging\nproblem in the field of computer vision. While DUSt3R and its successors have\nachieved breakthroughs in 3D reconstruction from unposed images, these methods\nexhibit significant limitations when scaling to multi-view scenarios, including\nhigh computational cost and cumulative error induced by global alignment. To\naddress these challenges, we propose Regist3R, a novel stereo foundation model\ntailored for efficient and scalable incremental reconstruction. Regist3R\nleverages an incremental reconstruction paradigm, enabling large-scale 3D\nreconstructions from unordered and many-view image collections. We evaluate\nRegist3R on public datasets for camera pose estimation and 3D reconstruction.\nOur experiments demonstrate that Regist3R achieves comparable performance with\noptimization-based methods while significantly improving computational\nefficiency, and outperforms existing multi-view reconstruction models.\nFurthermore, to assess its performance in real-world applications, we introduce\na challenging oblique aerial dataset which has long spatial spans and hundreds\nof views. The results highlight the effectiveness of Regist3R. We also\ndemonstrate the first attempt to reconstruct large-scale scenes encompassing\nover thousands of views through pointmap-based foundation models, showcasing\nits potential for practical applications in large-scale 3D reconstruction\ntasks, including urban modeling, aerial mapping, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12682", "pdf": "https://arxiv.org/pdf/2504.12682", "abs": "https://arxiv.org/abs/2504.12682", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12644", "pdf": "https://arxiv.org/pdf/2504.12644", "abs": "https://arxiv.org/abs/2504.12644", "authors": ["Reek Majumder", "Mashrur Chowdhury", "Sakib Mahmud Khan", "Zadid Khan", "Fahim Ahmad", "Frank Ngeni", "Gurcan Comert", "Judith Mwakalonge", "Dimitra Michalaka"], "title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.ET"], "comment": null, "summary": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12680", "pdf": "https://arxiv.org/pdf/2504.12680", "abs": "https://arxiv.org/abs/2504.12680", "authors": ["Baining Zhao", "Ziyou Wang", "Jianjie Fang", "Chen Gao", "Fanhang Man", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li", "Wenwu Zhu"], "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning", "categories": ["cs.AI", "cs.CV"], "comment": "12 pages, 5 figures", "summary": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12977", "pdf": "https://arxiv.org/pdf/2504.12977", "abs": "https://arxiv.org/abs/2504.12977", "authors": ["Maksim Vishnevskiy"], "title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "comment": "12 pages, no figures", "summary": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12788", "pdf": "https://arxiv.org/pdf/2504.12788", "abs": "https://arxiv.org/abs/2504.12788", "authors": ["Xiao Han", "Runze Tian", "Yifei Tong", "Fenggen Yu", "Dingyao Liu", "Yan Zhang"], "title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Drag-driven editing has become popular among designers for its ability to\nmodify complex geometric structures through simple and intuitive manipulation,\nallowing users to adjust and reshape content with minimal technical skill. This\ndrag operation has been incorporated into numerous methods to facilitate the\nediting of 2D images and 3D meshes in design. However, few studies have\nexplored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)\nrepresentation, as deforming 3DGS while preserving shape coherence and visual\ncontinuity remains challenging. In this paper, we introduce ARAP-GS, a\ndrag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)\ndeformation. Unlike previous 3DGS editing methods, we are the first to apply\nARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven\ngeometric transformations. To preserve scene appearance after deformation, we\nincorporate an advanced diffusion prior for image super-resolution within our\niterative optimization process. This approach enhances visual quality while\nmaintaining multi-view consistency in the edited results. Experiments show that\nARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its\neffectiveness and superiority for drag-driven 3DGS editing. Additionally, our\nmethod is highly efficient, requiring only 10 to 20 minutes to edit a scene on\na single RTX 3090 GPU.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13085", "pdf": "https://arxiv.org/pdf/2504.13085", "abs": "https://arxiv.org/abs/2504.13085", "authors": ["Georgina Curto", "Svetlana Kiritchenko", "Muhammad Hammad Fahim Siddiqui", "Isar Nejadgholi", "Kathleen C. Fraser"], "title": "Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia", "categories": ["cs.CY", "cs.CL"], "comment": "In Findings of the Association for Computational Linguistics: NAACL\n  2025", "summary": "Eradicating poverty is the first goal in the United Nations Sustainable\nDevelopment Goals. However, aporophobia -- the societal bias against people\nliving in poverty -- constitutes a major obstacle to designing, approving and\nimplementing poverty-mitigation policies. This work presents an initial step\ntowards operationalizing the concept of aporophobia to identify and track\nharmful beliefs and discriminative actions against poor people on social media.\nIn close collaboration with non-profits and governmental organizations, we\nconduct data collection and exploration. Then we manually annotate a corpus of\nEnglish tweets from five world regions for the presence of (1) direct\nexpressions of aporophobia, and (2) statements referring to or criticizing\naporophobic views or actions of others, to comprehensively characterize the\nsocial media discourse related to bias and discrimination against the poor.\nBased on the annotated data, we devise a taxonomy of categories of aporophobic\nattitudes and actions expressed through speech on social media. Finally, we\ntrain several classifiers and identify the main challenges for automatic\ndetection of aporophobia in social networks. This work paves the way towards\nidentifying, tracking, and mitigating aporophobic views on social media at\nscale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13120", "pdf": "https://arxiv.org/pdf/2504.13120", "abs": "https://arxiv.org/abs/2504.13120", "authors": ["Yongqian Peng", "Yuxi Ma", "Mengmeng Wang", "Yuxuan Wang", "Yizhou Wang", "Chi Zhang", "Yixin Zhu", "Zilong Zheng"], "title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://ppyyqq.github.io/aicc/ The first two authors\n  contribute equally", "summary": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12811", "pdf": "https://arxiv.org/pdf/2504.12811", "abs": "https://arxiv.org/abs/2504.12811", "authors": ["Michael Steiner", "Thomas K√∂hler", "Lukas Radl", "Felix Windisch", "Dieter Schmalstieg", "Markus Steinberger"], "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12817", "pdf": "https://arxiv.org/pdf/2504.12817", "abs": "https://arxiv.org/abs/2504.12817", "authors": ["Nassim Belmecheri", "Arnaud Gotlieb", "Nadjib Lazaar", "Helge Spieker"], "title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Workshop \"Advancing Automated Driving in Highly Interactive Scenarios\n  through Behavior Prediction, Trustworthy AI, and Remote Operations\" @ 36th\n  IEEE Intelligent Vehicles Symposium (IV)", "summary": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13171", "pdf": "https://arxiv.org/pdf/2504.13171", "abs": "https://arxiv.org/abs/2504.13171", "authors": ["Kevin Lin", "Charlie Snell", "Yu Wang", "Charles Packer", "Sarah Wooders", "Ion Stoica", "Joseph E. Gonzalez"], "title": "Sleep-time Compute: Beyond Inference Scaling at Test-time", "categories": ["cs.AI", "cs.CL"], "comment": "Code and data released at:\n  https://github.com/letta-ai/sleep-time-compute", "summary": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12856", "pdf": "https://arxiv.org/pdf/2504.12856", "abs": "https://arxiv.org/abs/2504.12856", "authors": ["Yifeng Cheng", "Juan Du"], "title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO", "I.5.4"], "comment": null, "summary": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.12908", "pdf": "https://arxiv.org/pdf/2504.12908", "abs": "https://arxiv.org/abs/2504.12908", "authors": ["Yuyang Li", "Wenxin Du", "Chang Yu", "Puhao Li", "Zihang Zhao", "Tengyu Liu", "Chenfanfu Jiang", "Yixin Zhu", "Siyuan Huang"], "title": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation", "categories": ["cs.RO", "cs.CV"], "comment": "17 pages, 7 figures", "summary": "Tactile sensing is crucial for achieving human-level robotic capabilities in\nmanipulation tasks. VBTSs have emerged as a promising solution, offering high\nspatial resolution and cost-effectiveness by sensing contact through\ncamera-captured deformation patterns of elastic gel pads. However, these\nsensors' complex physical characteristics and visual signal processing\nrequirements present unique challenges for robotic applications. The lack of\nefficient and accurate simulation tools for VBTS has significantly limited the\nscale and scope of tactile robotics research. Here we present Taccel, a\nhigh-performance simulation platform that integrates IPC and ABD to model\nrobots, tactile sensors, and objects with both accuracy and unprecedented\nspeed, achieving an 18-fold acceleration over real-time across thousands of\nparallel environments. Unlike previous simulators that operate at sub-real-time\nspeeds with limited parallelization, Taccel provides precise physics simulation\nand realistic tactile signals while supporting flexible robot-sensor\nconfigurations through user-friendly APIs. Through extensive validation in\nobject recognition, robotic grasping, and articulated object manipulation, we\ndemonstrate precise simulation and successful sim-to-real transfer. These\ncapabilities position Taccel as a powerful tool for scaling up tactile robotics\nresearch and development. By enabling large-scale simulation and\nexperimentation with tactile sensing, Taccel accelerates the development of\nmore capable robotic systems, potentially transforming how robots interact with\nand understand their physical environment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13022", "pdf": "https://arxiv.org/pdf/2504.13022", "abs": "https://arxiv.org/abs/2504.13022", "authors": ["Xiangrui Liu", "Xinju Wu", "Shiqi Wang", "Zhu Li", "Sam Kwong"], "title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation", "categories": ["cs.GR", "cs.CV"], "comment": "Submitted to a journal", "summary": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13023", "pdf": "https://arxiv.org/pdf/2504.13023", "abs": "https://arxiv.org/abs/2504.13023", "authors": ["Sangwook Kim", "Soonyoung Lee", "Jongseong Jang"], "title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13037", "pdf": "https://arxiv.org/pdf/2504.13037", "abs": "https://arxiv.org/abs/2504.13037", "authors": ["Yundi Zhang", "Paul Hager", "Che Liu", "Suprosanna Shit", "Chen Chen", "Daniel Rueckert", "Jiazhen Pan"], "title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
{"id": "2504.13131", "pdf": "https://arxiv.org/pdf/2504.13131", "abs": "https://arxiv.org/abs/2504.13131", "authors": ["Xin Li", "Kun Yuan", "Bingchen Li", "Fengbin Guan", "Yizhen Shao", "Zihao Yu", "Xijun Wang", "Yiting Lu", "Wei Luo", "Suhang Yao", "Ming Sun", "Chao Zhou", "Zhibo Chen", "Radu Timofte", "Yabin Zhang", "Ao-Xiang Zhang", "Tianwu Zhi", "Jianzhao Liu", "Yang Li", "Jingwen Xu", "Yiting Liao", "Yushen Zuo", "Mingyang Wu", "Renjie Li", "Shengyun Zhong", "Zhengzhong Tu", "Yufan Liu", "Xiangguang Chen", "Zuowei Cao", "Minhao Tang", "Shan Liu", "Kexin Zhang", "Jingfen Xie", "Yan Wang", "Kai Chen", "Shijie Zhao", "Yunchen Zhang", "Xiangkai Xu", "Hong Gao", "Ji Shi", "Yiming Bao", "Xiugang Dong", "Xiangsheng Zhou", "Yaofeng Tu", "Ying Liang", "Yiwen Wang", "Xinning Chai", "Yuxuan Zhang", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song", "Wei Sun", "Kang Fu", "Linhan Cao", "Dandan Zhu", "Kaiwei Zhang", "Yucheng Zhu", "Zicheng Zhang", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Zhi Jin", "Jiawei Wu", "Wei Wang", "Wenjian Zhang", "Yuhai Lan", "Gaoxiong Yi", "Hengyuan Na", "Wang Luo", "Di Wu", "MingYin Bai", "Jiawang Du", "Zilong Lu", "Zhenyu Jiang", "Hui Zeng", "Ziguan Cui", "Zongliang Gan", "Guijin Tang", "Xinglin Xie", "Kehuan Song", "Xiaoqiang Lu", "Licheng Jiao", "Fang Liu", "Xu Liu", "Puhua Chen", "Ha Thu Nguyen", "Katrien De Moor", "Seyed Ali Amirshahi", "Mohamed-Chaker Larabi", "Qi Tang", "Linfeng He", "Zhiyong Gao", "Zixuan Gao", "Guohua Zhang", "Zhiye Huang", "Yi Deng", "Qingmiao Jiang", "Lu Chen", "Yi Yang", "Xi Liao", "Nourine Mohammed Nadir", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Meiqin Liu", "Chao Yao", "Yao Zhao"], "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Challenge Report of NTIRE 2025; Methods from 18 Teams; Accepted by\n  CVPR Workshop; 21 pages", "summary": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-20.jsonl"}
