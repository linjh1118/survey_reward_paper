{"id": "2504.15776", "pdf": "https://arxiv.org/pdf/2504.15776", "abs": "https://arxiv.org/abs/2504.15776", "authors": ["Quentin Herau", "Nathan Piasco", "Moussab Bennehar", "Luis Rolado", "Dzmitry Tsishkou", "Bingbing Liu", "Cyrille Migniot", "Pascal Vasseur", "CÃ©dric Demonceaux"], "title": "Pose Optimization for Autonomous Driving Datasets using Neural Rendering Models", "categories": ["cs.CV", "cs.RO"], "comment": "under review", "summary": "Autonomous driving systems rely on accurate perception and localization of\nthe ego car to ensure safety and reliability in challenging real-world driving\nscenarios. Public datasets play a vital role in benchmarking and guiding\nadvancement in research by providing standardized resources for model\ndevelopment and evaluation. However, potential inaccuracies in sensor\ncalibration and vehicle poses within these datasets can lead to erroneous\nevaluations of downstream tasks, adversely impacting the reliability and\nperformance of the autonomous systems. To address this challenge, we propose a\nrobust optimization method based on Neural Radiance Fields (NeRF) to refine\nsensor poses and calibration parameters, enhancing the integrity of dataset\nbenchmarks. To validate improvement in accuracy of our optimized poses without\nground truth, we present a thorough evaluation process, relying on reprojection\nmetrics, Novel View Synthesis rendering quality, and geometric alignment. We\ndemonstrate that our method achieves significant improvements in sensor pose\naccuracy. By optimizing these critical parameters, our approach not only\nimproves the utility of existing datasets but also paves the way for more\nreliable autonomous driving models. To foster continued progress in this field,\nwe make the optimized sensor poses publicly available, providing a valuable\nresource for the research community.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15376", "pdf": "https://arxiv.org/pdf/2504.15376", "abs": "https://arxiv.org/abs/2504.15376", "authors": ["Zhiqiu Lin", "Siyuan Cen", "Daniel Jiang", "Jay Karhade", "Hewei Wang", "Chancharik Mitra", "Tiffany Ling", "Yuhan Huang", "Sifan Liu", "Mingyu Chen", "Rushikesh Zawar", "Xue Bai", "Yilun Du", "Chuang Gan", "Deva Ramanan"], "title": "Towards Understanding Camera Motions in Any Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Project site: https://linzhiqiu.github.io/papers/camerabench/", "summary": "We introduce CameraBench, a large-scale dataset and benchmark designed to\nassess and improve camera motion understanding. CameraBench consists of ~3,000\ndiverse internet videos, annotated by experts through a rigorous multi-stage\nquality control process. One of our contributions is a taxonomy of camera\nmotion primitives, designed in collaboration with cinematographers. We find,\nfor example, that some motions like \"follow\" (or tracking) require\nunderstanding scene content like moving subjects. We conduct a large-scale\nhuman study to quantify human annotation performance, revealing that domain\nexpertise and tutorial-based training can significantly enhance accuracy. For\nexample, a novice may confuse zoom-in (a change of intrinsics) with translating\nforward (a change of extrinsics), but can be trained to differentiate the two.\nUsing CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language\nModels (VLMs), finding that SfM models struggle to capture semantic primitives\nthat depend on scene content, while VLMs struggle to capture geometric\nprimitives that require precise estimation of trajectories. We then fine-tune a\ngenerative VLM on CameraBench to achieve the best of both worlds and showcase\nits applications, including motion-augmented captioning, video question\nanswering, and video-text retrieval. We hope our taxonomy, benchmark, and\ntutorials will drive future efforts towards the ultimate goal of understanding\ncamera motions in any video.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15780", "pdf": "https://arxiv.org/pdf/2504.15780", "abs": "https://arxiv.org/abs/2504.15780", "authors": ["Daocheng Fu", "Zijun Chen", "Renqiu Xia", "Qi Liu", "Yuan Feng", "Hongbin Zhou", "Renrui Zhang", "Shiyang Feng", "Peng Gao", "Junchi Yan", "Botian Shi", "Bo Zhang", "Yu Qiao"], "title": "TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical geometric problem solving (GPS) often requires effective\nintegration of multimodal information and verifiable logical coherence. Despite\nthe fast development of large language models in general problem solving, it\nremains unresolved regarding with both methodology and benchmarks, especially\ngiven the fact that exiting synthetic GPS benchmarks are often not\nself-verified and contain noise and self-contradicted information due to the\nillusion of LLMs. In this paper, we propose a scalable data engine called\nTrustGeoGen for problem generation, with formal verification to provide a\nprincipled benchmark, which we believe lays the foundation for the further\ndevelopment of methods for GPS. The engine synthesizes geometric data through\nfour key innovations: 1) multimodal-aligned generation of diagrams, textual\ndescriptions, and stepwise solutions; 2) formal verification ensuring\nrule-compliant reasoning paths; 3) a bootstrapping mechanism enabling\ncomplexity escalation via recursive state generation and 4) our devised\nGeoExplore series algorithms simultaneously produce multi-solution variants and\nself-reflective backtracking traces. By formal logical verification,\nTrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity,\nalong with GeoTrust-test testset. Experiments reveal the state-of-the-art\nmodels achieve only 49.17\\% accuracy on GeoTrust-test, demonstrating its\nevaluation stringency. Crucially, models trained on GeoTrust achieve OOD\ngeneralization on GeoQA, significantly reducing logical inconsistencies\nrelative to pseudo-label annotated by OpenAI-o1. Our code is available at\nhttps://github.com/Alpha-Innovator/TrustGeoGen", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16081", "pdf": "https://arxiv.org/pdf/2504.16081", "abs": "https://arxiv.org/abs/2504.16081", "authors": ["Yimu Wang", "Xuye Liu", "Wei Pang", "Li Ma", "Shuai Yuan", "Paul Debevec", "Ning Yu"], "title": "Survey of Video Diffusion Models: Foundations, Implementations, and Applications", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in diffusion models have revolutionized video generation,\noffering superior temporal consistency and visual quality compared to\ntraditional generative adversarial networks-based approaches. While this\nemerging field shows tremendous promise in applications, it faces significant\nchallenges in motion consistency, computational efficiency, and ethical\nconsiderations. This survey provides a comprehensive review of diffusion-based\nvideo generation, examining its evolution, technical foundations, and practical\napplications. We present a systematic taxonomy of current methodologies,\nanalyze architectural innovations and optimization strategies, and investigate\napplications across low-level vision tasks such as denoising and\nsuper-resolution. Additionally, we explore the synergies between diffusionbased\nvideo generation and related domains, including video representation learning,\nquestion answering, and retrieval. Compared to the existing surveys (Lei et\nal., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which\nfocus on specific aspects of video generation, such as human video synthesis\n(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our\nwork provides a broader, more updated, and more fine-grained perspective on\ndiffusion-based approaches with a special section for evaluation metrics,\nindustry solutions, and training engineering techniques in video generation.\nThis survey serves as a foundational resource for researchers and practitioners\nworking at the intersection of diffusion models and video generation, providing\ninsights into both the theoretical frameworks and practical implementations\nthat drive this rapidly evolving field. A structured list of related works\ninvolved in this survey is also available on\nhttps://github.com/Eyeline-Research/Survey-Video-Diffusion.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15521", "pdf": "https://arxiv.org/pdf/2504.15521", "abs": "https://arxiv.org/abs/2504.15521", "authors": ["Minghao Wu", "Weixuan Wang", "Sinuo Liu", "Huifeng Yin", "Xintong Wang", "Yu Zhao", "Chenyang Lyu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "categories": ["cs.CL"], "comment": "work in progress; 22 pages, 8 figures, 3 tables;", "summary": "As large language models (LLMs) continue to advance in linguistic\ncapabilities, robust multilingual evaluation has become essential for promoting\nequitable technological progress. This position paper examines over 2,000\nmultilingual (non-English) benchmarks from 148 countries, published between\n2021 and 2024, to evaluate past, present, and future practices in multilingual\nbenchmarking. Our findings reveal that, despite significant investments\namounting to tens of millions of dollars, English remains significantly\noverrepresented in these benchmarks. Additionally, most benchmarks rely on\noriginal language content rather than translations, with the majority sourced\nfrom high-resource countries such as China, India, Germany, the UK, and the\nUSA. Furthermore, a comparison of benchmark performance with human judgments\nhighlights notable disparities. STEM-related tasks exhibit strong correlations\nwith human evaluations (0.70 to 0.85), while traditional NLP tasks like\nquestion answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30).\nMoreover, translating English benchmarks into other languages proves\ninsufficient, as localized benchmarks demonstrate significantly higher\nalignment with local human judgments (0.68) than their translated counterparts\n(0.47). This underscores the importance of creating culturally and\nlinguistically tailored benchmarks rather than relying solely on translations.\nThrough this comprehensive analysis, we highlight six key limitations in\ncurrent multilingual evaluation practices, propose the guiding principles\naccordingly for effective multilingual benchmarking, and outline five critical\nresearch directions to drive progress in the field. Finally, we call for a\nglobal collaborative effort to develop human-aligned benchmarks that prioritize\nreal-world applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15681", "pdf": "https://arxiv.org/pdf/2504.15681", "abs": "https://arxiv.org/abs/2504.15681", "authors": ["Vidi Team", "Celong Liu", "Chia-Wen Kuo", "Dawei Du", "Fan Chen", "Guang Chen", "Jiamin Yuan", "Lingxi Zhang", "Lu Guo", "Lusha Li", "Longyin Wen", "Qingyu Chen", "Rachel Deng", "Sijie Zhu", "Stuart Siew", "Tong Jin", "Wei Lu", "Wen Zhong", "Xiaohui Shen", "Xin Gu", "Xing Mei", "Xueqiong Qu"], "title": "Vidi: Large Multimodal Models for Video Understanding and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Humans naturally share information with those they are connected to, and\nvideo has become one of the dominant mediums for communication and expression\non the Internet. To support the creation of high-quality large-scale video\ncontent, a modern pipeline requires a comprehensive understanding of both the\nraw input materials (e.g., the unedited footage captured by cameras) and the\nediting components (e.g., visual effects). In video editing scenarios, models\nmust process multiple modalities (e.g., vision, audio, text) with strong\nbackground knowledge and handle flexible input lengths (e.g., hour-long raw\nvideos), which poses significant challenges for traditional models. In this\nreport, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a\nwide range of video understand editing scenarios. The first release focuses on\ntemporal retrieval, i.e., identifying the time ranges within the input videos\ncorresponding to a given text query, which plays a critical role in intelligent\nediting. The model is capable of processing hour-long videos with strong\ntemporal understanding capability, e.g., retrieve time ranges for certain\nqueries. To support a comprehensive evaluation in real-world scenarios, we also\npresent the VUE-TR benchmark, which introduces five key advancements. 1) Video\nduration: significantly longer than existing temporal retrival datasets, 2)\nAudio support: includes audio-based queries, 3) Query format: diverse query\nlengths/formats, 4) Annotation quality: ground-truth time ranges are manually\nannotated. 5) Evaluation metric: a refined IoU metric to support evaluation\nover multiple time ranges. Remarkably, Vidi significantly outperforms leading\nproprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task,\nindicating its superiority in video editing scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15848", "pdf": "https://arxiv.org/pdf/2504.15848", "abs": "https://arxiv.org/abs/2504.15848", "authors": ["Luwei Xiao", "Rui Mao", "Shuai Zhao", "Qika Lin", "Yanhao Jia", "Liang He", "Erik Cambria"], "title": "Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": "Accepted by TAFFC 2025", "summary": "Multimodal aspect-based sentiment classification (MASC) is an emerging task\ndue to an increase in user-generated multimodal content on social platforms,\naimed at predicting sentiment polarity toward specific aspect targets (i.e.,\nentities or attributes explicitly mentioned in text-image pairs). Despite\nextensive efforts and significant achievements in existing MASC, substantial\ngaps remain in understanding fine-grained visual content and the cognitive\nrationales derived from semantic content and impressions (cognitive\ninterpretations of emotions evoked by image content). In this study, we present\nChimera: a cognitive and aesthetic sentiment causality understanding framework\nto derive fine-grained holistic features of aspects and infer the fundamental\ndrivers of sentiment expression from both semantic perspectives and\naffective-cognitive resonance (the synergistic effect between emotional\nresponses and cognitive interpretations). Specifically, this framework first\nincorporates visual patch features for patch-word alignment. Meanwhile, it\nextracts coarse-grained visual features (e.g., overall image representation)\nand fine-grained visual regions (e.g., aspect-related regions) and translates\nthem into corresponding textual descriptions (e.g., facial, aesthetic).\nFinally, we leverage the sentimental causes and impressions generated by a\nlarge language model (LLM) to enhance the model's awareness of sentimental cues\nevoked by semantic content and affective-cognitive resonance. Experimental\nresults on standard MASC datasets demonstrate the effectiveness of the proposed\nmodel, which also exhibits greater flexibility to MASC compared to LLMs such as\nGPT-4o. We have publicly released the complete implementation and dataset at\nhttps://github.com/Xillv/Chimera", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "aspect-based", "fine-grained"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15983", "pdf": "https://arxiv.org/pdf/2504.15983", "abs": "https://arxiv.org/abs/2504.15983", "authors": ["Shang Wang"], "title": "W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025", "summary": "The demand for efficient natural language processing (NLP) systems has led to\nthe development of lightweight language models. Previous work in this area has\nprimarily focused on manual design or training-based neural architecture search\n(NAS) methods. Recently, zero-shot NAS methods have been proposed for\nevaluating language models without the need for training. However, prevailing\napproaches to zero-shot NAS often face challenges such as biased evaluation\nmetrics and computational inefficiencies. In this paper, we introduce\nweight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored\nfor lightweight language models. Our approach utilizes two evaluation proxies:\nthe parameter count and the number of principal components with cumulative\ncontribution exceeding $\\eta$ in the feed-forward neural (FFN) layer.\nAdditionally, by eliminating the need for gradient computations, we optimize\nthe evaluation time, thus enhancing the efficiency of designing and evaluating\nlightweight language models. We conduct a comparative analysis on the GLUE and\nSQuAD datasets to evaluate our approach. The results demonstrate that our\nmethod significantly reduces training time compared to one-shot NAS methods and\nachieves higher scores in the testing phase compared to previous\nstate-of-the-art training-based methods. Furthermore, we perform ranking\nevaluations on a dataset sampled from the FlexiBERT search space. Our approach\nexhibits superior ranking correlation and further reduces solving time compared\nto other zero-shot NAS methods that require gradient computation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16060", "pdf": "https://arxiv.org/pdf/2504.16060", "abs": "https://arxiv.org/abs/2504.16060", "authors": ["Ziqiao Ma", "Jing Ding", "Xuejun Zhang", "Dezhi Luo", "Jiahe Ding", "Sihan Xu", "Yuchen Huang", "Run Peng", "Joyce Chai"], "title": "Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation", "categories": ["cs.CL"], "comment": "Homepage: https://vlm-reg.github.io/", "summary": "Referring Expression Generation (REG) is a core task for evaluating the\npragmatic competence of vision-language systems, requiring not only accurate\nsemantic grounding but also adherence to principles of cooperative\ncommunication (Grice, 1975). However, current evaluations of vision-language\nmodels (VLMs) often overlook the pragmatic dimension, reducing REG to a\nregion-based captioning task and neglecting Gricean maxims. In this work, we\nrevisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of\n1.5k images annotated with both written and spoken referring expressions.\nThrough a systematic evaluation of state-of-the-art VLMs, we identify three key\nfailures of pragmatic competence: (1) failure to uniquely identify the\nreferent, (2) inclusion of excessive or irrelevant information, and (3)\nmisalignment with human pragmatic preference, such as the underuse of minimal\nspatial cues. We also show that standard automatic evaluations fail to capture\nthese pragmatic violations, reinforcing superficial cues rather than genuine\nreferential success. Our findings call for a renewed focus on pragmatically\ninformed models and evaluation frameworks that align with real human\ncommunication.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "dimension"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16074", "pdf": "https://arxiv.org/pdf/2504.16074", "abs": "https://arxiv.org/abs/2504.16074", "authors": ["Shi Qiu", "Shaoyang Guo", "Zhuo-Yang Song", "Yunbo Sun", "Zeyu Cai", "Jiashen Wei", "Tianyu Luo", "Yixuan Yin", "Haoxu Zhang", "Yi Hu", "Chenyang Wang", "Chencheng Tang", "Haoling Chang", "Qi Liu", "Ziheng Zhou", "Tianyu Zhang", "Jingtian Zhang", "Zhangyi Liu", "Minghao Li", "Yuku Zhang", "Boxuan Jing", "Xianqi Yin", "Yutong Ren", "Zizhuo Fu", "Weike Wang", "Xudong Tian", "Anqi Lv", "Laifu Man", "Jianxiang Li", "Feiyu Tao", "Qihua Sun", "Zhou Liang", "Yushu Mu", "Zhongxuan Li", "Jing-Jun Zhang", "Shutao Zhang", "Xiaotian Li", "Xingqi Xia", "Jiawei Lin", "Zheyu Shen", "Jiahang Chen", "Qiuhao Xiong", "Binran Wang", "Fengyuan Wang", "Ziyang Ni", "Bohan Zhang", "Fan Cui", "Changkun Shao", "Qing-Hong Cao", "Ming-xing Luo", "Muhan Zhang", "Hua Xing Zhu"], "title": "PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "21 pages ,8 figures, 4 tables", "summary": "We introduce PHYBench, a novel, high-quality benchmark designed for\nevaluating reasoning capabilities of large language models (LLMs) in physical\ncontexts. PHYBench consists of 500 meticulously curated physics problems based\non real-world physical scenarios, designed to assess the ability of models to\nunderstand and reason about realistic physical processes. Covering mechanics,\nelectromagnetism, thermodynamics, optics, modern physics, and advanced physics,\nthe benchmark spans difficulty levels from high school exercises to\nundergraduate problems and Physics Olympiad challenges. Additionally, we\npropose the Expression Edit Distance (EED) Score, a novel evaluation metric\nbased on the edit distance between mathematical expressions, which effectively\ncaptures differences in model reasoning processes and results beyond\ntraditional binary scoring methods. We evaluate various LLMs on PHYBench and\ncompare their performance with human experts. Our results reveal that even\nstate-of-the-art reasoning models significantly lag behind human experts,\nhighlighting their limitations and the need for improvement in complex physical\nreasoning scenarios. Our benchmark results and dataset are publicly available\nat https://phybench-official.github.io/phybench-demo/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15330", "pdf": "https://arxiv.org/pdf/2504.15330", "abs": "https://arxiv.org/abs/2504.15330", "authors": ["Mohit Gupta", "Akiko Aizawa", "Rajiv Ratn Shah"], "title": "Med-CoDE: Medical Critique based Disagreement Evaluation Framework", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "8 pages, 4 figures, NAACL SRW 2025", "summary": "The emergence of large language models (LLMs) has significantly influenced\nnumerous fields, including healthcare, by enhancing the capabilities of\nautomated systems to process and generate human-like text. However, despite\ntheir advancements, the reliability and accuracy of LLMs in medical contexts\nremain critical concerns. Current evaluation methods often lack robustness and\nfail to provide a comprehensive assessment of LLM performance, leading to\npotential risks in clinical settings. In this work, we propose Med-CoDE, a\nspecifically designed evaluation framework for medical LLMs to address these\nchallenges. The framework leverages a critique-based approach to quantitatively\nmeasure the degree of disagreement between model-generated responses and\nestablished medical ground truths. This framework captures both accuracy and\nreliability in medical settings. The proposed evaluation framework aims to fill\nthe existing gap in LLM assessment by offering a systematic method to evaluate\nthe quality and trustworthiness of medical LLMs. Through extensive experiments\nand case studies, we illustrate the practicality of our framework in providing\na comprehensive and reliable evaluation of medical LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15883", "pdf": "https://arxiv.org/pdf/2504.15883", "abs": "https://arxiv.org/abs/2504.15883", "authors": ["Farida Mohsen", "Samir Belhaouari", "Zubair Shah"], "title": "Integrating Non-Linear Radon Transformation for Diabetic Retinopathy Grading", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diabetic retinopathy is a serious ocular complication that poses a\nsignificant threat to patients' vision and overall health. Early detection and\naccurate grading are essential to prevent vision loss. Current automatic\ngrading methods rely heavily on deep learning applied to retinal fundus images,\nbut the complex, irregular patterns of lesions in these images, which vary in\nshape and distribution, make it difficult to capture subtle changes. This study\nintroduces RadFuse, a multi-representation deep learning framework that\nintegrates non-linear RadEx-transformed sinogram images with traditional fundus\nimages to enhance diabetic retinopathy detection and grading. Our RadEx\ntransformation, an optimized non-linear extension of the Radon transform,\ngenerates sinogram representations to capture complex retinal lesion patterns.\nBy leveraging both spatial and transformed domain information, RadFuse enriches\nthe feature set available to deep learning models, improving the\ndifferentiation of severity levels. We conducted extensive experiments on two\nbenchmark datasets, APTOS-2019 and DDR, using three convolutional neural\nnetworks (CNNs): ResNeXt-50, MobileNetV2, and VGG19. RadFuse showed significant\nimprovements over fundus-image-only models across all three CNN architectures\nand outperformed state-of-the-art methods on both datasets. For severity\ngrading across five stages, RadFuse achieved a quadratic weighted kappa of\n93.24%, an accuracy of 87.07%, and an F1-score of 87.17%. In binary\nclassification between healthy and diabetic retinopathy cases, the method\nreached an accuracy of 99.09%, precision of 98.58%, and recall of 99.6%,\nsurpassing previously established models. These results demonstrate RadFuse's\ncapacity to capture complex non-linear features, advancing diabetic retinopathy\nclassification and promoting the integration of advanced mathematical\ntransforms in medical image analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "kappa", "accuracy"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15415", "pdf": "https://arxiv.org/pdf/2504.15415", "abs": "https://arxiv.org/abs/2504.15415", "authors": ["David Ma", "Yuanxing Zhang", "Jincheng Ren", "Jarvis Guo", "Yifan Yao", "Zhenlin Wei", "Zhenzhu Yang", "Zhongyuan Peng", "Boyu Feng", "Jun Ma", "Xiao Gu", "Zhoufutu Wen", "King Zhu", "Yancheng He", "Meng Cao", "Shiwen Ni", "Jiaheng Liu", "Wenhao Huang", "Ge Zhang", "Xiaojie Jin"], "title": "IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Existing evaluation frameworks for Multimodal Large Language Models (MLLMs)\nprimarily focus on image reasoning or general video understanding tasks,\nlargely overlooking the significant role of image context in video\ncomprehension. To bridge this gap, we propose IV-Bench, the first comprehensive\nbenchmark for evaluating Image-Grounded Video Perception and Reasoning.\nIV-Bench consists of 967 videos paired with 2,585 meticulously annotated\nimage-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5\nrepresentative categories. Extensive evaluations of state-of-the-art\nopen-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o,\nGemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models\nsubstantially underperform in image-grounded video Perception and Reasoning,\nmerely achieving at most 28.9% accuracy. Further analysis reveals key factors\ninfluencing model performance on IV-Bench, including inference pattern, frame\nnumber, and resolution. Additionally, through a simple data synthesis approach,\nwe demonstratethe challenges of IV- Bench extend beyond merely aligning the\ndata format in the training proecss. These findings collectively provide\nvaluable insights for future research. Our codes and data are released in\nhttps://github.com/multimodal-art-projection/IV-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15629", "pdf": "https://arxiv.org/pdf/2504.15629", "abs": "https://arxiv.org/abs/2504.15629", "authors": ["Harsh Maheshwari", "Srikanth Tenneti", "Alwarappan Nakkiran"], "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) has emerged as a powerful application of\nLarge Language Models (LLMs), revolutionizing information search and\nconsumption. RAG systems combine traditional search capabilities with LLMs to\ngenerate comprehensive answers to user queries, ideally with accurate\ncitations. However, in our experience of developing a RAG product, LLMs often\nstruggle with source attribution, aligning with other industry studies\nreporting citation accuracy rates of only about 74% for popular generative\nsearch engines. To address this, we present efficient post-processing\nalgorithms to improve citation accuracy in LLM-generated responses, with\nminimal impact on latency and cost. Our approaches cross-check generated\ncitations against retrieved articles using methods including keyword + semantic\nmatching, fine tuned model with BERTScore, and a lightweight LLM-based\ntechnique. Our experimental results demonstrate a relative improvement of\n15.46% in the overall accuracy metrics of our RAG system. This significant\nenhancement potentially enables a shift from our current larger language model\nto a relatively smaller model that is approximately 12x more cost-effective and\n3x faster in inference time, while maintaining comparable performance. This\nresearch contributes to enhancing the reliability and trustworthiness of\nAI-generated content in information retrieval and summarization tasks which is\ncritical to gain customer trust especially in commercial products.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy", "summarization"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16030", "pdf": "https://arxiv.org/pdf/2504.16030", "abs": "https://arxiv.org/abs/2504.16030", "authors": ["Joya Chen", "Ziyun Zeng", "Yiqi Lin", "Wei Li", "Zejun Ma", "Mike Zheng Shou"], "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale", "categories": ["cs.CV"], "comment": "CVPR 2025. If any references are missing, please contact\n  joyachen@u.nus.edu", "summary": "Recent video large language models (Video LLMs) often depend on costly human\nannotations or proprietary model APIs (e.g., GPT-4o) to produce training data,\nwhich limits their training at scale. In this paper, we explore large-scale\ntraining for Video LLM with cheap automatic speech recognition (ASR)\ntranscripts. Specifically, we propose a novel streaming training approach that\ndensely interleaves the ASR words and video frames according to their\ntimestamps. Compared to previous studies in vision-language representation with\nASR, our method naturally fits the streaming characteristics of ASR, thus\nenabling the model to learn temporally-aligned, fine-grained vision-language\nmodeling. To support the training algorithm, we introduce a data production\npipeline to process YouTube videos and their closed captions (CC, same as ASR),\nresulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset\nfor high-quality supervised fine-tuning (SFT). Remarkably, even without SFT,\nthe ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general\nvideo QA performance and exhibits a new capability in real-time video\ncommentary. To evaluate this, we carefully design a new LiveSports-3K\nbenchmark, using LLM-as-a-judge to measure the free-form commentary.\nExperiments show our final LiveCC-7B-Instruct model can surpass advanced 72B\nmodels (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even\nworking in a real-time mode. Meanwhile, it achieves state-of-the-art results at\nthe 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench,\ndemonstrating the broad generalizability of our approach. All resources of this\npaper have been released at https://showlab.github.io/livecc.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-TaixÃ©", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15349", "pdf": "https://arxiv.org/pdf/2504.15349", "abs": "https://arxiv.org/abs/2504.15349", "authors": ["William Bruns"], "title": "Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)", "categories": ["cs.CL"], "comment": "8 pages main text with 3 figures and 1 table; limitations page and\n  references separate; 4 more figures, 1 image, and 1 more table in the\n  appendices supplement the work. 29 pages of appendix content", "summary": "Humans understand new combinations of words encountered if they are\ncombinations of words recognized from different contexts, an ability called\nCompositional Generalization. The COGS benchmark (Kim and Linzen, 2020)\narXiv:2010.05465 reports 0% accuracy for Transformer models on some structural\ngeneralizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted\nAccess Sequence Processing (RASP), a Transformer-equivalent programming\nlanguage, to prove by construction that a Transformer encoder-decoder can\nperform the semantically equivalent ReCOGS_pos (Wu et al., 2024)\narXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP\nmodel attains 100% semantic exact match on the ReCOGS test set and 100% SEM on\nall generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore,\nour RASP model shows the ReCOGS_pos task does not require a hierarchical or\ntree-structured solution: we use word-level tokens with an \"embedding\" layer\nthat tags with possible parts of speech, applying just once per encoder pass 19\nattention-head compatible flat pattern-matching rules, shown using grammar\ncoverage (Zeller et al., 2023) to be learnable from the training data, plus\ngeneral prepositional phrase (pp) handling and sentential complement (cp)\nhandling logic, and output the next logical form (LF) token (repeating until\nthe LF is complete). The model does not apply recursive, tree-structured rules\nlike 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact\nmatch on pp recursion, cp recursion using the decoder loop.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15371", "pdf": "https://arxiv.org/pdf/2504.15371", "abs": "https://arxiv.org/abs/2504.15371", "authors": ["Wei Fang", "Priyadarshini Panda"], "title": "Event2Vec: Processing neuromorphic events directly by representations in vector space", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "The neuromorphic event cameras have overwhelming advantages in temporal\nresolution, power efficiency, and dynamic range compared to traditional\ncameras. However, the event cameras output asynchronous, sparse, and irregular\nevents, which are not compatible with mainstream computer vision and deep\nlearning methods. Various methods have been proposed to solve this issue but at\nthe cost of long preprocessing procedures, losing temporal resolutions, or\nbeing incompatible with massively parallel computation. Inspired by the great\nsuccess of the word to vector, we summarize the similarities between words and\nevents, then propose the first event to vector (event2vec) representation. We\nvalidate event2vec on classifying the ASL-DVS dataset, showing impressive\nparameter efficiency, accuracy, and speed than previous graph/image/voxel-based\nrepresentations. Beyond task performance, the most attractive advantage of\nevent2vec is that it aligns events to the domain of natural language\nprocessing, showing the promising prospect of integrating events into large\nlanguage and multimodal models. Our codes, models, and training logs are\navailable at https://github.com/fangwei123456/event2vec.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15432", "pdf": "https://arxiv.org/pdf/2504.15432", "abs": "https://arxiv.org/abs/2504.15432", "authors": ["Yucheng Lu", "Kazimier Smith"], "title": "Feeding LLM Annotations to BERT Classifiers at Your Own Risk", "categories": ["cs.CL"], "comment": null, "summary": "Using LLM-generated labels to fine-tune smaller encoder-only models for text\nclassification has gained popularity in various settings. While this approach\nmay be justified in simple and low-stakes applications, we conduct empirical\nanalysis to demonstrate how the perennial curse of training on synthetic data\nmanifests itself in this specific setup. Compared to models trained on gold\nlabels, we observe not only the expected performance degradation in accuracy\nand F1 score, but also increased instability across training runs and premature\nperformance plateaus. These findings cast doubts on the reliability of such\napproaches in real-world applications. We contextualize the observed phenomena\nthrough the lens of error propagation and offer several practical mitigation\nstrategies, including entropy-based filtering and ensemble techniques. Although\nthese heuristics offer partial relief, they do not fully resolve the inherent\nrisks of propagating non-random errors from LLM annotations to smaller\nclassifiers, underscoring the need for caution when applying this workflow in\nhigh-stakes text classification tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15384", "pdf": "https://arxiv.org/pdf/2504.15384", "abs": "https://arxiv.org/abs/2504.15384", "authors": ["Chen Zhao", "Anjum Shaik", "Joyce H. Keyak", "Nancy E. Lane", "Jeffrey D. Deng", "Kuan-Jui Su", "Qiuying Sha", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images", "categories": ["cs.CV"], "comment": "23 pages, 4 figures", "summary": "Hip fractures represent a major health concern, particularly among the\nelderly, often leading decreased mobility and increased mortality. Early and\naccurate detection of at risk individuals is crucial for effective\nintervention. In this study, we propose Iterative Cross Graph Matching for Hip\nFracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip\nfractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX\ninvolves iteratively comparing a test (subject) graph with multiple template\ngraphs representing the characteristics of hip fracture subjects to assess the\nsimilarity and accurately to predict hip fracture risk. These graphs are\nobtained as follows. The DXA images are separated into multiple regions of\ninterest (RoIs), such as the femoral head, shaft, and lesser trochanter.\nRadiomic features are then calculated for each RoI, with the central\ncoordinates used as nodes in a graph. The connectivity between nodes is\nestablished according to the Euclidean distance between these coordinates. This\nprocess transforms each DXA image into a graph, where each node represents a\nRoI, and edges derived by the centroids of RoIs capture the spatial\nrelationships between them. If the test graph closely matches a set of template\ngraphs representing subjects with incident hip fractures, it is classified as\nindicating high hip fracture risk. We evaluated our method using 547 subjects\nfrom the UK Biobank dataset, and experimental results show that ICGM-FRAX\nachieved a sensitivity of 0.9869, demonstrating high accuracy in predicting hip\nfractures.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15524", "pdf": "https://arxiv.org/pdf/2504.15524", "abs": "https://arxiv.org/abs/2504.15524", "authors": ["Qiyao Wang", "Guhong Chen", "Hongbo Wang", "Huaren Liu", "Minghui Zhu", "Zhifei Qin", "Linwei Li", "Yilin Yue", "Shiqiang Wang", "Jiayan Li", "Yihang Wu", "Ziqiang Liu", "Longze Chen", "Run Luo", "Liyang Fan", "Jiaming Li", "Lei Zhang", "Kan Xu", "Hongfei Lin", "Hamid Alinejad-Rokny", "Shiwen Ni", "Yuan Lin", "Min Yang"], "title": "IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property", "categories": ["cs.CL", "cs.AI"], "comment": "89 pages, 75 figures, 55 tables", "summary": "Intellectual Property (IP) is a unique domain that integrates technical and\nlegal knowledge, making it inherently complex and knowledge-intensive. As large\nlanguage models (LLMs) continue to advance, they show great potential for\nprocessing IP tasks, enabling more efficient analysis, understanding, and\ngeneration of IP-related content. However, existing datasets and benchmarks\neither focus narrowly on patents or cover limited aspects of the IP field,\nlacking alignment with real-world scenarios. To bridge this gap, we introduce\nthe first comprehensive IP task taxonomy and a large, diverse bilingual\nbenchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is\ndesigned to evaluate LLMs in real-world intellectual property applications,\nencompassing both understanding and generation. We benchmark 16 LLMs, ranging\nfrom general-purpose to domain-specific models, and find that even the\nbest-performing model achieves only 75.8% accuracy, revealing substantial room\nfor improvement. Notably, open-source IP and law-oriented models lag behind\nclosed-source general-purpose models. We publicly release all data and code of\nIPBench and will continue to update it with additional IP-related tasks to\nbetter reflect real-world challenges in the intellectual property domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15527", "pdf": "https://arxiv.org/pdf/2504.15527", "abs": "https://arxiv.org/abs/2504.15527", "authors": ["Sophia Maria"], "title": "Compass-V2 Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Predominant LLMs focus on high-resource languages while leaving low-resource\nlanguages, particularly those in Southeast Asia (SEA), underrepresented. In\naddition, those models are general-purpose and pay limited attention to the\ne-commerce domain. To overcome these limitations, we introduce Compass-v2, a\nlightweight Mixture-of-Experts (MoE) model specifically designed for Southeast\nAsian languages and e-commerce applications. To balance model performance and\ninference cost, the model is designed with 30B total parameters and 5B active\nparameters, incorporating both fine-grained and shared expert modules. To\nenhance multilingual performance, we curated and constructed a high-quality,\nindustry-leading SEA dataset, to the best of our knowledge. To boost\nperformance in the e-commerce domain, we built a dataset comprising hundreds of\nbillions of tokens, sourced through external data mining and internal platform\ncollection. Besides, we pioneered a hybrid reasoning model that supports both\nfast thinking and deep thinking within a unified framework to enhance the\nreasoning capabilities, diverging from the conventional industry practice of\ndeploying two separate models. Through extensive experimental evaluations, our\nmodel demonstrates state-of-the-art SEA multilingual and e-commerce performance\namong sub-30B models, while maintaining significantly lower inference cost.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15604", "pdf": "https://arxiv.org/pdf/2504.15604", "abs": "https://arxiv.org/abs/2504.15604", "authors": ["Pavan Yadav", "Nikhil Khandalkar", "Krishna Shinde", "Lokesh B. Ramegowda", "Rajarshi Das"], "title": "Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models", "categories": ["cs.CL", "cs.AI"], "comment": "75 pages, 60 figures", "summary": "Language models have made significant progress in generating coherent text\nand predicting next tokens based on input prompts. This study compares the\nnext-token prediction performance of two well-known models: OpenAI's GPT-2 and\nMeta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their\ncapabilities, we built a dataset from 10 short stories sourced from the Explore\nToM Dataset. We enhanced these stories by programmatically inserting additional\nsentences (infills) using GPT-4, creating variations that introduce different\nlevels of contextual complexity. This setup enables analysis of how increasing\ncontext affects model performance. We tested both models under four temperature\nsettings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next\ntoken across three reasoning levels. Zero-order reasoning involves tracking the\nstate, either current (ground truth) or past (memory). First-order reasoning\nconcerns understanding another's mental state (e.g., \"Does Anne know the apple\nis salted?\"). Second-order reasoning adds recursion (e.g., \"Does Anne think\nthat Charles knows the apple is salted?\").\n  Our results show that adding more infill sentences slightly reduces\nprediction accuracy, as added context increases complexity and ambiguity.\nLlama-2 consistently outperforms GPT-2 in prediction accuracy, especially at\nlower temperatures, demonstrating greater confidence in selecting the most\nprobable token. As reasoning complexity rises, model responses diverge more.\nNotably, GPT-2 and Llama-2 display greater variability in predictions during\nfirst- and second-order reasoning tasks. These findings illustrate how model\narchitecture, temperature, and contextual complexity influence next-token\nprediction, contributing to a better understanding of the strengths and\nlimitations of current language models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15609", "pdf": "https://arxiv.org/pdf/2504.15609", "abs": "https://arxiv.org/abs/2504.15609", "authors": ["Yunfeng Li", "Bo Wang", "Jiahao Wan", "Xueyi Wu", "Ye Li"], "title": "SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Underwater observation systems typically integrate optical cameras and\nimaging sonar systems. When underwater visibility is insufficient, only sonar\nsystems can provide stable data, which necessitates exploration of the\nunderwater acoustic object tracking (UAOT) task. Previous studies have explored\ntraditional methods and Siamese networks for UAOT. However, the absence of a\nunified evaluation benchmark has significantly constrained the value of these\nmethods. To alleviate this limitation, we propose the first large-scale UAOT\nbenchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and\n205K high-quality annotations. Experimental results demonstrate that SonarT165\nreveals limitations in current state-of-the-art SOT trackers. To address these\nlimitations, we propose STFTrack, an efficient framework for acoustic object\ntracking. It includes two novel modules, a multi-view template fusion module\n(MTFM) and an optimal trajectory correction module (OTCM). The MTFM module\nintegrates multi-view feature of both the original image and the binary image\nof the dynamic template, and introduces a cross-attention-like layer to fuse\nthe spatio-temporal target representations. The OTCM module introduces the\nacoustic-response-equivalent pixel property and proposes normalized pixel\nbrightness response scores, thereby suppressing suboptimal matches caused by\ninaccurate Kalman filter prediction boxes. To further improve the model\nfeature, STFTrack introduces a acoustic image enhancement method and a\nFrequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive\nexperiments show the proposed STFTrack achieves state-of-the-art performance on\nthe proposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/SonarT165.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15777", "pdf": "https://arxiv.org/pdf/2504.15777", "abs": "https://arxiv.org/abs/2504.15777", "authors": ["Shangshang Wang", "Julian Asilis", "Ãmer Faruk AkgÃ¼l", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "title": "Tina: Tiny Reasoning Models via LoRA", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15784", "pdf": "https://arxiv.org/pdf/2504.15784", "abs": "https://arxiv.org/abs/2504.15784", "authors": ["Ruizhe Li", "Chiwei Zhu", "Benfeng Xu", "Xiaorui Wang", "Zhendong Mao"], "title": "Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Creative writing is a key capability of Large Language Models (LLMs), with\npotential applications in literature, storytelling, and various creative\ndomains. However, evaluating the creativity of machine-generated texts remains\na significant challenge, as existing methods either rely on costly manual\nannotations or fail to align closely with human assessments. In this paper, we\npropose an effective automated evaluation method based on the Torrance Test of\nCreative Writing (TTCW), which evaluates creativity as product. Our method\nemploys a reference-based Likert-style approach, scoring generated creative\ntexts relative to high-quality reference texts across various tests.\nExperimental results demonstrate that our method significantly improves the\nalignment between LLM evaluations and human assessments, achieving a pairwise\naccuracy of 0.75 (+15\\%).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815", "abs": "https://arxiv.org/abs/2504.15815", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Barbara Plank"], "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods, either automated metrics or human\nevaluation, have limitations, such as providing limited insights or being\nlabor-intensive. We propose Spotlight, a new approach that combines both\nautomation and human analysis. Based on data mining techniques, we\nautomatically distinguish between random (decoding) variations and systematic\ndifferences in language model outputs. This process provides token patterns\nthat describe the systematic differences and guide the user in manually\nanalyzing the effects of their prompt and model changes efficiently. We create\nthree benchmarks to quantitatively test the reliability of token pattern\nextraction methods and demonstrate that our approach provides new insights into\nestablished prompt data. From a human-centric perspective, through\ndemonstration studies and a user study, we show that our token pattern approach\nhelps users understand the systematic differences of language model outputs,\nand we are able to discover relevant differences caused by prompt and model\nchanges (e.g. related to gender or culture), thus supporting the prompt\nengineering process and human-centric model behavior research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15707", "pdf": "https://arxiv.org/pdf/2504.15707", "abs": "https://arxiv.org/abs/2504.15707", "authors": ["Yannic Neuhaus", "Matthias Hein"], "title": "RePOPE: Impact of Annotation Errors on the POPE Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Since data annotation is costly, benchmark datasets often incorporate labels\nfrom established image datasets. In this work, we assess the impact of label\nerrors in MSCOCO on the frequently used object hallucination benchmark POPE. We\nre-annotate the benchmark images and identify an imbalance in annotation errors\nacross different subsets. Evaluating multiple models on the revised labels,\nwhich we denote as RePOPE, we observe notable shifts in model rankings,\nhighlighting the impact of label quality. Code and data are available at\nhttps://github.com/YanNeu/RePOPE .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15723", "pdf": "https://arxiv.org/pdf/2504.15723", "abs": "https://arxiv.org/abs/2504.15723", "authors": ["Dasol Jeong", "Donggoo Kang", "Jiwon Park", "Hyebean Lee", "Joonki Paik"], "title": "Structure-Preserving Zero-Shot Image Editing via Stage-Wise Latent Injection in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We propose a diffusion-based framework for zero-shot image editing that\nunifies text-guided and reference-guided approaches without requiring\nfine-tuning. Our method leverages diffusion inversion and timestep-specific\nnull-text embeddings to preserve the structural integrity of the source image.\nBy introducing a stage-wise latent injection strategy-shape injection in early\nsteps and attribute injection in later steps-we enable precise, fine-grained\nmodifications while maintaining global consistency. Cross-attention with\nreference latents facilitates semantic alignment between the source and\nreference. Extensive experiments across expression transfer, texture\ntransformation, and style infusion demonstrate state-of-the-art performance,\nconfirming the method's scalability and adaptability to diverse image editing\nscenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15900", "pdf": "https://arxiv.org/pdf/2504.15900", "abs": "https://arxiv.org/abs/2504.15900", "authors": ["Cheng Wen", "Tingwei Guo", "Shuaijiang Zhao", "Wei Zou", "Xiangang Li"], "title": "SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that reinforcement learning(RL) can markedly sharpen the\nreasoning ability of large language models (LLMs) by prompting them to \"think\nbefore answering.\" Yet whether and how these gains transfer to audio-language\nreasoning remains largely unexplored. We extend the Group-Relative Policy\nOptimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model\n(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage\nregimen supervised fine-tuning on structured and unstructured\nchains-of-thought, followed by curriculum-guided GRPO, we systematically\ncompare implicit vs. explicit, and structured vs. free form reasoning under\nidentical architectures. Our structured audio reasoning model, SARI (Structured\nAudio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a\n16.35% improvement in average accuracy over the base model\nQwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni\nreaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.\nAblation experiments show that on the base model we use: (i) SFT warm-up is\nimportant for stable RL training, (ii) structured chains yield more robust\ngeneralization than unstructured ones, and (iii) easy-to-hard curricula\naccelerate convergence and improve final performance. These findings\ndemonstrate that explicit, structured reasoning and curriculum learning\nsubstantially enhances audio-language understanding.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15941", "pdf": "https://arxiv.org/pdf/2504.15941", "abs": "https://arxiv.org/abs/2504.15941", "authors": ["Fanny Jourdan", "Yannick Chevalier", "CÃ©cile Favre"], "title": "FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity", "categories": ["cs.CL", "cs.AI"], "comment": "FAccT 2025", "summary": "Large Language Models (LLMs) are increasingly leveraged for translation tasks\nbut often fall short when translating inclusive language -- such as texts\ncontaining the singular 'they' pronoun or otherwise reflecting fair linguistic\nprotocols. Because these challenges span both computational and societal\ndomains, it is imperative to critically evaluate how well LLMs handle inclusive\ntranslation with a well-founded framework.\n  This paper presents FairTranslate, a novel, fully human-annotated dataset\ndesigned to evaluate non-binary gender biases in machine translation systems\nfrom English to French. FairTranslate includes 2418 English-French sentence\npairs related to occupations, annotated with rich metadata such as the\nstereotypical alignment of the occupation, grammatical gender indicator\nambiguity, and the ground-truth gender label (male, female, or inclusive).\n  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,\nLlama3.3-70B) on this dataset under different prompting procedures. Our results\nreveal substantial biases in gender representation across LLMs, highlighting\npersistent challenges in achieving equitable outcomes in machine translation.\nThese findings underscore the need for focused strategies and interventions\naimed at ensuring fair and inclusive language usage in LLM-based translation\nsystems.\n  We make the FairTranslate dataset publicly available on Hugging Face, and\ndisclose the code for all experiments on GitHub.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15783", "pdf": "https://arxiv.org/pdf/2504.15783", "abs": "https://arxiv.org/abs/2504.15783", "authors": ["Johan Ãfverstedt", "Elin LundstrÃ¶m", "HÃ¥kan AhlstrÃ¶m", "Joel Kullberg"], "title": "Towards prediction of morphological heart age from computed tomography angiography", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Age prediction from medical images or other health-related non-imaging data\nis an important approach to data-driven aging research, providing knowledge of\nhow much information a specific tissue or organ carries about the chronological\nage of the individual. In this work, we studied the prediction of age from\ncomputed tomography angiography (CTA) images, which provide detailed\nrepresentations of the heart morphology, with the goals of (i) studying the\nrelationship between morphology and aging, and (ii) developing a novel\n\\emph{morphological heart age} biomarker. We applied an image\nregistration-based method that standardizes the images from the whole cohort\ninto a single space. We then extracted supervoxels (using unsupervised\nsegmentation), and corresponding robust features of density and local volume,\nwhich provide a detailed representation of the heart morphology while being\nrobust to registration errors. Machine learning models are then trained to fit\nregression models from these features to the chronological age. We applied the\nmethod to a subset of the images from the Swedish CArdioPulomonary bioImage\nStudy (SCAPIS) dataset, consisting of 721 females and 666 males. We observe a\nmean absolute error of $2.74$ years for females and $2.77$ years for males. The\npredictions from different sub-regions of interest were observed to be more\nhighly correlated with the predictions from the whole heart, compared to the\nchronological age, revealing a high consistency in the predictions from\nmorphology. Saliency analysis was also performed on the prediction models to\nstudy what regions are associated positively and negatively with the predicted\nage. This resulted in detailed association maps where the density and volume of\nknown, as well as some novel sub-regions of interest, are determined to be\nimportant. The saliency analysis aids in the interpretability of the models and\ntheir predictions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15786", "pdf": "https://arxiv.org/pdf/2504.15786", "abs": "https://arxiv.org/abs/2504.15786", "authors": ["Ningli Xu", "Rongjun Qin"], "title": "Satellite to GroundScape -- Large-scale Consistent Ground View Generation from Satellite Views", "categories": ["cs.CV"], "comment": "8 figures", "summary": "Generating consistent ground-view images from satellite imagery is\nchallenging, primarily due to the large discrepancies in viewing angles and\nresolution between satellite and ground-level domains. Previous efforts mainly\nconcentrated on single-view generation, often resulting in inconsistencies\nacross neighboring ground views. In this work, we propose a novel cross-view\nsynthesis approach designed to overcome these challenges by ensuring\nconsistency across ground-view images generated from satellite views. Our\nmethod, based on a fixed latent diffusion model, introduces two conditioning\nmodules: satellite-guided denoising, which extracts high-level scene layout to\nguide the denoising process, and satellite-temporal denoising, which captures\ncamera motion to maintain consistency across multiple generated views. We\nfurther contribute a large-scale satellite-ground dataset containing over\n100,000 perspective pairs to facilitate extensive ground scene or video\ngeneration. Experimental results demonstrate that our approach outperforms\nexisting methods on perceptual and temporal metrics, achieving high\nphotorealism and consistency in multi-view outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16056", "pdf": "https://arxiv.org/pdf/2504.16056", "abs": "https://arxiv.org/abs/2504.16056", "authors": ["Daniel Hendriks", "Philipp Spitzer", "Niklas KÃ¼hl", "Gerhard Satzger"], "title": "Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability", "categories": ["cs.CL"], "comment": null, "summary": "Artificial Intelligence (AI) has increasingly influenced modern society,\nrecently in particular through significant advancements in Large Language\nModels (LLMs). However, high computational and storage demands of LLMs still\nlimit their deployment in resource-constrained environments. Knowledge\ndistillation addresses this challenge by training a small student model from a\nlarger teacher model. Previous research has introduced several distillation\nmethods for both generating training data and for training the student model.\nDespite their relevance, the effects of state-of-the-art distillation methods\non model performance and explainability have not been thoroughly investigated\nand compared. In this work, we enlarge the set of available methods by applying\ncritique-revision prompting to distillation for data generation and by\nsynthesizing existing methods for training. For these methods, we provide a\nsystematic comparison based on the widely used Commonsense Question-Answering\n(CQA) dataset. While we measure performance via student model accuracy, we\nemploy a human-grounded study to evaluate explainability. We contribute new\ndistillation methods and their comparison in terms of both performance and\nexplainability. This should further advance the distillation of small language\nmodels and, thus, contribute to broader applicability and faster diffusion of\nLLM technology.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15792", "pdf": "https://arxiv.org/pdf/2504.15792", "abs": "https://arxiv.org/abs/2504.15792", "authors": ["Dinh Nam Pham", "Torsten Rahne"], "title": "Development and evaluation of a deep learning algorithm for German word recognition from lip movements", "categories": ["cs.CV"], "comment": "English version of journal article in HNO 2022", "summary": "When reading lips, many people benefit from additional visual information\nfrom the lip movements of the speaker, which is, however, very error prone.\nAlgorithms for lip reading with artificial intelligence based on artificial\nneural networks significantly improve word recognition but are not available\nfor the German language. A total of 1806 video clips with only one\nGerman-speaking person each were selected, split into word segments, and\nassigned to word classes using speech-recognition software. In 38,391 video\nsegments with 32 speakers, 18 polysyllabic, visually distinguishable words were\nused to train and validate a neural network. The 3D Convolutional Neural\nNetwork and Gated Recurrent Units models and a combination of both models\n(GRUConv) were compared, as were different image sections and color spaces of\nthe videos. The accuracy was determined in 5000 training epochs. Comparison of\nthe color spaces did not reveal any relevant different correct classification\nrates in the range from 69% to 72%. With a cut to the lips, a significantly\nhigher accuracy of 70% was achieved than when cut to the entire speaker's face\n(34%). With the GRUConv model, the maximum accuracies were 87% with known\nspeakers and 63% in the validation with unknown speakers. The neural network\nfor lip reading, which was first developed for the German language, shows a\nvery high level of accuracy, comparable to English-language algorithms. It\nworks with unknown speakers as well and can be generalized with more word\nclasses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16063", "pdf": "https://arxiv.org/pdf/2504.16063", "abs": "https://arxiv.org/abs/2504.16063", "authors": ["A. Fronzetti Colladon", "R. Vestrelli"], "title": "A Python Tool for Reconstructing Full News Text from GDELT", "categories": ["cs.CL", "cs.DB", "cs.IR", "I.2.7; H.2.8; H.3.1"], "comment": null, "summary": "News data have become an essential resource across various disciplines,\nincluding economics, finance, management, social sciences, and computer\nscience. Researchers leverage newspaper articles to study economic trends,\nmarket dynamics, corporate strategies, public perception, political discourse,\nand the evolution of public opinion. Additionally, news datasets have been\ninstrumental in training large-scale language models, with applications in\nsentiment analysis, fake news detection, and automated news summarization.\nDespite their significance, access to comprehensive news corpora remains a key\nchallenge. Many full-text news providers, such as Factiva and LexisNexis,\nrequire costly subscriptions, while free alternatives often suffer from\nincomplete data and transparency issues. This paper presents a novel approach\nto obtaining full-text newspaper articles at near-zero cost by leveraging data\nfrom the Global Database of Events, Language, and Tone (GDELT). Specifically,\nwe focus on the GDELT Web News NGrams 3.0 dataset, which provides\nhigh-frequency updates of n-grams extracted from global online news sources. We\nprovide Python code to reconstruct full-text articles from these n-grams by\nidentifying overlapping textual fragments and intelligently merging them. Our\nmethod enables researchers to access structured, large-scale newspaper data for\ntext analysis while overcoming the limitations of existing proprietary\ndatasets. The proposed approach enhances the accessibility of news data for\nempirical research, facilitating applications in economic forecasting,\ncomputational social science, and natural language processing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16073", "pdf": "https://arxiv.org/pdf/2504.16073", "abs": "https://arxiv.org/abs/2504.16073", "authors": ["Zhiyuan Hu", "Shiyun Xiong", "Yifan Zhang", "See-Kiong Ng", "Anh Tuan Luu", "Bo An", "Shuicheng Yan", "Bryan Hooi"], "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in visual language models (VLMs) have notably enhanced\ntheir capabilities in handling complex Graphical User Interface (GUI)\ninteraction tasks. Despite these improvements, current frameworks often\nstruggle to generate correct actions in challenging GUI environments.\nState-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source\nVLMs for GUI tasks requires significant resources. Additionally, existing\ntrajectory-level evaluation and refinement techniques frequently fall short due\nto delayed feedback and local optimization issues. To address these challenges,\nwe propose an approach that guides VLM agents with process supervision by a\nreward model during GUI navigation and control at inference time. This guidance\nallows the VLM agent to optimize actions at each inference step, thereby\nimproving performance in both static and dynamic environments. In particular,\nour method demonstrates significant performance gains in three GUI navigation\ntasks, achieving a 3.4% improvement in single step action accuracy for static\nenvironments, along with a around 33% increase in task success rate in one\ndynamic environment. With further integration of trajectory reflection and\nretry mechanisms, we also demonstrate even greater enhancement in task success.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15865", "pdf": "https://arxiv.org/pdf/2504.15865", "abs": "https://arxiv.org/abs/2504.15865", "authors": ["Lotfi Abdelkrim Mecharbat", "Ibrahim Elmakky", "Martin Takac", "Mohammed Yaqub"], "title": "MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning (DL) has achieved remarkable progress in the field of medical\nimaging. However, adapting DL models to medical tasks remains a significant\nchallenge, primarily due to two key factors: (1) architecture selection, as\ndifferent tasks necessitate specialized model designs, and (2) weight\ninitialization, which directly impacts the convergence speed and final\nperformance of the models. Although transfer learning from ImageNet is a widely\nadopted strategy, its effectiveness is constrained by the substantial\ndifferences between natural and medical images. To address these challenges, we\nintroduce Medical Neural Network Search (MedNNS), the first Neural Network\nSearch framework for medical imaging applications. MedNNS jointly optimizes\narchitecture selection and weight initialization by constructing a meta-space\nthat encodes datasets and models based on how well they perform together. We\nbuild this space using a Supernetwork-based approach, expanding the model zoo\nsize by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we\nintroduce rank loss and Fr\\'echet Inception Distance (FID) loss into the\nconstruction of the space to capture inter-model and inter-dataset\nrelationships, thereby achieving more accurate alignment in the meta-space.\nExperimental results across multiple datasets demonstrate that MedNNS\nsignificantly outperforms both ImageNet pre-trained DL models and SOTA Neural\nArchitecture Search (NAS) methods, achieving an average accuracy improvement of\n1.7% across datasets while converging substantially faster. The code and the\nprocessed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15362", "pdf": "https://arxiv.org/pdf/2504.15362", "abs": "https://arxiv.org/abs/2504.15362", "authors": ["Yuan-Hong Liao", "Sven Elflein", "Liu He", "Laura Leal-TaixÃ©", "Yejin Choi", "Sanja Fidler", "David Acuna"], "title": "LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "24 pages, 10 figures, in submission. Project page:\n  https://andrewliao11.github.io/LongPerceptualThoughts", "summary": "Recent reasoning models through test-time scaling have demonstrated that long\nchain-of-thoughts can unlock substantial performance boosts in hard reasoning\ntasks such as math and code. However, the benefit of such long thoughts for\nsystem-2 reasoning is relatively less explored in other domains such as\nperceptual tasks where shallower, system-1 reasoning seems sufficient. In this\npaper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K\nlong-thought traces for perceptual tasks. The key challenges in synthesizing\nelaborate reasoning thoughts for perceptual tasks are that off-the-shelf models\nare not yet equipped with such thinking behavior and that it is not\nstraightforward to build a reliable process verifier for perceptual tasks.\nThus, we propose a novel three-stage data synthesis framework that first\nsynthesizes verifiable multiple-choice questions from dense image descriptions,\nthen extracts simple CoTs from VLMs for those verifiable problems, and finally\nexpands those simple thoughts to elaborate long thoughts via frontier reasoning\nmodels. In controlled experiments with a strong instruction-tuned 7B model, we\ndemonstrate notable improvements over existing visual reasoning data-generation\nmethods. Our model, trained on the generated dataset, achieves an average +3.4\npoints improvement over 5 vision-centric benchmarks, including +11.8 points on\nV$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves\nperformance on the text reasoning benchmark, MMLU-Pro, by +2 points.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15888", "pdf": "https://arxiv.org/pdf/2504.15888", "abs": "https://arxiv.org/abs/2504.15888", "authors": ["Zhiqiang Wei", "Lianqing Zheng", "Jianan Liu", "Tao Huang", "Qing-Long Han", "Wenwen Zhang", "Fengdeng Zhang"], "title": "MS-Occ: Multi-Stage LiDAR-Camera Fusion for 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Accurate 3D semantic occupancy perception is essential for autonomous driving\nin complex environments with diverse and irregular objects. While\nvision-centric methods suffer from geometric inaccuracies, LiDAR-based\napproaches often lack rich semantic information. To address these limitations,\nMS-Occ, a novel multi-stage LiDAR-camera fusion framework which includes\nmiddle-stage fusion and late-stage fusion, is proposed, integrating LiDAR's\ngeometric fidelity with camera-based semantic richness via hierarchical\ncross-modal fusion. The framework introduces innovations at two critical\nstages: (1) In the middle-stage feature fusion, the Gaussian-Geo module\nleverages Gaussian kernel rendering on sparse LiDAR depth maps to enhance 2D\nimage features with dense geometric priors, and the Semantic-Aware module\nenriches LiDAR voxels with semantic context via deformable cross-attention; (2)\nIn the late-stage voxel fusion, the Adaptive Fusion (AF) module dynamically\nbalances voxel features across modalities, while the High Classification\nConfidence Voxel Fusion (HCCVF) module resolves semantic inconsistencies using\nself-attention-based refinement. Experiments on the nuScenes-OpenOccupancy\nbenchmark show that MS-Occ achieves an Intersection over Union (IoU) of 32.1%\nand a mean IoU (mIoU) of 25.3%, surpassing the state-of-the-art by +0.7% IoU\nand +2.4% mIoU. Ablation studies further validate the contribution of each\nmodule, with substantial improvements in small-object perception, demonstrating\nthe practical value of MS-Occ for safety-critical autonomous driving scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15928", "pdf": "https://arxiv.org/pdf/2504.15928", "abs": "https://arxiv.org/abs/2504.15928", "authors": ["Meng Wang", "Tian Lin", "Qingshan Hou", "Aidi Lin", "Jingcheng Wang", "Qingsheng Peng", "Truong X. Nguyen", "Danqi Fang", "Ke Zou", "Ting Xu", "Cancan Xue", "Ten Cheer Quek", "Qinkai Yu", "Minxin Liu", "Hui Zhou", "Zixuan Xiao", "Guiqin He", "Huiyu Liang", "Tingkun Shi", "Man Chen", "Linna Liu", "Yuanyuan Peng", "Lianyu Wang", "Qiuming Hu", "Junhong Chen", "Zhenhua Zhang", "Cheng Chen", "Yitian Zhao", "Dianbo Liu", "Jianhua Wu", "Xinjian Chen", "Changqing Zhang", "Triet Thanh Nguyen", "Yanda Meng", "Yalin Zheng", "Yih Chung Tham", "Carol Y. Cheung", "Huazhu Fu", "Haoyu Chen", "Ching-Yu Cheng"], "title": "A Clinician-Friendly Platform for Ophthalmic Image Analysis Without Technical Barriers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) shows remarkable potential in medical imaging\ndiagnostics, but current models typically require retraining when deployed\nacross different clinical centers, limiting their widespread adoption. We\nintroduce GlobeReady, a clinician-friendly AI platform that enables ocular\ndisease diagnosis without retraining/fine-tuning or technical expertise.\nGlobeReady achieves high accuracy across imaging modalities: 93.9-98.5% for an\n11-category fundus photo dataset and 87.2-92.7% for a 15-category OCT dataset.\nThrough training-free local feature augmentation, it addresses domain shifts\nacross centers and populations, reaching an average accuracy of 88.9% across\nfive centers in China, 86.3% in Vietnam, and 90.2% in the UK. The built-in\nconfidence-quantifiable diagnostic approach further boosted accuracy to\n94.9-99.4% (fundus) and 88.2-96.2% (OCT), while identifying out-of-distribution\ncases at 86.3% (49 CFP categories) and 90.6% (13 OCT categories). Clinicians\nfrom multiple countries rated GlobeReady highly (average 4.6 out of 5) for its\nusability and clinical relevance. These results demonstrate GlobeReady's\nrobust, scalable diagnostic capability and potential to support ophthalmic care\nwithout technical barriers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15466", "pdf": "https://arxiv.org/pdf/2504.15466", "abs": "https://arxiv.org/abs/2504.15466", "authors": ["Jiayi Pan", "Xiuyu Li", "Long Lian", "Charlie Snell", "Yifei Zhou", "Adam Yala", "Trevor Darrell", "Kurt Keutzer", "Alane Suhr"], "title": "Learning Adaptive Parallel Reasoning with Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code, model, and data are available at\n  https://github.com/Parallel-Reasoning/APR. The first three authors\n  contributed equally to this work", "summary": "Scaling inference-time computation has substantially improved the reasoning\ncapabilities of language models. However, existing methods have significant\nlimitations: serialized chain-of-thought approaches generate overly long\noutputs, leading to increased latency and exhausted context windows, while\nparallel methods such as self-consistency suffer from insufficient\ncoordination, resulting in redundant computations and limited performance\ngains. To address these shortcomings, we propose Adaptive Parallel Reasoning\n(APR), a novel reasoning framework that enables language models to orchestrate\nboth serialized and parallel computations end-to-end. APR generalizes existing\nreasoning methods by enabling adaptive multi-threaded inference using spawn()\nand join() operations. A key innovation is our end-to-end reinforcement\nlearning strategy, optimizing both parent and child inference threads to\nenhance task success rate without requiring predefined reasoning structures.\nExperiments on the Countdown reasoning task demonstrate significant benefits of\nAPR: (1) higher performance within the same context window (83.4% vs. 60.0% at\n4k context); (2) superior scalability with increased computation (80.1% vs.\n66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2%\nvs. 57.3% at approximately 5,000ms). APR represents a step towards enabling\nlanguage models to autonomously optimize their reasoning processes through\nadaptive allocation of computation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15931", "pdf": "https://arxiv.org/pdf/2504.15931", "abs": "https://arxiv.org/abs/2504.15931", "authors": ["Ekaterina Kondrateva", "Sandzhi Barg", "Mikhail Vasiliev"], "title": "Benchmarking the Reproducibility of Brain MRI Segmentation Across Scanners and Time", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and reproducible brain morphometry from structural MRI is critical\nfor monitoring neuroanatomical changes across time and across imaging domains.\nAlthough deep learning has accelerated segmentation workflows, scanner-induced\nvariability and reproducibility limitations remain-especially in longitudinal\nand multi-site settings. In this study, we benchmark two modern segmentation\npipelines, FastSurfer and SynthSeg, both integrated into FreeSurfer, one of the\nmost widely adopted tools in neuroimaging.\n  Using two complementary datasets - a 17-year longitudinal cohort (SIMON) and\na 9-site test-retest cohort (SRPBS)-we quantify inter-scan segmentation\nvariability using Dice coefficient, Surface Dice, Hausdorff Distance (HD95),\nand Mean Absolute Percentage Error (MAPE). Our results reveal up to 7-8% volume\nvariation in small subcortical structures such as the amygdala and ventral\ndiencephalon, even under controlled test-retest conditions. This raises a key\nquestion: is it feasible to detect subtle longitudinal changes on the order of\n5-10% in pea-sized brain regions, given the magnitude of domain-induced\nmorphometric noise?\n  We further analyze the effects of registration templates and interpolation\nmodes, and propose surface-based quality filtering to improve segmentation\nreliability. This study provides a reproducible benchmark for morphometric\nreproducibility and emphasizes the need for harmonization strategies in\nreal-world neuroimaging studies.\n  Code and figures: https://github.com/kondratevakate/brain-mri-segmentation", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15659", "pdf": "https://arxiv.org/pdf/2504.15659", "abs": "https://arxiv.org/abs/2504.15659", "authors": ["Anjiang Wei", "Huanmi Tan", "Tarun Suresh", "Daniel Mendoza", "Thiago S. F. X. Teixeira", "Ke Wang", "Caroline Trippel", "Alex Aiken"], "title": "VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have sparked growing interest\nin applying them to Electronic Design Automation (EDA) tasks, particularly\nRegister Transfer Level (RTL) code generation. While several RTL datasets have\nbeen introduced, most focus on syntactic validity rather than functional\nvalidation with tests, leading to training examples that compile but may not\nimplement the intended behavior. We present VERICODER, a model for RTL code\ngeneration fine-tuned on a dataset validated for functional correctness. This\nfine-tuning dataset is constructed using a novel methodology that combines unit\ntest generation with feedback-directed refinement. Given a natural language\nspecification and an initial RTL design, we prompt a teacher model\n(GPT-4o-mini) to generate unit tests and iteratively revise the RTL design\nbased on its simulation results using the generated tests. If necessary, the\nteacher model also updates the tests to ensure they comply with the natural\nlanguage specification. As a result of this process, every example in our\ndataset is functionally validated, consisting of a natural language\ndescription, an RTL implementation, and passing tests. Fine-tuned on this\ndataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics\nin functional correctness on VerilogEval and RTLLM, with relative gains of up\nto 71.7% and 27.4% respectively. An ablation study further shows that models\ntrained on our functionally validated dataset outperform those trained on\nfunctionally non-validated datasets, underscoring the importance of\nhigh-quality datasets in RTL code generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "code generation"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16016", "pdf": "https://arxiv.org/pdf/2504.16016", "abs": "https://arxiv.org/abs/2504.16016", "authors": ["Xinyuan Song", "Yangfan He", "Sida Li", "Jianhui Wang", "Hongyang He", "Xinhang Yuan", "Ruoyu Wang", "Jiaqi Chen", "Keqin Li", "Kuan Lu", "Menghao Huo", "Binxu Li", "Pei Liu"], "title": "Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2501.04606", "summary": "Adapter-based methods are commonly used to enhance model performance with\nminimal additional complexity, especially in video editing tasks that require\nframe-to-frame consistency. By inserting small, learnable modules into\npretrained diffusion models, these adapters can maintain temporal coherence\nwithout extensive retraining. Approaches that incorporate prompt learning with\nboth shared and frame-specific tokens are particularly effective in preserving\ncontinuity across frames at low training cost. In this work, we want to provide\na general theoretical framework for adapters that maintain frame consistency in\nDDIM-based models under a temporal consistency loss. First, we prove that the\ntemporal consistency objective is differentiable under bounded feature norms,\nand we establish a Lipschitz bound on its gradient. Second, we show that\ngradient descent on this objective decreases the loss monotonically and\nconverges to a local minimum if the learning rate is within an appropriate\nrange. Finally, we analyze the stability of modules in the DDIM inversion\nprocedure, showing that the associated error remains controlled. These\ntheoretical findings will reinforce the reliability of diffusion-based video\nediting methods that rely on adapter strategies and provide theoretical\ninsights in video generation tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16061", "pdf": "https://arxiv.org/pdf/2504.16061", "abs": "https://arxiv.org/abs/2504.16061", "authors": ["Sangeet Khemlani", "Tyler Tran", "Nathaniel Gyory", "Anthony M. Harrison", "Wallace E. Lawson", "Ravenna Thielstrom", "Hunter Thompson", "Taaren Singh", "J. Gregory Trafton"], "title": "Vision language models are unreliable at trivial spatial cognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision language models (VLMs) are designed to extract relevant visuospatial\ninformation from images. Some research suggests that VLMs can exhibit humanlike\nscene understanding, while other investigations reveal difficulties in their\nability to process relational information. To achieve widespread applicability,\nVLMs must perform reliably, yielding comparable competence across a wide\nvariety of related tasks. We sought to test how reliable these architectures\nare at engaging in trivial spatial cognition, e.g., recognizing whether one\nobject is left of another in an uncluttered scene. We developed a benchmark\ndataset -- TableTest -- whose images depict 3D scenes of objects arranged on a\ntable, and used it to evaluate state-of-the-art VLMs. Results show that\nperformance could be degraded by minor variations of prompts that use logically\nequivalent descriptions. These analyses suggest limitations in how VLMs may\nreason about spatial relations in real-world applications. They also reveal\nnovel opportunities for bolstering image caption corpora for more efficient\ntraining and testing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16080", "pdf": "https://arxiv.org/pdf/2504.16080", "abs": "https://arxiv.org/abs/2504.16080", "authors": ["Le Zhuo", "Liangbing Zhao", "Sayak Paul", "Yue Liao", "Renrui Zhang", "Yi Xin", "Peng Gao", "Mohamed Elhoseiny", "Hongsheng Li"], "title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning", "categories": ["cs.CV"], "comment": "All code, checkpoints, and datasets are available at\n  \\url{https://diffusion-cot.github.io/reflection2perfection}", "summary": "Recent text-to-image diffusion models achieve impressive visual quality\nthrough extensive scaling of training data and model parameters, yet they often\nstruggle with complex scenes and fine-grained details. Inspired by the\nself-reflection capabilities emergent in large language models, we propose\nReflectionFlow, an inference-time framework enabling diffusion models to\niteratively reflect upon and refine their outputs. ReflectionFlow introduces\nthree complementary inference-time scaling axes: (1) noise-level scaling to\noptimize latent initialization; (2) prompt-level scaling for precise semantic\nguidance; and most notably, (3) reflection-level scaling, which explicitly\nprovides actionable reflections to iteratively assess and correct previous\ngenerations. To facilitate reflection-level scaling, we construct GenRef, a\nlarge-scale dataset comprising 1 million triplets, each containing a\nreflection, a flawed image, and an enhanced image. Leveraging this dataset, we\nefficiently perform reflection tuning on state-of-the-art diffusion\ntransformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified\nframework. Experimental results show that ReflectionFlow significantly\noutperforms naive noise-level scaling methods, offering a scalable and\ncompute-efficient solution toward higher-quality image synthesis on challenging\ntasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.09697", "pdf": "https://arxiv.org/pdf/2504.09697", "abs": "https://arxiv.org/abs/2504.09697", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "24 pages, 21 figures. Figure 9(b) has been accepted by CVPR AI Art\n  Gallery 2025", "summary": "Recent prompt-based image editing models have demonstrated impressive\nprompt-following capability at structural editing tasks. However, existing\nmodels still fail to perform local edits, follow detailed editing prompts, or\nmaintain global image quality beyond a single editing step. To address these\nchallenges, we introduce SPICE, a training-free workflow that accepts arbitrary\nresolutions and aspect ratios, accurately follows user requirements, and\nimproves image quality consistently during more than 100 editing steps. By\nsynergizing the strengths of a base diffusion model and a Canny edge ControlNet\nmodel, SPICE robustly handles free-form editing instructions from the user.\nSPICE outperforms state-of-the-art baselines on a challenging realistic\nimage-editing dataset consisting of semantic editing (object addition, removal,\nreplacement, and background change), stylistic editing (texture changes), and\nstructural editing (action change) tasks. Not only does SPICE achieve the\nhighest quantitative performance according to standard evaluation metrics, but\nit is also consistently preferred by users over existing image-editing methods.\nWe release the workflow implementation for popular diffusion model Web UIs to\nsupport further research and artistic exploration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15317", "pdf": "https://arxiv.org/pdf/2504.15317", "abs": "https://arxiv.org/abs/2504.15317", "authors": ["Meher Boulaabi", "Takwa Ben AÃ¯cha Gader", "Afef Kacem Echi", "Zied Bouraoui"], "title": "Enhancing DR Classification with Swin Transformer and Shifted Window Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide,\nunderscoring the importance of early detection for effective treatment.\nHowever, automated DR classification remains challenging due to variations in\nimage quality, class imbalance, and pixel-level similarities that hinder model\ntraining. To address these issues, we propose a robust preprocessing pipeline\nincorporating image cropping, Contrast-Limited Adaptive Histogram Equalization\n(CLAHE), and targeted data augmentation to improve model generalization and\nresilience. Our approach leverages the Swin Transformer, which utilizes\nhierarchical token processing and shifted window attention to efficiently\ncapture fine-grained features while maintaining linear computational\ncomplexity. We validate our method on the Aptos and IDRiD datasets for\nmulti-class DR classification, achieving accuracy rates of 89.65% and 97.40%,\nrespectively. These results demonstrate the effectiveness of our model,\nparticularly in detecting early-stage DR, highlighting its potential for\nimproving automated retinal screening in clinical settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15667", "pdf": "https://arxiv.org/pdf/2504.15667", "abs": "https://arxiv.org/abs/2504.15667", "authors": ["Jingchen Zou", "Jianqiang Li", "Gabriel Jimenez", "Qing Zhao", "Daniel Racoceanu", "Matias Cosarinsky", "Enzo Ferrante", "Guanghui Fu"], "title": "Performance Estimation for Supervised Medical Image Segmentation Models on Unlabeled Data Using UniverSeg", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The performance of medical image segmentation models is usually evaluated\nusing metrics like the Dice score and Hausdorff distance, which compare\npredicted masks to ground truth annotations. However, when applying the model\nto unseen data, such as in clinical settings, it is often impractical to\nannotate all the data, making the model's performance uncertain. To address\nthis challenge, we propose the Segmentation Performance Evaluator (SPE), a\nframework for estimating segmentation models' performance on unlabeled data.\nThis framework is adaptable to various evaluation metrics and model\narchitectures. Experiments on six publicly available datasets across six\nevaluation metrics including pixel-based metrics such as Dice score and\ndistance-based metrics like HD95, demonstrated the versatility and\neffectiveness of our approach, achieving a high correlation (0.956$\\pm$0.046)\nand low MAE (0.025$\\pm$0.019) compare with real Dice score on the independent\ntest set. These results highlight its ability to reliably estimate model\nperformance without requiring annotations. The SPE framework integrates\nseamlessly into any model training process without adding training overhead,\nenabling performance estimation and facilitating the real-world application of\nmedical image segmentation algorithms. The source code is publicly available", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15431", "pdf": "https://arxiv.org/pdf/2504.15431", "abs": "https://arxiv.org/abs/2504.15431", "authors": ["Sungjun Han", "Juyoung Suk", "Suyeong An", "Hyungguk Kim", "Kyuseok Kim", "Wonsuk Yang", "Seungtaek Choi", "Jamin Shin"], "title": "Trillion 7B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preview version", "summary": "We introduce Trillion-7B, the most token-efficient Korean-centric\nmultilingual LLM available. Our novel Cross-lingual Document Attention (XLDA)\nmechanism enables highly efficient and effective knowledge transfer from\nEnglish to target languages like Korean and Japanese. Combined with optimized\ndata mixtures, language-specific filtering, and tailored tokenizer\nconstruction, Trillion-7B achieves competitive performance while dedicating\nonly 10\\% of its 2T training tokens to multilingual data and requiring just\n59.4K H100 GPU hours (\\$148K) for full training. Comprehensive evaluations\nacross 27 benchmarks in four languages demonstrate Trillion-7B's robust\nmultilingual performance and exceptional cross-lingual consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15397", "pdf": "https://arxiv.org/pdf/2504.15397", "abs": "https://arxiv.org/abs/2504.15397", "authors": ["Ankit Dhiman", "Manan Shah", "R Venkatesh Babu"], "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page: https://mirror-verse.github.io/", "summary": "Diffusion models have become central to various image editing tasks, yet they\noften fail to fully adhere to physical laws, particularly with effects like\nshadows, reflections, and occlusions. In this work, we address the challenge of\ngenerating photorealistic mirror reflections using diffusion-based generative\nmodels. Despite extensive training data, existing diffusion models frequently\noverlook the nuanced details crucial to authentic mirror reflections. Recent\napproaches have attempted to resolve this by creating synhetic datasets and\nframing reflection generation as an inpainting task; however, they struggle to\ngeneralize across different object orientations and positions relative to the\nmirror. Our method overcomes these limitations by introducing key augmentations\ninto the synthetic data pipeline: (1) random object positioning, (2) randomized\nrotations, and (3) grounding of objects, significantly enhancing generalization\nacross poses and placements. To further address spatial relationships and\nocclusions in scenes with multiple objects, we implement a strategy to pair\nobjects during dataset generation, resulting in a dataset robust enough to\nhandle these complex scenarios. Achieving generalization to real-world scenes\nremains a challenge, so we introduce a three-stage training curriculum to\ndevelop the MirrorFusion 2.0 model to improve real-world performance. We\nprovide extensive qualitative and quantitative evaluations to support our\napproach. The project page is available at: https://mirror-verse.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15470", "pdf": "https://arxiv.org/pdf/2504.15470", "abs": "https://arxiv.org/abs/2504.15470", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025 (The International Conference on Learning\n  Representations)", "summary": "Distinguishing between real and AI-generated images, commonly referred to as\n'image detection', presents a timely and significant challenge. Despite\nextensive research in the (semi-)supervised regime, zero-shot and few-shot\nsolutions have only recently emerged as promising alternatives. Their main\nadvantage is in alleviating the ongoing data maintenance, which quickly becomes\noutdated due to advances in generative technologies. We identify two main gaps:\n(1) a lack of theoretical grounding for the methods, and (2) significant room\nfor performance improvements in zero-shot and few-shot regimes. Our approach is\nfounded on understanding and quantifying the biases inherent in generated\ncontent, where we use these quantities as criteria for characterizing generated\nimages. Specifically, we explore the biases of the implicit probability\nmanifold, captured by a pre-trained diffusion model. Through score-function\nanalysis, we approximate the curvature, gradient, and bias towards points on\nthe probability manifold, establishing criteria for detection in the zero-shot\nregime. We further extend our contribution to the few-shot setting by employing\na mixture-of-experts methodology. Empirical results across 20 generative models\ndemonstrate that our method outperforms current approaches in both zero-shot\nand few-shot settings. This work advances the theoretical understanding and\npractical usage of generated content biases through the lens of manifold\nanalysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15544", "pdf": "https://arxiv.org/pdf/2504.15544", "abs": "https://arxiv.org/abs/2504.15544", "authors": ["Issa Sugiura", "Kouta Nakayama", "Yusuke Oda"], "title": "llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Encoder-only transformer models like BERT are widely adopted as a pre-trained\nbackbone for tasks like sentence classification and retrieval. However,\npretraining of encoder models with large-scale corpora and long contexts has\nbeen relatively underexplored compared to decoder-only transformers. In this\nwork, we present llm-jp-modernbert, a ModernBERT model trained on a publicly\navailable, massive Japanese corpus with a context length of 8192 tokens. While\nour model does not surpass existing baselines on downstream tasks, it achieves\ngood results on fill-mask test evaluations. We also analyze the effect of\ncontext length expansion through pseudo-perplexity experiments. Furthermore, we\ninvestigate sentence embeddings in detail, analyzing their transitions during\ntraining and comparing them with those from other existing models, confirming\nsimilar trends with models sharing the same architecture. To support\nreproducibility and foster the development of long-context BERT, we release our\nmodel, along with the training and evaluation code.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15548", "pdf": "https://arxiv.org/pdf/2504.15548", "abs": "https://arxiv.org/abs/2504.15548", "authors": ["Elyas Meguellati", "Assaad Zeghina", "Shazia Sadiq", "Gianluca Demartini"], "title": "LLM-based Semantic Augmentation for Harmful Content Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated strong\nperformance on simple text classification tasks, frequently under zero-shot\nsettings. However, their efficacy declines when tackling complex social media\nchallenges such as propaganda detection, hateful meme classification, and\ntoxicity identification. Much of the existing work has focused on using LLMs to\ngenerate synthetic training data, overlooking the potential of LLM-based text\npreprocessing and semantic augmentation. In this paper, we introduce an\napproach that prompts LLMs to clean noisy text and provide context-rich\nexplanations, thereby enhancing training sets without substantial increases in\ndata volume. We systematically evaluate on the SemEval 2024 multi-label\nPersuasive Meme dataset and further validate on the Google Jigsaw toxic\ncomments and Facebook hateful memes datasets to assess generalizability. Our\nresults reveal that zero-shot LLM classification underperforms on these\nhigh-context tasks compared to supervised models. In contrast, integrating\nLLM-based semantic augmentation yields performance on par with approaches that\nrely on human-annotated data, at a fraction of the cost. These findings\nunderscore the importance of strategically incorporating LLMs into machine\nlearning (ML) pipeline for social media classification tasks, offering broad\nimplications for combating harmful content online.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15599", "pdf": "https://arxiv.org/pdf/2504.15599", "abs": "https://arxiv.org/abs/2504.15599", "authors": ["Shichen Li", "Chenhui Shao"], "title": "Multi-Modal Fusion of In-Situ Video Data and Process Parameters for Online Forecasting of Cookie Drying Readiness", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Food drying is essential for food production, extending shelf life, and\nreducing transportation costs. Accurate real-time forecasting of drying\nreadiness is crucial for minimizing energy consumption, improving productivity,\nand ensuring product quality. However, this remains challenging due to the\ndynamic nature of drying, limited data availability, and the lack of effective\npredictive analytical methods. To address this gap, we propose an end-to-end\nmulti-modal data fusion framework that integrates in-situ video data with\nprocess parameters for real-time food drying readiness forecasting. Our\napproach leverages a new encoder-decoder architecture with modality-specific\nencoders and a transformer-based decoder to effectively extract features while\npreserving the unique structure of each modality. We apply our approach to\nsugar cookie drying, where time-to-ready is predicted at each timestamp.\nExperimental results demonstrate that our model achieves an average prediction\nerror of only 15 seconds, outperforming state-of-the-art data fusion methods by\n65.69% and a video-only model by 11.30%. Additionally, our model balances\nprediction accuracy, model size, and computational efficiency, making it\nwell-suited for heterogenous industrial datasets. The proposed model is\nextensible to various other industrial modality fusion tasks for online\ndecision-making.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15612", "pdf": "https://arxiv.org/pdf/2504.15612", "abs": "https://arxiv.org/abs/2504.15612", "authors": ["Hongxing Peng", "Kang Lin", "Huanai Liu"], "title": "HS-Mamba: Full-Field Interaction Multi-Groups Mamba for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification has been one of the hot topics in\nremote sensing fields. Recently, the Mamba architecture based on selective\nstate-space models (S6) has demonstrated great advantages in long sequence\nmodeling. However, the unique properties of hyperspectral data, such as high\ndimensionality and feature inlining, pose challenges to the application of\nMamba to HSI classification. To compensate for these shortcomings, we propose\nan full-field interaction multi-groups Mamba framework (HS-Mamba), which adopts\na strategy different from pixel-patch based or whole-image based, but combines\nthe advantages of both. The patches cut from the whole image are sent to\nmulti-groups Mamba, combined with positional information to perceive local\ninline features in the spatial and spectral domains, and the whole image is\nsent to a lightweight attention module to enhance the global feature\nrepresentation ability. Specifically, HS-Mamba consists of a dual-channel\nspatial-spectral encoder (DCSS-encoder) module and a lightweight global inline\nattention (LGI-Att) branch. The DCSS-encoder module uses multiple groups of\nMamba to decouple and model the local features of dual-channel sequences with\nnon-overlapping patches. The LGI-Att branch uses a lightweight compressed and\nextended attention module to perceive the global features of the spatial and\nspectral domains of the unsegmented whole image. By fusing local and global\nfeatures, high-precision classification of hyperspectral images is achieved.\nExtensive experiments demonstrate the superiority of the proposed HS-Mamba,\noutperforming state-of-the-art methods on four benchmark HSI datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15640", "pdf": "https://arxiv.org/pdf/2504.15640", "abs": "https://arxiv.org/abs/2504.15640", "authors": ["Hongtao Wang", "Taiyan Zhang", "Renchi Yang", "Jianliang Xu"], "title": "Cost-Effective Text Clustering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text clustering aims to automatically partition a collection of text\ndocuments into distinct clusters based on linguistic features. In the\nliterature, this task is usually framed as metric clustering based on text\nembeddings from pre-trained encoders or a graph clustering problem upon\npairwise similarities from an oracle, e.g., a large ML model. Recently, large\nlanguage models (LLMs) bring significant advancement in this field by offering\ncontextualized text embeddings and highly accurate similarity scores, but\nmeanwhile, present grand challenges to cope with substantial computational\nand/or financial overhead caused by numerous API-based queries or inference\ncalls to the models.\n  In response, this paper proposes TECL, a cost-effective framework that taps\ninto the feedback from LLMs for accurate text clustering within a limited\nbudget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or\nTriangleLLM to construct must-link/cannot-link constraints for text pairs, and\nfurther leverages such constraints as supervision signals input to our weighted\nconstrained clustering approach to generate clusters. Particularly, EdgeLLM\n(resp. TriangleLLM) enables the identification of informative text pairs (resp.\ntriplets) for querying LLMs via well-thought-out greedy algorithms and accurate\nextraction of pairwise constraints through carefully-crafted prompting\ntechniques. Our experiments on multiple benchmark datasets exhibit that TECL\nconsistently and considerably outperforms existing solutions in unsupervised\ntext clustering under the same query cost for LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15624", "pdf": "https://arxiv.org/pdf/2504.15624", "abs": "https://arxiv.org/abs/2504.15624", "authors": ["Jingzhi Li", "Changjiang Luo", "Ruoyu Chen", "Hua Zhang", "Wenqi Ren", "Jianhou Gan", "Xiaochun Cao"], "title": "FaceInsight: A Multimodal Large Language Model for Face Perception", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nstrong capabilities in understanding general visual content. However, these\ngeneral-domain MLLMs perform poorly in face perception tasks, often producing\ninaccurate or misleading responses to face-specific queries. To address this\ngap, we propose FaceInsight, the versatile face perception MLLM that provides\nfine-grained facial information. Our approach introduces visual-textual\nalignment of facial knowledge to model both uncertain dependencies and\ndeterministic relationships among facial information, mitigating the\nlimitations of language-driven reasoning. Additionally, we incorporate face\nsegmentation maps as an auxiliary perceptual modality, enriching the visual\ninput with localized structural cues to enhance semantic understanding.\nComprehensive experiments and analyses across three face perception tasks\ndemonstrate that FaceInsight consistently outperforms nine compared MLLMs under\nboth training-free and fine-tuned settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15650", "pdf": "https://arxiv.org/pdf/2504.15650", "abs": "https://arxiv.org/abs/2504.15650", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Teli Ma", "Hengzhuang Li", "Yong liu", "Guang Dai", "Lei Zhang"], "title": "AffordanceSAM: Segment Anything Once More in Affordance Grounding", "categories": ["cs.CV"], "comment": "SAM Meets Affordance Grounding", "summary": "Improving the generalization ability of an affordance grounding model to\nrecognize regions for unseen objects and affordance functions is crucial for\nreal-world application. However, current models are still far away from such\nstandards. To address this problem, we introduce AffordanceSAM, an effective\napproach that extends SAM's generalization capacity to the domain of affordance\ngrounding. For the purpose of thoroughly transferring SAM's robust performance\nin segmentation to affordance, we initially propose an affordance-adaption\nmodule in order to help modify SAM's segmentation output to be adapted to the\nspecific functional regions required for affordance grounding. We concurrently\nmake a coarse-to-fine training recipe to make SAM first be aware of affordance\nobjects and actions coarsely, and then be able to generate affordance heatmaps\nfinely. Both quantitative and qualitative experiments show the strong\ngeneralization capacity of our AffordanceSAM, which not only surpasses previous\nmethods under AGD20K benchmark but also shows evidence to handle the task with\nnovel objects and affordance functions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15661", "pdf": "https://arxiv.org/pdf/2504.15661", "abs": "https://arxiv.org/abs/2504.15661", "authors": ["Xian Wu", "Chang Liu"], "title": "DiTPainter: Efficient Video Inpainting with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Many existing video inpainting algorithms utilize optical flows to construct\nthe corresponding maps and then propagate pixels from adjacent frames to\nmissing areas by mapping. Despite the effectiveness of the propagation\nmechanism, they might encounter blurry and inconsistencies when dealing with\ninaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT)\nhas emerged as a revolutionary technique for video generation tasks. However,\npretrained DiT models for video generation all contain a large amount of\nparameters, which makes it very time consuming to apply to video inpainting\ntasks. In this paper, we present DiTPainter, an end-to-end video inpainting\nmodel based on Diffusion Transformer (DiT). DiTPainter uses an efficient\ntransformer network designed for video inpainting, which is trained from\nscratch instead of initializing from any large pretrained models. DiTPainter\ncan address videos with arbitrary lengths and can be applied to video\ndecaptioning and video completion tasks with an acceptable time cost.\nExperiments show that DiTPainter outperforms existing video inpainting\nalgorithms with higher quality and better spatial-temporal consistency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15665", "pdf": "https://arxiv.org/pdf/2504.15665", "abs": "https://arxiv.org/abs/2504.15665", "authors": ["Pei Liu", "Yisi Luo", "Wenzhen Wang", "Xiangyong Cao"], "title": "Motion-Enhanced Nonlocal Similarity Implicit Neural Representation for Infrared Dim and Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared dim and small target detection presents a significant challenge due\nto dynamic multi-frame scenarios and weak target signatures in the infrared\nmodality. Traditional low-rank plus sparse models often fail to capture dynamic\nbackgrounds and global spatial-temporal correlations, which results in\nbackground leakage or target loss. In this paper, we propose a novel\nmotion-enhanced nonlocal similarity implicit neural representation (INR)\nframework to address these challenges. We first integrate motion estimation via\noptical flow to capture subtle target movements, and propose multi-frame fusion\nto enhance motion saliency. Second, we leverage nonlocal similarity to\nconstruct patch tensors with strong low-rank properties, and propose an\ninnovative tensor decomposition-based INR model to represent the nonlocal patch\ntensor, effectively encoding both the nonlocal low-rankness and\nspatial-temporal correlations of background through continuous neural\nrepresentations. An alternating direction method of multipliers is developed\nfor the nonlocal INR model, which enjoys theoretical fixed-point convergence.\nExperimental results show that our approach robustly separates dim targets from\ncomplex infrared backgrounds, outperforming state-of-the-art methods in\ndetection accuracy and robustness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15669", "pdf": "https://arxiv.org/pdf/2504.15669", "abs": "https://arxiv.org/abs/2504.15669", "authors": ["Wei Zhuo", "Zhiyue Tang", "Wufeng Xue", "Hao Ding", "Linlin Shen"], "title": "DINOv2-powered Few-Shot Semantic Segmentation: A Unified Framework via Cross-Model Distillation and 4D Correlation Mining", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot semantic segmentation has gained increasing interest due to its\ngeneralization capability, i.e., segmenting pixels of novel classes requiring\nonly a few annotated images. Prior work has focused on meta-learning for\nsupport-query matching, with extensive development in both prototype-based and\naggregation-based methods. To address data scarcity, recent approaches have\nturned to foundation models to enhance representation transferability for novel\nclass segmentation. Among them, a hybrid dual-modal framework including both\nDINOv2 and SAM has garnered attention due to their complementary capabilities.\nWe wonder \"can we build a unified model with knowledge from both foundation\nmodels?\" To this end, we propose FS-DINO, with only DINOv2's encoder and a\nlightweight segmenter. The segmenter features a bottleneck adapter, a\nmeta-visual prompt generator based on dense similarities and semantic\nembeddings, and a decoder. Through coarse-to-fine cross-model distillation, we\neffectively integrate SAM's knowledge into our lightweight segmenter, which can\nbe further enhanced by 4D correlation mining on support-query pairs. Extensive\nexperiments on COCO-20i, PASCAL-5i, and FSS-1000 demonstrate the effectiveness\nand superiority of our method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15694", "pdf": "https://arxiv.org/pdf/2504.15694", "abs": "https://arxiv.org/abs/2504.15694", "authors": ["Jun Dong", "Wenli Wu", "Jintao Cheng", "Xiaoyu Tang"], "title": "You Sense Only Once Beneath: Ultra-Light Real-Time Underwater Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable achievements in object detection, the model's accuracy\nand efficiency still require further improvement under challenging underwater\nconditions, such as low image quality and limited computational resources. To\naddress this, we propose an Ultra-Light Real-Time Underwater Object Detection\nframework, You Sense Only Once Beneath (YSOOB). Specifically, we utilize a\nMulti-Spectrum Wavelet Encoder (MSWE) to perform frequency-domain encoding on\nthe input image, minimizing the semantic loss caused by underwater optical\ncolor distortion. Furthermore, we revisit the unique characteristics of\neven-sized and transposed convolutions, allowing the model to dynamically\nselect and enhance key information during the resampling process, thereby\nimproving its generalization ability. Finally, we eliminate model redundancy\nthrough a simple yet effective channel compression and reconstructed large\nkernel convolution (RLKC) to achieve model lightweight. As a result, forms a\nhigh-performance underwater object detector YSOOB with only 1.2 million\nparameters. Extensive experimental results demonstrate that, with the fewest\nparameters, YSOOB achieves mAP50 of 83.1% and 82.9% on the URPC2020 and DUO\ndatasets, respectively, comparable to the current SOTA detectors. The inference\nspeed reaches 781.3 FPS and 57.8 FPS on the T4 GPU (TensorRT FP16) and the edge\ncomputing device Jetson Xavier NX (TensorRT FP16), surpassing YOLOv12-N by\n28.1% and 22.5%, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15895", "pdf": "https://arxiv.org/pdf/2504.15895", "abs": "https://arxiv.org/abs/2504.15895", "authors": ["Chenxu Yang", "Qingyi Si", "Yongjie Duan", "Zheliang Zhu", "Chenyu Zhu", "Zheng Lin", "Li Cao", "Weiping Wang"], "title": "Dynamic Early Exit in Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 11 figures", "summary": "Recent advances in large reasoning language models (LRLMs) rely on test-time\nscaling, which extends long chain-of-thought (CoT) generation to solve complex\ntasks. However, overthinking in long CoT not only slows down the efficiency of\nproblem solving, but also risks accuracy loss due to the extremely detailed or\nredundant reasoning steps. We propose a simple yet effective method that allows\nLLMs to self-truncate CoT sequences by early exit during generation. Instead of\nrelying on fixed heuristics, the proposed method monitors model behavior at\npotential reasoning transition points (e.g.,\"Wait\" tokens) and dynamically\nterminates the next reasoning chain's generation when the model exhibits high\nconfidence in a trial answer. Our method requires no additional training and\ncan be seamlessly integrated into existing o1-like reasoning LLMs. Experiments\non multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024\nshow that the proposed method is consistently effective on deepseek-series\nreasoning LLMs, reducing the length of CoT sequences by an average of 31% to\n43% while improving accuracy by 1.7% to 5.7%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "o1"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15728", "pdf": "https://arxiv.org/pdf/2504.15728", "abs": "https://arxiv.org/abs/2504.15728", "authors": ["Manjunath D", "Aniruddh Sikdar", "Prajwal Gurunath", "Sumanth Udupa", "Suresh Sundaram"], "title": "SAGA: Semantic-Aware Gray color Augmentation for Visible-to-Thermal Domain Adaptation across Multi-View Drone and Ground-Based Vision Systems", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W PBVS 2025", "summary": "Domain-adaptive thermal object detection plays a key role in facilitating\nvisible (RGB)-to-thermal (IR) adaptation by reducing the need for co-registered\nimage pairs and minimizing reliance on large annotated IR datasets. However,\ninherent limitations of IR images, such as the lack of color and texture cues,\npose challenges for RGB-trained models, leading to increased false positives\nand poor-quality pseudo-labels. To address this, we propose Semantic-Aware Gray\ncolor Augmentation (SAGA), a novel strategy for mitigating color bias and\nbridging the domain gap by extracting object-level features relevant to IR\nimages. Additionally, to validate the proposed SAGA for drone imagery, we\nintroduce the IndraEye, a multi-sensor (RGB-IR) dataset designed for diverse\napplications. The dataset contains 5,612 images with 145,666 instances,\ncaptured from diverse angles, altitudes, backgrounds, and times of day,\noffering valuable opportunities for multimodal learning, domain adaptation for\nobject detection and segmentation, and exploration of sensor-specific strengths\nand weaknesses. IndraEye aims to enhance the development of more robust and\naccurate aerial perception systems, especially in challenging environments.\nExperimental results show that SAGA significantly improves RGB-to-IR adaptation\nfor autonomous driving and IndraEye dataset, achieving consistent performance\ngains of +0.4% to +7.6% (mAP) when integrated with state-of-the-art domain\nadaptation techniques. The dataset and codes are available at\nhttps://github.com/airliisc/IndraEye.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15751", "pdf": "https://arxiv.org/pdf/2504.15751", "abs": "https://arxiv.org/abs/2504.15751", "authors": ["Menan Velayuthan", "Asiri Gawesha", "Purushoth Velayuthan", "Nuwan Kodagoda", "Dharshana Kasthurirathna", "Pradeepa Samarasinghe"], "title": "GADS: A Super Lightweight Model for Head Pose Estimation", "categories": ["cs.CV"], "comment": "16 pages, 5 tables, 10 figures, not submitted to any conference or\n  journal", "summary": "In human-computer interaction, head pose estimation profoundly influences\napplication functionality. Although utilizing facial landmarks is valuable for\nthis purpose, existing landmark-based methods prioritize precision over\nsimplicity and model size, limiting their deployment on edge devices and in\ncompute-poor environments. To bridge this gap, we propose \\textbf{Grouped\nAttention Deep Sets (GADS)}, a novel architecture based on the Deep Set\nframework. By grouping landmarks into regions and employing small Deep Set\nlayers, we reduce computational complexity. Our multihead attention mechanism\nextracts and combines inter-group information, resulting in a model that is\n$7.5\\times$ smaller and executes $25\\times$ faster than the current lightest\nstate-of-the-art model. Notably, our method achieves an impressive reduction,\nbeing $4321\\times$ smaller than the best-performing model. We introduce vanilla\nGADS and Hybrid-GADS (landmarks + RGB) and evaluate our models on three\nbenchmark datasets -- AFLW2000, BIWI, and 300W-LP. We envision our architecture\nas a robust baseline for resource-constrained head pose estimation methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15756", "pdf": "https://arxiv.org/pdf/2504.15756", "abs": "https://arxiv.org/abs/2504.15756", "authors": ["Qirui Yang", "Fangpu Zhang", "Yeying Jin", "Qihua Cheng", "Pengtao Jiang", "Huanjing Yue", "Jingyu Yang"], "title": "DSDNet: Raw Domain DemoirÃ©ing via Dual Color-Space Synergy", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "With the rapid advancement of mobile imaging, capturing screens using\nsmartphones has become a prevalent practice in distance learning and conference\nrecording. However, moir\\'e artifacts, caused by frequency aliasing between\ndisplay screens and camera sensors, are further amplified by the image signal\nprocessing pipeline, leading to severe visual degradation. Existing sRGB domain\ndemoir\\'eing methods struggle with irreversible information loss, while recent\ntwo-stage raw domain approaches suffer from information bottlenecks and\ninference inefficiency. To address these limitations, we propose a single-stage\nraw domain demoir\\'eing framework, Dual-Stream Demoir\\'eing Network (DSDNet),\nwhich leverages the synergy of raw and YCbCr images to remove moir\\'e while\npreserving luminance and color fidelity. Specifically, to guide luminance\ncorrection and moir\\'e removal, we design a raw-to-YCbCr mapping pipeline and\nintroduce the Synergic Attention with Dynamic Modulation (SADM) module. This\nmodule enriches the raw-to-sRGB conversion with cross-domain contextual\nfeatures. Furthermore, to better guide color fidelity, we develop a\nLuminance-Chrominance Adaptive Transformer (LCAT), which decouples luminance\nand chrominance representations. Extensive experiments demonstrate that DSDNet\noutperforms state-of-the-art methods in both visual quality and quantitative\nevaluation, and achieves an inference speed $\\mathrm{\\textbf{2.4x}}$ faster\nthan the second-best method, highlighting its practical advantages. We provide\nan anonymous online demo at https://xxxxxxxxdsdnet.github.io/DSDNet/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15987", "pdf": "https://arxiv.org/pdf/2504.15987", "abs": "https://arxiv.org/abs/2504.15987", "authors": ["Zhenkai Qin", "Dongze Wu", "Yuxin Liu", "Guifang Yang"], "title": "Few-shot Hate Speech Detection Based on the MindSpore Framework", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The proliferation of hate speech on social media poses a significant threat\nto online communities, requiring effective detection systems. While deep\nlearning models have shown promise, their performance often deteriorates in\nfew-shot or low-resource settings due to reliance on large annotated corpora.\nTo address this, we propose MS-FSLHate, a prompt-enhanced neural framework for\nfew-shot hate speech detection implemented on the MindSpore deep learning\nplatform. The model integrates learnable prompt embeddings, a CNN-BiLSTM\nbackbone with attention pooling, and synonym-based adversarial data\naugmentation to improve generalization. Experimental results on two benchmark\ndatasets-HateXplain and HSOL-demonstrate that our approach outperforms\ncompetitive baselines in precision, recall, and F1-score. Additionally, the\nframework shows high efficiency and scalability, suggesting its suitability for\ndeployment in resource-constrained environments. These findings highlight the\npotential of combining prompt-based learning with adversarial augmentation for\nrobust and adaptable hate speech detection in few-shot scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15770", "pdf": "https://arxiv.org/pdf/2504.15770", "abs": "https://arxiv.org/abs/2504.15770", "authors": ["Lei Xu", "Mehmet Yamac", "Mete Ahishali", "Moncef Gabbouj"], "title": "Multi-Scale Tensorial Summation and Dimensional Reduction Guided Neural Network for Edge Detection", "categories": ["cs.CV"], "comment": null, "summary": "Edge detection has attracted considerable attention thanks to its exceptional\nability to enhance performance in downstream computer vision tasks. In recent\nyears, various deep learning methods have been explored for edge detection\ntasks resulting in a significant performance improvement compared to\nconventional computer vision algorithms. In neural networks, edge detection\ntasks require considerably large receptive fields to provide satisfactory\nperformance. In a typical convolutional operation, such a large receptive field\ncan be achieved by utilizing a significant number of consecutive layers, which\nyields deep network structures. Recently, a Multi-scale Tensorial Summation\n(MTS) factorization operator was presented, which can achieve very large\nreceptive fields even from the initial layers. In this paper, we propose a\nnovel MTS Dimensional Reduction (MTS-DR) module guided neural network,\nMTS-DR-Net, for the edge detection task. The MTS-DR-Net uses MTS layers, and\ncorresponding MTS-DR blocks as a new backbone to remove redundant information\ninitially. Such a dimensional reduction module enables the neural network to\nfocus specifically on relevant information (i.e., necessary subspaces).\nFinally, a weight U-shaped refinement module follows MTS-DR blocks in the\nMTS-DR-Net. We conducted extensive experiments on two benchmark edge detection\ndatasets: BSDS500 and BIPEDv2 to verify the effectiveness of our model. The\nimplementation of the proposed MTS-DR-Net can be found at\nhttps://github.com/LeiXuAI/MTS-DR-Net.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15448", "pdf": "https://arxiv.org/pdf/2504.15448", "abs": "https://arxiv.org/abs/2504.15448", "authors": ["Yanampally Abhiram Reddy", "Siddhi Agarwal", "Vikram Parashar", "Arshiya Arora"], "title": "Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": "19 pages, 2 figures", "summary": "In the age of social media, understanding public sentiment toward major\ncorporations is crucial for investors, policymakers, and researchers. This\npaper presents a comprehensive sentiment analysis system tailored for corporate\nreputation monitoring, combining Natural Language Processing (NLP) and machine\nlearning techniques to accurately interpret public opinion in real time. The\nmethodology integrates a hybrid sentiment detection framework leveraging both\nrule-based models (VADER) and transformer-based deep learning models\n(DistilBERT), applied to social media data from multiple platforms. The system\nbegins with robust preprocessing involving noise removal and text\nnormalization, followed by sentiment classification using an ensemble approach\nto ensure both interpretability and contextual accuracy. Results are visualized\nthrough sentiment distribution plots, comparative analyses, and temporal\nsentiment trends for enhanced interpretability. Our analysis reveals\nsignificant disparities in public sentiment across major corporations, with\ncompanies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment\nscores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment\nprofiles. These findings demonstrate the utility of our multi-source sentiment\nframework in providing actionable insights regarding corporate public\nperception, enabling stakeholders to make informed strategic decisions based on\ncomprehensive sentiment analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15929", "pdf": "https://arxiv.org/pdf/2504.15929", "abs": "https://arxiv.org/abs/2504.15929", "authors": ["Saban Ozturk", "Melih B. Yilmaz", "Muti Kara", "M. Talat Yavuz", "Aykut KoÃ§", "Tolga Ãukur"], "title": "Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Diagnostic imaging relies on interpreting both images and radiology reports,\nbut the growing data volumes place significant pressure on medical experts,\nyielding increased errors and workflow backlogs. Medical vision-language models\n(med-VLMs) have emerged as a powerful framework to efficiently process\nmultimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit\ntheir performance hinges on how well image and text representations are\naligned. Existing alignment methods, predominantly based on contrastive\nlearning, prioritize separation between disease classes over segregation of\nfine-grained pathology attributes like location, size or severity, leading to\nsuboptimal representations. Here, we propose MedTrim (Meta-entity-driven\nTriplet mining), a novel method that enhances image-text alignment through\nmultimodal triplet learning synergistically guided by disease class as well as\nadjectival and directional pathology descriptors. Unlike common alignment\nmethods that separate broad disease classes, MedTrim leverages structured\nmeta-entity information to preserve subtle but clinically significant\nintra-class variations. For this purpose, we first introduce an ontology-based\nentity recognition module that extracts pathology-specific meta-entities from\nCXR reports, as annotations on pathology attributes are rare in public\ndatasets. For refined sample selection in triplet mining, we then introduce a\nnovel score function that captures an aggregate measure of inter-sample\nsimilarity based on disease classes and adjectival/directional descriptors.\nLastly, we introduce a multimodal triplet alignment objective for explicit\nwithin- and cross-modal alignment between samples sharing detailed pathology\ncharacteristics. Our demonstrations indicate that MedTrim improves performance\nin downstream retrieval and classification tasks compared to state-of-the-art\nalignment methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15485", "pdf": "https://arxiv.org/pdf/2504.15485", "abs": "https://arxiv.org/abs/2504.15485", "authors": ["Atin Pothiraj", "Elias Stengel-Eskin", "Jaemin Cho", "Mohit Bansal"], "title": "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code and data: https://github.com/atinpothiraj/CAPTURe", "summary": "Recognizing and reasoning about occluded (partially or fully hidden) objects\nis vital to understanding visual scenes, as occlusions frequently occur in\nreal-world environments and act as obstacles for spatial comprehension. To test\nmodels' ability to reason about multiple occluded objects, we introduce a novel\ntask, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which\nrequires a model to count objects arranged in a pattern by inferring how the\npattern continues behind an occluder (an object which blocks parts of the\nscene). CAPTURe requires both recognizing visual patterns and reasoning, making\nit a useful testbed for evaluating vision-language models (VLMs) on whether\nthey understand occluded patterns and possess spatial understanding skills. By\nrequiring models to reason about occluded objects, CAPTURe also tests VLMs'\nability to form world models that would allow them to fill in missing\ninformation. CAPTURe consists of two parts: (1) CAPTURe-real, with manually\nfiltered images of real objects in patterns and (2) CAPTURe-synthetic, a\ncontrolled diagnostic with generated patterned images. We evaluate four strong\nVLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models\nstruggle to count on both occluded and unoccluded patterns. Crucially, we find\nthat models perform worse with occlusion, suggesting that VLMs are also\ndeficient in inferring unseen spatial relationships: even the strongest VLMs\nlike GPT-4o fail to count with occlusion. In contrast, we find that humans\nachieve very little error on CAPTURe. We also find that providing auxiliary\ninformation of occluded object locations increases performance, underscoring\nthat the model error comes both from an inability to handle occlusion as well\nas difficulty counting in images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["testbed"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15585", "pdf": "https://arxiv.org/pdf/2504.15585", "abs": "https://arxiv.org/abs/2504.15585", "authors": ["Kun Wang", "Guibin Zhang", "Zhenhong Zhou", "Jiahao Wu", "Miao Yu", "Shiqian Zhao", "Chenlong Yin", "Jinhu Fu", "Yibo Yan", "Hanjun Luo", "Liang Lin", "Zhihao Xu", "Haolang Lu", "Xinye Cao", "Xinyun Zhou", "Weifei Jin", "Fanci Meng", "Junyuan Mao", "Hao Wu", "Minghe Wang", "Fan Zhang", "Junfeng Fang", "Chengwei Liu", "Yifan Zhang", "Qiankun Li", "Chongye Guo", "Yalan Qin", "Yi Ding", "Donghai Hong", "Jiaming Ji", "Xinfeng Li", "Yifan Jiang", "Dongxia Wang", "Yihao Huang", "Yufei Guo", "Jen-tse Huang", "Yanwei Yue", "Wenke Huang", "Guancheng Wan", "Tianlin Li", "Lei Bai", "Jie Zhang", "Qing Guo", "Jingyi Wang", "Tianlong Chen", "Joey Tianyi Zhou", "Xiaojun Jia", "Weisong Sun", "Cong Wu", "Jing Chen", "Xuming Hu", "Yiming Li", "Xiao Wang", "Ningyu Zhang", "Luu Anh Tuan", "Guowen Xu", "Tianwei Zhang", "Xingjun Ma", "Xiang Wang", "Bo An", "Jun Sun", "Mohit Bansal", "Shirui Pan", "Yuval Elovici", "Bhavya Kailkhura", "Bo Li", "Yaodong Yang", "Hongwei Li", "Wenyuan Xu", "Yizhou Sun", "Wei Wang", "Qing Li", "Ke Tang", "Yu-Gang Jiang", "Felix Juefei-Xu", "Hui Xiong", "Xiaofeng Wang", "Shuicheng Yan", "Dacheng Tao", "Philip S. Yu", "Qingsong Wen", "Yang Liu"], "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a\npromising pathway toward achieving Artificial General Intelligence for both\nacademic and industrial communities, owing to their unprecedented performance\nacross various applications. As LLMs continue to gain prominence in both\nresearch and commercial domains, their security and safety implications have\nbecome a growing concern, not only for researchers and corporations but also\nfor every nation. Currently, existing surveys on LLM safety primarily focus on\nspecific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning\nphase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs.\nTo address this gap, this paper introduces, for the first time, the concept of\n\"full-stack\" safety to systematically consider safety issues throughout the\nentire process of LLM training, deployment, and eventual commercialization.\nCompared to the off-the-shelf LLM safety surveys, our work demonstrates several\ndistinctive advantages: (I) Comprehensive Perspective. We define the complete\nLLM lifecycle as encompassing data preparation, pre-training, post-training,\ndeployment and final commercialization. To our knowledge, this represents the\nfirst safety survey to encompass the entire lifecycle of LLMs. (II) Extensive\nLiterature Support. Our research is grounded in an exhaustive review of over\n800+ papers, ensuring comprehensive coverage and systematic organization of\nsecurity issues within a more holistic understanding. (III) Unique Insights.\nThrough systematic literature analysis, we have developed reliable roadmaps and\nperspectives for each chapter. Our work identifies promising research\ndirections, including safety in data generation, alignment techniques, model\nediting, and LLM-based agent systems. These insights provide valuable guidance\nfor researchers pursuing future work in this field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15932", "pdf": "https://arxiv.org/pdf/2504.15932", "abs": "https://arxiv.org/abs/2504.15932", "authors": ["Wang Lin", "Liyu Jia", "Wentao Hu", "Kaihang Pan", "Zhongqi Yue", "Wei Zhao", "Jingyuan Chen", "Fei Wu", "Hanwang Zhang"], "title": "Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, producing videos that adhere to\nphysical laws remains a significant challenge. Traditional diffusion-based\nmethods struggle to extrapolate to unseen physical conditions (eg, velocity)\ndue to their reliance on data-driven approximations. To address this, we\npropose to integrate symbolic reasoning and reinforcement learning to enforce\nphysical consistency in video generation. We first introduce the Diffusion\nTimestep Tokenizer (DDT), which learns discrete, recursive visual tokens by\nrecovering visual attributes lost during the diffusion process. The recursive\nvisual tokens enable symbolic reasoning by a large language model. Based on it,\nwe propose the Phys-AR framework, which consists of two stages: The first stage\nuses supervised fine-tuning to transfer symbolic knowledge, while the second\nstage applies reinforcement learning to optimize the model's reasoning\nabilities through reward functions based on physical conditions. Our approach\nallows the model to dynamically adjust and improve the physical properties of\ngenerated videos, ensuring adherence to physical laws. Experimental results\ndemonstrate that PhysAR can generate videos that are physically consistent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15958", "pdf": "https://arxiv.org/pdf/2504.15958", "abs": "https://arxiv.org/abs/2504.15958", "authors": ["Zebin Yao", "Lei Ren", "Huixing Jiang", "Chen Wei", "Xiaojie Wang", "Ruifan Li", "Fangxiang Feng"], "title": "FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven image generation aims to synthesize novel scenes that\nfaithfully preserve subject identity from reference images while adhering to\ntextual guidance, yet existing methods struggle with a critical trade-off\nbetween fidelity and efficiency. Tuning-based approaches rely on time-consuming\nand resource-intensive subject-specific optimization, while zero-shot methods\nfail to maintain adequate subject consistency. In this work, we propose\nFreeGraftor, a training-free framework that addresses these limitations through\ncross-image feature grafting. Specifically, FreeGraftor employs semantic\nmatching and position-constrained attention fusion to transfer visual details\nfrom reference subjects to the generated image. Additionally, our framework\nincorporates a novel noise initialization strategy to preserve geometry priors\nof reference subjects for robust feature matching. Extensive qualitative and\nquantitative experiments demonstrate that our method enables precise subject\nidentity transfer while maintaining text-aligned scene synthesis. Without\nrequiring model fine-tuning or additional training, FreeGraftor significantly\noutperforms existing zero-shot and training-free approaches in both subject\nfidelity and text alignment. Furthermore, our framework can seamlessly extend\nto multi-subject generation, making it practical for real-world deployment. Our\ncode is available at https://github.com/Nihukat/FreeGraftor.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16000", "pdf": "https://arxiv.org/pdf/2504.16000", "abs": "https://arxiv.org/abs/2504.16000", "authors": ["Soham Bonnerjee", "Zhen Wei", "Yeon", "Anna Asch", "Sagnik Nandy", "Promit Ghosal"], "title": "How Private is Your Attention? Bridging Privacy with In-Context Learning", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "In-context learning (ICL)-the ability of transformer-based models to perform\nnew tasks from examples provided at inference time-has emerged as a hallmark of\nmodern language models. While recent works have investigated the mechanisms\nunderlying ICL, its feasibility under formal privacy constraints remains\nlargely unexplored. In this paper, we propose a differentially private\npretraining algorithm for linear attention heads and present the first\ntheoretical analysis of the privacy-accuracy trade-off for ICL in linear\nregression. Our results characterize the fundamental tension between\noptimization and privacy-induced noise, formally capturing behaviors observed\nin private training via iterative methods. Additionally, we show that our\nmethod is robust to adversarial perturbations of training prompts, unlike\nstandard ridge regression. All theoretical findings are supported by extensive\nsimulations across diverse settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16047", "pdf": "https://arxiv.org/pdf/2504.16047", "abs": "https://arxiv.org/abs/2504.16047", "authors": ["Frank Li", "Hari Trivedi", "Bardia Khosravi", "Theo Dapamede", "Mohammadreza Chavoshi", "Abdulhameed Dere", "Rohan Satya Isaac", "Aawez Mansuri", "Janice Newsome", "Saptarshi Purkayastha", "Judy Gichoya"], "title": "Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models, trained on vast amounts of data using self-supervised\ntechniques, have emerged as a promising frontier for advancing artificial\nintelligence (AI) applications in medicine. This study evaluates three\ndifferent vision-language foundation models (RAD-DINO, CheXagent, and\nBiomedCLIP) on their ability to capture fine-grained imaging features for\nradiology tasks. The models were assessed across classification, segmentation,\nand regression tasks for pneumothorax and cardiomegaly on chest radiographs.\nSelf-supervised RAD-DINO consistently excelled in segmentation tasks, while\ntext-supervised CheXagent demonstrated superior classification performance.\nBiomedCLIP showed inconsistent performance across tasks. A custom segmentation\nmodel that integrates global and local features substantially improved\nperformance for all foundation models, particularly for challenging\npneumothorax segmentation. The findings highlight that pre-training methodology\nsignificantly influences model performance on specific downstream tasks. For\nfine-grained segmentation tasks, models trained without text supervision\nperformed better, while text-supervised models offered advantages in\nclassification and interpretability. These insights provide guidance for\nselecting foundation models based on specific clinical applications in\nradiology.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16072", "pdf": "https://arxiv.org/pdf/2504.16072", "abs": "https://arxiv.org/abs/2504.16072", "authors": ["Long Lian", "Yifan Ding", "Yunhao Ge", "Sifei Liu", "Hanzi Mao", "Boyi Li", "Marco Pavone", "Ming-Yu Liu", "Trevor Darrell", "Adam Yala", "Yin Cui"], "title": "Describe Anything: Detailed Localized Image and Video Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://describe-anything.github.io/", "summary": "Generating detailed and accurate descriptions for specific regions in images\nand videos remains a fundamental challenge for vision-language models. We\nintroduce the Describe Anything Model (DAM), a model designed for detailed\nlocalized captioning (DLC). DAM preserves both local details and global context\nthrough two key innovations: a focal prompt, which ensures high-resolution\nencoding of targeted regions, and a localized vision backbone, which integrates\nprecise localization with its broader context. To tackle the scarcity of\nhigh-quality DLC data, we propose a Semi-supervised learning (SSL)-based Data\nPipeline (DLC-SDP). DLC-SDP starts with existing segmentation datasets and\nexpands to unlabeled web images using SSL. We introduce DLC-Bench, a benchmark\ndesigned to evaluate DLC without relying on reference captions. DAM sets new\nstate-of-the-art on 7 benchmarks spanning keyword-level, phrase-level, and\ndetailed multi-sentence localized image and video captioning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16082", "pdf": "https://arxiv.org/pdf/2504.16082", "abs": "https://arxiv.org/abs/2504.16082", "authors": ["Ziqi Pang", "Yu-Xiong Wang"], "title": "MR. Video: \"MapReduce\" is the Principle for Long Video Understanding", "categories": ["cs.CV"], "comment": "Preprint", "summary": "We propose MR. Video, an agentic long video understanding framework that\ndemonstrates the simple yet effective MapReduce principle for processing long\nvideos: (1) Map: independently and densely perceiving short video clips, and\n(2) Reduce: jointly aggregating information from all clips. Compared with\nsequence-to-sequence vision-language models (VLMs), MR. Video performs detailed\nshort video perception without being limited by context length. Compared with\nexisting video agents that typically rely on sequential key segment selection,\nthe Map operation enables simpler and more scalable sequence parallel\nperception of short video segments. Its Reduce step allows for more\ncomprehensive context aggregation and reasoning, surpassing explicit key\nsegment retrieval. This MapReduce principle is applicable to both VLMs and\nvideo agents, and we use LLM agents to validate its effectiveness.\n  In practice, MR. Video employs two MapReduce stages: (A) Captioning:\ngenerating captions for short video clips (map), then standardizing repeated\ncharacters and objects into shared names (reduce); (B) Analysis: for each user\nquestion, analyzing relevant information from individual short videos (map),\nand integrating them into a final answer (reduce). MR. Video achieves over 10%\naccuracy improvement on the challenging LVBench compared to state-of-the-art\nVLMs and video agents.\n  Code is available at: https://github.com/ziqipang/MR-Video", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16083", "pdf": "https://arxiv.org/pdf/2504.16083", "abs": "https://arxiv.org/abs/2504.16083", "authors": ["Yucheng Li", "Huiqiang Jiang", "Chengruidong Zhang", "Qianhui Wu", "Xufang Luo", "Surin Ahn", "Amir H. Abdi", "Dongsheng Li", "Jianfeng Gao", "Yuqing Yang", "Lili Qiu"], "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The integration of long-context capabilities with visual understanding\nunlocks unprecedented potential for Vision Language Models (VLMs). However, the\nquadratic attention complexity during the pre-filling phase remains a\nsignificant obstacle to real-world deployment. To overcome this limitation, we\nintroduce MMInference (Multimodality Million tokens Inference), a dynamic\nsparse attention method that accelerates the prefilling stage for long-context\nmulti-modal inputs. First, our analysis reveals that the temporal and spatial\nlocality of video input leads to a unique sparse pattern, the Grid pattern.\nSimultaneously, VLMs exhibit markedly different sparse distributions across\ndifferent modalities. We introduce a permutation-based method to leverage the\nunique Grid pattern and handle modality boundary issues. By offline search the\noptimal sparse patterns for each head, MMInference constructs the sparse\ndistribution dynamically based on the input. We also provide optimized GPU\nkernels for efficient sparse computations. Notably, MMInference integrates\nseamlessly into existing VLM pipelines without any model modifications or\nfine-tuning. Experiments on multi-modal benchmarks-including Video QA,\nCaptioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art\nlong-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that\nMMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while\nmaintaining accuracy. Our code is available at https://aka.ms/MMInference.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15323", "pdf": "https://arxiv.org/pdf/2504.15323", "abs": "https://arxiv.org/abs/2504.15323", "authors": ["Donggyun Kim", "Chanwoo Kim", "Seunghoon Hong"], "title": "HyperFlow: Gradient-Free Emulation of Few-Shot Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "While test-time fine-tuning is beneficial in few-shot learning, the need for\nmultiple backpropagation steps can be prohibitively expensive in real-time or\nlow-resource scenarios. To address this limitation, we propose an approach that\nemulates gradient descent without computing gradients, enabling efficient\ntest-time adaptation. Specifically, we formulate gradient descent as an Euler\ndiscretization of an ordinary differential equation (ODE) and train an\nauxiliary network to predict the task-conditional drift using only the few-shot\nsupport set. The adaptation then reduces to a simple numerical integration\n(e.g., via the Euler method), which requires only a few forward passes of the\nauxiliary network -- no gradients or forward passes of the target model are\nneeded. In experiments on cross-domain few-shot classification using the\nMeta-Dataset and CDFSL benchmarks, our method significantly improves\nout-of-domain performance over the non-fine-tuned baseline while incurring only\n6\\% of the memory cost and 0.02\\% of the computation time of standard\nfine-tuning, thus establishing a practical middle ground between direct\ntransfer and fully fine-tuned approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation", "test-time fine-tuning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15329", "pdf": "https://arxiv.org/pdf/2504.15329", "abs": "https://arxiv.org/abs/2504.15329", "authors": ["Yike Zhang", "Eduardo Davalos", "Jack Noble"], "title": "Vision6D: 3D-to-2D Interactive Visualization and Annotation Tool for 6D Pose Estimation", "categories": ["cs.GR", "cs.CV", "cs.HC", "cs.RO"], "comment": null, "summary": "Accurate 6D pose estimation has gained more attention over the years for\nrobotics-assisted tasks that require precise interaction with physical objects.\nThis paper presents an interactive 3D-to-2D visualization and annotation tool\nto support the 6D pose estimation research community. To the best of our\nknowledge, the proposed work is the first tool that allows users to visualize\nand manipulate 3D objects interactively on a 2D real-world scene, along with a\ncomprehensive user study. This system supports robust 6D camera pose annotation\nby providing both visual cues and spatial relationships to determine object\nposition and orientation in various environments. The annotation feature in\nVision6D is particularly helpful in scenarios where the transformation matrix\nbetween the camera and world objects is unknown, as it enables accurate\nannotation of these objects' poses using only the camera intrinsic matrix. This\ncapability serves as a foundational step in developing and training advanced\npose estimation models across various domains. We evaluate Vision6D's\neffectiveness by utilizing widely-used open-source pose estimation datasets\nLinemod and HANDAL through comparisons between the default ground-truth camera\nposes with manual annotations. A user study was performed to show that Vision6D\ngenerates accurate pose annotations via visual cues in an intuitive 3D user\ninterface. This approach aims to bridge the gap between 2D scene projections\nand 3D scenes, offering an effective way for researchers and developers to\nsolve 6D pose annotation related problems. The software is open-source and\npublicly available at https://github.com/InteractiveGL/vision6D.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15479", "pdf": "https://arxiv.org/pdf/2504.15479", "abs": "https://arxiv.org/abs/2504.15479", "authors": ["Jeremy Goldwasser", "Giles Hooker"], "title": "Unifying Image Counterfactuals and Feature Attributions with Latent-Space Adversarial Attacks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Counterfactuals are a popular framework for interpreting machine learning\npredictions. These what if explanations are notoriously challenging to create\nfor computer vision models: standard gradient-based methods are prone to\nproduce adversarial examples, in which imperceptible modifications to image\npixels provoke large changes in predictions. We introduce a new,\neasy-to-implement framework for counterfactual images that can flexibly adapt\nto contemporary advances in generative modeling. Our method, Counterfactual\nAttacks, resembles an adversarial attack on the representation of the image\nalong a low-dimensional manifold. In addition, given an auxiliary dataset of\nimage descriptors, we show how to accompany counterfactuals with feature\nattribution that quantify the changes between the original and counterfactual\nimages. These importance scores can be aggregated into global counterfactual\nexplanations that highlight the overall features driving model predictions.\nWhile this unification is possible for any counterfactual method, it has\nparticular computational efficiency for ours. We demonstrate the efficacy of\nour approach with the MNIST and CelebA datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15496", "pdf": "https://arxiv.org/pdf/2504.15496", "abs": "https://arxiv.org/abs/2504.15496", "authors": ["Eammon A. Littler", "Emmanuel A. Mannoh", "Ethan P. M. LaRochelle"], "title": "Fluorescence Reference Target Quantitative Analysis Library", "categories": ["physics.med-ph", "cs.CV", "eess.IV", "q-bio.QM"], "comment": "12 pages, 1 table, 4 figures. Code available:\n  https://github.com/QUEL-Imaging/quel-qal), PyPi: quel-qal", "summary": "Standardized performance evaluation of fluorescence imaging systems remains a\ncritical unmet need in the field of fluorescence-guided surgery (FGS). While\nthe American Association of Physicists in Medicine (AAPM) TG311 report and\nrecent FDA draft guidance provide recommended metrics for system\ncharacterization, practical tools for extracting these metrics remain limited,\ninconsistent, and often inaccessible. We present QUEL-QAL, an open-source\nPython library designed to streamline and standardize the quantitative analysis\nof fluorescence images using solid reference targets. The library provides a\nmodular, reproducible workflow that includes region of interest (ROI)\ndetection, statistical analysis, and visualization capabilities. QUEL-QAL\nsupports key metrics such as response linearity, limit of detection, depth\nsensitivity, and spatial resolution, in alignment with regulatory and academic\nguidance. Built on widely adopted Python packages, the library is designed to\nbe extensible, enabling users to adapt it to novel target designs and analysis\nprotocols. By promoting transparency, reproducibility, and regulatory\nalignment, QUEL-QAL offers a foundational tool to support standardized\nbenchmarking and accelerate the development and evaluation of fluorescence\nimaging systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15545", "pdf": "https://arxiv.org/pdf/2504.15545", "abs": "https://arxiv.org/abs/2504.15545", "authors": ["Zizhi Chen", "Xinyu Zhang", "Minghao Han", "Yizhou Liu", "Ziyun Qian", "Weifeng Zhang", "Xukun Zhang", "Jingwei Wei", "Lihua Zhang"], "title": "VLM-based Prompts as the Optimal Assistant for Unpaired Histopathology Virtual Staining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In histopathology, tissue sections are typically stained using common H&E\nstaining or special stains (MAS, PAS, PASM, etc.) to clearly visualize specific\ntissue structures. The rapid advancement of deep learning offers an effective\nsolution for generating virtually stained images, significantly reducing the\ntime and labor costs associated with traditional histochemical staining.\nHowever, a new challenge arises in separating the fundamental visual\ncharacteristics of tissue sections from the visual differences induced by\nstaining agents. Additionally, virtual staining often overlooks essential\npathological knowledge and the physical properties of staining, resulting in\nonly style-level transfer. To address these issues, we introduce, for the first\ntime in virtual staining tasks, a pathological vision-language large model\n(VLM) as an auxiliary tool. We integrate contrastive learnable prompts,\nfoundational concept anchors for tissue sections, and staining-specific concept\nanchors to leverage the extensive knowledge of the pathological VLM. This\napproach is designed to describe, frame, and enhance the direction of virtual\nstaining. Furthermore, we have developed a data augmentation method based on\nthe constraints of the VLM. This method utilizes the VLM's powerful image\ninterpretation capabilities to further integrate image style and structural\ninformation, proving beneficial in high-precision pathological diagnostics.\nExtensive evaluations on publicly available multi-domain unpaired staining\ndatasets demonstrate that our method can generate highly realistic images and\nenhance the accuracy of downstream tasks, such as glomerular detection and\nsegmentation. Our code is available at:\nhttps://github.com/CZZZZZZZZZZZZZZZZZ/VPGAN-HARBOR", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15562", "pdf": "https://arxiv.org/pdf/2504.15562", "abs": "https://arxiv.org/abs/2504.15562", "authors": ["Dip Roy"], "title": "Bayesian Autoencoder for Medical Anomaly Detection: Uncertainty-Aware Approach for Brain 2 MRI Analysis", "categories": ["cs.LG", "cs.CV"], "comment": "16 pages, 6 figures", "summary": "In medical imaging, anomaly detection is a vital element of healthcare\ndiagnostics, especially for neurological conditions which can be\nlife-threatening. Conventional deterministic methods often fall short when it\ncomes to capturing the inherent uncertainty of anomaly detection tasks. This\npaper introduces a Bayesian Variational Autoencoder (VAE) equipped with\nmulti-head attention mechanisms for detecting anomalies in brain magnetic\nresonance imaging (MRI). For the purpose of improving anomaly detection\nperformance, we incorporate both epistemic and aleatoric uncertainty estimation\nthrough Bayesian inference. The model was tested on the BraTS2020 dataset, and\nthe findings were a 0.83 ROC AUC and a 0.83 PR AUC. The data in our paper\nsuggests that modeling uncertainty is an essential component of anomaly\ndetection, enhancing both performance and interpretability and providing\nconfidence estimates, as well as anomaly predictions, for clinicians to\nleverage in making medical decisions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.15975", "pdf": "https://arxiv.org/pdf/2504.15975", "abs": "https://arxiv.org/abs/2504.15975", "authors": ["Peter Fletcher"], "title": "A New Graph Grammar Formalism for Robust Syntactic Pattern Recognition", "categories": ["cs.FL", "cs.CV", "F.4.2; F.4.3"], "comment": "64 pages, 23 figures", "summary": "I introduce a formalism for representing the syntax of recursively structured\ngraph-like patterns. It does not use production rules, like a conventional\ngraph grammar, but represents the syntactic structure in a more direct and\ndeclarative way. The grammar and the pattern are both represented as networks,\nand parsing is seen as the construction of a homomorphism from the pattern to\nthe grammar. The grammars can represent iterative, hierarchical and nested\nrecursive structure in more than one dimension.\n  This supports a highly parallel style of parsing, in which all aspects of\npattern recognition (feature detection, segmentation, parsing, filling in\nmissing symbols, top-down and bottom-up inference) are integrated into a single\nprocess, to exploit the synergy between them.\n  The emphasis of this paper is on underlying theoretical issues, but I also\ngive some example runs to illustrate the error-tolerant parsing of complex\nrecursively structured patterns of 50-1000 symbols, involving variability in\ngeometric relationships, blurry and indistinct symbols, overlapping symbols,\ncluttered images, and erased patches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
{"id": "2504.16062", "pdf": "https://arxiv.org/pdf/2504.16062", "abs": "https://arxiv.org/abs/2504.16062", "authors": ["Hardik Shah", "Jiaxu Xing", "Nico Messikommer", "Boyang Sun", "Marc Pollefeys", "Davide Scaramuzza"], "title": "ForesightNav: Learning Scene Imagination for Efficient Exploration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding how humans leverage prior knowledge to navigate unseen\nenvironments while making exploratory decisions is essential for developing\nautonomous robots with similar abilities. In this work, we propose\nForesightNav, a novel exploration strategy inspired by human imagination and\nreasoning. Our approach equips robotic agents with the capability to predict\ncontextual information, such as occupancy and semantic details, for unexplored\nregions. These predictions enable the robot to efficiently select meaningful\nlong-term navigation goals, significantly enhancing exploration in unseen\nenvironments. We validate our imagination-based approach using the Structured3D\ndataset, demonstrating accurate occupancy prediction and superior performance\nin anticipating unseen scene geometry. Our experiments show that the\nimagination module improves exploration efficiency in unseen environments,\nachieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav\non the Structured3D Validation split. These contributions demonstrate the power\nof imagination-driven reasoning for autonomous systems to enhance generalizable\nand efficient exploration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-23.jsonl"}
