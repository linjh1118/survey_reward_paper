{"id": "2504.20771", "pdf": "https://arxiv.org/pdf/2504.20771", "abs": "https://arxiv.org/abs/2504.20771", "authors": ["Haitao Wu", "Zongbo Han", "Huaxi Huang", "Changqing Zhang"], "title": "Turing Machine Evaluation for Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid development and widespread application of Large Language\nModels (LLMs), rigorous evaluation has become particularly crucial. This\nresearch adopts a novel perspective, focusing on evaluating the core\ncomputational reasoning ability of LLMs, defined as the capacity of model to\naccurately understand rules, and execute logically computing operations. This\ncapability assesses the reliability of LLMs as precise executors, and is\ncritical to advanced tasks such as complex code generation and multi-step\nproblem-solving. We propose an evaluation framework based on Universal Turing\nMachine (UTM) simulation. This framework requires LLMs to strictly follow\ninstructions and track dynamic states, such as tape content and read/write head\nposition, during multi-step computations. To enable standardized evaluation, we\ndeveloped TMBench, a benchmark for systematically studying the computational\nreasoning capabilities of LLMs. TMBench provides several key advantages,\nincluding knowledge-agnostic evaluation, adjustable difficulty, foundational\ncoverage through Turing machine encoding, and unlimited capacity for instance\ngeneration, ensuring scalability as models continue to evolve. We find that\nmodel performance on TMBench correlates strongly with performance on other\nrecognized reasoning benchmarks (Pearson correlation coefficient is 0.73),\nclearly demonstrating that computational reasoning is a significant dimension\nfor measuring the deep capabilities of LLMs. Code and data are available at\nhttps://github.com/HaitaoWuTJU/Turing-Machine-Bench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation", "reliability", "code generation", "dimension"], "score": 6}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20234", "pdf": "https://arxiv.org/pdf/2504.20234", "abs": "https://arxiv.org/abs/2504.20234", "authors": ["Bartosz Ptak", "Marek Kraft"], "title": "Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint submitted to the Expert Systems with Applications journal", "summary": "Drone-based crowd monitoring is the key technology for applications in\nsurveillance, public safety, and event management. However, maintaining\ntracking continuity and consistency remains a significant challenge.\nTraditional detection-assignment tracking methods struggle with false\npositives, false negatives, and frequent identity switches, leading to degraded\ncounting accuracy and making in-depth analysis impossible. This paper\nintroduces a point-oriented online tracking algorithm that improves trajectory\ncontinuity and counting reliability in drone-based crowd monitoring. Our method\nbuilds on the Simple Online and Real-time Tracking (SORT) framework, replacing\nthe original bounding-box assignment with a point-distance metric. The\nalgorithm is enhanced with three cost-effective techniques: camera motion\ncompensation, altitude-aware assignment, and classification-based trajectory\nvalidation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use\nspatial feature maps from localisation algorithms for increased computational\nefficiency through neural network resource sharing are integrated to refine\nobject tracking by reducing noise and handling missed detections. The proposed\nmethod is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,\ndemonstrating substantial improvements in tracking metrics, reducing counting\nerrors to 23% and 15%, respectively. The results also indicate a significant\nreduction of identity switches while maintaining high tracking accuracy,\noutperforming baseline online trackers and even an offline greedy optimisation\nmethod.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "correlation", "consistency", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "factuality", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "factuality", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20384", "pdf": "https://arxiv.org/pdf/2504.20384", "abs": "https://arxiv.org/abs/2504.20384", "authors": ["Yanan Guo", "Wenhui Dong", "Jun Song", "Shiding Zhu", "Xuan Zhang", "Hanqing Yang", "Yingbo Wang", "Yang Du", "Xianing Chen", "Bo Zheng"], "title": "FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Recent advancements in video understanding within visual large language\nmodels (VLLMs) have led to notable progress. However, the complexity of video\ndata and contextual processing limitations still hinder long-video\ncomprehension. A common approach is video feature compression to reduce token\ninput to large language models, yet many methods either fail to prioritize\nessential features, leading to redundant inter-frame information, or introduce\ncomputationally expensive modules.To address these issues, we propose\nFiLA(Fine-grained Vision Language Model)-Video, a novel framework that\nleverages a lightweight dynamic-weight multi-frame fusion strategy, which\nadaptively integrates multiple frames into a single representation while\npreserving key video information and reducing computational costs. To enhance\nframe selection for fusion, we introduce a keyframe selection strategy,\neffectively identifying informative frames from a larger pool for improved\nsummarization. Additionally, we present a simple yet effective long-video\ntraining data generation strategy, boosting model performance without extensive\nmanual annotation. Experimental results demonstrate that FiLA-Video achieves\nsuperior efficiency and accuracy in long-video comprehension compared to\nexisting methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy", "summarization", "fine-grained"], "score": 4}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20708", "pdf": "https://arxiv.org/pdf/2504.20708", "abs": "https://arxiv.org/abs/2504.20708", "authors": ["Hasan Abed Al Kader Hammoud", "Hani Itani", "Bernard Ghanem"], "title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex\nproblems. Standard evaluation practice involves generating a complete reasoning\ntrace and assessing the correctness of the final answer presented at its\nconclusion. In this paper, we challenge the reliance on the final answer by\nposing the following two questions: Does the final answer reliably represent\nthe model's optimal conclusion? Can alternative reasoning paths yield different\nresults? To answer these questions, we analyze intermediate reasoning steps,\ntermed subthoughts, and propose a method based on our findings. Our approach\ninvolves segmenting a reasoning trace into sequential subthoughts based on\nlinguistic cues. We start by prompting the model to generate continuations from\nthe end-point of each intermediate subthought. We extract a potential answer\nfrom every completed continuation originating from different subthoughts. We\nfind that aggregating these answers by selecting the most frequent one (the\nmode) often yields significantly higher accuracy compared to relying solely on\nthe answer derived from the original complete trace. Analyzing the consistency\namong the answers derived from different subthoughts reveals characteristics\nthat correlate with the model's confidence and correctness, suggesting\npotential for identifying less reliable answers. Our experiments across various\nLLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)\nshow consistent accuracy improvements, with gains reaching up to 13\\% and 10\\%\nrespectively. Implementation is available at:\nhttps://github.com/hammoudhasan/SubthoughtReasoner.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20682", "pdf": "https://arxiv.org/pdf/2504.20682", "abs": "https://arxiv.org/abs/2504.20682", "authors": ["Long Liu", "Cihui Yang"], "title": "OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition is a key task in document analysis. However, the\ngeometric deformation in deformed tables causes a weak correlation between\ncontent information and structure, resulting in downstream tasks not being able\nto obtain accurate content information. To obtain fine-grained spatial\ncoordinates of cells, we propose the OG-HFYOLO model, which enhances the edge\nresponse by Gradient Orientation-aware Extractor, combines a Heterogeneous\nKernel Cross Fusion module and a scale-aware loss function to adapt to\nmulti-scale objective features, and introduces mask-driven non-maximal\nsuppression in the post-processing, which replaces the traditional bounding box\nsuppression mechanism. Furthermore, we also propose a data generator, filling\nthe gap in the dataset for fine-grained deformation table cell spatial\ncoordinate localization, and derive a large-scale dataset named Deformation\nWired Table (DWTAL). Experiments show that our proposed model demonstrates\nexcellent segmentation accuracy on all mainstream instance segmentation models.\nThe dataset and the source code are open source:\nhttps://github.com/justliulong/OGHFYOLO.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20454", "pdf": "https://arxiv.org/pdf/2504.20454", "abs": "https://arxiv.org/abs/2504.20454", "authors": ["Jiajun Ding", "Beiyao Zhu", "Xiaosheng Liu", "Lishen Zhang", "Zhao Liu"], "title": "LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight", "categories": ["eess.IV", "cs.CV"], "comment": "17pages,4 figures", "summary": "This study integrates PET metabolic information with CT anatomical structures\nto establish a 3D multimodal segmentation dataset for lymphoma based on\nwhole-body FDG PET/CT examinations, which bridges the gap of the lack of\nstandardised multimodal segmentation datasets in the field of haematological\nmalignancies. We retrospectively collected 483 examination datasets acquired\nbetween March 2011 and May 2024, involving 220 patients (106 non-Hodgkin\nlymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were\nrigorously de-identified. Complete 3D structural information was preserved\nduring data acquisition, preprocessing and annotation, and a high-quality\ndataset was constructed based on the nnUNet format. By systematic technical\nvalidation and evaluation of the preprocessing process, annotation quality and\nautomatic segmentation algorithm, the deep learning model trained based on this\ndataset is verified to achieve accurate segmentation of lymphoma lesions in\nPET/CT images with high accuracy, good robustness and reproducibility, which\nproves the applicability and stability of this dataset in accurate segmentation\nand quantitative analysis. The deep fusion of PET/CT images achieved with this\ndataset not only significantly improves the accurate portrayal of the\nmorphology, location and metabolic features of tumour lesions, but also\nprovides solid data support for early diagnosis, clinical staging and\npersonalized treatment, and promotes the development of automated image\nsegmentation and precision medicine based on deep learning. The dataset and\nrelated resources are available at https://github.com/SuperD0122/LymphAtlas-.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20322", "pdf": "https://arxiv.org/pdf/2504.20322", "abs": "https://arxiv.org/abs/2504.20322", "authors": ["Sumit Mamtani", "Yash Thesia"], "title": "Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 4 figures. Submitted to arXiv", "summary": "Fine-grained visual classification aims to recognize objects belonging to\nmultiple subordinate categories within a super-category. However, this remains\na challenging problem, as appearance information alone is often insufficient to\naccurately differentiate between fine-grained visual categories. To address\nthis, we propose a novel and unified framework that leverages meta-information\nto assist fine-grained identification. We tackle the joint learning of visual\nand meta-information through cross-contrastive pre-training. In the first\nstage, we employ three encoders for images, text, and meta-information,\naligning their projected embeddings to achieve better representations. We then\nfine-tune the image and meta-information encoders for the classification task.\nExperiments on the NABirds dataset demonstrate that our framework effectively\nutilizes meta-information to enhance fine-grained recognition performance. With\nthe addition of meta-information, our framework surpasses the current baseline\non NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the\nNABirds dataset, outperforming many existing state-of-the-art approaches that\nutilize meta-information.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20609", "pdf": "https://arxiv.org/pdf/2504.20609", "abs": "https://arxiv.org/abs/2504.20609", "authors": ["Xinyu Yao", "Mengdi Wang", "Bo Chen", "Xiaobing Zhao"], "title": "WenyanGPT: A Large Language Model for Classical Chinese Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Classical Chinese, as the core carrier of Chinese culture, plays a crucial\nrole in the inheritance and study of ancient literature. However, existing\nnatural language processing models primarily optimize for Modern Chinese,\nresulting in inadequate performance on Classical Chinese. This paper presents a\ncomprehensive solution for Classical Chinese language processing. By continuing\npre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we\nconstruct a large language model, WenyanGPT, which is specifically designed for\nClassical Chinese tasks. Additionally, we develop an evaluation benchmark\ndataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that\nWenyanGPT significantly outperforms current advanced LLMs in various Classical\nChinese tasks. We make the model's training data, instruction fine-tuning\ndata\\footnote, and evaluation benchmark dataset publicly available to promote\nfurther research and development in the field of Classical Chinese processing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20435", "pdf": "https://arxiv.org/pdf/2504.20435", "abs": "https://arxiv.org/abs/2504.20435", "authors": ["Love Panta", "Suraj Prasai", "Karishma Malla Vaidya", "Shyam Shrestha", "Suresh Manandhar"], "title": "AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries", "categories": ["cs.CV"], "comment": null, "summary": "Cervical cancer remains a significant health challenge, with high incidence\nand mortality rates, particularly in transitioning countries. Conventional\nLiquid-Based Cytology(LBC) is a labor-intensive process, requires expert\npathologists and is highly prone to errors, highlighting the need for more\nefficient screening methods. This paper introduces an innovative approach that\nintegrates low-cost biological microscopes with our simple and efficient AI\nalgorithms for automated whole-slide analysis. Our system uses a motorized\nmicroscope to capture cytology images, which are then processed through an AI\npipeline involving image stitching, cell segmentation, and classification. We\nutilize the lightweight UNet-based model involving human-in-the-loop approach\nto train our segmentation model with minimal ROIs. CvT-based classification\nmodel, trained on the SIPaKMeD dataset, accurately categorizes five cell types.\nOur framework offers enhanced accuracy and efficiency in cervical cancer\nscreening compared to various state-of-art methods, as demonstrated by\ndifferent evaluation metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20496", "pdf": "https://arxiv.org/pdf/2504.20496", "abs": "https://arxiv.org/abs/2504.20496", "authors": ["Shuo Sun", "Torsten Sattler", "Malcolm Mielle", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Large-scale visual SLAM for in-the-wild videos", "categories": ["cs.CV"], "comment": "fix the overview figure", "summary": "Accurate and robust 3D scene reconstruction from casual, in-the-wild videos\ncan significantly simplify robot deployment to new environments. However,\nreliable camera pose estimation and scene reconstruction from such\nunconstrained videos remains an open challenge. Existing visual-only SLAM\nmethods perform well on benchmark datasets but struggle with real-world footage\nwhich often exhibits uncontrolled motion including rapid rotations and pure\nforward movements, textureless regions, and dynamic objects. We analyze the\nlimitations of current methods and introduce a robust pipeline designed to\nimprove 3D reconstruction from casual videos. We build upon recent deep visual\nodometry methods but increase robustness in several ways. Camera intrinsics are\nautomatically recovered from the first few frames using structure-from-motion.\nDynamic objects and less-constrained areas are masked with a predictive model.\nAdditionally, we leverage monocular depth estimates to regularize bundle\nadjustment, mitigating errors in low-parallax situations. Finally, we integrate\nplace recognition and loop closure to reduce long-term drift and refine both\nintrinsics and pose estimates through global bundle adjustment. We demonstrate\nlarge-scale contiguous 3D models from several online videos in various\nenvironments. In contrast, baseline methods typically produce locally\ninconsistent results at several points, producing separate segments or\ndistorted maps. In lieu of ground-truth pose data, we evaluate map consistency,\nexecution time and visual accuracy of re-rendered NeRF models. Our proposed\nsystem establishes a new baseline for visual reconstruction from casual\nuncontrolled videos found online, demonstrating more consistent reconstructions\nover longer sequences of in-the-wild videos than previously achieved.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20510", "pdf": "https://arxiv.org/pdf/2504.20510", "abs": "https://arxiv.org/abs/2504.20510", "authors": ["Irina Ruzavina", "Lisa Sophie Theis", "Jesse Lemeer", "Rutger de Groen", "Leo Ebeling", "Andrej Hulak", "Jouaria Ali", "Guangzhi Tang", "Rico Mockel"], "title": "SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by IJCNN 2025", "summary": "Automating the quality control of shot-blasted steel surfaces is crucial for\nimproving manufacturing efficiency and consistency. This study presents a\ndataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as\neither \"ready for paint\" or \"needs shot-blasting.\" The dataset captures\nreal-world surface defects, including discoloration, welding lines, scratches\nand corrosion, making it well-suited for training computer vision models.\nAdditionally, three classification approaches were evaluated: Compact\nConvolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50\nfeature extraction, and a Convolutional Autoencoder (CAE). The supervised\nmethods (CCT and SVM) achieve 95% classification accuracy on the test set, with\nCCT leveraging transformer-based attention mechanisms and SVM offering a\ncomputationally efficient alternative. The CAE approach, while less effective,\nestablishes a baseline for unsupervised quality control. We present\ninterpretable decision-making by all three neural networks, allowing industry\nusers to visually pinpoint problematic regions and understand the model's\nrationale. By releasing the dataset and baseline codes, this work aims to\nsupport further research in defect detection, advance the development of\ninterpretable computer vision models for quality control, and encourage the\nadoption of automated inspection systems in industrial applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20964", "pdf": "https://arxiv.org/pdf/2504.20964", "abs": "https://arxiv.org/abs/2504.20964", "authors": ["Shangyu Li", "Juyong Jiang", "Tiancheng Zhao", "Jiasi Shen"], "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification", "categories": ["cs.CL", "cs.AI", "cs.OS", "cs.PL", "cs.SE"], "comment": null, "summary": "We introduce OSVBench, a new benchmark for evaluating Large Language Models\n(LLMs) in generating complete specification code pertaining to operating system\nkernel verification tasks. The benchmark first defines the specification\ngeneration problem into a program synthesis problem within a confined scope of\nsyntax and semantics by providing LLMs with the programming model. The LLMs are\nrequired to understand the provided verification assumption and the potential\nsyntax and semantics space to search for, then generate the complete\nspecification for the potentially buggy operating system code implementation\nunder the guidance of the high-level functional description of the operating\nsystem. This benchmark is built upon a real-world operating system kernel,\nHyperkernel, and consists of 245 complex specification generation tasks in\ntotal, each is a long context task of about 20k-30k tokens. Our comprehensive\nevaluation of 12 LLMs exhibits the limited performance of the current LLMs on\nthe specification generation tasks for operating system verification.\nSignificant disparities in their performance on the benchmark highlight\ndifferences in their ability to handle long-context code generation tasks. The\nevaluation toolkit and benchmark are available at\nhttps://github.com/lishangyu-hkust/OSVBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20648", "pdf": "https://arxiv.org/pdf/2504.20648", "abs": "https://arxiv.org/abs/2504.20648", "authors": ["Michael Ogezi", "Freda Shi"], "title": "SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) work well in tasks ranging from image\ncaptioning to visual question answering (VQA), yet they struggle with spatial\nreasoning, a key skill for understanding our physical world that humans excel\nat. We find that spatial relations are generally rare in widely used VL\ndatasets, with only a few being well represented, while most form a long tail\nof underrepresented relations. This gap leaves VLMs ill-equipped to handle\ndiverse spatial relationships. To bridge it, we construct a synthetic VQA\ndataset focused on spatial reasoning generated from hyper-detailed image\ndescriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset\nconsists of 455k samples containing 3.4 million QA pairs. Trained on this\ndataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements\non spatial reasoning benchmarks, achieving up to a 49% performance gain on the\nWhat's Up benchmark, while maintaining strong results on general tasks. Our\nwork narrows the gap between human and VLM spatial reasoning and makes VLMs\nmore capable in real-world tasks such as robotics and navigation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094", "abs": "https://arxiv.org/abs/2504.20094", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20865", "pdf": "https://arxiv.org/pdf/2504.20865", "abs": "https://arxiv.org/abs/2504.20865", "authors": ["Lorenzo Pellegrini", "Davide Cozzolino", "Serafino Pandolfini", "Davide Maltoni", "Matteo Ferrara", "Luisa Verdoliva", "Marco Prati", "Marco Ramilli"], "title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 4 tables, code available:\n  https://github.com/MI-BioLab/AI-GenBench", "summary": "The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20948", "pdf": "https://arxiv.org/pdf/2504.20948", "abs": "https://arxiv.org/abs/2504.20948", "authors": ["Yanghui Song", "Chengfu Yang"], "title": "DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Given the severe challenges confronting the global growth security of\neconomic crops, precise identification and prevention of plant diseases has\nemerged as a critical issue in artificial intelligence-enabled agricultural\ntechnology. To address the technical challenges in plant disease recognition,\nincluding small-sample learning, leaf occlusion, illumination variations, and\nhigh inter-class similarity, this study innovatively proposes a Dynamic\nDual-Stream Fusion Network (DS_FusionNet). The network integrates a\ndual-backbone architecture, deformable dynamic fusion modules, and\nbidirectional knowledge distillation strategy, significantly enhancing\nrecognition accuracy. Experimental results demonstrate that DS_FusionNet\nachieves classification accuracies exceeding 90% using only 10% of the\nPlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the\ncomplex PlantWild dataset, exhibiting exceptional generalization capabilities.\nThis research not only provides novel technical insights for fine-grained image\nclassification but also establishes a robust foundation for precise\nidentification and management of agricultural diseases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20898", "pdf": "https://arxiv.org/pdf/2504.20898", "abs": "https://arxiv.org/abs/2504.20898", "authors": ["Hasan Md Tusfiqur Alam", "Devansh Srivastav", "Abdulrahman Mohamed Selim", "Md Abdul Kadir", "Md Moktadiurl Hoque Shuvo", "Daniel Sonntag"], "title": "CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted in the 17th ACM SIGCHI Symposium on Engineering Interactive\n  Computing Systems (EICS 2025)", "summary": "Advancements in generative Artificial Intelligence (AI) hold great promise\nfor automating radiology workflows, yet challenges in interpretability and\nreliability hinder clinical adoption. This paper presents an automated\nradiology report generation framework that combines Concept Bottleneck Models\n(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge\nAI performance with clinical explainability. CBMs map chest X-ray features to\nhuman-understandable clinical concepts, enabling transparent disease\nclassification. Meanwhile, the RAG system integrates multi-agent collaboration\nand external knowledge to produce contextually rich, evidence-based reports.\nOur demonstration showcases the system's ability to deliver interpretable\npredictions, mitigate hallucinations, and generate high-quality, tailored\nreports with an interactive interface addressing accuracy, trust, and usability\nchallenges. This framework provides a pathway to improving diagnostic\nconsistency and empowering radiologists with actionable insights.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20054", "pdf": "https://arxiv.org/pdf/2504.20054", "abs": "https://arxiv.org/abs/2504.20054", "authors": ["Jiayang Sun", "Hongbo Wang", "Jie Cao", "Huaibo Huang", "Ran He"], "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images, they often\nstruggle with accurate counting, attributes, and spatial relationships in\ncomplex multi-object scenes. To address these challenges, we propose Marmot, a\nnovel and generalizable framework that employs Multi-Agent Reasoning for\nMulti-Object Self-Correcting, enhancing image-text alignment and facilitating\nmore coherent multi-object image editing. Our framework adopts a\ndivide-and-conquer strategy that decomposes the self-correction task into three\ncritical dimensions (counting, attributes, and spatial relationships), and\nfurther divided into object-level subtasks. We construct a multi-agent editing\nsystem featuring a decision-execution-verification mechanism, effectively\nmitigating inter-object interference and enhancing editing reliability. To\nresolve the problem of subtask integration, we propose a Pixel-Domain Stitching\nSmoother that employs mask-guided two-stage latent space optimization. This\ninnovation enables parallel processing of subtask results, thereby enhancing\nruntime efficiency while eliminating multi-stage distortion accumulation.\nExtensive experiments demonstrate that Marmot significantly improves accuracy\nin object counting, attribute assignment, and spatial relationships for image\ngeneration tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20091", "pdf": "https://arxiv.org/pdf/2504.20091", "abs": "https://arxiv.org/abs/2504.20091", "authors": ["Noriyuki Kugo", "Xiang Li", "Zixin Li", "Ashish Gupta", "Arpandeep Khatua", "Nidhish Jain", "Chaitanya Patel", "Yuta Kyuragi", "Masamoto Tanabiki", "Kazuki Kozuka", "Ehsan Adeli"], "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "Video Question Answering (VQA) inherently relies on multimodal reasoning,\nintegrating visual, temporal, and linguistic cues to achieve a deeper\nunderstanding of video content. However, many existing methods rely on feeding\nframe-level captions into a single model, making it difficult to adequately\ncapture temporal and interactive contexts. To address this limitation, we\nintroduce VideoMultiAgents, a framework that integrates specialized agents for\nvision, scene graph analysis, and text processing. It enhances video\nunderstanding leveraging complementary multimodal reasoning from independently\noperating agents. Our approach is also supplemented with a question-guided\ncaption generation, which produces captions that highlight objects, actions,\nand temporal transitions directly relevant to a given query, thus improving the\nanswer accuracy. Experimental results demonstrate that our method achieves\nstate-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),\nEgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20104", "pdf": "https://arxiv.org/pdf/2504.20104", "abs": "https://arxiv.org/abs/2504.20104", "authors": ["Luiz F. P. Southier", "Marcelo Filipak", "Luiz A. Zanlorensi", "Ildefonso Wasilevski", "Fabio Favarim", "Jefferson T. Oliva", "Marcelo Teixeira", "Dalcimar Casanova"], "title": "An on-production high-resolution longitudinal neonatal fingerprint database in Brazil", "categories": ["cs.CV"], "comment": null, "summary": "The neonatal period is critical for survival, requiring accurate and early\nidentification to enable timely interventions such as vaccinations, HIV\ntreatment, and nutrition programs. Biometric solutions offer potential for\nchild protection by helping to prevent baby swaps, locate missing children, and\nsupport national identity systems. However, developing effective biometric\nidentification systems for newborns remains a major challenge due to the\nphysiological variability caused by finger growth, weight changes, and skin\ntexture alterations during early development. Current literature has attempted\nto address these issues by applying scaling factors to emulate growth-induced\ndistortions in minutiae maps, but such approaches fail to capture the complex\nand non-linear growth patterns of infants. A key barrier to progress in this\ndomain is the lack of comprehensive, longitudinal biometric datasets capturing\nthe evolution of neonatal fingerprints over time. This study addresses this gap\nby focusing on designing and developing a high-quality biometric database of\nneonatal fingerprints, acquired at multiple early life stages. The dataset is\nintended to support the training and evaluation of machine learning models\naimed at emulating the effects of growth on biometric features. We hypothesize\nthat such a dataset will enable the development of more robust and accurate\nDeep Learning-based models, capable of predicting changes in the minutiae map\nwith higher fidelity than conventional scaling-based methods. Ultimately, this\neffort lays the groundwork for more reliable biometric identification systems\ntailored to the unique developmental trajectory of newborns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157", "abs": "https://arxiv.org/abs/2504.20157", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "policy optimization", "RLAIF", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20323", "pdf": "https://arxiv.org/pdf/2504.20323", "abs": "https://arxiv.org/abs/2504.20323", "authors": ["Chao-Lin Liu", "Po-Hsien Wu", "Yi-Ting Yu"], "title": "Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG"], "comment": "16 pages, 9 figures, 2 tables, the Nineteenth International Workshop\n  on Juris-Informatics (JURISIN 2025), associated with the Seventeenth JSAI\n  International Symposium on AI (JSAI-isAI 2025)", "summary": "This report addresses the challenge of limited labeled datasets for\ndeveloping legal recommender systems, particularly in specialized domains like\nlabor disputes. We propose a new approach leveraging the co-citation of legal\narticles within cases to establish similarity and enable algorithmic\nannotation. This method draws a parallel to the concept of case co-citation,\nutilizing cited precedents as indicators of shared legal issues. To evaluate\nthe labeled results, we employ a system that recommends similar cases based on\nplaintiffs' accusations, defendants' rebuttals, and points of disputes. The\nevaluation demonstrates that the recommender, with finetuned text embedding\nmodels and a reasonable BiLSTM module can recommend labor cases whose\nsimilarity was measured by the co-citation of the legal articles. This research\ncontributes to the development of automated annotation techniques for legal\ndocuments, particularly in areas with limited access to comprehensive legal\ndatabases.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20371", "pdf": "https://arxiv.org/pdf/2504.20371", "abs": "https://arxiv.org/abs/2504.20371", "authors": ["Zhibo Man", "Yuanmeng Chen", "Yujie Zhang", "Yufeng Chen", "Jinan Xu"], "title": "DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation", "categories": ["cs.CL"], "comment": null, "summary": "Currently, Large Language Models (LLMs) have achieved remarkable results in\nmachine translation. However, their performance in multi-domain translation\n(MDT) is less satisfactory; the meanings of words can vary across different\ndomains, highlighting the significant ambiguity inherent in MDT. Therefore,\nevaluating the disambiguation ability of LLMs in MDT remains an open problem.\nTo this end, we present an evaluation and analysis of LLMs on disambiguation in\nmulti-domain translation (DMDTEval), our systematic evaluation framework\nconsisting of three critical aspects: (1) we construct a translation test set\nwith multi-domain ambiguous word annotation, (2) we curate a diverse set of\ndisambiguation prompting templates, and (3) we design precise disambiguation\nmetrics, and study the efficacy of various prompting strategies on multiple\nstate-of-the-art LLMs. Our extensive experiments reveal a number of crucial\nfindings that we believe will pave the way and also facilitate further research\nin the critical area of improving the disambiguation of LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20343", "pdf": "https://arxiv.org/pdf/2504.20343", "abs": "https://arxiv.org/abs/2504.20343", "authors": ["Amaan Izhar", "Nurul Japar", "Norisma Idris", "Ting Dang"], "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025, 8 pages, 8 figures, 3 tables", "summary": "Medical image reporting (MIR) aims to generate structured clinical\ndescriptions from radiological images. Existing methods struggle with\nfine-grained feature extraction, multimodal alignment, and generalization\nacross diverse imaging types, often relying on vanilla transformers and\nfocusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language\nmixture-of-experts model with gated cross-aligned fusion, designed to address\nthese limitations. Our architecture includes: (i) a multiscale vision encoder\n(MSVE) for capturing anatomical details at varying resolutions, (ii) a\nmultihead dual-branch latent attention (MDLA) module for vision-language\nalignment through latent bottleneck representations, and (iii) a modulated\nmixture-of-experts (MoE) decoder for adaptive expert specialization. We extend\nMIR to CT scans, retinal imaging, MRI scans, and gross pathology images,\nreporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.\nExtensive experiments and ablations confirm improved clinical accuracy,\ncross-modal alignment, and model interpretability. Code is available at\nhttps://github.com/AI-14/micar-vl-moe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20469", "pdf": "https://arxiv.org/pdf/2504.20469", "abs": "https://arxiv.org/abs/2504.20469", "authors": ["Enfa Fane", "Mihai Surdeanu", "Eduardo Blanco", "Steven R. Corman"], "title": "Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models", "categories": ["cs.CL", "cs.CY", "I.2.7"], "comment": "Accepted to The 19th International Workshop on Semantic Evaluation\n  (Semeval 2025)", "summary": "Understanding how news narratives frame entities is crucial for studying\nmedia's impact on societal perceptions of events. In this paper, we evaluate\nthe zero-shot capabilities of large language models (LLMs) in classifying\nframing roles. Through systematic experimentation, we assess the effects of\ninput context, prompting strategies, and task decomposition. Our findings show\nthat a hierarchical approach of first identifying broad roles and then\nfine-grained roles, outperforms single-step classification. We also demonstrate\nthat optimal input contexts and prompts vary across task levels, highlighting\nthe need for subtask-specific strategies. We achieve a Main Role Accuracy of\n89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our\napproach. Our findings emphasize the importance of tailored prompt design and\ninput context optimization for improving LLM performance in entity framing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20362", "pdf": "https://arxiv.org/pdf/2504.20362", "abs": "https://arxiv.org/abs/2504.20362", "authors": ["Qinhua Xie", "Hao Tang"], "title": "TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing use of surgical robots in clinical practice, enhancing\ntheir ability to process multimodal medical images has become a key research\nchallenge. Although traditional medical image fusion methods have made progress\nin improving fusion accuracy, they still face significant challenges in\nreal-time performance, fine-grained feature extraction, and edge\npreservation.In this paper, we introduce TTTFusion, a Test-Time Training\n(TTT)-based image fusion strategy that dynamically adjusts model parameters\nduring inference to efficiently fuse multimodal medical images. By adapting the\nmodel during the test phase, our method optimizes the parameters based on the\ninput image data, leading to improved accuracy and better detail preservation\nin the fusion results.Experimental results demonstrate that TTTFusion\nsignificantly enhances the fusion quality of multimodal images compared to\ntraditional fusion methods, particularly in fine-grained feature extraction and\nedge preservation. This approach not only improves image fusion accuracy but\nalso offers a novel technical solution for real-time image processing in\nsurgical robots.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time training"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20547", "pdf": "https://arxiv.org/pdf/2504.20547", "abs": "https://arxiv.org/abs/2504.20547", "authors": ["Jesus Lovon", "Thouria Ben-Haddi", "Jules Di Scala", "Jose G. Moreno", "Lynda Tamine"], "title": "Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records", "categories": ["cs.CL"], "comment": null, "summary": "The lack of standardized evaluation benchmarks in the medical domain for text\ninputs can be a barrier to widely adopting and leveraging the potential of\nnatural language models for health-related downstream tasks. This paper\nrevisited an openly available MIMIC-IV benchmark for electronic health records\n(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the\nHugging Face datasets library to allow an easy share and use of this\ncollection. Second, we investigate the application of templates to convert EHR\ntabular data to text. Experiments using fine-tuned and zero-shot LLMs on the\nmortality of patients task show that fine-tuned text-based models are\ncompetitive against robust tabular classifiers. In contrast, zero-shot LLMs\nstruggle to leverage EHR representations. This study underlines the potential\nof text-based approaches in the medical field and highlights areas for further\nimprovement.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20379", "pdf": "https://arxiv.org/pdf/2504.20379", "abs": "https://arxiv.org/abs/2504.20379", "authors": ["Jongwon Lee", "Timothy Bretl"], "title": "GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we present a method for localizing a query image with respect\nto a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the\nmethod uses 3DGS to render a synthetic RGBD image at some initial pose\nestimate. Second, it establishes 2D-2D correspondences between the query image\nand this synthetic image. Third, it uses the depth map to lift the 2D-2D\ncorrespondences to 2D-3D correspondences and solves a perspective-n-point (PnP)\nproblem to produce a final pose estimate. Results from evaluation across three\nexisting datasets with 38 scenes and over 2,700 test images show that our\nmethod significantly reduces both inference time (by over two orders of\nmagnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation\nerror compared to baseline methods that use photometric loss minimization.\nResults also show that our method tolerates large errors in the initial pose\nestimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized\nby scene scale), achieving final pose errors of less than 5{\\deg} in rotation\nand 0.05 units in translation on 90% of images from the Synthetic NeRF and\nMip-NeRF360 datasets and on 42% of images from the more challenging Tanks and\nTemples dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20581", "pdf": "https://arxiv.org/pdf/2504.20581", "abs": "https://arxiv.org/abs/2504.20581", "authors": ["Iwona Christop", "Tomasz Kuczyski", "Marek Kubis"], "title": "ClonEval: An Open Voice Cloning Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel benchmark for voice cloning text-to-speech models. The\nbenchmark consists of an evaluation protocol, an open-source library for\nassessing the performance of voice cloning models, and an accompanying\nleaderboard. The paper discusses design considerations and presents a detailed\ndescription of the evaluation procedure. The usage of the software library is\nexplained, along with the organization of results on the leaderboard.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605", "abs": "https://arxiv.org/abs/2504.20605", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20419", "pdf": "https://arxiv.org/pdf/2504.20419", "abs": "https://arxiv.org/abs/2504.20419", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas", "Dimitrios K. Nasiopoulos"], "title": "Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Automation in agriculture plays a vital role in addressing challenges related\nto crop monitoring and disease management, particularly through early detection\nsystems. This study investigates the effectiveness of combining multimodal\nLarge Language Models (LLMs), specifically GPT-4o, with Convolutional Neural\nNetworks (CNNs) for automated plant disease classification using leaf imagery.\nLeveraging the PlantVillage dataset, we systematically evaluate model\nperformance across zero-shot, few-shot, and progressive fine-tuning scenarios.\nA comparative analysis between GPT-4o and the widely used ResNet-50 model was\nconducted across three resolutions (100, 150, and 256 pixels) and two plant\nspecies (apple and corn). Results indicate that fine-tuned GPT-4o models\nachieved slightly better performance compared to the performance of ResNet-50,\nachieving up to 98.12% classification accuracy on apple leaf images, compared\nto 96.88% achieved by ResNet-50, with improved generalization and near-zero\ntraining loss. However, zero-shot performance of GPT-4o was significantly\nlower, underscoring the need for minimal training. Additional evaluations on\ncross-resolution and cross-plant generalization revealed the models'\nadaptability and limitations when applied to new domains. The findings\nhighlight the promise of integrating multimodal LLMs into automated disease\ndetection pipelines, enhancing the scalability and intelligence of precision\nagriculture systems while reducing the dependence on large, labeled datasets\nand high-resolution sensor infrastructure. Large Language Models, Vision\nLanguage Models, LLMs and CNNs, Disease Detection with Vision Language Models,\nVLMs", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20438", "pdf": "https://arxiv.org/pdf/2504.20438", "abs": "https://arxiv.org/abs/2504.20438", "authors": ["Ziyang Xu", "Kangsheng Duan", "Xiaolei Shen", "Zhifeng Ding", "Wenyu Liu", "Xiaohu Ruan", "Xiaoxin Chen", "Xinggang Wang"], "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting is a fundamental research area between image editing and\nimage generation. Recent state-of-the-art (SOTA) methods have explored novel\nattention mechanisms, lightweight architectures, and context-aware modeling,\ndemonstrating impressive performance. However, they often struggle with complex\nstructure (e.g., texture, shape, spatial relations) and semantics (e.g., color\nconsistency, object restoration, and logical correctness), leading to artifacts\nand inappropriate generation. To address this challenge, we design a simple yet\neffective inpainting paradigm called latent categories guidance, and further\npropose a diffusion-based model named PixelHacker. Specifically, we first\nconstruct a large dataset containing 14 million image-mask pairs by annotating\nforeground and background (potential 116 and 21 categories, respectively).\nThen, we encode potential foreground and background representations separately\nthrough two fixed-size embeddings, and intermittently inject these features\ninto the denoising process via linear attention. Finally, by pre-training on\nour dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.\nExtensive experiments show that PixelHacker comprehensively outperforms the\nSOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits\nremarkable consistency in both structure and semantics. Project page at\nhttps://hustvl.github.io/projects/PixelHacker.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20466", "pdf": "https://arxiv.org/pdf/2504.20466", "abs": "https://arxiv.org/abs/2504.20466", "authors": ["Woo Yi Yang", "Jiarui Wang", "Sijing Wu", "Huiyu Duan", "Yuxin Zhu", "Liu Yang", "Kang Fu", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679", "abs": "https://arxiv.org/abs/2504.20679", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20703", "pdf": "https://arxiv.org/pdf/2504.20703", "abs": "https://arxiv.org/abs/2504.20703", "authors": ["Foteini Papadopoulou", "Osman Mutlu", "Neris zen", "Bas H. M. van der Velden", "Iris Hendrickx", "Ali Hrriyetolu"], "title": "BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system developed for the SemEval-2025 Task 9: The\nFood Hazard Detection Challenge. The shared task's objective is to evaluate\nexplainable classification systems for classifying hazards and products in two\nlevels of granularity from food recall incident reports. In this work, we\npropose text augmentation techniques as a way to improve poor performance on\nminority classes and compare their effect for each category on various\ntransformer and machine learning models. We explore three word-level data\naugmentation techniques, namely synonym replacement, random word swapping, and\ncontextual word insertion. The results show that transformer models tend to\nhave a better overall performance. None of the three augmentation techniques\nconsistently improved overall performance for classifying hazards and products.\nWe observed a statistically significant improvement (P < 0.05) in the\nfine-grained categories when using the BERT model to compare the baseline with\neach augmented model. Compared to the baseline, the contextual words insertion\naugmentation improved the accuracy of predictions for the minority hazard\nclasses by 6%. This suggests that targeted augmentation of minority classes can\nimprove the performance of transformer models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20752", "pdf": "https://arxiv.org/pdf/2504.20752", "abs": "https://arxiv.org/abs/2504.20752", "authors": ["Roman Abramov", "Felix Steinbauer", "Gjergji Kasneci"], "title": "Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.3; I.7"], "comment": null, "summary": "Transformers have achieved great success in numerous NLP tasks but continue\nto exhibit notable gaps in multi-step factual reasoning, especially when\nreal-world knowledge is sparse. Recent advances in grokking have demonstrated\nthat neural networks can transition from memorizing to perfectly generalizing\nonce they detect underlying logical patterns - yet these studies have primarily\nused small, synthetic tasks. In this paper, for the first time, we extend\ngrokking to real-world factual data and address the challenge of dataset\nsparsity by augmenting existing knowledge graphs with carefully designed\nsynthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts\nabove the threshold required for grokking. Surprisingly, we find that even\nfactually incorrect synthetic data can strengthen emergent reasoning circuits\nrather than degrade accuracy, as it forces the model to rely on relational\nstructure rather than memorization. When evaluated on multi-hop reasoning\nbenchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -\nsubstantially improving over strong baselines and matching or exceeding current\nstate-of-the-art results. We further provide an in-depth analysis of how\nincreasing $\\phi_r$ drives the formation of generalizing circuits inside\nTransformers. Our findings suggest that grokking-based data augmentation can\nunlock implicit multi-hop reasoning capabilities, opening the door to more\nrobust and interpretable factual reasoning in large-scale language models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20530", "pdf": "https://arxiv.org/pdf/2504.20530", "abs": "https://arxiv.org/abs/2504.20530", "authors": ["Wenxuan Liu", "Xian Zhong", "Zhuo Zhou", "Siyuan Yang", "Chia-Wen Lin", "Alex Chichung Kot"], "title": "Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges\ndue to significant view variations along the vertical spatial axis. Unlike\ntraditional ground-based settings, UAVs capture actions from a wide range of\naltitudes, resulting in considerable appearance discrepancies. We introduce a\nmulti-view formulation tailored to varying UAV altitudes and empirically\nobserve a partial order among views, where recognition accuracy consistently\ndecreases as the altitude increases. This motivates a novel approach that\nexplicitly models the hierarchical structure of UAV views to improve\nrecognition performance across altitudes. To this end, we propose the Partial\nOrder Guided Multi-View Network (POG-MVNet), designed to address drastic view\nvariations by effectively leveraging view-dependent information across\ndifferent altitude levels. The framework comprises three key components: a View\nPartition (VP) module, which uses the head-to-body ratio to group views by\naltitude; an Order-aware Feature Decoupling (OFD) module, which disentangles\naction-relevant and view-specific features under partial order guidance; and an\nAction Partial Order Guide (APOG), which leverages the partial order to\ntransfer informative knowledge from easier views to support learning in more\nchallenging ones. We conduct experiments on Drone-Action, MOD20, and UAV\ndatasets, demonstrating that POG-MVNet significantly outperforms competing\nmethods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action\ndataset and a 3.5% improvement on UAV dataset compared to state-of-the-art\nmethods ASAT and FAR. The code for POG-MVNet will be made available soon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20922", "pdf": "https://arxiv.org/pdf/2504.20922", "abs": "https://arxiv.org/abs/2504.20922", "authors": ["Miguel Nogales", "Matteo Gambella", "Manuel Roveri"], "title": "DYNAMAX: Dynamic computing for Transformers and Mamba based architectures", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary), 68T07 (Secondary)"], "comment": "Accepted to IJCNN 2025", "summary": "Early exits (EEs) offer a promising approach to reducing computational costs\nand latency by dynamically terminating inference once a satisfactory prediction\nconfidence on a data sample is achieved. Although many works integrate EEs into\nencoder-only Transformers, their application to decoder-only architectures and,\nmore importantly, Mamba models, a novel family of state-space architectures in\nthe LLM realm, remains insufficiently explored. This work introduces DYNAMAX,\nthe first framework to exploit the unique properties of Mamba architectures for\nearly exit mechanisms. We not only integrate EEs into Mamba but also repurpose\nMamba as an efficient EE classifier for both Mamba-based and transformer-based\nLLMs, showcasing its versatility. Our experiments employ the Mistral 7B\ntransformer compared to the Codestral 7B Mamba model, using data sets such as\nTruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and\nconsistency. The results highlight the adaptability of Mamba as a powerful EE\nclassifier and its efficiency in balancing computational cost and performance\nquality across NLP tasks. By leveraging Mamba's inherent design for dynamic\nprocessing, we open pathways for scalable and efficient inference in embedded\napplications and resource-constrained environments. This study underscores the\ntransformative potential of Mamba in redefining dynamic computing paradigms for\nLLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20645", "pdf": "https://arxiv.org/pdf/2504.20645", "abs": "https://arxiv.org/abs/2504.20645", "authors": ["Weiqin Jiao", "Hao Cheng", "George Vosselman", "Claudio Persello"], "title": "LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Polygonal road outline extraction from high-resolution aerial images is an\nimportant task in large-scale topographic mapping, where roads are represented\nas vectorized polygons, capturing essential geometric features with minimal\nvertex redundancy. Despite its importance, no existing method has been\nexplicitly designed for this task. While polygonal building outline extraction\nhas been extensively studied, the unique characteristics of roads, such as\nbranching structures and topological connectivity, pose challenges to these\nmethods. To address this gap, we introduce LDPoly, the first dedicated\nframework for extracting polygonal road outlines from high-resolution aerial\nimages. Our method leverages a novel Dual-Latent Diffusion Model with a\nChannel-Embedded Fusion Module, enabling the model to simultaneously generate\nroad masks and vertex heatmaps. A tailored polygonization method is then\napplied to obtain accurate vectorized road polygons with minimal vertex\nredundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which\ncontains detailed polygonal annotations for various topographic objects in\nseveral Dutch regions. Our experiments include both in-region and cross-region\nevaluations, with the latter designed to assess the model's generalization\nperformance on unseen regions. Quantitative and qualitative results demonstrate\nthat LDPoly outperforms state-of-the-art polygon extraction methods across\nvarious metrics, including pixel-level coverage, vertex efficiency, polygon\nregularity, and road connectivity. We also design two new metrics to assess\npolygon simplicity and boundary smoothness. Moreover, this work represents the\nfirst application of diffusion models for extracting precise vectorized object\noutlines without redundant vertices from remote-sensing imagery, paving the way\nfor future advancements in this field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20972", "pdf": "https://arxiv.org/pdf/2504.20972", "abs": "https://arxiv.org/abs/2504.20972", "authors": ["Yifan Wei", "Xiaoyan Yu", "Ran Song", "Hao Peng", "Angsheng Li"], "title": "SetKE: Knowledge Editing for Knowledge Elements Overlap", "categories": ["cs.CL"], "comment": "The CR version will be updated subsequently", "summary": "Large Language Models (LLMs) excel in tasks such as retrieval and question\nanswering but require updates to incorporate new knowledge and reduce\ninaccuracies and hallucinations. Traditional updating methods, like fine-tuning\nand incremental learning, face challenges such as overfitting and high\ncomputational costs. Knowledge Editing (KE) provides a promising alternative\nbut often overlooks the Knowledge Element Overlap (KEO) phenomenon, where\nmultiple triplets share common elements, leading to editing conflicts. We\nidentify the prevalence of KEO in existing KE datasets and show its significant\nimpact on current KE methods, causing performance degradation in handling such\ntriplets. To address this, we propose a new formulation, Knowledge Set Editing\n(KSE), and introduce SetKE, a method that edits sets of triplets\nsimultaneously. Experimental results demonstrate that SetKE outperforms\nexisting methods in KEO scenarios on mainstream LLMs. Additionally, we\nintroduce EditSet, a dataset containing KEO triplets, providing a comprehensive\nbenchmark.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20669", "pdf": "https://arxiv.org/pdf/2504.20669", "abs": "https://arxiv.org/abs/2504.20669", "authors": ["Joy Battocchio", "Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "Advance Fake Video Detection via Vision Transformers", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent advancements in AI-based multimedia generation have enabled the\ncreation of hyper-realistic images and videos, raising concerns about their\npotential use in spreading misinformation. The widespread accessibility of\ngenerative techniques, which allow for the production of fake multimedia from\nprompts or existing media, along with their continuous refinement, underscores\nthe urgent need for highly accurate and generalizable AI-generated media\ndetection methods, underlined also by new regulations like the European Digital\nAI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based\nfake image detection and extend this idea to video. We propose an {original}\n%innovative framework that effectively integrates ViT embeddings over time to\nenhance detection performance. Our method shows promising accuracy,\ngeneralization, and few-shot learning capabilities across a new, large and\ndiverse dataset of videos generated using five open source generative\ntechniques from the state-of-the-art, as well as a separate dataset containing\nvideos produced by proprietary generative methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084", "abs": "https://arxiv.org/abs/2504.20084", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "title": "AI Awareness", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20677", "pdf": "https://arxiv.org/pdf/2504.20677", "abs": "https://arxiv.org/abs/2504.20677", "authors": ["Paola Natalia Caas", "Alexander Diez", "David Galva", "Marcos Nieto", "Igor Rodrguez"], "title": "Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset", "categories": ["cs.CV"], "comment": "Submitted for review to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "This paper presents a robust, occlusion-aware driver monitoring system (DMS)\nutilizing the Driver Monitoring Dataset (DMD). The system performs driver\nidentification, gaze estimation by regions, and face occlusion detection under\nvarying lighting conditions, including challenging low-light scenarios. Aligned\nwith EuroNCAP recommendations, the inclusion of occlusion detection enhances\nsituational awareness and system trustworthiness by indicating when the\nsystem's performance may be degraded. The system employs separate algorithms\ntrained on RGB and infrared (IR) images to ensure reliable functioning. We\ndetail the development and integration of these algorithms into a cohesive\npipeline, addressing the challenges of working with different sensors and\nreal-car implementation. Evaluation on the DMD and in real-world scenarios\ndemonstrates the effectiveness of the proposed system, highlighting the\nsuperior performance of RGB-based models and the pioneering contribution of\nrobust occlusion detection in DMS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20294", "pdf": "https://arxiv.org/pdf/2504.20294", "abs": "https://arxiv.org/abs/2504.20294", "authors": ["William P. McCarthy", "Saujas Vaduguru", "Karl D. D. Willis", "Justin Matejka", "Judith E. Fan", "Daniel Fried", "Yewen Pu"], "title": "mrCAD: Multimodal Refinement of Computer-aided Designs", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "the first two authors contributed equally", "summary": "A key feature of human collaboration is the ability to iteratively refine the\nconcepts we have communicated. In contrast, while generative AI excels at the\n\\textit{generation} of content, it often struggles to make specific\nlanguage-guided \\textit{modifications} of its prior outputs. To bridge the gap\nbetween how humans and machines perform edits, we present mrCAD, a dataset of\nmultimodal instructions in a communication game. In each game, players created\ncomputer aided designs (CADs) and refined them over several rounds to match\nspecific target designs. Only one player, the Designer, could see the target,\nand they must instruct the other player, the Maker, using text, drawing, or a\ncombination of modalities. mrCAD consists of 6,082 communication games, 15,163\ninstruction-execution rounds, played between 1,092 pairs of human players. We\nanalyze the dataset and find that generation and refinement instructions differ\nin their composition of drawing and text. Using the mrCAD task as a benchmark,\nwe find that state-of-the-art VLMs are better at following generation\ninstructions than refinement instructions. These results lay a foundation for\nanalyzing and modeling a multimodal language of refinement that is not\nrepresented in previous datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20456", "pdf": "https://arxiv.org/pdf/2504.20456", "abs": "https://arxiv.org/abs/2504.20456", "authors": ["Gabe Guo", "Stefano Ermon"], "title": "Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In arbitrary-order language models, it is an open question how to sample\ntokens in parallel from the correct joint distribution. With discrete diffusion\nmodels, the more tokens they generate in parallel, the less their predicted\ndistributions adhere to the originally learned data distribution, as they rely\non a conditional independence assumption that only works with infinitesimally\nsmall timesteps. We find that a different class of models, any-subset\nautoregressive models (AS-ARMs), holds the solution. As implied by the name,\nAS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs\nsupport parallelized joint probability density estimation, allowing them to\ncorrect their own parallel-generated token distributions, via our Any-Subset\nSpeculative Decoding (ASSD) algorithm. ASSD provably enables generation of\ntokens from the correct joint distribution, with the number of neural network\ncalls upper bounded by the number of tokens predicted. We empirically verify\nthat ASSD speeds up language generation, without sacrificing quality.\nFurthermore, we provide a mathematically justified scheme for training AS-ARMs\nfor generation, and show that AS-ARMs achieve state-of-the-art performance\namong sub-200M parameter models on infilling benchmark tasks, and nearly match\nthe performance of models 50X larger on code generation. Our theoretical and\nempirical results indicate that the once-forgotten AS-ARMs are a promising\ndirection of language modeling.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "code generation"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20800", "pdf": "https://arxiv.org/pdf/2504.20800", "abs": "https://arxiv.org/abs/2504.20800", "authors": ["Weizhen He", "Yunfeng Yan", "Shixiang Tang", "Yiheng Deng", "Yangyang Zhong", "Pengxin Luo", "Donglian Qi"], "title": "Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Human-centric perception is the core of diverse computer vision tasks and has\nbeen a long-standing research focus. However, previous research studied these\nhuman-centric tasks individually, whose performance is largely limited to the\nsize of the public task-specific datasets. Recent human-centric methods\nleverage the additional modalities, e.g., depth, to learn fine-grained semantic\ninformation, which limits the benefit of pretraining models due to their\nsensitivity to camera views and the scarcity of RGB-D data on the Internet.\nThis paper improves the data scalability of human-centric pretraining methods\nby discarding depth information and exploring semantic information of RGB\nimages in the frequency space by Discrete Cosine Transform (DCT). We further\npropose new annotation denoising auxiliary tasks with keypoints and DCT maps to\nenforce the RGB image extractor to learn fine-grained semantic information of\nhuman bodies. Our extensive experiments show that when pretrained on\nlarge-scale datasets (COCO and AIC datasets) without depth annotation, our\nmodel achieves better performance than state-of-the-art methods by +0.5 mAP on\nCOCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by\n+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on\nSHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for\ncrowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for\nperson ReID. We also validate the effectiveness of our method on MPII+NTURGBD\ndatasets", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571", "abs": "https://arxiv.org/abs/2504.20571", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20837", "pdf": "https://arxiv.org/pdf/2504.20837", "abs": "https://arxiv.org/abs/2504.20837", "authors": ["Julien Khlaut", "Elodie Ferreres", "Daniel Tordjman", "Hlne Philippe", "Tom Boeken", "Pierre Manceron", "Corentin Dancette"], "title": "RadSAM: Segmenting 3D radiological images with a 2D promptable model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image segmentation is a crucial and time-consuming task in clinical\ncare, where mask precision is extremely important. The Segment Anything Model\n(SAM) offers a promising approach, as it provides an interactive interface\nbased on visual prompting and edition to refine an initial segmentation. This\nmodel has strong generalization capabilities, does not rely on predefined\nclasses, and adapts to diverse objects; however, it is pre-trained on natural\nimages and lacks the ability to process medical data effectively. In addition,\nthis model is built for 2D images, whereas a whole medical domain is based on\n3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging\nare based on 2D models, thus requiring one prompt per slice to segment 3D\nobjects, making the segmentation process tedious. They also lack important\nfeatures such as editing. To bridge this gap, we propose RadSAM, a novel method\nfor segmenting 3D objects with a 2D model from a single prompt. In practice, we\ntrain a 2D model using noisy masks as initial prompts, in addition to bounding\nboxes and points. We then use this novel prompt type with an iterative\ninference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a\nbenchmark to evaluate the model's ability to segment 3D objects in CT images\nfrom a single prompt and evaluate the models' out-of-domain transfer and\nedition capabilities. We demonstrate the effectiveness of our approach against\nstate-of-the-art models on this benchmark using the AMOS abdominal organ\nsegmentation dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20860", "pdf": "https://arxiv.org/pdf/2504.20860", "abs": "https://arxiv.org/abs/2504.20860", "authors": ["Mainak Singha", "Subhankar Roy", "Sarthak Mehrotra", "Ankit Jha", "Moloud Abdar", "Biplab Banerjee", "Elisa Ricci"], "title": "FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated\nlearning by tuning lightweight input tokens (or prompts) on local client data,\nwhile keeping network weights frozen. Post training, only the prompts are\nshared by the clients with the central server for aggregation. However, textual\nprompt tuning often struggles with overfitting to known concepts and may be\noverly reliant on memorized text features, limiting its adaptability to unseen\nconcepts. To address this limitation, we propose Federated Multimodal Visual\nPrompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual\ninformation -- image-conditioned features and textual attribute features of a\nclass -- that is multimodal in nature. At the core of FedMVP is a PromptFormer\nmodule that synergistically aligns textual and visual features through\ncross-attention, enabling richer contexual integration. The dynamically\ngenerated multimodal visual prompts are then input to the frozen vision encoder\nof CLIP, and trained with a combination of CLIP similarity loss and a\nconsistency loss. Extensive evaluation on 20 datasets spanning three\ngeneralization settings demonstrates that FedMVP not only preserves performance\non in-distribution classes and domains, but also displays higher\ngeneralizability to unseen classes and domains when compared to\nstate-of-the-art methods. Codes will be released upon acceptance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20995", "pdf": "https://arxiv.org/pdf/2504.20995", "abs": "https://arxiv.org/abs/2504.20995", "authors": ["Haoyu Zhen", "Qiao Sun", "Hongxin Zhang", "Junyan Li", "Siyuan Zhou", "Yilun Du", "Chuang Gan"], "title": "TesserAct: Learning 4D Embodied World Models", "categories": ["cs.CV", "cs.RO"], "comment": "Project Page: https://tesseractworld.github.io/", "summary": "This paper presents an effective approach for learning novel 4D embodied\nworld models, which predict the dynamic evolution of 3D scenes over time in\nresponse to an embodied agent's actions, providing both spatial and temporal\nconsistency. We propose to learn a 4D world model by training on RGB-DN (RGB,\nDepth, and Normal) videos. This not only surpasses traditional 2D models by\nincorporating detailed shape, configuration, and temporal changes into their\npredictions, but also allows us to effectively learn accurate inverse dynamic\nmodels for an embodied agent. Specifically, we first extend existing robotic\nmanipulation video datasets with depth and normal information leveraging\noff-the-shelf models. Next, we fine-tune a video generation model on this\nannotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for\neach frame. We then present an algorithm to directly convert generated RGB,\nDepth, and Normal videos into a high-quality 4D scene of the world. Our method\nensures temporal and spatial coherence in 4D scene predictions from embodied\nscenarios, enables novel view synthesis for embodied environments, and\nfacilitates policy learning that significantly outperforms those derived from\nprior video-based world models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20584", "pdf": "https://arxiv.org/pdf/2504.20584", "abs": "https://arxiv.org/abs/2504.20584", "authors": ["Martin Huber", "Huanyu Tian", "Christopher E. Mower", "Lucas-Raphael Mller", "Sbastien Ourselin", "Christos Bergeles", "Tom Vercauteren"], "title": "Hydra: Marker-Free RGB-D Hand-Eye Calibration", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This work presents an RGB-D imaging-based approach to marker-free hand-eye\ncalibration using a novel implementation of the iterative closest point (ICP)\nalgorithm with a robust point-to-plane (PTP) objective formulated on a Lie\nalgebra. Its applicability is demonstrated through comprehensive experiments\nusing three well known serial manipulators and two RGB-D cameras. With only\nthree randomly chosen robot configurations, our approach achieves approximately\n90% successful calibrations, demonstrating 2-3x higher convergence rates to the\nglobal optimum compared to both marker-based and marker-free baselines. We also\nreport 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9\nrobot configurations over other marker-free methods. Our method exhibits\nsignificantly improved accuracy (5 mm in task space) over classical approaches\n(7 mm in task space) whilst being marker-free. The benchmarking dataset and\ncode are open sourced under Apache 2.0 License, and a ROS 2 integration with\nrobot abstraction is provided to facilitate deployment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20658", "pdf": "https://arxiv.org/pdf/2504.20658", "abs": "https://arxiv.org/abs/2504.20658", "authors": ["Stefano Dell'Anna", "Andrea Montibeller", "Giulia Boato"], "title": "TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "AI-generated synthetic media are increasingly used in real-world scenarios,\noften with the purpose of spreading misinformation and propaganda through\nsocial media platforms, where compression and other processing can degrade fake\ndetection cues. Currently, many forensic tools fail to account for these\nin-the-wild challenges. In this work, we introduce TrueFake, a large-scale\nbenchmarking dataset of 600,000 images including top notch generative\ntechniques and sharing via three different social networks. This dataset allows\nfor rigorous evaluation of state-of-the-art fake image detectors under very\nrealistic and challenging conditions. Through extensive experimentation, we\nanalyze how social media sharing impacts detection performance, and identify\ncurrent most effective detection and training strategies. Our findings\nhighlight the need for evaluating forensic models in conditions that mirror\nreal-world use.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20923", "pdf": "https://arxiv.org/pdf/2504.20923", "abs": "https://arxiv.org/abs/2504.20923", "authors": ["Andrea Di Pierno", "Luca Guarnera", "Dario Allegra", "Sebastiano Battiato"], "title": "End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Audio deepfakes represent a growing threat to digital security and trust,\nleveraging advanced generative models to produce synthetic speech that closely\nmimics real human voices. Detecting such manipulations is especially\nchallenging under open-world conditions, where spoofing methods encountered\nduring testing may differ from those seen during training. In this work, we\npropose an end-to-end deep learning framework for audio deepfake detection that\noperates directly on raw waveforms. Our model, RawNetLite, is a lightweight\nconvolutional-recurrent architecture designed to capture both spectral and\ntemporal features without handcrafted preprocessing. To enhance robustness, we\nintroduce a training strategy that combines data from multiple domains and\nadopts Focal Loss to emphasize difficult or ambiguous samples. We further\ndemonstrate that incorporating codec-based manipulations and applying\nwaveform-level audio augmentations (e.g., pitch shifting, noise, and time\nstretching) leads to significant generalization improvements under realistic\nacoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on\nin-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging\nout-of-distribution test set (AVSpoof2021 + CodecFake). These findings\nhighlight the importance of diverse training data, tailored objective functions\nand audio augmentations in building resilient and generalizable audio forgery\ndetectors. Code and pretrained models are available at\nhttps://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20052", "pdf": "https://arxiv.org/pdf/2504.20052", "abs": "https://arxiv.org/abs/2504.20052", "authors": ["Floriane Magera", "Thomas Hoyoux", "Martin Castin", "Olivier Barnich", "Anthony Cioppa", "Marc Van Droogenbroeck"], "title": "Can Geometry Save Central Views for Sports Field Registration?", "categories": ["cs.CV"], "comment": "10 pages, 10 figures, 1 table, 40 references", "summary": "Single-frame sports field registration often serves as the foundation for\nextracting 3D information from broadcast videos, enabling applications related\nto sports analytics, refereeing, or fan engagement. As sports fields have\nrigorous specifications in terms of shape and dimensions of their line, circle\nand point components, sports field markings are commonly used as calibration\ntargets for this task. However, because of the sparse and uneven distribution\nof field markings, close-up camera views around central areas of the field\noften depict only line and circle markings. On these views, sports field\nregistration is challenging for the vast majority of existing methods, as they\nfocus on leveraging line field markings and their intersections. It is indeed a\nchallenge to include circle correspondences in a set of linear equations. In\nthis work, we propose a novel method to derive a set of points and lines from\ncircle correspondences, enabling the exploitation of circle correspondences for\nboth sports field registration and image annotation. In our experiments, we\nillustrate the benefits of our bottom-up geometric method against\ntop-performing detectors and show that our method successfully complements\nthem, enabling sports field registration in difficult scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20077", "pdf": "https://arxiv.org/pdf/2504.20077", "abs": "https://arxiv.org/abs/2504.20077", "authors": ["Manish Kansana", "Keyan Alexander Rahimi", "Elias Hossain", "Iman Dehzangi", "Noorbakhsh Amiri Golilarz"], "title": "Edge-Based Learning for Improved Classification Under Adversarial Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial noise introduces small perturbations in images, misleading deep\nlearning models into misclassification and significantly impacting recognition\naccuracy. In this study, we analyzed the effects of Fast Gradient Sign Method\n(FGSM) adversarial noise on image classification and investigated whether\ntraining on specific image features can improve robustness. We hypothesize that\nwhile adversarial noise perturbs various regions of an image, edges may remain\nrelatively stable and provide essential structural information for\nclassification. To test this, we conducted a series of experiments using brain\ntumor and COVID datasets. Initially, we trained the models on clean images and\nthen introduced subtle adversarial perturbations, which caused deep learning\nmodels to significantly misclassify the images. Retraining on a combination of\nclean and noisy images led to improved performance. To evaluate the robustness\nof the edge features, we extracted edges from the original/clean images and\ntrained the models exclusively on edge-based representations. When noise was\nintroduced to the images, the edge-based models demonstrated greater resilience\nto adversarial attacks compared to those trained on the original or clean\nimages. These results suggest that while adversarial noise is able to exploit\ncomplex non-edge regions significantly more than edges, the improvement in the\naccuracy after retraining is marginally more in the original data as compared\nto the edges. Thus, leveraging edge-based learning can improve the resilience\nof deep learning models against adversarial perturbations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20097", "pdf": "https://arxiv.org/pdf/2504.20097", "abs": "https://arxiv.org/abs/2504.20097", "authors": ["Junran Guo", "Tonglin Mu", "Keyuan Li", "Jianing Li", "Ziyang Luo", "Ye Chen", "Xiaodong Fan", "Jinquan Huang", "Minjie Liu", "Jinbei Zhang", "Ruoyang Qi", "Naiting Gu", "Shihai Sun"], "title": "Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments", "categories": ["cs.CV", "quant-ph"], "comment": "15 pages, 9 figures", "summary": "Detecting small objects, such as drones, over long distances presents a\nsignificant challenge with broad implications for security, surveillance,\nenvironmental monitoring, and autonomous systems. Traditional imaging-based\nmethods rely on high-resolution image acquisition, but are often constrained by\nrange, power consumption, and cost. In contrast, data-driven\nsingle-photon-single-pixel light detection and ranging\n(\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}) provides an\nimaging-free alternative, directly enabling target identification while\nreducing system complexity and cost. However, its detection range has been\nlimited to a few hundred meters. Here, we introduce a novel integration of\nresidual neural networks (ResNet) with\n\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}, incorporating a refined\nobservation model to extend the detection range to 5~\\si{\\kilo\\meter} in an\nintracity environment while enabling high-accuracy identification of drone\nposes and types. Experimental results demonstrate that our approach not only\noutperforms conventional imaging-based recognition systems, but also achieves\n94.93\\% pose identification accuracy and 97.99\\% type classification accuracy,\neven under weak signal conditions with long distances and low signal-to-noise\nratios (SNRs). These findings highlight the potential of imaging-free methods\nfor robust long-range detection of small targets in real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20051", "pdf": "https://arxiv.org/pdf/2504.20051", "abs": "https://arxiv.org/abs/2504.20051", "authors": ["Frances Laureano De Leon", "Harish Tayyar Madabushi", "Mark G. Lee"], "title": "Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Multiword expressions, characterised by non-compositional meanings and\nsyntactic irregularities, are an example of nuanced language. These expressions\ncan be used literally or idiomatically, leading to significant changes in\nmeaning. While large language models have demonstrated strong performance\nacross many tasks, their ability to handle such linguistic subtleties remains\nuncertain. Therefore, this study evaluates how state-of-the-art language models\nprocess the ambiguity of potentially idiomatic multiword expressions,\nparticularly in contexts that are less frequent, where models are less likely\nto rely on memorisation. By evaluating models across in Portuguese and\nGalician, in addition to English, and using a novel code-switched dataset and a\nnovel task, we find that large language models, despite their strengths,\nstruggle with nuanced language. In particular, we find that the latest models,\nincluding GPT-4, fail to outperform the xlm-roBERTa-base baselines in both\ndetection and semantic tasks, with especially poor performance on the novel\ntasks we introduce, despite its similarity to existing tasks. Overall, our\nresults demonstrate that multiword expressions, especially those which are\nambiguous, continue to be a challenge to models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20086", "pdf": "https://arxiv.org/pdf/2504.20086", "abs": "https://arxiv.org/abs/2504.20086", "authors": ["Sebastian Gehrmann", "Claire Huang", "Xian Teng", "Sergei Yurovski", "Iyanuoluwa Shode", "Chirag S. Patel", "Arjun Bhorkar", "Naveen Thomas", "John Doucette", "David Rosenberg", "Mark Dredze", "David Rabinowitz"], "title": "Understanding and Mitigating Risks of Generative AI in Financial Services", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to FAccT 2025", "summary": "To responsibly develop Generative AI (GenAI) products, it is critical to\ndefine the scope of acceptable inputs and outputs. What constitutes a \"safe\"\nresponse is an actively debated question. Academic work puts an outsized focus\non evaluating models by themselves for general purpose aspects such as\ntoxicity, bias, and fairness, especially in conversational applications being\nused by a broad audience. In contrast, less focus is put on considering\nsociotechnical systems in specialized domains. Yet, those specialized systems\ncan be subject to extensive and well-understood legal and regulatory scrutiny.\nThese product-specific considerations need to be set in industry-specific laws,\nregulations, and corporate governance requirements. In this paper, we aim to\nhighlight AI content safety considerations specific to the financial services\ndomain and outline an associated AI content risk taxonomy. We compare this\ntaxonomy to existing work in this space and discuss implications of risk\ncategory violations on various stakeholders. We evaluate how existing\nopen-source technical guardrail solutions cover this taxonomy by assessing them\non data collected via red-teaming activities. Our results demonstrate that\nthese guardrails fail to detect most of the content risks we discuss.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20178", "pdf": "https://arxiv.org/pdf/2504.20178", "abs": "https://arxiv.org/abs/2504.20178", "authors": ["Zhe Cui", "Yuli Li", "Le-Nam Tran"], "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals", "categories": ["cs.CV", "cs.LG"], "comment": "This paper was accepted at IEEE WCNC 2025", "summary": "Current crowd-counting models often rely on single-modal inputs, such as\nvisual images or wireless signal data, which can result in significant\ninformation loss and suboptimal recognition performance. To address these\nshortcomings, we propose TransFusion, a novel multimodal fusion-based\ncrowd-counting model that integrates Channel State Information (CSI) with image\ndata. By leveraging the powerful capabilities of Transformer networks,\nTransFusion effectively combines these two distinct data modalities, enabling\nthe capture of comprehensive global contextual information that is critical for\naccurate crowd estimation. However, while transformers are well capable of\ncapturing global features, they potentially fail to identify finer-grained,\nlocal details essential for precise crowd counting. To mitigate this, we\nincorporate Convolutional Neural Networks (CNNs) into the model architecture,\nenhancing its ability to extract detailed local features that complement the\nglobal context provided by the Transformer. Extensive experimental evaluations\ndemonstrate that TransFusion achieves high accuracy with minimal counting\nerrors while maintaining superior efficiency.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20168", "pdf": "https://arxiv.org/pdf/2504.20168", "abs": "https://arxiv.org/abs/2504.20168", "authors": ["Nishant Subramani", "Jason Eisner", "Justin Svegliato", "Benjamin Van Durme", "Yu Su", "Sam Thomson"], "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at NAACL 2025. Code:\n  https://github.com/microsoft/mice_for_cats", "summary": "Tool-using agents that act in the world need to be both useful and safe.\nWell-calibrated model confidences can be used to weigh the risk versus reward\nof potential actions, but prior work shows that many models are poorly\ncalibrated. Inspired by interpretability literature exploring the internals of\nmodels, we propose a novel class of model-internal confidence estimators (MICE)\nto better assess confidence when calling tools. MICE first decodes from each\nintermediate layer of the language model using logitLens and then computes\nsimilarity scores between each layer's generation and the final output. These\nfeatures are fed into a learned probabilistic classifier to assess confidence\nin the decoded output. On the simulated trial and error (STE) tool-calling\ndataset using Llama3 models, we find that MICE beats or matches the baselines\non smoothed expected calibration error. Using MICE confidences to determine\nwhether to call a tool significantly improves over strong baselines on a new\nmetric, expected tool-calling utility. Further experiments show that MICE is\nsample-efficient, can generalize zero-shot to unseen APIs, and results in\nhigher tool-calling utility in scenarios with varying risk levels. Our code is\nopen source, available at https://github.com/microsoft/mice_for_cats.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20179", "pdf": "https://arxiv.org/pdf/2504.20179", "abs": "https://arxiv.org/abs/2504.20179", "authors": ["Jingjing Wang", "Dan Zhang", "Joshua Luo", "Yin Yang", "Feng Luo"], "title": "Integration Flow Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Ordinary differential equation (ODE) based generative models have emerged as\na powerful approach for producing high-quality samples in many applications.\nHowever, the ODE-based methods either suffer the discretization error of\nnumerical solvers of ODE, which restricts the quality of samples when only a\nfew NFEs are used, or struggle with training instability. In this paper, we\nproposed Integration Flow, which directly learns the integral of ODE-based\ntrajectory paths without solving the ODE functions. Moreover, Integration Flow\nexplicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in\nguiding the reverse-time dynamics. We have theoretically proven this can\ncontribute to both stability and accuracy. To the best of our knowledge,\nIntegration Flow is the first model with a unified structure to estimate\nODE-based generative models and the first to show the exact straightness of\n1-Rectified Flow without reflow. Through theoretical analysis and empirical\nevaluations, we show that Integration Flows achieve improved performance when\nit is applied to existing ODE-based models, such as diffusion models, Rectified\nFlows, and PFGM++. Specifically, Integration Flow achieves one-step generation\non CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,\n3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet\nwith FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without\nreflow and 4.15 for PFGM++.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199", "abs": "https://arxiv.org/abs/2504.20199", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20203", "pdf": "https://arxiv.org/pdf/2504.20203", "abs": "https://arxiv.org/abs/2504.20203", "authors": ["Vladyslav Polushko", "Damjan Hatic", "Ronald Rsch", "Thomas Mrz", "Markus Rauhut", "Andreas Weinmann"], "title": "Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Floods cause serious problems around the world. Responding quickly and\neffectively requires accurate and timely information about the affected areas.\nThe effective use of Remote Sensing images for accurate flood detection\nrequires specific detection methods. Typically, Deep Neural Networks are\nemployed, which are trained on specific datasets. For the purpose of river\nflood detection in RGB imagery, we use the BlessemFlood21 dataset. We here\nexplore the use of different augmentation strategies, ranging from basic\napproaches to more complex techniques, including optical distortion. By\nidentifying effective strategies, we aim to refine the training process of\nstate-of-the-art Deep Learning segmentation networks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20222", "pdf": "https://arxiv.org/pdf/2504.20222", "abs": "https://arxiv.org/abs/2504.20222", "authors": ["Naoko Sawada", "Pedro Miraldo", "Suhas Lohit", "Tim K. Marks", "Moitreya Chatterjee"], "title": "FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 CV4Metaverse Workshop", "summary": "Neural implicit surface representation techniques are in high demand for\nadvancing technologies in augmented reality/virtual reality, digital twins,\nautonomous navigation, and many other fields. With their ability to model\nobject surfaces in a scene as a continuous function, such techniques have made\nremarkable strides recently, especially over classical 3D surface\nreconstruction methods, such as those that use voxels or point clouds. However,\nthese methods struggle with scenes that have varied and complex surfaces\nprincipally because they model any given scene with a single encoder network\nthat is tasked to capture all of low through high-surface frequency information\nin the scene simultaneously. In this work, we propose a novel, neural implicit\nsurface representation approach called FreBIS to overcome this challenge.\nFreBIS works by stratifying the scene based on the frequency of surfaces into\nmultiple frequency levels, with each level (or a group of levels) encoded by a\ndedicated encoder. Moreover, FreBIS encourages these encoders to capture\ncomplementary information by promoting mutual dissimilarity of the encoded\nfeatures via a novel, redundancy-aware weighting module. Empirical evaluations\non the challenging BlendedMVS dataset indicate that replacing the standard\nencoder in an off-the-shelf neural surface reconstruction method with our\nfrequency-stratified encoders yields significant improvements. These\nenhancements are evident both in the quality of the reconstructed 3D surfaces\nand in the fidelity of their renderings from any viewpoint.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20304", "pdf": "https://arxiv.org/pdf/2504.20304", "abs": "https://arxiv.org/abs/2504.20304", "authors": ["Xiulin Yang", "Zhuoxuan Ju", "Lanni Bu", "Zoey Liu", "Nathan Schneider"], "title": "UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "CHILDES is a widely used resource of transcribed child and child-directed\nspeech. This paper introduces UD-English-CHILDES, the first officially released\nUniversal Dependencies (UD) treebank derived from previously\ndependency-annotated CHILDES data with consistent and unified annotation\nguidelines. Our corpus harmonizes annotations from 11 children and their\ncaregivers, totaling over 48k sentences. We validate existing gold-standard\nannotations under the UD v2 framework and provide an additional 1M\nsilver-standard sentences, offering a consistent resource for computational and\nlinguistic research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20241", "pdf": "https://arxiv.org/pdf/2504.20241", "abs": "https://arxiv.org/abs/2504.20241", "authors": ["Kamirul Kamirul", "Odysseas Pappas", "Alin Achim"], "title": "Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts", "categories": ["cs.CV"], "comment": "4 pages; Submitted Machine Intelligence for GeoAnalytics and Remote\n  Sensing (MIGARS) - 2025", "summary": "Detecting ship presence via wake signatures in SAR imagery is attracting\nconsiderable research interest, but limited annotated data availability poses\nsignificant challenges for supervised learning. Physics-based simulations are\ncommonly used to address this data scarcity, although they are slow and\nconstrain end-to-end learning. In this work, we explore a new direction for\nmore efficient and end-to-end SAR ship wake simulation using a diffusion model\ntrained on data generated by a physics-based simulator. The training dataset is\nbuilt by pairing images produced by the simulator with text prompts derived\nfrom simulation parameters. Experimental result show that the model generates\nrealistic Kelvin wake patterns and achieves significantly faster inference than\nthe physics-based simulator. These results highlight the potential of diffusion\nmodels for fast and controllable wake image generation, opening new\npossibilities for end-to-end downstream tasks in maritime SAR analysis.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20303", "pdf": "https://arxiv.org/pdf/2504.20303", "abs": "https://arxiv.org/abs/2504.20303", "authors": ["Junlin Guo", "James R. Zimmer-Dauphinee", "Jordan M. Nieusma", "Siqi Lu", "Quan Liu", "Ruining Deng", "Can Cui", "Jialin Yue", "Yizhe Lin", "Tianyuan Yao", "Juming Xiong", "Junchao Zhu", "Chongyu Qu", "Yuechen Yang", "Mitchell Wilkes", "Xiao Wang", "Parker VanValkenburgh", "Steven A. Wernke", "Yuankai Huo"], "title": "DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes", "categories": ["cs.CV"], "comment": null, "summary": "By mapping sites at large scales using remotely sensed data, archaeologists\ncan generate unique insights into long-term demographic trends, inter-regional\nsocial networks, and past adaptations to climate change. Remote sensing surveys\ncomplement field-based approaches, and their reach can be especially great when\ncombined with deep learning and computer vision techniques. However,\nconventional supervised deep learning methods face challenges in annotating\nfine-grained archaeological features at scale. While recent vision foundation\nmodels have shown remarkable success in learning large-scale remote sensing\ndata with minimal annotations, most off-the-shelf solutions are designed for\nRGB images rather than multi-spectral satellite imagery, such as the 8-band\ndata used in our study. In this paper, we introduce DeepAndes, a\ntransformer-based vision foundation model trained on three million\nmulti-spectral satellite images, specifically tailored for Andean archaeology.\nDeepAndes incorporates a customized DINOv2 self-supervised learning algorithm\noptimized for 8-band multi-spectral imagery, marking the first foundation model\ndesigned explicitly for the Andes region. We evaluate its image understanding\nperformance through imbalanced image classification, image instance retrieval,\nand pixel-level semantic segmentation tasks. Our experiments show that\nDeepAndes achieves superior F1 scores, mean average precision, and Dice scores\nin few-shot learning scenarios, significantly outperforming models trained from\nscratch or pre-trained on smaller datasets. This underscores the effectiveness\nof large-scale self-supervised pre-training in archaeological remote sensing.\nCodes will be available on https://github.com/geopacha/DeepAndes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20451", "pdf": "https://arxiv.org/pdf/2504.20451", "abs": "https://arxiv.org/abs/2504.20451", "authors": ["Daniel Lee", "Harsh Sharma", "Jieun Han", "Sunny Jeong", "Alice Oh", "Vered Shwartz"], "title": "Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at SemEval-2025 Workshop (ACL 2025)", "summary": "Translating knowledge-intensive and entity-rich text between English and\nKorean requires transcreation to preserve language-specific and cultural\nnuances beyond literal, phonetic or word-for-word conversion. We evaluate 13\nmodels (LLMs and MT models) using automatic metrics and human assessment by\nbilingual annotators. Our findings show LLMs outperform traditional MT systems\nbut struggle with entity translation requiring cultural adaptation. By\nconstructing an error taxonomy, we identify incorrect responses and entity name\nerrors as key issues, with performance varying by entity type and popularity\nlevel. This work exposes gaps in automatic evaluation metrics and hope to\nenable future work in completing culturally-nuanced machine translation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20500", "pdf": "https://arxiv.org/pdf/2504.20500", "abs": "https://arxiv.org/abs/2504.20500", "authors": ["Huimin Lu", "Masaru Isonuma", "Junichiro Mori", "Ichiro Sakata"], "title": "UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICLR 2025 (poster)", "summary": "We present UniDetox, a universally applicable method designed to mitigate\ntoxicity across various large language models (LLMs). Previous detoxification\nmethods are typically model-specific, addressing only individual models or\nmodel families, and require careful hyperparameter tuning due to the trade-off\nbetween detoxification efficacy and language modeling performance. In contrast,\nUniDetox provides a detoxification technique that can be universally applied to\na wide range of LLMs without the need for separate model-specific tuning.\nSpecifically, we propose a novel and efficient dataset distillation technique\nfor detoxification using contrastive decoding. This approach distills\ndetoxifying representations in the form of synthetic text data, enabling\nuniversal detoxification of any LLM through fine-tuning with the distilled\ntext. Our experiments demonstrate that the detoxifying text distilled from\nGPT-2 can effectively detoxify larger models, including OPT, Falcon, and\nLLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter\ntuning for each model, as a single hyperparameter configuration can be\nseamlessly applied across different models. Additionally, analysis of the\ndetoxifying text reveals a reduction in politically biased content, providing\ninsights into the attributes necessary for effective detoxification of LLMs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20552", "pdf": "https://arxiv.org/pdf/2504.20552", "abs": "https://arxiv.org/abs/2504.20552", "authors": ["Baz Roland", "Kristina Malyseva", "Anna Pappa", "Tristan Cazenave"], "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters", "categories": ["cs.CL"], "comment": null, "summary": "This project introduces BrAIcht, an AI conversational agent that creates\ndialogues in the distinctive style of the famous German playwright Bertolt\nBrecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7\nbillion parameters and a modified version of the base Llama2 suitable for\nGerman language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of\nother German plays that are stylistically similar to Bertolt Brecht are used to\nform a more di-erse dataset. Due to the limited memory capacity, a\nparameterefficient fine-tuning technique called QLoRA is implemented to train\nthe large language model. The results, based on BLEU score and perplexity, show\nvery promising performance of BrAIcht in generating dialogues in the style of\nBertolt Brecht.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20409", "pdf": "https://arxiv.org/pdf/2504.20409", "abs": "https://arxiv.org/abs/2504.20409", "authors": ["Jingfeng Guo", "Jinnan Chen", "Weikai Chen", "Zhenyu Sun", "Lanjiong Li", "Baozhu Zhao", "Lingting Zhu", "Xin Wang", "Qi Liu"], "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation", "categories": ["cs.CV"], "comment": null, "summary": "This work presents GarmentX, a novel framework for generating diverse,\nhigh-fidelity, and wearable 3D garments from a single input image. Traditional\ngarment reconstruction methods directly predict 2D pattern edges and their\nconnectivity, an overly unconstrained approach that often leads to severe\nself-intersections and physically implausible garment structures. In contrast,\nGarmentX introduces a structured and editable parametric representation\ncompatible with GarmentCode, ensuring that the decoded sewing patterns always\nform valid, simulation-ready 3D garments while allowing for intuitive\nmodifications of garment shape and style. To achieve this, we employ a masked\nautoregressive model that sequentially predicts garment parameters, leveraging\nautoregressive modeling for structured generation while mitigating\ninconsistencies in direct pattern prediction. Additionally, we introduce\nGarmentX dataset, a large-scale dataset of 378,682 garment parameter-image\npairs, constructed through an automatic data generation pipeline that\nsynthesizes diverse and high-quality garment images conditioned on parametric\ngarment representations. Through integrating our method with GarmentX dataset,\nwe achieve state-of-the-art performance in geometric fidelity and input image\nalignment, significantly outperforming prior approaches. We will release\nGarmentX dataset upon publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20468", "pdf": "https://arxiv.org/pdf/2504.20468", "abs": "https://arxiv.org/abs/2504.20468", "authors": ["Yuanchen Wu", "Lu Zhang", "Hang Yao", "Junlong Du", "Ke Yan", "Shouhong Ding", "Yunsheng Wu", "Xiaoqiang Li"], "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20509", "pdf": "https://arxiv.org/pdf/2504.20509", "abs": "https://arxiv.org/abs/2504.20509", "authors": ["Yichu Xu", "Di Wang", "Hongzan Jiao", "Lefei Zhang", "Liangpei Zhang"], "title": "MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "The Mamba model has recently demonstrated strong potential in hyperspectral\nimage (HSI) classification, owing to its ability to perform context modeling\nwith linear computational complexity. However, existing Mamba-based methods\nusually neglect the spectral and spatial directional characteristics related to\nheterogeneous objects in hyperspectral scenes, leading to limited\nclassification performance. To address these issues, we propose MambaMoE, a\nnovel spectral-spatial mixture-of-experts framework, representing the first\nMoE-based approach in the HSI classification community. Specifically, we design\na Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation\nto enable adaptive spectral-spatial modeling. Furthermore, we introduce an\nuncertainty-guided corrective learning (UGCL) strategy to encourage the model's\nattention toward complex regions prone to prediction ambiguity. Extensive\nexperiments on multiple public HSI benchmarks demonstrate that MambaMoE\nachieves state-of-the-art performance in both accuracy and efficiency compared\nto existing advanced approaches, especially for Mamba-based methods. Code will\nbe released.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734", "abs": "https://arxiv.org/abs/2504.20734", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20769", "pdf": "https://arxiv.org/pdf/2504.20769", "abs": "https://arxiv.org/abs/2504.20769", "authors": ["Wenxiao Wang", "Parsa Hosseini", "Soheil Feizi"], "title": "Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought prompting has demonstrated great success in facilitating the\nreasoning abilities of large language models. In this work, we explore how\nthese enhanced reasoning abilities can be exploited to improve the robustness\nof large language models in tasks that are not necessarily reasoning-focused.\nIn particular, we show how a wide range of large language models exhibit\nsignificantly improved robustness against reference corruption using a simple\nmethod called chain-of-defensive-thought, where only a few exemplars with\nstructured and defensive reasoning are provided as demonstrations. Empirically,\nthe improvements can be astounding, especially given the simplicity and\napplicability of the method. For example, in the Natural Questions task, the\naccuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting\nwhen 1 out of 10 references provided is corrupted with prompt injection\nattacks. In contrast, GPT-4o using chain-of-defensive-thought prompting\nmaintains an accuracy of 50%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20525", "pdf": "https://arxiv.org/pdf/2504.20525", "abs": "https://arxiv.org/abs/2504.20525", "authors": ["Huan Zheng", "Wencheng Han", "Tianyi Yan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D lane detection aims to estimate 3D position of lanes from\nfrontal-view (FV) images. However, current monocular 3D lane detection methods\nsuffer from two limitations, including inaccurate geometric information of the\npredicted 3D lanes and difficulties in maintaining lane integrity. To address\nthese issues, we seek to fully exploit the potential of multiple input frames.\nFirst, we aim at enhancing the ability to perceive the geometry of scenes by\nleveraging temporal geometric consistency. Second, we strive to improve the\nintegrity of lanes by revealing more instance information from temporal\nsequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation\nNetwork (GTA-Net) for monocular 3D lane detection. On one hand, we develop the\nTemporal Geometry Enhancement Module (TGEM), which exploits geometric\nconsistency across successive frames, facilitating effective geometry\nperception. On the other hand, we present the Temporal Instance-aware Query\nGeneration (TIQG), which strategically incorporates temporal cues into query\ngeneration, thereby enabling the exploration of comprehensive instance\ninformation. Experiments demonstrate that our GTA-Net achieves SoTA results,\nsurpassing existing monocular 3D lane detection solutions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20849", "pdf": "https://arxiv.org/pdf/2504.20849", "abs": "https://arxiv.org/abs/2504.20849", "authors": ["Anum Afzal", "Alexandre Mercier", "Florian Matthes"], "title": "JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry", "categories": ["cs.CL"], "comment": null, "summary": "Online platforms are increasingly interested in using Data-to-Text\ntechnologies to generate content and help their users. Unfortunately,\ntraditional generative methods often fall into repetitive patterns, resulting\nin monotonous galleries of texts after only a few iterations. In this paper, we\ninvestigate LLM-based data-to-text approaches to automatically generate\nmarketing texts that are of sufficient quality and diverse enough for broad\nadoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in\nconjunction with fine-tuning, few-shot, and zero-shot approaches to set a\nbaseline for diverse marketing texts. We also introduce a metric JaccDiv to\nevaluate the diversity of a set of texts. This research extends its relevance\nbeyond the music industry, proving beneficial in various fields where\nrepetitive automated content generation is prevalent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20602", "pdf": "https://arxiv.org/pdf/2504.20602", "abs": "https://arxiv.org/abs/2504.20602", "authors": ["Siwei Wang", "Zhiwei Chen", "Liujuan Cao", "Rongrong Ji"], "title": "Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection is a broadly investigated research task and is\ncommonly conceptualized as a \"pipeline-style\" engineering process. In the\nupstream, images serve as raw materials for processing in the detection\npipeline, where pre-trained models are employed to generate initial feature\nmaps. In the midstream, an assigner selects training positive and negative\nsamples. Subsequently, these samples and features are fed into the downstream\nfor classification and regression. Previous small object detection methods\noften focused on improving isolated stages of the pipeline, thereby neglecting\nholistic optimization and consequently constraining overall performance gains.\nTo address this issue, we have optimized three key aspects, namely Purifying,\nLabeling, and Utilizing, in this pipeline, proposing a high-quality Small\nobject detection framework termed PLUSNet. Specifically, PLUSNet comprises\nthree sequential components: the Hierarchical Feature Purifier (HFP) for\npurifying upstream features, the Multiple Criteria Label Assignment (MCLA) for\nimproving the quality of midstream training samples, and the Frequency\nDecoupled Head (FDHead) for more effectively exploiting information to\naccomplish downstream tasks. The proposed PLUS modules are readily integrable\ninto various object detectors, thus enhancing their detection capabilities in\nmulti-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet\nconsistently achieves significant and consistent improvements across multiple\ndatasets for small object detection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20607", "pdf": "https://arxiv.org/pdf/2504.20607", "abs": "https://arxiv.org/abs/2504.20607", "authors": ["Hao Tian", "Rui Liu", "Wen Shen", "Yilong Hu", "Zhihao Zheng", "Xiaolin Qin"], "title": "EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian", "categories": ["cs.CV"], "comment": "11 pages, 3 figures", "summary": "3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in\nscene reconstruction and novel view synthesis. Recent work on reconstructing\nthe 3D human body using 3DGS attempts to leverage prior information on human\npose to enhance rendering quality and improve training speed. However, it\nstruggles to effectively fit dynamic surface planes due to multi-view\ninconsistency and redundant Gaussians. This inconsistency arises because\nGaussian ellipsoids cannot accurately represent the surfaces of dynamic\nobjects, which hinders the rapid reconstruction of the dynamic human body.\nMeanwhile, the prevalence of redundant Gaussians means that the training time\nof these works is still not ideal for quickly fitting a dynamic human body. To\naddress these, we propose EfficientHuman, a model that quickly accomplishes the\ndynamic reconstruction of the human body using Articulated 2D Gaussian while\nensuring high rendering quality. The key innovation involves encoding Gaussian\nsplats as Articulated 2D Gaussian surfels in canonical space and then\ntransforming them to pose space via Linear Blend Skinning (LBS) to achieve\nefficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian\nsurfels can quickly conform to the dynamic human body while ensuring\nview-consistent geometries. Additionally, we introduce a pose calibration\nmodule and an LBS optimization module to achieve precise fitting of dynamic\nhuman poses, enhancing the model's performance. Extensive experiments on the\nZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic\nhuman reconstruction in less than a minute on average, which is 20 seconds\nfaster than the current state-of-the-art method, while also reducing the number\nof redundant Gaussians.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20059", "pdf": "https://arxiv.org/pdf/2504.20059", "abs": "https://arxiv.org/abs/2504.20059", "authors": ["Joey Chan", "Qiao Jin", "Nicholas Wan", "Charalampos S. Floudas", "Elisabetta Xue", "Zhiyong Lu"], "title": "Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "10 pages with 2 figures and 2 tables", "summary": "Clinical trials are crucial for assessing new treatments; however,\nrecruitment challenges - such as limited awareness, complex eligibility\ncriteria, and referral barriers - hinder their success. With the growth of\nonline platforms, patients increasingly turn to social media and health\ncommunities for support, research, and advocacy, expanding recruitment pools\nand established enrollment pathways. Recognizing this potential, we utilized\nTrialGPT, a framework that leverages a large language model (LLM) as its\nbackbone, to match 50 online patient cases (collected from published case\nreports and a social media website) to clinical trials and evaluate performance\nagainst traditional keyword-based searches. Our results show that TrialGPT\noutperforms traditional methods by 46% in identifying eligible trials, with\neach patient, on average, being eligible for around 7 trials. Additionally, our\noutreach efforts to case authors and trial organizers regarding these\npatient-trial matches yielded highly positive feedback, which we present from\nboth perspectives.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20657", "pdf": "https://arxiv.org/pdf/2504.20657", "abs": "https://arxiv.org/abs/2504.20657", "authors": ["Alex Michie", "Simon J Doran"], "title": "Image deidentification in the XNAT ecosystem: use cases and solutions", "categories": ["cs.CV", "J.3"], "comment": "For submission to MELBA (Machine Learning for Biomedical Imaging)\n  special issue on the MIDI-B deidentification challenge\n  (https://www.synapse.org/Synapse:syn53065760). 11 pages, 1 fig, 2 tables; 1\n  supplementary data file (supplementary_tables_S1_S2_S3.xlsx) containing three\n  spreadsheet tabs", "summary": "XNAT is a server-based data management platform widely used in academia for\ncurating large databases of DICOM images for research projects. We describe in\ndetail a deidentification workflow for DICOM data using facilities in XNAT,\ntogether with independent tools in the XNAT \"ecosystem\". We list different\ncontexts in which deidentification might be needed, based on our prior\nexperience. The starting point for participation in the Medical Image\nDe-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local\nmethodologies, which were adapted during the validation phase of the challenge.\nOur result in the test phase was 97.91\\%, considerably lower than our peers,\ndue largely to an arcane technical incompatibility of our methodology with the\nchallenge's Synapse platform, which prevented us receiving feedback during the\nvalidation phase. Post-submission, additional discrepancy reports from the\norganisers and via the MIDI-B Continuous Benchmarking facility, enabled us to\nimprove this score significantly to 99.61\\%. An entirely rule-based approach\nwas shown to be capable of removing all name-related information in the test\ncorpus, but exhibited failures in dealing fully with address data. Initial\nexperiments using published machine-learning models to remove addresses were\npartially successful but showed the models to be \"over-aggressive\" on other\ntypes of free-text data, leading to a slight overall degradation in performance\nto 99.54\\%. Future development will therefore focus on improving\naddress-recognition capabilities, but also on better removal of identifiable\ndata burned into the image pixels. Several technical aspects relating to the\n\"answer key\" are still under discussion with the challenge organisers, but we\nestimate that our percentage of genuine deidentification failures on the MIDI-B\ntest corpus currently stands at 0.19\\%. (Abridged from original for arXiv\nsubmission)", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073", "abs": "https://arxiv.org/abs/2504.20073", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20670", "pdf": "https://arxiv.org/pdf/2504.20670", "abs": "https://arxiv.org/abs/2504.20670", "authors": ["Yao Xiao", "Tingfa Xu", "Yu Xin", "Jianan Li"], "title": "FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "Embedded flight devices with visual capabilities have become essential for a\nwide range of applications. In aerial image detection, while many existing\nmethods have partially addressed the issue of small target detection,\nchallenges remain in optimizing small target detection and balancing detection\naccuracy with efficiency. These issues are key obstacles to the advancement of\nreal-time aerial image detection. In this paper, we propose a new family of\nreal-time detectors for aerial image detection, named FBRT-YOLO, to address the\nimbalance between detection accuracy and efficiency. Our method comprises two\nlightweight modules: Feature Complementary Mapping Module (FCM) and\nMulti-Kernel Perception Unit(MKP), designed to enhance object perception for\nsmall targets in aerial images. FCM focuses on alleviating the problem of\ninformation imbalance caused by the loss of small target information in deep\nnetworks. It aims to integrate spatial positional information of targets more\ndeeply into the network,better aligning with semantic information in the deeper\nlayers to improve the localization of small targets. We introduce MKP, which\nleverages convolutions with kernels of different sizes to enhance the\nrelationships between targets of various scales and improve the perception of\ntargets at different scales. Extensive experimental results on three major\naerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that\nFBRT-YOLO outperforms various real-time detectors in terms of performance and\nspeed.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20199", "pdf": "https://arxiv.org/pdf/2504.20199", "abs": "https://arxiv.org/abs/2504.20199", "authors": ["Juntian Zhang", "Chuanqi cheng", "Yuhan Liu", "Wei Liu", "Jian Luan", "Rui Yan"], "title": "Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) achieve remarkable success in single-image\ntasks. However, real-world scenarios often involve intricate multi-image\ninputs, leading to a notable performance decline as models struggle to\ndisentangle critical information scattered across complex visual features. In\nthis work, we propose Focus-Centric Visual Chain, a novel paradigm that\nenhances VLMs'perception, comprehension, and reasoning abilities in multi-image\nscenarios. To facilitate this paradigm, we propose Focus-Centric Data\nSynthesis, a scalable bottom-up approach for synthesizing high-quality data\nwith elaborate reasoning paths. Through this approach, We construct VISC-150K,\na large-scale dataset with reasoning data in the form of Focus-Centric Visual\nChain, specifically designed for multi-image tasks. Experimental results on\nseven multi-image benchmarks demonstrate that our method achieves average\nperformance gains of 3.16% and 2.24% across two distinct model architectures,\nwithout compromising the general vision-language capabilities. our study\nrepresents a significant step toward more robust and capable vision-language\nsystems that can handle complex visual scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20458", "pdf": "https://arxiv.org/pdf/2504.20458", "abs": "https://arxiv.org/abs/2504.20458", "authors": ["Xiaolei Wang", "Chunxuan Xia", "Junyi Li", "Fanzhe Meng", "Lei Huang", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations. A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations. User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge. While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience.\n  To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs. The simulated user\nprovides feedback to the items recommended by CRSs, enabling them to better\ncapture intricate user preferences through multi-turn interaction. Inspired by\ngenerative reward models, we design two types of feedback actions for the\nsimulated user: i.e., generative item scoring, which offers coarse-grained\nfeedback, and attribute-based item critique, which provides fine-grained\nfeedback. To ensure seamless integration, these feedback actions are unified\ninto an instruction-based format, allowing the development of a unified\nsimulated user via instruction tuning on synthesized data. With this simulated\nuser, automatic multi-turn interaction with CRSs can be effectively conducted.\nFurthermore, to strike a balance between effectiveness and efficiency, we draw\ninspiration from the paradigm of reward-guided search in complex reasoning\ntasks and employ beam search for the interaction process. On top of this, we\npropose an efficient candidate ranking method to improve the recommendation\nresults derived from interaction. Extensive experiments on public datasets\ndemonstrate the effectiveness, efficiency, and transferability of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20829", "pdf": "https://arxiv.org/pdf/2504.20829", "abs": "https://arxiv.org/abs/2504.20829", "authors": ["Jiaxin Hong", "Sixu Chen", "Shuoyang Sun", "Hongyao Yu", "Hao Fang", "Yuqi Tan", "Bin Chen", "Shuhan Qi", "Jiawei Li"], "title": "GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene\nrepresentation and novel view synthesis, its rapid adoption in safety-critical\ndomains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of\npotential security vulnerabilities. This paper presents the first systematic\nstudy of backdoor threats in 3DGS pipelines. We identify that adversaries may\nimplant backdoor views to induce malicious scene confusion during inference,\npotentially leading to environmental misperception in autonomous navigation or\nspatial distortion in immersive environments. To uncover this risk, we propose\nGuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap\ninjects malicious views at specific attack viewpoints while preserving\nhigh-quality rendering in non-target views, ensuring minimal detectability and\nmaximizing potential harm. Specifically, the proposed method consists of a\nthree-stage pipeline (attack, stabilization, and normal training) to implant\nstealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing\nattack efficacy and perceptual realism to expose security risks in 3D\nrendering. Extensive experiments on both synthetic and real-world datasets\ndemonstrate that GuassTrap can effectively embed imperceptible yet harmful\nbackdoor views while maintaining high-quality rendering in normal views,\nvalidating its robustness, adaptability, and practical applicability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20830", "pdf": "https://arxiv.org/pdf/2504.20830", "abs": "https://arxiv.org/abs/2504.20830", "authors": ["Jianyu Wu", "Yizhou Wang", "Xiangyu Yue", "Xinzhu Ma", "Jingyang Guo", "Dongzhan Zhou", "Wanli Ouyang", "Shixiang Tang"], "title": "CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation", "categories": ["cs.CV"], "comment": null, "summary": "While accurate and user-friendly Computer-Aided Design (CAD) is crucial for\nindustrial design and manufacturing, existing methods still struggle to achieve\nthis due to their over-simplified representations or architectures incapable of\nsupporting multimodal design requirements. In this paper, we attempt to tackle\nthis problem from both methods and datasets aspects. First, we propose a\ncascade MAR with topology predictor (CMT), the first multimodal framework for\nCAD generation based on Boundary Representation (B-Rep). Specifically, the\ncascade MAR can effectively capture the ``edge-counters-surface'' priors that\nare essential in B-Reps, while the topology predictor directly estimates\ntopology in B-Reps from the compact tokens in MAR. Second, to facilitate\nlarge-scale training, we develop a large-scale multimodal CAD dataset, mmABC,\nwhich includes over 1.3 million B-Rep models with multimodal annotations,\nincluding point clouds, text descriptions, and multi-view images. Extensive\nexperiments show the superior of CMT in both conditional and unconditional CAD\ngeneration tasks. For example, we improve Coverage and Valid ratio by +10.68%\nand +10.3%, respectively, compared to state-of-the-art methods on ABC in\nunconditional generation. CMT also improves +4.01 Chamfer on image conditioned\nCAD generation on mmABC. The dataset, code and pretrained network shall be\nreleased.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20595", "pdf": "https://arxiv.org/pdf/2504.20595", "abs": "https://arxiv.org/abs/2504.20595", "authors": ["Rulin Shao", "Rui Qiao", "Varsha Kishore", "Niklas Muennighoff", "Xi Victoria Lin", "Daniela Rus", "Bryan Kian Hsiang Low", "Sewon Min", "Wen-tau Yih", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "ReasonIR: Training Retrievers for Reasoning Tasks", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Our code is released at\n  \\url{https://github.com/facebookresearch/ReasonIR}", "summary": "We present ReasonIR-8B, the first retriever specifically trained for general\nreasoning tasks. Existing retrievers have shown limited gains on reasoning\ntasks, in part because existing training datasets focus on short factual\nqueries tied to documents that straightforwardly answer them. We develop a\nsynthetic data generation pipeline that, for each document, our pipeline\ncreates a challenging and relevant query, along with a plausibly related but\nultimately unhelpful hard negative. By training on a mixture of our synthetic\ndata and existing public data, ReasonIR-8B achieves a new state-of-the-art of\n29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a\nwidely-used reasoning-intensive information retrieval (IR) benchmark. When\napplied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%\nand 22.6% respectively, relative to the closed-book baseline, outperforming\nother retrievers and search engines. In addition, ReasonIR-8B uses test-time\ncompute more effectively: on BRIGHT, its performance consistently increases\nwith longer and more information-rich rewritten queries; it continues to\noutperform other retrievers when combined with an LLM reranker. Our training\nrecipe is general and can be easily extended to future LLMs; to this end, we\nopen-source our code, data, and model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20859", "pdf": "https://arxiv.org/pdf/2504.20859", "abs": "https://arxiv.org/abs/2504.20859", "authors": ["Guy Hadad", "Haggai Roitman", "Yotam Eshel", "Bracha Shapira", "Lior Rokach"], "title": "X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in SIGIR '25", "summary": "As new products are emerging daily, recommendation systems are required to\nquickly adapt to possible new domains without needing extensive retraining.\nThis work presents ``X-Cross'' -- a novel cross-domain\nsequential-recommendation model that recommends products in new domains by\nintegrating several domain-specific language models; each model is fine-tuned\nwith low-rank adapters (LoRA). Given a recommendation prompt, operating layer\nby layer, X-Cross dynamically refines the representation of each source\nlanguage model by integrating knowledge from all other models. These refined\nrepresentations are propagated from one layer to the next, leveraging the\nactivations from each domain adapter to ensure domain-specific nuances are\npreserved while enabling adaptability across domains. Using Amazon datasets for\nsequential recommendation, X-Cross achieves performance comparable to a model\nthat is fine-tuned with LoRA, while using only 25% of the additional\nparameters. In cross-domain tasks, such as adapting from Toys domain to Tools,\nElectronics or Sports, X-Cross demonstrates robust performance, while requiring\nabout 50%-75% less fine-tuning data than LoRA to make fine-tuning effective.\nFurthermore, X-Cross achieves significant improvement in accuracy over\nalternative cross-domain baselines. Overall, X-Cross enables scalable and\nadaptive cross-domain recommendations, reducing computational overhead and\nproviding an efficient solution for data-constrained environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet stn", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20902", "pdf": "https://arxiv.org/pdf/2504.20902", "abs": "https://arxiv.org/abs/2504.20902", "authors": ["Quentin Guimard", "Moreno D'Inc", "Massimiliano Mancini", "Elisa Ricci"], "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025. Code: https://github.com/mardgui/C2B", "summary": "A person downloading a pre-trained model from the web should be aware of its\nbiases. Existing approaches for bias identification rely on datasets containing\nlabels for the task of interest, something that a non-expert may not have\naccess to, or may not have the necessary resources to collect: this greatly\nlimits the number of tasks where model biases can be identified. In this work,\nwe present Classifier-to-Bias (C2B), the first bias discovery framework that\nworks without access to any labeled data: it only relies on a textual\ndescription of the classification task to identify biases in the target\nclassification model. This description is fed to a large language model to\ngenerate bias proposals and corresponding captions depicting biases together\nwith task-specific target labels. A retrieval model collects images for those\ncaptions, which are then used to assess the accuracy of the model w.r.t. the\ngiven biases. C2B is training-free, does not require any annotations, has no\nconstraints on the list of biases, and can be applied to any pre-trained model\non any classification task. Experiments on two publicly available datasets show\nthat C2B discovers biases beyond those of the original datasets and outperforms\na recent state-of-the-art bias detection baseline that relies on task-specific\nannotations, being a promising first step toward addressing task-agnostic\nunsupervised bias detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20970", "pdf": "https://arxiv.org/pdf/2504.20970", "abs": "https://arxiv.org/abs/2504.20970", "authors": ["Mete Erdogan", "Sebnem Demirtas"], "title": "SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Preprint submitted to IEEE International Workshop on Machine Learning\n  for Signal Processing (MLSP), 2025", "summary": "Accurate and early diagnosis of pneumonia through X-ray imaging is essential\nfor effective treatment and improved patient outcomes. Recent advancements in\nmachine learning have enabled automated diagnostic tools that assist\nradiologists in making more reliable and efficient decisions. In this work, we\npropose a Singular Value Decomposition-based Least Squares (SVD-LS) framework\nfor multi-class pneumonia classification, leveraging powerful feature\nrepresentations from state-of-the-art self-supervised and transfer learning\nmodels. Rather than relying on computationally expensive gradient based\nfine-tuning, we employ a closed-form, non-iterative classification approach\nthat ensures efficiency without compromising accuracy. Experimental results\ndemonstrate that SVD-LS achieves competitive performance while offering\nsignificantly reduced computational costs, making it a viable alternative for\nreal-time medical imaging applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20405", "pdf": "https://arxiv.org/pdf/2504.20405", "abs": "https://arxiv.org/abs/2504.20405", "authors": ["Sahil Sethi", "Sai Reddy", "Mansi Sakarvadia", "Jordan Serotte", "Darlington Nwaudo", "Nicholas Maassen", "Lewis Shi"], "title": "SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "While deep learning has shown strong performance in musculoskeletal imaging,\nexisting work has largely focused on pathologies where diagnosis is not a\nclinical challenge, leaving more difficult problems underexplored, such as\ndetecting Bankart lesions (anterior-inferior glenoid labral tears) on standard\nMRIs. Diagnosing these lesions is challenging due to their subtle imaging\nfeatures, often leading to reliance on invasive MRI arthrograms (MRAs). This\nstudy introduces ScopeMRI, the first publicly available, expert-annotated\ndataset for shoulder pathologies, and presents a deep learning (DL) framework\nfor detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes\n586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent\narthroscopy. Ground truth labels were derived from intraoperative findings, the\ngold standard for diagnosis. Separate DL models for MRAs and standard MRIs were\ntrained using a combination of CNNs and transformers. Predictions from\nsagittal, axial, and coronal views were ensembled to optimize performance. The\nmodels were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71\nstandard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%\nand 94%, and specificity of 91% and 86% for standard MRIs and MRAs,\nrespectively. Notably, model performance on non-invasive standard MRIs matched\nor surpassed radiologists interpreting MRAs. External validation demonstrated\ninitial generalizability across imaging protocols. This study demonstrates that\nDL models can achieve radiologist-level diagnostic performance on standard\nMRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular\ncodebase for training and evaluating deep learning models on 3D medical imaging\ndata, we aim to accelerate research in musculoskeletal imaging and support the\ndevelopment of new datasets for clinically challenging diagnostic tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20501", "pdf": "https://arxiv.org/pdf/2504.20501", "abs": "https://arxiv.org/abs/2504.20501", "authors": ["Jia Wang", "Yunan Mei", "Jiarui Liu", "Xin Fan"], "title": "SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "One-shot medical image segmentation (MIS) is crucial for medical analysis due\nto the burden of medical experts on manual annotation. The recent emergence of\nthe segment anything model (SAM) has demonstrated remarkable adaptation in MIS\nbut cannot be directly applied to one-shot medical image segmentation (MIS) due\nto its reliance on labor-intensive user interactions and the high computational\ncost. To cope with these limitations, we propose a novel SAM-guided robust\nrepresentation learning framework, named RRL-MedSAM, to adapt SAM to one-shot\n3D MIS, which exploits the strong generalization capabilities of the SAM\nencoder to learn better feature representation. We devise a dual-stage\nknowledge distillation (DSKD) strategy to distill general knowledge between\nnatural and medical images from the foundation model to train a lightweight\nencoder, and then adopt a mutual exponential moving average (mutual-EMA) to\nupdate the weights of the general lightweight encoder and medical-specific\nencoder. Specifically, pseudo labels from the registration network are used to\nperform mutual supervision for such two encoders. Moreover, we introduce an\nauto-prompting (AP) segmentation decoder which adopts the mask generated from\nthe general lightweight model as a prompt to assist the medical-specific model\nin boosting the final segmentation performance. Extensive experiments conducted\non three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed\nRRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both\nsegmentation and registration tasks. Especially, our lightweight encoder uses\nonly 3\\% of the parameters compared to the encoder of SAM-Base.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20734", "pdf": "https://arxiv.org/pdf/2504.20734", "abs": "https://arxiv.org/abs/2504.20734", "authors": ["Woongyeong Yeo", "Kangsan Kim", "Soyeong Jeong", "Jinheon Baek", "Sung Ju Hwang"], "title": "UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Project page : https://universalrag.github.io", "summary": "Retrieval-Augmented Generation (RAG) has shown substantial promise in\nimproving factual accuracy by grounding model responses with external knowledge\nrelevant to queries. However, most existing RAG approaches are limited to a\ntext-only corpus, and while recent efforts have extended RAG to other\nmodalities such as images and videos, they typically operate over a single\nmodality-specific corpus. In contrast, real-world queries vary widely in the\ntype of knowledge they require, which a single type of knowledge source cannot\naddress. To address this, we introduce UniversalRAG, a novel RAG framework\ndesigned to retrieve and integrate knowledge from heterogeneous sources with\ndiverse modalities and granularities. Specifically, motivated by the\nobservation that forcing all modalities into a unified representation space\nderived from a single combined corpus causes a modality gap, where the\nretrieval tends to favor items from the same modality as the query, we propose\na modality-aware routing mechanism that dynamically identifies the most\nappropriate modality-specific corpus and performs targeted retrieval within it.\nAlso, beyond modality, we organize each modality into multiple granularity\nlevels, enabling fine-tuned retrieval tailored to the complexity and scope of\nthe query. We validate UniversalRAG on 8 benchmarks spanning multiple\nmodalities, showing its superiority over modality-specific and unified\nbaselines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
