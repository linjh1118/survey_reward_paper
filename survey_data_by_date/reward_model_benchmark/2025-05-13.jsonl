{"id": "2505.07205", "pdf": "https://arxiv.org/pdf/2505.07205", "abs": "https://arxiv.org/abs/2505.07205", "authors": ["Mouxiao Bian", "Rongzhao Zhang", "Chao Ding", "Xinwei Peng", "Jie Xu"], "title": "Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are poised to transform healthcare under China's\nHealthy China 2030 initiative, yet they introduce new ethical and\npatient-safety challenges. We present a novel 12,000-item Q&A benchmark\ncovering 11 ethics and 9 safety dimensions in medical contexts, to\nquantitatively evaluate these risks. Using this dataset, we assess\nstate-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing\nmoderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant\nimprovements after fine-tuning on our data (up to 50.8% accuracy). Results show\nnotable gaps in LLM decision-making on ethics and safety scenarios, reflecting\ninsufficient institutional oversight. We then identify systemic governance\nshortfalls-including the lack of fine-grained ethical audit protocols, slow\nadaptation by hospital IRBs, and insufficient evaluation tools-that currently\nhinder safe LLM deployment. Finally, we propose a practical governance\nframework for healthcare institutions (embedding LLM auditing teams, enacting\ndata ethics guidelines, and implementing safety simulation pipelines) to\nproactively manage LLM risks. Our study highlights the urgent need for robust\nLLM governance in Chinese healthcare, aligning AI innovation with patient\nsafety and ethical standards.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety", "accuracy", "fine-grained"], "score": 6}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07247", "pdf": "https://arxiv.org/pdf/2505.07247", "abs": "https://arxiv.org/abs/2505.07247", "authors": ["Peichao Lai", "Kexuan Zhang", "Yi Lin", "Linyihan Zhang", "Feiyang Ye", "Jinhao Yan", "Yanwei Xu", "Conghui He", "Yilei Wang", "Wentao Zhang", "Bin Cui"], "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07365", "pdf": "https://arxiv.org/pdf/2505.07365", "abs": "https://arxiv.org/abs/2505.07365", "authors": ["Chao-Han Huck Yang", "Sreyan Ghosh", "Qing Wang", "Jaeyeon Kim", "Hengyi Hong", "Sonal Kumar", "Guirui Zhong", "Zhifeng Kong", "S Sakshi", "Vaibhavi Lokegaonkar", "Oriol Nieto", "Ramani Duraiswami", "Dinesh Manocha", "Gunhee Kim", "Jun Du", "Rafael Valle", "Bryan Catanzaro"], "title": "Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": "Preprint. DCASE 2025 Audio QA Challenge:\n  https://dcase.community/challenge2025/task-audio-question-answering", "summary": "We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering\n(AQA) benchmark spanning multiple domains of sound understanding. This task\ndefines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA)\nto test audio-language models on interactive question-answering over diverse\nacoustic scenes. We describe the dataset composition (from marine mammal calls\nto soundscapes and complex real-world clips), the evaluation protocol (top-1\naccuracy with answer-shuffling robustness), and baseline systems\n(Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the\ndevelopment set are compared, showing strong variation across models and\nsubsets. This challenge aims to advance the audio understanding and reasoning\ncapabilities of audio-language models toward human-level acuity, which are\ncrucial for enabling AI agents to perceive and interact about the world\neffectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06538", "pdf": "https://arxiv.org/pdf/2505.06538", "abs": "https://arxiv.org/abs/2505.06538", "authors": ["Xinyue Lou", "You Li", "Jinan Xu", "Xiangyu Shi", "Chi Chen", "Kaiyu Huang"], "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "The rapid development of multimodal large reasoning models (MLRMs) has\ndemonstrated broad application potential, yet their safety and reliability\nremain critical concerns that require systematic exploration. To address this\ngap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs\nacross 5 benchmarks and unveil prevalent safety degradation phenomena in most\nadvanced models. Moreover, our analysis reveals distinct safety patterns across\ndifferent benchmarks: significant safety degradation is observed across\njailbreak robustness benchmarks, whereas safety-awareness benchmarks\ndemonstrate less pronounced degradation. In particular, a long thought process\nin some scenarios even enhances safety performance. Therefore, it is a\npotential approach to addressing safety issues in MLRMs by leveraging the\nintrinsic reasoning capabilities of the model to detect unsafe intent. To\noperationalize this insight, we construct a multimodal tuning dataset that\nincorporates a safety-oriented thought process. Experimental results from\nfine-tuning existing MLRMs with this dataset effectively enhances the safety on\nboth jailbreak robustness and safety-awareness benchmarks. This study provides\na new perspective for developing safe MLRMs. Our dataset is available at\nhttps://github.com/xinyuelou/Think-in-Safety.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594", "abs": "https://arxiv.org/abs/2505.06594", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "summarization", "dialogue"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698", "abs": "https://arxiv.org/abs/2505.06698", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\nFeedbacker, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our homepage project is available at\nhttps://liudan193.github.io/Feedbacker.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained", "criteria"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06684", "pdf": "https://arxiv.org/pdf/2505.06684", "abs": "https://arxiv.org/abs/2505.06684", "authors": ["Xuefeng Jiang", "Jia Li", "Nannan Wu", "Zhiyuan Wu", "Xujing Li", "Sheng Sun", "Gang Xu", "Yuwei Wang", "Qi Li", "Min Liu"], "title": "FNBench: Benchmarking Robust Federated Learning against Noisy Labels", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to IEEE TDSC, currently under major revision", "summary": "Robustness to label noise within data is a significant challenge in federated\nlearning (FL). From the data-centric perspective, the data quality of\ndistributed datasets can not be guaranteed since annotations of different\nclients contain complicated label noise of varying degrees, which causes the\nperformance degradation. There have been some early attempts to tackle noisy\nlabels in FL. However, there exists a lack of benchmark studies on\ncomprehensively evaluating their practical performance under unified settings.\nTo this end, we propose the first benchmark study FNBench to provide an\nexperimental investigation which considers three diverse label noise patterns\ncovering synthetic label noise, imperfect human-annotation errors and\nsystematic errors. Our evaluation incorporates eighteen state-of-the-art\nmethods over five image recognition datasets and one text classification\ndataset. Meanwhile, we provide observations to understand why noisy labels\nimpair FL, and additionally exploit a representation-aware regularization\nmethod to enhance the robustness of existing methods against noisy labels based\non our observations. Finally, we discuss the limitations of this work and\npropose three-fold future directions. To facilitate related communities, our\nsource code is open-sourced at https://github.com/Sprinter1999/FNBench.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07416", "pdf": "https://arxiv.org/pdf/2505.07416", "abs": "https://arxiv.org/abs/2505.07416", "authors": ["Truc Mai-Thanh Nguyen", "Dat Minh Nguyen", "Son T. Luu", "Kiet Van Nguyen"], "title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation", "categories": ["cs.CL"], "comment": "Accepted at NLDB 2025", "summary": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in\nrecommender systems, particularly in E-commerce platforms. Determining the\nhelpfulness of user-generated reviews enhances user experience and improves\nconsumer decision-making. However, existing datasets focus predominantly on\nEnglish and Indonesian, resulting in a lack of linguistic diversity, especially\nfor low-resource languages such as Vietnamese. In this paper, we introduce\nViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale\nbenchmark dataset for MRHP task in Vietnamese. This dataset covers four\ndomains, including 2K products with 46K reviews. Meanwhile, a large-scale\ndataset requires considerable time and cost. To optimize the annotation\nprocess, we leverage AI to assist annotators in constructing the ViMRHP\ndataset. With AI assistance, annotation time is reduced (90 to 120 seconds per\ntask down to 20 to 40 seconds per task) while maintaining data quality and\nlowering overall costs by approximately 65%. However, AI-generated annotations\nstill have limitations in complex annotation tasks, which we further examine\nthrough a detailed performance analysis. In our experiment on ViMRHP, we\nevaluate baseline models on human-verified and AI-generated annotations to\nassess their quality differences. The ViMRHP dataset is publicly available at\nhttps://github.com/trng28/ViMRHP", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "helpfulness"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "question answering"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07057", "pdf": "https://arxiv.org/pdf/2505.07057", "abs": "https://arxiv.org/abs/2505.07057", "authors": ["Junhao Xia", "Chaoyang Zhang", "Yecheng Zhang", "Chengyang Zhou", "Zhichang Wang", "Bochun Liu", "Dongshuo Yin"], "title": "DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video generation based on diffusion models presents a challenging multimodal\ntask, with video editing emerging as a pivotal direction in this field. Recent\nvideo editing approaches primarily fall into two categories: training-required\nand training-free methods. While training-based methods incur high\ncomputational costs, training-free alternatives often yield suboptimal\nperformance. To address these limitations, we propose DAPE, a high-quality yet\ncost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for\nvideo editing. In the first stage, we design an efficient norm-tuning method to\nenhance temporal consistency in generated videos. The second stage introduces a\nvision-friendly adapter to improve visual quality. Additionally, we identify\ncritical shortcomings in existing benchmarks, including limited category\ndiversity, imbalanced object distribution, and inconsistent frame counts. To\nmitigate these issues, we curate a large dataset benchmark comprising 232\nvideos with rich annotations and 6 editing prompts, enabling objective and\ncomprehensive evaluation of advanced methods. Extensive experiments on existing\ndatasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate\nthat DAPE significantly improves temporal coherence and text-video alignment\nwhile outperforming previous state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "question answering"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07347", "pdf": "https://arxiv.org/pdf/2505.07347", "abs": "https://arxiv.org/abs/2505.07347", "authors": ["Jiewen Yang", "Taoran Huang", "Shangwei Ding", "Xiaowei Xu", "Qinhua Zhao", "Yong Jiang", "Jiarong Guo", "Bin Pu", "Jiexuan Zheng", "Caojin Zhang", "Hongwen Fei", "Xiaomeng Li"], "title": "AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography", "categories": ["cs.CV"], "comment": null, "summary": "Echocardiographers can detect pulmonary hypertension using Doppler\nechocardiography; however, accurately assessing its progression often proves\nchallenging. Right heart catheterization (RHC), the gold standard for precise\nevaluation, is invasive and unsuitable for routine use, limiting its\npracticality for timely diagnosis and monitoring of pulmonary hypertension\nprogression. Here, we propose MePH, a multi-view, multi-modal vision-language\nmodel to accurately assess pulmonary hypertension progression using\nnon-invasive echocardiography. We constructed a large dataset comprising paired\nstandardized echocardiogram videos, spectral images and RHC data, covering\n1,237 patient cases from 12 medical centers. For the first time, MePH precisely\nmodels the correlation between non-invasive multi-view, multi-modal\nechocardiography and the pressure and resistance obtained via RHC. We show that\nMePH significantly outperforms echocardiographers' assessments using\nechocardiography, reducing the mean absolute error in estimating mean pulmonary\narterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and\n43.81%, respectively. In eight independent external hospitals, MePH achieved a\nmean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an\narea under the curve of 0.921, surpassing echocardiographers (area under the\ncurve of 0.842) in accurately predicting the severity of pulmonary\nhypertension, whether mild or severe. A prospective study demonstrated that\nMePH can predict treatment efficacy for patients. Our work provides pulmonary\nhypertension patients with a non-invasive and timely method for monitoring\ndisease progression, improving the accuracy and efficiency of pulmonary\nhypertension management while enabling earlier interventions and more\npersonalized treatment decisions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation", "accuracy"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07622", "pdf": "https://arxiv.org/pdf/2505.07622", "abs": "https://arxiv.org/abs/2505.07622", "authors": ["Zhuo Song", "Ye Zhang", "Kunhong Li", "Longguang Wang", "Yulan Guo"], "title": "A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization is a promising solution for large-scale\nlocalization problems, requiring the sequential execution of retrieval and\nmetric localization tasks to achieve fine-grained predictions. However,\nexisting methods typically focus on designing standalone models for these two\ntasks, resulting in inefficient collaboration and increased training overhead.\nIn this paper, we propose UnifyGeo, a novel unified hierarchical\ngeo-localization framework that integrates retrieval and metric localization\ntasks into a single network. Specifically, we first employ a unified learning\nstrategy with shared parameters to jointly learn multi-granularity\nrepresentation, facilitating mutual reinforcement between these two tasks.\nSubsequently, we design a re-ranking mechanism guided by a dedicated loss\nfunction, which enhances geo-localization performance by improving both\nretrieval accuracy and metric localization references. Extensive experiments\ndemonstrate that UnifyGeo significantly outperforms the state-of-the-arts in\nboth task-isolated and task-associated settings. Remarkably, on the challenging\nVIGOR benchmark, which supports fine-grained localization evaluation, the\n1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from\n0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively.\nCode will be made publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06594", "pdf": "https://arxiv.org/pdf/2505.06594", "abs": "https://arxiv.org/abs/2505.06594", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) often struggle to balance visual and textual\ninformation when summarizing complex multimodal inputs, such as entire TV show\nepisodes. In this paper, we propose a zero-shot video-to-text summarization\napproach that builds its own screenplay representation of an episode,\neffectively integrating key video moments, dialogue, and character information\ninto a unified document. Unlike previous approaches, we simultaneously generate\nscreenplays and name the characters in zero-shot, using only the audio, video,\nand transcripts as input. Additionally, we highlight that existing\nsummarization metrics can fail to assess the multimodal content in summaries.\nTo address this, we introduce MFactSum, a multimodal metric that evaluates\nsummaries with respect to both vision and text modalities. Using MFactSum, we\nevaluate our screenplay summaries on the SummScreen3D dataset, demonstrating\nsuperiority against state-of-the-art VLMs such as Gemini 1.5 by generating\nsummaries containing 20% more relevant visual information while requiring 75%\nless of the video as input.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "summarization", "dialogue"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07175", "pdf": "https://arxiv.org/pdf/2505.07175", "abs": "https://arxiv.org/abs/2505.07175", "authors": ["Yash Deo", "Yan Jia", "Toni Lassila", "William A. P. Smith", "Tom Lawton", "Siyuan Kang", "Alejandro F. Frangi", "Ibrahim Habli"], "title": "Metrics that matter: Evaluating image quality metrics for medical image generation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Evaluating generative models for synthetic medical imaging is crucial yet\nchallenging, especially given the high standards of fidelity, anatomical\naccuracy, and safety required for clinical applications. Standard evaluation of\ngenerated images often relies on no-reference image quality metrics when ground\ntruth images are unavailable, but their reliability in this complex domain is\nnot well established. This study comprehensively assesses commonly used\nno-reference image quality metrics using brain MRI data, including tumour and\nvascular images, providing a representative exemplar for the field. We\nsystematically evaluate metric sensitivity to a range of challenges, including\nnoise, distribution shifts, and, critically, localised morphological\nalterations designed to mimic clinically relevant inaccuracies. We then compare\nthese metric scores against model performance on a relevant downstream\nsegmentation task, analysing results across both controlled image perturbations\nand outputs from different generative model architectures. Our findings reveal\nsignificant limitations: many widely-used no-reference image quality metrics\ncorrelate poorly with downstream task suitability and exhibit a profound\ninsensitivity to localised anatomical details crucial for clinical validity.\nFurthermore, these metrics can yield misleading scores regarding distribution\nshifts, e.g. data memorisation. This reveals the risk of misjudging model\nreadiness, potentially leading to the deployment of flawed tools that could\ncompromise patient safety. We conclude that ensuring generative models are\ntruly fit for clinical purpose requires a multifaceted validation framework,\nintegrating performance on relevant downstream tasks with the cautious\ninterpretation of carefully selected no-reference image quality metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06381", "pdf": "https://arxiv.org/pdf/2505.06381", "abs": "https://arxiv.org/abs/2505.06381", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal", "categories": ["cs.CV"], "comment": null, "summary": "Medical disease prediction, particularly through imaging, remains a\nchallenging task due to the complexity and variability of medical data,\nincluding noise, ambiguity, and differing image quality. Recent deep learning\nmodels, including Knowledge Distillation (KD) methods, have shown promising\nresults in brain tumor image identification but still face limitations in\nhandling uncertainty and generalizing across diverse medical conditions.\nTraditional KD methods often rely on a context-unaware temperature parameter to\nsoften teacher model predictions, which does not adapt effectively to varying\nuncertainty levels present in medical images. To address this issue, we propose\na novel framework that integrates Ant Colony Optimization (ACO) for optimal\nteacher-student model selection and a novel context-aware predictor approach\nfor temperature scaling. The proposed context-aware framework adjusts the\ntemperature based on factors such as image quality, disease complexity, and\nteacher model confidence, allowing for more robust knowledge transfer.\nAdditionally, ACO efficiently selects the most appropriate teacher-student\nmodel pair from a set of pre-trained models, outperforming current optimization\nmethods by exploring a broader solution space and better handling complex,\nnon-linear relationships within the data. The proposed framework is evaluated\nusing three publicly available benchmark datasets, each corresponding to a\ndistinct medical imaging task. The results demonstrate that the proposed\nframework significantly outperforms current state-of-the-art methods, achieving\ntop accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on\nthe Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced\nperformance is further evidenced by the improved results, surpassing existing\nbenchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06393", "pdf": "https://arxiv.org/pdf/2505.06393", "abs": "https://arxiv.org/abs/2505.06393", "authors": ["Valfride Nascimento", "Gabriel E. Lima", "Rafael O. Ribeiro", "William Robson Schwartz", "Rayson Laroca", "David Menotti"], "title": "Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark", "categories": ["cs.CV"], "comment": "Accepted for publication in the Journal of the Brazilian Computer\n  Society", "summary": "Recent advancements in super-resolution for License Plate Recognition (LPR)\nhave sought to address challenges posed by low-resolution (LR) and degraded\nimages in surveillance, traffic monitoring, and forensic applications. However,\nexisting studies have relied on private datasets and simplistic degradation\nmodels. To address this gap, we introduce UFPR-SR-Plates, a novel dataset\ncontaining 10,000 tracks with 100,000 paired low and high-resolution license\nplate images captured under real-world conditions. We establish a benchmark\nusing multiple sequential LR and high-resolution (HR) images per vehicle --\nfive of each -- and two state-of-the-art models for super-resolution of license\nplates. We also investigate three fusion strategies to evaluate how combining\npredictions from a leading Optical Character Recognition (OCR) model for\nmultiple super-resolved license plates enhances overall performance. Our\nfindings demonstrate that super-resolution significantly boosts LPR\nperformance, with further improvements observed when applying majority\nvote-based fusion techniques. Specifically, the Layout-Aware and\nCharacter-Driven Network (LCDNet) model combined with the Majority Vote by\nCharacter Position (MVCP) strategy led to the highest recognition rates,\nincreasing from 1.7% with low-resolution images to 31.1% with super-resolution,\nand up to 44.7% when combining OCR outputs from five super-resolved images.\nThese findings underscore the critical role of super-resolution and temporal\ninformation in enhancing LPR accuracy under real-world, adverse conditions. The\nproposed dataset is publicly available to support further research and can be\naccessed at: https://valfride.github.io/nascimento2024toward/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06418", "pdf": "https://arxiv.org/pdf/2505.06418", "abs": "https://arxiv.org/abs/2505.06418", "authors": ["Ming Liu", "Liwen Wang", "Wensheng Zhang"], "title": "Is your multimodal large language model a good science tutor?", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate impressive performance\non scientific reasoning tasks (e.g., ScienceQA). However, most existing\nbenchmarks focus narrowly on the accuracy of the final answer while ignoring\nother metrics. In particular, when applying MLLMs to educational contexts, the\ngoal is not only correctness but also the ability to teach. In this paper, we\npropose a framework that evaluates MLLMs as science tutors using a\ncomprehensive educational rubric and a simulated student model that judges the\nteaching performance of the tutors. Given a list of candidate MLLM science\ntutors, we use rubric-based student judgments to produce a range of tutor\nperformance scores, identifying both strong and weak tutors. Using the training\nsection of the ScienceQA dataset, we then construct a data set of pairwise\ncomparisons between the outputs of strong and weak tutors. This enables us to\napply multiple preference optimization methods to fine-tune an underperforming\ntutor model (Qwen2-VL-2B) into more effective ones. Our results also show that\nstrong problem-solving skills do not guarantee high-quality tutoring and that\nperformance optimization-guided refinements can yield more educationally\naligned tutor models. This approach opens avenues for building MLLMs that serve\nnot only as problem solvers, but as genuinely helpful educational assistants.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "rubric"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06599", "pdf": "https://arxiv.org/pdf/2505.06599", "abs": "https://arxiv.org/abs/2505.06599", "authors": ["Abbas Bertina", "Shahab Beirami", "Hossein Biniazian", "Elham Esmaeilnia", "Soheil Shahi", "Mahdi Pirnia"], "title": "Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "pdf, 8 pages, 4 figures, 4 tables", "summary": "Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges\ndue to its complex phonological features, particularly homographs and Ezafe,\nwhich exist in formal and informal language contexts. This paper introduces an\nintermediate language specifically designed for Persian language processing\nthat addresses these challenges through a multi-faceted approach. Our\nmethodology combines two key components: Large Language Model (LLM) prompting\ntechniques and a specialized sequence-to-sequence machine transliteration\narchitecture. We developed and implemented a systematic approach for\nconstructing a comprehensive lexical database for homographs with multiple\npronunciations disambiguation often termed polyphones, utilizing formal concept\nanalysis for semantic differentiation. We train our model using two distinct\ndatasets: the LLM-generated dataset for formal and informal Persian and the\nB-Plus podcasts for informal language variants. The experimental results\ndemonstrate superior performance compared to existing state-of-the-art\napproaches, particularly in handling the complexities of Persian phoneme\nconversion. Our model significantly improves Phoneme Error Rate (PER) metrics,\nestablishing a new benchmark for Persian G2P conversion accuracy. This work\ncontributes to the growing research in low-resource language processing and\nprovides a robust solution for Persian text-to-speech systems and demonstrating\nits applicability beyond Persian. Specifically, the approach can extend to\nlanguages with rich homographic phenomena such as Chinese and Arabic", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06528", "pdf": "https://arxiv.org/pdf/2505.06528", "abs": "https://arxiv.org/abs/2505.06528", "authors": ["Mahmudul Hasan"], "title": "Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection", "categories": ["cs.CV"], "comment": null, "summary": "Deepfake videos, produced through advanced artificial intelligence methods\nnow a days, pose a new challenge to the truthfulness of the digital media. As\nDeepfake becomes more convincing day by day, detecting them requires advanced\nmethods capable of identifying subtle inconsistencies. The primary motivation\nof this paper is to recognize deepfake videos using deep learning techniques,\nspecifically by using convolutional neural networks. Deep learning excels in\npattern recognition, hence, makes it an ideal approach for detecting the\nintricate manipulations in deepfakes. In this paper, we consider using MTCNN as\na face detector and EfficientNet-B5 as encoder model to predict if a video is\ndeepfake or not. We utilize training and evaluation dataset from Kaggle DFDC.\nThe results shows that our deepfake detection model acquired 42.78% log loss,\n93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "truthfulness"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06537", "pdf": "https://arxiv.org/pdf/2505.06537", "abs": "https://arxiv.org/abs/2505.06537", "authors": ["Xianghao Kong", "Qiaosong Qi", "Yuanbin Wang", "Anyi Rao", "Biaolong Chen", "Aixi Zhang", "Si Liu", "Hao Jiang"], "title": "ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion video generation aims to synthesize temporally consistent videos from\nreference images of a designated character. Despite significant progress,\nexisting diffusion-based methods only support a single reference image as\ninput, severely limiting their capability to generate view-consistent fashion\nvideos, especially when there are different patterns on the clothes from\ndifferent perspectives. Moreover, the widely adopted motion module does not\nsufficiently model human body movement, leading to sub-optimal spatiotemporal\nconsistency. To address these issues, we propose ProFashion, a fashion video\ngeneration framework leveraging multiple reference images to achieve improved\nview consistency and temporal coherency. To effectively leverage features from\nmultiple reference images while maintaining a reasonable computational cost, we\ndevise a Pose-aware Prototype Aggregator, which selects and aggregates global\nand fine-grained reference features according to pose information to form\nframe-wise prototypes, which serve as guidance in the denoising process. To\nfurther enhance motion consistency, we introduce a Flow-enhanced Prototype\nInstantiator, which exploits the human keypoint motion flow to guide an extra\nspatiotemporal attention process in the denoiser. To demonstrate the\neffectiveness of ProFashion, we extensively evaluate our method on the\nMRFashion-7K dataset we collected from the Internet. ProFashion also\noutperforms previous methods on the UBC Fashion dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07495", "pdf": "https://arxiv.org/pdf/2505.07495", "abs": "https://arxiv.org/abs/2505.07495", "authors": ["Isabelle van der Vegt", "Bennett Kleinberg", "Marilu Miotto", "Jonas Festor"], "title": "Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces and evaluates three translations of the Grievance\nDictionary, a psycholinguistic dictionary for the analysis of violent,\nthreatening or grievance-fuelled texts. Considering the relevance of these\nthemes in languages beyond English, we translated the Grievance Dictionary to\nDutch, German, and Italian. We describe the process of automated translation\nsupplemented by human annotation. Psychometric analyses are performed,\nincluding internal reliability of dictionary categories and correlations with\nthe LIWC dictionary. The Dutch and German translations perform similarly to the\noriginal English version, whereas the Italian dictionary shows low reliability\nfor some categories. Finally, we make suggestions for further validation and\napplication of the dictionary, as well as for future dictionary translations\nfollowing a similar approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "reliability"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07637", "pdf": "https://arxiv.org/pdf/2505.07637", "abs": "https://arxiv.org/abs/2505.07637", "authors": ["Krish Goel", "Sanskar Pandey", "KS Mahadevan", "Harsh Kumar", "Vishesh Khadaria"], "title": "Chronocept: Instilling a Sense of Time in Machines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 18 tables", "summary": "Human cognition is deeply intertwined with a sense of time, known as\nChronoception. This sense allows us to judge how long facts remain valid and\nwhen knowledge becomes outdated. Despite progress in vision, language, and\nmotor control, AI still struggles to reason about temporal validity. We\nintroduce Chronocept, the first benchmark to model temporal validity as a\ncontinuous probability distribution over time. Using skew-normal curves fitted\nalong semantically decomposed temporal axes, Chronocept captures nuanced\npatterns of emergence, decay, and peak relevance. It includes two datasets:\nBenchmark I (atomic facts) and Benchmark II (multi-sentence passages).\nAnnotations show strong inter-annotator agreement (84% and 89%). Our baselines\npredict curve parameters - location, scale, and skewness - enabling\ninterpretable, generalizable learning and outperforming classification-based\napproaches. Chronocept fills a foundational gap in AI's temporal reasoning,\nsupporting applications in knowledge grounding, fact-checking,\nretrieval-augmented generation (RAG), and proactive agents. Code and data are\npublicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "agreement", "inter-annotator agreement"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07001", "pdf": "https://arxiv.org/pdf/2505.07001", "abs": "https://arxiv.org/abs/2505.07001", "authors": ["Bidur Khanal", "Sandesh Pokhrel", "Sanjay Bhandari", "Ramesh Rana", "Nikesh Shrestha", "Ram Bahadur Gurung", "Cristian Linte", "Angus Watson", "Yash Raj Shrestha", "Binod Bhattarai"], "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) are becoming increasingly popular in the\nmedical domain, bridging the gap between medical images and clinical language.\nExisting VLMs demonstrate an impressive ability to comprehend medical images\nand text queries to generate detailed, descriptive diagnostic medical reports.\nHowever, hallucination--the tendency to generate descriptions that are\ninconsistent with the visual content--remains a significant issue in VLMs, with\nparticularly severe implications in the medical field. To facilitate VLM\nresearch on gastrointestinal (GI) image analysis and study hallucination, we\ncurate a multimodal image-text GI dataset: Gut-VLM. This dataset is created\nusing a two-stage pipeline: first, descriptive medical reports of Kvasir-v2\nimages are generated using ChatGPT, which introduces some hallucinated or\nincorrect texts. In the second stage, medical experts systematically review\nthese reports, and identify and correct potential inaccuracies to ensure\nhigh-quality, clinically reliable annotations. Unlike traditional datasets that\ncontain only descriptive texts, our dataset also features tags identifying\nhallucinated sentences and their corresponding corrections. A common approach\nto reducing hallucination in VLM is to finetune the model on a small-scale,\nproblem-specific dataset. However, we take a different strategy using our\ndataset. Instead of finetuning the VLM solely for generating textual reports,\nwe finetune it to detect and correct hallucinations, an approach we call\nhallucination-aware finetuning. Our results show that this approach is better\nthan simply finetuning for descriptive report generation. Additionally, we\nconduct an extensive evaluation of state-of-the-art VLMs across several\nmetrics, establishing a benchmark. GitHub Repo:\nhttps://github.com/bhattarailab/Hallucination-Aware-VLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07007", "pdf": "https://arxiv.org/pdf/2505.07007", "abs": "https://arxiv.org/abs/2505.07007", "authors": ["Zhengye Zhang", "Sirui Zhao", "Shifeng Liu", "Shukang Yin", "Xinglong Mao", "Tong Xu", "Enhong Chen"], "title": "MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are crucial psychological responses with significant\npotential for affective computing. However, current automatic micro-expression\nrecognition (MER) research primarily focuses on discrete emotion\nclassification, neglecting a convincing analysis of the subtle dynamic\nmovements and inherent emotional cues. The rapid progress in multimodal large\nlanguage models (MLLMs), known for their strong multimodal comprehension and\nlanguage generation abilities, offers new possibilities. MLLMs have shown\nsuccess in various vision-language tasks, indicating their potential to\nunderstand MEs comprehensively, including both fine-grained motion patterns and\nunderlying emotional semantics. Nevertheless, challenges remain due to the\nsubtle intensity and short duration of MEs, as existing MLLMs are not designed\nto capture such delicate frame-level facial dynamics. In this paper, we propose\na novel Micro-Expression Large Language Model (MELLM), which incorporates a\nsubtle facial motion perception strategy with the strong inference capabilities\nof MLLMs, representing the first exploration of MLLMs in the domain of ME\nanalysis. Specifically, to explicitly guide the MLLM toward motion-sensitive\nregions, we construct an interpretable motion-enhanced color map by fusing\nonset-apex optical flow dynamics with the corresponding grayscale onset frame\nas the model input. Additionally, specialized fine-tuning strategies are\nincorporated to further enhance the model's visual perception of MEs.\nFurthermore, we construct an instruction-description dataset based on Facial\nAction Coding System (FACS) annotations and emotion labels to train our MELLM.\nComprehensive evaluations across multiple benchmark datasets demonstrate that\nour model exhibits superior robustness and generalization capabilities in ME\nunderstanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07013", "pdf": "https://arxiv.org/pdf/2505.07013", "abs": "https://arxiv.org/abs/2505.07013", "authors": ["Jitesh Joshi", "Youngjun Cho"], "title": "Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 6 figures", "summary": "Remote physiological sensing using camera-based technologies offers\ntransformative potential for non-invasive vital sign monitoring across\nhealthcare and human-computer interaction domains. Although deep learning\napproaches have advanced the extraction of physiological signals from video\ndata, existing methods have not been sufficiently assessed for their robustness\nto domain shifts. These shifts in remote physiological sensing include\nvariations in ambient conditions, camera specifications, head movements, facial\nposes, and physiological states which often impact real-world performance\nsignificantly. Cross-dataset evaluation provides an objective measure to assess\ngeneralization capabilities across these domain shifts. We introduce Target\nSignal Constrained Factorization module (TSFM), a novel multidimensional\nattention mechanism that explicitly incorporates physiological signal\ncharacteristics as factorization constraints, allowing more precise feature\nextraction. Building on this innovation, we present MMRPhys, an efficient\ndual-branch 3D-CNN architecture designed for simultaneous multitask estimation\nof photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal\nRGB and thermal video inputs. Through comprehensive cross-dataset evaluation on\nfive benchmark datasets, we demonstrate that MMRPhys with TSFM significantly\noutperforms state-of-the-art methods in generalization across domain shifts for\nrPPG and rRSP estimation, while maintaining a minimal inference latency\nsuitable for real-time applications. Our approach establishes new benchmarks\nfor robust multitask and multimodal physiological sensing and offers a\ncomputationally efficient framework for practical deployment in unconstrained\nenvironments. The web browser-based application featuring on-device real-time\ninference of MMRPhys model is available at\nhttps://physiologicailab.github.io/mmrphys-live", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07256", "pdf": "https://arxiv.org/pdf/2505.07256", "abs": "https://arxiv.org/abs/2505.07256", "authors": ["Christoph Huber", "Ludwig Schleeh", "Dino Knoll", "Michael Guthe"], "title": "Synthetic Similarity Search in Automotive Production", "categories": ["cs.CV"], "comment": "Accepted for publication in Procedia CIRP", "summary": "Visual quality inspection in automotive production is essential for ensuring\nthe safety and reliability of vehicles. Computer vision (CV) has become a\npopular solution for these inspections due to its cost-effectiveness and\nreliability. However, CV models require large, annotated datasets, which are\ncostly and time-consuming to collect. To reduce the need for extensive training\ndata, we propose a novel image classification pipeline that combines similarity\nsearch using a vision-based foundation model with synthetic data. Our approach\nleverages a DINOv2 model to transform input images into feature vectors, which\nare then compared to pre-classified reference images using cosine distance\nmeasurements. By utilizing synthetic data instead of real images as references,\nour pipeline achieves high classification accuracy without relying on real\ndata. We evaluate this approach in eight real-world inspection scenarios and\ndemonstrate that it meets the high performance requirements of production\nenvironments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07263", "pdf": "https://arxiv.org/pdf/2505.07263", "abs": "https://arxiv.org/abs/2505.07263", "authors": ["Xiaokun Wang", "Chris", "Jiangbo Pei", "Wei Shen", "Yi Peng", "Yunzhuo Hao", "Weijie Qiu", "Ai Jian", "Tianyidan Xie", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "ranking", "pairwise", "alignment"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "preference dataset"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07373", "pdf": "https://arxiv.org/pdf/2505.07373", "abs": "https://arxiv.org/abs/2505.07373", "authors": ["Lintao Xiang", "Hongpei Zheng", "Bailin Deng", "Hujun Yin"], "title": "Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Neural implicit surface reconstruction using volume rendering techniques has\nrecently achieved significant advancements in creating high-fidelity surfaces\nfrom multiple 2D images. However, current methods primarily target scenes with\nconsistent illumination and struggle to accurately reconstruct 3D geometry in\nuncontrolled environments with transient occlusions or varying appearances.\nWhile some neural radiance field (NeRF)-based variants can better manage\nphotometric variations and transient objects in complex scenes, they are\ndesigned for novel view synthesis rather than precise surface reconstruction\ndue to limited surface constraints. To overcome this limitation, we introduce a\nnovel approach that applies multiple geometric constraints to the implicit\nsurface optimization process, enabling more accurate reconstructions from\nunconstrained image collections. First, we utilize sparse 3D points from\nstructure-from-motion (SfM) to refine the signed distance function estimation\nfor the reconstructed surface, with a displacement compensation to accommodate\nnoise in the sparse points. Additionally, we employ robust normal priors\nderived from a normal predictor, enhanced by edge prior filtering and\nmulti-view consistency constraints, to improve alignment with the actual\nsurface geometry. Extensive testing on the Heritage-Recon benchmark and other\ndatasets has shown that the proposed method can accurately reconstruct surfaces\nfrom in-the-wild images, yielding geometries with superior accuracy and\ngranularity compared to existing techniques. Our approach enables high-quality\n3D reconstruction of various landmarks, making it applicable to diverse\nscenarios such as digital preservation of cultural heritage sites.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07396", "pdf": "https://arxiv.org/pdf/2505.07396", "abs": "https://arxiv.org/abs/2505.07396", "authors": ["Olaf Wysocki", "Benedikt Schwab", "Manoj Kumar Biswanath", "Qilin Zhang", "Jingwei Zhu", "Thomas Froech", "Medhini Heeramaglore", "Ihab Hijazi", "Khaoula Kanna", "Mathias Pechinger", "Zhaiyu Chen", "Yao Sun", "Alejandro Rueda Segura", "Ziyang Xu", "Omar AbdelGafar", "Mansour Mehranfar", "Chandan Yeshwanth", "Yueh-Cheng Liu", "Hadi Yazdi", "Jiapan Wang", "Stefan Auer", "Katharina Anders", "Klaus Bogenberger", "Andre Borrmann", "Angela Dai", "Ludwig Hoegner", "Christoph Holst", "Thomas H. Kolbe", "Ferdinand Ludwig", "Matthias Niener", "Frank Petzold", "Xiao Xiang Zhu", "Boris Jutzi"], "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to the ISPRS Journal of Photogrammetry and Remote Sensing", "summary": "Urban Digital Twins (UDTs) have become essential for managing cities and\nintegrating complex, heterogeneous data from diverse sources. Creating UDTs\ninvolves challenges at multiple process stages, including acquiring accurate 3D\nsource data, reconstructing high-fidelity 3D models, maintaining models'\nupdates, and ensuring seamless interoperability to downstream tasks. Current\ndatasets are usually limited to one part of the processing chain, hampering\ncomprehensive UDTs validation. To address these challenges, we introduce the\nfirst comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN.\nThis dataset includes georeferenced, semantically aligned 3D models and\nnetworks along with various terrestrial, mobile, aerial, and satellite\nobservations boasting 32 data subsets over roughly 100,000 $m^2$ and currently\n767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high\naccuracy, and multimodal data integration, the benchmark supports robust\nanalysis of sensors and the development of advanced reconstruction methods.\nAdditionally, we explore downstream tasks demonstrating the potential of\nTUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar\npotential analysis, point cloud semantic segmentation, and LoD3 building\nreconstruction. We are convinced this contribution lays a foundation for\novercoming current limitations in UDT creation, fostering new research\ndirections and practical solutions for smarter, data-driven urban environments.\nThe project is available under: https://tum2t.win", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07620", "pdf": "https://arxiv.org/pdf/2505.07620", "abs": "https://arxiv.org/abs/2505.07620", "authors": ["Simone Azeglio", "Victor Calbiague Garcia", "Guilhem Glaziou", "Peter Neri", "Olivier Marre", "Ulisse Ferrari"], "title": "Higher-Order Convolution Improves Neural Predictivity in the Retina", "categories": ["cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "We present a novel approach to neural response prediction that incorporates\nhigher-order operations directly within convolutional neural networks (CNNs).\nOur model extends traditional 3D CNNs by embedding higher-order operations\nwithin the convolutional operator itself, enabling direct modeling of\nmultiplicative interactions between neighboring pixels across space and time.\nOur model increases the representational power of CNNs without increasing their\ndepth, therefore addressing the architectural disparity between deep artificial\nnetworks and the relatively shallow processing hierarchy of biological visual\nsystems. We evaluate our approach on two distinct datasets: salamander retinal\nganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC\nresponses to controlled geometric transformations. Our higher-order CNN (HoCNN)\nachieves superior performance while requiring only half the training data\ncompared to standard architectures, demonstrating correlation coefficients up\nto 0.75 with neural responses (against 0.80$\\pm$0.02 retinal reliability). When\nintegrated into state-of-the-art architectures, our approach consistently\nimproves performance across different species and stimulus conditions. Analysis\nof the learned representations reveals that our network naturally encodes\nfundamental geometric transformations, particularly scaling parameters that\ncharacterize object expansion and contraction. This capability is especially\nrelevant for specific cell types, such as transient OFF-alpha and transient ON\ncells, which are known to detect looming objects and object motion\nrespectively, and where our model shows marked improvement in response\nprediction. The correlation coefficients for scaling parameters are more than\ntwice as high in HoCNN (0.72) compared to baseline models (0.32).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "reliability"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07721", "pdf": "https://arxiv.org/pdf/2505.07721", "abs": "https://arxiv.org/abs/2505.07721", "authors": ["Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Gameplay Highlights Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we enable gamers to share their gaming experience on social\nmedia by automatically generating eye-catching highlight reels from their\ngameplay session Our automation will save time for gamers while increasing\naudience engagement. We approach the highlight generation problem by first\nidentifying intervals in the video where interesting events occur and then\nconcatenate them. We developed an in-house gameplay event detection dataset\ncontaining interesting events annotated by humans using VIA video annotator.\nTraditional techniques for highlight detection such as game engine integration\nrequires expensive collaboration with game developers. OCR techniques which\ndetect patches of specific images or texts require expensive per game\nengineering and may not generalize across game UI and different language. We\nfinetuned a multimodal general purpose video understanding model such as X-CLIP\nusing our dataset which generalizes across multiple games in a genre without\nper game engineering. Prompt engineering was performed to improve the\nclassification performance of this multimodal model. Our evaluation showed that\nsuch a finetuned model can detect interesting events in first person shooting\ngames from unseen gameplay footage with more than 90% accuracy. Moreover, our\nmodel performed significantly better on low resource games (small dataset) when\ntrained along with high resource games, showing signs of transfer learning. To\nmake the model production ready, we used ONNX libraries to enable cross\nplatform inference. These libraries also provide post training quantization\ntools to reduce model size and inference time for deployment. ONNX runtime\nlibraries with DirectML backend were used to perform efficient inference on\nWindows OS. We show that natural language supervision in the X-CLIP model leads\nto data efficient and highly performant video recognition models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07747", "pdf": "https://arxiv.org/pdf/2505.07747", "abs": "https://arxiv.org/abs/2505.07747", "authors": ["Weiyu Li", "Xuanyang Zhang", "Zheng Sun", "Di Qi", "Hao Li", "Wei Cheng", "Weiwei Cai", "Shihao Wu", "Jiarui Liu", "Zihao Wang", "Xiao Chen", "Feipeng Tian", "Jianxiong Pan", "Zeming Li", "Gang Yu", "Xiangyu Zhang", "Daxin Jiang", "Ping Tan"], "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets", "categories": ["cs.CV"], "comment": "Technical report", "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06370", "pdf": "https://arxiv.org/pdf/2505.06370", "abs": "https://arxiv.org/abs/2505.06370", "authors": ["Adhora Madhuri", "Nusaiba Sobir", "Tasnia Binte Mamun", "Taufiq Hasan"], "title": "LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures, 6 tables", "summary": "Lung cancer is the leading cause of patient mortality in the world. Early\ndiagnosis of malignant pulmonary nodules in CT images can have a significant\nimpact on reducing disease mortality and morbidity. In this work, we propose\nLMLCC-Net, a novel deep learning framework for classifying nodules from CT scan\nimages using a 3D CNN, considering Hounsfield Unit (HU)-based intensity\nfiltering. Benign and malignant nodules have significant differences in their\nintensity profile of HU, which was not exploited in the literature. Our method\nconsiders the intensity pattern as well as the texture for the prediction of\nmalignancies. LMLCC-Net extracts features from multiple branches that each use\na separate learnable HU-based intensity filtering stage. Various combinations\nof branches and learnable ranges of filters were explored to finally produce\nthe best-performing model. In addition, we propose a semi-supervised learning\nscheme for labeling ambiguous cases and also developed a lightweight model to\nclassify the nodules. The experimental evaluations are carried out on the\nLUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of\n91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of\n91.87%, showing improved performance compared to existing methods. The proposed\nmethod can have a significant impact in helping radiologists in the\nclassification of pulmonary nodules and improving patient care.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06523", "pdf": "https://arxiv.org/pdf/2505.06523", "abs": "https://arxiv.org/abs/2505.06523", "authors": ["Xijie Yang", "Linning Xu", "Lihan Jiang", "Dahua Lin", "Bo Dai"], "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes", "categories": ["cs.GR"], "comment": "project page: https://xijie-yang.github.io/V3DG/", "summary": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital\n3D assets from multi-view images by leveraging a set of 3D Gaussian primitives\nfor rendering. Its explicit and discrete representation facilitates the\nseamless composition of complex digital worlds, offering significant advantages\nover previous neural implicit methods. However, when applied to large-scale\ncompositions, such as crowd-level scenes, it can encompass numerous 3D\nGaussians, posing substantial challenges for real-time rendering. To address\nthis, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D\nGaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D\nGaussian clusters and dynamically selects only the necessary ones to accelerate\nrendering speed. Our approach consists of two stages: (1) Offline Build, where\nhierarchical clusters are generated using a local splatting method to minimize\nvisual differences across granularities, and (2) Online Selection, where\nfootprint evaluation determines perceptible clusters for efficient\nrasterization during rendering. We curate a dataset of synthetic and real-world\nscenes, including objects, trees, people, and buildings, each requiring 0.1\nbillion 3D Gaussians to capture fine details. Experiments show that our\nsolution balances rendering efficiency and visual quality across user-defined\ntolerances, facilitating downstream interactive applications that compose\nextensive 3DGS assets for consistent rendering performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06411", "pdf": "https://arxiv.org/pdf/2505.06411", "abs": "https://arxiv.org/abs/2505.06411", "authors": ["Fangyu Du", "Yang Yang", "Xuehao Gao", "Hongye Hou"], "title": "MAGE:A Multi-stage Avatar Generator with Sparse Observations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inferring full-body poses from Head Mounted Devices, which capture only\n3-joint observations from the head and wrists, is a challenging task with wide\nAR/VR applications. Previous attempts focus on learning one-stage motion\nmapping and thus suffer from an over-large inference space for unobserved body\njoint motions. This often leads to unsatisfactory lower-body predictions and\npoor temporal consistency, resulting in unrealistic or incoherent motion\nsequences. To address this, we propose a powerful Multi-stage Avatar GEnerator\nnamed MAGE that factorizes this one-stage direct motion mapping learning with a\nprogressive prediction strategy. Specifically, given initial 3-joint motions,\nMAGE gradually inferring multi-scale body part poses at different abstract\ngranularity levels, starting from a 6-part body representation and gradually\nrefining to 22 joints. With decreasing abstract levels step by step, MAGE\nintroduces more motion context priors from former prediction stages and thus\nimproves realistic motion completion with richer constraint conditions and less\nambiguity. Extensive experiments on large-scale datasets verify that MAGE\nsignificantly outperforms state-of-the-art methods with better accuracy and\ncontinuity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06413", "pdf": "https://arxiv.org/pdf/2505.06413", "abs": "https://arxiv.org/abs/2505.06413", "authors": ["Ming Liu", "Siyuan Liang", "Koushik Howlader", "Liwen Wang", "Dacheng Tao", "Wensheng Zhang"], "title": "Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have been integrated into autonomous driving\nsystems to enhance reasoning capabilities through tasks such as Visual Question\nAnswering (VQA). However, the robustness of these systems against backdoor\nattacks remains underexplored. In this paper, we propose a natural\nreflection-based backdoor attack targeting VLM systems in autonomous driving\nscenarios, aiming to induce substantial response delays when specific visual\ntriggers are present. We embed faint reflection patterns, mimicking natural\nsurfaces such as glass or water, into a subset of images in the DriveLM\ndataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories\nor system update notifications) to the corresponding textual labels. This\nstrategy trains the model to generate abnormally long responses upon\nencountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and\nLLaMA-Adapter, using parameter-efficient methods. Experimental results\ndemonstrate that while the models maintain normal performance on clean inputs,\nthey exhibit significantly increased inference latency when triggered,\npotentially leading to hazardous delays in real-world autonomous driving\ndecision-making. Further analysis examines factors such as poisoning rates,\ncamera perspectives, and cross-view transferability. Our findings uncover a new\nclass of attacks that exploit the stringent real-time requirements of\nautonomous driving, posing serious challenges to the security and reliability\nof VLM-augmented driving systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06548", "pdf": "https://arxiv.org/pdf/2505.06548", "abs": "https://arxiv.org/abs/2505.06548", "authors": ["Aniruddha Roy", "Pretam Ray", "Abhilash Nandy", "Somak Aditya", "Pawan Goyal"], "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback", "categories": ["cs.CL"], "comment": "11 pages", "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06467", "pdf": "https://arxiv.org/pdf/2505.06467", "abs": "https://arxiv.org/abs/2505.06467", "authors": ["Nisan Chhetri", "Arpan Sainju"], "title": "PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation", "categories": ["cs.CV", "cs.HC"], "comment": "4 pages, 2 figures", "summary": "Generating high-quality images without prompt engineering expertise remains a\nchallenge for text-to-image (T2I) models, which often misinterpret poorly\nstructured prompts, leading to distortions and misalignments. While humans\neasily recognize these flaws, metrics like CLIP fail to capture structural\ninconsistencies, exposing a key limitation in current evaluation methods. To\naddress this, we introduce PromptIQ, an automated framework that refines\nprompts and assesses image quality using our novel Component-Aware Similarity\n(CAS) metric, which detects and penalizes structural errors. Unlike\nconventional methods, PromptIQ iteratively generates and evaluates images until\nthe user is satisfied, eliminating trial-and-error prompt tuning. Our results\nshow that PromptIQ significantly improves generation quality and evaluation\naccuracy, making T2I models more accessible for users with little to no prompt\nengineering expertise.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06552", "pdf": "https://arxiv.org/pdf/2505.06552", "abs": "https://arxiv.org/abs/2505.06552", "authors": ["Doyoung Kim", "Youngjun Lee", "Joeun Kim", "Jihwan Bang", "Hwanjun Song", "Susik Yoon", "Jae-Gil Lee"], "title": "References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dialogue"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06527", "pdf": "https://arxiv.org/pdf/2505.06527", "abs": "https://arxiv.org/abs/2505.06527", "authors": ["Jing Hu", "Kaiwei Yu", "Hongjiang Xian", "Shu Hu", "Xin Wang"], "title": "Improving Generalization of Medical Image Registration Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN", "summary": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06573", "pdf": "https://arxiv.org/pdf/2505.06573", "abs": "https://arxiv.org/abs/2505.06573", "authors": ["Xingchen Li", "LiDian Wang", "Yu Sheng", "ZhiPeng Tang", "Haojie Ren", "Guoliang You", "YiFan Duan", "Jianmin Ji", "Yanyong Zhang"], "title": "ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors", "categories": ["cs.CV"], "comment": null, "summary": "Protecting power transmission lines from potential hazards involves critical\ntasks, one of which is the accurate measurement of distances between power\nlines and potential threats, such as large cranes. The challenge with this task\nis that the current sensor-based methods face challenges in balancing accuracy\nand cost in distance measurement. A common practice is to install cameras on\ntransmission towers, which, however, struggle to measure true 3D distances due\nto the lack of depth information. Although 3D lasers can provide accurate depth\ndata, their high cost makes large-scale deployment impractical.\n  To address this challenge, we present ElectricSight, a system designed for 3D\ndistance measurement and monitoring of potential hazards to power transmission\nlines. This work's key innovations lie in both the overall system framework and\na monocular depth estimation method. Specifically, the system framework\ncombines real-time images with environmental point cloud priors, enabling\ncost-effective and precise 3D distance measurements. As a core component of the\nsystem, the monocular depth estimation method enhances the performance by\nintegrating 3D point cloud data into image-based estimates, improving both the\naccuracy and reliability of the system.\n  To assess ElectricSight's performance, we conducted tests with data from a\nreal-world power transmission scenario. The experimental results demonstrate\nthat ElectricSight achieves an average accuracy of 1.08 m for distance\nmeasurements and an early warning accuracy of 92%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06575", "pdf": "https://arxiv.org/pdf/2505.06575", "abs": "https://arxiv.org/abs/2505.06575", "authors": ["Chengfeng Wang", "Wei Zhai", "Yuhang Yang", "Yang Cao", "Zhengjun Zha"], "title": "GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images", "categories": ["cs.CV"], "comment": null, "summary": "Estimating the geometry level of human-scene contact aims to ground specific\ncontact surface points at 3D human geometries, which provides a spatial prior\nand bridges the interaction between human and scene, supporting applications\nsuch as human behavior analysis, embodied AI, and AR/VR. To complete the task,\nexisting approaches predominantly rely on parametric human models (e.g., SMPL),\nwhich establish correspondences between images and contact regions through\nfixed SMPL vertex sequences. This actually completes the mapping from image\nfeatures to an ordered sequence. However, this approach lacks consideration of\ngeometry, limiting its generalizability in distinct human geometries. In this\npaper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact\nEstimation), a new paradigm for 3D human contact estimation. GRACE incorporates\na point cloud encoder-decoder architecture along with a hierarchical feature\nextraction and fusion module, enabling the effective integration of 3D human\ngeometric structures with 2D interaction semantics derived from images. Guided\nby visual cues, GRACE establishes an implicit mapping from geometric features\nto the vertex space of the 3D human mesh, thereby achieving accurate modeling\nof contact regions. This design ensures high prediction accuracy and endows the\nframework with strong generalization capability across diverse human\ngeometries. Extensive experiments on multiple benchmark datasets demonstrate\nthat GRACE achieves state-of-the-art performance in contact estimation, with\nadditional results further validating its robust generalization to unstructured\nhuman point clouds.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06862", "pdf": "https://arxiv.org/pdf/2505.06862", "abs": "https://arxiv.org/abs/2505.06862", "authors": ["Lhuqita Fazry"], "title": "A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "$\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on\nabstractive text summarization for long documents. However it's capacity still\nlimited to maximum of $4,096$ tokens, thus caused performance degradation on\nsummarization for very long documents. Common method to deal with the issue is\nto truncate the documents. In this reasearch, we'll use different approach.\nWe'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the\nmodel on other domain dataset. First, we filter out all documents which length\nless than $20,000$ tokens to focus on very long documents. To prevent domain\nshifting problem and overfitting on transfer learning due to small dataset, we\naugment the dataset by splitting document-summary training pair into parts, to\nfit the document into $4,096$ tokens. Source code available on\n$\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "summarization"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06578", "pdf": "https://arxiv.org/pdf/2505.06578", "abs": "https://arxiv.org/abs/2505.06578", "authors": ["Maxim Vashkevich", "Egor Krivalcevich"], "title": "Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform", "categories": ["cs.CV", "cs.LG", "68T07", "I.5.1"], "comment": "6 pages, 9 figures", "summary": "The paper presents a learned two-dimensional separable transform (LST) that\ncan be considered as a new type of computational layer for constructing neural\nnetwork (NN) architecture for image recognition tasks. The LST based on the\nidea of sharing the weights of one fullyconnected (FC) layer to process all\nrows of an image. After that, a second shared FC layer is used to process all\ncolumns of image representation obtained from the first layer. The use of LST\nlayers in a NN architecture significantly reduces the number of model\nparameters compared to models that use stacked FC layers. We show that a\nNN-classifier based on a single LST layer followed by an FC layer achieves\n98.02\\% accuracy on the MNIST dataset, while having only 9.5k parameters. We\nalso implemented a LST-based classifier for handwritten digit recognition on\nthe FPGA platform to demonstrate the efficiency of the suggested approach for\ndesigning a compact and high-performance implementation of NN models. Git\nrepository with supplementary materials: https://github.com/Mak-Sim/LST-2d", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06889", "pdf": "https://arxiv.org/pdf/2505.06889", "abs": "https://arxiv.org/abs/2505.06889", "authors": ["Mihyeon Kim", "Juhyoung Park", "Youngbin Kim"], "title": "IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2024 Main", "summary": "Pre-trained Language Models (PLMs) have achieved remarkable performance on\ndiverse NLP tasks through pre-training and fine-tuning. However, fine-tuning\nthe model with a large number of parameters on limited downstream datasets\noften leads to vulnerability to adversarial attacks, causing overfitting of the\nmodel on standard datasets.\n  To address these issues, we propose IM-BERT from the perspective of a dynamic\nsystem by conceptualizing a layer of BERT as a solution of Ordinary\nDifferential Equations (ODEs). Under the situation of initial value\nperturbation, we analyze the numerical stability of two main numerical ODE\nsolvers: the explicit and implicit Euler approaches.\n  Based on these analyses, we introduce a numerically robust IM-connection\nincorporating BERT's layers. This strategy enhances the robustness of PLMs\nagainst adversarial attacks, even in low-resource scenarios, without\nintroducing additional parameters or adversarial training strategies.\n  Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the\nrobustness of IM-BERT under various conditions. Compared to the original BERT,\nIM-BERT exhibits a performance improvement of approximately 8.3\\%p on the\nAdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms\nBERT by achieving 5.9\\%p higher accuracy.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07162", "pdf": "https://arxiv.org/pdf/2505.07162", "abs": "https://arxiv.org/abs/2505.07162", "authors": ["Hajar Sakai", "Sarah S. Lam"], "title": "KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "The increasing volume of healthcare textual data requires computationally\nefficient, yet highly accurate classification approaches able to handle the\nnuanced and complex nature of medical terminology. This research presents\nKnowledge Distillation for Healthcare Multi-Label Text Classification\n(KDH-MLTC), a framework leveraging model compression and Large Language Models\n(LLMs). The proposed approach addresses conventional healthcare Multi-Label\nText Classification (MLTC) challenges by integrating knowledge distillation and\nsequential fine-tuning, subsequently optimized through Particle Swarm\nOptimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from\na more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e.,\nDistilBERT) through sequential training adapted to MLTC that preserves the\nteacher's learned information while significantly reducing computational\nrequirements. As a result, the classification is enabled to be conducted\nlocally, making it suitable for healthcare textual data characterized by\nsensitivity and, therefore, ensuring HIPAA compliance. The experiments\nconducted on three medical literature datasets of different sizes, sampled from\nthe Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves\nsuperior performance compared to existing approaches, particularly for the\nlargest dataset, reaching an F1 score of 82.70%. Additionally, statistical\nvalidation and an ablation study are carried out, proving the robustness of\nKDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process\nallowed the identification of optimal configurations. The proposed approach\ncontributes to healthcare text classification research, balancing efficiency\nrequirements in resource-constrained healthcare settings with satisfactory\naccuracy demands.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06670", "pdf": "https://arxiv.org/pdf/2505.06670", "abs": "https://arxiv.org/abs/2505.06670", "authors": ["Zhe Li", "Hadrien Reynaud", "Mischa Dombrowski", "Sarah Cechnicka", "Franciskus Xaverius Erick", "Bernhard Kainz"], "title": "Video Dataset Condensation with Diffusion Models", "categories": ["cs.CV"], "comment": "10 pages", "summary": "In recent years, the rapid expansion of dataset sizes and the increasing\ncomplexity of deep learning models have significantly escalated the demand for\ncomputational resources, both for data storage and model training. Dataset\ndistillation has emerged as a promising solution to address this challenge by\ngenerating a compact synthetic dataset that retains the essential information\nfrom a large real dataset. However, existing methods often suffer from limited\nperformance and poor data quality, particularly in the video domain. In this\npaper, we focus on video dataset distillation by employing a video diffusion\nmodel to generate high-quality synthetic videos. To enhance representativeness,\nwe introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select\na diverse and informative subset of videos that effectively captures the\ncharacteristics of the original dataset. To further optimize computational\nefficiency, we explore a training-free clustering algorithm, Temporal-Aware\nCluster-based Distillation (TAC-DT), to select representative videos without\nrequiring additional training overhead. We validate the effectiveness of our\napproach through extensive experiments on four benchmark datasets,\ndemonstrating performance improvements of up to \\(10.61\\%\\) over the\nstate-of-the-art. Our method consistently outperforms existing approaches\nacross all datasets, establishing a new benchmark for video dataset\ndistillation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06796", "pdf": "https://arxiv.org/pdf/2505.06796", "abs": "https://arxiv.org/abs/2505.06796", "authors": ["Ye Zhu", "Yunan Wang", "Zitong Yu"], "title": "Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal news contains a wealth of information and is easily affected by\ndeepfake modeling attacks. To combat the latest image and text generation\nmethods, we present a new Multimodal Fake News Detection dataset (MFND)\ncontaining 11 manipulated types, designed to detect and localize highly\nauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning\n(SDML) model for fake news, which fully uses unimodal and mutual modal features\nto mine the intrinsic semantics of news. Under shallow inference, we propose\nthe momentum distillation-based light punishment contrastive learning for\nfine-grained uniform spatial image and text semantic alignment, and an adaptive\ncross-modal fusion module to enhance mutual modal features. Under deep\ninference, we design a two-branch framework to augment the image and text\nunimodal features, respectively merging with mutual modalities features, for\nfour predictions via dedicated detection and localization projections.\nExperiments on both mainstream and our proposed datasets demonstrate the\nsuperiority of the model. Codes and dataset are released at\nhttps://github.com/yunan-wang33/sdml.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07345", "pdf": "https://arxiv.org/pdf/2505.07345", "abs": "https://arxiv.org/abs/2505.07345", "authors": ["Ohjoon Kwon", "Changsu Lee", "Jihye Back", "Lim Sun Suk", "Inho Kang", "Donghyeon Jeon"], "title": "QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have been widely used for relevance assessment\nin information retrieval. However, our study demonstrates that combining two\ndistinct small language models (SLMs) with different architectures can\noutperform LLMs in this task. Our approach -- QUPID -- integrates a generative\nSLM with an embedding-based SLM, achieving higher relevance judgment accuracy\nwhile reducing computational costs compared to state-of-the-art LLM solutions.\nThis computational efficiency makes QUPID highly scalable for real-world search\nsystems processing millions of queries daily. In experiments across diverse\ndocument types, our method demonstrated consistent performance improvements\n(Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x\nfaster inference times. Furthermore, when integrated into production search\npipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how\narchitectural diversity in model combinations can significantly enhance both\nsearch relevance and operational efficiency in information retrieval systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["kappa", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07528", "pdf": "https://arxiv.org/pdf/2505.07528", "abs": "https://arxiv.org/abs/2505.07528", "authors": ["Lei Wang"], "title": "SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) models frequently encounter\nhallucination phenomena when integrating external information with internal\nparametric knowledge. Empirical studies demonstrate that the disequilibrium\nbetween external contextual information and internal parametric knowledge\nconstitutes a primary factor in hallucination generation. Existing\nhallucination detection methodologies predominantly emphasize either the\nexternal or internal mechanism in isolation, thereby overlooking their\nsynergistic effects. The recently proposed ReDeEP framework decouples these\ndual mechanisms, identifying two critical contributors to hallucinations:\nexcessive reliance on parametric knowledge encoded in feed-forward networks\n(FFN) and insufficient utilization of external information by attention\nmechanisms (particularly copy heads). ReDeEP quantitatively assesses these\nfactors to detect hallucinations and dynamically modulates the contributions of\nFFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and\nnumerous other hallucination detection approaches have been employed at\nlogit-level uncertainty estimation or language-level self-consistency\nevaluation, inadequately address the semantic dimensions of model responses,\nresulting in inconsistent hallucination assessments in RAG implementations.\nBuilding upon ReDeEP's foundation, this paper introduces SEReDeEP, which\nenhances computational processes through semantic entropy captured via trained\nlinear probes, thereby achieving hallucination assessments that more accurately\nreflect ground truth evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06903", "pdf": "https://arxiv.org/pdf/2505.06903", "abs": "https://arxiv.org/abs/2505.06903", "authors": ["Yuanzhuo Wang", "Junwen Duan", "Xinyu Li", "Jianxin Wang"], "title": "CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal medical image analysis is essential for clinical decision-making,\nyet existing methods either align images and text at a coarse level - causing\npotential semantic mismatches - or depend solely on visual information, lacking\nmedical semantic integration. We present CheXLearner, the first end-to-end\nframework that unifies anatomical region detection, Riemannian manifold-based\nstructure alignment, and fine-grained regional semantic guidance. Our proposed\nMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to\nrobustly align anatomical structures and capture pathologically meaningful\ndiscrepancies across temporal chest X-rays. By introducing regional progression\ndescriptions as supervision, CheXLearner achieves enhanced cross-modal\nrepresentation learning and supports dynamic low-level feature optimization.\nExperiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and\n80.32% (+11.05%) F1-score on anatomical region progression detection -\nsubstantially outperforming state-of-the-art baselines, especially in\nstructurally complex regions. Additionally, our model attains a 91.52% average\nAUC score in downstream disease classification, validating its superior feature\nrepresentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07591", "pdf": "https://arxiv.org/pdf/2505.07591", "abs": "https://arxiv.org/abs/2505.07591", "authors": ["Junjie Ye", "Caishuang Huang", "Zhuohan Chen", "Wenjie Fu", "Chenyuan Yang", "Leyi Yang", "Yilong Wu", "Peng Wang", "Meng Zhou", "Xiaolong Yang", "Tao Gui", "Qi Zhang", "Zhongchao Shi", "Jianping Fan", "Xuanjing Huang"], "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06912", "pdf": "https://arxiv.org/pdf/2505.06912", "abs": "https://arxiv.org/abs/2505.06912", "authors": ["Chao Ding", "Mouxiao Bian", "Pengcheng Chen", "Hongliang Zhang", "Tianbin Li", "Lihao Liu", "Jiayuan Chen", "Zhuoran Li", "Yabei Zhong", "Yongqi Liu", "Haiqing Huang", "Dongming Shan", "Junjun He", "Jie Xu"], "title": "Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI", "categories": ["cs.CV"], "comment": null, "summary": "Despite strong performance in medical question-answering, the clinical\nadoption of Large Language Models (LLMs) is critically hampered by their opaque\n'black-box' reasoning, limiting clinician trust. This challenge is compounded\nby the predominant reliance of current medical LLMs on corpora from scientific\nliterature or synthetic data, which often lack the granular expert validation\nand high clinical relevance essential for advancing their specialized medical\ncapabilities. To address these critical gaps, we introduce a highly clinically\nrelevant dataset with 31,247 medical question-answer pairs, each accompanied by\nexpert-validated chain-of-thought (CoT) explanations. This resource, spanning\nmultiple clinical domains, was curated via a scalable human-LLM hybrid\npipeline: LLM-generated rationales were iteratively reviewed, scored, and\nrefined by medical experts against a structured rubric, with substandard\noutputs revised through human effort or guided LLM regeneration until expert\nconsensus. This publicly available dataset provides a vital source for the\ndevelopment of medical LLMs that capable of transparent and verifiable\nreasoning, thereby advancing safer and more interpretable AI in medicine.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "rubric"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07671", "pdf": "https://arxiv.org/pdf/2505.07671", "abs": "https://arxiv.org/abs/2505.07671", "authors": ["Xianrui Zhong", "Bowen Jin", "Siru Ouyang", "Yanzhen Shen", "Qiao Jin", "Yin Fang", "Zhiyong Lu", "Jiawei Han"], "title": "Benchmarking Retrieval-Augmented Generation for Chemistry", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06985", "pdf": "https://arxiv.org/pdf/2505.06985", "abs": "https://arxiv.org/abs/2505.06985", "authors": ["Panwen Hu", "Jiehui Huang", "Qiang Sun", "Xiaodan Liang"], "title": "BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Both zero-shot and tuning-based customized text-to-image (CT2I) generation\nhave made significant progress for storytelling content creation. In contrast,\nresearch on customized text-to-video (CT2V) generation remains relatively\nlimited. Existing zero-shot CT2V methods suffer from poor generalization, while\nanother line of work directly combining tuning-based T2I models with temporal\nmotion modules often leads to the loss of structural and texture information.\nTo bridge this gap, we propose an autoregressive structure and texture\npropagation module (STPM), which extracts key structural and texture features\nfrom the reference subject and injects them autoregressively into each video\nframe to enhance consistency. Additionally, we introduce a test-time reward\noptimization (TTRO) method to further refine fine-grained details. Quantitative\nand qualitative experiments validate the effectiveness of STPM and TTRO,\ndemonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency\nmetrics over the baseline, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07705", "pdf": "https://arxiv.org/pdf/2505.07705", "abs": "https://arxiv.org/abs/2505.07705", "authors": ["Letian Peng", "Jingbo Shang"], "title": "Codifying Character Logic in Role-Playing", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07809", "pdf": "https://arxiv.org/pdf/2505.07809", "abs": "https://arxiv.org/abs/2505.07809", "authors": ["Mt Gedeon"], "title": "A Comparative Analysis of Static Word Embeddings for Hungarian", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07073", "pdf": "https://arxiv.org/pdf/2505.07073", "abs": "https://arxiv.org/abs/2505.07073", "authors": ["Payal Varshney", "Adriano Lucieri", "Christoph Balada", "Andreas Dengel", "Sheraz Ahmed"], "title": "Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Concept-based explanations have emerged as an effective approach within\nExplainable Artificial Intelligence, enabling interpretable insights by\naligning model decisions with human-understandable concepts. However, existing\nmethods rely on computationally intensive procedures and struggle to\nefficiently capture complex, semantic concepts. Recently, the Concept Discovery\nthrough Latent Diffusion-based Counterfactual Trajectories (CDCT) framework,\nintroduced by Varshney et al. (2025), attempts to identify concepts via\ndimension-wise traversal of the latent space of a Variational Autoencoder\ntrained on counterfactual trajectories. Extending the CDCT framework, this work\nintroduces Concept Directions via Latent Clustering (CDLC), which extracts\nglobal, class-specific concept directions by clustering latent difference\nvectors derived from factual and diffusion-generated counterfactual image\npairs. CDLC substantially reduces computational complexity by eliminating the\nexhaustive latent dimension traversal required in CDCT and enables the\nextraction of multidimensional semantic concepts encoded across the latent\ndimensions. This approach is validated on a real-world skin lesion dataset,\ndemonstrating that the extracted concept directions align with clinically\nrecognized dermoscopic features and, in some cases, reveal dataset-specific\nbiases or unknown biomarkers. These results highlight that CDLC is\ninterpretable, scalable, and applicable across high-stakes domains and diverse\ndata modalities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dimension"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07119", "pdf": "https://arxiv.org/pdf/2505.07119", "abs": "https://arxiv.org/abs/2505.07119", "authors": ["Arianna Stropeni", "Francesco Borsatti", "Manuel Barusco", "Davide Dalle Pezze", "Marco Fabris", "Gian Antonio Susto"], "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing waste and operational costs is essential. Deploying deep learning\nmodels within Internet of Things (IoT) environments introduces specific\nchallenges due to the limited computational power and bandwidth of edge\ndevices. This study investigates how to perform VAD effectively under such\nconstraints by leveraging compact and efficient processing strategies. We\nevaluate several data compression techniques, examining the trade-off between\nsystem latency and detection accuracy. Experiments on the MVTec AD benchmark\ndemonstrate that significant compression can be achieved with minimal loss in\nanomaly detection performance compared to uncompressed data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06972", "pdf": "https://arxiv.org/pdf/2505.06972", "abs": "https://arxiv.org/abs/2505.06972", "authors": ["Yuichi Sasazawa", "Yasuhiro Sogawa"], "title": "Web Page Classification using LLMs for Crawling Support", "categories": ["cs.IR", "cs.CL"], "comment": "8 pages, 2 figures", "summary": "A web crawler is a system designed to collect web pages, and efficient\ncrawling of new pages requires appropriate algorithms. While website features\nsuch as XML sitemaps and the frequency of past page updates provide important\nclues for accessing new pages, their universal application across diverse\nconditions is challenging. In this study, we propose a method to efficiently\ncollect new pages by classifying web pages into two types, \"Index Pages\" and\n\"Content Pages,\" using a large language model (LLM), and leveraging the\nclassification results to select index pages as starting points for accessing\nnew pages. We construct a dataset with automatically annotated web page types\nand evaluate our approach from two perspectives: the page type classification\nperformance and coverage of new pages. Experimental results demonstrate that\nthe LLM-based method outperformed baseline methods in both evaluation metrics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07209", "pdf": "https://arxiv.org/pdf/2505.07209", "abs": "https://arxiv.org/abs/2505.07209", "authors": ["Yan Xie", "Zequn Zeng", "Hao Zhang", "Yucheng Ding", "Yi Wang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu"], "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Concept Bottleneck Models (CBMs) try to make the decision-making process\ntransparent by exploring an intermediate concept space between the input image\nand the output prediction. Existing CBMs just learn coarse-grained relations\nbetween the whole image and the concepts, less considering local image\ninformation, leading to two main drawbacks: i) they often produce spurious\nvisual-concept relations, hence decreasing model reliability; and ii) though\nCBMs could explain the importance of every concept to the final prediction, it\nis still challenging to tell which visual region produces the prediction. To\nsolve these problems, this paper proposes a Disentangled Optimal Transport CBM\n(DOT-CBM) framework to explore fine-grained visual-concept relations between\nlocal image patches and concepts. Specifically, we model the concept prediction\nprocess as a transportation problem between the patches and concepts, thereby\nachieving explicit fine-grained feature alignment. We also incorporate\northogonal projection losses within the modality to enhance local feature\ndisentanglement. To further address the shortcut issues caused by statistical\nbiases in the data, we utilize the visual saliency map and concept label\nstatistics as transportation priors. Thus, DOT-CBM can visualize inversion\nheatmaps, provide more reliable concept predictions, and produce more accurate\nclass predictions. Comprehensive experiments demonstrate that our proposed\nDOT-CBM achieves SOTA performance on several tasks, including image\nclassification, local part detection and out-of-distribution generalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07251", "pdf": "https://arxiv.org/pdf/2505.07251", "abs": "https://arxiv.org/abs/2505.07251", "authors": ["Wenqiang Wang", "Yangshijie Zhang"], "title": "Incomplete In-context Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision language models (LVLMs) achieve remarkable performance through\nVision In-context Learning (VICL), a process that depends significantly on\ndemonstrations retrieved from an extensive collection of annotated examples\n(retrieval database). Existing studies often assume that the retrieval database\ncontains annotated examples for all labels. However, in real-world scenarios,\ndelays in database updates or incomplete data annotation may result in the\nretrieval database containing labeled samples for only a subset of classes. We\nrefer to this phenomenon as an \\textbf{incomplete retrieval database} and\ndefine the in-context learning under this condition as \\textbf{Incomplete\nIn-context Learning (IICL)}. To address this challenge, we propose\n\\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage\nframework designed to mitigate the limitations of IICL. The Iterative Judgments\nStage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a\nseries of \\(\\boldsymbol{m}\\) binary classification tasks, effectively\nconverting the IICL setting into a standard VICL scenario. The Integrated\nPrediction Stage further refines the classification process by leveraging both\nthe input image and the predictions from the Iterative Judgments Stage to\nenhance overall classification accuracy. IJIP demonstrates considerable\nperformance across two LVLMs and two datasets under three distinct conditions\nof label incompleteness, achieving the highest accuracy of 93.9\\%. Notably,\neven in scenarios where labels are fully available, IJIP still achieves the\nbest performance of all six baselines. Furthermore, IJIP can be directly\napplied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text\ndomain}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167", "abs": "https://arxiv.org/abs/2505.07167", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "code generation"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07254", "pdf": "https://arxiv.org/pdf/2505.07254", "abs": "https://arxiv.org/abs/2505.07254", "authors": ["Mohamed Nagy", "Naoufel Werghi", "Bilal Hassan", "Jorge Dias", "Majid Khonji"], "title": "Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This work addresses the critical lack of precision in state estimation in the\nKalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of\nselecting the appropriate motion model. Existing literature commonly relies on\nconstant motion models for estimating the states of objects, neglecting the\ncomplex motion dynamics unique to each object. Consequently, trajectory\ndivision and imprecise object localization arise, especially under occlusion\nconditions. The core of these challenges lies in the limitations of the current\nKalman filter formulation, which fails to account for the variability of motion\ndynamics as objects navigate their environments. This work introduces a novel\nformulation of the Kalman filter that incorporates motion dynamics, allowing\nthe motion model to adaptively adjust according to changes in the object's\nmovement. The proposed Kalman filter substantially improves state estimation,\nlocalization, and trajectory prediction compared to the traditional Kalman\nfilter. This is reflected in tracking performance that surpasses recent\nbenchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and\n0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking\naccuracy (MOTA), respectively. Furthermore, the proposed Kalman filter\nconsistently outperforms the baseline across various detectors. Additionally,\nit shows an enhanced capability in managing long occlusions compared to the\nbaseline Kalman filter, achieving margins of 1.22\\% in higher order tracking\naccuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the\nKITTI dataset. The formulation's efficiency is evident, with an additional\nprocessing time of only approximately 0.078 ms per frame, ensuring its\napplicability in real-time applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07460", "pdf": "https://arxiv.org/pdf/2505.07460", "abs": "https://arxiv.org/abs/2505.07460", "authors": ["Yi Chen", "JiaHao Zhao", "HaoHao Han"], "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver powerful AI capabilities but face\ndeployment challenges due to high resource costs and latency, whereas Small\nLanguage Models (SLMs) offer efficiency and deployability at the cost of\nreduced performance. Collaboration between LLMs and SLMs emerges as a crucial\nparadigm to synergistically balance these trade-offs, enabling advanced AI\napplications, especially on resource-constrained edge devices. This survey\nprovides a comprehensive overview of LLM-SLM collaboration, detailing various\ninteraction mechanisms (pipeline, routing, auxiliary, distillation, fusion),\nkey enabling technologies, and diverse application scenarios driven by\non-device needs like low latency, privacy, personalization, and offline\noperation. While highlighting the significant potential for creating more\nefficient, adaptable, and accessible AI, we also discuss persistent challenges\nincluding system overhead, inter-model consistency, robust task allocation,\nevaluation complexity, and security/privacy concerns. Future directions point\ntowards more intelligent adaptive frameworks, deeper model fusion, and\nexpansion into multimodal and embodied AI, positioning LLM-SLM collaboration as\na key driver for the next generation of practical and ubiquitous artificial\nintelligence.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704", "abs": "https://arxiv.org/abs/2505.07704", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07306", "pdf": "https://arxiv.org/pdf/2505.07306", "abs": "https://arxiv.org/abs/2505.07306", "authors": ["Sander De Coninck", "Emilio Gamba", "Bart Van Doninck", "Abdellatif Bey-Temsamani", "Sam Leroux", "Pieter Simoens"], "title": "Enabling Privacy-Aware AI-Based Ergonomic Analysis", "categories": ["cs.CV"], "comment": "Accepted and presented at the 35th CIRP Design conference", "summary": "Musculoskeletal disorders (MSDs) are a leading cause of injury and\nproductivity loss in the manufacturing industry, incurring substantial economic\ncosts. Ergonomic assessments can mitigate these risks by identifying workplace\nadjustments that improve posture and reduce strain. Camera-based systems offer\na non-intrusive, cost-effective method for continuous ergonomic tracking, but\nthey also raise significant privacy concerns. To address this, we propose a\nprivacy-aware ergonomic assessment framework utilizing machine learning\ntechniques. Our approach employs adversarial training to develop a lightweight\nneural network that obfuscates video data, preserving only the essential\ninformation needed for human pose estimation. This obfuscation ensures\ncompatibility with standard pose estimation algorithms, maintaining high\naccuracy while protecting privacy. The obfuscated video data is transmitted to\na central server, where state-of-the-art keypoint detection algorithms extract\nbody landmarks. Using multi-view integration, 3D keypoints are reconstructed\nand evaluated with the Rapid Entire Body Assessment (REBA) method. Our system\nprovides a secure, effective solution for ergonomic monitoring in industrial\nenvironments, addressing both privacy and workplace safety concerns.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07444", "pdf": "https://arxiv.org/pdf/2505.07444", "abs": "https://arxiv.org/abs/2505.07444", "authors": ["Zeynep Galymzhankyzy", "Eric Martinson"], "title": "Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture", "categories": ["cs.CV", "I.4.6"], "comment": "4 pages, 5 figures, 1 table", "summary": "Efficient crop-weed segmentation is critical for site-specific weed control\nin precision agriculture. Conventional CNN-based methods struggle to generalize\nand rely on RGB imagery, limiting performance under complex field conditions.\nTo address these challenges, we propose a lightweight transformer-CNN hybrid.\nIt processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using\nspecialized encoders and dynamic modality integration. Evaluated on the\nWeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of\n78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7\nmillion parameters, the model offers high accuracy, computational efficiency,\nand potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and\nedge devices, advancing precision weed management.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07496", "pdf": "https://arxiv.org/pdf/2505.07496", "abs": "https://arxiv.org/abs/2505.07496", "authors": ["Mohamed Ali Souibgui", "Changkyu Choi", "Andrey Barsky", "Kangsoo Jung", "Ernest Valveny", "Dimosthenis Karatzas"], "title": "DocVXQA: Context-Aware Visual Explanations for Document Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose DocVXQA, a novel framework for visually self-explainable document\nquestion answering. The framework is designed not only to produce accurate\nanswers to questions but also to learn visual heatmaps that highlight\ncontextually critical regions, thereby offering interpretable justifications\nfor the model's decisions. To integrate explanations into the learning process,\nwe quantitatively formulate explainability principles as explicit learning\nobjectives. Unlike conventional methods that emphasize only the regions\npertinent to the answer, our framework delivers explanations that are\n\\textit{contextually sufficient} while remaining\n\\textit{representation-efficient}. This fosters user trust while achieving a\nbalance between predictive performance and interpretability in DocVQA\napplications. Extensive experiments, including human evaluation, provide strong\nevidence supporting the effectiveness of our method. The code is available at\nhttps://github.com/dali92002/DocVXQA.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07530", "pdf": "https://arxiv.org/pdf/2505.07530", "abs": "https://arxiv.org/abs/2505.07530", "authors": ["Raul Ismayilov", "Luuk Spreeuwers", "Dzemila Sero"], "title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nwith user-defined identity attribute distributions and paired document-style\nand trusted live capture images. The dataset generated using the FLUXSynID\nframework shows improved alignment with real-world identity distributions and\ngreater inter-set diversity compared to prior work. The FLUXSynID framework for\ngenerating custom datasets, along with a dataset of 14,889 synthetic\nidentities, is publicly released to support biometric research, including face\nrecognition and morphing attack detection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07538", "pdf": "https://arxiv.org/pdf/2505.07538", "abs": "https://arxiv.org/abs/2505.07538", "authors": ["Bohan Wang", "Zhongqi Yue", "Fengda Zhang", "Shuo Chen", "Li'an Bi", "Junzhe Zhang", "Xue Song", "Kennard Yanting Chan", "Jiachun Pan", "Weijia Wu", "Mingze Zhou", "Wang Lin", "Kaihang Pan", "Saining Zhang", "Liyu Jia", "Wentao Hu", "Wei Zhao", "Hanwang Zhang"], "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We completely discard the conventional spatial prior in image representation\nand introduce a novel discrete visual tokenizer: Self-consistency Tokenizer\n(Selftok). At its design core, we compose an autoregressive (AR) prior --\nmirroring the causal structure of language -- into visual tokens by using the\nreverse diffusion process of image generation. The AR property makes Selftok\nfundamentally distinct from traditional spatial tokens in the following two key\nways: - Selftok offers an elegant and minimalist approach to unify diffusion\nand AR for vision-language models (VLMs): By representing images with Selftok\ntokens, we can train a VLM using a purely discrete autoregressive architecture\n-- like that in LLMs -- without requiring additional modules or training\nobjectives. - We theoretically show that the AR prior satisfies the Bellman\nequation, whereas the spatial prior does not. Therefore, Selftok supports\nreinforcement learning (RL) for visual generation with effectiveness comparable\nto that achieved in LLMs. Besides the AR property, Selftok is also a SoTA\ntokenizer that achieves a favorable trade-off between high-quality\nreconstruction and compression rate. We use Selftok to build a pure AR VLM for\nboth visual comprehension and generation tasks. Impressively, without using any\ntext-image training pairs, a simple policy gradient RL working in the visual\ntokens can significantly boost the visual generation benchmark, surpassing all\nthe existing models by a large margin. Therefore, we believe that Selftok\neffectively addresses the long-standing challenge that visual tokens cannot\nsupport effective RL. When combined with the well-established strengths of RL\nin LLMs, this brings us one step closer to realizing a truly multimodal LLM.\nProject Page: https://selftok-team.github.io/report/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07576", "pdf": "https://arxiv.org/pdf/2505.07576", "abs": "https://arxiv.org/abs/2505.07576", "authors": ["Manuel Barusco", "Francesco Borsatti", "Youssef Ben Khalifa", "Davide Dalle Pezze", "Gian Antonio Susto"], "title": "Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semiconductor manufacturing is a complex, multistage process. Automated\nvisual inspection of Scanning Electron Microscope (SEM) images is indispensable\nfor minimizing equipment downtime and containing costs. Most previous research\nconsiders supervised approaches, assuming a sufficient number of anomalously\nlabeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging\nresearch domain, focuses on unsupervised learning, avoiding the costly defect\ncollection phase while providing explanations of the predictions. We introduce\na benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset.\nOur results demonstrate the efficacy of modern VAD approaches in this field.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07611", "pdf": "https://arxiv.org/pdf/2505.07611", "abs": "https://arxiv.org/abs/2505.07611", "authors": ["Yi Zhang", "Wenye Zhou", "Ruonan Lin", "Xin Yang", "Hao Zheng"], "title": "Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions", "categories": ["cs.CV"], "comment": null, "summary": "Traffic accident prediction and detection are critical for enhancing road\nsafety,and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning.This paper reviews 147\nrecent studies,focusing on the application of supervised,unsupervised,and\nhybrid deep learning models for accident prediction,alongside the use of\nreal-world and synthetic datasets.Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatiotemporal\nfeature-based prediction, scene understanding,and multimodal data fusion.While\nthese methods demonstrate significant potential,challenges such as data\nscarcity,limited generalization to complex scenarios,and real-time performance\nconstraints remain prevalent. This review highlights opportunities for future\nresearch,including the integration of multimodal data fusion, self-supervised\nlearning,and Transformer-based architectures to enhance prediction accuracy and\nscalability.By synthesizing existing advancements and identifying critical\ngaps, this paper provides a foundational reference for developing robust and\nadaptive Vision-TAA systems,contributing to road safety and traffic management.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07652", "pdf": "https://arxiv.org/pdf/2505.07652", "abs": "https://arxiv.org/abs/2505.07652", "authors": ["Ozgur Kara", "Krishna Kumar Singh", "Feng Liu", "Duygu Ceylan", "James M. Rehg", "Tobias Hinz"], "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Current diffusion-based text-to-video methods are limited to producing short\nvideo clips of a single shot and lack the capability to generate multi-shot\nvideos with discrete transitions where the same character performs distinct\nactivities across the same or different backgrounds. To address this limitation\nwe propose a framework that includes a dataset collection pipeline and\narchitectural extensions to video diffusion models to enable text-to-multi-shot\nvideo generation. Our approach enables generation of multi-shot videos as a\nsingle video with full attention across all frames of all shots, ensuring\ncharacter and background consistency, and allows users to control the number,\nduration, and content of shots through shot-specific conditioning. This is\nachieved by incorporating a transition token into the text-to-video model to\ncontrol at which frames a new shot begins and a local attention masking\nstrategy which controls the transition token's effect and allows shot-specific\nprompting. To obtain training data we propose a novel data collection pipeline\nto construct a multi-shot video dataset from existing single-shot video\ndatasets. Extensive experiments demonstrate that fine-tuning a pre-trained\ntext-to-video model for a few thousand iterations is enough for the model to\nsubsequently be able to generate multi-shot videos with shot-specific control,\noutperforming the baselines. You can find more details in\nhttps://shotadapter.github.io/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07691", "pdf": "https://arxiv.org/pdf/2505.07691", "abs": "https://arxiv.org/abs/2505.07691", "authors": ["Negin Ghamsarian", "Sahar Nasirihaghighi", "Klaus Schoeffmann", "Raphael Sznitman"], "title": "Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 5 Figures", "summary": "Semi-supervised learning leverages unlabeled data to enhance model\nperformance, addressing the limitations of fully supervised approaches. Among\nits strategies, pseudo-supervision has proven highly effective, typically\nrelying on one or multiple teacher networks to refine pseudo-labels before\ntraining a student network. A common practice in pseudo-supervision is\nfiltering pseudo-labels based on pre-defined confidence thresholds or entropy.\nHowever, selecting optimal thresholds requires large labeled datasets, which\nare often scarce in real-world semi-supervised scenarios. To overcome this\nchallenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic\nfeedback-driven thresholding strategy for pseudo-label selection. Instead of\nrelying on static confidence thresholds, ENCORE estimates class-wise\ntrue-positive confidence within the unlabeled dataset and continuously adjusts\nthresholds based on the model's response to different levels of pseudo-label\nfiltering. This feedback-driven mechanism ensures the retention of informative\npseudo-labels while filtering unreliable ones, enhancing model training without\nmanual threshold tuning. Our method seamlessly integrates into existing\npseudo-supervision frameworks and significantly improves segmentation\nperformance, particularly in data-scarce conditions. Extensive experiments\ndemonstrate that integrating ENCORE with existing pseudo-supervision frameworks\nenhances performance across multiple datasets and network architectures,\nvalidating its effectiveness in semi-supervised learning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07704", "pdf": "https://arxiv.org/pdf/2505.07704", "abs": "https://arxiv.org/abs/2505.07704", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Anton Razzhigaev", "Alexander Panchenko", "Vasily Konovalov"], "title": "Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Measuring how real images look is a complex task in artificial intelligence\nresearch. For example, an image of a boy with a vacuum cleaner in a desert\nviolates common sense. We introduce a novel method, which we call Through the\nLooking Glass (TLG), to assess image common sense consistency using Large\nVision-Language Models (LVLMs) and Transformer-based encoder. By leveraging\nLVLMs to extract atomic facts from these images, we obtain a mix of accurate\nfacts. We proceed by fine-tuning a compact attention-pooling classifier over\nencoded atomic facts. Our TLG has achieved a new state-of-the-art performance\non the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning\ncomponent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07715", "pdf": "https://arxiv.org/pdf/2505.07715", "abs": "https://arxiv.org/abs/2505.07715", "authors": ["Qi Xu", "Jie Deng", "Jiangrong Shen", "Biwu Chen", "Huajin Tang", "Gang Pan"], "title": "Hybrid Spiking Vision Transformer for Object Detection with Event Cameras", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Event-based object detection has gained increasing attention due to its\nadvantages such as high temporal resolution, wide dynamic range, and\nasynchronous address-event representation. Leveraging these advantages, Spiking\nNeural Networks (SNNs) have emerged as a promising approach, offering low\nenergy consumption and rich spatiotemporal dynamics. To further enhance the\nperformance of event-based object detection, this study proposes a novel hybrid\nspike vision Transformer (HsVT) model. The HsVT model integrates a spatial\nfeature extraction module to capture local and global features, and a temporal\nfeature extraction module to model time dependencies and long-term patterns in\nevent sequences. This combination enables HsVT to capture spatiotemporal\nfeatures, improving its capability to handle complex event-based object\ndetection tasks. To support research in this area, we developed and publicly\nreleased The Fall Detection Dataset as a benchmark for event-based object\ndetection tasks. This dataset, captured using an event-based camera, ensures\nfacial privacy protection and reduces memory usage due to the event\nrepresentation format. We evaluated the HsVT model on GEN1 and Fall Detection\ndatasets across various model sizes. Experimental results demonstrate that HsVT\nachieves significant performance improvements in event detection with fewer\nparameters.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06275", "pdf": "https://arxiv.org/pdf/2505.06275", "abs": "https://arxiv.org/abs/2505.06275", "authors": ["Yuzhou Zhu", "Zheng Zhang", "Ruyi Zhang", "Liang Zhou"], "title": "Attonsecond Streaking Phase Retrieval Via Deep Learning Methods", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.optics"], "comment": null, "summary": "Attosecond streaking phase retrieval is essential for resolving electron\ndynamics on sub-femtosecond time scales yet traditional algorithms rely on\niterative minimization and central momentum approximations that degrade\naccuracy for broadband pulses. In this work phase retrieval is reformulated as\na supervised computer-vision problem and four neural architectures are\nsystematically compared. A convolutional network demonstrates strong\nsensitivity to local streak edges but lacks global context; a vision\ntransformer captures long-range delay-energy correlations at the expense of\nlocal inductive bias; a hybrid CNN-ViT model unites local feature extraction\nand full-graph attention; and a capsule network further enforces spatial pose\nagreement through dynamic routing. A theoretical analysis introduces local,\nglobal and positional sensitivity measures and derives surrogate error bounds\nthat predict the strict ordering $CNN<ViT<Hybrid<Capsule$. Controlled\nexperiments on synthetic streaking spectrograms confirm this hierarchy, with\nthe capsule network achieving the highest retrieval fidelity. Looking forward,\nembedding the strong-field integral into physics-informed neural networks and\nexploring photonic hardware implementations promise pathways toward real-time\nattosecond pulse characterization under demanding experimental conditions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06502", "pdf": "https://arxiv.org/pdf/2505.06502", "abs": "https://arxiv.org/abs/2505.06502", "authors": ["Md Rakibul Hasan", "Pouria Behnoudfar", "Dan MacKinlay", "Thomas Poulet"], "title": "PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations", "categories": ["eess.IV", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Machine Learning, particularly Generative Adversarial Networks (GANs), has\nrevolutionised Super Resolution (SR). However, generated images often lack\nphysical meaningfulness, which is essential for scientific applications. Our\napproach, PC-SRGAN, enhances image resolution while ensuring physical\nconsistency for interpretable simulations. PC-SRGAN significantly improves both\nthe Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure\ncompared to conventional methods, even with limited training data (e.g., only\n13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments\nphysically meaningful machine learning, incorporating numerically justified\ntime integrators and advanced quality metrics. These advancements promise\nreliable and causal machine-learning models in scientific domains. A\nsignificant advantage of PC-SRGAN over conventional SR techniques is its\nphysical consistency, which makes it a viable surrogate model for\ntime-dependent problems. PC-SRGAN advances scientific machine learning,\noffering improved accuracy and efficiency for image processing, enhanced\nprocess understanding, and broader applications to scientific research. The\nsource codes and data will be made publicly available at\nhttps://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06685", "pdf": "https://arxiv.org/pdf/2505.06685", "abs": "https://arxiv.org/abs/2505.06685", "authors": ["Dawei Huang", "Qing Li", "Chuan Yan", "Zebang Cheng", "Yurong Huang", "Xiang Li", "Bin Li", "Xiaohui Wang", "Zheng Lian", "Xiaojiang Peng"], "title": "Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Emotion understanding in videos aims to accurately recognize and interpret\nindividuals' emotional states by integrating contextual, visual, textual, and\nauditory cues. While Large Multimodal Models (LMMs) have demonstrated\nsignificant progress in general vision-language (VL) tasks, their performance\nin emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on\nemotion-related tasks often leads to catastrophic forgetting, hindering their\nability to generalize across diverse tasks. To address these challenges, we\npresent Emotion-Qwen, a tailored multimodal framework designed to enhance both\nemotion understanding and general VL reasoning. Emotion-Qwen incorporates a\nsophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,\nwhich dynamically routes inputs to balance emotion-specific and general-purpose\nprocessing. The model is pre-trained in a three-stage pipeline on large-scale\ngeneral and emotional image datasets to support robust multimodal\nrepresentations. Furthermore, we construct the Video Emotion Reasoning (VER)\ndataset, comprising more than 40K bilingual video clips with fine-grained\ndescriptive annotations, to further enrich Emotion-Qwen's emotional reasoning\ncapability. Experimental results demonstrate that Emotion-Qwen achieves\nstate-of-the-art performance on multiple emotion recognition benchmarks, while\nmaintaining competitive results on general VL tasks. Code and models are\navailable at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06746", "pdf": "https://arxiv.org/pdf/2505.06746", "abs": "https://arxiv.org/abs/2505.06746", "authors": ["Morui Zhu", "Yongqi Zhu", "Yihao Zhu", "Qi Chen", "Deyuan Qu", "Song Fu", "Qing Yang"], "title": "M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark", "categories": ["cs.RO", "cs.CV", "I.2.10; I.2.9"], "comment": "supplementary material included", "summary": "We introduce M$^3$CAD, a novel benchmark designed to advance research in\ngeneric cooperative autonomous driving. M$^3$CAD comprises 204 sequences with\n30k frames, spanning a diverse range of cooperative driving scenarios. Each\nsequence includes multiple vehicles and sensing modalities, e.g., LiDAR point\nclouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving\ntasks, including object detection and tracking, mapping, motion forecasting,\noccupancy prediction, and path planning. This rich multimodal setup enables\nM$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving\nresearch, significantly broadening the scope of research in the field. To our\nknowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored\nfor cooperative multi-task autonomous driving research. We evaluate the\nstate-of-the-art end-to-end solution on M$^3$CAD to establish baseline\nperformance. To foster cooperative autonomous driving research, we also propose\nE2EC, a simple yet effective framework for cooperative driving solution that\nleverages inter-vehicle shared information for improved path planning. We\nrelease M$^3$CAD, along with our baseline models and evaluation results, to\nsupport the development of robust cooperative autonomous driving systems. All\nresources will be made publicly available on https://github.com/zhumorui/M3CAD", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06793", "pdf": "https://arxiv.org/pdf/2505.06793", "abs": "https://arxiv.org/abs/2505.06793", "authors": ["Erik Grokopf", "Valay Bundele", "Mehran Hossienzadeh", "Hendrik P. A. Lensch"], "title": "HistDiST: Histopathological Diffusion-based Stain Transfer", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 4 figures", "summary": "Hematoxylin and Eosin (H&E) staining is the cornerstone of histopathology but\nlacks molecular specificity. While Immunohistochemistry (IHC) provides\nmolecular insights, it is costly and complex, motivating H&E-to-IHC translation\nas a cost-effective alternative. Existing translation methods are mainly\nGAN-based, often struggling with training instability and limited structural\nfidelity, while diffusion-based approaches remain underexplored. We propose\nHistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity\nH&E-to-IHC translation. HistDiST introduces a dual-conditioning strategy,\nutilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&E\nrepresentations to ensure pathology-relevant context and structural\nconsistency. To overcome brightness biases, we incorporate a rescaled noise\nschedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition\nat the final timestep. During inference, DDIM inversion preserves the\nmorphological structure, while an eta-cosine noise schedule introduces\ncontrolled stochasticity, balancing structural consistency and molecular\nfidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel\npathology-aware metric leveraging GigaPath embeddings to assess molecular\nrelevance. Extensive evaluations on MIST and BCI datasets demonstrate that\nHistDiST significantly outperforms existing methods, achieving a 28%\nimprovement in MRA on the H&E-to-Ki67 translation task, highlighting its\neffectiveness in capturing true IHC semantics.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07449", "pdf": "https://arxiv.org/pdf/2505.07449", "abs": "https://arxiv.org/abs/2505.07449", "authors": ["Wei Li", "Ming Hu", "Guoan Wang", "Lihao Liu", "Kaijin Zhou", "Junzhi Ning", "Xin Guo", "Zongyuan Ge", "Lixu Gu", "Junjun He"], "title": "Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In ophthalmic surgery, developing an AI system capable of interpreting\nsurgical videos and predicting subsequent operations requires numerous\nophthalmic surgical videos with high-quality annotations, which are difficult\nto collect due to privacy concerns and labor consumption. Text-guided video\ngeneration (T2V) emerges as a promising solution to overcome this issue by\ngenerating ophthalmic surgical videos based on surgeon instructions. In this\npaper, we present Ophora, a pioneering model that can generate ophthalmic\nsurgical videos following natural language instructions. To construct Ophora,\nwe first propose a Comprehensive Data Curation pipeline to convert narrative\nophthalmic surgical videos into a large-scale, high-quality dataset comprising\nover 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive\nVideo-Instruction Tuning scheme to transfer rich spatial-temporal knowledge\nfrom a T2V model pre-trained on natural video-text datasets for\nprivacy-preserved ophthalmic surgical video generation based on Ophora-160K.\nExperiments on video quality evaluation via quantitative analysis and\nophthalmologist feedback demonstrate that Ophora can generate realistic and\nreliable ophthalmic surgical videos based on surgeon instructions. We also\nvalidate the capability of Ophora for empowering downstream tasks of ophthalmic\nsurgical workflow understanding. Code is available at\nhttps://github.com/mar-cry/Ophora.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07548", "pdf": "https://arxiv.org/pdf/2505.07548", "abs": "https://arxiv.org/abs/2505.07548", "authors": ["Lingkun Luo", "Shiqiang Hu", "Liming Chen"], "title": "Noise Optimized Conditional Diffusion for Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 figures This work has been accepted by the International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet\nthe scarcity of High-Confidence Pseudo-Labeled Target Domain Samples\n(\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical\nalignment, causing DA failures. To address this challenge, we propose\n\\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for\n\\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly\nintegrates the generative capabilities of conditional diffusion models with the\ndecision-making requirements of DA to achieve task-coupled optimization for\nefficient adaptation. For robust cross-domain consistency, we modify the DA\nclassifier to align with the conditional diffusion classifier within a unified\noptimization framework, enabling forward training on noise-varying cross-domain\nsamples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0},\n\\mathbf{I}) \\) initialization in diffusion models often generates\nclass-confused hcpl-tds, compromising discriminative DA. To resolve this, we\nintroduce a class-aware noise optimization strategy that refines sampling\nregions for reverse class-specific hcpl-tds generation, effectively enhancing\ncross-domain alignment. Extensive experiments across 5 benchmark datasets and\n29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over\n31 state-of-the-art methods, validating its robustness and effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07675", "pdf": "https://arxiv.org/pdf/2505.07675", "abs": "https://arxiv.org/abs/2505.07675", "authors": ["Seongjae Kang", "Dong Bok Lee", "Hyungjoon Jang", "Sung Ju Hwang"], "title": "Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "41 pages, 19 figures, preprint", "summary": "Vision-language models (VLMs) have achieved remarkable success across diverse\ntasks by leveraging rich textual information with minimal labeled data.\nHowever, deploying such large models remains challenging, particularly in\nresource-constrained environments. Knowledge distillation (KD) offers a\nwell-established solution to this problem; however, recent KD approaches from\nVLMs often involve multi-stage training or additional tuning, increasing\ncomputational overhead and optimization complexity. In this paper, we propose\n$\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead\n$\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet\neffective KD framework that transfers knowledge from VLMs to compact,\ntask-specific models in semi-supervised settings. Specifically, we introduce\ndual prediction heads that independently learn from labeled data and teacher\npredictions, and propose to linearly combine their outputs during inference. We\nobserve that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and\ndistillation signals, enabling more effective feature learning than single-head\nKD baselines. As a result, extensive experiments show that $\\texttt{DHO}$\nconsistently outperforms baselines across multiple domains and fine-grained\ndatasets. Notably, on ImageNet, it achieves state-of-the-art performance,\nimproving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively,\nwhile using fewer parameters.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07687", "pdf": "https://arxiv.org/pdf/2505.07687", "abs": "https://arxiv.org/abs/2505.07687", "authors": ["Feng Yuan", "Yifan Gao", "Wenbin Wu", "Keqing Wu", "Xiaotong Guo", "Jie Jiang", "Xin Gao"], "title": "ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025(under view)", "summary": "Accurate multi-modal medical image translation requires ha-rmonizing global\nanatomical semantics and local structural fidelity, a challenge complicated by\nintermodality information loss and structural distortion. We propose ABS-Mamba,\na novel architecture integrating the Segment Anything Model 2 (SAM2) for\norgan-aware semantic representation, specialized convolutional neural networks\n(CNNs) for preserving modality-specific edge and texture details, and Mamba's\nselective state-space modeling for efficient long- and short-range feature\ndependencies. Structurally, our dual-resolution framework leverages SAM2's\nimage encoder to capture organ-scale semantics from high-resolution inputs,\nwhile a parallel CNNs branch extracts fine-grained local features. The Robust\nFeature Fusion Network (RFFN) integrates these epresentations, and the\nBidirectional Mamba Residual Network (BMRN) models spatial dependencies using\nspiral scanning and bidirectional state-space dynamics. A three-stage skip\nfusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank\nAdaptation (LoRA+) fine-tuning to enable precise domain specialization while\nmaintaining the foundational capabilities of the pre-trained components.\nExtensive experimental validation on the SynthRAD2023 and BraTS2019 datasets\ndemonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering\nhigh-fidelity cross-modal synthesis that preserves anatomical semantics and\nstructural details to enhance diagnostic accuracy in clinical applications. The\ncode is available at https://github.com/gatina-yone/ABS-Mamba", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06356", "pdf": "https://arxiv.org/pdf/2505.06356", "abs": "https://arxiv.org/abs/2505.06356", "authors": ["Karthik Reddy Kanjula", "Surya Guthikonda", "Nahid Alam", "Shayekh Bin Islam"], "title": "Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA", "categories": ["cs.CV"], "comment": "Accepted at ReGenAI CVPR2025 Workshop as Oral", "summary": "Pretraining datasets are foundational to the development of multimodal\nmodels, yet they often have inherent biases and toxic content from the\nweb-scale corpora they are sourced from. In this paper, we investigate the\nprevalence of toxicity in LLaVA image-text pretraining dataset, examining how\nharmful content manifests in different modalities. We present a comprehensive\nanalysis of common toxicity categories and propose targeted mitigation\nstrategies, resulting in the creation of a refined toxicity-mitigated dataset.\nThis dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training\ndataset. We offer guidelines for implementing robust toxicity detection\npipelines. Our findings underscore the need to actively identify and filter\ntoxic content - such as hate speech, explicit imagery, and targeted harassment\n- to build more responsible and equitable multimodal systems. The\ntoxicity-mitigated dataset is open source and is available for further\nresearch.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06416", "pdf": "https://arxiv.org/pdf/2505.06416", "abs": "https://arxiv.org/abs/2505.06416", "authors": ["Elias Lumer", "Anmol Gulati", "Vamse Kumar Subbiah", "Pradeep Honaganahalli Basavaraju", "James A. Burke"], "title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Recent advancements in Large Language Models (LLMs) and the introduction of\nthe Model Context Protocol (MCP) have significantly expanded LLM agents'\ncapability to interact dynamically with external tools and APIs. However,\nexisting tool selection frameworks do not integrate MCP servers, instead\nrelying heavily on error-prone manual updates to monolithic local tool\nrepositories, leading to duplication, inconsistencies, and inefficiencies.\nAdditionally, current approaches abstract tool selection before the LLM agent\nis invoked, limiting its autonomy and hindering dynamic re-querying\ncapabilities during multi-turn interactions. To address these issues, we\nintroduce ScaleMCP, a novel tool selection approach that dynamically equips LLM\nagents with a MCP tool retriever, giving agents the autonomy to add tools into\ntheir memory, as well as an auto-synchronizing tool storage system pipeline\nthrough CRUD (create, read, update, delete) operations with MCP servers as the\nsingle source of truth. We also propose a novel embedding strategy, Tool\nDocument Weighted Average (TDWA), designed to selectively emphasize critical\ncomponents of tool documents (e.g. tool name or synthetic questions) during the\nembedding process. Comprehensive evaluations conducted on a created dataset of\n5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,\nand 5 retriever types, demonstrate substantial improvements in tool retrieval\nand agent invocation performance, emphasizing ScaleMCP's effectiveness in\nscalable, dynamic tool selection and invocation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06512", "pdf": "https://arxiv.org/pdf/2505.06512", "abs": "https://arxiv.org/abs/2505.06512", "authors": ["Hang Wang", "Zhi-Qi Cheng", "Chenhao Lin", "Chao Shen", "Lei Zhang"], "title": "HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation.Our code\nis available at https://github.com/hwang-cs-ime/HCMA", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06516", "pdf": "https://arxiv.org/pdf/2505.06516", "abs": "https://arxiv.org/abs/2505.06516", "authors": ["Yilin Dong", "Tianyun Zhu", "Xinde Li", "Jean Dezert", "Rigui Zhou", "Changming Zhu", "Lei Cao", "Shuzhi Sam Ge"], "title": "Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "16 pages, 28 figures", "summary": "Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to\nderive a quantum mass function (QMF) as a fuzzy metric type from information\nobtained from various data sources. In addition, QDST uses quantum parallel\ncomputing to speed up computation. Nevertheless, the effective management of\nconflicts between multiple QMFs in QDST is a challenging question. This work\naims to address this problem by proposing a Quantum Conflict Indicator (QCI)\nthat measures the conflict between two QMFs in decision-making. Then, the\nproperties of the QCI are carefully investigated. The obtained results validate\nits compliance with desirable conflict measurement properties such as\nnon-negativity, symmetry, boundedness, extreme consistency and insensitivity to\nrefinement. We then apply the proposed QCI in conflict fusion methods and\ncompare its performance with several commonly used fusion approaches. This\ncomparison demonstrates the superiority of the QCI-based conflict fusion\nmethod. Moreover, the Class Description Domain Space (C-DDS) and its optimized\nversion, C-DDS+ by utilizing the QCI-based fusion method, are proposed to\naddress the Out-of-Distribution (OOD) detection task. The experimental results\nshow that the proposed approach gives better OOD performance with respect to\nseveral state-of-the-art baseline OOD detection methods. Specifically, it\nachieves an average increase in Area Under the Receiver Operating\nCharacteristic Curve (AUC) of 1.2% and a corresponding average decrease in\nFalse Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the\noptimal baseline method.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06517", "pdf": "https://arxiv.org/pdf/2505.06517", "abs": "https://arxiv.org/abs/2505.06517", "authors": ["Xiaohong Huang", "Cui Yang", "Miaowen Wen"], "title": "Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages with 9 figures", "summary": "This paper presents a visual-inertial odometry (VIO) method using\nlong-tracked features. Long-tracked features can constrain more visual frames,\nreducing localization drift. However, they may also lead to accumulated\nmatching errors and drift in feature tracking. Current VIO methods adjust\nobservation weights based on re-projection errors, yet this approach has flaws.\nRe-projection errors depend on estimated camera poses and map points, so\nincreased errors might come from estimation inaccuracies, not actual feature\ntracking errors. This can mislead the optimization process and make\nlong-tracked features ineffective for suppressing localization drift.\nFurthermore, long-tracked features constrain a larger number of frames, which\nposes a significant challenge to real-time performance of the system. To tackle\nthese issues, we propose an active decoupling mechanism for accumulated errors\nin long-tracked feature utilization. We introduce a visual reference frame\nreset strategy to eliminate accumulated tracking errors and a depth prediction\nstrategy to leverage the long-term constraint. To ensure real time preformane,\nwe implement three strategies for efficient system state estimation: a parallel\nelimination strategy based on predefined elimination order, an inverse-depth\nelimination simplification strategy, and an elimination skipping strategy.\nExperiments on various datasets show that our method offers higher positioning\naccuracy with relatively short consumption time, making it more suitable for\nedge-enabled low-altitude IoT navigation, where high-accuracy positioning and\nreal-time operation on edge device are required. The code will be published at\ngithub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06524", "pdf": "https://arxiv.org/pdf/2505.06524", "abs": "https://arxiv.org/abs/2505.06524", "authors": ["Jingyao Wang", "Jianqi Zhang", "Wenwen Qiang", "Changwen Zheng"], "title": "Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the strength of the Segment Anything Model (SAM), it struggles with\ngeneralization issues in open-vocabulary multi-entity segmentation (OVMS).\nThrough empirical and causal analyses, we find that (i) the prompt bias is the\nprimary cause of the generalization issues; (ii) this bias is closely tied to\nthe task-irrelevant generating factors within the prompts, which act as\nconfounders and affect generalization. To address the generalization issues, we\naim to propose a method that can calibrate prompts to eliminate confounders for\naccurate OVMS. Building upon the causal analysis, we propose that the optimal\nprompt for OVMS should contain only task-relevant causal factors. We define it\nas the causal prompt, serving as the goal of calibration. Next, our theoretical\nanalysis, grounded by causal multi-distribution consistency theory, proves that\nthis prompt can be obtained by enforcing segmentation consistency and\noptimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration\nmethod for SAM to achieve accurate OVMS. It integrates a lightweight causal\nprompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first\ngenerate multiple prompts using random annotations to simulate diverse\ndistributions and then reweight them via CaPL by enforcing causal\nmulti-distribution consistency in both task and entity levels. To ensure\nobtaining causal prompts, CaPL is optimized by minimizing the cumulative\nsegmentation loss across the reweighted prompts to achieve consistency and\noptimality. A bi-level optimization strategy alternates between optimizing CaPL\nand SAM, ensuring accurate OVMS. Extensive experiments validate its\nsuperiority.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06607", "pdf": "https://arxiv.org/pdf/2505.06607", "abs": "https://arxiv.org/abs/2505.06607", "authors": ["Min Li", "Chun Yuan"], "title": "Boosting Neural Language Inference via Cascaded Interactive Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Inference (NLI) focuses on ascertaining the logical\nrelationship (entailment, contradiction, or neutral) between a given premise\nand hypothesis. This task presents significant challenges due to inherent\nlinguistic features such as diverse phrasing, semantic complexity, and\ncontextual nuances. While Pre-trained Language Models (PLMs) built upon the\nTransformer architecture have yielded substantial advancements in NLI,\nprevailing methods predominantly utilize representations from the terminal\nlayer. This reliance on final-layer outputs may overlook valuable information\nencoded in intermediate layers, potentially limiting the capacity to model\nintricate semantic interactions effectively. Addressing this gap, we introduce\nthe Cascaded Interactive Reasoning Network (CIRN), a novel architecture\ndesigned for deeper semantic comprehension in NLI. CIRN implements a\nhierarchical feature extraction strategy across multiple network depths,\noperating within an interactive space where cross-sentence information is\ncontinuously integrated. This mechanism aims to mimic a process of progressive\nreasoning, transitioning from surface-level feature matching to uncovering more\nprofound logical and semantic connections between the premise and hypothesis.\nBy systematically mining latent semantic relationships at various\nrepresentational levels, CIRN facilitates a more thorough understanding of the\ninput pair. Comprehensive evaluations conducted on several standard NLI\nbenchmark datasets reveal consistent performance gains achieved by CIRN over\ncompetitive baseline approaches, demonstrating the efficacy of leveraging\nmulti-level interactive features for complex relational reasoning.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06630", "pdf": "https://arxiv.org/pdf/2505.06630", "abs": "https://arxiv.org/abs/2505.06630", "authors": ["Chunyi Yue", "Ang Li"], "title": "Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "17 pages, 5 figures, 3 tables", "summary": "Multi-domain sentiment classification aims to mitigate poor performance\nmodels due to the scarcity of labeled data in a single domain, by utilizing\ndata labeled from various domains. A series of models that jointly train domain\nclassifiers and sentiment classifiers have demonstrated their advantages,\nbecause domain classification helps generate necessary information for\nsentiment classification. Intuitively, the importance of sentiment\nclassification tasks is the same in all domains for multi-domain sentiment\nclassification; but domain classification tasks are different because the\nimpact of domain information on sentiment classification varies across\ndifferent fields; this can be controlled through adjustable weights or hyper\nparameters. However, as the number of domains increases, existing\nhyperparameter optimization algorithms may face the following challenges: (1)\ntremendous demand for computing resources, (2) convergence problems, and (3)\nhigh algorithm complexity. To efficiently generate the domain information\nrequired for sentiment classification in each domain, we propose a dynamic\ninformation modulation algorithm. Specifically, the model training process is\ndivided into two stages. In the first stage, a shared hyperparameter, which\nwould control the proportion of domain classification tasks across all fields,\nis determined. In the second stage, we introduce a novel domain-aware\nmodulation algorithm to adjust the domain information contained in the input\ntext, which is then calculated based on a gradient-based and loss-based method.\nIn summary, experimental results on a public sentiment analysis dataset\ncontaining 16 domains prove the superiority of the proposed method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06543", "pdf": "https://arxiv.org/pdf/2505.06543", "abs": "https://arxiv.org/abs/2505.06543", "authors": ["Shuhan Zhuang", "Mengqi Huang", "Fengyi Fu", "Nan Chen", "Bohan Lei", "Zhendong Mao"], "title": "HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Visual text rendering, which aims to accurately integrate specified textual\ncontent within generated images, is critical for various applications such as\ncommercial design. Despite recent advances, current methods struggle with\nlong-tail text cases, particularly when handling unseen or small-sized text. In\nthis work, we propose a novel Hierarchical Disentangled Glyph-Based framework\n(HDGlyph) that hierarchically decouples text generation from non-text visual\nsynthesis, enabling joint optimization of both common and long-tail text\nrendering. At the training stage, HDGlyph disentangles pixel-level\nrepresentations via the Multi-Linguistic GlyphNet and the Glyph-Aware\nPerceptual Loss, ensuring robust rendering even for unseen characters. At\ninference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and\nLatent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both\nbackground and small-sized text. Extensive evaluations show our model\nconsistently outperforms others, with 5.08% and 11.7% accuracy gains in English\nand Chinese text rendering while maintaining high image quality. It also excels\nin long-tail scenarios with strong accuracy and visual performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06660", "pdf": "https://arxiv.org/pdf/2505.06660", "abs": "https://arxiv.org/abs/2505.06660", "authors": ["Junyi Peng", "Takanori Ashihara", "Marc Delcroix", "Tsubasa Ochiai", "Oldrich Plchot", "Shoko Araki", "Jan ernock"], "title": "TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at ICASSP 2025", "summary": "Self-supervised learning (SSL) models have significantly advanced speech\nprocessing tasks, and several benchmarks have been proposed to validate their\neffectiveness. However, previous benchmarks have primarily focused on\nsingle-speaker scenarios, with less exploration of target-speaker tasks in\nnoisy, multi-talker conditions -- a more challenging yet practical case. In\nthis paper, we introduce the Target-Speaker Speech Processing Universal\nPerformance Benchmark (TS-SUPERB), which includes four widely recognized\ntarget-speaker processing tasks that require identifying the target speaker and\nextracting information from the speech mixture. In our benchmark, the speaker\nembedding extracted from enrollment speech is used as a clue to condition\ndownstream models. The benchmark result reveals the importance of evaluating\nSSL models in target speaker scenarios, demonstrating that performance cannot\nbe easily inferred from related single-speaker tasks. Moreover, by using a\nunified SSL-based target speech encoder, consisting of a speaker encoder and an\nextractor module, we also investigate joint optimization across TS tasks to\nleverage mutual information and demonstrate its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06696", "pdf": "https://arxiv.org/pdf/2505.06696", "abs": "https://arxiv.org/abs/2505.06696", "authors": ["Dominik Koterwa", "Maciej witaa"], "title": "Enhancing BERTopic with Intermediate Layer Representations", "categories": ["cs.CL"], "comment": "Repository with code for reproduction:\n  https://github.com/dkoterwa/optimizing_bertopic", "summary": "BERTopic is a topic modeling algorithm that leverages transformer-based\nembeddings to create dense clusters, enabling the estimation of topic\nstructures and the extraction of valuable insights from a corpus of documents.\nThis approach allows users to efficiently process large-scale text data and\ngain meaningful insights into its structure. While BERTopic is a powerful tool,\nembedding preparation can vary, including extracting representations from\nintermediate model layers and applying transformations to these embeddings. In\nthis study, we evaluate 18 different embedding representations and present\nfindings based on experiments conducted on three diverse datasets. To assess\nthe algorithm's performance, we report topic coherence and topic diversity\nmetrics across all experiments. Our results demonstrate that, for each dataset,\nit is possible to find an embedding configuration that performs better than the\ndefault setting of BERTopic. Additionally, we investigate the influence of stop\nwords on different embedding configurations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06566", "pdf": "https://arxiv.org/pdf/2505.06566", "abs": "https://arxiv.org/abs/2505.06566", "authors": ["Zequn Xie", "Haoming Ji", "Lingwei Meng"], "title": "Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image person search aims to identify an individual based on a text\ndescription. To reduce data collection costs, large-scale text-image datasets\nare created from co-occurrence pairs found online. However, this can introduce\nnoise, particularly mismatched pairs, which degrade retrieval performance.\nExisting methods often focus on negative samples, amplifying this noise. To\naddress these issues, we propose the Dynamic Uncertainty and Relational\nAlignment (DURA) framework, which includes the Key Feature Selector (KFS) and a\nnew loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and\nmodels noise uncertainty, improving retrieval reliability. The bidirectional\nevidence from cross-modal similarity is modeled as a Dirichlet distribution,\nenhancing adaptability to noisy data. DSH adjusts the difficulty of negative\nsamples to improve robustness in noisy environments. Our experiments on three\ndatasets show that the method offers strong noise resistance and improves\nretrieval performance in both low- and high-noise scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06708", "pdf": "https://arxiv.org/pdf/2505.06708", "abs": "https://arxiv.org/abs/2505.06708", "authors": ["Zihan Qiu", "Zekun Wang", "Bo Zheng", "Zeyu Huang", "Kaiyue Wen", "Songlin Yang", "Rui Men", "Le Yu", "Fei Huang", "Suozhi Huang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", "categories": ["cs.CL"], "comment": null, "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and\nHighway Networks to recent state space models, linear attention, and also\nsoftmax attention. Yet, existing literature rarely examines the specific\neffects of gating. In this work, we conduct comprehensive experiments to\nsystematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B\nMixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion\ntoken dataset. Our central finding is that a simple modification-applying a\nhead-specific sigmoid gate after the Scaled Dot-Product Attention\n(SDPA)-consistently improves performance. This modification also enhances\ntraining stability, tolerates larger learning rates, and improves scaling\nproperties. By comparing various gating positions and computational variants,\nwe attribute this effectiveness to two key factors: (1) introducing\nnon-linearity upon the low-rank mapping in the softmax attention, and (2)\napplying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates 'attention sink' and\nenhances long-context extrapolation performance, and we also release related\n$\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and\n$\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate\nfuture research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06592", "pdf": "https://arxiv.org/pdf/2505.06592", "abs": "https://arxiv.org/abs/2505.06592", "authors": ["H M Dipu Kabir", "Subrota Kumar Mondal", "Mohammad Ali Moni"], "title": "Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes batch augmentation with unimodal fine-tuning to detect\nthe fetus's organs from ultrasound images and associated clinical textual\ninformation. We also prescribe pre-training initial layers with investigated\nmedical data before the multimodal training. At first, we apply a transferred\ninitialization with the unimodal image portion of the dataset with batch\naugmentation. This step adjusts the initial layer weights for medical data.\nThen, we apply neural networks (NNs) with fine-tuned initial layers to images\nin batches with batch augmentation to obtain features. We also extract\ninformation from descriptions of images. We combine this information with\nfeatures obtained from images to train the head layer. We write a dataloader\nscript to load the multimodal data and use existing unimodal image augmentation\ntechniques with batch augmentation for the multimodal data. The dataloader\nbrings a new random augmentation for each batch to get a good generalization.\nWe investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The\nmultimodal large language model (LLM) with the proposed training provides the\nbest results among the investigated methods. We receive near state-of-the-art\n(SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the\nproposed method with traditional counterparts at the following repository:\ngithub.com/dipuk0506/multimodal", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06904", "pdf": "https://arxiv.org/pdf/2505.06904", "abs": "https://arxiv.org/abs/2505.06904", "authors": ["Xinyi Mou", "Chen Qian", "Wei Liu", "Xuanjing Huang", "Zhongyu Wei"], "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have demonstrated an impressive ability to\nrole-play humans and replicate complex social dynamics. While large-scale\nsocial simulations are gaining increasing attention, they still face\nsignificant challenges, particularly regarding high time and computation costs.\nExisting solutions, such as distributed mechanisms or hybrid agent-based model\n(ABM) integrations, either fail to address inference costs or compromise\naccuracy and generalizability. To this end, we propose EcoLANG: Efficient and\nEffective Agent Communication Language Induction for Social Simulation. EcoLANG\noperates in two stages: (1) language evolution, where we filter synonymous\nwords and optimize sentence-level rules through natural selection, and (2)\nlanguage utilization, where agents in social simulations communicate using the\nevolved language. Experimental results demonstrate that EcoLANG reduces token\nconsumption by over 20%, enhancing efficiency without sacrificing simulation\naccuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06603", "pdf": "https://arxiv.org/pdf/2505.06603", "abs": "https://arxiv.org/abs/2505.06603", "authors": ["Lei Hu", "Zhiyong Gan", "Ling Deng", "Jinglin Liang", "Lingyu Liang", "Shuangping Huang", "Tianshui Chen"], "title": "ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Continual Anomaly Detection (CAD) enables anomaly detection models in\nlearning new classes while preserving knowledge of historical classes. CAD\nfaces two key challenges: catastrophic forgetting and segmentation of small\nanomalous regions. Existing CAD methods store image distributions or patch\nfeatures to mitigate catastrophic forgetting, but they fail to preserve\npixel-level detailed features for accurate segmentation. To overcome this\nlimitation, we propose ReplayCAD, a novel diffusion-driven generative replay\nframework that replay high-quality historical data, thus effectively preserving\npixel-level detailed features. Specifically, we compress historical data by\nsearching for a class semantic embedding in the conditional space of the\npre-trained diffusion model, which can guide the model to replay data with\nfine-grained pixel details, thus improving the segmentation performance.\nHowever, relying solely on semantic features results in limited spatial\ndiversity. Hence, we further use spatial features to guide data compression,\nachieving precise control of sample space, thereby generating more diverse\ndata. Our method achieves state-of-the-art performance in both classification\nand segmentation, with notable improvements in segmentation: 11.5% on VisA and\n8.1% on MVTec. Our source code is available at\nhttps://github.com/HULEI7/ReplayCAD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06914", "pdf": "https://arxiv.org/pdf/2505.06914", "abs": "https://arxiv.org/abs/2505.06914", "authors": ["Chen Amiraz", "Florin Cuconasu", "Simone Filice", "Zohar Karnin"], "title": "The Distracting Effect: Understanding Irrelevant Passages in RAG", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "A well-known issue with Retrieval Augmented Generation (RAG) is that\nretrieved passages that are irrelevant to the query sometimes distract the\nanswer-generating LLM, causing it to provide an incorrect response. In this\npaper, we shed light on this core issue and formulate the distracting effect of\na passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the\ndistracting effect of a passage and demonstrate its robustness across LLMs.\n  Our research introduces novel methods for identifying and using hard\ndistracting passages to improve RAG systems. By fine-tuning LLMs with these\ncarefully selected distracting passages, we achieve up to a 7.5% increase in\nanswering accuracy compared to counterparts fine-tuned on conventional RAG\ndatasets. Our contribution is two-fold: first, we move beyond the simple binary\nclassification of irrelevant passages as either completely unrelated vs.\ndistracting, and second, we develop and analyze multiple methods for finding\nhard distracting passages. To our knowledge, no other research has provided\nsuch a comprehensive framework for identifying and utilizing hard distracting\npassages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06647", "pdf": "https://arxiv.org/pdf/2505.06647", "abs": "https://arxiv.org/abs/2505.06647", "authors": ["Zhe Li", "Sarah Cechnicka", "Cheng Ouyang", "Katharina Breininger", "Peter Schffler", "Bernhard Kainz"], "title": "Dataset Distillation with Probabilistic Latent Features", "categories": ["cs.CV"], "comment": "23 pages", "summary": "As deep learning models grow in complexity and the volume of training data\nincreases, reducing storage and computational costs becomes increasingly\nimportant. Dataset distillation addresses this challenge by synthesizing a\ncompact set of synthetic data that can effectively replace the original dataset\nin downstream classification tasks. While existing methods typically rely on\nmapping data from pixel space to the latent space of a generative model, we\npropose a novel stochastic approach that models the joint distribution of\nlatent features. This allows our method to better capture spatial structures\nand produce diverse synthetic samples, which benefits model training.\nSpecifically, we introduce a low-rank multivariate normal distribution\nparameterized by a lightweight network. This design maintains low computational\ncomplexity and is compatible with various matching networks used in dataset\ndistillation. After distillation, synthetic images are generated by feeding the\nlearned latent features into a pretrained generator. These synthetic images are\nthen used to train classification models, and performance is evaluated on real\ntest set. We validate our method on several benchmarks, including ImageNet\nsubsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach\nachieves state-of-the-art cross architecture performance across a range of\nbackbone architectures, demonstrating its generality and effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07161", "pdf": "https://arxiv.org/pdf/2505.07161", "abs": "https://arxiv.org/abs/2505.07161", "authors": ["Jannatun Naim", "Jie Cao", "Fareen Tasneem", "Jennifer Jacobs", "Brent Milne", "James Martin", "Tamara Sumner"], "title": "Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted to EDM'2025", "summary": "Effective feedback is essential for refining instructional practices in\nmathematics education, and researchers often turn to advanced natural language\nprocessing (NLP) models to analyze classroom dialogues from multiple\nperspectives. However, utterance-level discourse analysis encounters two\nprimary challenges: (1) multifunctionality, where a single utterance may serve\nmultiple purposes that a single tag cannot capture, and (2) the exclusion of\nmany utterances from domain-specific discourse move classifications, leading to\ntheir omission in feedback. To address these challenges, we proposed a\nmulti-perspective discourse analysis that integrates domain-specific talk moves\nwith dialogue act (using the flattened multi-functional SWBD-MASL schema with\n43 tags) and discourse relation (applying Segmented Discourse Representation\nTheory with 16 relations). Our top-down analysis framework enables a\ncomprehensive understanding of utterances that contain talk moves, as well as\nutterances that do not contain talk moves. This is applied to two mathematics\neducation datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through\ndistributional unigram analysis, sequential talk move analysis, and multi-view\ndeep dive, we discovered meaningful discourse patterns, and revealed the vital\nrole of utterances without talk moves, demonstrating that these utterances, far\nfrom being mere fillers, serve crucial functions in guiding, acknowledging, and\nstructuring classroom discourse. These insights underscore the importance of\nincorporating discourse relations and dialogue acts into AI-assisted education\nsystems to enhance feedback and create more responsive learning environments.\nOur framework may prove helpful for providing human educator feedback, but also\naiding in the development of AI agents that can effectively emulate the roles\nof both educators and students.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06679", "pdf": "https://arxiv.org/pdf/2505.06679", "abs": "https://arxiv.org/abs/2505.06679", "authors": ["Jiayang Liu", "Siyuan Liang", "Shiqian Zhao", "Rongcheng Tu", "Wenbo Zhou", "Xiaochun Cao", "Dacheng Tao", "Siew Kei Lam"], "title": "Jailbreaking the Text-to-Video Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generative models have achieved significant progress, driven by\nthe rapid advancements in diffusion models, with notable examples including\nPika, Luma, Kling, and Sora. Despite their remarkable generation ability, their\nvulnerability to jailbreak attack, i.e. to generate unsafe content, including\npornography, violence, and discrimination, raises serious safety concerns.\nExisting efforts, such as T2VSafetyBench, have provided valuable benchmarks for\nevaluating the safety of text-to-video models against unsafe prompts but lack\nsystematic studies for exploiting their vulnerabilities effectively. In this\npaper, we propose the \\textit{first} optimization-based jailbreak attack\nagainst text-to-video models, which is specifically designed. Our approach\nformulates the prompt generation task as an optimization problem with three key\nobjectives: (1) maximizing the semantic similarity between the input and\ngenerated prompts, (2) ensuring that the generated prompts can evade the safety\nfilter of the text-to-video model, and (3) maximizing the semantic similarity\nbetween the generated videos and the original input prompts. To further enhance\nthe robustness of the generated prompts, we introduce a prompt mutation\nstrategy that creates multiple prompt variants in each iteration, selecting the\nmost effective one based on the averaged score. This strategy not only improves\nthe attack success rate but also boosts the semantic relevance of the generated\nvideo. We conduct extensive experiments across multiple text-to-video models,\nincluding Open-Sora, Pika, Luma, and Kling. The results demonstrate that our\nmethod not only achieves a higher attack success rate compared to baseline\nmethods but also generates videos with greater semantic similarity to the\noriginal input prompts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06694", "pdf": "https://arxiv.org/pdf/2505.06694", "abs": "https://arxiv.org/abs/2505.06694", "authors": ["XiaoTong Gu", "Shengyu Tang", "Yiming Cao", "Changdong Yu"], "title": "Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater object detection using sonar imagery has become a critical and\nrapidly evolving research domain within marine technology. However, sonar\nimages are characterized by lower resolution and sparser features compared to\noptical images, which seriously degrades the performance of object detection.To\naddress these challenges, we specifically propose a Detection Transformer\n(DETR) architecture optimized with a Neural Architecture Search (NAS) approach\ncalled NAS-DETR for object detection in sonar images. First, an improved\nZero-shot Neural Architecture Search (NAS) method based on the maximum entropy\nprinciple is proposed to identify a real-time, high-representational-capacity\nCNN-Transformer backbone for sonar image detection. This method enables the\nefficient discovery of high-performance network architectures with low\ncomputational and time overhead. Subsequently, the backbone is combined with a\nFeature Pyramid Network (FPN) and a deformable attention-based Transformer\ndecoder to construct a complete network architecture. This architecture\nintegrates various advanced components and training schemes to enhance overall\nperformance. Extensive experiments demonstrate that this architecture achieves\nstate-of-the-art performance on two Representative datasets, while maintaining\nminimal overhead in real-time efficiency and computational complexity.\nFurthermore, correlation analysis between the key parameters and differential\nentropy-based fitness function is performed to enhance the interpretability of\nthe proposed framework. To the best of our knowledge, this is the first work in\nthe field of sonar object detection to integrate the DETR architecture with a\nNAS search mechanism.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07258", "pdf": "https://arxiv.org/pdf/2505.07258", "abs": "https://arxiv.org/abs/2505.07258", "authors": ["Wenqiang Wang", "Siyuan Liang", "Yangshijie Zhang", "Xiaojun Jia", "Hao Lin", "Xiaochun Cao"], "title": "No Query, No Access", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual adversarial attacks mislead NLP models, including Large Language\nModels (LLMs), by subtly modifying text. While effective, existing attacks\noften require knowledge of the victim model, extensive queries, or access to\ntraining data, limiting real-world feasibility. To overcome these constraints,\nwe introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which\noperates using only victim texts. To prevent access to the victim model, we\ncreate a shadow dataset with publicly available pre-trained models and\nclustering methods as a foundation for developing substitute models. To address\nthe low attack success rate (ASR) due to insufficient information feedback, we\npropose the hierarchical substitution model design, generating substitute\nmodels to mitigate the failure of a single substitute model at the decision\nboundary.\n  Concurrently, we use diverse adversarial example generation, employing\nvarious attack methods to generate and select the adversarial example with\nbetter similarity and attack effectiveness. Experiments on the Emotion and SST5\ndatasets show that VDBA outperforms state-of-the-art methods, achieving an ASR\nimprovement of 52.08\\% while significantly reducing attack queries to 0. More\nimportantly, we discover that VDBA poses a significant threat to LLMs such as\nQwen2 and the GPT family, and achieves the highest ASR of 45.99% even without\naccess to the API, confirming that advanced NLP models still face serious\nsecurity risks. Our codes can be found at\nhttps://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06745", "pdf": "https://arxiv.org/pdf/2505.06745", "abs": "https://arxiv.org/abs/2505.06745", "authors": ["Parth Padalkar", "Gopal Gupta"], "title": "Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent neuro-symbolic approaches have successfully extracted symbolic\nrule-sets from CNN-based models to enhance interpretability. However, applying\nsimilar techniques to Vision Transformers (ViTs) remains challenging due to\ntheir lack of modular concept detectors and reliance on global self-attention\nmechanisms. We propose a framework for symbolic rule extraction from ViTs by\nintroducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This\nlinear layer operates on attention-weighted patch representations and learns a\ndisentangled, binarized representation in which individual neurons activate for\nhigh-level visual concepts. To encourage interpretability, we apply a\ncombination of L1 sparsity, entropy minimization, and supervised contrastive\nloss. These binarized concept activations are used as input to the FOLD-SE-M\nalgorithm, which generates a rule-set in the form of logic programs. Our method\nachieves a 5.14% better classification accuracy than the standard ViT while\nenabling symbolic reasoning. Crucially, the extracted rule-set is not merely\npost-hoc but acts as a logic-based decision layer that operates directly on the\nsparse concept representations. The resulting programs are concise and\nsemantically meaningful. This work is the first to extract executable logic\nprograms from ViTs using sparse symbolic representations. It bridges the gap\nbetween transformer-based vision models and symbolic logic programming,\nproviding a step forward in interpretable and verifiable neuro-symbolic AI.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07289", "pdf": "https://arxiv.org/pdf/2505.07289", "abs": "https://arxiv.org/abs/2505.07289", "authors": ["Stanislas Laborde", "Martin Cousseau", "Antoun Yaacoub", "Lionel Prevost"], "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "categories": ["cs.CL", "cs.AI", "cs.LG", "68P30 (Primary) 68T07, 68T50 (Secondary)", "I.2.6; I.5.1; I.2.7"], "comment": "Accepted for publication in the Proceedings of the 2025 International\n  Joint Conference on Neural Networks (IJCNN); this arXiv version includes an\n  appendix with 6 result tables; 10 pages, 15 figures, 7 tables", "summary": "The exponential growth in Large Language Model (LLM) deployment has\nintensified the need for efficient model compression techniques to reduce\ncomputational and memory costs. While pruning and quantization have shown\npromise, their combined potential remains largely unexplored. In this paper, we\nexamine joint compression and how strategically combining pruning and\nquantization could yield superior performance-to-compression ratios compared to\nsingle-method approaches. Recognizing the challenges in accurately assessing\nLLM performance, we address key limitations of previous evaluation frameworks\nand introduce the Semantic Retention Compression Rate (SrCr), a novel metric\nthat quantifies the trade-off between model compression and semantic\npreservation, facilitating the optimization of pruning-quantization\nconfigurations. Experiments demonstrate that our recommended combination\nachieves, on average, a 20% performance increase compared to an equivalent\nquantization-only model at the same theoretical compression rate.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814", "abs": "https://arxiv.org/abs/2505.06814", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06831", "pdf": "https://arxiv.org/pdf/2505.06831", "abs": "https://arxiv.org/abs/2505.06831", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification", "categories": ["cs.CV"], "comment": null, "summary": "Achieving group-robust generalization in the presence of spurious\ncorrelations remains a significant challenge, particularly when bias\nannotations are unavailable. Recent studies on Class-Conditional Distribution\nBalancing (CCDB) reveal that spurious correlations often stem from mismatches\nbetween the class-conditional and marginal distributions of bias attributes.\nThey achieve promising results by addressing this issue through simple\ndistribution matching in a bias-agnostic manner. However, CCDB approximates\neach distribution using a single Gaussian, which is overly simplistic and\nrarely holds in real-world applications. To address this limitation, we propose\na novel method called Bias Exploration via Overfitting (BEO), which captures\neach distribution in greater detail by modeling it as a mixture of latent\ngroups. Building on these group-level descriptions, we introduce a fine-grained\nvariant of CCDB, termed FG-CCDB, which performs more precise distribution\nmatching and balancing within each group. Through group-level reweighting,\nFG-CCDB learns sample weights from a global perspective, achieving stronger\nmitigation of spurious correlations without incurring substantial storage or\ncomputational costs. Extensive experiments demonstrate that BEO serves as a\nstrong proxy for ground-truth bias annotations and can be seamlessly integrated\nwith bias-supervised methods. Moreover, when combined with FG-CCDB, our method\nperforms on par with bias-supervised approaches on binary classification tasks\nand significantly outperforms them in highly biased multi-class scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07409", "pdf": "https://arxiv.org/pdf/2505.07409", "abs": "https://arxiv.org/abs/2505.07409", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Markus Stocker", "Sren Auer"], "title": "Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles", "categories": ["cs.CL"], "comment": "4 pages, 4 figures, submitted to ACM Web Conference 2025", "summary": "Democratic societies need reliable information. Misinformation in popular\nmedia such as news articles or videos threatens to impair civic discourse.\nCitizens are, unfortunately, not equipped to verify this content flood consumed\ndaily at increasing rates. This work aims to semi-automatically quantify\nscientific accuracy of online media. By semantifying media of unknown veracity,\ntheir statements can be compared against equally processed trusted sources. We\nimplemented a workflow using LLM-based statement extraction and knowledge graph\nanalysis. Our neurosymbolic system was able to evidently streamline\nstate-of-the-art veracity quantification. Evaluated via expert interviews and a\nuser survey, the tool provides a beneficial veracity indication. This\nindicator, however, is unable to annotate public media at the required\ngranularity and scale. Further work towards a FAIR (Findable, Accessible,\nInteroperable, Reusable) ground truth and complementary metrics are required to\nscientifically support civic discourse.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06853", "pdf": "https://arxiv.org/pdf/2505.06853", "abs": "https://arxiv.org/abs/2505.06853", "authors": ["Carolina Vargas-Ecos", "Edwin Salcedo"], "title": "Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach", "categories": ["cs.CV"], "comment": "Accepted for publication at the 6th BioSMART Conference, 2025", "summary": "According to the Pan American Health Organization, the number of cancer cases\nin Latin America was estimated at 4.2 million in 2022 and is projected to rise\nto 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone\ncancers affecting young people, is difficult to detect due to its unique\ntexture and intensity. Surgical removal of osteosarcoma requires precise safety\nmargins to ensure complete resection while preserving healthy tissue.\nTherefore, this study proposes a method for estimating the confidence interval\nof surgical safety margins in osteosarcoma surgery around the knee. The\nproposed approach uses MRI and X-ray data from open-source repositories,\ndigital processing techniques, and unsupervised learning algorithms (such as\nk-means clustering) to define tumor boundaries. Experimental results highlight\nthe potential for automated, patient-specific determination of safety margins.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07440", "pdf": "https://arxiv.org/pdf/2505.07440", "abs": "https://arxiv.org/abs/2505.07440", "authors": ["Rituraj Singh", "Sachin Pawar", "Girish Palshikar"], "title": "Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Commonsense knowledge bases (KB) are a source of specialized knowledge that\nis widely used to improve machine learning applications. However, even for a\nlarge KB such as ConceptNet, capturing explicit knowledge from each industry\ndomain is challenging. For example, only a few samples of general {\\em tasks}\nperformed by various industries are available in ConceptNet. Here, a task is a\nwell-defined knowledge-based volitional action to achieve a particular goal. In\nthis paper, we aim to fill this gap and present a weakly-supervised framework\nto augment commonsense KB with tasks carried out by various industry groups\n(IG). We attempt to {\\em match} each task with one or more suitable IGs by\ntraining a neural model to learn task-IG affinity and apply clustering to\nselect the top-k tasks per IG. We extract a total of 2339 triples of the form\n$\\langle IG, is~capable~of, task \\rangle$ from two publicly available news\ndatasets for 24 IGs with the precision of 0.86. This validates the reliability\nof the extracted task-IG pairs that can be directly added to existing KBs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06894", "pdf": "https://arxiv.org/pdf/2505.06894", "abs": "https://arxiv.org/abs/2505.06894", "authors": ["Ahmed Qazi", "Abdul Basit", "Asim Iqbal"], "title": "NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "18 pages, 6 figures", "summary": "Neural Radiance Fields (NeRF) have significantly advanced the field of novel\nview synthesis, yet their generalization across diverse scenes and conditions\nremains challenging. Addressing this, we propose the integration of a novel\nbrain-inspired normalization technique Neural Generalization (NeuGen) into\nleading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts\nthe domain-invariant features, thereby enhancing the models' generalization\ncapabilities. It can be seamlessly integrated into NeRF architectures and\ncultivates a comprehensive feature set that significantly improves accuracy and\nrobustness in image rendering. Through this integration, NeuGen shows improved\nperformance on benchmarks on diverse datasets across state-of-the-art NeRF\narchitectures, enabling them to generalize better across varied scenes. Our\ncomprehensive evaluations, both quantitative and qualitative, confirm that our\napproach not only surpasses existing models in generalizability but also\nmarkedly improves rendering quality. Our work exemplifies the potential of\nmerging neuroscientific principles with deep learning frameworks, setting a new\nprecedent for enhanced generalizability and efficiency in novel view synthesis.\nA demo of our study is available at https://neugennerf.github.io.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06905", "pdf": "https://arxiv.org/pdf/2505.06905", "abs": "https://arxiv.org/abs/2505.06905", "authors": ["Jian Song", "Hongruixuan Chen", "Naoto Yokoya"], "title": "Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Monocular height estimation (MHE) from very-high-resolution (VHR) remote\nsensing imagery via deep learning is notoriously challenging due to the lack of\nsufficient structural information. Conventional digital elevation models\n(DEMs), typically derived from airborne LiDAR or multi-view stereo, remain\ncostly and geographically limited. Recently, models trained on synthetic data\nand refined through domain adaptation have shown remarkable performance in MHE,\nyet it remains unclear how these models make predictions or how reliable they\ntruly are. In this paper, we investigate a state-of-the-art MHE model trained\npurely on synthetic data to explore where the model looks when making height\npredictions. Through systematic analyses, we find that the model relies heavily\non shadow cues, a factor that can lead to overestimation or underestimation of\nheights when shadows deviate from expected norms. Furthermore, the inherent\ndifficulty of evaluating regression tasks with the human eye underscores\nadditional limitations of purely synthetic training. To address these issues,\nwe propose a novel correction pipeline that integrates sparse, imperfect global\nLiDAR measurements (ICESat-2) with deep-learning outputs to improve local\naccuracy and achieve spatially consistent corrections. Our method comprises two\nstages: pre-processing raw ICESat-2 data, followed by a random forest-based\napproach to densely refine height estimates. Experiments in three\nrepresentative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal\nsubstantial error reductions, with mean absolute error (MAE) decreased by\n22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical\nrole of shadow awareness in synthetic data-driven models and demonstrate how\nfusing imperfect real-world LiDAR data can bolster the robustness of MHE,\npaving the way for more reliable and scalable 3D mapping solutions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07596", "pdf": "https://arxiv.org/pdf/2505.07596", "abs": "https://arxiv.org/abs/2505.07596", "authors": ["Ziyang Huang", "Xiaowei Yuan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07601", "pdf": "https://arxiv.org/pdf/2505.07601", "abs": "https://arxiv.org/abs/2505.07601", "authors": ["Edirlei Soares de Lima", "Marco A. Casanova", "Bruno Feij", "Antonio L. Furtado"], "title": "Characterizing the Investigative Methods of Fictional Detectives with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detective fiction, a genre defined by its complex narrative structures and\ncharacter-driven storytelling, presents unique challenges for computational\nnarratology, a research field focused on integrating literary theory into\nautomated narrative generation. While traditional literary studies have offered\ndeep insights into the methods and archetypes of fictional detectives, these\nanalyses often focus on a limited number of characters and lack the scalability\nneeded for the extraction of unique traits that can be used to guide narrative\ngeneration methods. In this paper, we present an AI-driven approach for\nsystematically characterizing the investigative methods of fictional\ndetectives. Our multi-phase workflow explores the capabilities of 15 Large\nLanguage Models (LLMs) to extract, synthesize, and validate distinctive\ninvestigative traits of fictional detectives. This approach was tested on a\ndiverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes,\nWilliam Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin -\ncapturing the distinctive investigative styles that define each character. The\nidentified traits were validated against existing literary analyses and further\ntested in a reverse identification phase, achieving an overall accuracy of\n91.43%, demonstrating the method's effectiveness in capturing the distinctive\ninvestigative approaches of each detective. This work contributes to the\nbroader field of computational narratology by providing a scalable framework\nfor character analysis, with potential applications in AI-driven interactive\nstorytelling and automated narrative generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06920", "pdf": "https://arxiv.org/pdf/2505.06920", "abs": "https://arxiv.org/abs/2505.06920", "authors": ["Timing Li", "Bing Cao", "Pengfei Zhu", "Bin Xiao", "Qinghua Hu"], "title": "Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Acquiring accurately aligned multi-modal image pairs is fundamental for\nachieving high-quality multi-modal image fusion. To address the lack of ground\ntruth in current multi-modal image registration and fusion methods, we propose\na novel self-supervised \\textbf{B}i-directional\n\\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically,\nB-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator\n(IPDG) to achieve self-supervised global-local registration. Visible-infrared\nimage pairs with spatially misaligned differences are aligned to obtain global\ndifferences through the registration module. The same image pairs are processed\nby PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain\nlocal differences. IPDG converts the obtained local differences into\npseudo-global differences, which are used to perform global-local difference\nconsistency with the global differences. Furthermore, aiming at eliminating the\neffect of modal gaps on the registration module, we design a neighborhood\ndynamic alignment loss to achieve cross-modal image edge alignment. Extensive\nexperiments on misaligned multi-modal images demonstrate the effectiveness of\nthe proposed method in multi-modal image alignment and fusion against the\ncompeting methods. Our code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07608", "pdf": "https://arxiv.org/pdf/2505.07608", "abs": "https://arxiv.org/abs/2505.07608", "authors": ["Xiaomi LLM-Core Team", ":", "Bingquan Xia", "Bowen Shen", "Cici", "Dawei Zhu", "Di Zhang", "Gang Wang", "Hailin Zhang", "Huaqiu Liu", "Jiebao Xiao", "Jinhao Dong", "Liang Zhao", "Peidian Li", "Peng Wang", "Shihua Yu", "Shimao Chen", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Yi Huang", "Yifan Song", "Zihan Jiang", "Bowen Ye", "Can Cai", "Chenhong He", "Dong Zhang", "Duo Zhang", "Guoan Wang", "Hao Tian", "Haochen Zhao", "Heng Qu", "Hongshen Xu", "Jun Shi", "Kainan Bao", "QingKai Fang", "Kang Zhou", "Kangyang Zhou", "Lei Li", "Menghang Zhu", "Nuo Chen", "Qiantong Wang", "Shaohui Liu", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Shuo Liu", "Sirui Deng", "Weiji Zhuang", "Weiwei Lv", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xing Zhang", "Xingchen Song", "Xinzhe Xu", "Xu Wang", "Yihan Yan", "Yu Tu", "Yuanyuan Tian", "Yudong Wang", "Yue Yu", "Zhenru Lin", "Zhichao Song", "Zihao Yue"], "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06937", "pdf": "https://arxiv.org/pdf/2505.06937", "abs": "https://arxiv.org/abs/2505.06937", "authors": ["Fei Zhou", "Yi Li", "Mingqing Zhu"], "title": "Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, the dual-optical attention fusion crowd head point counting\nmodel (TAPNet) is proposed to address the problem of the difficulty of accurate\ncounting in complex scenes such as crowd dense occlusion and low light in crowd\ncounting tasks under UAV view. The model designs a dual-optical attention\nfusion module (DAFP) by introducing complementary information from infrared\nimages to improve the accuracy and robustness of all-day crowd counting. In\norder to fully utilize different modal information and solve the problem of\ninaccurate localization caused by systematic misalignment between image pairs,\nthis paper also proposes an adaptive two-optical feature decomposition fusion\nmodule (AFDF). In addition, we optimize the training strategy to improve the\nmodel robustness through spatial random offset data augmentation. Experiments\non two challenging public datasets, DroneRGBT and GAIIC2, show that the\nproposed method outperforms existing techniques in terms of performance,\nespecially in challenging dense low-light scenes. Code is available at\nhttps://github.com/zz-zik/TAPNet", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610", "abs": "https://arxiv.org/abs/2505.07610", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06948", "pdf": "https://arxiv.org/pdf/2505.06948", "abs": "https://arxiv.org/abs/2505.06948", "authors": ["Pan Du", "Wangbo Zhao", "Xinai Lu", "Nian Liu", "Zhikai Li", "Chaoyu Gong", "Suyun Zhao", "Hong Chen", "Cuiping Li", "Kai Wang", "Yang You"], "title": "Unsupervised Learning for Class Distribution Mismatch", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Class distribution mismatch (CDM) refers to the discrepancy between class\ndistributions in training data and target tasks. Previous methods address this\nby designing classifiers to categorize classes known during training, while\ngrouping unknown or new classes into an \"other\" category. However, they focus\non semi-supervised scenarios and heavily rely on labeled data, limiting their\napplicability and performance. To address this, we propose Unsupervised\nLearning for Class Distribution Mismatch (UCDM), which constructs\npositive-negative pairs from unlabeled data for classifier training. Our\napproach randomly samples images and uses a diffusion model to add or erase\nsemantic classes, synthesizing diverse training pairs. Additionally, we\nintroduce a confidence-based labeling mechanism that iteratively assigns\npseudo-labels to valuable real-world data and incorporates them into the\ntraining process. Extensive experiments on three datasets demonstrate UCDM's\nsuperiority over previous semi-supervised methods. Specifically, with a 60%\nmismatch proportion on Tiny-ImageNet dataset, our approach, without relying on\nlabeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%,\nand 72.5% in classifying known, unknown, and new classes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07653", "pdf": "https://arxiv.org/pdf/2505.07653", "abs": "https://arxiv.org/abs/2505.07653", "authors": ["Iman Johary", "Raphael Romero", "Alexandru C. Mara", "Tijl De Bie"], "title": "JobHop: A Large-Scale Dataset of Career Trajectories", "categories": ["cs.CL"], "comment": null, "summary": "Understanding labor market dynamics is essential for policymakers, employers,\nand job seekers. However, comprehensive datasets that capture real-world career\ntrajectories are scarce. In this paper, we introduce JobHop, a large-scale\npublic dataset derived from anonymized resumes provided by VDAB, the public\nemployment service in Flanders, Belgium. Utilizing Large Language Models\n(LLMs), we process unstructured resume data to extract structured career\ninformation, which is then mapped to standardized ESCO occupation codes using a\nmulti-label classification model. This results in a rich dataset of over 2.3\nmillion work experiences, extracted from and grouped into more than 391,000\nuser resumes and mapped to standardized ESCO occupation codes, offering\nvaluable insights into real-world occupational transitions. This dataset\nenables diverse applications, such as analyzing labor market mobility, job\nstability, and the effects of career breaks on occupational transitions. It\nalso supports career path prediction and other data-driven decision-making\nprocesses. To illustrate its potential, we explore key dataset characteristics,\nincluding job distributions, career breaks, and job transitions, demonstrating\nits value for advancing labor market research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07659", "pdf": "https://arxiv.org/pdf/2505.07659", "abs": "https://arxiv.org/abs/2505.07659", "authors": ["Ethan Gotlieb Wilcox", "Cui Ding", "Giovanni Acampa", "Tiago Pimentel", "Alex Warstadt", "Tamar I. Regev"], "title": "Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent", "categories": ["cs.CL"], "comment": null, "summary": "This paper argues that the relationship between lexical identity and prosody\n-- one well-studied parameter of linguistic variation -- can be characterized\nusing information theory. We predict that languages that use prosody to make\nlexical distinctions should exhibit a higher mutual information between word\nidentity and prosody, compared to languages that don't. We test this hypothesis\nin the domain of pitch, which is used to make lexical distinctions in tonal\nlanguages, like Cantonese. We use a dataset of speakers reading sentences aloud\nin ten languages across five language families to estimate the mutual\ninformation between the text and their pitch curves. We find that, across\nlanguages, pitch curves display similar amounts of entropy. However, these\ncurves are easier to predict given their associated text in the tonal\nlanguages, compared to pitch- and stress-accent languages, and thus the mutual\ninformation is higher in these languages, supporting our hypothesis. Our\nresults support perspectives that view linguistic typology as gradient, rather\nthan categorical.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06982", "pdf": "https://arxiv.org/pdf/2505.06982", "abs": "https://arxiv.org/abs/2505.06982", "authors": ["Md. Naimur Asif Borno", "Md Sakib Hossain Shovon", "MD Hanif Sikder", "Iffat Firozy Rimi", "Tahani Jaser Alahmadi", "Mohammad Ali Moni"], "title": "Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in image-based medical disease detection encounters\nchallenges such as limited annotated data sets, inadequate spatial feature\nanalysis, data security issues, and inefficient training frameworks. This study\nintroduces a data-efficient image transformer (DeIT)-based approach that\novercomes these challenges by utilizing multiscale patch embedding for better\nfeature extraction and stratified weighted random sampling to address class\nimbalance. The model also incorporates a LoRA-enhanced transformer encoder, a\ndistillation framework, and federated learning for decentralized training,\nimproving both efficiency and data security. Consequently, it achieves\nstate-of-the-art performance, with the highest AUC, F1 score, precision,\nminimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations\nimprove interpretability by highlighting critical pathological regions,\nenhancing the model's clinical relevance. These results highlight the potential\nof this approach to advance AI-powered medical imaging and disease detection.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07672", "pdf": "https://arxiv.org/pdf/2505.07672", "abs": "https://arxiv.org/abs/2505.07672", "authors": ["Arun S. Maiya"], "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages", "summary": "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07775", "pdf": "https://arxiv.org/pdf/2505.07775", "abs": "https://arxiv.org/abs/2505.07775", "authors": ["Nimet Beyza Bozdag", "Shuhaib Mehri", "Xiaocheng Yang", "Hyeonjeong Ha", "Zirui Cheng", "Esin Durmus", "Jiaxuan You", "Heng Ji", "Gokhan Tur", "Dilek Hakkani-Tr"], "title": "Must Read: A Systematic Survey of Computational Persuasion", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Persuasion is a fundamental aspect of communication, influencing\ndecision-making across diverse contexts, from everyday conversations to\nhigh-stakes scenarios such as politics, marketing, and law. The rise of\nconversational AI systems has significantly expanded the scope of persuasion,\nintroducing both opportunities and risks. AI-driven persuasion can be leveraged\nfor beneficial applications, but also poses threats through manipulation and\nunethical influence. Moreover, AI systems are not only persuaders, but also\nsusceptible to persuasion, making them vulnerable to adversarial attacks and\nbias reinforcement. Despite rapid advancements in AI-generated persuasive\ncontent, our understanding of what makes persuasion effective remains limited\ndue to its inherently subjective and context-dependent nature. In this survey,\nwe provide a comprehensive overview of computational persuasion, structured\naround three key perspectives: (1) AI as a Persuader, which explores\nAI-generated persuasive content and its applications; (2) AI as a Persuadee,\nwhich examines AI's susceptibility to influence and manipulation; and (3) AI as\na Persuasion Judge, which analyzes AI's role in evaluating persuasive\nstrategies, detecting manipulation, and ensuring ethical persuasion. We\nintroduce a taxonomy for computational persuasion research and discuss key\nchallenges, including evaluating persuasiveness, mitigating manipulative\npersuasion, and developing responsible AI-driven persuasive systems. Our survey\noutlines future research directions to enhance the safety, fairness, and\neffectiveness of AI-powered persuasion while addressing the risks posed by\nincreasingly capable language models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07787", "pdf": "https://arxiv.org/pdf/2505.07787", "abs": "https://arxiv.org/abs/2505.07787", "authors": ["Tongxu Luo", "Wenyu Du", "Jiaxi Bi", "Stephen Chung", "Zhengyang Tang", "Hao Yang", "Min Zhang", "Benyou Wang"], "title": "Learning from Peers in Reasoning Models", "categories": ["cs.CL"], "comment": "29 pages, 32 figures", "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07019", "pdf": "https://arxiv.org/pdf/2505.07019", "abs": "https://arxiv.org/abs/2505.07019", "authors": ["Khang Nguyen Quoc", "Lan Le Thi Thu", "Luyl-Da Quach"], "title": "A Vision-Language Foundation Model for Leaf Disease Identification", "categories": ["cs.CV"], "comment": null, "summary": "Leaf disease identification plays a pivotal role in smart agriculture.\nHowever, many existing studies still struggle to integrate image and textual\nmodalities to compensate for each other's limitations. Furthermore, many of\nthese approaches rely on pretraining with constrained datasets such as\nImageNet, which lack domain-specific information. We propose SCOLD (Soft-target\nCOntrastive learning for Leaf Disease identification), a context-aware\nvision-language foundation model tailored to address these challenges for\nagricultural tasks. SCOLD is developed using a diverse corpus of plant leaf\nimages and corresponding symptom descriptions, comprising over 186,000\nimage-caption pairs aligned with 97 unique concepts. Through task-agnostic\npretraining, SCOLD leverages contextual soft targets to mitigate overconfidence\nin contrastive learning by smoothing labels, thereby improving model\ngeneralization and robustness on fine-grained classification tasks.\nExperimental results demonstrate that SCOLD outperforms existing\nvision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across\nseveral benchmarks, including zero-shot and few-shot classification, image-text\nretrieval, and image classification, while maintaining a competitive parameter\nfootprint. Ablation studies further highlight SCOLD's effectiveness in contrast\nto its counterparts. The proposed approach significantly advances the\nagricultural vision-language foundation model, offering strong performance with\nminimal or no supervised fine-tuning. This work lays a solid groundwork for\nfuture research on models trained with long-form and simplified contexts, tasks\ninvolving class ambiguity, and multi-modal systems for intelligent plant\ndisease diagnostics. The code for this study is available at\nhttps://huggingface.co/enalis/scold", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06297", "pdf": "https://arxiv.org/pdf/2505.06297", "abs": "https://arxiv.org/abs/2505.06297", "authors": ["Yu Mao", "Holger Pirk", "Chun Jason Xue"], "title": "Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to be deployed and utilized across\ndomains, the volume of LLM-generated data is growing rapidly. This trend\nhighlights the increasing importance of effective and lossless compression for\nsuch data in modern text management systems. However, compressing LLM-generated\ndata presents unique challenges compared to traditional human- or\nmachine-generated content. Traditional machine-generated data is typically\nderived from computational processes or device outputs, often highly structured\nand limited to low-level elements like labels or numerical values. This\nstructure enables conventional lossless compressors to perform efficiently. In\ncontrast, LLM-generated data is more complex and diverse, requiring new\napproaches for effective compression. In this work, we conduct the first\nsystematic investigation of lossless compression techniques tailored\nspecifically to LLM-generated data. Notably, because LLMs are trained via\nnext-token prediction, we find that LLM-generated data is highly predictable\nfor the models themselves. This predictability enables LLMs to serve as\nefficient compressors of their own outputs. Through extensive experiments with\n14 representative LLMs and 8 LLM-generated datasets from diverse domains, we\nshow that LLM-based prediction methods achieve remarkable compression rates,\nexceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used\ngeneral-purpose compressor. Furthermore, this advantage holds across different\nLLM sizes and dataset types, demonstrating the robustness and practicality of\nLLM-based methods in lossless text compression under generative AI workloads.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07040", "pdf": "https://arxiv.org/pdf/2505.07040", "abs": "https://arxiv.org/abs/2505.07040", "authors": ["Zhengyang Lu", "Bingjie Lu", "Weifan Wang", "Feng Wang"], "title": "Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection", "categories": ["cs.CV"], "comment": null, "summary": "Fabric defect detection confronts two fundamental challenges. First,\nconventional non-maximum suppression disrupts gradient flow, which hinders\ngenuine end-to-end learning. Second, acquiring pixel-level annotations at\nindustrial scale is prohibitively costly. Addressing these limitations, we\npropose a differentiable NMS framework for fabric defect detection that\nachieves superior localization precision through end-to-end optimization. We\nreformulate NMS as a differentiable bipartite matching problem solved through\nthe Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow\nthroughout the network. This approach specifically targets the irregular\nmorphologies and ambiguous boundaries of fabric defects by integrating proposal\nquality, feature similarity, and spatial relationships. Our entropy-constrained\nmask refinement mechanism further enhances localization precision through\nprincipled uncertainty modeling. Extensive experiments on the Tianchi fabric\ndefect dataset demonstrate significant performance improvements over existing\nmethods while maintaining real-time speeds suitable for industrial deployment.\nThe framework exhibits remarkable adaptability across different architectures\nand generalizes effectively to general object detection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07062", "pdf": "https://arxiv.org/pdf/2505.07062", "abs": "https://arxiv.org/abs/2505.07062", "authors": ["Dong Guo", "Faming Wu", "Feida Zhu", "Fuxing Leng", "Guang Shi", "Haobin Chen", "Haoqi Fan", "Jian Wang", "Jianyu Jiang", "Jiawei Wang", "Jingji Chen", "Jingjia Huang", "Kang Lei", "Liping Yuan", "Lishu Luo", "Pengfei Liu", "Qinghao Ye", "Rui Qian", "Shen Yan", "Shixiong Zhao", "Shuai Peng", "Shuangye Li", "Sihang Yuan", "Sijin Wu", "Tianheng Cheng", "Weiwei Liu", "Wenqian Wang", "Xianhan Zeng", "Xiao Liu", "Xiaobo Qin", "Xiaohan Ding", "Xiaojun Xiao", "Xiaoying Zhang", "Xuanwei Zhang", "Xuehan Xiong", "Yanghua Peng", "Yangrui Chen", "Yanwei Li", "Yanxu Hu", "Yi Lin", "Yiyuan Hu", "Yiyuan Zhang", "Youbin Wu", "Yu Li", "Yudong Liu", "Yue Ling", "Yujia Qin", "Zanbo Wang", "Zhiwu He", "Aoxue Zhang", "Bairen Yi", "Bencheng Liao", "Can Huang", "Can Zhang", "Chaorui Deng", "Chaoyi Deng", "Cheng Lin", "Cheng Yuan", "Chenggang Li", "Chenhui Gou", "Chenwei Lou", "Chengzhi Wei", "Chundian Liu", "Chunyuan Li", "Deyao Zhu", "Donghong Zhong", "Feng Li", "Feng Zhang", "Gang Wu", "Guodong Li", "Guohong Xiao", "Haibin Lin", "Haihua Yang", "Haoming Wang", "Heng Ji", "Hongxiang Hao", "Hui Shen", "Huixia Li", "Jiahao Li", "Jialong Wu", "Jianhua Zhu", "Jianpeng Jiao", "Jiashi Feng", "Jiaze Chen", "Jianhui Duan", "Jihao Liu", "Jin Zeng", "Jingqun Tang", "Jingyu Sun", "Joya Chen", "Jun Long", "Junda Feng", "Junfeng Zhan", "Junjie Fang", "Junting Lu", "Kai Hua", "Kai Liu", "Kai Shen", "Kaiyuan Zhang", "Ke Shen", "Ke Wang", "Keyu Pan", "Kun Zhang", "Kunchang Li", "Lanxin Li", "Lei Li", "Lei Shi", "Li Han", "Liang Xiang", "Liangqiang Chen", "Lin Chen", "Lin Li", "Lin Yan", "Liying Chi", "Longxiang Liu", "Mengfei Du", "Mingxuan Wang", "Ningxin Pan", "Peibin Chen", "Pengfei Chen", "Pengfei Wu", "Qingqing Yuan", "Qingyao Shuai", "Qiuyan Tao", "Renjie Zheng", "Renrui Zhang", "Ru Zhang", "Rui Wang", "Rui Yang", "Rui Zhao", "Shaoqiang Xu", "Shihao Liang", "Shipeng Yan", "Shu Zhong", "Shuaishuai Cao", "Shuangzhi Wu", "Shufan Liu", "Shuhan Chang", "Songhua Cai", "Tenglong Ao", "Tianhao Yang", "Tingting Zhang", "Wanjun Zhong", "Wei Jia", "Wei Weng", "Weihao Yu", "Wenhao Huang", "Wenjia Zhu", "Wenli Yang", "Wenzhi Wang", "Xiang Long", "XiangRui Yin", "Xiao Li", "Xiaolei Zhu", "Xiaoying Jia", "Xijin Zhang", "Xin Liu", "Xinchen Zhang", "Xinyu Yang", "Xiongcai Luo", "Xiuli Chen", "Xuantong Zhong", "Xuefeng Xiao", "Xujing Li", "Yan Wu", "Yawei Wen", "Yifan Du", "Yihao Zhang", "Yining Ye", "Yonghui Wu", "Yu Liu", "Yu Yue", "Yufeng Zhou", "Yufeng Yuan", "Yuhang Xu", "Yuhong Yang", "Yun Zhang", "Yunhao Fang", "Yuntao Li", "Yurui Ren", "Yuwen Xiong", "Zehua Hong", "Zehua Wang", "Zewei Sun", "Zeyu Wang", "Zhao Cai", "Zhaoyue Zha", "Zhecheng An", "Zhehui Zhao", "Zhengzhuo Xu", "Zhipeng Chen", "Zhiyong Wu", "Zhuofan Zheng", "Zihao Wang", "Zilong Huang", "Ziyu Zhu", "Zuquan Song"], "title": "Seed1.5-VL Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance\ngeneral-purpose multimodal understanding and reasoning. Seed1.5-VL is composed\nwith a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B\nactive parameters. Despite its relatively compact architecture, it delivers\nstrong performance across a wide spectrum of public VLM benchmarks and internal\nevaluation suites, achieving the state-of-the-art performance on 38 out of 60\npublic benchmarks. Moreover, in agent-centric tasks such as GUI control and\ngameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI\nCUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates\nstrong reasoning abilities, making it particularly effective for multimodal\nreasoning challenges such as visual puzzles. We believe these capabilities will\nempower broader applications across diverse tasks. In this report, we mainly\nprovide a comprehensive review of our experiences in building Seed1.5-VL across\nmodel design, data construction, and training at various stages, hoping that\nthis report can inspire further research. Seed1.5-VL is now accessible at\nhttps://www.volcengine.com/ (Volcano Engine Model ID:\ndoubao-1-5-thinking-vision-pro-250428)", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07071", "pdf": "https://arxiv.org/pdf/2505.07071", "abs": "https://arxiv.org/abs/2505.07071", "authors": ["Zihang Liu", "Zhenyu Zhang", "Hao Tang"], "title": "Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based image super-resolution (SR) methods have demonstrated\nremarkable performance. Recent advancements have introduced deterministic\nsampling processes that reduce inference from 15 iterative steps to a single\nstep, thereby significantly improving the inference speed of existing diffusion\nmodels. However, their efficiency remains limited when handling complex\nsemantic regions due to the single-step inference. To address this limitation,\nwe propose SAMSR, a semantic-guided diffusion framework that incorporates\nsemantic segmentation masks into the sampling process. Specifically, we\nintroduce the SAM-Noise Module, which refines Gaussian noise using segmentation\nmasks to preserve spatial and semantic features. Furthermore, we develop a\npixel-wise sampling strategy that dynamically adjusts the residual transfer\nrate and noise strength based on pixel-level semantic weights, prioritizing\nsemantically rich regions during the diffusion process. To enhance model\ntraining, we also propose a semantic consistency loss, which aligns pixel-wise\nsemantic weights between predictions and ground truth. Extensive experiments on\nboth real-world and synthetic datasets demonstrate that SAMSR significantly\nimproves perceptual quality and detail recovery, particularly in semantically\ncomplex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06814", "pdf": "https://arxiv.org/pdf/2505.06814", "abs": "https://arxiv.org/abs/2505.06814", "authors": ["Bin Li", "Shenxi Liu", "Yixuan Weng", "Yue Du", "Yuhang Tian", "Shoujun Zhou"], "title": "Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "12 pages, 5 figures, 4 tables", "summary": "Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the\n2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been\nintroduced to further advance research in multi-modal, multilingual, and\nmulti-hop medical instructional question answering (M4IVQA) systems, with a\nspecific focus on medical instructional videos. The M4IVQA challenge focuses on\nevaluating models that integrate information from medical instructional videos,\nunderstand multiple languages, and answer multi-hop questions requiring\nreasoning over various modalities. This task consists of three tracks:\nmulti-modal, multilingual, and multi-hop Temporal Answer Grounding in Single\nVideo (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus\nRetrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer\nGrounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to\ndevelop algorithms capable of processing both video and text data,\nunderstanding multilingual queries, and providing relevant answers to multi-hop\nmedical questions. We believe the newly introduced M4IVQA challenge will drive\ninnovations in multimodal reasoning systems for healthcare scenarios,\nultimately contributing to smarter emergency response systems and more\neffective medical education platforms in multilingual communities. Our official\nwebsite is https://cmivqa.github.io/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843", "abs": "https://arxiv.org/abs/2505.06843", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07165", "pdf": "https://arxiv.org/pdf/2505.07165", "abs": "https://arxiv.org/abs/2505.07165", "authors": ["Jun Li", "Hongzhang Zhu", "Tao Chen", "Xiaohua Qian"], "title": "Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework", "categories": ["cs.CV"], "comment": "accept by IEEE JBHI. Due to the limitation \"The abstract field cannot\n  be longer than 1,920 characters\", the abstract here is shorter than that in\n  the PDF file", "summary": "Recently, numerous pancreas segmentation methods have achieved promising\nperformance on local single-source datasets. However, these methods don't\nadequately account for generalizability issues, and hence typically show\nlimited performance and low stability on test data from other sources.\nConsidering the limited availability of distinct data sources, we seek to\nimprove the generalization performance of a pancreas segmentation model trained\nwith a single-source dataset, i.e., the single source generalization task. In\nparticular, we propose a dual self-supervised learning model that incorporates\nboth global and local anatomical contexts. Our model aims to fully exploit the\nanatomical features of the intra-pancreatic and extra-pancreatic regions, and\nhence enhance the characterization of the high-uncertainty regions for more\nrobust generalization. Specifically, we first construct a global-feature\ncontrastive self-supervised learning module that is guided by the pancreatic\nspatial structure. This module obtains complete and consistent pancreatic\nfeatures through promoting intra-class cohesion, and also extracts more\ndiscriminative features for differentiating between pancreatic and\nnon-pancreatic tissues through maximizing inter-class separation. It mitigates\nthe influence of surrounding tissue on the segmentation outcomes in\nhigh-uncertainty regions. Subsequently, a local-image restoration\nself-supervised learning module is introduced to further enhance the\ncharacterization of the high uncertainty regions. In this module, informative\nanatomical contexts are actually learned to recover randomly corrupted\nappearance patterns in those regions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07172", "pdf": "https://arxiv.org/pdf/2505.07172", "abs": "https://arxiv.org/abs/2505.07172", "authors": ["Zexian Yang", "Dian Li", "Dayan Wu", "Gang Liu", "Weiping Wang"], "title": "Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in multimodal reasoning tasks, existing\nLarge Vision-Language Models (LVLMs) are prone to producing visually ungrounded\nresponses when interpreting associated images. In contrast, when humans embark\non learning new knowledge, they often rely on a set of fundamental pre-study\nprinciples: reviewing outlines to grasp core concepts, summarizing key points\nto guide their focus and enhance understanding. However, such preparatory\nactions are notably absent in the current instruction tuning processes. This\npaper presents Re-Critic, an easily scalable rationale-augmented framework\ndesigned to incorporate fundamental rules and chain-of-thought (CoT) as a\nbridge to enhance reasoning abilities. Specifically, Re-Critic develops a\nvisual rationale synthesizer that scalably augments raw instructions with\nrationale explanation. To probe more contextually grounded responses, Re-Critic\nemploys an in-context self-critic mechanism to select response pairs for\npreference tuning. Experiments demonstrate that models fine-tuned with our\nrationale-augmented dataset yield gains that extend beyond\nhallucination-specific tasks to broader multimodal reasoning tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07219", "pdf": "https://arxiv.org/pdf/2505.07219", "abs": "https://arxiv.org/abs/2505.07219", "authors": ["Hongda Qin", "Xiao Lu", "Zhiyong Wei", "Yihong Cao", "Kailun Yang", "Ningjiang Chen"], "title": "Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code and pre-trained models will be publicly available at\n  https://github.com/qinhongda8/LDDS", "summary": "Generalizing an object detector trained on a single domain to multiple unseen\ndomains is a challenging task. Existing methods typically introduce image or\nfeature augmentation to diversify the source domain to raise the robustness of\nthe detector. Vision-Language Model (VLM)-based augmentation techniques have\nbeen proven to be effective, but they require that the detector's backbone has\nthe same structure as the image encoder of VLM, limiting the detector framework\nselection. To address this problem, we propose Language-Driven Dual Style\nMixing (LDDS) for single-domain generalization, which diversifies the source\ndomain by fully utilizing the semantic information of the VLM. Specifically, we\nfirst construct prompts to transfer style semantics embedded in the VLM to an\nimage translation network. This facilitates the generation of style diversified\nimages with explicit semantic information. Then, we propose image-level style\nmixing between the diversified images and source domain images. This\neffectively mines the semantic information for image augmentation without\nrelying on specific augmentation selections. Finally, we propose feature-level\nstyle mixing in a double-pipeline manner, allowing feature augmentation to be\nmodel-agnostic and can work seamlessly with the mainstream detector frameworks,\nincluding the one-stage, two-stage, and transformer-based detectors. Extensive\nexperiments demonstrate the effectiveness of our approach across various\nbenchmark datasets, including real to cartoon and normal to adverse weather\ntasks. The source code and pre-trained models will be publicly available at\nhttps://github.com/qinhongda8/LDDS.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07249", "pdf": "https://arxiv.org/pdf/2505.07249", "abs": "https://arxiv.org/abs/2505.07249", "authors": ["Philippe Colantoni", "Rafique Ahmed", "Prashant Ghimire", "Damien Muselet", "Alain Trmeau"], "title": "When Dance Video Archives Challenge Computer Vision", "categories": ["cs.CV"], "comment": null, "summary": "The accuracy and efficiency of human body pose estimation depend on the\nquality of the data to be processed and of the particularities of these data.\nTo demonstrate how dance videos can challenge pose estimation techniques, we\nproposed a new 3D human body pose estimation pipeline which combined up-to-date\ntechniques and methods that had not been yet used in dance analysis. Second, we\nperformed tests and extensive experimentations from dance video archives, and\nused visual analytic tools to evaluate the impact of several data parameters on\nhuman body pose. Our results are publicly available for research at\nhttps://www.couleur.org/articles/arXiv-1-2025/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07300", "pdf": "https://arxiv.org/pdf/2505.07300", "abs": "https://arxiv.org/abs/2505.07300", "authors": ["Sofia Casarin", "Sergio Escalera", "Oswald Lanz"], "title": "L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers", "categories": ["cs.CV"], "comment": "accepted at CVPR 2025", "summary": "Training-free Neural Architecture Search (NAS) efficiently identifies\nhigh-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot\nand one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the\nneed for model training, and (ii) interpretable, with proxy designs often\ntheoretically grounded. Despite rapid developments in the field, current SOTA\nZC proxies are typically constrained to well-established convolutional search\nspaces. With the rise of Large Language Models shaping the future of deep\nlearning, this work extends ZC proxy applicability to Vision Transformers\n(ViTs). We present a new benchmark using the Autoformer search space evaluated\non 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients\ninformation (L-SWAG), a novel, generalizable metric that characterizes both\nconvolutional and transformer architectures across 14 tasks. Additionally,\nprevious works highlighted how different proxies contain complementary\ninformation, motivating the need for a ML model to identify useful\ncombinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low\nInformation gain and Bias Re-Alignment), a method that strategically combines\nproxies to best represent a specific benchmark. Integrated into the NAS search,\nLIBRA-NAS outperforms evolution and gradient-based NAS techniques by\nidentifying an architecture with a 17.0% test error on ImageNet1k in just 0.1\nGPU days.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558", "abs": "https://arxiv.org/abs/2505.07558", "authors": ["Rei Higuchi", "Taiji Suzuki"], "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07768", "pdf": "https://arxiv.org/pdf/2505.07768", "abs": "https://arxiv.org/abs/2505.07768", "authors": ["Yifeng Di", "Tianyi Zhang"], "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to ICSE 2025", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07344", "pdf": "https://arxiv.org/pdf/2505.07344", "abs": "https://arxiv.org/abs/2505.07344", "authors": ["Yuan Zhang", "Jiacheng Jiang", "Guoqing Ma", "Zhiying Lu", "Haoyang Huang", "Jianlong Yuan", "Nan Duan"], "title": "Generative Pre-trained Autoregressive Diffusion Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07375", "pdf": "https://arxiv.org/pdf/2505.07375", "abs": "https://arxiv.org/abs/2505.07375", "authors": ["Yuqi Cheng", "Yunkang Cao", "Dongfang Wang", "Weiming Shen", "Wenlong Li"], "title": "Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection", "categories": ["cs.CV"], "comment": "12 pages, 12 figures", "summary": "Point cloud anomaly detection is essential for various industrial\napplications. The huge computation and storage costs caused by the increasing\nproduct classes limit the application of single-class unsupervised methods,\nnecessitating the development of multi-class unsupervised methods. However, the\nfeature similarity between normal and anomalous points from different class\ndata leads to the feature confusion problem, which greatly hinders the\nperformance of multi-class methods. Therefore, we introduce a multi-class point\ncloud anomaly detection method, named GLFM, leveraging global-local feature\nmatching to progressively separate data that are prone to confusion across\nmultiple classes. Specifically, GLFM is structured into three stages: Stage-I\nproposes an anomaly synthesis pipeline that stretches point clouds to create\nabundant anomaly data that are utilized to adapt the point cloud feature\nextractor for better feature representation. Stage-II establishes the global\nand local memory banks according to the global and local feature distributions\nof all the training data, weakening the impact of feature confusion on the\nestablishment of the memory bank. Stage-III implements anomaly detection of\ntest data leveraging its feature distance from global and local memory banks.\nExtensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts\ndataset showcase our proposed GLFM's superior point cloud anomaly detection\nperformance. The code is available at\nhttps://github.com/hustCYQ/GLFM-Multi-class-3DAD.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07398", "pdf": "https://arxiv.org/pdf/2505.07398", "abs": "https://arxiv.org/abs/2505.07398", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art LiDAR-camera 3D object detectors usually focus on feature\nfusion. However, they neglect the factor of depth while designing the fusion\nstrategy. In this work, we are the first to observe that different modalities\nplay different roles as depth varies via statistical analysis and\nvisualization. Based on this finding, we propose a Depth-Aware Hybrid Feature\nFusion (DepthFusion) strategy that guides the weights of point cloud and RGB\nimage modalities by introducing depth encoding at both global and local levels.\nSpecifically, the Depth-GFusion module adaptively adjusts the weights of image\nBird's-Eye-View (BEV) features in multi-modal global features via depth\nencoding. Furthermore, to compensate for the information lost when transferring\nraw features to the BEV space, we propose a Depth-LFusion module, which\nadaptively adjusts the weights of original voxel features and multi-view image\nfeatures in multi-modal local features via depth encoding. Extensive\nexperiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion\nmethod surpasses previous state-of-the-art methods. Moreover, our DepthFusion\nis more robust to various kinds of corruptions, outperforming previous methods\non the nuScenes-C dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07511", "pdf": "https://arxiv.org/pdf/2505.07511", "abs": "https://arxiv.org/abs/2505.07511", "authors": ["Mauricio Orbes-Arteaga", "Oeslle Lucena", "Sabastien Ourselin", "M. Jorge Cardoso"], "title": "MAIS: Memory-Attention for Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Interactive medical segmentation reduces annotation effort by refining\npredictions through user feedback. Vision Transformer (ViT)-based models, such\nas the Segment Anything Model (SAM), achieve state-of-the-art performance using\nuser clicks and prior masks as prompts. However, existing methods treat\ninteractions as independent events, leading to redundant corrections and\nlimited refinement gains. We address this by introducing MAIS, a\nMemory-Attention mechanism for Interactive Segmentation that stores past user\ninputs and segmentation states, enabling temporal context integration. Our\napproach enhances ViT-based segmentation across diverse imaging modalities,\nachieving more efficient and accurate refinements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07533", "pdf": "https://arxiv.org/pdf/2505.07533", "abs": "https://arxiv.org/abs/2505.07533", "authors": ["Ahmad Fall", "Federica Granese", "Alex Lence", "Dominique Fourer", "Blaise Hanczar", "Joe-Elie Salem", "Jean-Daniel Zucker", "Edi Prifti"], "title": "IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monitoring and analyzing electrocardiogram (ECG) signals, even under varying\nphysiological conditions, including those influenced by physical activity,\ndrugs and stress, is crucial to accurately assess cardiac health. However,\ncurrent AI-based methods often fail to account for how these factors interact\nand alter ECG patterns, ultimately limiting their applicability in real-world\nsettings. This study introduces IKrNet, a novel neural network model, which\nidentifies drug-specific patterns in ECGs amidst certain physiological\nconditions. IKrNet's architecture incorporates spatial and temporal dynamics by\nusing a convolutional backbone with varying receptive field size to capture\nspatial features. A bi-directional Long Short-Term Memory module is also\nemployed to model temporal dependencies. By treating heart rate variability as\na surrogate for physiological fluctuations, we evaluated IKrNet's performance\nacross diverse scenarios, including conditions with physical stress, drug\nintake alone, and a baseline without drug presence. Our assessment follows a\nclinical protocol in which 990 healthy volunteers were administered 80mg of\nSotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a\nlife-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art\nmodels' accuracy and stability in varying physiological conditions,\nunderscoring its clinical viability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07540", "pdf": "https://arxiv.org/pdf/2505.07540", "abs": "https://arxiv.org/abs/2505.07540", "authors": ["Juan E. Tapia", "Fabian Stockhardt", "Lzaro Janier Gonzlez-Soler", "Christoph Busch"], "title": "SynID: Passport Synthetic Dataset for Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "The demand for Presentation Attack Detection (PAD) to identify fraudulent ID\ndocuments in remote verification systems has significantly risen in recent\nyears. This increase is driven by several factors, including the rise of remote\nwork, online purchasing, migration, and advancements in synthetic images.\nAdditionally, we have noticed a surge in the number of attacks aimed at the\nenrolment process. Training a PAD to detect fake ID documents is very\nchallenging because of the limited number of ID documents available due to\nprivacy concerns. This work proposes a new passport dataset generated from a\nhybrid method that combines synthetic data and open-access information using\nthe ICAO requirement to obtain realistic training and testing images.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07556", "pdf": "https://arxiv.org/pdf/2505.07556", "abs": "https://arxiv.org/abs/2505.07556", "authors": ["Kamil Jeziorek", "Tomasz Kryjak"], "title": "Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs", "categories": ["cs.CV"], "comment": "Presented at the Real-time Processing of Image, Depth and Video\n  Information 2025 workshop and to be considered for publication is the SPIE\n  Proceedings", "summary": "Event cameras offer significant advantages over traditional frame-based\nsensors. These include microsecond temporal resolution, robustness under\nvarying lighting conditions and low power consumption. Nevertheless, the\neffective processing of their sparse, asynchronous event streams remains\nchallenging. Existing approaches to this problem can be categorised into two\ndistinct groups. The first group involves the direct processing of event data\nwith neural models, such as Spiking Neural Networks or Graph Convolutional\nNeural Networks. However, this approach is often accompanied by a compromise in\nterms of qualitative performance. The second group involves the conversion of\nevents into dense representations with handcrafted aggregation functions, which\ncan boost accuracy at the cost of temporal fidelity. This paper introduces a\nnovel Self-Supervised Event Representation (SSER) method leveraging Gated\nRecurrent Unit (GRU) networks to achieve precise per-pixel encoding of event\ntimestamps and polarities without temporal discretisation. The recurrent layers\nare trained in a self-supervised manner to maximise the fidelity of event-time\nencoding. The inference is performed with event representations generated\nasynchronously, thus ensuring compatibility with high-throughput sensors. The\nexperimental validation demonstrates that SSER outperforms aggregation-based\nbaselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx\nobject detection datasets. Furthermore, the paper presents the first hardware\nimplementation of recurrent representation for event data on a System-on-Chip\nFPGA, achieving sub-microsecond latency and power consumption between 1-2 W,\nsuitable for real-time, power-efficient applications. Code is available at\nhttps://github.com/vision-agh/RecRepEvent.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07573", "pdf": "https://arxiv.org/pdf/2505.07573", "abs": "https://arxiv.org/abs/2505.07573", "authors": ["Sarah de Boer", "Hartmut Hntze", "Kiran Vaidhya Venkadesh", "Myrthe A. D. Buser", "Gabriel E. Humpire Mamani", "Lina Xu", "Lisa C. Adams", "Jawed Nawabi", "Keno K. Bressem", "Bram van Ginneken", "Mathias Prokop", "Alessa Hering"], "title": "Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 11 figures", "summary": "Kidney abnormality segmentation has important potential to enhance the\nclinical workflow, especially in settings requiring quantitative assessments.\nKidney volume could serve as an important biomarker for renal diseases, with\nchanges in volume correlating directly with kidney function. Currently,\nclinical practice often relies on subjective visual assessment for evaluating\nkidney size and abnormalities, including tumors and cysts, which are typically\nstaged based on diameter, volume, and anatomical location. To support a more\nobjective and reproducible approach, this research aims to develop a robust,\nthoroughly validated kidney abnormality segmentation algorithm, made publicly\navailable for clinical and research use. We employ publicly available training\ndatasets and leverage the state-of-the-art medical image segmentation framework\nnnU-Net. Validation is conducted using both proprietary and public test\ndatasets, with segmentation performance quantified by Dice coefficient and the\n95th percentile Hausdorff distance. Furthermore, we analyze robustness across\nsubgroups based on patient sex, age, CT contrast phases, and tumor histologic\nsubtypes. Our findings demonstrate that our segmentation algorithm, trained\nexclusively on publicly available data, generalizes effectively to external\ntest sets and outperforms existing state-of-the-art models across all tested\ndatasets. Subgroup analyses reveal consistent high performance, indicating\nstrong robustness and reliability. The developed algorithm and associated code\nare publicly accessible at\nhttps://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07689", "pdf": "https://arxiv.org/pdf/2505.07689", "abs": "https://arxiv.org/abs/2505.07689", "authors": ["Quang Vinh Nguyen", "Minh Duc Nguyen", "Thanh Hoang Son Vo", "Hyung-Jeong Yang", "Soo-Hyung Kim"], "title": "Anatomical Attention Alignment representation for Radiology Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07690", "pdf": "https://arxiv.org/pdf/2505.07690", "abs": "https://arxiv.org/abs/2505.07690", "authors": ["Songlin Dong", "Chenhao Ding", "Jiangyang Li", "Jizhou Han", "Qiang Wang", "Yuhang He", "Yihong Gong"], "title": "Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This study aims to address the problem of multi-domain task incremental\nlearning~(MTIL), which requires that vision-language models~(VLMs) continuously\nacquire new knowledge while maintaining their inherent zero-shot recognition\ncapability. Existing paradigms delegate the testing of unseen-domain samples to\nthe original CLIP, which only prevents the degradation of the model's zero-shot\ncapability but fails to enhance the generalization of the VLM further. To this\nend, we propose a novel MTIL framework, named AFA, which comprises two core\nmodules: (1) an against forward-forgetting adapter that learns task-invariant\ninformation for each dataset in the incremental tasks to enhance the zero-shot\nrecognition ability of VLMs; (2) an against backward-forgetting adapter that\nstrengthens the few-shot learning capability of VLMs while supporting\nincremental learning. Extensive experiments demonstrate that the AFA method\nsignificantly outperforms existing state-of-the-art approaches, especially in\nfew-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP\nin terms of transferability. The code is provided in the Supplementary\nMaterial.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06483", "pdf": "https://arxiv.org/pdf/2505.06483", "abs": "https://arxiv.org/abs/2505.06483", "authors": ["Shehryar Khattak", "Timon Homberger", "Lukas Bernreiter", "Julian Nubert", "Olov Andersson", "Roland Siegwart", "Kostas Alexis", "Marco Hutter"], "title": "CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 9 figures, Code:\n  https://github.com/leggedrobotics/compslam_subt", "summary": "Robot autonomy in unknown, GPS-denied, and complex underground environments\nrequires real-time, robust, and accurate onboard pose estimation and mapping\nfor reliable operations. This becomes particularly challenging in\nperception-degraded subterranean conditions under harsh environmental factors,\nincluding darkness, dust, and geometrically self-similar structures. This paper\ndetails CompSLAM, a highly resilient and hierarchical multi-modal localization\nand mapping framework designed to address these challenges. Its flexible\narchitecture achieves resilience through redundancy by leveraging the\ncomplementary nature of pose estimates derived from diverse sensor modalities.\nDeveloped during the DARPA Subterranean Challenge, CompSLAM was successfully\ndeployed on all aerial, legged, and wheeled robots of Team Cerberus during\ntheir competition-winning final run. Furthermore, it has proven to be a\nreliable odometry and mapping solution in various subsequent projects, with\nextensions enabling multi-robot map sharing for marsupial robotic deployments\nand collaborative mapping. This paper also introduces a comprehensive dataset\nacquired by a manually teleoperated quadrupedal robot, covering a significant\nportion of the DARPA Subterranean Challenge finals course. This dataset\nevaluates CompSLAM's robustness to sensor degradations as the robot traverses\n740 meters in an environment characterized by highly variable geometries and\ndemanding lighting conditions. The CompSLAM code and the DARPA SubT Finals\ndataset are made publicly available for the benefit of the robotics community", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06507", "pdf": "https://arxiv.org/pdf/2505.06507", "abs": "https://arxiv.org/abs/2505.06507", "authors": ["Haoyang Xie", "Feng Ju"], "title": "Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is fundamental to modern engineering and\nmanufacturing, but creating CAD models still requires expert knowledge and\nspecialized software. Recent advances in large language models (LLMs) open up\nthe possibility of generative CAD, where natural language is directly\ntranslated into parametric 3D models. However, most existing methods generate\ntask-specific command sequences that pretrained models cannot directly handle.\nThese sequences must be converted into CAD representations such as CAD vectors\nbefore a 3D model can be produced, which requires training models from scratch\nand adds unnecessary complexity. To tackle this issue, we propose generating\nCadQuery code directly from text, leveraging the strengths of pretrained LLMs\nto produce 3D models without intermediate representations, using this\nPython-based scripting language. Since LLMs already excel at Python generation\nand spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly\neffective. Given that these capabilities typically improve with scale, we\nhypothesize that larger models will perform better after fine-tuning. To enable\nthis, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We\nfine-tune six open-source LLMs of varying sizes and observe consistent\nimprovements. Our best model achieves a top-1 exact match of 69.3%, up from\n58.8%, and reduces Chamfer Distance by 48.6%. Project page:\nhttps://github.com/Text-to-CadQuery/Text-to-CadQuery.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06621", "pdf": "https://arxiv.org/pdf/2505.06621", "abs": "https://arxiv.org/abs/2505.06621", "authors": ["Thamiris Coelho", "Leo S. F. Ribeiro", "Joo Macedo", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models", "categories": ["cs.LG", "cs.CV"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "The distribution of child sexual abuse imagery (CSAI) is an ever-growing\nconcern of our modern world; children who suffered from this heinous crime are\nrevictimized, and the growing amount of illegal imagery distributed overwhelms\nlaw enforcement agents (LEAs) with the manual labor of categorization. To ease\nthis burden researchers have explored methods for automating data triage and\ndetection of CSAI, but the sensitive nature of the data imposes restricted\naccess and minimal interaction between real data and learning algorithms,\navoiding leaks at all costs. In observing how these restrictions have shaped\nthe literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute\ntasks used for training models for CSAI without making use of CSA data. Under\nthis new terminology we review current literature and present a protocol for\nmaking conscious use of Proxy Tasks together with consistent input from LEAs to\ndesign better automation in this field. Finally, we apply this protocol to\nstudy -- for the first time -- the task of Few-shot Indoor Scene Classification\non CSAI, showing a final model that achieves promising results on a real-world\nCSAI dataset whilst having no weights actually trained on sensitive data.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06646", "pdf": "https://arxiv.org/pdf/2505.06646", "abs": "https://arxiv.org/abs/2505.06646", "authors": ["Daniel Strick", "Carlos Garcia", "Anthony Huang"], "title": "Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "12 pages, 4 figures", "summary": "Deep learning for radiologic image analysis is a rapidly growing field in\nbiomedical research and is likely to become a standard practice in modern\nmedicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray\nimages that are classified by the presence or absence of 14 different diseases,\nwe reproduced an algorithm known as CheXNet, as well as explored other\nalgorithms that outperform CheXNet's baseline metrics. Model performance was\nprimarily evaluated using the F1 score and AUC-ROC, both of which are critical\nmetrics for imbalanced, multi-label classification tasks in medical imaging.\nThe best model achieved an average AUC-ROC score of 0.85 and an average F1\nscore of 0.39 across all 14 disease classifications present in the dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06861", "pdf": "https://arxiv.org/pdf/2505.06861", "abs": "https://arxiv.org/abs/2505.06861", "authors": ["Dongxiu Liu", "Haoyi Niu", "Zhihao Wang", "Jinliang Zheng", "Yinan Zheng", "Zhonghong Ou", "Jianming Hu", "Jianxiong Li", "Xianyuan Zhan"], "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Current robotic planning methods often rely on predicting multi-frame images\nwith full pixel details. While this fine-grained approach can serve as a\ngeneric world model, it introduces two significant challenges for downstream\npolicy learning: substantial computational costs that hinder real-time\ndeployment, and accumulated inaccuracies that can mislead action extraction.\nPlanning with coarse-grained subgoals partially alleviates efficiency issues.\nHowever, their forward planning schemes can still result in off-task\npredictions due to accumulation errors, leading to misalignment with long-term\ngoals. This raises a critical question: Can robotic planning be both efficient\nand accurate enough for real-time control in long-horizon, multi-stage tasks?\nTo address this, we propose a Latent Space Backward Planning scheme (LBP),\nwhich begins by grounding the task into final latent goals, followed by\nrecursively predicting intermediate subgoals closer to the current state. The\ngrounded final goal enables backward subgoal planning to always remain aware of\ntask completion, facilitating on-task prediction along the entire planning\nhorizon. The subgoal-conditioned policy incorporates a learnable token to\nsummarize the subgoal sequences and determines how each subgoal guides action\nextraction. Through extensive simulation and real-robot long-horizon\nexperiments, we show that LBP outperforms existing fine-grained and forward\nplanning methods, achieving SOTA performance. Project Page:\nhttps://lbp-authors.github.io", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06890", "pdf": "https://arxiv.org/pdf/2505.06890", "abs": "https://arxiv.org/abs/2505.06890", "authors": ["Kosuke Ukita", "Ye Xiaolong", "Tsuyoshi Okita"], "title": "Image Classification Using a Diffusion Model as a Pre-Training Model", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": "10 pages, 9 figures", "summary": "In this paper, we propose a diffusion model that integrates a\nrepresentation-conditioning mechanism, where the representations derived from a\nVision Transformer (ViT) are used to condition the internal process of a\nTransformer-based diffusion model. This approach enables\nrepresentation-conditioned data generation, addressing the challenge of\nrequiring large-scale labeled datasets by leveraging self-supervised learning\non unlabeled data. We evaluate our method through a zero-shot classification\ntask for hematoma detection in brain imaging. Compared to the strong\ncontrastive learning baseline, DINOv2, our method achieves a notable\nimprovement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its\neffectiveness in image classification.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06918", "pdf": "https://arxiv.org/pdf/2505.06918", "abs": "https://arxiv.org/abs/2505.06918", "authors": ["Yanhui Hong", "Nan Wang", "Zhiyi Xia", "Haoyi Tao", "Xi Fang", "Yiming Li", "Jiankun Wang", "Peng Jin", "Xiaochen Cai", "Shengyu Li", "Ziqi Chen", "Zezhong Zhang", "Guolin Ke", "Linfeng Zhang"], "title": "Uni-AIMS: AI-Powered Microscopy Image Analysis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents a systematic solution for the intelligent recognition and\nautomatic analysis of microscopy images. We developed a data engine that\ngenerates high-quality annotated datasets through a combination of the\ncollection of diverse microscopy images from experiments, synthetic data\ngeneration and a human-in-the-loop annotation process. To address the unique\nchallenges of microscopy images, we propose a segmentation model capable of\nrobustly detecting both small and large objects. The model effectively\nidentifies and separates thousands of closely situated targets, even in\ncluttered visual environments. Furthermore, our solution supports the precise\nautomatic recognition of image scale bars, an essential feature in quantitative\nmicroscopic analysis. Building upon these components, we have constructed a\ncomprehensive intelligent analysis platform and validated its effectiveness and\npracticality in real-world applications. This study not only advances automatic\nrecognition in microscopy imaging but also ensures scalability and\ngeneralizability across multiple application domains, offering a powerful tool\nfor automated microscopic analysis in interdisciplinary research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06934", "pdf": "https://arxiv.org/pdf/2505.06934", "abs": "https://arxiv.org/abs/2505.06934", "authors": ["Roy Betser", "Meir Yossef Levi", "Guy Gilboa"], "title": "Whitened CLIP as a Likelihood Surrogate of Images and Captions", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to ICML 2025. This version matches the camera-ready version", "summary": "Likelihood approximations for images are not trivial to compute and can be\nuseful in many applications. We examine the use of Contrastive Language-Image\nPre-training (CLIP) to assess the likelihood of images and captions. We\nintroduce \\textit{Whitened CLIP}, a novel transformation of the CLIP latent\nspace via an invertible linear operation. This transformation ensures that each\nfeature in the embedding space has zero mean, unit standard deviation, and no\ncorrelation with all other features, resulting in an identity covariance\nmatrix. We show that the whitened embeddings statistics can be well\napproximated as a standard normal distribution, thus, the log-likelihood is\nestimated simply by the square Euclidean norm in the whitened embedding space.\nThe whitening procedure is completely training-free and performed using a\npre-computed whitening matrix, hence, is very fast. We present several\npreliminary experiments demonstrating the properties and applicability of these\nlikelihood scores to images and captions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07110", "pdf": "https://arxiv.org/pdf/2505.07110", "abs": "https://arxiv.org/abs/2505.07110", "authors": ["Tong Zhang", "Fenghua Shao", "Runsheng Zhang", "Yifan Zhuang", "Liuqingqing Yang"], "title": "DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "Based on the DeepSORT algorithm, this study explores the application of\nvisual tracking technology in intelligent human-computer interaction,\nespecially in the field of gesture recognition and tracking. With the rapid\ndevelopment of artificial intelligence and deep learning technology,\nvisual-based interaction has gradually replaced traditional input devices and\nbecome an important way for intelligent systems to interact with users. The\nDeepSORT algorithm can achieve accurate target tracking in dynamic environments\nby combining Kalman filters and deep learning feature extraction methods. It is\nespecially suitable for complex scenes with multi-target tracking and fast\nmovements. This study experimentally verifies the superior performance of\nDeepSORT in gesture recognition and tracking. It can accurately capture and\ntrack the user's gesture trajectory and is superior to traditional tracking\nmethods in terms of real-time and accuracy. In addition, this study also\ncombines gesture recognition experiments to evaluate the recognition ability\nand feedback response of the DeepSORT algorithm under different gestures (such\nas sliding, clicking, and zooming). The experimental results show that DeepSORT\ncan not only effectively deal with target occlusion and motion blur but also\ncan stably track in a multi-target environment, achieving a smooth user\ninteraction experience. Finally, this paper looks forward to the future\ndevelopment direction of intelligent human-computer interaction systems based\non visual tracking and proposes future research focuses such as algorithm\noptimization, data fusion, and multimodal interaction in order to promote a\nmore intelligent and personalized interactive experience. Keywords-DeepSORT,\nvisual tracking, gesture recognition, human-computer interaction", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07159", "pdf": "https://arxiv.org/pdf/2505.07159", "abs": "https://arxiv.org/abs/2505.07159", "authors": ["Jong Sung Park", "Juhyung Ha", "Siddhesh Thakur", "Alexandra Badea", "Spyridon Bakas", "Eleftherios Garyfallidis"], "title": "Skull stripping with purely synthetic data", "categories": ["eess.IV", "cs.CV"], "comment": "Oral at ISMRM 2025", "summary": "While many skull stripping algorithms have been developed for multi-modal and\nmulti-species cases, there is still a lack of a fundamentally generalizable\napproach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain\nextrAction), a strategy to train a model for brain extraction with no real\nbrain images or labels. Our results show that even without any real images or\nanatomical priors, the model achieves comparable accuracy in multi-modal,\nmulti-species and pathological cases. This work presents a new direction of\nresearch for any generalizable medical image segmentation task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07349", "pdf": "https://arxiv.org/pdf/2505.07349", "abs": "https://arxiv.org/abs/2505.07349", "authors": ["Badhan Kumar Das", "Gengyan Zhao", "Boris Mailhe", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages", "summary": "Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a\ncritical task for healthcare professionals. The diverse nature of MRI\nacquisitions with varying contrasts and orientation introduce complexity in\nidentifying hemorrhage using neural networks. For acquisitions with varying\norientations, traditional methods often involve resampling images to a fixed\nplane, which can lead to information loss. To address this, we propose a 3D\nmulti-plane vision transformer (MP-ViT) for hemorrhage classification with\nvarying orientation data. It employs two separate transformer encoders for\naxial and sagittal contrasts, using cross-attention to integrate information\nacross orientations. MP-ViT also includes a modality indication vector to\nprovide missing contrast information to the model. The effectiveness of the\nproposed model is demonstrated with extensive experiments on real world\nclinical dataset consists of 10,084 training, 1,289 validation and 1,496 test\nsubjects. MP-ViT achieved substantial improvement in area under the curve\n(AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based\narchitectures by 1.8%. These results highlight the potential of MP-ViT in\nimproving performance for hemorrhage detection when different orientation\ncontrasts are needed.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07411", "pdf": "https://arxiv.org/pdf/2505.07411", "abs": "https://arxiv.org/abs/2505.07411", "authors": ["Wenhao Hu", "Paul Henderson", "Jos Cano"], "title": "ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "Accepted to International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "Pruning is a widely used method for compressing Deep Neural Networks (DNNs),\nwhere less relevant parameters are removed from a DNN model to reduce its size.\nHowever, removing parameters reduces model accuracy, so pruning is typically\ncombined with fine-tuning, and sometimes other operations such as rewinding\nweights, to recover accuracy. A common approach is to repeatedly prune and then\nfine-tune, with increasing amounts of model parameters being removed in each\nstep. While straightforward to implement, pruning pipelines that follow this\napproach are computationally expensive due to the need for repeated\nfine-tuning.\n  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs\nthat significantly decreases the time required for pruning by reducing the\noverall cost of fine-tuning, while maintaining a similar accuracy to existing\npruning pipelines. ICE-Pruning is based on three main components: i) an\nautomatic mechanism to determine after which pruning steps fine-tuning should\nbe performed; ii) a freezing strategy for faster fine-tuning in each pruning\nstep; and iii) a custom pruning-aware learning rate scheduler to further\nimprove the accuracy of each pruning step and reduce the overall time\nconsumption. We also propose an efficient auto-tuning stage for the\nhyperparameters (e.g., freezing percentage) introduced by the three components.\nWe evaluate ICE-Pruning on several DNN models and datasets, showing that it can\naccelerate pruning by up to 9.61x. Code is available at\nhttps://github.com/gicLAB/ICE-Pruning", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07447", "pdf": "https://arxiv.org/pdf/2505.07447", "abs": "https://arxiv.org/abs/2505.07447", "authors": ["Peng Sun", "Yi Jiang", "Tao Lin"], "title": "Unified Continuous Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "https://github.com/LINs-lab/UCGM", "summary": "Recent advances in continuous generative models, including multi-step\napproaches like diffusion and flow-matching (typically requiring 8-1000\nsampling steps) and few-step methods such as consistency models (typically 1-8\nsteps), have demonstrated impressive generative performance. However, existing\nwork often treats these approaches as distinct paradigms, resulting in separate\ntraining and sampling methodologies. We introduce a unified framework for\ntraining, sampling, and analyzing these models. Our implementation, the Unified\nContinuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves\nstate-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a\n675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID\nin 20 steps and a few-step model reaching 1.42 FID in just 2 steps.\nAdditionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at\n250 steps) improves performance to 1.06 FID in only 40 steps. Code is available\nat: https://github.com/LINs-lab/UCGM.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07600", "pdf": "https://arxiv.org/pdf/2505.07600", "abs": "https://arxiv.org/abs/2505.07600", "authors": ["Oriol Barbany", "Adri Colom", "Carme Torras"], "title": "Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025 Workshop \"Reflections on Representations and\n  Manipulating Deformable Objects\". Project page\n  https://barbany.github.io/bifold/", "summary": "Manipulating clothes is challenging due to their complex dynamics, high\ndeformability, and frequent self-occlusions. Garments exhibit a nearly infinite\nnumber of configurations, making explicit state representations difficult to\ndefine. In this paper, we analyze BiFold, a model that predicts\nlanguage-conditioned pick-and-place actions from visual observations, while\nimplicitly encoding garment state through end-to-end learning. To address\nscenarios such as crumpled garments or recovery from failed manipulations,\nBiFold leverages temporal context to improve state estimation. We examine the\ninternal representations of the model and present evidence that its fine-tuning\nand temporal context enable effective alignment between text and image regions,\nas well as temporal consistency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07654", "pdf": "https://arxiv.org/pdf/2505.07654", "abs": "https://arxiv.org/abs/2505.07654", "authors": ["Pouya Afshin", "David Helminiak", "Tongtong Lu", "Tina Yen", "Julie M. Jorns", "Mollie Patton", "Bing Yu", "Dong Hye Ye"], "title": "Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Breast-conserving surgery (BCS) aims to completely remove malignant lesions\nwhile maximizing healthy tissue preservation. Intraoperative margin assessment\nis essential to achieve a balance between thorough cancer resection and tissue\nconservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM)\nenables rapid acquisition of whole surface images (WSIs) for excised tissue,\nproviding contrast between malignant and normal tissues. However, breast cancer\nclassification with DUV WSIs is challenged by high resolutions and complex\nhistopathological features. This study introduces a DUV WSI classification\nframework using a patch-level vision transformer (ViT) model, capturing local\nand global features. Grad-CAM++ saliency weighting highlights relevant spatial\nregions, enhances result interpretability, and improves diagnostic accuracy for\nbenign and malignant tissue classification. A comprehensive 5-fold\ncross-validation demonstrates the proposed approach significantly outperforms\nconventional deep learning methods, achieving a classification accuracy of\n98.33%.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07661", "pdf": "https://arxiv.org/pdf/2505.07661", "abs": "https://arxiv.org/abs/2505.07661", "authors": ["Elad Yoshai", "Dana Yagoda-Aharoni", "Eden Dotan", "Natan T. Shaked"], "title": "Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present SparseAttnNet, a new hierarchical attention-driven framework for\nefficient image classification that adaptively selects and processes only the\nmost informative pixels from images. Traditional convolutional neural networks\ntypically process the entire images regardless of information density, leading\nto computational inefficiency and potential focus on irrelevant features. Our\napproach leverages a dynamic selection mechanism that uses coarse attention\ndistilled by fine multi-head attention from the downstream layers of the model,\nallowing the model to identify and extract the most salient k pixels, where k\nis adaptively learned during training based on loss convergence trends. Once\nthe top-k pixels are selected, the model processes only these pixels, embedding\nthem as words in a language model to capture their semantics, followed by\nmulti-head attention to incorporate global context. For biological cell images,\nwe demonstrate that SparseAttnNet can process approximately 15% of the pixels\ninstead of the full image. Applied to cell classification tasks using white\nblood cells images from the following modalities: optical path difference (OPD)\nimages from digital holography for stain-free cells, images from\nmotion-sensitive (event) camera from stain-free cells, and brightfield\nmicroscopy images of stained cells, For all three imaging modalities,\nSparseAttnNet achieves competitive accuracy while drastically reducing\ncomputational requirements in terms of both parameters and floating-point\noperations per second, compared to traditional CNNs and Vision Transformers.\nSince the model focuses on biologically relevant regions, it also offers\nimproved explainability. The adaptive and lightweight nature of SparseAttnNet\nmakes it ideal for deployment in resource-constrained and high-throughput\nsettings, including imaging flow cytometry.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07813", "pdf": "https://arxiv.org/pdf/2505.07813", "abs": "https://arxiv.org/abs/2505.07813", "authors": ["Tony Tao", "Mohan Kumar Srirama", "Jason Jingzhou Liu", "Kenneth Shaw", "Deepak Pathak"], "title": "DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "In RSS 2025. Website at https://dexwild.github.io", "summary": "Large-scale, diverse robot datasets have emerged as a promising path toward\nenabling dexterous manipulation policies to generalize to novel environments,\nbut acquiring such datasets presents many challenges. While teleoperation\nprovides high-fidelity datasets, its high cost limits its scalability. Instead,\nwhat if people could use their own hands, just as they do in everyday life, to\ncollect data? In DexWild, a diverse team of data collectors uses their hands to\ncollect hours of interactions across a multitude of environments and objects.\nTo record this data, we create DexWild-System, a low-cost, mobile, and\neasy-to-use device. The DexWild learning framework co-trains on both human and\nrobot demonstrations, leading to improved performance compared to training on\neach dataset individually. This combination results in robust robot policies\ncapable of generalizing to novel environments, tasks, and embodiments with\nminimal additional robot-specific data. Experimental results demonstrate that\nDexWild significantly improves performance, achieving a 68.5% success rate in\nunseen environments-nearly four times higher than policies trained with robot\ndata only-and offering 5.8x better cross-embodiment generalization. Video\nresults, codebases, and instructions at https://dexwild.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
