{"id": "2506.02167", "pdf": "https://arxiv.org/pdf/2506.02167", "abs": "https://arxiv.org/abs/2506.02167", "authors": ["Aditi Tiwari", "Farzaneh Masoud", "Dac Trong Nguyen", "Jill Kraft", "Heng Ji", "Klara Nahrstedt"], "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 9 figures, 6 tables", "summary": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety", "reliability", "question answering"], "score": 6}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02845", "pdf": "https://arxiv.org/pdf/2506.02845", "abs": "https://arxiv.org/abs/2506.02845", "authors": ["Di Wen", "Lei Qi", "Kunyu Peng", "Kailun Yang", "Fei Teng", "Ao Luo", "Jia Fu", "Yufan Chen", "Ruiping Liu", "Yitian Shi", "M. Saquib Sarfraz", "Rainer Stiefelhagen"], "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, submitted to NeurIPS 2025", "summary": "Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety", "question answering", "fine-grained"], "score": 6}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02018", "pdf": "https://arxiv.org/pdf/2506.02018", "abs": "https://arxiv.org/abs/2506.02018", "authors": ["Christopher Lee Lübbers"], "title": "Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 11 figures. Master's thesis, University of Goettingen,\n  December 2025. Code: https://github.com/cluebbers/dpo-rlhf-paraphrase-types.\n  Models:\n  https://huggingface.co/collections/cluebbers/enhancing-paraphrase-type-generation-673ca8d75dfe2ce962a48ac0", "summary": "Paraphrasing re-expresses meaning to enhance applications like text\nsimplification, machine translation, and question-answering. Specific\nparaphrase types facilitate accurate semantic analysis and robust language\nmodels. However, existing paraphrase-type generation methods often misalign\nwith human preferences due to reliance on automated metrics and limited\nhuman-annotated training data, obscuring crucial aspects of semantic fidelity\nand linguistic transformations.\n  This study addresses this gap by leveraging a human-ranked paraphrase-type\ndataset and integrating Direct Preference Optimization (DPO) to align model\noutputs directly with human judgments. DPO-based training increases\nparaphrase-type generation accuracy by 3 percentage points over a supervised\nbaseline and raises human preference ratings by 7 percentage points. A newly\ncreated human-annotated dataset supports more rigorous future evaluations.\nAdditionally, a paraphrase-type detection model achieves F1 scores of 0.91 for\naddition/deletion, 0.78 for same polarity substitution, and 0.70 for\npunctuation changes.\n  These findings demonstrate that preference data and DPO training produce more\nreliable, semantically accurate paraphrases, enabling downstream applications\nsuch as improved summarization and more robust question-answering. The PTD\nmodel surpasses automated metrics and provides a more reliable framework for\nevaluating paraphrase quality, advancing paraphrase-type research toward\nricher, user-aligned language generation and establishing a stronger foundation\nfor future evaluations grounded in human-centric criteria.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "accuracy", "summarization", "criteria"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02022", "pdf": "https://arxiv.org/pdf/2506.02022", "abs": "https://arxiv.org/abs/2506.02022", "authors": ["Aditya Kanade", "Tanuja Ganu"], "title": "Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show reasoning promise, yet their\nvisual perception is a critical bottleneck. Strikingly, MLLMs can produce\ncorrect answers even while misinterpreting crucial visual elements, masking\nthese underlying failures. Our preliminary study on a joint\nperception-reasoning dataset revealed that for one leading MLLM, 29% of its\ncorrect answers to reasoning questions still exhibited visual perception\nerrors. To systematically address this, we introduce \"Do You See Me\", a\nscalable benchmark with 1,758 images and 2,612 questions. It spans seven\nhuman-psychology inspired subtasks in 2D and 3D, featuring controllable\ncomplexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading\nclosed-source and 5 major open-source models reveal a stark deficit: humans\nachieve 96.49% accuracy, while top MLLMs average below 50%. This performance\ngap widens rapidly with increased task complexity (e.g., from 12% to 45% in the\nvisual form constancy subtask). Further analysis into the root causes suggests\nthat failures stem from challenges like misallocated visual attention and the\ninstability of internal representations for fine-grained details, especially at\nor below encoder patch resolution. This underscores an urgent need for MLLMs\nwith truly robust visual perception. The benchmark dataset, source code and\nevaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02449", "pdf": "https://arxiv.org/pdf/2506.02449", "abs": "https://arxiv.org/abs/2506.02449", "authors": ["Bo Peng", "Zhiheng Wang", "Heyang Gong", "Chaochao Lu"], "title": "IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "In modern dialogue systems, the ability to implicitly infer user backgrounds\nfrom conversations and leverage this information for personalized assistance is\ncrucial. However, the scarcity of high-quality data remains a fundamental\nchallenge to evaluating and improving this capability. Traditional dataset\nconstruction methods are labor-intensive, resource-demanding, and raise privacy\nconcerns. To address these issues, we propose a novel approach for automatic\nsynthetic data generation and introduce the Implicit Personalized Dialogue\n(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12\nuser attribute types. Additionally, we develop a systematic evaluation\nframework with four metrics to assess both attribute awareness and reasoning\ncapabilities. We further propose five causal graphs to elucidate models'\nreasoning pathways during implicit personalization. Extensive experiments yield\ninsightful observations and prove the reliability of our dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "dialogue"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02510", "pdf": "https://arxiv.org/pdf/2506.02510", "abs": "https://arxiv.org/abs/2506.02510", "authors": ["Jie Zhu", "Junhui Li", "Yalong Wen", "Xiandong Li", "Lifan Guo", "Feng Chen"], "title": "M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL-2025", "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called $\\texttt{M$^3$FinMeeting}$, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, $\\texttt{M$^3$FinMeeting}$ supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\n$\\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\n$\\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "summarization", "question answering"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02726", "pdf": "https://arxiv.org/pdf/2506.02726", "abs": "https://arxiv.org/abs/2506.02726", "authors": ["Qihang Yan", "Xinyu Zhang", "Luming Guo", "Qi Zhang", "Feifan Liu"], "title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 8}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "reliability", "accuracy", "dimension"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02692", "pdf": "https://arxiv.org/pdf/2506.02692", "abs": "https://arxiv.org/abs/2506.02692", "authors": ["Shu Yang", "Fengtao Zhou", "Leon Mayer", "Fuxiang Huang", "Yiliang Chen", "Yihui Wang", "Sunan He", "Yuxiang Nie", "Xi Wang", "Ömer Sümer", "Yueming Jin", "Huihui Sun", "Shuchang Xu", "Alex Qinyang Liu", "Zheng Li", "Jing Qin", "Jeremy YuenChun Teoh", "Lena Maier-Hein", "Hao Chen"], "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize\nmodern surgery, with surgical scene understanding serving as a critical\ncomponent in supporting decision-making, improving procedural efficacy, and\nensuring intraoperative safety. While existing AI-driven approaches alleviate\nannotation burdens via self-supervised spatial representation learning, their\nlack of explicit temporal modeling during pre-training fundamentally restricts\nthe capture of dynamic surgical contexts, resulting in incomplete\nspatiotemporal understanding. In this work, we introduce the first video-level\nsurgical pre-training framework that enables joint spatiotemporal\nrepresentation learning from large-scale surgical video data. To achieve this,\nwe constructed a large-scale surgical video dataset comprising 3,650 videos and\napproximately 3.55 million frames, spanning more than 20 surgical procedures\nand over 10 anatomical structures. Building upon this dataset, we propose\nSurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a\nreconstruction-based pre-training method that captures intricate spatial\nstructures and temporal dynamics through joint spatiotemporal modeling.\nAdditionally, SurgVISTA incorporates image-level knowledge distillation guided\nby a surgery-specific expert to enhance the learning of fine-grained anatomical\nand semantic features. To validate its effectiveness, we established a\ncomprehensive benchmark comprising 13 video-level datasets spanning six\nsurgical procedures across four tasks. Extensive experiments demonstrate that\nSurgVISTA consistently outperforms both natural- and surgical-domain\npre-trained models, demonstrating strong potential to advance intelligent\nsurgical systems in clinically meaningful scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation", "safety", "fine-grained"], "score": 5}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02358", "pdf": "https://arxiv.org/pdf/2506.02358", "abs": "https://arxiv.org/abs/2506.02358", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Sun"], "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The classification of the type of road surface (RSC) aims to utilize pavement\nfeatures to identify the roughness, wet and dry conditions, and material\ninformation of the road surface. Due to its ability to effectively enhance road\nsafety and traffic management, it has received widespread attention in recent\nyears. In autonomous driving, accurate RSC allows vehicles to better understand\nthe road environment, adjust driving strategies, and ensure a safer and more\nefficient driving experience. For a long time, vision-based RSC has been\nfavored. However, existing visual classification methods have overlooked the\nexploration of fine-grained classification of pavement types (such as similar\npavement textures). In this work, we propose a pure vision-based fine-grained\nRSC method for autonomous driving scenarios, which fuses local and global\nfeature information through the stacking of convolutional and transformer\nmodules. We further explore the stacking strategies of local and global feature\nextraction modules to find the optimal feature extraction strategy. In\naddition, since fine-grained tasks also face the challenge of relatively large\nintra-class differences and relatively small inter-class differences, we\npropose a Foreground-Background Module (FBM) that effectively extracts\nfine-grained context features of the pavement, enhancing the classification\nability for complex pavements. Experiments conducted on a large-scale pavement\ndataset containing one million samples and a simplified dataset reorganized\nfrom this dataset achieved Top-1 classification accuracies of 92.52% and\n96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.\nThese results demonstrate that RoadFormer outperforms existing methods in RSC\ntasks, providing significant progress in improving the reliability of pavement\nperception in autonomous driving systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety", "reliability", "fine-grained"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02372", "pdf": "https://arxiv.org/pdf/2506.02372", "abs": "https://arxiv.org/abs/2506.02372", "authors": ["Hisami Suzuki", "Satoru Katsumata", "Takashi Kodama", "Tetsuro Takahashi", "Kouta Nakayama", "Satoshi Sekine"], "title": "AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we present AnswerCarefully, a dataset for promoting the safety\nand appropriateness of Japanese LLM outputs. The dataset consists of 1,800\npairs of questions and reference answers, where the questions require special\nattention in answering. It covers a wide range of risk categories established\nin prior English-language datasets, but the data samples are original in that\nthey are manually created to reflect the socio-cultural context of LLM usage in\nJapan. We show that using this dataset for instruction to fine-tune a Japanese\nLLM led to improved output safety without compromising the utility of general\nresponses. We also report the results of a safety evaluation of 12 Japanese\nLLMs using this dataset as a benchmark. Finally, we describe the latest update\non the dataset which provides English translations and annotations of the\nquestions, aimed at facilitating the derivation of similar datasets in\ndifferent languages and regions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02382", "pdf": "https://arxiv.org/pdf/2506.02382", "abs": "https://arxiv.org/abs/2506.02382", "authors": ["Seulgi Kim", "Ghazal Kaviani", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "Multi-level and Multi-modal Action Anticipation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Action anticipation, the task of predicting future actions from partially\nobserved videos, is crucial for advancing intelligent systems. Unlike action\nrecognition, which operates on fully observed videos, action anticipation must\nhandle incomplete information. Hence, it requires temporal reasoning, and\ninherent uncertainty handling. While recent advances have been made,\ntraditional methods often focus solely on visual modalities, neglecting the\npotential of integrating multiple sources of information. Drawing inspiration\nfrom human behavior, we introduce \\textit{Multi-level and Multi-modal Action\nAnticipation (m\\&m-Ant)}, a novel multi-modal action anticipation approach that\ncombines both visual and textual cues, while explicitly modeling hierarchical\nsemantic information for more accurate predictions. To address the challenge of\ninaccurate coarse action labels, we propose a fine-grained label generator\npaired with a specialized temporal consistency loss function to optimize\nperformance. Extensive experiments on widely used datasets, including\nBreakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,\nachieving state-of-the-art results with an average anticipation accuracy\nimprovement of 3.08\\% over existing methods. This work underscores the\npotential of multi-modal and hierarchical modeling in advancing action\nanticipation and establishes a new benchmark for future research in the field.\nOur code is available at: https://github.com/olivesgatech/mM-ant.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02404", "pdf": "https://arxiv.org/pdf/2506.02404", "abs": "https://arxiv.org/abs/2506.02404", "authors": ["Yilin Xiao", "Junnan Dong", "Chuang Zhou", "Su Dong", "Qianwen Zhang", "Di Yin", "Xing Sun", "Xiao Huang"], "title": "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing\nrecognition for its potential to enhance large language models (LLMs) by\nstructurally organizing domain-specific corpora and facilitating complex\nreasoning. However, current evaluations of GraphRAG models predominantly rely\non traditional question-answering datasets. Their limited scope in questions\nand evaluation metrics fails to comprehensively assess the reasoning capacity\nimprovements enabled by GraphRAG models. To address this gap, we introduce\nGraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously\nevaluate GraphRAG models. Our benchmark offers three key superiorities: \\((i)\\)\nChallenging question design. Featuring college-level, domain-specific questions\nthat demand multi-hop reasoning, the benchmark ensures that simple content\nretrieval is insufficient for problem-solving. For example, some questions\nrequire mathematical reasoning or programming. \\((ii)\\) Diverse task coverage.\nThe dataset includes a broad spectrum of reasoning tasks, multiple-choice,\ntrue/false, multi-select, open-ended, and fill-in-the-blank. It spans 16\ndisciplines in twenty core textbooks. \\((iii)\\) Holistic evaluation framework.\nGraphRAG-Bench provides comprehensive assessment across the entire GraphRAG\npipeline, including graph construction, knowledge retrieval, and answer\ngeneration. Beyond final-answer correctness, it evaluates the logical coherence\nof the reasoning process. By applying nine contemporary GraphRAG methods to\nGraphRAG-Bench, we demonstrate its utility in quantifying how graph-based\nstructuring improves model reasoning capabilities. Our analysis reveals\ncritical insights about graph architectures, retrieval efficacy, and reasoning\ncapabilities, offering actionable guidance for the research community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "mathematical reasoning"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02493", "pdf": "https://arxiv.org/pdf/2506.02493", "abs": "https://arxiv.org/abs/2506.02493", "authors": ["Jiachen Liu", "Rui Yu", "Sili Chen", "Sharon X. Huang", "Hengkai Guo"], "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlighted Paper", "summary": "3D plane reconstruction from a single image is a crucial yet challenging\ntopic in 3D computer vision. Previous state-of-the-art (SOTA) methods have\nfocused on training their system on a single dataset from either indoor or\noutdoor domain, limiting their generalizability across diverse testing data. In\nthis work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based\nmodel targeting zero-shot 3D plane detection and reconstruction from a single\nimage, over diverse domains and environments. To enable data-driven models\nacross multiple domains, we have curated a large-scale planar benchmark,\ncomprising over 14 datasets and 560,000 high-resolution, dense planar\nannotations for diverse indoor and outdoor scenes. To address the challenge of\nachieving desirable planar geometry on multi-dataset training, we propose to\ndisentangle the representation of plane normal and offset, and employ an\nexemplar-guided, classification-then-regression paradigm to learn plane and\noffset respectively. Additionally, we employ advanced backbones as image\nencoder, and present an effective pixel-geometry-enhanced plane embedding\nmodule to further facilitate planar reconstruction. Extensive experiments\nacross multiple zero-shot evaluation datasets have demonstrated that our\napproach significantly outperforms previous methods on both reconstruction\naccuracy and generalizability, especially over in-the-wild data. Our code and\ndata are available at: https://github.com/jcliu0428/ZeroPlane.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02596", "pdf": "https://arxiv.org/pdf/2506.02596", "abs": "https://arxiv.org/abs/2506.02596", "authors": ["Fan Gao", "Dongyuan Li", "Ding Xia", "Fei Mi", "Yasheng Wang", "Lifeng Shang", "Baojun Wang"], "title": "EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chinese essay writing and its evaluation are critical in educational\ncontexts, yet the capabilities of Large Language Models (LLMs) in this domain\nremain largely underexplored. Existing benchmarks often rely on coarse-grained\ntext quality metrics, largely overlooking the structural and rhetorical\ncomplexities of Chinese essays, particularly across diverse genres. To address\nthis gap, we propose \\benchName, a multi-genre benchmark specifically designed\nfor Chinese essay writing across four major genres: Argumentative, Narrative,\nDescriptive, and Expository. We curate and refine a total of 728 real-world\nprompts to ensure authenticity and meticulously categorize them into the\n\\textit{Open-Ended} and \\textit{Constrained} sets to capture diverse writing\nscenarios. To reliably evaluate generated essays, we develop a fine-grained,\ngenre-specific scoring framework that hierarchically aggregates scores. We\nfurther validate our evaluation protocol through a comprehensive human\nagreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their\nstrengths and limitations across genres and instruction types. With \\benchName,\nwe aim to advance LLM-based Chinese essay evaluation and inspire future\nresearch on improving essay generation in educational settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "agreement", "fine-grained"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03090", "pdf": "https://arxiv.org/pdf/2506.03090", "abs": "https://arxiv.org/abs/2506.03090", "authors": ["Katherine Thai", "Mohit Iyyer"], "title": "Literary Evidence Retrieval via Long-Context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02037", "pdf": "https://arxiv.org/pdf/2506.02037", "abs": "https://arxiv.org/abs/2506.02037", "authors": ["Feng Wang", "Yiding Sun", "Jiaxin Mao", "Wei Xue", "Danqing Xu"], "title": "FinS-Pilot: A Benchmark for Online Financial System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious professional domains, with their performance typically evaluated\nthrough standardized benchmarks. However, the development of financial RAG\nbenchmarks has been constrained by data confidentiality issues and the lack of\ndynamic data integration. To address this issue, we introduces FinS-Pilot, a\nnovel benchmark for evaluating RAG systems in online financial applications.\nConstructed from real-world financial assistant interactions, our benchmark\nincorporates both real-time API data and structured text sources, organized\nthrough an intent classification framework covering critical financial domains\nsuch as equity analysis and macroeconomic forecasting. The benchmark enables\ncomprehensive evaluation of financial assistants' capabilities in handling both\nstatic knowledge and time-sensitive market information. Through systematic\nexperiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's\neffectiveness in identifying models suitable for financial applications while\naddressing the current gap in specialized evaluation tools for the financial\ndomain. Our work contributes both a practical evaluation framework and a\ncurated dataset to advance research in financial NLP systems. The code and\ndataset are accessible on\nGitHub\\footnote{https://github.com/PhealenWang/financial\\_rag\\_benchmark}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02126", "pdf": "https://arxiv.org/pdf/2506.02126", "abs": "https://arxiv.org/abs/2506.02126", "authors": ["Juncheng Wu", "Sheng Liu", "Haoqin Tu", "Hang Yu", "Xiaoke Huang", "James Zou", "Cihang Xie", "Yuyin Zhou"], "title": "Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains", "categories": ["cs.CL"], "comment": "17 pages, preprint", "summary": "Recent advances in reasoning-enhanced Large Language Models such as\nOpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex\ntasks. However, the quality and transparency of their internal reasoning\nprocesses remain underexplored. This work moves beyond the final-answer\naccuracy and investigates step-by-step reasoning in the medical and\nmathematical domains by explicitly decomposing the thinking trajectories into\ntwo parts: knowledge and reasoning. Specifically, we introduce a fine-grained\nevaluation framework that judges: (1) the correctness of knowledge used\n(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured\nby Information Gain (InfoGain)). Using this framework, we study R1-distilled\nand base Qwen models trained with supervised fine-tuning (SFT) and/or\nreinforcement learning (RL) in the medical and math domains. Three intriguing\nfindings emerge: (1) The general reasoning abilities in R1-distilled models do\nnot transfer effectively to the medical domain through either SFT or RL. (2)\nSFT raises final-answer accuracy in both domains, but often at the cost of\nreasoning quality: InfoGain drops by 38.9% on average compared with untrained\nmodels; In the medical domain, however, SFT remains crucial because domain\nknowledge is indispensable. (3) RL enhances medical reasoning by pruning\ninaccurate or irrelevant knowledge from reasoning paths, thereby improving both\nreasoning accuracy and knowledge correctness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02095", "pdf": "https://arxiv.org/pdf/2506.02095", "abs": "https://arxiv.org/abs/2506.02095", "authors": ["Hyojin Bahng", "Caroline Chan", "Fredo Durand", "Phillip Isola"], "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "comparison", "alignment", "DPO"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "consistency"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02161", "pdf": "https://arxiv.org/pdf/2506.02161", "abs": "https://arxiv.org/abs/2506.02161", "authors": ["Xinyu Wei", "Jinrui Zhang", "Zeqing Wang", "Hongyang Wei", "Zhen Guo", "Lei Zhang"], "title": "TIIF-Bench: How Does Your T2I Model Follow Your Instructions?", "categories": ["cs.CV"], "comment": "23 pages, 12 figures, 11 tables", "summary": "The rapid advancements of Text-to-Image (T2I) models have ushered in a new\nphase of AI-generated content, marked by their growing ability to interpret and\nfollow user instructions. However, existing T2I model evaluation benchmarks\nfall short in limited prompt diversity and complexity, as well as coarse\nevaluation metrics, making it difficult to evaluate the fine-grained alignment\nperformance between textual instructions and generated images. In this paper,\nwe present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming\nto systematically assess T2I models' ability in interpreting and following\nintricate textual instructions. TIIF-Bench comprises a set of 5000 prompts\norganized along multiple dimensions, which are categorized into three levels of\ndifficulties and complexities. To rigorously evaluate model robustness to\nvarying prompt lengths, we provide a short and a long version for each prompt\nwith identical core semantics. Two critical attributes, i.e., text rendering\nand style control, are introduced to evaluate the precision of text synthesis\nand the aesthetic coherence of T2I models. In addition, we collect 100\nhigh-quality designer level prompts that encompass various scenarios to\ncomprehensively assess model performance. Leveraging the world knowledge\nencoded in large vision language models, we propose a novel computable\nframework to discern subtle variations in T2I model outputs. Through meticulous\nbenchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and\ncons of current T2I models and reveal the limitations of current T2I\nbenchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02175", "pdf": "https://arxiv.org/pdf/2506.02175", "abs": "https://arxiv.org/abs/2506.02175", "authors": ["Salman Rahman", "Sheriff Issaka", "Ashima Suvarna", "Genglin Liu", "James Shiffer", "Jaeyoung Lee", "Md Rizwan Parvez", "Hamid Palangi", "Shi Feng", "Nanyun Peng", "Yejin Choi", "Julian Michael", "Liwei Jiang", "Saadia Gabriel"], "title": "AI Debate Aids Assessment of Controversial Claims", "categories": ["cs.CL"], "comment": null, "summary": "As AI grows more powerful, it will increasingly shape how we understand the\nworld. But with this influence comes the risk of amplifying misinformation and\ndeepening social divides-especially on consequential topics like public health\nwhere factual accuracy directly impacts well-being. Scalable Oversight aims to\nensure AI truthfulness by enabling humans to supervise systems that may exceed\nhuman capabilities--yet humans themselves hold different beliefs and biases\nthat impair their judgment. We study whether AI debate can guide biased judges\ntoward the truth by having two AI systems debate opposing sides of\ncontroversial COVID-19 factuality claims where people hold strong prior\nbeliefs. We conduct two studies: one with human judges holding either\nmainstream or skeptical beliefs evaluating factuality claims through\nAI-assisted debate or consultancy protocols, and a second examining the same\nproblem with personalized AI judges designed to mimic these different human\nbelief systems. In our human study, we find that debate-where two AI advisor\nsystems present opposing evidence-based arguments-consistently improves\njudgment accuracy and confidence calibration, outperforming consultancy with a\nsingle-advisor system by 10% overall. The improvement is most significant for\njudges with mainstream beliefs (+15.2% accuracy), though debate also helps\nskeptical judges who initially misjudge claims move toward accurate views\n(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like\npersonas achieve even higher accuracy (78.5%) than human judges (70.1%) and\ndefault AI judges without personas (69.8%), suggesting their potential for\nsupervising frontier AI models. These findings highlight AI debate as a\npromising path toward scalable, bias-resilient oversight--leveraging both\ndiverse human and AI judgments to move closer to truth in contested domains.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["truthfulness", "factuality", "accuracy"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02291", "pdf": "https://arxiv.org/pdf/2506.02291", "abs": "https://arxiv.org/abs/2506.02291", "authors": ["Cristian-Ioan Blaga", "Paul Suganthan", "Sahil Dua", "Krishna Srinivasan", "Enrique Alfonseca", "Peter Dornbach", "Tom Duerig", "Imed Zitouni", "Zhe Dong"], "title": "Entity Image and Mixed-Modal Image Retrieval Datasets", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Despite advances in multimodal learning, challenging benchmarks for\nmixed-modal image retrieval that combines visual and textual information are\nlacking. This paper introduces a novel benchmark to rigorously evaluate image\nretrieval that demands deep cross-modal contextual understanding. We present\ntwo new datasets: the Entity Image Dataset (EI), providing canonical images for\nWikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived\nfrom the WIT dataset. The MMIR benchmark features two challenging query types\nrequiring models to ground textual descriptions in the context of provided\nvisual entities: single entity-image queries (one entity image with descriptive\ntext) and multi-entity-image queries (multiple entity images with relational\ntext). We empirically validate the benchmark's utility as both a training\ncorpus and an evaluation set for mixed-modal retrieval. The quality of both\ndatasets is further affirmed through crowd-sourced human annotations. The\ndatasets are accessible through the GitHub page:\nhttps://github.com/google-research-datasets/wit-retrieval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02347", "pdf": "https://arxiv.org/pdf/2506.02347", "abs": "https://arxiv.org/abs/2506.02347", "authors": ["Jiaming Li", "Yukun Chen", "Ziqiang Liu", "Minghuan Tan", "Lei Zhang", "Yunshui Li", "Run Luo", "Longze Chen", "Jing Luo", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Wei Zhou", "Min Yang"], "title": "STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Stories are central to human culture, serving to share ideas, preserve\ntraditions, and foster connections. Automatic story generation, a key\nadvancement in artificial intelligence (AI), offers new possibilities for\ncreating personalized content, exploring creative ideas, and enhancing\ninteractive experiences. However, existing methods struggle to maintain\nnarrative coherence and logical consistency. This disconnect compromises the\noverall storytelling experience, underscoring the need for substantial\nimprovements. Inspired by human cognitive processes, we introduce Storyteller,\na novel approach that systemically improves the coherence and consistency of\nautomatically generated stories. Storyteller introduces a plot node structure\nbased on linguistically grounded subject verb object (SVO) triplets, which\ncapture essential story events and ensure a consistent logical flow. Unlike\nprevious methods, Storyteller integrates two dynamic modules, the STORYLINE and\nnarrative entity knowledge graph (NEKG),that continuously interact with the\nstory generation process. This integration produces structurally sound,\ncohesive and immersive narratives. Extensive experiments demonstrate that\nStoryteller significantly outperforms existing approaches, achieving an 84.33%\naverage win rate through human preference evaluation. At the same time, it is\nalso far ahead in other aspects including creativity, coherence, engagement,\nand relevance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference", "consistency"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02442", "pdf": "https://arxiv.org/pdf/2506.02442", "abs": "https://arxiv.org/abs/2506.02442", "authors": ["Utsav Maskey", "Mark Dras", "Usman Naseem"], "title": "Should LLM Safety Be More Than Refusing Harmful Instructions?", "categories": ["cs.CL"], "comment": "Preprint", "summary": "This paper presents a systematic evaluation of Large Language Models' (LLMs)\nbehavior on long-tail distributed (encrypted) texts and their safety\nimplications. We introduce a two-dimensional framework for assessing LLM\nsafety: (1) instruction refusal-the ability to reject harmful obfuscated\ninstructions, and (2) generation safety-the suppression of generating harmful\nresponses. Through comprehensive experiments, we demonstrate that models that\npossess capabilities to decrypt ciphers may be susceptible to\nmismatched-generalization attacks: their safety mechanisms fail on at least one\nsafety dimension, leading to unsafe responses or over-refusal. Based on these\nfindings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss\ntheir strengths and limitations. This work contributes to understanding the\nsafety of LLM in long-tail text scenarios and provides directions for\ndeveloping robust safety mechanisms.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "dimension"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02515", "pdf": "https://arxiv.org/pdf/2506.02515", "abs": "https://arxiv.org/abs/2506.02515", "authors": ["Zhuohan Xie", "Dhruv Sahnan", "Debopriyo Banerjee", "Georgi Georgiev", "Rushil Thareja", "Hachem Madmoun", "Jinyan Su", "Aaryamonvikram Singh", "Yuxia Wang", "Rui Xing", "Fajri Koto", "Haonan Li", "Ivan Koychev", "Tanmoy Chakraborty", "Salem Lahlou", "Veselin Stoyanov", "Preslav Nakov"], "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02537", "pdf": "https://arxiv.org/pdf/2506.02537", "abs": "https://arxiv.org/abs/2506.02537", "authors": ["Hao Yan", "Handong Zheng", "Hao Wang", "Liang Yin", "Xingchen Liu", "Zhenbiao Cao", "Xinxing Su", "Zihao Chen", "Jihao Wu", "Minghui Liao", "Chao Weng", "Wei Chen", "Yuliang Liu", "Xiang Bai"], "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02573", "pdf": "https://arxiv.org/pdf/2506.02573", "abs": "https://arxiv.org/abs/2506.02573", "authors": ["Muhammad Falensi Azmi", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "title": "IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages", "categories": ["cs.CL"], "comment": "25 pages", "summary": "Although region-specific large language models (LLMs) are increasingly\ndeveloped, their safety remains underexplored, particularly in culturally\ndiverse settings like Indonesia, where sensitivity to local norms is essential\nand highly valued by the community. In this work, we present IndoSafety, the\nfirst high-quality, human-verified safety evaluation dataset tailored for the\nIndonesian context, covering five language varieties: formal and colloquial\nIndonesian, along with three major local languages: Javanese, Sundanese, and\nMinangkabau. IndoSafety is constructed by extending prior safety frameworks to\ndevelop a taxonomy that captures Indonesia's sociocultural context. We find\nthat existing Indonesian-centric LLMs often generate unsafe outputs,\nparticularly in colloquial and local language settings, while fine-tuning on\nIndoSafety significantly improves safety while preserving task performance. Our\nwork highlights the critical need for culturally grounded safety evaluation and\nprovides a concrete step toward responsible LLM deployment in multilingual\nsettings. Warning: This paper contains example data that may be offensive,\nharmful, or biased.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02555", "pdf": "https://arxiv.org/pdf/2506.02555", "abs": "https://arxiv.org/abs/2506.02555", "authors": ["Zhitao Zeng", "Zhu Zhuo", "Xiaojun Jia", "Erli Zhang", "Junde Wu", "Jiaan Zhang", "Yuxuan Wang", "Chang Han Low", "Jian Jiang", "Zilong Zheng", "Xiaochun Cao", "Yutong Ban", "Qi Dou", "Yang Liu", "Yueming Jin"], "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence", "categories": ["cs.CV", "68T45", "I.2.10"], "comment": "29 pages, 5 figures", "summary": "Foundation models have achieved transformative success across biomedical\ndomains by enabling holistic understanding of multimodal data. However, their\napplication in surgery remains underexplored. Surgical intelligence presents\nunique challenges - requiring surgical visual perception, temporal analysis,\nand reasoning. Existing general-purpose vision-language models fail to address\nthese needs due to insufficient domain-specific supervision and the lack of a\nlarge-scale high-quality surgical database. To bridge this gap, we propose\nSurgVLM, one of the first large vision-language foundation models for surgical\nintelligence, where this single universal model can tackle versatile surgical\ntasks. To enable this, we construct a large-scale multimodal surgical database,\nSurgVLM-DB, comprising over 1.81 million frames with 7.79 million\nconversations, spanning more than 16 surgical types and 18 anatomical\nstructures. We unify and reorganize 23 public datasets across 10 surgical\ntasks, followed by standardizing labels and doing hierarchical vision-language\nalignment to facilitate comprehensive coverage of gradually finer-grained\nsurgical tasks, from visual perception, temporal analysis, to high-level\nreasoning. Building upon this comprehensive dataset, we propose SurgVLM, which\nis built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical\ntasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for\nmethod evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets\nin surgical domain, covering several crucial downstream tasks. Based on\nSurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:\nSurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive\ncomparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,\nQwen2.5-Max).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02571", "pdf": "https://arxiv.org/pdf/2506.02571", "abs": "https://arxiv.org/abs/2506.02571", "authors": ["Abhishek Vivekanandan", "Christian Hubschneider", "J. Marius Zöllner"], "title": "Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories", "categories": ["cs.CV"], "comment": "Submitted for peer review", "summary": "The ability to retrieve semantically and directionally similar short-range\ntrajectories with both accuracy and efficiency is foundational for downstream\napplications such as motion forecasting and autonomous navigation. However,\nprevailing approaches often depend on computationally intensive heuristics or\nlatent anchor representations that lack interpretability and controllability.\nIn this work, we propose a novel framework for learning fixed-dimensional\nembeddings for short trajectories by leveraging a Transformer encoder trained\nwith a contrastive triplet loss that emphasize the importance of discriminative\nfeature spaces for trajectory data. We analyze the influence of Cosine and\nFFT-based similarity metrics within the contrastive learning paradigm, with a\nfocus on capturing the nuanced directional intent that characterizes short-term\nmaneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates\nthat embeddings shaped by Cosine similarity objectives yield superior\nclustering of trajectories by both semantic and directional attributes,\noutperforming FFT-based baselines in retrieval tasks. Notably, we show that\ncompact Transformer architectures, even with low-dimensional embeddings (e.g.,\n16 dimensions, but qualitatively down to 4), achieve a compelling balance\nbetween retrieval performance (minADE, minFDE) and computational overhead,\naligning with the growing demand for scalable and interpretable motion priors\nin real-time systems. The resulting embeddings provide a compact, semantically\nmeaningful, and efficient representation of trajectory data, offering a robust\nalternative to heuristic similarity measures and paving the way for more\ntransparent and controllable motion forecasting pipelines.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02672", "pdf": "https://arxiv.org/pdf/2506.02672", "abs": "https://arxiv.org/abs/2506.02672", "authors": ["Shihan Dou", "Ming Zhang", "Chenhao Huang", "Jiayi Chen", "Feng Chen", "Shichun Liu", "Yan Liu", "Chenxiao Liu", "Cheng Zhong", "Zongzhang Zhang", "Tao Gui", "Chao Xin", "Wei Chengzhi", "Lin Yan", "Qi Zhang", "Xuanjing Huang"], "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages, 24 figures", "summary": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large\nlanguage models (LLMs) on their learning capability and efficiency in\nchallenging tasks, a critical, yet underexplored aspect of model potential.\nEvaLearn contains 648 challenging problems across six task types, grouped into\n182 sequences, each sequence dedicated to one task type. Diverging from most\nexisting benchmarks that evaluate models in parallel, EvaLearn requires models\nto solve problems sequentially, allowing them to leverage the experience gained\nfrom previous solutions. EvaLearn provides five comprehensive automated metrics\nto evaluate models and quantify their learning capability and efficiency. We\nextensively benchmark nine frontier models and observe varied performance\nprofiles: some models, such as Claude-3.7-sonnet, start with moderate initial\nperformance but exhibit strong learning ability, while some models struggle to\nbenefit from experience and may even show negative transfer. Moreover, we\ninvestigate model performance under two learning settings and find that\ninstance-level rubrics and teacher-model feedback further facilitate model\nlearning. Importantly, we observe that current LLMs with stronger static\nabilities do not show a clear advantage in learning capability across all\ntasks, highlighting that EvaLearn evaluates a new dimension of model\nperformance. We hope EvaLearn provides a novel evaluation perspective for\nassessing LLM potential and understanding the gap between models and human\ncapabilities, promoting the development of deeper and more dynamic evaluation\napproaches. All datasets, the automatic evaluation framework, and the results\nstudied in this paper are available at the GitHub repository.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dimension"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02758", "pdf": "https://arxiv.org/pdf/2506.02758", "abs": "https://arxiv.org/abs/2506.02758", "authors": ["Stefano Bannò", "Kate Knill", "Mark Gales"], "title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications", "summary": "Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02894", "pdf": "https://arxiv.org/pdf/2506.02894", "abs": "https://arxiv.org/abs/2506.02894", "authors": ["Verena Blaschke", "Miriam Winkler", "Constantin Förster", "Gabriele Wenger-Glemser", "Barbara Plank"], "title": "A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Although Germany has a diverse landscape of dialects, they are\nunderrepresented in current automatic speech recognition (ASR) research. To\nenable studies of how robust models are towards dialectal variation, we present\nBetthupferl, an evaluation dataset containing four hours of read speech in\nthree dialect groups spoken in Southeast Germany (Franconian, Bavarian,\nAlemannic), and half an hour of Standard German speech. We provide both\ndialectal and Standard German transcriptions, and analyze the linguistic\ndifferences between them. We benchmark several multilingual state-of-the-art\nASR models on speech translation into Standard German, and find differences\nbetween how much the output resembles the dialectal vs. standardized\ntranscriptions. Qualitative error analyses of the best ASR model reveal that it\nsometimes normalizes grammatical differences, but often stays closer to the\ndialectal constructions.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02899", "pdf": "https://arxiv.org/pdf/2506.02899", "abs": "https://arxiv.org/abs/2506.02899", "authors": ["Yusuke Sakai", "Takumi Goto", "Taro Watanabe"], "title": "IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "We propose IMPARA-GED, a novel reference-free automatic grammatical error\ncorrection (GEC) evaluation method with grammatical error detection (GED)\ncapabilities. We focus on the quality estimator of IMPARA, an existing\nautomatic GEC evaluation method, and construct that of IMPARA-GED using a\npre-trained language model with enhanced GED capabilities. Experimental results\non SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,\ndemonstrate that IMPARA-GED achieves the highest correlation with human\nsentence-level evaluations.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "correlation"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02911", "pdf": "https://arxiv.org/pdf/2506.02911", "abs": "https://arxiv.org/abs/2506.02911", "authors": ["Yin Fang", "Qiao Jin", "Guangzhi Xiong", "Bowen Jin", "Xianrui Zhong", "Siru Ouyang", "Aidong Zhang", "Jiawei Han", "Zhiyong Lu"], "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "cs.LG"], "comment": "28 pages; 16 tables; 7 figures; Code:\n  https://github.com/ncbi-nlp/cell-o1", "summary": "Cell type annotation is a key task in analyzing the heterogeneity of\nsingle-cell RNA sequencing data. Although recent foundation models automate\nthis process, they typically annotate cells independently, without considering\nbatch-level cellular context or providing explanatory reasoning. In contrast,\nhuman experts often annotate distinct cell types for different cell clusters\nbased on their domain knowledge. To mimic this workflow, we introduce the\nCellPuzzles task, where the objective is to assign unique cell types to a batch\nof cells. This benchmark spans diverse tissues, diseases, and donor conditions,\nand requires reasoning across the batch-level cellular context to ensure label\nuniqueness. We find that off-the-shelf large language models (LLMs) struggle on\nCellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%\nbatch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained\nvia supervised fine-tuning on distilled reasoning traces, followed by\nreinforcement learning with batch-level rewards. Cell-o1 achieves\nstate-of-the-art performance, outperforming o1 by over 73% and generalizing\nwell across contexts. Further analysis of training dynamics and reasoning\nbehaviors provides insights into batch-level annotation performance and\nemergent expert-like reasoning. Code and data are available at\nhttps://github.com/ncbi-nlp/cell-o1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02741", "pdf": "https://arxiv.org/pdf/2506.02741", "abs": "https://arxiv.org/abs/2506.02741", "authors": ["Pengchong Hu", "Zhizhong Han"], "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Jointly estimating camera poses and mapping scenes from RGBD images is a\nfundamental task in simultaneous localization and mapping (SLAM).\nState-of-the-art methods employ 3D Gaussians to represent a scene, and render\nthese Gaussians through splatting for higher efficiency and better rendering.\nHowever, these methods cannot scale up to extremely large scenes, due to the\ninefficient tracking and mapping strategies that need to optimize all 3D\nGaussians in the limited GPU memories throughout the training to maintain the\ngeometry and color consistency to previous RGBD observations. To resolve this\nissue, we propose novel tracking and mapping strategies to work with a novel 3D\nrepresentation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied\n3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,\nwithout needing to learn locations, rotations, and multi-dimensional variances.\nTying Gaussians to views not only significantly saves storage but also allows\nus to employ many more Gaussians to represent local details in the limited GPU\nmemory. Moreover, our strategies remove the need of maintaining all Gaussians\nlearnable throughout the training, while improving rendering quality, and\ntracking accuracy. We justify the effectiveness of these designs, and report\nbetter performance over the latest methods on the widely used benchmarks in\nterms of rendering and tracking accuracy and scalability. Please see our\nproject page for code and videos at\nhttps://machineperceptionlab.github.io/VTGaussian-SLAM-Project .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "multi-dimensional"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02893", "pdf": "https://arxiv.org/pdf/2506.02893", "abs": "https://arxiv.org/abs/2506.02893", "authors": ["Jonathan Astermark", "Anders Heyden", "Viktor Larsson"], "title": "Dense Match Summarization for Faster Two-view Estimation", "categories": ["cs.CV"], "comment": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2025", "summary": "In this paper, we speed up robust two-view relative pose from dense\ncorrespondences. Previous work has shown that dense matchers can significantly\nimprove both accuracy and robustness in the resulting pose. However, the large\nnumber of matches comes with a significantly increased runtime during robust\nestimation in RANSAC. To avoid this, we propose an efficient match\nsummarization scheme which provides comparable accuracy to using the full set\nof dense matches, while having 10-100x faster runtime. We validate our approach\non standard benchmark datasets together with multiple state-of-the-art dense\nmatchers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "summarization"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02914", "pdf": "https://arxiv.org/pdf/2506.02914", "abs": "https://arxiv.org/abs/2506.02914", "authors": ["Yechi Ma", "Wei Hua", "Shu Kong"], "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection", "categories": ["cs.CV"], "comment": null, "summary": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02160", "pdf": "https://arxiv.org/pdf/2506.02160", "abs": "https://arxiv.org/abs/2506.02160", "authors": ["Madan Krishnamurthy", "Daniel Korn", "Melissa A Haendel", "Christopher J Mungall", "Anne E Thessen"], "title": "A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "This research aims to develop a dynamic and scalable framework to facilitate\nharmonization of Common Data Elements (CDEs) across heterogeneous biomedical\ndatasets by addressing challenges such as semantic heterogeneity, structural\nvariability, and context dependence to streamline integration, enhance\ninteroperability, and accelerate scientific discovery. Our methodology\nleverages Large Language Models (LLMs) for context-aware text embeddings that\nconvert CDEs into dense vectors capturing semantic relationships and patterns.\nThese embeddings are clustered using Hierarchical Density-Based Spatial\nClustering of Applications with Noise (HDBSCAN) to group semantically similar\nCDEs. The framework incorporates four key steps: (1) LLM-based text embedding\nto mathematically represent semantic context, (2) unsupervised clustering of\nembeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)\nsupervised learning to train a classifier assigning new or unclustered CDEs to\nlabeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000\nCDEs, the system identified 118 meaningful clusters at an optimized minimum\ncluster size of 20. The classifier achieved 90.46 percent overall accuracy,\nperforming best in larger categories. External validation against Gravity\nProjects Social Determinants of Health domains showed strong agreement\n(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that\nembeddings effectively capture cluster characteristics. This adaptable and\nscalable approach offers a practical solution to CDE harmonization, improving\nselection efficiency and supporting ongoing data interoperability.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy", "summarization"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03079", "pdf": "https://arxiv.org/pdf/2506.03079", "abs": "https://arxiv.org/abs/2506.03079", "authors": ["Xiuyu Yang", "Bohan Li", "Shaocong Xu", "Nan Wang", "Chongjie Ye", "Zhaoxi Chen", "Minghan Qin", "Yikang Ding", "Xin Jin", "Hang Zhao", "Hao Zhao"], "title": "ORV: 4D Occupancy-centric Robot Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://orangesodahub.github.io/ORV/ ; Code:\n  https://github.com/OrangeSodahub/ORV", "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02314", "pdf": "https://arxiv.org/pdf/2506.02314", "abs": "https://arxiv.org/abs/2506.02314", "authors": ["Tianyu Hua", "Harper Hua", "Violet Xiang", "Benjamin Klieger", "Sang T. Truong", "Weixin Liang", "Fan-Yun Sun", "Nick Haber"], "title": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promise in transforming machine\nlearning research, yet their capability to faithfully implement novel ideas\nfrom recent research papers-ideas unseen during pretraining-remains unclear. We\nintroduce ResearchCodeBench, a benchmark of 212 coding challenges that\nevaluates LLMs' ability to translate cutting-edge ML contributions from top\n2024-2025 research papers into executable code. We assessed 30+ proprietary and\nopen-source LLMs, finding that even the best models correctly implement less\nthan 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%\nsuccess rate, with O3 (High) and O4-mini (High) following behind at 32.3% and\n30.8% respectively. We present empirical findings on performance comparison,\ncontamination, and error patterns. By providing a rigorous and community-driven\nevaluation platform, ResearchCodeBench enables continuous understanding and\nadvancement of LLM-driven innovation in research code generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "code generation"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03097", "pdf": "https://arxiv.org/pdf/2506.03097", "abs": "https://arxiv.org/abs/2506.03097", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM", "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03107", "pdf": "https://arxiv.org/pdf/2506.03107", "abs": "https://arxiv.org/abs/2506.03107", "authors": ["Di Chang", "Mingdeng Cao", "Yichun Shi", "Bo Liu", "Shengqu Cai", "Shijie Zhou", "Weilin Huang", "Gordon Wetzstein", "Mohammad Soleymani", "Peng Wang"], "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "categories": ["cs.CV"], "comment": "Website: https://boese0601.github.io/bytemorph Dataset:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-6M Benchmark:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-Bench Code:\n  https://github.com/ByteDance-Seed/BM-code Demo:\n  https://huggingface.co/spaces/Boese0601/ByteMorph-Demo", "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02890", "pdf": "https://arxiv.org/pdf/2506.02890", "abs": "https://arxiv.org/abs/2506.02890", "authors": ["Jakub Krajewski", "Marcin Chochowski", "Daniel Korzekwa"], "title": "Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have emerged as pivotal for scaling\nLarge Language Models (LLMs) efficiently. Fine-grained MoE approaches -\nutilizing more numerous, smaller experts - have demonstrated potential in\nimproving model convergence and quality. This work proposes a set of training\nrecipes and provides a comprehensive empirical evaluation of fine-grained MoE,\ndirectly comparing its scaling properties against standard MoE configurations\nfor models with up to 56B total (17B active) parameters. We investigate\nconvergence speed, model performance on downstream benchmarks, and practical\ntraining considerations across various setups. Overall, at the largest scale we\nshow that fine-grained MoE achieves better validation loss and higher accuracy\nacross a set of downstream benchmarks. This study offers empirical grounding\nand practical insights for leveraging fine-grained MoE in the development of\nfuture large-scale models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03053", "pdf": "https://arxiv.org/pdf/2506.03053", "abs": "https://arxiv.org/abs/2506.03053", "authors": ["Sinem Erisken", "Timothy Gothard", "Martin Leitgab", "Ram Potham"], "title": "MAEBE: Multi-Agent Emergent Behavior Framework", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "Preprint. This work has been submitted to the Multi-Agent Systems\n  Workshop at ICML 2025 for review", "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation", "fine-grained"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02312", "pdf": "https://arxiv.org/pdf/2506.02312", "abs": "https://arxiv.org/abs/2506.02312", "authors": ["Md Tauhidul Islam", "Wu Da-Wen", "Tang Qing-Qing", "Zhao Kai-Yang", "Yin Teng", "Li Yan-Fei", "Shang Wen-Yi", "Liu Jing-Yu", "Zhang Hai-Xian"], "title": "Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation", "categories": ["eess.IV", "cs.CV", "I.4; I.5"], "comment": null, "summary": "Retinal blood vessel segmentation is crucial for diagnosing ocular and\ncardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf\nRonneberger significantly advanced this field, yet issues like limited training\ndata, imbalance data distribution, and inadequate feature extraction persist,\nhindering both the segmentation performance and optimal model generalization.\nAddressing these critical issues, the DEFFA-Unet is proposed featuring an\nadditional encoder to process domain-invariant pre-processed inputs, thereby\nimproving both richer feature encoding and enhanced model generalization. A\nfeature filtering fusion module is developed to ensure the precise feature\nfiltering and robust hybrid feature fusion. In response to the task-specific\nneed for higher precision where false positives are very costly, traditional\nskip connections are replaced with the attention-guided feature reconstructing\nfusion module. Additionally, innovative data augmentation and balancing methods\nare proposed to counter data scarcity and distribution imbalance, further\nboosting the robustness and generalization of the model. With a comprehensive\nsuite of evaluation metrics, extensive validations on four benchmark datasets\n(DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the\nproposed method's superiority over both baseline and state-of-the-art models.\nParticularly the proposed method significantly outperforms the compared methods\nin cross-validation model generalization.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02010", "pdf": "https://arxiv.org/pdf/2506.02010", "abs": "https://arxiv.org/abs/2506.02010", "authors": ["Zehua Liu", "Xiaolou Li", "Chen Chen", "Lantian Li", "Dong Wang"], "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "to be published in INTERSPEECH 2025", "summary": "This paper presents the second Chinese Continuous Visual Speech Recognition\nChallenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in\nChinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The\nchallenge evaluates two test scenarios: reading in recording studios and\nInternet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC\n2023, which involves CN-CVS for training and CNVSRC-Single/Multi for\ndevelopment and evaluation. However, CNVSRC 2024 introduced two key\nimprovements: (1) a stronger baseline system, and (2) an additional dataset,\nCN-CVS2-P1, for open tracks to improve data volume and diversity. The new\nchallenge has demonstrated several important innovations in data preprocessing,\nfeature extraction, model design, and training strategies, further pushing the\nstate-of-the-art in Chinese LVC-VSR. More details and resources are available\nat the official website.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01992", "pdf": "https://arxiv.org/pdf/2506.01992", "abs": "https://arxiv.org/abs/2506.01992", "authors": ["Lukas Rauch", "Moritz Wirth", "Denis Huseljic", "Marek Herde", "Bernhard Sick", "Matthias Aßenmacher"], "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "under review @NeurIPS2025", "summary": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02000", "pdf": "https://arxiv.org/pdf/2506.02000", "abs": "https://arxiv.org/abs/2506.02000", "authors": ["Abhay Gupta", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) struggle to answer questions that span\ntens of thousands of tokens, especially when multi-hop reasoning is involved.\nWhile prior benchmarks explore long-context comprehension or multi-hop\nreasoning in isolation, none jointly vary context length and reasoning depth in\nnatural narrative settings. We introduce NovelHopQA, the first benchmark to\nevaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length\npublic-domain novels. A keyword-guided pipeline builds hop-separated chains\ngrounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models\nand apply oracle-context filtering to ensure all questions are genuinely\nanswerable. Human annotators validate both alignment and hop depth. We noticed\nconsistent accuracy drops with increased hops and context length, even in\nfrontier models-revealing that sheer scale does not guarantee robust reasoning.\nOur failure mode analysis highlights common breakdowns, such as missed\nfinal-hop integration and long-range drift. NovelHopQA offers a controlled\ndiagnostic setting to stress-test multi-hop reasoning at scale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03004", "pdf": "https://arxiv.org/pdf/2506.03004", "abs": "https://arxiv.org/abs/2506.03004", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02014", "pdf": "https://arxiv.org/pdf/2506.02014", "abs": "https://arxiv.org/abs/2506.02014", "authors": ["Wang Mengjie", "Zhu Huiping", "Li Jian", "Shi Wenxiu", "Zhang Song"], "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of autonomous and assisted driving technologies, higher\ndemands are placed on the ability to understand complex driving scenarios.\nMultimodal general large models have emerged as a solution for this challenge.\nHowever, applying these models in vertical domains involves difficulties such\nas data collection, model training, and deployment optimization. This paper\nproposes a comprehensive method for optimizing multimodal models in driving\nscenarios, including cone detection, traffic light recognition, speed limit\nrecommendation, and intersection alerts. The method covers key aspects such as\ndynamic prompt optimization, dataset construction, model training, and\ndeployment. Specifically, the dynamic prompt optimization adjusts the prompts\nbased on the input image content to focus on objects affecting the ego vehicle,\nenhancing the model's task-specific focus and judgment capabilities. The\ndataset is constructed by combining real and synthetic data to create a\nhigh-quality and diverse multimodal training dataset, improving the model's\ngeneralization in complex driving environments. In model training, advanced\ntechniques like knowledge distillation, dynamic fine-tuning, and quantization\nare integrated to reduce storage and computational costs while boosting\nperformance. Experimental results show that this systematic optimization method\nnot only significantly improves the model's accuracy in key tasks but also\nachieves efficient resource utilization, providing strong support for the\npractical application of driving scenario perception technologies.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02005", "pdf": "https://arxiv.org/pdf/2506.02005", "abs": "https://arxiv.org/abs/2506.02005", "authors": ["Timothy Do", "Pranav Saran", "Harshita Poojary", "Pranav Prabhu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT", "categories": ["cs.CL"], "comment": "9 pages, 7 figures", "summary": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02016", "pdf": "https://arxiv.org/pdf/2506.02016", "abs": "https://arxiv.org/abs/2506.02016", "authors": ["Nuolin Sun", "Linyuan Wang", "Dongyang Li", "Bin Yan", "Lei Li"], "title": "Are classical deep neural networks weakly adversarially robust?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02204", "pdf": "https://arxiv.org/pdf/2506.02204", "abs": "https://arxiv.org/abs/2506.02204", "authors": ["Lindia Tjuatja", "Graham Neubig"], "title": "BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Language model evaluation is a daunting task: prompts are brittle,\ncorpus-level perplexities are vague, and the choice of benchmarks are endless.\nFinding examples that show meaningful, generalizable differences between two\nLMs is crucial to understanding where one model succeeds and another fails. Can\nthis process be done automatically? In this work, we propose methodology for\nautomated comparison of language models that uses performance-aware contextual\nembeddings to find fine-grained features of text where one LM outperforms\nanother. Our method, which we name BehaviorBox, extracts coherent features that\ndemonstrate differences with respect to the ease of generation between two LMs.\nSpecifically, BehaviorBox finds features that describe groups of words in\nfine-grained contexts, such as \"conditional 'were' in the phrase 'if you were'\"\nand \"exclamation marks after emotional statements\", where one model outperforms\nanother within a particular datatset. We apply BehaviorBox to compare models\nthat vary in size, model family, and post-training, and enumerate insights into\nspecific contexts that illustrate meaningful differences in performance which\ncannot be found by measures such as corpus-level perplexity alone.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02239", "pdf": "https://arxiv.org/pdf/2506.02239", "abs": "https://arxiv.org/abs/2506.02239", "authors": ["Sofoklis Kakouros"], "title": "Investigating the Impact of Word Informativeness on Speech Emotion Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "In emotion recognition from speech, a key challenge lies in identifying\nspeech signal segments that carry the most relevant acoustic variations for\ndiscerning specific emotions. Traditional approaches compute functionals for\nfeatures such as energy and F0 over entire sentences or longer speech portions,\npotentially missing essential fine-grained variation in the long-form\nstatistics. This research investigates the use of word informativeness, derived\nfrom a pre-trained language model, to identify semantically important segments.\nAcoustic features are then computed exclusively for these identified segments,\nenhancing emotion recognition accuracy. The methodology utilizes standard\nacoustic prosodic features, their functionals, and self-supervised\nrepresentations. Results indicate a notable improvement in recognition\nperformance when features are computed on segments selected based on word\ninformativeness, underscoring the effectiveness of this approach.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02264", "pdf": "https://arxiv.org/pdf/2506.02264", "abs": "https://arxiv.org/abs/2506.02264", "authors": ["Radin Shayanfar", "Chu Fei Luo", "Rohan Bhambhoria", "Samuel Dahan", "Xiaodan Zhu"], "title": "CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment", "categories": ["cs.CL"], "comment": null, "summary": "It is often challenging to teach specialized, unseen tasks to dialogue\nsystems due to the high cost of expert knowledge, training data, and high\ntechnical difficulty. To support domain-specific applications - such as law,\nmedicine, or finance - it is essential to build frameworks that enable\nnon-technical experts to define, test, and refine system behaviour with minimal\neffort. Achieving this requires cross-disciplinary collaboration between\ndevelopers and domain specialists. In this work, we introduce a novel\nframework, CoDial (Code for Dialogue), that converts expert knowledge,\nrepresented as a novel structured heterogeneous graph, into executable\nconversation logic. CoDial can be easily implemented in existing guardrailing\nlanguages, such as Colang, to enable interpretable, modifiable, and true\nzero-shot specification of task-oriented dialogue systems. Empirically, CoDial\nachieves state-of-the-art performance on the STAR dataset for inference-based\nmodels and is competitive with similar baselines on the well-known MultiWOZ\ndataset. We also demonstrate CoDial's iterative improvement via manual and\nLLM-aided feedback, making it a practical tool for expert-guided alignment of\nLLMs in high-stakes domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02326", "pdf": "https://arxiv.org/pdf/2506.02326", "abs": "https://arxiv.org/abs/2506.02326", "authors": ["Berk Atil", "Namrata Sureddy", "Rebecca J. Passonneau"], "title": "Something Just Like TRuST : Toxicity Recognition of Span and Target", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Toxicity in online content, including content generated by language models,\nhas become a critical concern due to its potential for negative psychological\nand social impact. This paper introduces TRuST, a comprehensive dataset\ndesigned to improve toxicity detection that merges existing datasets, and has\nlabels for toxicity, target social group, and toxic spans. It includes a\ndiverse range of target groups such as ethnicity, gender, religion, disability,\nand politics, with both human/machine-annotated and human machine-generated\ndata. We benchmark state-of-the-art large language models (LLMs) on toxicity\ndetection, target group identification, and toxic span extraction. We find that\nfine-tuned models consistently outperform zero-shot and few-shot prompting,\nthough performance remains low for certain social groups. Further, reasoning\ncapabilities do not significantly improve performance, indicating that LLMs\nhave weak social reasoning skills.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02354", "pdf": "https://arxiv.org/pdf/2506.02354", "abs": "https://arxiv.org/abs/2506.02354", "authors": ["Junjie Li", "Nan Zhang", "Xiaoyang Qu", "Kai Lu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial\nintelligence. Although significant progress has been made in semantic map\nconstruction and target direction prediction in current research, redundant\nexploration and exploration failures remain inevitable. A critical but\nunderexplored direction is the timely termination of exploration to overcome\nthese challenges. We observe a diminishing marginal effect between exploration\nsteps and exploration rates and analyze the cost-benefit relationship of\nexploration. Inspired by this, we propose RATE-Nav, a Region-Aware\nTermination-Enhanced method. It includes a geometric predictive region\nsegmentation algorithm and region-Based exploration estimation algorithm for\nexploration rate calculation. By leveraging the visual question answering\ncapabilities of visual language models (VLMs) and exploration rates enables\nefficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of\n31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav\nshows approximately 10% improvement over previous zero-shot methods.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02356", "pdf": "https://arxiv.org/pdf/2506.02356", "abs": "https://arxiv.org/abs/2506.02356", "authors": ["Woojeong Jin", "Seongchan Kim", "Seungryong Kim"], "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring video object segmentation aims to segment the object in a video\ncorresponding to a given natural language expression. While prior works have\nexplored various referring scenarios, including motion-centric or\nmulti-instance expressions, most approaches still focus on localizing a single\ntarget object in isolation. However, in comprehensive video understanding, an\nobject's role is often defined by its interactions with other entities, which\nare largely overlooked in existing datasets and models. In this work, we\nintroduce Interaction-aware referring video object sgementation (InterRVOS), a\nnew task that requires segmenting both actor and target entities involved in an\ninteraction. Each interactoin is described through a pair of complementary\nexpressions from different semantic perspectives, enabling fine-grained\nmodeling of inter-object relationships. To tackle this task, we propose\nInterRVOS-8K, the large-scale and automatically constructed dataset containing\ndiverse interaction-aware expressions with corresponding masks, including\nchallenging cases such as motion-only multi-instance expressions. We also\npresent a baseline architecture, ReVIOSa, designed to handle actor-target\nsegmentation from a single expression, achieving strong performance in both\nstandard and interaction-focused settings. Furthermore, we introduce an\nactor-target-aware evalaution setting that enables a more targeted assessment\nof interaction understanding. Experimental results demonstrate that our\napproach outperforms prior methods in modeling complex object interactions for\nreferring video object segmentation task, establishing a strong foundation for\nfuture research in interaction-centric video understanding. Our project page is\navailable at\n\\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02350", "pdf": "https://arxiv.org/pdf/2506.02350", "abs": "https://arxiv.org/abs/2506.02350", "authors": ["Herun Wan", "Jiaying Wu", "Minnan Luo", "Zhi Zeng", "Zhixiong Su"], "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "Misinformation detection models often rely on superficial cues (i.e.,\n\\emph{shortcuts}) that correlate with misinformation in training data but fail\nto generalize to the diverse and evolving nature of real-world misinformation.\nThis issue is exacerbated by large language models (LLMs), which can easily\ngenerate convincing misinformation through simple prompts. We introduce\nTruthOverTricks, a unified evaluation paradigm for measuring shortcut learning\nin misinformation detection. TruthOverTricks categorizes shortcut behaviors\ninto intrinsic shortcut induction and extrinsic shortcut injection, and\nevaluates seven representative detectors across 14 popular benchmarks, along\nwith two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.\nEmpirical results reveal that existing detectors suffer severe performance\ndegradation when exposed to both naturally occurring and adversarially crafted\nshortcuts. To address this, we propose SMF, an LLM-augmented data augmentation\nframework that mitigates shortcut reliance through paraphrasing, factual\nsummarization, and sentiment normalization. SMF consistently enhances\nrobustness across 16 benchmarks, encouraging models to rely on deeper semantic\nunderstanding rather than shortcut cues. To promote the development of\nmisinformation detectors, we have published the resources publicly at\nhttps://github.com/whr000001/TruthOverTricks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "summarization"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02393", "pdf": "https://arxiv.org/pdf/2506.02393", "abs": "https://arxiv.org/abs/2506.02393", "authors": ["Yongxian Liu", "Boyang Li", "Ting Liu", "Zaiping Lin", "Wei An"], "title": "RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods. Our code will be available at https://github.com/yongxianLiu/\nsoon.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02395", "pdf": "https://arxiv.org/pdf/2506.02395", "abs": "https://arxiv.org/abs/2506.02395", "authors": ["Xiaofeng Cong", "Yu-Xin Zhang", "Haoran Wei", "Yeying Jin", "Junming Hou", "Jie Gui", "Jing Zhang", "Dacheng Tao"], "title": "The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception", "categories": ["cs.CV"], "comment": null, "summary": "While nighttime image dehazing has been extensively studied, converting\nnighttime hazy images to daytime-equivalent brightness remains largely\nunaddressed. Existing methods face two critical limitations: (1) datasets\noverlook the brightness relationship between day and night, resulting in the\nbrightness mapping being inconsistent with the real world during image\nsynthesis; and (2) models do not explicitly incorporate daytime brightness\nknowledge, limiting their ability to reconstruct realistic lighting. To address\nthese challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)\nframework, which excels in both data synthesis and lighting reconstruction. Our\napproach starts with a data synthesis pipeline that simulates severe\ndistortions while enforcing brightness consistency between synthetic and\nreal-world scenes, providing a strong foundation for learning night-to-day\nbrightness mapping. Next, we propose a restoration model that integrates a\npre-trained diffusion model guided by a brightness perception network. This\ndesign harnesses the diffusion model's generative ability while adapting it to\nnighttime dehazing through brightness-aware optimization. Experiments validate\nour dataset's utility and the model's superior performance in joint haze\nremoval and brightness mapping.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02426", "pdf": "https://arxiv.org/pdf/2506.02426", "abs": "https://arxiv.org/abs/2506.02426", "authors": ["Maryam Berijanian", "Kuldeep Singh", "Amin Sehati"], "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Entity relationship classification remains a challenging task in information\nextraction, especially in scenarios with limited labeled data and complex\nrelational structures. In this study, we conduct a comparative analysis of\nthree distinct AI agent architectures designed to perform relation\nclassification using large language models (LLMs). The agentic architectures\nexplored include (1) reflective self-evaluation, (2) hierarchical task\ndecomposition, and (3) a novel multi-agent dynamic example generation\nmechanism, each leveraging different modes of reasoning and prompt adaptation.\nIn particular, our dynamic example generation approach introduces real-time\ncooperative and adversarial prompting. We systematically compare their\nperformance across multiple domains and model backends. Our experiments\ndemonstrate that multi-agent coordination consistently outperforms standard\nfew-shot prompting and approaches the performance of fine-tuned models. These\nfindings offer practical guidance for the design of modular, generalizable\nLLM-based systems for structured relation extraction. The source codes and\ndataset are available at\n\\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02460", "pdf": "https://arxiv.org/pdf/2506.02460", "abs": "https://arxiv.org/abs/2506.02460", "authors": ["Yupeng Qi", "Ziyu Lyu", "Min Yang", "Yanlin Wang", "Lu Bai", "Lixin Cui"], "title": "MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly applied across various\ndomains, enhancing safety while maintaining the helpfulness of LLMs has become\na critical challenge. Recent studies solve this problem through\nsafety-constrained online preference optimization or safety-constrained offline\npreference optimization. However, the safety-constrained online methods often\nsuffer from excessive safety, which might reduce helpfulness, while the\nsafety-constrained offline methods perform poorly in adaptively balancing\nsafety and helpfulness. To address these limitations, we propose MidPO, a\n\\textbf{\\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness\n\\textbf{\\underline{d}}ual \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization. Firstly, MidPO devises single-preference\nenhanced direct preference optimization approach to transform the base model\ninto two independent experts, termed safety and helpfulness experts, and\nfine-tunes the two independent experts for optimal safety or helpfulness\nperformance. Secondly, to achieve an effective balance between safety and\nhelpfulness, MidPO incorporates the two experts into the MoE framework and\ndesigns a dynamic routing mechanism to allocate contributions from each expert\nadaptively. We conduct quantitative and qualitative experiments on three\npopular datasets to demonstrate the proposed MidPO significantly outperforms\nstate-of-the-art approaches in both safety and helpfulness. The code and models\nwill be released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "safety"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02448", "pdf": "https://arxiv.org/pdf/2506.02448", "abs": "https://arxiv.org/abs/2506.02448", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02481", "pdf": "https://arxiv.org/pdf/2506.02481", "abs": "https://arxiv.org/abs/2506.02481", "authors": ["Inderjeet Nair", "Lu Wang"], "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on\nshort-form surveys and psychometric tests, yet real-world use involves\nlong-form, open-ended responses -- leaving value-related risks and preferences\nin practical settings largely underexplored. In this work, we ask: Do value\npreferences inferred from short-form tests align with those expressed in\nlong-form outputs? To address this question, we compare value preferences\nelicited from short-form reactions and long-form responses, varying the number\nof arguments in the latter to capture users' differing verbosity preferences.\nAnalyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),\nwe find (1) a weak correlation between value preferences inferred from\nshort-form and long-form responses across varying argument counts, and (2)\nsimilarly weak correlation between preferences derived from any two distinct\nlong-form generation settings. (3) Alignment yields only modest gains in the\nconsistency of value expression. Further, we examine how long-form generation\nattributes relate to value preferences, finding that argument specificity\nnegatively correlates with preference strength, while representation across\nscenarios shows a positive correlation. Our findings underscore the need for\nmore robust methods to ensure consistent value expression across diverse\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02459", "pdf": "https://arxiv.org/pdf/2506.02459", "abs": "https://arxiv.org/abs/2506.02459", "authors": ["Martin JJ. Bucher", "Iro Armeni"], "title": "ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment", "categories": ["cs.CV", "I.2.10; I.2.7"], "comment": "20 pages, 17 figures (incl. appendix)", "summary": "Scene synthesis and editing has emerged as a promising direction in computer\ngraphics. Current trained approaches for 3D indoor scenes either oversimplify\nobject semantics through one-hot class encodings (e.g., 'chair' or 'table'),\nrequire masked diffusion for editing, ignore room boundaries, or rely on floor\nplan renderings that fail to capture complex layouts. In contrast, LLM-based\nmethods enable richer semantics via natural language (e.g., 'modern studio with\nlight wood furniture') but do not support editing, remain limited to\nrectangular layouts or rely on weak spatial reasoning from implicit world\nmodels. We introduce ReSpace, a generative framework for text-driven 3D indoor\nscene synthesis and editing using autoregressive language models. Our approach\nfeatures a compact structured scene representation with explicit room\nboundaries that frames scene editing as a next-token prediction task. We\nleverage a dual-stage training approach combining supervised fine-tuning and\npreference alignment, enabling a specially trained language model for object\naddition that accounts for user instructions, spatial geometry, object\nsemantics, and scene-level composition. For scene editing, we employ a\nzero-shot LLM to handle object removal and prompts for addition. We further\nintroduce a novel voxelization-based evaluation that captures fine-grained\ngeometry beyond 3D bounding boxes. Experimental results surpass\nstate-of-the-art on object addition while maintaining competitive results on\nfull scene synthesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02477", "pdf": "https://arxiv.org/pdf/2506.02477", "abs": "https://arxiv.org/abs/2506.02477", "authors": ["Kunyu Wang", "Xueyang Fu", "Chengzhi Cao", "Chengjie Ge", "Wei Zhai", "Zheng-Jun Zha"], "title": "Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay", "categories": ["cs.CV"], "comment": null, "summary": "Current image de-raining methods primarily learn from a limited dataset,\nleading to inadequate performance in varied real-world rainy conditions. To\ntackle this, we introduce a new framework that enables networks to\nprogressively expand their de-raining knowledge base by tapping into a growing\npool of datasets, significantly boosting their adaptability. Drawing\ninspiration from the human brain's ability to continuously absorb and\ngeneralize from ongoing experiences, our approach borrow the mechanism of the\ncomplementary learning system. Specifically, we first deploy Generative\nAdversarial Networks (GANs) to capture and retain the unique features of new\ndata, mirroring the hippocampus's role in learning and memory. Then, the\nde-raining network is trained with both existing and GAN-synthesized data,\nmimicking the process of hippocampal replay and interleaved learning.\nFurthermore, we employ knowledge distillation with the replayed data to\nreplicate the synergy between the neocortex's activity patterns triggered by\nhippocampal replays and the pre-existing neocortical knowledge. This\ncomprehensive framework empowers the de-raining network to amass knowledge from\nvarious datasets, continually enhancing its performance on previously unseen\nrainy scenes. Our testing on three benchmark de-raining networks confirms the\nframework's effectiveness. It not only facilitates continuous knowledge\naccumulation across six datasets but also surpasses state-of-the-art methods in\ngeneralizing to new real-world scenarios.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02536", "pdf": "https://arxiv.org/pdf/2506.02536", "abs": "https://arxiv.org/abs/2506.02536", "authors": ["Xin Liu", "Lu Wang"], "title": "Answer Convergence as a Signal for Early Stopping in Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) prompting enhances reasoning in large language models\n(LLMs) but often leads to verbose and redundant outputs, thus increasing\ninference cost. We hypothesize that many reasoning steps are unnecessary for\nproducing correct answers. To investigate this, we start with a systematic\nstudy to examine what is the minimum reasoning required for a model to reach a\nstable decision. We find that on math reasoning tasks like math, models\ntypically converge to their final answers after 60\\% of the reasoning steps,\nsuggesting substantial redundancy in the remaining content. Based on these\ninsights, we propose three inference-time strategies to improve efficiency: (1)\nearly stopping via answer consistency, (2) boosting the probability of\ngenerating end-of-reasoning signals, and (3) a supervised method that learns\nwhen to stop based on internal activations. Experiments across five benchmarks\nand five open-weights LLMs show that our methods significantly reduce token\nusage with little or no accuracy drop. In particular, on NaturalQuestions,\nAnswer Consistency reduces tokens by over 40\\% while further improving\naccuracy. Our work underscores the importance of cost-effective reasoning\nmethods that operate at inference time, offering practical benefits for\nreal-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02535", "pdf": "https://arxiv.org/pdf/2506.02535", "abs": "https://arxiv.org/abs/2506.02535", "authors": ["Juntong Li", "Lingwei Dang", "Yukun Su", "Yun Hao", "Qingxin Xiao", "Yongwei Nie", "Qingyao Wu"], "title": "MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) methods based on reconstruction or prediction\nface two critical challenges: (1) strong generalization capability often\nresults in accurate reconstruction or prediction of abnormal events, making it\ndifficult to distinguish normal from abnormal patterns; (2) reliance only on\nlow-level appearance and motion cues limits their ability to identify\nhigh-level semantic in abnormal events from complex scenes. To address these\nlimitations, we propose a novel VAD framework with two key innovations. First,\nto suppress excessive generalization, we introduce the Sparse Feature Filtering\nModule (SFFM) that employs bottleneck filters to dynamically and adaptively\nremove abnormal information from features. Unlike traditional memory modules,\nit does not need to memorize the normal prototypes across the training dataset.\nFurther, we design the Mixture of Experts (MoE) architecture for SFFM. Each\nexpert is responsible for extracting specialized principal features during\nrunning time, and different experts are selectively activated to ensure the\ndiversity of the learned principal features. Second, to overcome the neglect of\nsemantics in existing methods, we integrate a Vision-Language Model (VLM) to\ngenerate textual descriptions for video clips, enabling comprehensive joint\nmodeling of semantic, appearance, and motion cues. Additionally, we enforce\nmodality consistency through semantic similarity constraints and motion\nframe-difference contrastive loss. Extensive experiments on multiple public\ndatasets validate the effectiveness of our multimodal joint modeling framework\nand sparse feature filtering paradigm. Project page at\nhttps://qzfm.github.io/sfn_vad_project_page/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02589", "pdf": "https://arxiv.org/pdf/2506.02589", "abs": "https://arxiv.org/abs/2506.02589", "authors": ["Maria Levchenko"], "title": "Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7; H.3.3"], "comment": null, "summary": "This paper addresses the challenge of Named Entity Recognition (NER) for\nperson names within the specialized domain of Russian news texts concerning\ncultural events. The study utilizes the unique SPbLitGuide dataset, a\ncollection of event announcements from Saint Petersburg spanning 1999 to 2019.\nA comparative evaluation of diverse NER models is presented, encompassing\nestablished transformer-based architectures such as DeepPavlov, RoBERTa, and\nSpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,\nand GPT-4o. Key findings highlight the superior performance of GPT-4o when\nprovided with specific prompting for JSON output, achieving an F1 score of\n0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The\nresearch contributes to a deeper understanding of current NER model\ncapabilities and limitations when applied to morphologically rich languages\nlike Russian within the cultural heritage domain, offering insights for\nresearchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)\nachieves F1=0.94 for both simple and structured prompts, demonstrating rapid\nprogress across model families and simplified deployment requirements.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02560", "pdf": "https://arxiv.org/pdf/2506.02560", "abs": "https://arxiv.org/abs/2506.02560", "authors": ["Zixiang Li", "Haoyu Wang", "Wei Wang", "Chuangchuang Tan", "Yunchao Wei", "Yao Zhao"], "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image generation and\nediting tasks. Inversion within these models aims to recover the latent noise\nrepresentation for a real or generated image, enabling reconstruction, editing,\nand other downstream tasks. However, to date, most inversion approaches suffer\nfrom an intrinsic trade-off between reconstruction accuracy and editing\nflexibility. This limitation arises from the difficulty of maintaining both\nsemantic alignment and structural consistency during the inversion process. In\nthis work, we introduce Dual-Conditional Inversion (DCI), a novel framework\nthat jointly conditions on the source prompt and reference image to guide the\ninversion process. Specifically, DCI formulates the inversion process as a\ndual-condition fixed-point optimization problem, minimizing both the latent\nnoise gap and the reconstruction error under the joint guidance. This design\nanchors the inversion trajectory in both semantic and visual space, leading to\nmore accurate and editable latent representations. Our novel setup brings new\nunderstanding to the inversion process. Extensive experiments demonstrate that\nDCI achieves state-of-the-art performance across multiple editing tasks,\nsignificantly improving both reconstruction quality and editing precision.\nFurthermore, we also demonstrate that our method achieves strong results in\nreconstruction tasks, implying a degree of robustness and generalizability\napproaching the ultimate goal of the inversion process.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02601", "pdf": "https://arxiv.org/pdf/2506.02601", "abs": "https://arxiv.org/abs/2506.02601", "authors": ["Shiyu Shen", "Bin Pan", "Ziye Zhang", "Zhenwei Shi"], "title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recently, hyperspectral image generation has received increasing attention,\nbut existing generative models rely on conditional generation schemes, which\nlimits the diversity of generated images. Diffusion models are popular for\ntheir ability to generate high-quality samples, but adapting these models from\nRGB to hyperspectral data presents the challenge of high dimensionality and\nphysical constraints. To address these challenges, we propose a novel diffusion\nmodel guided by hyperspectral unmixing. Our model comprises two key modules: an\nunmixing autoencoder module and an abundance diffusion module. The unmixing\nautoencoder module leverages unmixing guidance to shift the generative task\nfrom the image space to the low-dimensional abundance space, significantly\nreducing computational complexity while preserving high fidelity. The abundance\ndiffusion module generates samples that satisfy the constraints of\nnon-negativity and unity, ensuring the physical consistency of the\nreconstructed HSIs. Additionally, we introduce two evaluation metrics tailored\nto hyperspectral data. Empirical results, evaluated using both traditional\nmetrics and our proposed metrics, indicate that our model is capable of\ngenerating high-quality and diverse hyperspectral images, offering an\nadvancement in hyperspectral data generation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02740", "pdf": "https://arxiv.org/pdf/2506.02740", "abs": "https://arxiv.org/abs/2506.02740", "authors": ["Amaç Herdağdelen", "Marco Baroni"], "title": "Stereotypical gender actions can be extracted from Web text", "categories": ["cs.CL"], "comment": null, "summary": "We extracted gender-specific actions from text corpora and Twitter, and\ncompared them to stereotypical expectations of people. We used Open Mind Common\nSense (OMCS), a commonsense knowledge repository, to focus on actions that are\npertinent to common sense and daily life of humans. We use the gender\ninformation of Twitter users and Web-corpus-based pronoun/name gender\nheuristics to compute the gender bias of the actions. With high recall, we\nobtained a Spearman correlation of 0.47 between corpus-based predictions and a\nhuman gold standard, and an area under the ROC curve of 0.76 when predicting\nthe polarity of the gold standard. We conclude that it is feasible to use\nnatural text (and a Twitter-derived corpus in particular) in order to augment\ncommonsense repositories with the stereotypical gender expectations of actions.\nWe also present a dataset of 441 commonsense actions with human judges' ratings\non whether the action is typically/slightly masculine/feminine (or neutral),\nand another larger dataset of 21,442 actions automatically rated by the methods\nwe investigate in this study.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02753", "pdf": "https://arxiv.org/pdf/2506.02753", "abs": "https://arxiv.org/abs/2506.02753", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "Multi-task Learning with Active Learning for Arabic Offensive Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of social media has amplified the spread of offensive,\nviolent, and vulgar speech, which poses serious societal and cybersecurity\nconcerns. Detecting such content in Arabic text is particularly complex due to\nlimited labeled data, dialectal variations, and the language's inherent\ncomplexity. This paper proposes a novel framework that integrates multi-task\nlearning (MTL) with active learning to enhance offensive speech detection in\nArabic social media text. By jointly training on two auxiliary tasks, violent\nand vulgar speech, the model leverages shared representations to improve the\ndetection accuracy of the offensive speech. Our approach dynamically adjusts\ntask weights during training to balance the contribution of each task and\noptimize performance. To address the scarcity of labeled data, we employ an\nactive learning strategy through several uncertainty sampling techniques to\niteratively select the most informative samples for model training. We also\nintroduce weighted emoji handling to better capture semantic cues. Experimental\nresults on the OSACT2022 dataset show that the proposed framework achieves a\nstate-of-the-art macro F1-score of 85.42%, outperforming existing methods while\nusing significantly fewer fine-tuning samples. The findings of this study\nhighlight the potential of integrating MTL with active learning for efficient\nand accurate offensive language detection in resource-constrained settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02677", "pdf": "https://arxiv.org/pdf/2506.02677", "abs": "https://arxiv.org/abs/2506.02677", "authors": ["Jintao Tong", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a\nsource-domain dataset to unseen target-domain datasets with limited\nannotations. Current methods typically compare the distance between training\nand testing samples for mask prediction. However, we find an entanglement\nproblem exists in this widely adopted method, which tends to bind sourcedomain\npatterns together and make each of them hard to transfer. In this paper, we aim\nto address this problem for the CD-FSS task. We first find a natural\ndecomposition of the ViT structure, based on which we delve into the\nentanglement problem for an interpretation. We find the decomposed ViT\ncomponents are crossly compared between images in distance calculation, where\nthe rational comparisons are entangled with those meaningless ones by their\nequal importance, leading to the entanglement problem. Based on this\ninterpretation, we further propose to address the entanglement problem by\nlearning to weigh for all comparisons of ViT components, which learn\ndisentangled features and re-compose them for the CD-FSS task, benefiting both\nthe generalization and finetuning. Experiments show that our model outperforms\nthe state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under\n1-shot and 5-shot settings, respectively.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02680", "pdf": "https://arxiv.org/pdf/2506.02680", "abs": "https://arxiv.org/abs/2506.02680", "authors": ["Julius Erbach", "Dominik Narnhofer", "Andreas Dombos", "Bernt Schiele", "Jan Eric Lenssen", "Konrad Schindler"], "title": "Solving Inverse Problems with FLAIR", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Flow-based latent generative models such as Stable Diffusion 3 are able to\ngenerate images with remarkable quality, even enabling photorealistic\ntext-to-image generation. Their impressive performance suggests that these\nmodels should also constitute powerful priors for inverse imaging problems, but\nthat approach has not yet led to comparable fidelity. There are several key\nobstacles: (i) the encoding into a lower-dimensional latent space makes the\nunderlying (forward) mapping non-linear; (ii) the data likelihood term is\nusually intractable; and (iii) learned generative models struggle to recover\nrare, atypical data modes during inference. We present FLAIR, a novel training\nfree variational framework that leverages flow-based generative models as a\nprior for inverse problems. To that end, we introduce a variational objective\nfor flow matching that is agnostic to the type of degradation, and combine it\nwith deterministic trajectory adjustments to recover atypical modes. To enforce\nexact consistency with the observed data, we decouple the optimization of the\ndata fidelity and regularization terms. Moreover, we introduce a time-dependent\ncalibration scheme in which the strength of the regularization is modulated\naccording to off-line accuracy estimates. Results on standard imaging\nbenchmarks demonstrate that FLAIR consistently outperforms existing diffusion-\nand flow-based methods in terms of reconstruction quality and sample diversity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02690", "pdf": "https://arxiv.org/pdf/2506.02690", "abs": "https://arxiv.org/abs/2506.02690", "authors": ["Yurui Zhao", "Xiang Wang", "Jiahong Liu", "Irwin King", "Zhitao Huang"], "title": "Towards Geometry Problem Solving in the Large Model Era: A Survey", "categories": ["cs.CV", "math.GT"], "comment": "8pages, 4 figures, conference submission", "summary": "Geometry problem solving (GPS) represents a critical frontier in artificial\nintelligence, with profound applications in education, computer-aided design,\nand computational graphics. Despite its significance, automating GPS remains\nchallenging due to the dual demands of spatial understanding and rigorous\nlogical reasoning. Recent advances in large models have enabled notable\nbreakthroughs, particularly for SAT-level problems, yet the field remains\nfragmented across methodologies, benchmarks, and evaluation frameworks. This\nsurvey systematically synthesizes GPS advancements through three core\ndimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,\nand (3) reasoning paradigms. We further propose a unified analytical paradigm,\nassess current limitations, and identify emerging opportunities to guide future\nresearch toward human-level geometric reasoning, including automated benchmark\ngeneration and interpretable neuro-symbolic integration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02827", "pdf": "https://arxiv.org/pdf/2506.02827", "abs": "https://arxiv.org/abs/2506.02827", "authors": ["Yulin Dou", "Jiangming Liu"], "title": "TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can effectively elicit human preferences through\nmulti-turn dialogue. Complex tasks can be accomplished through iterative\nclarifying questions and final responses generated by an LLM acting as a\nquestioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches\nbased on self-taught reasoning struggle to identify optimal dialogue\ntrajectories and avoid irrelevant questions to the tasks. To address this\nlimitation, we propose TO-GATE, a novel framework that enhances question\ngeneration through trajectory optimization, which consists of two key\ncomponents: a clarification resolver that generates optimal questioning\ntrajectories, and a summarizer that ensures task-aligned final responses. The\ntrajectory optimization enables the model to produce effective elicitation\nquestions and summary responses tailored to specific tasks. Experimental\nresults demonstrate that TO-GATE significantly outperforms baseline methods,\nachieving a 9.32% improvement on standard preference elicitation tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "dialogue"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02733", "pdf": "https://arxiv.org/pdf/2506.02733", "abs": "https://arxiv.org/abs/2506.02733", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02921", "pdf": "https://arxiv.org/pdf/2506.02921", "abs": "https://arxiv.org/abs/2506.02921", "authors": ["Yijun Yang", "Zeyu Huang", "Wenhao Zhu", "Zihan Qiu", "Fei Yuan", "Jeff Z. Pan", "Ivan Titov"], "title": "A Controllable Examination for Long-Context Language Models", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: $\\textit{seamless context}$, $\\textit{controllable setting}$, and\n$\\textit{sound evaluation}$. This study introduces $\\textbf{LongBioBench}$, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\n$\\textit{understanding}$, $\\textit{reasoning}$, and $\\textit{trustworthiness}$.\nOur experimental evaluation, which includes $\\textbf{18}$ LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02959", "pdf": "https://arxiv.org/pdf/2506.02959", "abs": "https://arxiv.org/abs/2506.02959", "authors": ["Zhixiong Su", "Yichen Wang", "Herun Wan", "Zhaohan Zhang", "Minnan Luo"], "title": "HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The misuse of large language models (LLMs) poses potential risks, motivating\nthe development of machine-generated text (MGT) detection. Existing literature\nprimarily concentrates on binary, document-level detection, thereby neglecting\ntexts that are composed jointly by human and LLM contributions. Hence, this\npaper explores the possibility of fine-grained MGT detection under human-AI\ncoauthoring. We suggest fine-grained detectors can pave pathways toward\ncoauthored text detection with a numeric AI ratio. Specifically, we propose a\ndataset, HACo-Det, which produces human-AI coauthored texts via an automatic\npipeline with word-level attribution labels. We retrofit seven prevailing\ndocument-level detectors to generalize them to word-level detection. Then we\nevaluate these detectors on HACo-Det on both word- and sentence-level detection\ntasks. Empirical results show that metric-based methods struggle to conduct\nfine-grained detection with a 0.462 average F1 score, while finetuned models\nshow superior performance and better generalization across domains. However, we\nargue that fine-grained co-authored text detection is far from solved. We\nfurther analyze factors influencing performance, e.g., context window, and\nhighlight the limitations of current methods, pointing to potential avenues for\nimprovement.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02961", "pdf": "https://arxiv.org/pdf/2506.02961", "abs": "https://arxiv.org/abs/2506.02961", "authors": ["Yan Gao", "Massimo Roberto Scamarcia", "Javier Fernandez-Marques", "Mohammad Naseri", "Chong Shen Ng", "Dimitris Stripelis", "Zexi Li", "Tao Shen", "Jiamu Bai", "Daoyuan Chen", "Zikai Zhang", "Rui Hu", "InSeo Song", "Lee KangYoon", "Hong Jia", "Ting Dang", "Junyan Wang", "Zheyuan Liu", "Daniel Janes Beutel", "Lingjuan Lyu", "Nicholas D. Lane"], "title": "FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved state-of-the-art results across\ndiverse domains, yet their development remains reliant on vast amounts of\npublicly available data, raising concerns about data scarcity and the lack of\naccess to domain-specific, sensitive information. Federated Learning (FL)\npresents a compelling framework to address these challenges by enabling\ndecentralized fine-tuning on pre-trained LLMs without sharing raw data.\nHowever, the compatibility and performance of pre-trained LLMs in FL settings\nremain largely under explored. We introduce the FlowerTune LLM Leaderboard, a\nfirst-of-its-kind benchmarking suite designed to evaluate federated fine-tuning\nof LLMs across four diverse domains: general NLP, finance, medical, and coding.\nEach domain includes federated instruction-tuning datasets and domain-specific\nevaluation metrics. Our results, obtained through a collaborative, open-source\nand community-driven approach, provide the first comprehensive comparison\nacross 26 pre-trained LLMs with different aggregation and fine-tuning\nstrategies under federated settings, offering actionable insights into model\nperformance, resource constraints, and domain adaptation. This work lays the\nfoundation for developing privacy-preserving, domain-specialized LLMs for\nreal-world applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02979", "pdf": "https://arxiv.org/pdf/2506.02979", "abs": "https://arxiv.org/abs/2506.02979", "authors": ["Atsumoto Ohashi", "Shinya Iizuka", "Jingjing Jiang", "Ryuichiro Higashinaka"], "title": "Towards a Japanese Full-duplex Spoken Dialogue System", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Full-duplex spoken dialogue systems, which can model simultaneous\nbidirectional features of human conversations such as speech overlaps and\nbackchannels, have attracted significant attention recently. However, the study\nof full-duplex spoken dialogue systems for the Japanese language has been\nlimited, and the research on their development in Japanese remains scarce. In\nthis paper, we present the first publicly available full-duplex spoken dialogue\nmodel in Japanese, which is built upon Moshi, a full-duplex dialogue model in\nEnglish. Our model is trained through a two-stage process: pre-training on a\nlarge-scale spoken dialogue data in Japanese, followed by fine-tuning on\nhigh-quality stereo spoken dialogue data. We further enhance the model's\nperformance by incorporating synthetic dialogue data generated by a\nmulti-stream text-to-speech system. Evaluation experiments demonstrate that the\ntrained model outperforms Japanese baseline models in both naturalness and\nmeaningfulness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02789", "pdf": "https://arxiv.org/pdf/2506.02789", "abs": "https://arxiv.org/abs/2506.02789", "authors": ["Renxing Li", "Weiyi Tang", "Peiqi Li", "Qiming Huang", "Jiayuan She", "Shengkai Li", "Haoran Xu", "Yeyun Wan", "Jing Liu", "Hailong Fu", "Xiang Li", "Jiangang Chen"], "title": "Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker\nof secondary brain injury, with a significant linear correlation observed\nbetween optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD\ncould effectively support dynamic evaluation of ICP. However, ONSD measurement\nis heavily reliant on the operator's experience and skill, particularly in\nmanually selecting the optimal frame from ultrasound sequences and measuring\nONSD. Approach. This paper presents a novel method to automatically identify\nthe optimal frame from video sequences for ONSD measurement by employing the\nKernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative\nClustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and\nmeasured using a Gaussian Mixture Model (GMM) combined with a\nKL-divergence-based method. Results. When compared with the average\nmeasurements of two expert clinicians, the proposed method achieved a mean\nerror, mean squared deviation, and intraclass correlation coefficient (ICC) of\n0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that\nthis method provides highly accurate automated ONSD measurements, showing\npotential for clinical application.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["SLiC"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02998", "pdf": "https://arxiv.org/pdf/2506.02998", "abs": "https://arxiv.org/abs/2506.02998", "authors": ["Đorđe Klisura", "Astrid R Bernaga Torres", "Anna Karen Gárate-Escamilla", "Rajesh Roshan Biswal", "Ke Yang", "Hilal Pataci", "Anthony Rios"], "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02866", "pdf": "https://arxiv.org/pdf/2506.02866", "abs": "https://arxiv.org/abs/2506.02866", "authors": ["Ahsan Baidar Bakht", "Muhayy Ud Din", "Sajid Javed", "Irfan Hussain"], "title": "MVTD: A Benchmark Dataset for Maritime Visual Object Tracking", "categories": ["cs.CV"], "comment": "Submited to Nature Scientific Data", "summary": "Visual Object Tracking (VOT) is a fundamental task with widespread\napplications in autonomous navigation, surveillance, and maritime robotics.\nDespite significant advances in generic object tracking, maritime environments\ncontinue to present unique challenges, including specular water reflections,\nlow-contrast targets, dynamically changing backgrounds, and frequent\nocclusions. These complexities significantly degrade the performance of\nstate-of-the-art tracking algorithms, highlighting the need for domain-specific\ndatasets. To address this gap, we introduce the Maritime Visual Tracking\nDataset (MVTD), a comprehensive and publicly available benchmark specifically\ndesigned for maritime VOT. MVTD comprises 182 high-resolution video sequences,\ntotaling approximately 150,000 frames, and includes four representative object\nclasses: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset\ncaptures a diverse range of operational conditions and maritime scenarios,\nreflecting the real-world complexities of maritime environments. We evaluated\n14 recent SOTA tracking algorithms on the MVTD benchmark and observed\nsubstantial performance degradation compared to their performance on\ngeneral-purpose datasets. However, when fine-tuned on MVTD, these models\ndemonstrate significant performance gains, underscoring the effectiveness of\ndomain adaptation and the importance of transfer learning in specialized\ntracking contexts. The MVTD dataset fills a critical gap in the visual tracking\ncommunity by providing a realistic and challenging benchmark for maritime\nscenarios. Dataset and Source Code can be accessed here\n\"https://github.com/AhsanBaidar/MVTD\".", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02882", "pdf": "https://arxiv.org/pdf/2506.02882", "abs": "https://arxiv.org/abs/2506.02882", "authors": ["Sohyun Lee", "Yeho Kwon", "Lukas Hoyer", "Suha Kwak"], "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03136", "pdf": "https://arxiv.org/pdf/2506.03136", "abs": "https://arxiv.org/abs/2506.03136", "authors": ["Yinjie Wang", "Ling Yang", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning", "categories": ["cs.CL"], "comment": "Project: https://github.com/Gen-Verse/CURE", "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "code generation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02976", "pdf": "https://arxiv.org/pdf/2506.02976", "abs": "https://arxiv.org/abs/2506.02976", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinović", "Donik Vršnak", "Sven Lončarić", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Araújo", "Marzieh Oghbaie", "Hrvoje Bogunović", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltrán-Carrero", "Juan J. Gómez-Valverde", "Javier Torresano-Rodríquez", "Álvaro Caballero-Sastre", "María J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica Kächele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "Béatrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenolé Quellec"], "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "categories": ["cs.CV", "cs.AI"], "comment": "MARIO-MICCAI-CHALLENGE 2024", "summary": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02057", "pdf": "https://arxiv.org/pdf/2506.02057", "abs": "https://arxiv.org/abs/2506.02057", "authors": ["David Sasu", "Kweku Andoh Yamoah", "Benedict Quartey", "Natalie Schluter"], "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to Interspeech 2025", "summary": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03007", "pdf": "https://arxiv.org/pdf/2506.03007", "abs": "https://arxiv.org/abs/2506.03007", "authors": ["Jiarui Wang", "Huiyu Duan", "Juntong Wang", "Ziheng Jia", "Woo Yi Yang", "Xiaorong Zhu", "Yu Zhao", "Jiaying Qian", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02178", "pdf": "https://arxiv.org/pdf/2506.02178", "abs": "https://arxiv.org/abs/2506.02178", "authors": ["Thai-Binh Nguyen", "Ngoc-Quan Pham", "Alexander Waibel"], "title": "Cocktail-Party Audio-Visual Speech Recognition", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech\nrecognition in challenging environments, such as cocktail-party scenarios,\nwhere relying solely on audio proves insufficient. However, current AVSR models\nare often optimized for idealized scenarios with consistently active speakers,\noverlooking the complexities of real-world settings that include both speaking\nand silent facial segments. This study addresses this gap by introducing a\nnovel audio-visual cocktail-party dataset designed to benchmark current AVSR\nsystems and highlight the limitations of prior approaches in realistic noisy\nconditions. Additionally, we contribute a 1526-hour AVSR dataset comprising\nboth talking-face and silent-face segments, enabling significant performance\ngains in cocktail-party environments. Our approach reduces WER by 67% relative\nto the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,\nwithout relying on explicit segmentation cues.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02479", "pdf": "https://arxiv.org/pdf/2506.02479", "abs": "https://arxiv.org/abs/2506.02479", "authors": ["Kalyan Nakka", "Nitesh Saxena"], "title": "BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage", "categories": ["cs.CR", "cs.CL"], "comment": "24 pages, 24 figures, and 7 tables", "summary": "The inherent risk of generating harmful and unsafe content by Large Language\nModels (LLMs), has highlighted the need for their safety alignment. Various\ntechniques like supervised fine-tuning, reinforcement learning from human\nfeedback, and red-teaming were developed for ensuring the safety alignment of\nLLMs. However, the robustness of these aligned LLMs is always challenged by\nadversarial attacks that exploit unexplored and underlying vulnerabilities of\nthe safety alignment. In this paper, we develop a novel black-box jailbreak\nattack, called BitBypass, that leverages hyphen-separated bitstream camouflage\nfor jailbreaking aligned LLMs. This represents a new direction in jailbreaking\nby exploiting fundamental information representation of data as continuous\nbits, rather than leveraging prompt engineering or adversarial manipulations.\nOur evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude\n3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the\ncapabilities of BitBypass in bypassing their safety alignment and tricking them\ninto generating harmful and unsafe content. Further, we observed that BitBypass\noutperforms several state-of-the-art jailbreak attacks in terms of stealthiness\nand attack success. Overall, these results highlights the effectiveness and\nefficiency of BitBypass in jailbreaking these state-of-the-art LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02529", "pdf": "https://arxiv.org/pdf/2506.02529", "abs": "https://arxiv.org/abs/2506.02529", "authors": ["Nguyen-Khang Le", "Quan Minh Bui", "Minh Ngoc Nguyen", "Hiep Nguyen", "Trung Vo", "Son T. Luu", "Shoshin Nomura", "Minh Le Nguyen"], "title": "Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs", "categories": ["cs.SE", "cs.AI", "cs.CL", "I.2.7"], "comment": "Published in the Proceedings of JSAI 2025", "summary": "Web applications are critical to modern software ecosystems, yet ensuring\ntheir reliability remains challenging due to the complexity and dynamic nature\nof web interfaces. Recent advances in large language models (LLMs) have shown\npromise in automating complex tasks, but limitations persist in handling\ndynamic navigation flows and complex form interactions. This paper presents an\nautomated system for generating test cases for two key aspects of web\napplication testing: site navigation and form filling. For site navigation, the\nsystem employs screen transition graphs and LLMs to model navigation flows and\ngenerate test scenarios. For form filling, it uses state graphs to handle\nconditional forms and automates Selenium script generation. Key contributions\ninclude: (1) a novel integration of graph structures and LLMs for site\nnavigation testing, (2) a state graph-based approach for automating\nform-filling test cases, and (3) a comprehensive dataset for evaluating\nform-interaction testing. Experimental results demonstrate the system's\neffectiveness in improving test coverage and robustness, advancing the state of\nweb application testing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03117", "pdf": "https://arxiv.org/pdf/2506.03117", "abs": "https://arxiv.org/abs/2506.03117", "authors": ["Zeliang Zhang", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Chenliang Xu"], "title": "Targeted Forgetting of Image Subgroups in CLIP Models", "categories": ["cs.CV"], "comment": "12 Figures,5 Pages. The project page is\n  \\url{https://zhangaipi.github.io/forget_clip/}", "summary": "Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03119", "pdf": "https://arxiv.org/pdf/2506.03119", "abs": "https://arxiv.org/abs/2506.03119", "authors": ["Zujin Guo", "Size Wu", "Zhongang Cai", "Wei Li", "Chen Change Loy"], "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior", "categories": ["cs.CV"], "comment": "Project Page: https://gseancdat.github.io/projects/PoseFuse3D_KI", "summary": "Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03126", "pdf": "https://arxiv.org/pdf/2506.03126", "abs": "https://arxiv.org/abs/2506.03126", "authors": ["Lu Qiu", "Yizhuo Li", "Yuying Ge", "Yixiao Ge", "Ying Shan", "Xihui Liu"], "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation", "categories": ["cs.CV"], "comment": "Project released at: https://qiulu66.github.io/animeshooter/", "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03139", "pdf": "https://arxiv.org/pdf/2506.03139", "abs": "https://arxiv.org/abs/2506.03139", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02554", "pdf": "https://arxiv.org/pdf/2506.02554", "abs": "https://arxiv.org/abs/2506.02554", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03004", "pdf": "https://arxiv.org/pdf/2506.03004", "abs": "https://arxiv.org/abs/2506.03004", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03095", "pdf": "https://arxiv.org/pdf/2506.03095", "abs": "https://arxiv.org/abs/2506.03095", "authors": ["Man Luo", "David Cobbley", "Xin Su", "Shachar Rosenman", "Vasudev Lal", "Shao-Yen Tseng", "Phillip Howard"], "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02219", "pdf": "https://arxiv.org/pdf/2506.02219", "abs": "https://arxiv.org/abs/2506.02219", "authors": ["Abhishek Madan", "Nicholas Sharp", "Francis Williams", "Ken Museth", "David I. W. Levin"], "title": "Stochastic Barnes-Hut Approximation for Fast Summation on the GPU", "categories": ["cs.GR"], "comment": "11 pages, 9 figures. To appear in ACM SIGGRAPH 2025", "summary": "We present a novel stochastic version of the Barnes-Hut approximation.\nRegarding the level-of-detail (LOD) family of approximations as control\nvariates, we construct an unbiased estimator of the kernel sum being\napproximated. Through several examples in graphics applications such as winding\nnumber computation and smooth distance evaluation, we demonstrate that our\nmethod is well-suited for GPU computation, capable of outperforming a\nGPU-optimized implementation of the deterministic Barnes-Hut approximation by\nachieving equal median error in up to 9.4x less time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02620", "pdf": "https://arxiv.org/pdf/2506.02620", "abs": "https://arxiv.org/abs/2506.02620", "authors": ["Dongyu Yan", "Leyi Wu", "Jiantao Lin", "Luozhou Wang", "Tianshuo Xu", "Zhifei Chen", "Zhen Yang", "Lie Xu", "Shunsi Zhang", "Yingcong Chen"], "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 10 figures in main paper, 10 pages, 12 figures in\n  supplementary", "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce \\textbf{FlexPainter},\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02895", "pdf": "https://arxiv.org/pdf/2506.02895", "abs": "https://arxiv.org/abs/2506.02895", "authors": ["Ahmad AlMughrabi", "Umair Haroon", "Ricardo Marques", "Petia Radeva"], "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02012", "pdf": "https://arxiv.org/pdf/2506.02012", "abs": "https://arxiv.org/abs/2506.02012", "authors": ["Zehua Liu", "Xiaolou Li", "Li Guo", "Lantian Li", "Dong Wang"], "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Visual Speech Recognition (VSR) transcribes speech by analyzing lip\nmovements. Recently, Large Language Models (LLMs) have been integrated into VSR\nsystems, leading to notable performance improvements. However, the potential of\nLLMs has not been extensively studied, and how to effectively utilize LLMs in\nVSR tasks remains unexplored. This paper systematically explores how to better\nleverage LLMs for VSR tasks and provides three key contributions: (1) Scaling\nTest: We study how the LLM size affects VSR performance, confirming a scaling\nlaw in the VSR task. (2) Context-Aware Decoding: We add contextual text to\nguide the LLM decoding, improving recognition accuracy. (3) Iterative\nPolishing: We propose iteratively refining LLM outputs, progressively reducing\nrecognition errors. Extensive experiments demonstrate that by these designs,\nthe great potential of LLMs can be largely harnessed, leading to significant\nVSR performance improvement.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03118", "pdf": "https://arxiv.org/pdf/2506.03118", "abs": "https://arxiv.org/abs/2506.03118", "authors": ["Zhiyuan Yu", "Zhe Li", "Hujun Bao", "Can Yang", "Xiaowei Zhou"], "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Track). Project page:\n  https://zju3dv.github.io/humanram/", "summary": "3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02015", "pdf": "https://arxiv.org/pdf/2506.02015", "abs": "https://arxiv.org/abs/2506.02015", "authors": ["Yoonjin Oh", "Yongjin Kim", "Hyomin Kim", "Donghwan Chi", "Sungwoong Kim"], "title": "Object-centric Self-improving Preference Optimization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly improved both image understanding and generation capabilities.\nDespite these improvements, MLLMs still struggle with fine-grained visual\ncomprehension, particularly in text-to-image generation tasks. While preference\noptimization methods have been explored to address these limitations in image\nunderstanding tasks, their application to image generation remains largely\nunderexplored. To address this gap, we propose an Object-centric Self-improving\nPreference Optimization (OSPO) framework designed for text-to-image generation\nby MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without\nrequiring any external datasets or models. OSPO emphasizes the importance of\nhigh-quality preference pair data, which is critical for effective preference\noptimization. To achieve this, it introduces a self-improving mechanism that\nautonomously constructs object-level contrastive preference pairs through\nobject-centric prompt perturbation, densification and VQA scoring. This process\neliminates ambiguous or disproportionate variations commonly found in naively\ngenerated preference pairs, thereby enhancing the effectiveness of preference\noptimization. We validate OSPO on three representative compositional\ntext-to-image benchmarks, demonstrating substantial performance gains over\nbaseline models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02019", "pdf": "https://arxiv.org/pdf/2506.02019", "abs": "https://arxiv.org/abs/2506.02019", "authors": ["E Fan", "Weizong Wang", "Tianhan Zhang"], "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking", "categories": ["cs.CL"], "comment": "19 pages, 8 figures", "summary": "Computational Fluid Dynamics (CFD) is essential for scientific and\nengineering advancements but is limited by operational complexity and the need\nfor extensive expertise. This paper presents ChatCFD, a large language\nmodel-driven pipeline that automates CFD workflows within the OpenFOAM\nframework. It enables users to configure and execute complex simulations from\nnatural language prompts or published literature with minimal expertise. The\ninnovation is its structured approach to database construction, configuration\nvalidation, and error reflection, integrating CFD and OpenFOAM knowledge with\ngeneral language models to improve accuracy and adaptability. Validation shows\nChatCFD can autonomously reproduce published CFD results, handling complex,\nunseen configurations beyond basic examples, a task challenging for general\nlanguage models.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02661", "pdf": "https://arxiv.org/pdf/2506.02661", "abs": "https://arxiv.org/abs/2506.02661", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": "12 pages, 5 figures", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02017", "pdf": "https://arxiv.org/pdf/2506.02017", "abs": "https://arxiv.org/abs/2506.02017", "authors": ["Camilla Quaresmini", "Giacomo Zanotti"], "title": "Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Automatic Gender Recognition (AGR) systems are an increasingly widespread\napplication in the Machine Learning (ML) landscape. While these systems are\ntypically understood as detecting gender, they often classify datapoints based\non observable features correlated at best with either male or female sex. In\naddition to questionable binary assumptions, from an epistemological point of\nview, this is problematic for two reasons. First, there exists a gap between\nthe categories the system is meant to predict (woman versus man) and those onto\nwhich their output reasonably maps (female versus male). What is more, gender\ncannot be inferred on the basis of such observable features. This makes AGR\ntools often unreliable, especially in the case of non-binary and gender\nnon-conforming people. We suggest a theoretical and practical rethinking of AGR\nsystems. To begin, distinctions are made between sex, gender, and gender\nexpression. Then, we build upon the observation that, unlike algorithmic\nmisgendering, human-human misgendering is open to the possibility of\nre-evaluation and correction. We suggest that analogous dynamics should be\nrecreated in AGR, giving users the possibility to correct the system's output.\nWhile implementing such a feedback mechanism could be regarded as diminishing\nthe system's autonomy, it represents a way to significantly increase fairness\nlevels in AGR. This is consistent with the conceptual change of paradigm that\nwe advocate for AGR systems, which should be understood as tools respecting\nindividuals' rights and capabilities of self-expression and determination.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02020", "pdf": "https://arxiv.org/pdf/2506.02020", "abs": "https://arxiv.org/abs/2506.02020", "authors": ["Youze Xue", "Dian Li", "Gang Liu"], "title": "Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the rapid advancement of multi-modal large language models (MLLMs) in\nrecent years, the foundational Contrastive Language-Image Pretraining (CLIP)\nframework has been successfully extended to MLLMs, enabling more powerful and\nuniversal multi-modal embeddings for a wide range of retrieval tasks. Despite\nthese developments, the core contrastive learning paradigm remains largely\nunchanged from CLIP-style models to MLLMs. Within this framework, the effective\nmining of hard negative samples continues to be a critical factor for enhancing\nperformance. Prior works have introduced both offline and online strategies for\nhard negative mining to improve the efficiency of contrastive learning. While\nthese approaches have led to improved multi-modal embeddings, the specific\ncontribution of each hard negative sample to the learning process has not been\nthoroughly investigated. In this work, we conduct a detailed analysis of the\ngradients of the info-NCE loss with respect to the query, positive, and\nnegative samples, elucidating the role of hard negatives in updating model\nparameters. Building upon this analysis, we propose to explicitly amplify the\ngradients associated with hard negative samples, thereby encouraging the model\nto learn more discriminative embeddings. Our multi-modal embedding model,\ntrained with the proposed Explicit Gradient Amplifier and based on the\nLLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the\nMMEB benchmark compared to previous methods utilizing the same MLLM backbone.\nFurthermore, when integrated with our self-developed MLLM, QQMM, our approach\nattains the top rank on the MMEB leaderboard. Code and models are released on\nhttps://github.com/QQ-MM/QQMM-embed.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02041", "pdf": "https://arxiv.org/pdf/2506.02041", "abs": "https://arxiv.org/abs/2506.02041", "authors": ["Duzhen Zhang", "Yong Ren", "Zhong-Zhi Li", "Yahan Yu", "Jiahua Dong", "Chenxing Li", "Zhilong Ji", "Jinfeng Bai"], "title": "Enhancing Multimodal Continual Instruction Tuning with BranchLoRA", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL2025 Main Conference", "summary": "Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal\nLarge Language Models (MLLMs) to continually align with human intent across\nsequential tasks. Existing approaches often rely on the Mixture-of-Experts\n(MoE) LoRA framework to preserve previous instruction alignments. However,\nthese methods are prone to Catastrophic Forgetting (CF), as they aggregate all\nLoRA blocks via simple summation, which compromises performance over time. In\nthis paper, we identify a critical parameter inefficiency in the MoELoRA\nframework within the MCIT context. Based on this insight, we propose\nBranchLoRA, an asymmetric framework to enhance both efficiency and performance.\nTo mitigate CF, we introduce a flexible tuning-freezing mechanism within\nBranchLoRA, enabling branches to specialize in intra-task knowledge while\nfostering inter-task collaboration. Moreover, we incrementally incorporate\ntask-specific routers to ensure an optimal branch distribution over time,\nrather than favoring the most recent task. To streamline inference, we\nintroduce a task selector that automatically routes test inputs to the\nappropriate router without requiring task identity. Extensive experiments on\nthe latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms\nMoELoRA and maintains its superiority across various MLLM sizes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02021", "pdf": "https://arxiv.org/pdf/2506.02021", "abs": "https://arxiv.org/abs/2506.02021", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Yew-Soon Ong", "Joey Tianyi Zhou"], "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of vision tasks and the scaling on datasets and\nmodels, redundancy reduction in vision datasets has become a key area of\nresearch. To address this issue, dataset distillation (DD) has emerged as a\npromising approach to generating highly compact synthetic datasets with\nsignificantly less redundancy while preserving essential information. However,\nwhile DD has been extensively studied for image datasets, DD on video datasets\nremains underexplored. Video datasets present unique challenges due to the\npresence of temporal information and varying levels of redundancy across\ndifferent classes. Existing DD approaches assume a uniform level of temporal\nredundancy across all different video semantics, which limits their\neffectiveness on video datasets. In this work, we propose Dynamic-Aware Video\nDistillation (DAViD), a Reinforcement Learning (RL) approach to predict the\noptimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop\nreward function is proposed to update the RL agent policy. To the best of our\nknowledge, this is the first study to introduce adaptive temporal resolution\nbased on video semantics in video dataset distillation. Our approach\nsignificantly outperforms existing DD methods, demonstrating substantial\nimprovements in performance. This work paves the way for future research on\nmore efficient and semantic-adaptive video dataset distillation research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02058", "pdf": "https://arxiv.org/pdf/2506.02058", "abs": "https://arxiv.org/abs/2506.02058", "authors": ["Xiang Li", "Jiayi Xin", "Qi Long", "Weijie J. Su"], "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02132", "pdf": "https://arxiv.org/pdf/2506.02132", "abs": "https://arxiv.org/abs/2506.02132", "authors": ["Michael Li", "Nishant Subramani"], "title": "Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large transformer-based language models dominate modern NLP, yet our\nunderstanding of how they encode linguistic information is rooted in studies of\nearly models like BERT and GPT-2. To better understand today's language models,\nwe investigate how both classical architectures (BERT, DeBERTa, GPT-2)and\ncontemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,\nLlama-3.1) represent lexical identity and inflectional morphology. We train\nlinear and nonlinear classifiers on layer-wise activations to predict word\nlemmas and inflectional features. We discover that models concentrate lexical\ninformation linearly in early layers and increasingly nonlinearly in later\nlayers, while keeping inflectional information uniformly accessible and\nlinearly separable throughout the layers. Further analysis reveals that these\nmodels encode inflectional morphology through generalizable abstractions, but\nrely predominantly on memorization to encode lexical identity. Remarkably,\nthese patterns emerge across all 16 models we test, despite differences in\narchitecture, size, and training regime (including pretrained and\ninstruction-tuned variants). This consistency suggests that, despite\nsubstantial advances in LLM technologies, transformer models organize\nlinguistic information in similar ways, indicating that these properties could\nbe fundamental for next token prediction and are learned early during\npretraining. Our code is available at\nhttps://github.com/ml5885/model_internal_sleuthing.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02112", "pdf": "https://arxiv.org/pdf/2506.02112", "abs": "https://arxiv.org/abs/2506.02112", "authors": ["Xuweiyi Chen", "Tian Xia", "Sihan Xu", "Jianing Yang", "Joyce Chai", "Zezhou Cheng"], "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/sab3r/", "summary": "We introduce a new task, Map and Locate, which unifies the traditionally\ndistinct objectives of open-vocabulary segmentation - detecting and segmenting\nobject instances based on natural language queries - and 3D reconstruction, the\nprocess of estimating a scene's 3D structure from visual inputs. Specifically,\nMap and Locate involves generating a point cloud from an unposed video and\nsegmenting object instances based on open-vocabulary queries. This task serves\nas a critical step toward real-world embodied AI applications and introduces a\npractical task that bridges reconstruction, recognition and reorganization. To\ntackle this task, we introduce a simple yet effective baseline, which we denote\nas SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer\nvision, and incorporates a lightweight distillation strategy. This method\ntransfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP\nand DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary\nfrozen networks, our model generates per-pixel semantic features and constructs\ncohesive point maps in a single forward pass. Compared to separately deploying\nMASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the\nMap and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic\nsegmentation and 3D tasks to comprehensively validate its effectiveness.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02150", "pdf": "https://arxiv.org/pdf/2506.02150", "abs": "https://arxiv.org/abs/2506.02150", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 Provisional Accept", "summary": "Deformable medical image registration is an essential task in\ncomputer-assisted interventions. This problem is particularly relevant to\noncological treatments, where precise image alignment is necessary for tracking\ntumor growth, assessing treatment response, and ensuring accurate delivery of\ntherapies. Recent AI methods can outperform traditional techniques in accuracy\nand speed, yet they often produce unreliable deformations that limit their\nclinical adoption. In this work, we address this challenge and introduce a\nnovel implicit registration framework that can predict accurate and reliable\ndeformations. Our insight is to reformulate image registration as a signal\nreconstruction problem: we learn a kernel function that can recover the dense\ndisplacement field from sparse keypoint correspondences. We integrate our\nmethod in a novel hierarchical architecture, and estimate the displacement\nfield in a coarse-to-fine manner. Our formulation also allows for efficient\nrefinement at test time, permitting clinicians to easily adjust registrations\nwhen needed. We validate our method on challenging intra-patient thoracic and\nabdominal zero-shot registration tasks, using public and internal datasets from\nthe local University Hospital. Our method not only shows competitive accuracy\nto state-of-the-art approaches, but also bridges the generalization gap between\nimplicit and explicit registration techniques. In particular, our method\ngenerates deformations that better preserve anatomical relationships and\nmatches the performance of specialized commercial systems, underscoring its\npotential for clinical adoption.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02157", "pdf": "https://arxiv.org/pdf/2506.02157", "abs": "https://arxiv.org/abs/2506.02157", "authors": ["Amir Hussein", "Cihan Xiao", "Matthew Wiesner", "Dan Povey", "Leibny Paola Garcia", "Sanjeev Khudanpur"], "title": "HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Neural transducers (NT) provide an effective framework for speech streaming,\ndemonstrating strong performance in automatic speech recognition (ASR).\nHowever, the application of NT to speech translation (ST) remains challenging,\nas existing approaches struggle with word reordering and performance\ndegradation when jointly modeling ASR and ST, resulting in a gap with\nattention-based encoder-decoder (AED) models. Existing NT-based ST approaches\nalso suffer from high computational training costs. To address these issues, we\npropose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech\nRecognition and Translation), a novel framework that factorizes ASR and\ntranslation tasks to better handle reordering. To ensure robust ST while\npreserving ASR performance, we use self-distillation with CTC consistency\nregularization. Moreover, we improve computational efficiency by incorporating\nbest practices from ASR transducers, including a down-sampled hierarchical\nencoder, a stateless predictor, and a pruned transducer loss to reduce training\ncomplexity. Finally, we introduce a blank penalty during decoding, reducing\ndeletions and improving translation quality. Our approach is evaluated on three\nconversational datasets Arabic, Spanish, and Mandarin achieving new\nstate-of-the-art performance among NT models and substantially narrowing the\ngap with AED-based systems.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02247", "pdf": "https://arxiv.org/pdf/2506.02247", "abs": "https://arxiv.org/abs/2506.02247", "authors": ["Yu Wang", "Juhyung Ha", "David J. Crandall"], "title": "PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss", "categories": ["cs.CV"], "comment": "4 pages, 1 figure, and 1 table", "summary": "Active speaker detection (ASD) in egocentric videos presents unique\nchallenges due to unstable viewpoints, motion blur, and off-screen speech\nsources - conditions under which traditional visual-centric methods degrade\nsignificantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with\nRegularization Network), an effective model that integrates a partially frozen\nWhisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly\nfuse cross-modal cues. To counteract modality imbalance, we introduce an\ninter-modal alignment loss that synchronizes audio and visual representations,\nenabling more consistent convergence across modalities. Without relying on\nmulti-speaker context or ideal frontal views, PAIR-Net achieves\nstate-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP,\nsurpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results\nhighlight the value of pretrained audio priors and alignment-based fusion for\nrobust ASD under real-world egocentric conditions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02294", "pdf": "https://arxiv.org/pdf/2506.02294", "abs": "https://arxiv.org/abs/2506.02294", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02298", "pdf": "https://arxiv.org/pdf/2506.02298", "abs": "https://arxiv.org/abs/2506.02298", "authors": ["Thai Hoang", "Kung-Hsiang Huang", "Shirley Kokane", "Jianguo Zhang", "Zuxin Liu", "Ming Zhu", "Jake Grigsby", "Tian Lan", "Michael S Ryoo", "Chien-Sheng Wu", "Shelby Heinecke", "Huan Wang", "Silvio Savarese", "Caiming Xiong", "Juan Carlos Niebles"], "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "LAM Simulator framework for agentic data generation", "summary": "Large Action Models (LAMs) for AI Agents offer incredible potential but face\nchallenges due to the need for high-quality training data, especially for\nmulti-steps tasks that involve planning, executing tool calls, and responding\nto feedback. To address these issues, we present LAM SIMULATOR, a comprehensive\nframework designed for online exploration of agentic tasks with high-quality\nfeedback. Our framework features a dynamic task query generator, an extensive\ncollection of tools, and an interactive environment where Large Language Model\n(LLM) Agents can call tools and receive real-time feedback. This setup enables\nLLM Agents to explore and solve tasks autonomously, facilitating the discovery\nof multiple approaches to tackle any given task. The resulting action\ntrajectory data are then used to create high-quality training datasets for\nLAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,\nhighlight the effectiveness of LAM SIMULATOR: models trained with\nself-generated datasets using our framework achieve significant performance\ngains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR\nrequires minimal human input during dataset creation, highlighting LAM\nSIMULATOR's efficiency and effectiveness in speeding up development of AI\nagents.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02295", "pdf": "https://arxiv.org/pdf/2506.02295", "abs": "https://arxiv.org/abs/2506.02295", "authors": ["Ahmed Wasfy", "Omer Nacar", "Abdelakreem Elkhateb", "Mahmoud Reda", "Omar Elshehy", "Adel Ammar", "Wadii Boulila"], "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02302", "pdf": "https://arxiv.org/pdf/2506.02302", "abs": "https://arxiv.org/abs/2506.02302", "authors": ["Russell Scheinberg", "Ameeta Agrawal", "Amber Shore", "So Young Lee"], "title": "Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Findings", "summary": "Large language models (LLMs) can explain grammatical rules, yet they often\nfail to apply those rules when judging sentence acceptability. We present\n\"grammar prompting\", an explain-then-process paradigm: a large LLM first\nproduces a concise explanation of the relevant syntactic phenomenon, then that\nexplanation is fed back as additional context to the target model -- either an\nLLM or a smaller language model (SLM) -- before deciding which sentence of a\nminimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian\nRuBLiMP benchmarks, this simple prompt design yields substantial improvements\nover strong baselines across many syntactic phenomena. Feeding an LLM's\nmetalinguistic explanation back to the target model bridges the gap between\nknowing a rule and using it. On SLMs, grammar prompting alone trims the average\nLLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by\n56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,\nlanguage-agnostic cue lets low-cost SLMs approach frontier-LLM performance in\nmultilingual settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02327", "pdf": "https://arxiv.org/pdf/2506.02327", "abs": "https://arxiv.org/abs/2506.02327", "authors": ["Yijun Yang", "Zhao-Yang Wang", "Qiuping Liu", "Shuwen Sun", "Kang Wang", "Rama Chellappa", "Zongwei Zhou", "Alan Yuille", "Lei Zhu", "Yu-Dong Zhang", "Jieneng Chen"], "title": "Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning", "categories": ["cs.CV"], "comment": null, "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02321", "pdf": "https://arxiv.org/pdf/2506.02321", "abs": "https://arxiv.org/abs/2506.02321", "authors": ["Pegah Alipoormolabashi", "Ajay Patel", "Niranjan Balasubramanian"], "title": "Quantifying Misattribution Unfairness in Authorship Attribution", "categories": ["cs.CL"], "comment": null, "summary": "Authorship misattribution can have profound consequences in real life. In\nforensic settings simply being considered as one of the potential authors of an\nevidential piece of text or communication can result in undesirable scrutiny.\nThis raises a fairness question: Is every author in the candidate pool at equal\nrisk of misattribution? Standard evaluation measures for authorship attribution\nsystems do not explicitly account for this notion of fairness. We introduce a\nsimple measure, Misattribution Unfairness Index (MAUIk), which is based on how\noften authors are ranked in the top k for texts they did not write. Using this\nmeasure we quantify the unfairness of five models on two different datasets.\nAll models exhibit high levels of unfairness with increased risks for some\nauthors. Furthermore, we find that this unfairness relates to how the models\nembed the authors as vectors in the latent search space. In particular, we\nobserve that the risk of misattribution is higher for authors closer to the\ncentroid (or center) of the embedded authors in the haystack. These results\nindicate the potential for harm and the need for communicating with and\ncalibrating end users on misattribution risk when building and providing such\nmodels for downstream use.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02338", "pdf": "https://arxiv.org/pdf/2506.02338", "abs": "https://arxiv.org/abs/2506.02338", "authors": ["Hyungjoo Chae", "Dongjin Kang", "Jihyuk Kim", "Beong-woo Kwak", "Sunghyun Park", "Haeju Park", "Jinyoung Yeo", "Moontae Lee", "Kyungjae Lee"], "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL", "categories": ["cs.CL"], "comment": "ACL 2025 Industry", "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "o1", "reasoning model"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02359", "pdf": "https://arxiv.org/pdf/2506.02359", "abs": "https://arxiv.org/abs/2506.02359", "authors": ["Brent A. Griffin", "Manushree Gangwar", "Jacob Sela", "Jason J. Corso"], "title": "Auto-Labeling Data for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Great labels make great models. However, traditional labeling approaches for\ntasks like object detection have substantial costs at scale. Furthermore,\nalternatives to fully-supervised object detection either lose functionality or\nrequire larger models with prohibitive computational costs for inference at\nscale. To that end, this paper addresses the problem of training standard\nobject detection models without any ground truth labels. Instead, we configure\npreviously-trained vision-language foundation models to generate\napplication-specific pseudo \"ground truth\" labels. These auto-generated labels\ndirectly integrate with existing model training frameworks, and we subsequently\ntrain lightweight detection models that are computationally efficient. In this\nway, we avoid the costs of traditional labeling, leverage the knowledge of\nvision-language models, and keep the efficiency of lightweight models for\npractical application. We perform exhaustive experiments across multiple\nlabeling configurations, downstream inference models, and datasets to establish\nbest practices and set an extensive auto-labeling benchmark. From our results,\nwe find that our approach is a viable alternative to standard labeling in that\nit maintains competitive performance on multiple datasets and substantially\nreduces labeling time and costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02378", "pdf": "https://arxiv.org/pdf/2506.02378", "abs": "https://arxiv.org/abs/2506.02378", "authors": ["Ukyo Honda", "Tatsushi Oka"], "title": "Exploring Explanations Improves the Robustness of In-Context Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "In-context learning (ICL) has emerged as a successful paradigm for leveraging\nlarge language models (LLMs). However, it often struggles to generalize beyond\nthe distribution of the provided demonstrations. A recent advancement in\nenhancing robustness is ICL with explanations (X-ICL), which improves\nprediction reliability by guiding LLMs to understand and articulate the\nreasoning behind correct labels. Building on this approach, we introduce an\nadvanced framework that extends X-ICL by systematically exploring explanations\nfor all possible labels (X$^2$-ICL), thereby enabling more comprehensive and\nrobust decision-making. Experimental results on multiple natural language\nunderstanding datasets validate the effectiveness of X$^2$-ICL, demonstrating\nsignificantly improved robustness to out-of-distribution data compared to the\nexisting ICL approaches.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02367", "pdf": "https://arxiv.org/pdf/2506.02367", "abs": "https://arxiv.org/abs/2506.02367", "authors": ["Jiayi Su", "Dequan Jin"], "title": "ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery", "categories": ["cs.CV", "68T07", "I.5.1"], "comment": "22 pages, 3 figures", "summary": "Generalized category discovery (GCD) is a highly popular task in open-world\nrecognition, aiming to identify unknown class samples using known class data.\nBy leveraging pre-training, meta-training, and fine-tuning, ViT achieves\nexcellent few-shot learning capabilities. Its MLP head is a feedforward\nnetwork, trained synchronously with the entire network in the same process,\nincreasing the training cost and difficulty without fully leveraging the power\nof the feature extractor. This paper proposes a new architecture by replacing\nthe MLP head with a neural field-based one. We first present a new static\nneural field function to describe the activity distribution of the neural field\nand then use two static neural field functions to build an efficient few-shot\nclassifier. This neural field-based (NF) classifier consists of two coupled\nstatic neural fields. It stores the feature information of support samples by\nits elementary field, the known categories by its high-level field, and the\ncategory information of support samples by its cross-field connections. We\nreplace the MLP head with the proposed NF classifier, resulting in a novel\narchitecture ViTNF, and simplify the three-stage training mode by pre-training\nthe feature extractor on source tasks and training the NF classifier with\nsupport samples in meta-testing separately, significantly reducing ViT's demand\nfor training samples and the difficulty of model training. To enhance the\nmodel's capability in identifying new categories, we provide an effective\nalgorithm to determine the lateral interaction scale of the elementary field.\nExperimental results demonstrate that our model surpasses existing\nstate-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard\nCars, achieving dramatic accuracy improvements of 19\\% and 16\\% in new and all\nclasses, respectively, indicating a notable advantage in GCD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02396", "pdf": "https://arxiv.org/pdf/2506.02396", "abs": "https://arxiv.org/abs/2506.02396", "authors": ["Longyu Yang", "Ping Hu", "Shangbo Yuan", "Lu Zhang", "Jun Liu", "Hengtao Shen", "Xiaofeng Zhu"], "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather", "categories": ["cs.CV"], "comment": null, "summary": "Existing LiDAR semantic segmentation models often suffer from decreased\naccuracy when exposed to adverse weather conditions. Recent methods addressing\nthis issue focus on enhancing training data through weather simulation or\nuniversal augmentation techniques. However, few works have studied the negative\nimpacts caused by the heterogeneous domain shifts in the geometric structure\nand reflectance intensity of point clouds. In this paper, we delve into this\nchallenge and address it with a novel Geometry-Reflectance Collaboration (GRC)\nframework that explicitly separates feature extraction for geometry and\nreflectance. Specifically, GRC employs a dual-branch architecture designed to\nindependently process geometric and reflectance features initially, thereby\ncapitalizing on their distinct characteristic. Then, GRC adopts a robust\nmulti-level feature collaboration module to suppress redundant and unreliable\ninformation from both branches. Consequently, without complex simulation or\naugmentation, our method effectively extracts intrinsic information about the\nscene while suppressing interference, thus achieving better robustness and\ngeneralization in adverse weather conditions. We demonstrate the effectiveness\nof GRC through comprehensive experiments on challenging benchmarks, showing\nthat our method outperforms previous approaches and establishes new\nstate-of-the-art results.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02405", "pdf": "https://arxiv.org/pdf/2506.02405", "abs": "https://arxiv.org/abs/2506.02405", "authors": ["Zhiya Tan", "Xin Zhang", "Joey Tianyi Zhou"], "title": "Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "As generative techniques become increasingly accessible, authentic visuals\nare frequently subjected to iterative alterations by various individuals\nemploying a variety of tools. Currently, to avoid misinformation and ensure\naccountability, a lot of research on detection and attribution is emerging.\nAlthough these methods demonstrate promise in single-stage manipulation\nscenarios, they fall short when addressing complex real-world iterative\nmanipulation. In this paper, we are the first, to the best of our knowledge, to\nsystematically model this real-world challenge and introduce a novel method to\nsolve it. We define a task called \"Modelship Attribution\", which aims to trace\nthe evolution of manipulated images by identifying the generative models\ninvolved and reconstructing the sequence of edits they performed. To\nrealistically simulate this scenario, we utilize three generative models,\nStyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct\nregions of the same image. This process leads to the creation of the first\nmodelship dataset, comprising 83,700 images (16,740 images*5). Given that later\nedits often overwrite the fingerprints of earlier models, the focus shifts from\nextracting blended fingerprints to characterizing each model's distinctive\nediting patterns. To tackle this challenge, we introduce the modelship\nattribution transformer (MAT), a purpose-built framework designed to\neffectively recognize and attribute the contributions of various models within\ncomplex, multi-stage manipulation workflows. Through extensive experiments and\ncomparative analysis with other related methods, our results, including\ncomprehensive ablation studies, demonstrate that the proposed approach is a\nhighly effective solution for modelship attribution.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02408", "pdf": "https://arxiv.org/pdf/2506.02408", "abs": "https://arxiv.org/abs/2506.02408", "authors": ["Wenhao Tang", "Rong Qin", "Heng Fang", "Fengtao Zhou", "Hao Chen", "Xiang Li", "Ming-Ming Cheng"], "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained encoders for offline feature extraction followed by multiple\ninstance learning (MIL) aggregators have become the dominant paradigm in\ncomputational pathology (CPath), benefiting cancer diagnosis and prognosis.\nHowever, performance limitations arise from the absence of encoder fine-tuning\nfor downstream tasks and disjoint optimization with MIL. While slide-level\nsupervised end-to-end (E2E) learning is an intuitive solution to this issue, it\nfaces challenges such as high computational demands and suboptimal results.\nThese limitations motivate us to revisit E2E learning. We argue that prior work\nneglects inherent E2E optimization challenges, leading to performance\ndisparities compared to traditional two-stage methods. In this paper, we\npioneer the elucidation of optimization challenge caused by sparse-attention\nMIL and propose a novel MIL called ABMILX. It mitigates this problem through\nglobal correlation-based attention refinement and multi-head mechanisms. With\nthe efficient multi-scale random patch sampling strategy, an E2E trained ResNet\nwith ABMILX surpasses SOTA foundation models under the two-stage paradigm\nacross multiple challenging benchmarks, while remaining computationally\nefficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath\nand calls for greater research focus in this area. The code is\nhttps://github.com/DearCaat/E2E-WSI-ABMILX.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02454", "pdf": "https://arxiv.org/pdf/2506.02454", "abs": "https://arxiv.org/abs/2506.02454", "authors": ["Zhaorui Yang", "Bo Pan", "Han Wang", "Yiyao Wang", "Xingyu Liu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework", "categories": ["cs.CL", "cs.AI"], "comment": "47 pages", "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02444", "pdf": "https://arxiv.org/pdf/2506.02444", "abs": "https://arxiv.org/abs/2506.02444", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at\n\\href{https://github.com/Droliven}{https://github.com/Droliven}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02461", "pdf": "https://arxiv.org/pdf/2506.02461", "abs": "https://arxiv.org/abs/2506.02461", "authors": ["Chunkit Chan", "Yauwai Yim", "Hongchuan Zeng", "Zhiying Zou", "Xinyuan Cheng", "Zhifan Sun", "Zheye Deng", "Kawai Chung", "Yuzhuo Ao", "Yixiang Fan", "Cheng Jiayang", "Ercong Nie", "Ginny Y. Wong", "Helmut Schmid", "Hinrich Schütze", "Simon See", "Yangqiu Song"], "title": "XToM: Exploring the Multilingual Theory of Mind for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Theory of Mind (ToM), the ability to infer mental states in others, is\npivotal for human social cognition. Existing evaluations of ToM in LLMs are\nlargely limited to English, neglecting the linguistic diversity that shapes\nhuman cognition. This limitation raises a critical question: can LLMs exhibit\nMultilingual Theory of Mind, which is the capacity to reason about mental\nstates across diverse linguistic contexts? To address this gap, we present\nXToM, a rigorously validated multilingual benchmark that evaluates ToM across\nfive languages and incorporates diverse, contextually rich task scenarios.\nUsing XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a\npronounced dissonance: while models excel in multilingual language\nunderstanding, their ToM performance varies across languages. Our findings\nexpose limitations in LLMs' ability to replicate human-like mentalizing across\nlinguistic contexts.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02472", "pdf": "https://arxiv.org/pdf/2506.02472", "abs": "https://arxiv.org/abs/2506.02472", "authors": ["Halil Ismail Helvaci", "Justin Philip Huber", "Jihye Bae", "Sen-ching Samson Cheung"], "title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation", "categories": ["cs.CV"], "comment": null, "summary": "Stroke rehabilitation often demands precise tracking of patient movements to\nmonitor progress, with complexities of rehabilitation exercises presenting two\ncritical challenges: fine-grained and sub-second (under one-second) action\ndetection. In this work, we propose the High Resolution Temporal Transformer\n(HRTR), to time-localize and classify high-resolution (fine-grained),\nsub-second actions in a single-stage transformer, eliminating the need for\nmulti-stage methods and post-processing. Without any refinements, HRTR\noutperforms state-of-the-art systems on both stroke related and general\ndatasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on\nStrokeRehab IMU, and 88.4 on 50Salads.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02503", "pdf": "https://arxiv.org/pdf/2506.02503", "abs": "https://arxiv.org/abs/2506.02503", "authors": ["Yongjian Li", "HaoCheng Chu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zheni Zeng", "Ruobing Wang", "Sen Song", "Zhiyuan Liu", "Maosong Sun"], "title": "KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess broader knowledge sources, yet factual inconsistencies persist due to\nnoise in retrieved documents-even with advanced retrieval methods. We\ndemonstrate that enhancing generative models' capacity to process noisy content\nis equally critical for robust performance. In this paper, we present KARE-RAG\n(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge\nutilization through three key innovations: (1) structured knowledge\nrepresentations that facilitate error detection during training, (2) Dense\nDirect Preference Optimization (DDPO)-a refined training objective that\nprioritizes correction of critical errors, and (3) a contrastive data\ngeneration pipeline that maintains semantic consistency while rectifying\nfactual inaccuracies. Experiments show our method significantly enhances\nstandard RAG pipelines across model scales, improving both in-domain and\nout-of-domain task performance without compromising general capabilities.\nNotably, these gains are achieved with modest training data, suggesting\ndata-efficient optimization is possible through targeted learning strategies.\nOur findings establish a new direction for RAG improvement: by improving how\nmodels learn to process retrieved content, we can enhance performance across\ndiverse inference paradigms. All data and code will be publicly available on\nGithub.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02488", "pdf": "https://arxiv.org/pdf/2506.02488", "abs": "https://arxiv.org/abs/2506.02488", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02528", "pdf": "https://arxiv.org/pdf/2506.02528", "abs": "https://arxiv.org/abs/2506.02528", "authors": ["Yan Gong", "Yiren Song", "Yicheng Li", "Chenglin Li", "Yin Zhang"], "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02544", "pdf": "https://arxiv.org/pdf/2506.02544", "abs": "https://arxiv.org/abs/2506.02544", "authors": ["Yang Tian", "Fan Liu", "Jingyuan Zhang", "Victoria W.", "Yupeng Hu", "Liqiang Nie"], "title": "CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main", "summary": "Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to\nenhance Multimodal Large Language Models by incorporating externally retrieved\nmultimodal knowledge, but it introduces two challenges: Parametric-Retrieved\nKnowledge Inconsistency (PRKI), where discrepancies between parametric and\nretrieved knowledge create uncertainty in determining reliability, and\nVisual-Textual Knowledge Inconsistency (VTKI), where misalignment between\nvisual and textual sources disrupts entity representation. To address these\nchallenges, we propose \\textbf{C}r\\textbf{o}ss-source knowledge\n\\textbf{Re}conciliation for \\textbf{M}ulti\\textbf{M}odal \\textbf{RAG}\n(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles\ninconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage\npipeline: it first generates an internal response from parametric knowledge,\nthen selects the most relevant multimodal evidence via joint similarity\nassessment, generates an external response, and finally integrates both to\nproduce a reliable answer. Additionally, a specialized training paradigm\nenhances knowledge source discrimination, multimodal integration, and unified\nanswer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG\nachieves substantial improvements over baseline methods, achieving 5.6\\% and\n9.3\\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We\nrelease code and data at\n\\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02561", "pdf": "https://arxiv.org/pdf/2506.02561", "abs": "https://arxiv.org/abs/2506.02561", "authors": ["Yirao Zhao", "Guizhen Chen", "Kenji Kawaguchi", "Lidong Bing", "Wenxuan Zhang"], "title": "Pruning General Large Language Models into Customized Expert Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their substantial model sizes often require substantial computational\nresources. To preserve computing resources and accelerate inference speed, it\nis crucial to prune redundant parameters, especially for experienced users who\noften need compact expert models tailored to specific downstream scenarios.\nHowever, most existing pruning methods focus on preserving the model's general\ncapabilities, often requiring extensive post-training or suffering from\ndegraded performance due to coarse-grained pruning. In this work, we design a\n$\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to\nprune a large general model into a smaller lightweight expert model, which is\npositioned along the \"language\", \"domain\" and \"task\" dimensions. By identifying\nand pruning irrelevant neurons of each dimension, $\\texttt{Cus-Prun}$ creates\nexpert models without any post-training. Our experiments demonstrate that\n$\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal\nloss in both expert and general capabilities across various models from\ndifferent model families and sizes.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02550", "pdf": "https://arxiv.org/pdf/2506.02550", "abs": "https://arxiv.org/abs/2506.02550", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02591", "pdf": "https://arxiv.org/pdf/2506.02591", "abs": "https://arxiv.org/abs/2506.02591", "authors": ["Minh Duc Bui", "Kyung Eun Park", "Goran Glavaš", "Fabian David Schmidt", "Katharina von der Wense"], "title": "On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main (Camera-Ready Version)", "summary": "Measurement systems (e.g., currencies) differ across cultures, but the\nconversions between them are well defined so that humans can state facts using\nany measurement system of their choice. Being available to users from diverse\ncultural backgrounds, large language models (LLMs) should also be able to\nprovide accurate information irrespective of the measurement system at hand.\nUsing newly compiled datasets we test if this is the case for seven open-source\nLLMs, addressing three key research questions: (RQ1) What is the default system\nused by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their\naccuracy vary across different measurement systems? (RQ3) Can LLMs mitigate\npotential challenges w.r.t. underrepresented systems via reasoning? Our\nfindings show that LLMs default to the measurement system predominantly used in\nthe data. Additionally, we observe considerable instability and variance in\nperformance across different measurement systems. While this instability can in\npart be mitigated by employing reasoning methods such as chain-of-thought\n(CoT), this implies longer responses and thereby significantly increases\ntest-time compute (and inference costs), marginalizing users from cultural\nbackgrounds that use underrepresented measurement systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02557", "pdf": "https://arxiv.org/pdf/2506.02557", "abs": "https://arxiv.org/abs/2506.02557", "authors": ["Shizhan Gong", "Yankai Jiang", "Qi Dou", "Farzan Farnia"], "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Vision-language models, such as CLIP, have achieved significant success in\naligning visual and textual representations, becoming essential components of\nmany multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.\nHowever, numerous studies have identified CLIP's limited fine-grained\nperception as a critical drawback, leading to substantial failures in\ndownstream MLLMs. In contrast, vision-centric foundation models like DINOv2\ndemonstrate remarkable capabilities in capturing fine details from images. In\nthis work, we propose a novel kernel-based method to align CLIP's visual\nrepresentation with that of DINOv2, ensuring that the resulting embeddings\nmaintain compatibility with text embeddings while enhancing perceptual\ncapabilities. Our alignment objective is designed for efficient stochastic\noptimization. Following this image-only alignment fine-tuning, the visual\nencoder retains compatibility with the frozen text encoder and exhibits\nsignificant improvements in zero-shot object recognition, fine-grained spatial\nreasoning, and localization. By integrating the aligned visual encoder,\ndownstream MLLMs also demonstrate enhanced performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02627", "pdf": "https://arxiv.org/pdf/2506.02627", "abs": "https://arxiv.org/abs/2506.02627", "authors": ["Ömer Tarik Özyilmaz", "Matt Coler", "Matias Valdenegro-Toro"], "title": "Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Although commercial Arabic automatic speech recognition (ASR) systems support\nModern Standard Arabic (MSA), they struggle with dialectal speech. We\ninvestigate the effect of fine-tuning OpenAI's Whisper on five major Arabic\ndialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common\nVoice for MSA and the MASC dataset for dialectal speech. We evaluate MSA\ntraining size effects, benefits of pre-training on MSA data, and\ndialect-specific versus dialect-pooled models. We find that small amounts of\nMSA fine-tuning data yield substantial improvements for smaller models,\nmatching larger non-fine-tuned models. While MSA pre-training shows minimal\nbenefit, suggesting limited shared features between MSA and dialects, our\ndialect-pooled models perform comparably to dialect-specific ones. This\nindicates that pooling dialectal data, when properly balanced, can help address\ndata scarcity in low-resource ASR without significant performance loss.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02587", "pdf": "https://arxiv.org/pdf/2506.02587", "abs": "https://arxiv.org/abs/2506.02587", "authors": ["Weiduo Yuan", "Jerry Li", "Justin Yue", "Divyank Shah", "Konstantinos Karydis", "Hang Qiu"], "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02659", "pdf": "https://arxiv.org/pdf/2506.02659", "abs": "https://arxiv.org/abs/2506.02659", "authors": ["Manon Reusens", "Bart Baesens", "David Jurgens"], "title": "Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Personalized Large Language Models (LLMs) are increasingly used in diverse\napplications, where they are assigned a specific persona - such as a happy high\nschool teacher - to guide their responses. While prior research has examined\nhow well LLMs adhere to predefined personas in writing style, a comprehensive\nanalysis of consistency across different personas and task types is lacking. In\nthis paper, we introduce a new standardized framework to analyze consistency in\npersona-assigned LLMs. We define consistency as the extent to which a model\nmaintains coherent responses when assigned the same persona across different\ntasks and runs. Our framework evaluates personas across four different\ncategories (happiness, occupation, personality, and political stance) spanning\nmultiple task dimensions (survey writing, essay generation, social media post\ngeneration, single turn, and multi-turn conversations). Our findings reveal\nthat consistency is influenced by multiple factors, including the assigned\npersona, stereotypes, and model design choices. Consistency also varies across\ntasks, increasing with more structured tasks and additional context. All code\nis available on GitHub.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02678", "pdf": "https://arxiv.org/pdf/2506.02678", "abs": "https://arxiv.org/abs/2506.02678", "authors": ["Zhong-Zhi Li", "Xiao Liang", "Zihao Tang", "Lei Ji", "Peijie Wang", "Haotian Xu", "Xing W", "Haizhen Huang", "Weiwei Deng", "Ying Nian Wu", "Yeyun Gong", "Zhijiang Guo", "Xiao Liu", "Fei Yin", "Cheng-Lin Liu"], "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression", "categories": ["cs.CL", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02605", "pdf": "https://arxiv.org/pdf/2506.02605", "abs": "https://arxiv.org/abs/2506.02605", "authors": ["Xue Wu", "Jingwei Xin", "Zhijun Tu", "Jie Hu", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based models have been widely used in various visual generation\ntasks, showing promising results in image super-resolution (SR), while\ntypically being limited by dozens or even hundreds of sampling steps. Although\nexisting methods aim to accelerate the inference speed of multi-step\ndiffusion-based SR methods through knowledge distillation, their generated\nimages exhibit insufficient semantic alignment with real images, resulting in\nsuboptimal perceptual quality reconstruction, specifically reflected in the\nCLIPIQA score. These methods still have many challenges in perceptual quality\nand semantic fidelity. Based on the challenges, we propose VPD-SR, a novel\nvisual perception diffusion distillation framework specifically designed for\nSR, aiming to construct an effective and efficient one-step SR model.\nSpecifically, VPD-SR consists of two components: Explicit Semantic-aware\nSupervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS\nleverages the powerful visual perceptual understanding capabilities of the CLIP\nmodel to extract explicit semantic supervision, thereby enhancing semantic\nconsistency. Then, Considering that high-frequency information contributes to\nthe visual perception quality of images, in addition to the vanilla\ndistillation loss, the HFP loss guides the student model to restore the missing\nhigh-frequency details in degraded images that are critical for enhancing\nperceptual quality. Lastly, we expand VPD-SR in adversarial training manner to\nfurther enhance the authenticity of the generated content. Extensive\nexperiments conducted on synthetic and real-world datasets demonstrate that the\nproposed VPD-SR achieves superior performance compared to both previous\nstate-of-the-art methods and the teacher model with just one-step sampling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02689", "pdf": "https://arxiv.org/pdf/2506.02689", "abs": "https://arxiv.org/abs/2506.02689", "authors": ["Liang Yue", "Yihong Tang", "Kehai Chen", "Jie Liu", "Min Zhang"], "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching", "categories": ["cs.CL"], "comment": null, "summary": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'\ninstruction-following capabilities and task-specific performance. However,\nobtaining high-quality fine-tuning data for large models is challenging due to\ndata collection difficulties and high production costs. To address this, we\npropose MASTER, a novel data augmentation method that enriches original data\nthrough interactions among multiple agents with varying cognitive levels. We\nsimulate three pedagogically grounded teaching scenarios, leveraging\nmulti-agent conversations to generate high-quality teacher-student interaction\ndata. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented\nfrom existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.\nExperiments show that models fine-tuned with BOOST-QA perform excellently\nacross multiple benchmarks, demonstrating strong multitask generalization.\nNotably, MASTER significantly improves models' reasoning abilities in complex\ntasks, providing valuable insights for future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02615", "pdf": "https://arxiv.org/pdf/2506.02615", "abs": "https://arxiv.org/abs/2506.02615", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02633", "pdf": "https://arxiv.org/pdf/2506.02633", "abs": "https://arxiv.org/abs/2506.02633", "authors": ["Cheng Yang", "Lijing Liang", "Zhixun Su"], "title": "ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes ControlMambaIR, a novel image restoration method designed\nto address perceptual challenges in image deraining, deblurring, and denoising\ntasks. By integrating the Mamba network architecture with the diffusion model,\nthe condition network achieves refined conditional control, thereby enhancing\nthe control and optimization of the image generation process. To evaluate the\nrobustness and generalization capability of our method across various image\ndegradation conditions, extensive experiments were conducted on several\nbenchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results\ndemonstrate that our proposed approach consistently surpasses existing methods\nin perceptual quality metrics, such as LPIPS and FID, while maintaining\ncomparable performance in image distortion metrics, including PSNR and SSIM,\nhighlighting its effectiveness and adaptability. Notably, ablation experiments\nreveal that directly noise prediction in the diffusion process achieves better\nperformance, effectively balancing noise suppression and detail preservation.\nFurthermore, the findings indicate that the Mamba architecture is particularly\nwell-suited as a conditional control network for diffusion models,\noutperforming both CNN- and Attention-based approaches in this context.\nOverall, these results highlight the flexibility and effectiveness of\nControlMambaIR in addressing a range of image restoration perceptual\nchallenges.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02695", "pdf": "https://arxiv.org/pdf/2506.02695", "abs": "https://arxiv.org/abs/2506.02695", "authors": ["Linquan Wu", "Tianxiang Jiang", "Wenhao Duan", "Yini Fang", "Jacky Keung"], "title": "FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Micro-expression recognition (MER) demands models that can amplify\nmillisecond-level, low-amplitude facial motions while suppressing\nidentity-specific appearance. We introduce FaceSleuth, a dual-stream\narchitecture that (1) enhances motion along the empirically dominant vertical\naxix through a Continuously Vertical Attention (CVA) block, (2) localises the\nresulting signals with a Facial Position Focalizer built on hierarchical\ncross-window attention, and (3) steers feature learning toward physiologically\nmeaningful regions via lightweight Action-Unit embeddings. To examine whether\nthe hand-chosen vertical axis is indeed optimal, we further propose a\nSingle-Orientation Attention (SOA) module that learns its own pooling direction\nend-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses\nto CVA when the learned angle converges to {\\Pi}/2. In practice, SOA reliably\ndrifts to 88{\\deg}, confirming the effectiveness of the vertical prior while\ndelivering consistent gains. On three standard MER benchmarks, FaceSleuth with\nCVA already surpasses previous state-of-the-art methods; plugging in SOA lifts\naccuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840\non SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.\nThese results establish a new state of the art and, for the first time, provide\nempirical evidence that the vertical attention bias is the most discriminative\norientation for MER.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02872", "pdf": "https://arxiv.org/pdf/2506.02872", "abs": "https://arxiv.org/abs/2506.02872", "authors": ["Ludovic Moncla", "Hédi Zeghidi"], "title": "Token and Span Classification for Entity Recognition in French Historical Encyclopedias", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Named Entity Recognition (NER) in historical texts presents unique challenges\ndue to non-standardized language, archaic orthography, and nested or\noverlapping entities. This study benchmarks a diverse set of NER approaches,\nranging from classical Conditional Random Fields (CRFs) and spaCy-based models\nto transformer-based architectures such as CamemBERT and sequence-labeling\nmodels like Flair. Experiments are conducted on the GeoEDdA dataset, a richly\nannotated corpus derived from 18th-century French encyclopedias. We propose\nframing NER as both token-level and span-level classification to accommodate\ncomplex nested entity structures typical of historical documents. Additionally,\nwe evaluate the emerging potential of few-shot prompting with generative\nlanguage models for low-resource scenarios. Our results demonstrate that while\ntransformer-based models achieve state-of-the-art performance, especially on\nnested entities, generative models offer promising alternatives when labeled\ndata are scarce. The study highlights ongoing challenges in historical NER and\nsuggests avenues for hybrid approaches combining symbolic and neural methods to\nbetter capture the intricacies of early modern French text.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02698", "pdf": "https://arxiv.org/pdf/2506.02698", "abs": "https://arxiv.org/abs/2506.02698", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xiaoyin Xu", "Min Zhang"], "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation\nmodels with human preferences using pairwise preference data. Although\nsubstantial resources are expended in collecting and labeling datasets, a\ncritical aspect is often neglected: \\textit{preferences vary across individuals\nand should be represented with more granularity.} To address this, we propose\nSmPO-Diffusion, a novel method for modeling preference distributions to improve\nthe DPO objective, along with a numerical upper bound estimation for the\ndiffusion optimization objective. First, we introduce a smoothed preference\ndistribution to replace the original binary distribution. We employ a reward\nmodel to simulate human preferences and apply preference likelihood averaging\nto improve the DPO loss, such that the loss function approaches zero when\npreferences are similar. Furthermore, we utilize an inversion technique to\nsimulate the trajectory preference distribution of the diffusion model,\nenabling more accurate alignment with the optimization objective. Our approach\neffectively mitigates issues of excessive optimization and objective\nmisalignment present in existing methods through straightforward modifications.\nOur SmPO-Diffusion achieves state-of-the-art performance in preference\nevaluation, outperforming baselines across metrics with lower training costs.\nThe project page is https://jaydenlyh.github.io/SmPO-project-page/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02702", "pdf": "https://arxiv.org/pdf/2506.02702", "abs": "https://arxiv.org/abs/2506.02702", "authors": ["Tibor Kubík", "François Guibault", "Michal Španěl", "Hervé Lombaert"], "title": "ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings", "categories": ["cs.CV"], "comment": "Information Processing in Medical Imaging (IPMI2025)", "summary": "We introduce ToothForge, a spectral approach for automatically generating\nnovel 3D teeth, effectively addressing the sparsity of dental shape datasets.\nBy operating in the spectral domain, our method enables compact machine\nlearning modeling, allowing the generation of high-resolution tooth meshes in\nmilliseconds. However, generating shape spectra comes with the instability of\nthe decomposed harmonics. To address this, we propose modeling the latent\nmanifold on synchronized frequential embeddings. Spectra of all data samples\nare aligned to a common basis prior to the training procedure, effectively\neliminating biases introduced by the decomposition instability. Furthermore,\nsynchronized modeling removes the limiting factor imposed by previous methods,\nwhich require all shapes to share a common fixed connectivity. Using a private\ndataset of real dental crowns, we observe a greater reconstruction quality of\nthe synthetized shapes, exceeding those of models trained on unaligned\nembeddings. We also explore additional applications of spectral analysis in\ndigital dentistry, such as shape compression and interpolation. ToothForge\nfacilitates a range of approaches at the intersection of spectral analysis and\nmachine learning, with fewer restrictions on mesh structure. This makes it\napplicable for shape analysis not only in dentistry, but also in broader\nmedical applications, where guaranteeing consistent connectivity across shapes\nfrom various clinics is unrealistic. The code is available at\nhttps://github.com/tiborkubik/toothForge.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02736", "pdf": "https://arxiv.org/pdf/2506.02736", "abs": "https://arxiv.org/abs/2506.02736", "authors": ["Shufan Qing", "Anzhen Li", "Qiandi Wang", "Yuefeng Niu", "Mingchen Feng", "Guoliang Hu", "Jinqiao Wu", "Fengtao Nan", "Yingchun Fan"], "title": "GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Existing semantic SLAM in dynamic environments mainly identify dynamic\nregions through object detection or semantic segmentation methods. However, in\ncertain highly dynamic scenarios, the detection boxes or segmentation masks\ncannot fully cover dynamic regions. Therefore, this paper proposes a robust and\nefficient GeneA-SLAM2 system that leverages depth variance constraints to\nhandle dynamic scenes. Our method extracts dynamic pixels via depth variance\nand creates precise depth masks to guide the removal of dynamic objects.\nSimultaneously, an autoencoder is used to reconstruct keypoints, improving the\ngenetic resampling keypoint algorithm to obtain more uniformly distributed\nkeypoints and enhance the accuracy of pose estimation. Our system was evaluated\non multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2\nmaintains high accuracy in dynamic scenes compared to current methods. Code is\navailable at: https://github.com/qingshufan/GeneA-SLAM2.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02924", "pdf": "https://arxiv.org/pdf/2506.02924", "abs": "https://arxiv.org/abs/2506.02924", "authors": ["Diogo A. P. Nunes", "Eugénio Ribeiro"], "title": "INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; J.3; H.3.3"], "comment": "12 pages, 1 figure, 6 tables", "summary": "In this work, we describe our team's approach to eRisk's 2025 Task 1: Search\nfor Symptoms of Depression. Given a set of sentences and the Beck's Depression\nInventory - II (BDI) questionnaire, participants were tasked with submitting up\nto 1,000 sentences per depression symptom in the BDI, sorted by relevance.\nParticipant submissions were evaluated according to standard Information\nRetrieval (IR) metrics, including Average Precision (AP) and R-Precision\n(R-PREC). The provided training data, however, consisted of sentences labeled\nas to whether a given sentence was relevant or not w.r.t. one of BDI's\nsymptoms. Due to this labeling limitation, we framed our development as a\nbinary classification task for each BDI symptom, and evaluated accordingly. To\nthat end, we split the available labeled data into training and validation\nsets, and explored foundation model fine-tuning, sentence similarity, Large\nLanguage Model (LLM) prompting, and ensemble techniques. The validation results\nrevealed that fine-tuning foundation models yielded the best performance,\nparticularly when enhanced with synthetic data to mitigate class imbalance. We\nalso observed that the optimal approach varied by symptom. Based on these\ninsights, we devised five independent test runs, two of which used ensemble\nmethods. These runs achieved the highest scores in the official IR evaluation,\noutperforming submissions from 16 other teams.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02738", "pdf": "https://arxiv.org/pdf/2506.02738", "abs": "https://arxiv.org/abs/2506.02738", "authors": ["Negin Baghbanzadeh", "Sajad Ashkezari", "Elham Dolatabadi", "Arash Afkanpour"], "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02945", "pdf": "https://arxiv.org/pdf/2506.02945", "abs": "https://arxiv.org/abs/2506.02945", "authors": ["Aishwarya Sahoo", "Jeevana Kruthi Karnuthala", "Tushar Parmanand Budhwani", "Pranchal Agarwal", "Sankaran Vaidyanathan", "Alexa Siu", "Franck Dernoncourt", "Jennifer Healey", "Nedim Lipka", "Ryan Rossi", "Uttaran Bhattacharya", "Branislav Kveton"], "title": "Quantitative LLM Judges", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02951", "pdf": "https://arxiv.org/pdf/2506.02951", "abs": "https://arxiv.org/abs/2506.02951", "authors": ["Boyi Li", "Zhonghan Zhao", "Der-Horng Lee", "Gaoang Wang"], "title": "Adaptive Graph Pruning for Multi-Agent Communication", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02751", "pdf": "https://arxiv.org/pdf/2506.02751", "abs": "https://arxiv.org/abs/2506.02751", "authors": ["Chuanyu Fu", "Yuqi Zhang", "Kunbin Yao", "Guanying Chen", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Xiaochun Cao"], "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS", "categories": ["cs.CV"], "comment": "Project page: https://fcyycf.github.io/RobustSplat/", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02973", "pdf": "https://arxiv.org/pdf/2506.02973", "abs": "https://arxiv.org/abs/2506.02973", "authors": ["Dingwei Chen", "Ziqiang Liu", "Feiteng Fang", "Chak Tou Leong", "Shiwen Ni", "Ahmadreza Argha", "Hamid Alinejad-Rokny", "Min Yang", "Chengming Li"], "title": "Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in text\nunderstanding and generation. However, their tendency to produce factually\ninconsistent outputs, commonly referred to as ''hallucinations'', remains a\ncritical challenge. Existing approaches, such as retrieval-based and\ninference-time correction methods, primarily address this issue at the input or\noutput level, often overlooking the intrinsic information refinement process\nand the role of premature layers. Meanwhile, alignment- and fine-tuning-based\nmethods are resource-intensive. In this paper, we propose PLI (Premature Layers\nInterpolation), a novel, training-free, and plug-and-play intervention designed\nto enhance factuality. PLI mitigates hallucinations by inserting premature\nlayers formed through mathematical interpolation with adjacent layers. Inspired\nby stable diffusion and sampling steps, PLI extends the depth of information\nprocessing and transmission in LLMs, improving factual coherence. Experiments\non four publicly available datasets demonstrate that PLI effectively reduces\nhallucinations while outperforming existing baselines in most cases. Further\nanalysis suggests that the success of layer interpolation is closely linked to\nLLMs' internal mechanisms. To promote reproducibility, we will release our code\nand data upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02781", "pdf": "https://arxiv.org/pdf/2506.02781", "abs": "https://arxiv.org/abs/2506.02781", "authors": ["Tongyuan Bai", "Wangyuanfan Bai", "Dong Chen", "Tieru Wu", "Manyi Li", "Rui Ma"], "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Controllability plays a crucial role in the practical applications of 3D\nindoor scene synthesis. Existing works either allow rough language-based\ncontrol, that is convenient but lacks fine-grained scene customization, or\nemploy graph based control, which offers better controllability but demands\nconsiderable knowledge for the cumbersome graph design process. To address\nthese challenges, we present FreeScene, a user-friendly framework that enables\nboth convenient and effective control for indoor scene synthesis.Specifically,\nFreeScene supports free-form user inputs including text description and/or\nreference images, allowing users to express versatile design intentions. The\nuser inputs are adequately analyzed and integrated into a graph representation\nby a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion\nTransformer, which performs graph-aware denoising to enhance scene generation.\nOur MG-DiT not only excels at preserving graph structure but also offers broad\napplicability to various tasks, including, but not limited to, text-to-scene,\ngraph-to-scene, and rearrangement, all within a single model. Extensive\nexperiments demonstrate that FreeScene provides an efficient and user-friendly\nsolution that unifies text-based and graph based scene synthesis, outperforming\nstate-of-the-art methods in terms of both generation quality and\ncontrollability in a range of applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02783", "pdf": "https://arxiv.org/pdf/2506.02783", "abs": "https://arxiv.org/abs/2506.02783", "authors": ["Carlos Garcia-Lopez-de-Haro", "Caterina Fuster-Barcelo", "Curtis T. Rueden", "Jonathan Heras", "Vladimir Ulman", "Daniel Franco-Barranco", "Adrian Ines", "Kevin W. Eliceiri", "Jean-Christophe Olivo-Marin", "Jean-Yves Tinevez", "Daniel Sage", "Arrate Munoz-Barrutia"], "title": "SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "Mask annotation remains a significant bottleneck in AI-driven biomedical\nimage analysis due to its labor-intensive nature. To address this challenge, we\nintroduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment\nAnything Model (SAM). SAMJ enables seamless, interactive annotations with\none-click installation on standard computers. Designed for real-time object\ndelineation in large scientific images, SAMJ is an easy-to-use solution that\nsimplifies and accelerates the creation of labeled image datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02843", "pdf": "https://arxiv.org/pdf/2506.02843", "abs": "https://arxiv.org/abs/2506.02843", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Random Registers for Cross-Domain Few-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a\ndata-sufficient source domain to data-scarce target domains. Although Vision\nTransformer (ViT) has shown superior capability in many vision tasks, its\ntransferability against huge domain gaps in CDFSL is still under-explored. In\nthis paper, we find an intriguing phenomenon: during the source-domain\ntraining, prompt tuning, as a common way to train ViT, could be harmful for the\ngeneralization of ViT in target domains, but setting them to random noises\n(i.e., random registers) could consistently improve target-domain performance.\nWe then delve into this phenomenon for an interpretation. We find that\nlearnable prompts capture domain information during the training on the source\ndataset, which views irrelevant visual patterns as vital cues for recognition.\nThis can be viewed as a kind of overfitting and increases the sharpness of the\nloss landscapes. In contrast, random registers are essentially a novel way of\nperturbing attention for the sharpness-aware minimization, which helps the\nmodel find a flattened minimum in loss landscapes, increasing the\ntransferability. Based on this phenomenon and interpretation, we further\npropose a simple but effective approach for CDFSL to enhance the perturbation\non attention maps by adding random registers on the semantic regions of image\ntokens, improving the effectiveness and efficiency of random registers.\nExtensive experiments on four benchmarks validate our rationale and\nstate-of-the-art performance. Codes and models are available at\nhttps://github.com/shuaiyi308/REAP.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03011", "pdf": "https://arxiv.org/pdf/2506.03011", "abs": "https://arxiv.org/abs/2506.03011", "authors": ["Aditya Bharat Soni", "Boxuan Li", "Xingyao Wang", "Valerie Chen", "Graham Neubig"], "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers", "categories": ["cs.CL"], "comment": null, "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02850", "pdf": "https://arxiv.org/pdf/2506.02850", "abs": "https://arxiv.org/abs/2506.02850", "authors": ["Mengyue Wang", "Shuo Chen", "Kristian Kersting", "Volker Tresp", "Yunpu Ma"], "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03035", "pdf": "https://arxiv.org/pdf/2506.03035", "abs": "https://arxiv.org/abs/2506.03035", "authors": ["Pierre Lepagnol", "Sahar Ghannay", "Thomas Gerald", "Christophe Servan", "Sophie Rosset"], "title": "Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Conference paper accepted to INTERSPEECH 2025", "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02853", "pdf": "https://arxiv.org/pdf/2506.02853", "abs": "https://arxiv.org/abs/2506.02853", "authors": ["Mingjie Wei", "Xuemei Xie", "Yutong Zhong", "Guangming Shi"], "title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Action coordination in human structure is indispensable for the spatial\nconstraints of 2D joints to recover 3D pose. Usually, action coordination is\nrepresented as a long-range dependence among body parts. However, there are two\nmain challenges in modeling long-range dependencies. First, joints should not\nonly be constrained by other individual joints but also be modulated by the\nbody parts. Second, existing methods make networks deeper to learn dependencies\nbetween non-linked parts. They introduce uncorrelated noise and increase the\nmodel size. In this paper, we utilize a pyramid structure to better learn\npotential long-range dependencies. It can capture the correlation across joints\nand groups, which complements the context of the human sub-structure. In an\neffective cross-scale way, it captures the pyramid-structured long-range\ndependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)\nmodule to capture long-range cross-scale dependencies. It concatenates\ninformation from various scales into a compact sequence, and then computes the\ncorrelation between scales in parallel. Combining PGA with graph convolution\nmodules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose\nestimation, which is a lightweight multi-scale transformer architecture. It\nencapsulates human sub-structures into self-attention by pooling. Extensive\nexperiments show that our approach achieves lower error and smaller model size\nthan state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code\nis available at https://github.com/MingjieWe/PGFormer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03038", "pdf": "https://arxiv.org/pdf/2506.03038", "abs": "https://arxiv.org/abs/2506.03038", "authors": ["Jintian Shao", "Yiming Cheng"], "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03051", "pdf": "https://arxiv.org/pdf/2506.03051", "abs": "https://arxiv.org/abs/2506.03051", "authors": ["Yuval Kansal", "Shmuel Berman", "Lydia Liu"], "title": "Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02857", "pdf": "https://arxiv.org/pdf/2506.02857", "abs": "https://arxiv.org/abs/2506.02857", "authors": ["Luca Maiano", "Fabrizio Casadei", "Irene Amerini"], "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03101", "pdf": "https://arxiv.org/pdf/2506.03101", "abs": "https://arxiv.org/abs/2506.03101", "authors": ["Jonas F. Lotz", "António V. Lopes", "Stephan Peitz", "Hendra Setiawan", "Leonardo Emili"], "title": "Beyond Text Compression: Evaluating Tokenizers Across Scales", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The choice of tokenizer can profoundly impact language model performance, yet\naccessible and reliable evaluations of tokenizer quality remain an open\nchallenge. Inspired by scaling consistency, we show that smaller models can\naccurately predict significant differences in tokenizer impact on larger models\nat a fraction of the compute cost. By systematically evaluating both\nEnglish-centric and multilingual tokenizers, we find that tokenizer choice has\nnegligible effects on tasks in English but results in consistent performance\ndifferences in multilingual settings. We propose new intrinsic tokenizer\nmetrics inspired by Zipf's law that correlate more strongly with downstream\nperformance than text compression when modeling unseen languages. By combining\nseveral metrics to capture multiple aspects of tokenizer behavior, we develop a\nreliable framework for intrinsic tokenizer evaluations. Our work offers a more\nefficient path to informed tokenizer selection in future language model\ndevelopment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02868", "pdf": "https://arxiv.org/pdf/2506.02868", "abs": "https://arxiv.org/abs/2506.02868", "authors": ["Amal S. Perera", "David Fernandez", "Chandi Witharana", "Elias Manos", "Michael Pimenta", "Anna K. Liljedahl", "Ingmar Nitze", "Yili Yang", "Todd Nicholson", "Chia-Yu Hsu", "Wenwen Li", "Guido Grosse"], "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings", "categories": ["cs.CV", "I.4.6; I.5.4; I.5.2; I.2.10"], "comment": "20 pages, 2 column IEEE format, 13 Figures", "summary": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02896", "pdf": "https://arxiv.org/pdf/2506.02896", "abs": "https://arxiv.org/abs/2506.02896", "authors": ["Adam Pardyl", "Dominik Matuszek", "Mateusz Przebieracz", "Marek Cygan", "Bartosz Zieliński", "Maciej Wołczyk"], "title": "FlySearch: Exploring how vision-language models explore", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01704", "pdf": "https://arxiv.org/pdf/2506.01704", "abs": "https://arxiv.org/abs/2506.01704", "authors": ["Jiongnan Liu", "Zhicheng Dou", "Ning Hu", "Chenyan Xiong"], "title": "Generate, Not Recommend: Personalized Multimodal Content Generation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "To address the challenge of information overload from massive web contents,\nrecommender systems are widely applied to retrieve and present personalized\nresults for users. However, recommendation tasks are inherently constrained to\nfiltering existing items and lack the ability to generate novel concepts,\nlimiting their capacity to fully satisfy user demands and preferences. In this\npaper, we propose a new paradigm that goes beyond content filtering and\nselecting: directly generating personalized items in a multimodal form, such as\nimages, tailored to individual users. To accomplish this, we leverage\nany-to-any Large Multimodal Models (LMMs) and train them in both supervised\nfine-tuning and online reinforcement learning strategy to equip them with the\nability to yield tailored next items for users. Experiments on two benchmark\ndatasets and user study confirm the efficacy of the proposed method. Notably,\nthe generated images not only align well with users' historical preferences but\nalso exhibit relevance to their potential future interests.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02981", "pdf": "https://arxiv.org/pdf/2506.02981", "abs": "https://arxiv.org/abs/2506.02981", "authors": ["Joonyeoup Kim", "Yu Yuan", "Xingguang Zhang", "Xijun Wang", "Stanley Chan"], "title": "Astrophotography turbulence mitigation via generative models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Photography is the cornerstone of modern astronomical and space research.\nHowever, most astronomical images captured by ground-based telescopes suffer\nfrom atmospheric turbulence, resulting in degraded imaging quality. While\nmulti-frame strategies like lucky imaging can mitigate some effects, they\ninvolve intensive data acquisition and complex manual processing. In this\npaper, we propose AstroDiff, a generative restoration method that leverages\nboth the high-quality generative priors and restoration capabilities of\ndiffusion models to mitigate atmospheric turbulence. Extensive experiments\ndemonstrate that AstroDiff outperforms existing state-of-the-art learning-based\nmethods in astronomical image turbulence mitigation, providing higher\nperceptual quality and better structural fidelity under severe turbulence\nconditions. Our code and additional results are available at\nhttps://web-six-kappa-66.vercel.app/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["kappa"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02077", "pdf": "https://arxiv.org/pdf/2506.02077", "abs": "https://arxiv.org/abs/2506.02077", "authors": ["Yoonjun Cho", "Soeun Kim", "Dongjae Jeon", "Kyelim Lee", "Beomsoo Lee", "Albert No"], "title": "Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Decomposing weight matrices into quantization and low-rank components\n($\\mathbf{W} \\approx \\mathbf{Q} + \\mathbf{L}\\mathbf{R}$) is a widely used\ntechnique for compressing large language models (LLMs). Existing joint\noptimization methods iteratively alternate between quantization and low-rank\napproximation. However, these methods tend to prioritize one component at the\nexpense of the other, resulting in suboptimal decompositions that fail to\nleverage each component's unique strengths. In this work, we introduce\nOutlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank\ncomponents the specific role of capturing activation-sensitive weights. This\nstructured decomposition mitigates outliers' negative impact on quantization,\nenabling more effective balance between quantization and low-rank\napproximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B\ndemonstrate that incorporating ODLRI into the joint optimization framework\nconsistently reduces activation-aware error, minimizes quantization scale, and\nimproves perplexity and zero-shot accuracy in low-bit settings.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03073", "pdf": "https://arxiv.org/pdf/2506.03073", "abs": "https://arxiv.org/abs/2506.03073", "authors": ["Roman Titkov", "Egor Zubkov", "Dmitry Yudin", "Jaafar Mahmoud", "Malik Mohrat", "Gennady Sidorov"], "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Modern Gaussian Splatting methods have proven highly effective for real-time\nphotorealistic rendering of 3D scenes. However, integrating semantic\ninformation into this representation remains a significant challenge,\nespecially in maintaining real-time performance for SLAM (Simultaneous\nLocalization and Mapping) applications. In this work, we introduce LEG-SLAM --\na novel approach that fuses an optimized Gaussian Splatting implementation with\nvisual-language feature extraction using DINOv2 followed by a learnable feature\ncompressor based on Principal Component Analysis, while enabling an online\ndense SLAM. Our method simultaneously generates high-quality photorealistic\nimages and semantically labeled scene maps, achieving real-time scene\nreconstruction with more than 10 fps on the Replica dataset and 18 fps on\nScanNet. Experimental results show that our approach significantly outperforms\nstate-of-the-art methods in reconstruction speed while achieving competitive\nrendering quality. The proposed system eliminates the need for prior data\npreparation such as camera's ego motion or pre-computed static semantic maps.\nWith its potential applications in autonomous robotics, augmented reality, and\nother interactive domains, LEG-SLAM represents a significant step forward in\nreal-time semantic 3D Gaussian-based SLAM. Project page:\nhttps://titrom025.github.io/LEG-SLAM/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03082", "pdf": "https://arxiv.org/pdf/2506.03082", "abs": "https://arxiv.org/abs/2506.03082", "authors": ["Ssharvien Kumar Sivakumar", "Yannik Frisch", "Ghazal Ghazaei", "Anirban Mukhopadhyay"], "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03084", "pdf": "https://arxiv.org/pdf/2506.03084", "abs": "https://arxiv.org/abs/2506.03084", "authors": ["Zizhao Wu", "Yingying Sun", "Yiming Chen", "Xiaoling Gu", "Ruyu Liu", "Jiazhou Chen"], "title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03096", "pdf": "https://arxiv.org/pdf/2506.03096", "abs": "https://arxiv.org/abs/2506.03096", "authors": ["Christian Schlarmann", "Francesco Croce", "Nicolas Flammarion", "Matthias Hein"], "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Code and models available at https://github.com/chs20/fuselip", "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03103", "pdf": "https://arxiv.org/pdf/2506.03103", "abs": "https://arxiv.org/abs/2506.03103", "authors": ["Xiaoyan Cong", "Angela Xing", "Chandradeep Pokhariya", "Rao Fu", "Srinath Sridhar"], "title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic hand-object contacts is essential for realistic\nmanipulation in AI character animation, XR, and robotics, yet it remains\nchallenging due to heavy occlusions, complex surface details, and limitations\nin existing capture techniques. In this paper, we introduce DyTact, a\nmarkerless capture method for accurately capturing dynamic contact in\nhand-object manipulations in a non-intrusive manner. Our approach leverages a\ndynamic, articulated representation based on 2D Gaussian surfels to model\ncomplex manipulations. By binding these surfels to MANO meshes, DyTact\nharnesses the inductive bias of template models to stabilize and accelerate\noptimization. A refinement module addresses time-dependent high-frequency\ndeformations, while a contact-guided adaptive sampling strategy selectively\nincreases surfel density in contact regions to handle heavy occlusion.\nExtensive experiments demonstrate that DyTact not only achieves\nstate-of-the-art dynamic contact estimation accuracy but also significantly\nimproves novel view synthesis quality, all while operating with fast\noptimization and efficient memory usage. Project Page:\nhttps://oliver-cong02.github.io/DyTact.github.io/ .", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02590", "pdf": "https://arxiv.org/pdf/2506.02590", "abs": "https://arxiv.org/abs/2506.02590", "authors": ["Dimitrios Koutsianos", "Stavros Zacharopoulos", "Yannis Panagakis", "Themos Stafylakis"], "title": "Synthetic Speech Source Tracing using Metric Learning", "categories": ["cs.SD", "cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "This paper addresses source tracing in synthetic speech-identifying\ngenerative systems behind manipulated audio via speaker recognition-inspired\npipelines. While prior work focuses on spoofing detection, source tracing lacks\nrobust solutions. We evaluate two approaches: classification-based and\nmetric-learning. We tested our methods on the MLAADv5 benchmark using ResNet\nand self-supervised learning (SSL) backbones. The results show that ResNet\nachieves competitive performance with the metric learning approach, matching\nand even exceeding SSL-based systems. Our work demonstrates ResNet's viability\nfor source tracing while underscoring the need to optimize SSL representations\nfor this task. Our work bridges speaker recognition methodologies with audio\nforensic challenges, offering new directions for combating synthetic media\nmanipulation.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02720", "pdf": "https://arxiv.org/pdf/2506.02720", "abs": "https://arxiv.org/abs/2506.02720", "authors": ["Xiaochong Lan", "Jie Feng", "Jiahuan Lei", "Xinlei Shi", "Yong Li"], "title": "Benchmarking and Advancing Large Language Models for Local Life Services", "categories": ["cs.AI", "cs.CL"], "comment": "KDD 2025", "summary": "Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03123", "pdf": "https://arxiv.org/pdf/2506.03123", "abs": "https://arxiv.org/abs/2506.03123", "authors": ["Zhengyao Lv", "Chenyang Si", "Tianlin Pan", "Zhaoxi Chen", "Kwan-Yee K. Wong", "Yu Qiao", "Ziwei Liu"], "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02992", "pdf": "https://arxiv.org/pdf/2506.02992", "abs": "https://arxiv.org/abs/2506.02992", "authors": ["Li Zhang", "Kevin D. Ashley"], "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation", "categories": ["cs.AI", "cs.CL", "cs.LG", "68T50", "I.2"], "comment": "13 pages, 2 figures, Workshop on Legally Compliant Intelligent\n  Chatbots at ICAIL 2025]{Workshop on Legally Compliant Intelligent Chatbots @\n  ICAIL 2025", "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03140", "pdf": "https://arxiv.org/pdf/2506.03140", "abs": "https://arxiv.org/abs/2506.03140", "authors": ["Yawen Luo", "Jianhong Bai", "Xiaoyu Shi", "Menghan Xia", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Tianfan Xue"], "title": "CamCloneMaster: Enabling Reference-based Camera Control for Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://camclonemaster.github.io/", "summary": "Camera control is crucial for generating expressive and cinematic videos.\nExisting methods rely on explicit sequences of camera parameters as control\nconditions, which can be cumbersome for users to construct, particularly for\nintricate camera movements. To provide a more intuitive camera control method,\nwe propose CamCloneMaster, a framework that enables users to replicate camera\nmovements from reference videos without requiring camera parameters or\ntest-time fine-tuning. CamCloneMaster seamlessly supports reference-based\ncamera control for both Image-to-Video and Video-to-Video tasks within a\nunified framework. Furthermore, we present the Camera Clone Dataset, a\nlarge-scale synthetic dataset designed for camera clone learning, encompassing\ndiverse scenes, subjects, and camera movements. Extensive experiments and user\nstudies demonstrate that CamCloneMaster outperforms existing methods in terms\nof both camera controllability and visual quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "test-time fine-tuning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03141", "pdf": "https://arxiv.org/pdf/2506.03141", "abs": "https://arxiv.org/abs/2506.03141", "authors": ["Jiwen Yu", "Jianhong Bai", "Yiran Qin", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Xihui Liu"], "title": "Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in interactive video generation have shown promising results,\nyet existing approaches struggle with scene-consistent memory capabilities in\nlong video generation due to limited use of historical context. In this work,\nwe propose Context-as-Memory, which utilizes historical context as memory for\nvideo generation. It includes two simple yet effective designs: (1) storing\ncontext in frame format without additional post-processing; (2) conditioning by\nconcatenating context and frames to be predicted along the frame dimension at\nthe input, requiring no external control modules. Furthermore, considering the\nenormous computational overhead of incorporating all historical context, we\npropose the Memory Retrieval module to select truly relevant context frames by\ndetermining FOV (Field of View) overlap between camera poses, which\nsignificantly reduces the number of candidate frames without substantial\ninformation loss. Experiments demonstrate that Context-as-Memory achieves\nsuperior memory capabilities in interactive long video generation compared to\nSOTAs, even generalizing effectively to open-domain scenarios not seen during\ntraining. The link of our project page is https://context-as-memory.github.io/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03148", "pdf": "https://arxiv.org/pdf/2506.03148", "abs": "https://arxiv.org/abs/2506.03148", "authors": ["Ayush Shrivastava", "Andrew Owens"], "title": "Self-Supervised Spatial Correspondence Across Modalities", "categories": ["cs.CV"], "comment": "CVPR 2025. Project link: https://www.ayshrv.com/cmrw . Code:\n  https://github.com/ayshrv/cmrw", "summary": "We present a method for finding cross-modal space-time correspondences. Given\ntwo images from different visual modalities, such as an RGB image and a depth\nmap, our model identifies which pairs of pixels correspond to the same physical\npoints in the scene. To solve this problem, we extend the contrastive random\nwalk framework to simultaneously learn cycle-consistent feature representations\nfor both cross-modal and intra-modal matching. The resulting model is simple\nand has no explicit photo-consistency assumptions. It can be trained entirely\nusing unlabeled data, without the need for any spatially aligned multimodal\nimage pairs. We evaluate our method on both geometric and semantic\ncorrespondence tasks. For geometric matching, we consider challenging tasks\nsuch as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic\nmatching, we evaluate on photo-sketch and cross-style image alignment. Our\nmethod achieves strong performance across all benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.01980", "pdf": "https://arxiv.org/pdf/2506.01980", "abs": "https://arxiv.org/abs/2506.01980", "authors": ["Lianhao Yin", "Ozanan Meireles", "Guy Rosman", "Daniela Rus"], "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time video understanding is critical to guide procedures in minimally\ninvasive surgery (MIS). However, supervised learning approaches require large,\nannotated datasets that are scarce due to annotation efforts that are\nprohibitive, e.g., in medical fields. Although self-supervision methods can\naddress such limitations, current self-supervised methods often fail to capture\nstructural and physical information in a form that generalizes across tasks. We\npropose Compress-to-Explore (C2E), a novel self-supervised framework that\nleverages Kolmogorov complexity to learn compact, informative representations\nfrom surgical videos. C2E uses entropy-maximizing decoders to compress images\nwhile preserving clinically relevant details, improving encoder performance\nwithout labeled data. Trained on large-scale unlabeled surgical datasets, C2E\ndemonstrates strong generalization across a variety of surgical ML tasks, such\nas workflow classification, tool-tissue interaction classification,\nsegmentation, and diagnosis tasks, providing improved performance as a surgical\nvisual foundation model. As we further show in the paper, the model's internal\ncompact representation better disentangles features from different structural\nparts of images. The resulting performance improvements highlight the yet\nuntapped potential of self-supervised learning to enhance surgical AI and\nimprove outcomes in MIS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02079", "pdf": "https://arxiv.org/pdf/2506.02079", "abs": "https://arxiv.org/abs/2506.02079", "authors": ["Xuefeng Jiang", "Tian Wen", "Zhiqin Yang", "Lvhua Wu", "Yufeng Chen", "Sheng Sun", "Yuwei Wang", "Min Liu"], "title": "Robust Federated Learning against Noisy Clients via Masked Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02093", "pdf": "https://arxiv.org/pdf/2506.02093", "abs": "https://arxiv.org/abs/2506.02093", "authors": ["Tianyu Lin", "Xinran Li", "Chuntung Zhuang", "Qi Chen", "Yuanhao Cai", "Kai Ding", "Alan L. Yuille", "Zongwei Zhou"], "title": "Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Widely adopted evaluation metrics for sparse-view CT reconstruction--such as\nStructural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize\npixel-wise fidelity but often fail to capture the completeness of critical\nanatomical structures, particularly small or thin regions that are easily\nmissed. To address this limitation, we propose a suite of novel anatomy-aware\nevaluation metrics designed to assess structural completeness across anatomical\nstructures, including large organs, small organs, intestines, and vessels.\nBuilding on these metrics, we introduce CARE, a Completeness-Aware\nReconstruction Enhancement framework that incorporates structural penalties\nduring training to encourage anatomical preservation of significant structures.\nCARE is model-agnostic and can be seamlessly integrated into analytical,\nimplicit, and generative methods. When applied to these methods, CARE\nsubstantially improves structural completeness in CT reconstructions, achieving\nup to +32% improvement for large organs, +22% for small organs, +40% for\nintestines, and +36% for vessels.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02467", "pdf": "https://arxiv.org/pdf/2506.02467", "abs": "https://arxiv.org/abs/2506.02467", "authors": ["Haowen Pang", "Weiyan Guo", "Chuyang Ye"], "title": "Multi-modal brain MRI synthesis based on SwinUNETR", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures", "summary": "Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in\nclinical diagnostics by providing complementary information across different\nimaging modalities. However, a common challenge in clinical practice is missing\nMRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing\nmodalities in brain MRI. SwinUNETR is a novel neural network architecture\ndesigned for medical image analysis, integrating the strengths of Swin\nTransformer and convolutional neural networks (CNNs). The Swin Transformer, a\nvariant of the Vision Transformer (ViT), incorporates hierarchical feature\nextraction and window-based self-attention mechanisms, enabling it to capture\nboth local and global contextual information effectively. By combining the Swin\nTransformer with CNNs, SwinUNETR merges global context awareness with detailed\nspatial resolution. This hybrid approach addresses the challenges posed by the\nvarying modality characteristics and complex brain structures, facilitating the\ngeneration of accurate and realistic synthetic images. We evaluate the\nperformance of SwinUNETR on brain MRI datasets and demonstrate its superior\ncapability in generating clinically valuable images. Our results show\nsignificant improvements in image quality, anatomical consistency, and\ndiagnostic value.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02542", "pdf": "https://arxiv.org/pdf/2506.02542", "abs": "https://arxiv.org/abs/2506.02542", "authors": ["Niklas Kormann", "Masoud Ramuz", "Zeeshan Nisar", "Nadine S. Schaadt", "Hendrik Annuth", "Benjamin Doerr", "Friedrich Feuerhake", "Thomas Lampert", "Johannes F. Lutzeyer"], "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Accepted for poster presentation at MIDL 2025", "summary": "Graph Neural Networks (GNNs) have recently been found to excel in\nhistopathology. However, an important histopathological task, where GNNs have\nnot been extensively explored, is the classification of glomeruli health as an\nimportant indicator in nephropathology. This task presents unique difficulties,\nparticularly for the graph construction, i.e., the identification of nodes,\nedges, and informative features. In this work, we propose a pipeline composed\nof different traditional and machine learning-based computer vision techniques\nto identify nodes, edges, and their corresponding features to form a\nheterogeneous graph. We then proceed to propose a novel heterogeneous GNN\narchitecture for glomeruli classification, called HIEGNet, that integrates both\nglomeruli and their surrounding immune cells. Hence, HIEGNet is able to\nconsider the immune environment of each glomerulus in its classification. Our\nHIEGNet was trained and tested on a dataset of Whole Slide Images from kidney\ntransplant patients. Experimental results demonstrate that HIEGNet outperforms\nseveral baseline models and generalises best between patients among all\nbaseline models. Our implementation is publicly available at\nhttps://github.com/nklsKrmnn/HIEGNet.git.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02620", "pdf": "https://arxiv.org/pdf/2506.02620", "abs": "https://arxiv.org/abs/2506.02620", "authors": ["Dongyu Yan", "Leyi Wu", "Jiantao Lin", "Luozhou Wang", "Tianshuo Xu", "Zhifei Chen", "Zhen Yang", "Lie Xu", "Shunsi Zhang", "Yingcong Chen"], "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 10 figures in main paper, 10 pages, 12 figures in\n  supplementary", "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce \\textbf{FlexPainter},\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02623", "pdf": "https://arxiv.org/pdf/2506.02623", "abs": "https://arxiv.org/abs/2506.02623", "authors": ["Yuyang Zhou", "Ferrante Neri", "Yew-Soon Ong", "Ruibin Bai"], "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)", "summary": "Modern neural architecture search (NAS) is inherently multi-objective,\nbalancing trade-offs such as accuracy, parameter count, and computational cost.\nThis complexity makes NAS computationally expensive and nearly impossible to\nsolve without efficient approximations. To address this, we propose a novel\nsurrogate modelling approach that leverages an ensemble of Siamese network\nblocks to predict dominance relationships between candidate architectures.\nLightweight and easy to train, the surrogate achieves 92% accuracy and replaces\nthe crowding distance calculation in the survivor selection strategy with a\nheuristic rule based on model size. Integrated into a framework termed SiamNAS,\nthis design eliminates costly evaluations during the search process.\nExperiments on NAS-Bench-201 demonstrate the framework's ability to identify\nPareto-optimal solutions with significantly reduced computational costs. The\nproposed SiamNAS identified a final non-dominated set containing the best\narchitecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in\nterms of test error rate, within 0.01 GPU days. This proof-of-concept study\nhighlights the potential of the proposed Siamese network surrogate model to\ngeneralise to multi-tasking optimisation, enabling simultaneous optimisation\nacross tasks. Additionally, it offers opportunities to extend the approach for\ngenerating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal\nsolutions for heterogeneous task settings.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02661", "pdf": "https://arxiv.org/pdf/2506.02661", "abs": "https://arxiv.org/abs/2506.02661", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": "12 pages, 5 figures", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.02895", "pdf": "https://arxiv.org/pdf/2506.02895", "abs": "https://arxiv.org/abs/2506.02895", "authors": ["Ahmad AlMughrabi", "Umair Haroon", "Ricardo Marques", "Petia Radeva"], "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03118", "pdf": "https://arxiv.org/pdf/2506.03118", "abs": "https://arxiv.org/abs/2506.03118", "authors": ["Zhiyuan Yu", "Zhe Li", "Hujun Bao", "Can Yang", "Xiaowei Zhou"], "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Track). Project page:\n  https://zju3dv.github.io/humanram/", "summary": "3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
{"id": "2506.03134", "pdf": "https://arxiv.org/pdf/2506.03134", "abs": "https://arxiv.org/abs/2506.03134", "authors": ["Weiqing Xiao", "Hao Huang", "Chonghao Zhong", "Yujie Lin", "Nan Wang", "Xiaoxue Chen", "Zhaoxi Chen", "Saining Zhang", "Shuocheng Yang", "Pierre Merriaux", "Lei Lei", "Hao Zhao"], "title": "Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding", "categories": ["eess.SP", "cs.CV"], "comment": "Code: https://github.com/zhuxing0/SA-Radar Project page:\n  https://zhuxing0.github.io/projects/SA-Radar", "summary": "We present SA-Radar (Simulate Any Radar), a radar simulation approach that\nenables controllable and efficient generation of radar cubes conditioned on\ncustomizable radar attributes. Unlike prior generative or physics-based\nsimulators, SA-Radar integrates both paradigms through a waveform-parameterized\nattribute embedding. We design ICFAR-Net, a 3D U-Net conditioned on radar\nattributes encoded via waveform parameters, which captures signal variations\ninduced by different radar configurations. This formulation bypasses the need\nfor detailed radar hardware specifications and allows efficient simulation of\nrange-azimuth-Doppler (RAD) tensors across diverse sensor settings. We further\nconstruct a mixed real-simulated dataset with attribute annotations to robustly\ntrain the network. Extensive evaluations on multiple downstream tasks-including\n2D/3D object detection and radar semantic segmentation-demonstrate that\nSA-Radar's simulated data is both realistic and effective, consistently\nimproving model performance when used standalone or in combination with real\ndata. Our framework also supports simulation in novel sensor viewpoints and\nedited scenes, showcasing its potential as a general-purpose radar data engine\nfor autonomous driving applications. Code and additional materials are\navailable at https://zhuxing0.github.io/projects/SA-Radar.", "survey_categories": {"reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-06-04.jsonl"}
