{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "correlation", "agreement"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12329", "pdf": "https://arxiv.org/pdf/2503.12329", "abs": "https://arxiv.org/abs/2503.12329", "authors": ["Kanzhi Cheng", "Wenpo Song", "Jiaxin Fan", "Zheng Ma", "Qiushi Sun", "Fangzhi Xu", "Chenyang Yan", "Nuo Chen", "Jianbing Zhang", "Jiajun Chen"], "title": "CapArena: Benchmarking and Analyzing Detailed Image Captioning in the LLM Era", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference", "correlation", "agreement"], "score": 5}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12575", "pdf": "https://arxiv.org/pdf/2503.12575", "abs": "https://arxiv.org/abs/2503.12575", "authors": ["Dipesh Tamboli", "Souradip Chakraborty", "Aditya Malusare", "Biplab Banerjee", "Amrit Singh Bedi", "Vaneet Aggarwal"], "title": "BalancedDPO: Adaptive Multi-Metric Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have made remarkable advancements, yet\naligning them with diverse preferences remains a persistent challenge. Current\nmethods often optimize single metrics or depend on narrowly curated datasets,\nleading to overfitting and limited generalization across key visual quality\nmetrics. We present BalancedDPO, a novel extension of Direct Preference\nOptimization (DPO) that addresses these limitations by simultaneously aligning\nT2I diffusion models with multiple metrics, including human preference, CLIP\nscore, and aesthetic quality. Our key novelty lies in aggregating consensus\nlabels from diverse metrics in the preference distribution space as compared to\nexisting reward mixing approaches, enabling robust and scalable multi-metric\nalignment while maintaining the simplicity of the standard DPO pipeline that we\nrefer to as BalancedDPO. Our evaluations on the Pick-a-Pic, PartiPrompt and HPD\ndatasets show that BalancedDPO achieves state-of-the-art results, outperforming\nexisting approaches across all major metrics. BalancedDPO improves the average\nwin rates by 15%, 7.1%, and 10.3% on Pick-a-pic, PartiPrompt and HPD,\nrespectively, from the DiffusionDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12689", "pdf": "https://arxiv.org/pdf/2503.12689", "abs": "https://arxiv.org/abs/2503.12689", "authors": ["Hengjia Li", "Lifan Jiang", "Xi Xiao", "Tianyang Wang", "Hongwei Yi", "Boxi Wu", "Deng Cai"], "title": "MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization", "categories": ["cs.CV"], "comment": null, "summary": "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\n$\\textbf{MagicID}$, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "pairwise"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12542", "pdf": "https://arxiv.org/pdf/2503.12542", "abs": "https://arxiv.org/abs/2503.12542", "authors": ["Peiran Wu", "Yunze Liu", "Chonghan Liu", "Miao Liu", "Junxiao Shen"], "title": "ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos", "categories": ["cs.CV"], "comment": null, "summary": "Humans excel at spatio-temporal reasoning, effortlessly interpreting dynamic\nvisual events from an egocentric viewpoint. However, whether multimodal large\nlanguage models (MLLMs) can similarly comprehend the 4D world remains\nuncertain. This paper explores multimodal spatio-temporal reasoning from an\negocentric perspective, aiming to equip MLLMs with human-like reasoning\ncapabilities. To support this objective, we introduce Ego-ST Bench, a novel\nbenchmark containing over 5,000 question-answer pairs across four categories,\nsystematically evaluating spatial, temporal, and integrated spatio-temporal\nreasoning. Additionally, we propose the ST-R1 Video model, a video-based\nreasoning model that incorporates reverse thinking into its reinforcement\nlearning process, significantly enhancing performance. We combine\nlong-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative\nPolicy Optimization (GRPO) reinforcement learning, achieving notable\nimprovements with limited high-quality data. Ego-ST Bench and ST-R1 provide\nvaluable insights and resources for advancing video-based spatio-temporal\nreasoning research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12688", "pdf": "https://arxiv.org/pdf/2503.12688", "abs": "https://arxiv.org/abs/2503.12688", "authors": ["Tianyuan Wang"], "title": "Dynamic Angle Selection in X-Ray CT: A Reinforcement Learning Approach to Optimal Stopping", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In industrial X-ray Computed Tomography (CT), the need for rapid in-line\ninspection is critical. Sparse-angle tomography plays a significant role in\nthis by reducing the required number of projections, thereby accelerating\nprocessing and conserving resources. Most existing methods aim to balance\nreconstruction quality and scanning time, typically relying on fixed scan\ndurations. Adaptive adjustment of the number of angles is essential; for\ninstance, more angles may be required for objects with complex geometries or\nnoisier projections. The concept of optimal stopping, which dynamically adjusts\nthis balance according to varying industrial needs, remains underutilized.\nBuilding on our previous work, we integrate optimal stopping into sequential\nOptimal Experimental Design (OED). We propose a novel method for computing the\npolicy gradient within the Actor-Critic framework, enabling the development of\nadaptive policies for informative angle selection and scan termination.\nAdditionally, we investigated the gap between simulation and real-world\napplications in the context of the developed learning-based method. Our trained\nmodel, developed using synthetic data, demonstrates reliable performance when\napplied to real-world data. This approach enhances the flexibility of CT\noperations and expands the applicability of sparse-angle tomography in\nindustrial settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12800", "pdf": "https://arxiv.org/pdf/2503.12800", "abs": "https://arxiv.org/abs/2503.12800", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Chunping Qiu", "Luoxi Jing", "Mengzhu Wang"], "title": "Pairwise Similarity Regularization for Semi-supervised Graph Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "With fully leveraging the value of unlabeled data, semi-supervised medical\nimage segmentation algorithms significantly reduces the limitation of limited\nlabeled data, achieving a significant improvement in accuracy. However, the\ndistributional shift between labeled and unlabeled data weakens the utilization\nof information from the labeled data. To alleviate the problem, we propose a\ngraph network feature alignment method based on pairwise similarity\nregularization (PaSR) for semi-supervised medical image segmentation. PaSR\naligns the graph structure of images in different domains by maintaining\nconsistency in the pairwise structural similarity of feature graphs between the\ntarget domain and the source domain, reducing distribution shift issues in\nmedical images. Meanwhile, further improving the accuracy of pseudo-labels in\nthe teacher network by aligning graph clustering information to enhance the\nsemi-supervised efficiency of the model. The experimental part was verified on\nthree medical image segmentation benchmark datasets, with results showing\nimprovements over advanced methods in various metrics. On the ACDC dataset, it\nachieved an average improvement of more than 10.66%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12937", "pdf": "https://arxiv.org/pdf/2503.12937", "abs": "https://arxiv.org/abs/2503.12937", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Huanjin Yao", "Shunyu Liu", "Xikun Zhang", "Shijian Lu", "Dacheng Tao"], "title": "R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11806", "pdf": "https://arxiv.org/pdf/2503.11806", "abs": "https://arxiv.org/abs/2503.11806", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "categories": ["cs.CV"], "comment": "Project page: https://www.projectaria.com/scenescript/", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12102", "pdf": "https://arxiv.org/pdf/2503.12102", "abs": "https://arxiv.org/abs/2503.12102", "authors": ["Paula Andrea Pérez-Toro", "Tomás Arias-Vergara", "Fangxu Xing", "Xiaofeng Liu", "Maureen Stone", "Jiachen Zhuo", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "Jana Hutter", "Jerry L. Prince", "Andreas Maier", "Jonghye Woo"], "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the relationship between vocal tract motion during speech and\nthe resulting acoustic signal is crucial for aided clinical assessment and\ndeveloping personalized treatment and rehabilitation strategies. Toward this\ngoal, we introduce an audio-to-video generation framework for creating Real\nTime/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract\nfrom speech signals. Our framework first preprocesses RT-/cine-MRI sequences\nand speech samples to achieve temporal alignment, ensuring synchronization\nbetween visual and audio data. We then employ a modified stable diffusion\nmodel, integrating structural and temporal blocks, to effectively capture\nmovement characteristics and temporal dynamics in the synchronized data. This\nprocess enables the generation of MRI sequences from new speech inputs,\nimproving the conversion of audio into visual data. We evaluated our framework\non healthy controls and tongue cancer patients by analyzing and comparing the\nvocal tract movements in synthesized videos. Our framework demonstrated\nadaptability to new speech inputs and effective generalization. In addition,\npositive human evaluations confirmed its effectiveness, with realistic and\naccurate visualizations, suggesting its potential for outpatient therapy and\npersonalized simulation of vocal tract visualizations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12131", "pdf": "https://arxiv.org/pdf/2503.12131", "abs": "https://arxiv.org/abs/2503.12131", "authors": ["Shentong Mo", "Zehua Chen", "Fan Bao", "Jun Zhu"], "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettré", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12519", "pdf": "https://arxiv.org/pdf/2503.12519", "abs": "https://arxiv.org/abs/2503.12519", "authors": ["Taein Kwon", "Zador Pataki", "Mahdi Rad", "Marc Pollefeys"], "title": "Multi Activity Sequence Alignment via Implicit Clustering", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Self-supervised temporal sequence alignment can provide rich and effective\nrepresentations for a wide range of applications. However, existing methods for\nachieving optimal performance are mostly limited to aligning sequences of the\nsame activity only and require separate models to be trained for each activity.\nWe propose a novel framework that overcomes these limitations using sequence\nalignment via implicit clustering. Specifically, our key idea is to perform\nimplicit clip-level clustering while aligning frames in sequences. This coupled\nwith our proposed dual augmentation technique enhances the network's ability to\nlearn generalizable and discriminative representations. Our experiments show\nthat our proposed method outperforms state-of-the-art results and highlight the\ngeneralization capability of our framework with multi activity and different\nmodalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will\nrelease our code upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12588", "pdf": "https://arxiv.org/pdf/2503.12588", "abs": "https://arxiv.org/abs/2503.12588", "authors": ["Xiaoyu Han", "Shengping Zhang", "Qinglin Liu", "Zonglin Li", "Chenyang Wang"], "title": "Progressive Limb-Aware Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2022. The code is available at\n  https://github.com/xyhanHIT/PL-VTON", "summary": "Existing image-based virtual try-on methods directly transfer specific\nclothing to a human image without utilizing clothing attributes to refine the\ntransferred clothing geometry and textures, which causes incomplete and blurred\nclothing appearances. In addition, these methods usually mask the limb textures\nof the input for the clothing-agnostic person representation, which results in\ninaccurate predictions for human limb regions (i.e., the exposed arm skin),\nespecially when transforming between long-sleeved and short-sleeved garments.\nTo address these problems, we present a progressive virtual try-on framework,\nnamed PL-VTON, which performs pixel-level clothing warping based on multiple\nattributes of clothing and embeds explicit limb-aware features to generate\nphoto-realistic try-on results. Specifically, we design a Multi-attribute\nClothing Warping (MCW) module that adopts a two-stage alignment strategy based\non multiple attributes to progressively estimate pixel-level clothing\ndisplacements. A Human Parsing Estimator (HPE) is then introduced to\nsemantically divide the person into various regions, which provides structural\nconstraints on the human body and therefore alleviates texture bleeding between\nclothing and limb regions. Finally, we propose a Limb-aware Texture Fusion\n(LTF) module to estimate high-quality details in limb regions by fusing\ntextures of the clothing and the human body with the guidance of explicit\nlimb-aware features. Extensive experiments demonstrate that our proposed method\noutperforms the state-of-the-art virtual try-on methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11806", "pdf": "https://arxiv.org/pdf/2503.11806", "abs": "https://arxiv.org/abs/2503.11806", "authors": ["Christopher Xie", "Armen Avetisyan", "Henry Howard-Jenkins", "Yawar Siddiqui", "Julian Straub", "Richard Newcombe", "Vasileios Balntas", "Jakob Engel"], "title": "Human-in-the-Loop Local Corrections of 3D Scene Layouts via Infilling", "categories": ["cs.CV"], "comment": "Project page: https://www.projectaria.com/scenescript/", "summary": "We present a novel human-in-the-loop approach to estimate 3D scene layout\nthat uses human feedback from an egocentric standpoint. We study this approach\nthrough introduction of a novel local correction task, where users identify\nlocal errors and prompt a model to automatically correct them. Building on\nSceneScript, a state-of-the-art framework for 3D scene layout estimation that\nleverages structured language, we propose a solution that structures this\nproblem as \"infilling\", a task studied in natural language processing. We train\na multi-task version of SceneScript that maintains performance on global\npredictions while significantly improving its local correction ability. We\nintegrate this into a human-in-the-loop system, enabling a user to iteratively\nrefine scene layout estimates via a low-friction \"one-click fix'' workflow. Our\nsystem enables the final refined layout to diverge from the training\ndistribution, allowing for more accurate modelling of complex layouts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11892", "pdf": "https://arxiv.org/pdf/2503.11892", "abs": "https://arxiv.org/abs/2503.11892", "authors": ["Chengxuan Qian", "Shuo Xing", "Shawn Li", "Yue Zhao", "Zhengzhong Tu"], "title": "DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "Project website: https://taco-group.github.io/DecAlign/", "summary": "Multimodal representation learning aims to capture both shared and\ncomplementary semantic information across multiple modalities. However, the\nintrinsic heterogeneity of diverse modalities presents substantial challenges\nto achieve effective cross-modal collaboration and integration. To address\nthis, we introduce DecAlign, a novel hierarchical cross-modal alignment\nframework designed to decouple multimodal representations into modality-unique\n(heterogeneous) and modality-common (homogeneous) features. For handling\nheterogeneity, we employ a prototype-guided optimal transport alignment\nstrategy leveraging gaussian mixture modeling and multi-marginal transport\nplans, thus mitigating distribution discrepancies while preserving\nmodality-unique characteristics. To reinforce homogeneity, we ensure semantic\nconsistency across modalities by aligning latent distribution matching with\nMaximum Mean Discrepancy regularization. Furthermore, we incorporate a\nmultimodal transformer to enhance high-level semantic feature fusion, thereby\nfurther reducing cross-modal inconsistencies. Our extensive experiments on four\nwidely used multimodal benchmarks demonstrate that DecAlign consistently\noutperforms existing state-of-the-art methods across five metrics. These\nresults highlight the efficacy of DecAlign in enhancing superior cross-modal\nalignment and semantic consistency while preserving modality-unique features,\nmarking a significant advancement in multimodal representation learning\nscenarios. Our project page is at https://taco-group.github.io/DecAlign and the\ncode is available at https://github.com/taco-group/DecAlign.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.11935", "pdf": "https://arxiv.org/pdf/2503.11935", "abs": "https://arxiv.org/abs/2503.11935", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "title": "Design of an Expression Recognition Solution Employing the Global Channel-Spatial Attention Mechanism", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a challenging classification task with broad\napplication prospects in the field of human - computer interaction. This paper\naims to introduce the methods of our upcoming 8th Affective Behavior Analysis\nin the Wild (ABAW) competition to be held at CVPR2025. To address issues such\nas low recognition accuracy caused by subtle expression changes and multi -\nscales in facial expression recognition in videos, we propose global channel -\nspatial attention and median - enhanced spatial - channel attention to\nstrengthen feature processing for speech and images respectively. Secondly, to\nfully utilize the complementarity between the speech and facial expression\nmodalities, a speech - and - facial - expression key - frame alignment\ntechnique is adopted to calculate the weights of speech and facial expressions.\nThese weights are input into the feature fusion layer for multi - scale dilated\nfusion, which effectively improves the recognition rate of facial expression\nrecognition. In the facial expression recognition task of the 6th ABAW\ncompetition, our method achieved excellent results on the official validation\nset, which fully demonstrates the effectiveness and competitiveness of the\nproposed method.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12018", "pdf": "https://arxiv.org/pdf/2503.12018", "abs": "https://arxiv.org/abs/2503.12018", "authors": ["Zhe Jin", "Tat-Seng Chua"], "title": "Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) diffusion models (DM) have garnered widespread adoption\ndue to their capability in generating high-fidelity outputs and accessibility\nto anyone able to put imagination into words. However, DMs are often\npredisposed to generate unappealing outputs, much like the random images on the\ninternet they were trained on. Existing approaches to address this are founded\non the implicit premise that visual aesthetics is universal, which is limiting.\nAesthetics in the T2I context should be about personalization and we propose\nthe novel task of aesthetics alignment which seeks to align user-specified\naesthetics with the T2I generation output. Inspired by how artworks provide an\ninvaluable perspective to approach aesthetics, we codify visual aesthetics\nusing the compositional framework artists employ, known as the Principles of\nArt (PoA). To facilitate this study, we introduce CompArt, a large-scale\ncompositional art dataset building on top of WikiArt with PoA analysis\nannotated by a capable Multimodal LLM. Leveraging the expressive power of LLMs\nand training a lightweight and transferrable adapter, we demonstrate that T2I\nDMs can effectively offer 10 compositional controls through user-specified PoA\nconditions. Additionally, we design an appropriate evaluation framework to\nassess the efficacy of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12024", "pdf": "https://arxiv.org/pdf/2503.12024", "abs": "https://arxiv.org/abs/2503.12024", "authors": ["Byeongjun Park", "Hyojun Go", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "SteerX: Creating Any Camera-Free 3D and 4D Scenes with Geometric Steering", "categories": ["cs.CV"], "comment": "Project page: https://byeongjun-park.github.io/SteerX/", "summary": "Recent progress in 3D/4D scene generation emphasizes the importance of\nphysical alignment throughout video generation and scene reconstruction.\nHowever, existing methods improve the alignment separately at each stage,\nmaking it difficult to manage subtle misalignments arising from another stage.\nHere, we present SteerX, a zero-shot inference-time steering method that\nunifies scene reconstruction into the generation process, tilting data\ndistributions toward better geometric alignment. To this end, we introduce two\ngeometric reward functions for 3D/4D scene generation by using pose-free\nfeed-forward scene reconstruction models. Through extensive experiments, we\ndemonstrate the effectiveness of SteerX in improving 3D/4D scene generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12067", "pdf": "https://arxiv.org/pdf/2503.12067", "abs": "https://arxiv.org/abs/2503.12067", "authors": ["Amir M. Mansourian", "Rozhan Ahmadi", "Masoud Ghafouri", "Amir Mohammad Babaei", "Elaheh Badali Golezani", "Zeynab Yasamani Ghamchi", "Vida Ramezanian", "Alireza Taherian", "Kimia Dinashi", "Amirali Miri", "Shohreh Kasaei"], "title": "A Comprehensive Survey on Knowledge Distillation", "categories": ["cs.CV"], "comment": "47 pages, 10 figures, 13 tables", "summary": "Deep Neural Networks (DNNs) have achieved notable performance in the fields\nof computer vision and natural language processing with various applications in\nboth academia and industry. However, with recent advancements in DNNs and\ntransformer models with a tremendous number of parameters, deploying these\nlarge models on edge devices causes serious issues such as high runtime and\nmemory consumption. This is especially concerning with the recent large-scale\nfoundation models, Vision-Language Models (VLMs), and Large Language Models\n(LLMs). Knowledge Distillation (KD) is one of the prominent techniques proposed\nto address the aforementioned problems using a teacher-student architecture.\nMore specifically, a lightweight student model is trained using additional\nknowledge from a cumbersome teacher model. In this work, a comprehensive survey\nof knowledge distillation methods is proposed. This includes reviewing KD from\ndifferent aspects: distillation sources, distillation schemes, distillation\nalgorithms, distillation by modalities, applications of distillation, and\ncomparison among existing methods. In contrast to most existing surveys, which\nare either outdated or simply update former surveys, this work proposes a\ncomprehensive survey with a new point of view and representation structure that\ncategorizes and investigates the most recent methods in knowledge distillation.\nThis survey considers various critically important subcategories, including KD\nfor diffusion models, 3D inputs, foundational models, transformers, and LLMs.\nFurthermore, existing challenges in KD and possible future research directions\nare discussed. Github page of the project:\nhttps://github.com/IPL-Sharif/KD_Survey", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12077", "pdf": "https://arxiv.org/pdf/2503.12077", "abs": "https://arxiv.org/abs/2503.12077", "authors": ["Zhengrong Yue", "Shaobin Zhuang", "Kunchang Li", "Yanbo Ding", "Yali Wang"], "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite the recent advancement in video stylization, most existing methods\nstruggle to render any video with complex transitions, based on an open style\ndescription of user query. To fill this gap, we introduce a generic multi-agent\nsystem for video stylization, V-Stylist, by a novel collaboration and\nreflection paradigm of multi-modal large language models. Specifically, our\nV-Stylist is a systematical workflow with three key roles: (1) Video Parser\ndecomposes the input video into a number of shots and generates their text\nprompts of key shot content. Via a concise video-to-shot prompting paradigm, it\nallows our V-Stylist to effectively handle videos with complex transitions. (2)\nStyle Parser identifies the style in the user query and progressively search\nthe matched style model from a style tree. Via a robust tree-of-thought\nsearching paradigm, it allows our V-Stylist to precisely specify vague style\npreference in the open user query. (3) Style Artist leverages the matched model\nto render all the video shots into the required style. Via a novel multi-round\nself-reflection paradigm, it allows our V-Stylist to adaptively adjust detail\ncontrol, according to the style requirement. With such a distinct design of\nmimicking human professionals, our V-Stylist achieves a major breakthrough over\nthe primary challenges for effective and automatic video stylization.\nMoreover,we further construct a new benchmark Text-driven Video Stylization\nBenchmark (TVSBench), which fills the gap to assess stylization of complex\nvideos on open user queries. Extensive experiments show that, V-Stylist\nachieves the state-of-the-art, e.g.,V-Stylist surpasses FRESCO and ControlVideo\nby 6.05% and 4.51% respectively in overall average metrics, marking a\nsignificant advance in video stylization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12102", "pdf": "https://arxiv.org/pdf/2503.12102", "abs": "https://arxiv.org/abs/2503.12102", "authors": ["Paula Andrea Pérez-Toro", "Tomás Arias-Vergara", "Fangxu Xing", "Xiaofeng Liu", "Maureen Stone", "Jiachen Zhuo", "Juan Rafael Orozco-Arroyave", "Elmar Nöth", "Jana Hutter", "Jerry L. Prince", "Andreas Maier", "Jonghye Woo"], "title": "A Speech-to-Video Synthesis Approach Using Spatio-Temporal Diffusion for Vocal Tract MRI", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the relationship between vocal tract motion during speech and\nthe resulting acoustic signal is crucial for aided clinical assessment and\ndeveloping personalized treatment and rehabilitation strategies. Toward this\ngoal, we introduce an audio-to-video generation framework for creating Real\nTime/cine-Magnetic Resonance Imaging (RT-/cine-MRI) visuals of the vocal tract\nfrom speech signals. Our framework first preprocesses RT-/cine-MRI sequences\nand speech samples to achieve temporal alignment, ensuring synchronization\nbetween visual and audio data. We then employ a modified stable diffusion\nmodel, integrating structural and temporal blocks, to effectively capture\nmovement characteristics and temporal dynamics in the synchronized data. This\nprocess enables the generation of MRI sequences from new speech inputs,\nimproving the conversion of audio into visual data. We evaluated our framework\non healthy controls and tongue cancer patients by analyzing and comparing the\nvocal tract movements in synthesized videos. Our framework demonstrated\nadaptability to new speech inputs and effective generalization. In addition,\npositive human evaluations confirmed its effectiveness, with realistic and\naccurate visualizations, suggesting its potential for outpatient therapy and\npersonalized simulation of vocal tract visualizations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12131", "pdf": "https://arxiv.org/pdf/2503.12131", "abs": "https://arxiv.org/abs/2503.12131", "authors": ["Shentong Mo", "Zehua Chen", "Fan Bao", "Jun Zhu"], "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent works in cross-modal understanding and generation, notably through\nmodels like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive\nAudio-Visual Pretraining), have significantly enhanced the alignment of text,\nvideo, and audio embeddings via a single contrastive loss. However, these\nmethods often overlook the bidirectional interactions and inherent noises\npresent in each modality, which can crucially impact the quality and efficacy\nof cross-modal integration. To address this limitation, we introduce DiffGAP, a\nnovel approach incorporating a lightweight generative module within the\ncontrastive space. Specifically, our DiffGAP employs a bidirectional diffusion\nprocess tailored to bridge the cross-modal gap more effectively. This involves\na denoising process on text and video embeddings conditioned on audio\nembeddings and vice versa, thus facilitating a more nuanced and robust\ncross-modal interaction. Our experimental results on VGGSound and AudioCaps\ndatasets demonstrate that DiffGAP significantly improves performance in\nvideo/text-audio generation and retrieval tasks, confirming its effectiveness\nin enhancing cross-modal understanding and generation capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12168", "pdf": "https://arxiv.org/pdf/2503.12168", "abs": "https://arxiv.org/abs/2503.12168", "authors": ["Feixiang He", "Jiangbei Yue", "Jialin Zhu", "Armin Seyfried", "Dan Casas", "Julien Pettré", "He Wang"], "title": "Learning Extremely High Density Crowds as Active Matters", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video-based high-density crowd analysis and prediction has been a\nlong-standing topic in computer vision. It is notoriously difficult due to, but\nnot limited to, the lack of high-quality data and complex crowd dynamics.\nConsequently, it has been relatively under studied. In this paper, we propose a\nnew approach that aims to learn from in-the-wild videos, often with low quality\nwhere it is difficult to track individuals or count heads. The key novelty is a\nnew physics prior to model crowd dynamics. We model high-density crowds as\nactive matter, a continumm with active particles subject to stochastic forces,\nnamed 'crowd material'. Our physics model is combined with neural networks,\nresulting in a neural stochastic differential equation system which can mimic\nthe complex crowd dynamics. Due to the lack of similar research, we adapt a\nrange of existing methods which are close to ours for comparison. Through\nexhaustive evaluation, we show our model outperforms existing methods in\nanalyzing and forecasting extremely high-density crowds. Furthermore, since our\nmodel is a continuous-time physics model, it can be used for simulation and\nanalysis, providing strong interpretability. This is categorically different\nfrom most deep learning methods, which are discrete-time models and\nblack-boxes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12193", "pdf": "https://arxiv.org/pdf/2503.12193", "abs": "https://arxiv.org/abs/2503.12193", "authors": ["S Balasubramanian", "Yedu Krishna P", "Talasu Sai Sriram", "M Sai Subramaniam", "Manepalli Pranav Phanindra Sai", "Darshan Gera"], "title": "S2IL: Structurally Stable Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Feature Distillation (FD) strategies are proven to be effective in mitigating\nCatastrophic Forgetting (CF) seen in Class Incremental Learning (CIL). However,\ncurrent FD approaches enforce strict alignment of feature magnitudes and\ndirections across incremental steps, limiting the model's ability to adapt to\nnew knowledge. In this paper we propose Structurally Stable Incremental\nLearning(S22IL), a FD method for CIL that mitigates CF by focusing on\npreserving the overall spatial patterns of features which promote flexible\n(plasticity) yet stable representations that preserve old knowledge\n(stability). We also demonstrate that our proposed method S2IL achieves strong\nincremental accuracy and outperforms other FD methods on SOTA benchmark\ndatasets CIFAR-100, ImageNet-100 and ImageNet-1K. Notably, S2IL outperforms\nother methods by a significant margin in scenarios that have a large number of\nincremental tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12242", "pdf": "https://arxiv.org/pdf/2503.12242", "abs": "https://arxiv.org/abs/2503.12242", "authors": ["Yuheng Jiang", "Zhehao Shen", "Chengcheng Guo", "Yu Hong", "Zhuo Su", "Yingliang Zhang", "Marc Habermann", "Lan Xu"], "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page:\n  https://moqiyinlun.github.io/Reperformer/", "summary": "Human-centric volumetric videos offer immersive free-viewpoint experiences,\nyet existing methods focus either on replaying general dynamic scenes or\nanimating human avatars, limiting their ability to re-perform general dynamic\nscenes. In this paper, we present RePerformer, a novel Gaussian-based\nrepresentation that unifies playback and re-performance for high-fidelity\nhuman-centric volumetric videos. Specifically, we hierarchically disentangle\nthe dynamic scenes into motion Gaussians and appearance Gaussians which are\nassociated in the canonical space. We further employ a Morton-based\nparameterization to efficiently encode the appearance Gaussians into 2D\nposition and attribute maps. For enhanced generalization, we adopt 2D CNNs to\nmap position maps to attribute maps, which can be assembled into appearance\nGaussians for high-fidelity rendering of the dynamic scenes. For\nre-performance, we develop a semantic-aware alignment module and apply\ndeformation transfer on motion Gaussians, enabling photo-real rendering under\nnovel motions. Extensive experiments validate the robustness and effectiveness\nof RePerformer, setting a new benchmark for playback-then-reperformance\nparadigm in human-centric volumetric videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12369", "pdf": "https://arxiv.org/pdf/2503.12369", "abs": "https://arxiv.org/abs/2503.12369", "authors": ["Ruoyu Wang", "Yukai Ma", "Yi Yao", "Sheng Tao", "Haoang Li", "Zongzhi Zhu", "Yong Liu", "Xingxing Zuo"], "title": "L2COcc: Lightweight Camera-Centric Semantic Scene Completion via Distillation of LiDAR Model", "categories": ["cs.CV"], "comment": null, "summary": "Semantic Scene Completion (SSC) constitutes a pivotal element in autonomous\ndriving perception systems, tasked with inferring the 3D semantic occupancy of\na scene from sensory data. To improve accuracy, prior research has implemented\nvarious computationally demanding and memory-intensive 3D operations, imposing\nsignificant computational requirements on the platform during training and\ntesting. This paper proposes L2COcc, a lightweight camera-centric SSC framework\nthat also accommodates LiDAR inputs. With our proposed efficient voxel\ntransformer (EVT) and cross-modal knowledge modules, including feature\nsimilarity distillation (FSD), TPV distillation (TPVD) and prediction alignment\ndistillation (PAD), our method substantially reduce computational burden while\nmaintaining high accuracy. The experimental evaluations demonstrate that our\nproposed method surpasses the current state-of-the-art vision-based SSC methods\nregarding accuracy on both the SemanticKITTI and SSCBench-KITTI-360 benchmarks,\nrespectively. Additionally, our method is more lightweight, exhibiting a\nreduction in both memory consumption and inference time by over 23% compared to\nthe current state-of-the-arts method. Code is available at our project\npage:https://studyingfufu.github.io/L2COcc/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12383", "pdf": "https://arxiv.org/pdf/2503.12383", "abs": "https://arxiv.org/abs/2503.12383", "authors": ["Songen Gu", "Haoxuan Song", "Binjie Liu", "Qian Yu", "Sanyi Zhang", "Haiyong Jiang", "Jin Huang", "Feng Tian"], "title": "VRsketch2Gaussian: 3D VR Sketch Guided 3D Object Generation with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We propose VRSketch2Gaussian, a first VR sketch-guided, multi-modal, native\n3D object generation framework that incorporates a 3D Gaussian Splatting\nrepresentation. As part of our work, we introduce VRSS, the first large-scale\npaired dataset containing VR sketches, text, images, and 3DGS, bridging the gap\nin multi-modal VR sketch-based generation. Our approach features the following\nkey innovations: 1) Sketch-CLIP feature alignment. We propose a two-stage\nalignment strategy that bridges the domain gap between sparse VR sketch\nembeddings and rich CLIP embeddings, facilitating both VR sketch-based\nretrieval and generation tasks. 2) Fine-Grained multi-modal conditioning. We\ndisentangle the 3D generation process by using explicit VR sketches for\ngeometric conditioning and text descriptions for appearance control. To\nfacilitate this, we propose a generalizable VR sketch encoder that effectively\naligns different modalities. 3) Efficient and high-fidelity 3D native\ngeneration. Our method leverages a 3D-native generation approach that enables\nfast and texture-rich 3D object synthesis. Experiments conducted on our VRSS\ndataset demonstrate that our method achieves high-quality, multi-modal VR\nsketch-based 3D generation. We believe our VRSS dataset and VRsketch2Gaussian\nmethod will be beneficial for the 3D generation community.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12404", "pdf": "https://arxiv.org/pdf/2503.12404", "abs": "https://arxiv.org/abs/2503.12404", "authors": ["Jianhao Yang", "Wenshuo Yu", "Yuanchao Lv", "Jiance Sun", "Bokang Sun", "Mingyang Liu"], "title": "SAM2-ELNet: Label Enhancement and Automatic Annotation for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image segmentation is crucial for environmental monitoring,\ndisaster assessment, and resource management, directly affecting the accuracy\nand efficiency of surface information extraction. The performance of existing\nsupervised models in remote sensing image segmentation tasks highly depends on\nthe quality of label data. However, current label data mainly relies on manual\nannotation, which comes with high time costs and is subject to subjective\ninterference, resulting in distortion of label boundaries and often a loss of\ndetail. To solve the above problems, our work proposes an Edge-enhanced\nLabeling Network, called SAM2-ELNet, which incorporates a labeling module and\nan edge attention mechanism. This model effectively addresses issues such as\nlabel detail loss, fragmentation, and inaccurate boundaries. Due to the\nscarcity of manually annotated remote sensing data, the feature extraction\ncapabilities of traditional neural networks are limited. Our method uses the\nHiera backbone of the pre-trained self-supervised large model segment anything\nmodel 2 (SAM2) as the encoder, achieves high-quality and efficient feature\nextraction even with small samples by fine-tuning on downstream tasks. This\nstudy compared the training effects of original and enhanced labels on the\nmanually annotated Deep-SAR Oil Spill (SOS) dataset. Results showed that the\nmodel trained with enhanced labels performed better and had a lower final loss,\nindicating closer alignment with the real data distribution. Our work also\nexplores the potential of extending the model into an efficient automatic\nannotation framework through generalization experiments, facilitating\nlarge-scale remote sensing image interpretation and intelligent recognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12519", "pdf": "https://arxiv.org/pdf/2503.12519", "abs": "https://arxiv.org/abs/2503.12519", "authors": ["Taein Kwon", "Zador Pataki", "Mahdi Rad", "Marc Pollefeys"], "title": "Multi Activity Sequence Alignment via Implicit Clustering", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "Self-supervised temporal sequence alignment can provide rich and effective\nrepresentations for a wide range of applications. However, existing methods for\nachieving optimal performance are mostly limited to aligning sequences of the\nsame activity only and require separate models to be trained for each activity.\nWe propose a novel framework that overcomes these limitations using sequence\nalignment via implicit clustering. Specifically, our key idea is to perform\nimplicit clip-level clustering while aligning frames in sequences. This coupled\nwith our proposed dual augmentation technique enhances the network's ability to\nlearn generalizable and discriminative representations. Our experiments show\nthat our proposed method outperforms state-of-the-art results and highlight the\ngeneralization capability of our framework with multi activity and different\nmodalities on three diverse datasets, H2O, PennAction, and IKEA ASM. We will\nrelease our code upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12588", "pdf": "https://arxiv.org/pdf/2503.12588", "abs": "https://arxiv.org/abs/2503.12588", "authors": ["Xiaoyu Han", "Shengping Zhang", "Qinglin Liu", "Zonglin Li", "Chenyang Wang"], "title": "Progressive Limb-Aware Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2022. The code is available at\n  https://github.com/xyhanHIT/PL-VTON", "summary": "Existing image-based virtual try-on methods directly transfer specific\nclothing to a human image without utilizing clothing attributes to refine the\ntransferred clothing geometry and textures, which causes incomplete and blurred\nclothing appearances. In addition, these methods usually mask the limb textures\nof the input for the clothing-agnostic person representation, which results in\ninaccurate predictions for human limb regions (i.e., the exposed arm skin),\nespecially when transforming between long-sleeved and short-sleeved garments.\nTo address these problems, we present a progressive virtual try-on framework,\nnamed PL-VTON, which performs pixel-level clothing warping based on multiple\nattributes of clothing and embeds explicit limb-aware features to generate\nphoto-realistic try-on results. Specifically, we design a Multi-attribute\nClothing Warping (MCW) module that adopts a two-stage alignment strategy based\non multiple attributes to progressively estimate pixel-level clothing\ndisplacements. A Human Parsing Estimator (HPE) is then introduced to\nsemantically divide the person into various regions, which provides structural\nconstraints on the human body and therefore alleviates texture bleeding between\nclothing and limb regions. Finally, we propose a Limb-aware Texture Fusion\n(LTF) module to estimate high-quality details in limb regions by fusing\ntextures of the clothing and the human body with the guidance of explicit\nlimb-aware features. Extensive experiments demonstrate that our proposed method\noutperforms the state-of-the-art virtual try-on methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/PL-VTON.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12720", "pdf": "https://arxiv.org/pdf/2503.12720", "abs": "https://arxiv.org/abs/2503.12720", "authors": ["Feng Qiao", "Zhexiao Xiong", "Eric Xing", "Nathan Jacobs"], "title": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching", "categories": ["cs.CV"], "comment": "Project page is available at https://qjizhi.github.io/genstereo", "summary": "Stereo images are fundamental to numerous applications, including extended\nreality (XR) devices, autonomous driving, and robotics. Unfortunately,\nacquiring high-quality stereo images remains challenging due to the precise\ncalibration requirements of dual-camera setups and the complexity of obtaining\naccurate, dense disparity maps. Existing stereo image generation methods\ntypically focus on either visual quality for viewing or geometric accuracy for\nmatching, but not both. We introduce GenStereo, a diffusion-based approach, to\nbridge this gap. The method includes two primary innovations (1) conditioning\nthe diffusion process on a disparity-aware coordinate embedding and a warped\ninput image, allowing for more precise stereo alignment than previous methods,\nand (2) an adaptive fusion mechanism that intelligently combines the\ndiffusion-generated image with a warped image, improving both realism and\ndisparity consistency. Through extensive training on 11 diverse stereo\ndatasets, GenStereo demonstrates strong generalization ability. GenStereo\nachieves state-of-the-art performance in both stereo image generation and\nunsupervised stereo matching tasks. Our framework eliminates the need for\ncomplex hardware setups while enabling high-quality stereo image generation,\nmaking it valuable for both real-world applications and unsupervised learning\nscenarios. Project page is available at https://qjizhi.github.io/genstereo", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12797", "pdf": "https://arxiv.org/pdf/2503.12797", "abs": "https://arxiv.org/abs/2503.12797", "authors": ["Xinyu Ma", "Ziyang Ding", "Zhicong Luo", "Chi Chen", "Zonghao Guo", "Derek F. Wong", "Xiaoyi Feng", "Maosong Sun"], "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12868", "pdf": "https://arxiv.org/pdf/2503.12868", "abs": "https://arxiv.org/abs/2503.12868", "authors": ["Zi Li", "Jianpeng Zhang", "Tai Ma", "Tony C. W. Mok", "Yan-Jie Zhou", "Zeli Chen", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "UniReg: Foundation Model for Controllable Medical Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based medical image registration has achieved performance parity\nwith conventional methods while demonstrating a substantial advantage in\ncomputational efficiency. However, learning-based registration approaches lack\ngeneralizability across diverse clinical scenarios, requiring the laborious\ndevelopment of multiple isolated networks for specific registration tasks,\ne.g., inter-/intra-subject registration or organ-specific alignment. % To\novercome this limitation, we propose \\textbf{UniReg}, the first interactive\nfoundation model for medical image registration, which combines the precision\nadvantages of task-specific learning methods with the generalization of\ntraditional optimization methods. Our key innovation is a unified framework for\ndiverse registration scenarios, achieved through a conditional deformation\nfield estimation within a unified registration model. This is realized through\na dynamic learning paradigm that explicitly encodes: (1) anatomical structure\npriors, (2) registration type constraints (inter/intra-subject), and (3)\ninstance-specific features, enabling the generation of scenario-optimal\ndeformation fields. % Through comprehensive experiments encompassing $90$\nanatomical structures at different body regions, our UniReg model demonstrates\ncomparable performance with contemporary state-of-the-art methodologies while\nachieving ~50\\% reduction in required training iterations relative to the\nconventional learning-based paradigm. This optimization contributes to a\nsignificant reduction in computational resources, such as training time. Code\nand model will be available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12875", "pdf": "https://arxiv.org/pdf/2503.12875", "abs": "https://arxiv.org/abs/2503.12875", "authors": ["Evelyn J. Mannix", "Bartholomew A. Woodham"], "title": "An interpretable approach to automating the assessment of biofouling in video footage", "categories": ["cs.CV"], "comment": null, "summary": "Biofouling$\\unicode{x2013}$communities of organisms that grow on hard\nsurfaces immersed in water$\\unicode{x2013}$provides a pathway for the spread of\ninvasive marine species and diseases. To address this risk, international\nvessels are increasingly being obligated to provide evidence of their\nbiofouling management practices. Verification that these activities are\neffective requires underwater inspections, using divers or underwater remotely\noperated vehicles (ROVs), and the collection and analysis of large amounts of\nimagery and footage. Automated assessment using computer vision techniques can\nsignificantly streamline this process, and this work shows how this challenge\ncan be addressed efficiently and effectively using the interpretable Component\nFeatures (ComFe) approach with a DINOv2 Vision Transformer (ViT) foundation\nmodel. ComFe is able to obtain improved performance in comparison to previous\nnon-interpretable Convolutional Neural Network (CNN) methods, with\nsignificantly fewer weights and greater transparency$\\unicode{x2013}$through\nidentifying which regions of the image contribute to the classification, and\nwhich images in the training data lead to that conclusion. All code, data and\nmodel weights are publicly released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12912", "pdf": "https://arxiv.org/pdf/2503.12912", "abs": "https://arxiv.org/abs/2503.12912", "authors": ["Bin Tang", "Keqi Pan", "Miao Zheng", "Ning Zhou", "Jialu Sui", "Dandan Zhu", "Cheng-Long Deng", "Shu-Guang Kuai"], "title": "Pose as a Modality: A Psychology-Inspired Network for Personality Recognition with a New Multimodal Dataset", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 6 figures, AAAI 2025 Oral", "summary": "In recent years, predicting Big Five personality traits from multimodal data\nhas received significant attention in artificial intelligence (AI). However,\nexisting computational models often fail to achieve satisfactory performance.\nPsychological research has shown a strong correlation between pose and\npersonality traits, yet previous research has largely ignored pose data in\ncomputational models. To address this gap, we develop a novel multimodal\ndataset that incorporates full-body pose data. The dataset includes video\nrecordings of 287 participants completing a virtual interview with 36\nquestions, along with self-reported Big Five personality scores as labels. To\neffectively utilize this multimodal data, we introduce the Psychology-Inspired\nNetwork (PINet), which consists of three key modules: Multimodal Feature\nAwareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed\nModality Correlation Loss (PIMC Loss). The MFA module leverages the Vision\nMamba Block to capture comprehensive visual features related to personality,\nwhile the MFI module efficiently fuses the multimodal features. The PIMC Loss,\ngrounded in psychological theory, guides the model to emphasize different\nmodalities for different personality dimensions. Experimental results show that\nthe PINet outperforms several state-of-the-art baseline models. Furthermore,\nthe three modules of PINet contribute almost equally to the model's overall\nperformance. Incorporating pose data significantly enhances the model's\nperformance, with the pose modality ranking mid-level in importance among the\nfive modalities. These findings address the existing gap in personality-related\ndatasets that lack full-body pose data and provide a new approach for improving\nthe accuracy of personality prediction models, highlighting the importance of\nintegrating psychological insights into AI frameworks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12972", "pdf": "https://arxiv.org/pdf/2503.12972", "abs": "https://arxiv.org/abs/2503.12972", "authors": ["Junming Liu", "Siyuan Meng", "Yanting Gao", "Song Mao", "Pinlong Cai", "Guohang Yan", "Yirong Chen", "Zilin Bian", "Botian Shi", "Ding Wang"], "title": "Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 7 figures, 6 tables", "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13053", "pdf": "https://arxiv.org/pdf/2503.13053", "abs": "https://arxiv.org/abs/2503.13053", "authors": ["Nassim Ali Ousalah", "Anis Kacem", "Enjie Ghorbel", "Emmanuel Koumandakis", "Djamila Aouada"], "title": "Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13214", "pdf": "https://arxiv.org/pdf/2503.13214", "abs": "https://arxiv.org/abs/2503.13214", "authors": ["Jie Huang", "Haorui Chen", "Jiaxuan Ren", "Siran Peng", "Liangjian Deng"], "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening", "categories": ["cs.CV", "cs.AI"], "comment": "This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025", "summary": "Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13229", "pdf": "https://arxiv.org/pdf/2503.13229", "abs": "https://arxiv.org/abs/2503.13229", "authors": ["Yongkang Cheng", "Shaoli Huang"], "title": "HoloGest: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures", "categories": ["cs.CV"], "comment": "Accepted by 3DV 2025", "summary": "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13358", "pdf": "https://arxiv.org/pdf/2503.13358", "abs": "https://arxiv.org/abs/2503.13358", "authors": ["Daniil Selikhanovych", "David Li", "Aleksei Leonov", "Nikita Gushchin", "Sergei Kushneriuk", "Alexander Filippov", "Evgeny Burnaev", "Iaroslav Koshelev", "Alexander Korotin"], "title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13383", "pdf": "https://arxiv.org/pdf/2503.13383", "abs": "https://arxiv.org/abs/2503.13383", "authors": ["Mengyao Lyu", "Yan Li", "Huasong Zhong", "Wenhao Yang", "Hui Chen", "Jungong Han", "Guiguang Ding", "Zhenheng Yang"], "title": "Cream of the Crop: Harvesting Rich, Scalable and Transferable Multi-Modal Data for Instruction Fine-Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "update comparison with sota and analysis", "summary": "The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13435", "pdf": "https://arxiv.org/pdf/2503.13435", "abs": "https://arxiv.org/abs/2503.13435", "authors": ["Ling Yang", "Kaixin Zhu", "Juanxi Tian", "Bohan Zeng", "Mingbao Lin", "Hongjuan Pei", "Wentao Zhang", "Shuicheng Yan"], "title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/WideRange4D", "summary": "With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12042", "pdf": "https://arxiv.org/pdf/2503.12042", "abs": "https://arxiv.org/abs/2503.12042", "authors": ["Zhedong Zhang", "Liang Li", "Chenggang Yan", "Chunshan Liu", "Anton van den Hengel", "Yuankai Qi"], "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Movie dubbing describes the process of transforming a script into speech that\naligns temporally and emotionally with a given movie clip while exemplifying\nthe speaker's voice demonstrated in a short reference audio clip. This task\ndemands the model bridge character performances and complicated prosody\nstructures to build a high-quality video-synchronized dubbing track. The\nlimited scale of movie dubbing datasets, along with the background noise\ninherent in audio data, hinder the acoustic modeling performance of trained\nmodels. To address these issues, we propose an acoustic-prosody disentangled\ntwo-stage method to achieve high-quality dubbing generation with precise\nprosody alignment. First, we propose a prosody-enhanced acoustic pre-training\nto develop robust acoustic modeling capabilities. Then, we freeze the\npre-trained acoustic system and design a disentangled framework to model\nprosodic text features and dubbing style while maintaining acoustic quality.\nAdditionally, we incorporate an in-domain emotion analysis module to reduce the\nimpact of visual domain shifts across different movies, thereby enhancing\nemotion-prosody alignment. Extensive experiments show that our method performs\nfavorably against the state-of-the-art models on two primary benchmarks. The\ndemos are available at https://zzdoog.github.io/ProDubber/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12141", "pdf": "https://arxiv.org/pdf/2503.12141", "abs": "https://arxiv.org/abs/2503.12141", "authors": ["Shayan Rokhva", "Babak Teimourpour", "Romina Babaei"], "title": "Enhanced Sentiment Analysis of Iranian Restaurant Reviews Utilizing Sentiment Intensity Analyzer & Fuzzy Logic", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "This research presents an advanced sentiment analysis framework studied on\nIranian restaurant reviews, combining fuzzy logic with conventional sentiment\nanalysis techniques to assess both sentiment polarity and intensity. A dataset\nof 1266 reviews, alongside corresponding star ratings, was compiled and\npreprocessed for analysis. Initial sentiment analysis was conducted using the\nSentiment Intensity Analyzer (VADER), a rule-based tool that assigns sentiment\nscores across positive, negative, and neutral categories. However, a noticeable\nbias toward neutrality often led to an inaccurate representation of sentiment\nintensity. To mitigate this issue, based on a fuzzy perspective, two refinement\ntechniques were introduced, applying square-root and fourth-root\ntransformations to amplify positive and negative sentiment scores while\nmaintaining neutrality. This led to three distinct methodologies: Approach 1,\nutilizing unaltered VADER scores; Approach 2, modifying sentiment values using\nthe square root; and Approach 3, applying the fourth root for further\nrefinement. A Fuzzy Inference System incorporating comprehensive fuzzy rules\nwas then developed to process these refined scores and generate a single,\ncontinuous sentiment value for each review based on each approach. Comparative\nanalysis, including human supervision and alignment with customer star ratings,\nrevealed that the refined approaches significantly improved sentiment analysis\nby reducing neutrality bias and better capturing sentiment intensity. Despite\nthese advancements, minor over-amplification and persistent neutrality in\ndomain-specific cases were identified, leading us to propose several future\nstudies to tackle these occasional barriers. The study's methodology and\noutcomes offer valuable insights for businesses seeking a more precise\nunderstanding of consumer sentiment, enhancing sentiment analysis across\nvarious industries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12505", "pdf": "https://arxiv.org/pdf/2503.12505", "abs": "https://arxiv.org/abs/2503.12505", "authors": ["Zhaopan Xu", "Pengfei Zhou", "Jiaxin Ai", "Wangbo Zhao", "Kai Wang", "Xiaojiang Peng", "Wenqi Shao", "Hongxun Yao", "Kaipeng Zhang"], "title": "MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reasoning is an essential capacity for large language models (LLMs) to\naddress complex tasks, where the identification of process errors is vital for\nimproving this ability. Recently, process-level reward models (PRMs) were\nproposed to provide step-wise rewards that facilitate reinforcement learning\nand data production during training and guide LLMs toward correct steps during\ninference, thereby improving reasoning accuracy. However, existing benchmarks\nof PRMs are text-based and focus on error detection, neglecting other scenarios\nlike reasoning search. To address this gap, we introduce MPBench, a\ncomprehensive, multi-task, multimodal benchmark designed to systematically\nassess the effectiveness of PRMs in diverse scenarios. MPBench employs three\nevaluation paradigms, each targeting a specific role of PRMs in the reasoning\nprocess: (1) Step Correctness, which assesses the correctness of each\nintermediate reasoning step; (2) Answer Aggregation, which aggregates multiple\nsolutions and selects the best one; and (3) Reasoning Process Search, which\nguides the search for optimal reasoning steps during inference. Through these\nparadigms, MPBench makes comprehensive evaluations and provides insights into\nthe development of multimodal PRMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12840", "pdf": "https://arxiv.org/pdf/2503.12840", "abs": "https://arxiv.org/abs/2503.12840", "authors": ["Chen Liu", "Liying Yang", "Peike Li", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "Accepted by CVPR2025", "summary": "Sound-guided object segmentation has drawn considerable attention for its\npotential to enhance multimodal perception. Previous methods primarily focus on\ndeveloping advanced architectures to facilitate effective audio-visual\ninteractions, without fully addressing the inherent challenges posed by audio\nnatures, \\emph{\\ie}, (1) feature confusion due to the overlapping nature of\naudio signals, and (2) audio-visual matching difficulty from the varied sounds\nproduced by the same object. To address these challenges, we propose Dynamic\nDerivation and Elimination (DDESeg): a novel audio-visual segmentation\nframework. Specifically, to mitigate feature confusion, DDESeg reconstructs the\nsemantic content of the mixed audio signal by enriching the distinct semantic\ninformation of each individual source, deriving representations that preserve\nthe unique characteristics of each sound. To reduce the matching difficulty, we\nintroduce a discriminative feature learning module, which enhances the semantic\ndistinctiveness of generated audio representations. Considering that not all\nderived audio representations directly correspond to visual features (e.g.,\noff-screen sounds), we propose a dynamic elimination module to filter out\nnon-matching elements. This module facilitates targeted interaction between\nsounding regions and relevant audio semantics. By scoring the interacted\nfeatures, we identify and filter out irrelevant audio information, ensuring\naccurate audio-visual alignment. Comprehensive experiments demonstrate that our\nframework achieves superior performance in AVS datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.12847", "pdf": "https://arxiv.org/pdf/2503.12847", "abs": "https://arxiv.org/abs/2503.12847", "authors": ["Chen Liu", "Peike Li", "Liying Yang", "Dadong Wang", "Lincheng Li", "Xin Yu"], "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Accurately localizing audible objects based on audio-visual cues is the core\nobjective of audio-visual segmentation. Most previous methods emphasize spatial\nor temporal multi-modal modeling, yet overlook challenges from ambiguous\naudio-visual correspondences such as nearby visually similar but acoustically\ndifferent objects and frequent shifts in objects' sounding status.\nConsequently, they may struggle to reliably correlate audio and visual cues,\nleading to over- or under-segmentation. To address these limitations, we\npropose a novel framework with two primary components: an audio-guided modality\nalignment (AMA) module and an uncertainty estimation (UE) module. Instead of\nindiscriminately correlating audio-visual cues through a global attention\nmechanism, AMA performs audio-visual interactions within multiple groups and\nconsolidates group features into compact representations based on their\nresponsiveness to audio cues, effectively directing the model's attention to\naudio-relevant areas. Leveraging contrastive learning, AMA further\ndistinguishes sounding regions from silent areas by treating features with\nstrong audio responses as positive samples and weaker responses as negatives.\nAdditionally, UE integrates spatial and temporal information to identify\nhigh-uncertainty regions caused by frequent changes in sound state, reducing\nprediction errors by lowering confidence in these areas. Experimental results\ndemonstrate that our approach achieves superior accuracy compared to existing\nstate-of-the-art methods, particularly in challenging scenarios where\ntraditional approaches struggle to maintain reliable segmentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13090", "pdf": "https://arxiv.org/pdf/2503.13090", "abs": "https://arxiv.org/abs/2503.13090", "authors": ["Václav Truhlařík", "Tomáš Pivoňka", "Michal Kasarda", "Libor Přeučil"], "title": "Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition Based on Deep-Learned Local Features", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages, 5 figures", "summary": "Uniform and variable environments still remain a challenge for stable visual\nlocalization and mapping in mobile robot navigation. One of the possible\napproaches suitable for such environments is appearance-based teach-and-repeat\nnavigation, relying on simplified localization and reactive robot motion\ncontrol - all without a need for standard mapping. This work brings an\ninnovative solution to such a system based on visual place recognition\ntechniques. Here, the major contributions stand in the employment of a new\nvisual place recognition technique, a novel horizontal shift computation\napproach, and a multi-platform system design for applications across various\ntypes of mobile robots. Secondly, a new public dataset for experimental testing\nof appearance-based navigation methods is introduced. Moreover, the work also\nprovides real-world experimental testing and performance comparison of the\nintroduced navigation system against other state-of-the-art methods. The\nresults confirm that the new system outperforms existing methods in several\ntesting scenarios, is capable of operation indoors and outdoors, and exhibits\nrobustness to day and night scene variations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-18.jsonl"}
{"id": "2503.13369", "pdf": "https://arxiv.org/pdf/2503.13369", "abs": "https://arxiv.org/abs/2503.13369", "authors": ["Wan Ju Kang", "Eunki Kim", "Na Min An", "Sangryul Kim", "Haemin Choi", "Ki Hoon Kwak", "James Thorne"], "title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "37 pages, 10 figures, 21 tables", "summary": "Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-18.jsonl"}
