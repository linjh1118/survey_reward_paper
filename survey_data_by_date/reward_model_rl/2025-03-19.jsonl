{"id": "2503.14189", "pdf": "https://arxiv.org/pdf/2503.14189", "abs": "https://arxiv.org/abs/2503.14189", "authors": ["Yongqi Li", "Lu Yang", "Jian Wang", "Runyang You", "Wenjie Li", "Liqiang Nie"], "title": "Towards Harmless Multimodal Assistants with Blind Preference Optimization", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in multimodal understanding, reasoning, and interaction. Given the\nextensive applications of MLLMs, the associated safety issues have become\nincreasingly critical. Due to the effectiveness of preference optimization in\naligning MLLMs with human preferences, there is an urgent need for\nsafety-related preference data for MLLMs. To address this, we construct the\nMMSafe-PO preference dataset towards harmless multimodal assistants, featuring\nmultimodal instructions, the conversational format, and ranked paired responses\nfrom human feedback. We also identify two insightful observations: modality\nco-defense and modality cheating, which illustrate that MLLMs possess a certain\nlevel of inherent defense while still presenting unique safety challenges.\nBased on these observations, we propose the Blind Preference Optimization (BPO)\napproach. Comprehensive experiments on three benchmarks show that BPO\neffectively enhances the safety capabilities of MLLMs. Notably, BPO\nsignificantly improves the safety rate of the base MLLM by 45.0%, outperforming\nthe DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly\nreduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on\nMM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and\nrobustness of both the dataset and the approach. We release code and data at\nhttps://lu-yang666.github.io/MMsafe-PO-Web/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "safety"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13939", "pdf": "https://arxiv.org/pdf/2503.13939", "abs": "https://arxiv.org/abs/2503.13939", "authors": ["Yuxiang Lai", "Jike Zhong", "Ming Li", "Shitian Zhao", "Xiaofeng Yang"], "title": "Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have advanced reasoning in natural scenes, but\ntheir role in medical imaging remains underexplored. Medical reasoning tasks\ndemand robust image analysis and well-justified answers, posing challenges due\nto the complexity of medical images. Transparency and trustworthiness are\nessential for clinical adoption and regulatory compliance. We introduce Med-R1,\na framework exploring reinforcement learning (RL) to enhance VLMs'\ngeneralizability and trustworthiness in medical reasoning. Leveraging the\nDeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide\nreasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which\noften overfits and lacks generalization, RL fosters robust and diverse\nreasoning. Med-R1 is evaluated across eight medical imaging modalities: CT,\nMRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography\n(OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B,\nMed-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B,\nwhich has 36 times more parameters. Testing across five question types-modality\nrecognition, anatomy identification, disease diagnosis, lesion grading, and\nbiological attribute analysis Med-R1 demonstrates superior generalization,\nexceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type\ngeneralization. These findings show that RL improves medical reasoning and\nenables parameter-efficient models to outperform significantly larger ones.\nWith interpretable reasoning outputs, Med-R1 represents a promising step toward\ngeneralizable, trustworthy, and clinically viable medical VLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14329", "pdf": "https://arxiv.org/pdf/2503.14329", "abs": "https://arxiv.org/abs/2503.14329", "authors": ["Yufei Zhu", "Yiming Zhong", "Zemin Yang", "Peishan Cong", "Jingyi Yu", "Xinge Zhu", "Yuexin Ma"], "title": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "reliability"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14504", "pdf": "https://arxiv.org/pdf/2503.14504", "abs": "https://arxiv.org/abs/2503.14504", "authors": ["Tao Yu", "Yi-Fan Zhangâ€ ", "Chaoyou Fu", "Junkang Wu", "Jinda Lu", "Kun Wang", "Xingyu Lu", "Yunhang Shen", "Guibin Zhang", "Dingjie Song", "Yibo Yan", "Tianlong Xu", "Qingsong Wen", "Zhang Zhang", "Yan Huang", "Liang Wang", "Tieniu Tan"], "title": "Aligning Multimodal LLM with Human Preference: A Survey", "categories": ["cs.CV"], "comment": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment", "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "truthfulness", "safety"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13587", "pdf": "https://arxiv.org/pdf/2503.13587", "abs": "https://arxiv.org/abs/2503.13587", "authors": ["Dingkang Liang", "Dingyuan Zhang", "Xin Zhou", "Sifan Tu", "Tianrui Feng", "Xiaofan Li", "Yumeng Zhang", "Mingyang Du", "Xiao Tan", "Xiang Bai"], "title": "Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception", "categories": ["cs.CV"], "comment": "The project page is at https://github.com/dk-liang/UniFuture", "summary": "We present UniFuture, a simple yet effective driving world model that\nseamlessly integrates future scene generation and perception within a single\nframework. Unlike existing models focusing solely on pixel-level future\nprediction or geometric reasoning, our approach jointly models future\nappearance (i.e., RGB image) and geometry (i.e., depth), ensuring coherent\npredictions. Specifically, during the training, we first introduce a\nDual-Latent Sharing scheme, which transfers image and depth sequence in a\nshared latent space, allowing both modalities to benefit from shared feature\nlearning. Additionally, we propose a Multi-scale Latent Interaction mechanism,\nwhich facilitates bidirectional refinement between image and depth features at\nmultiple spatial scales, effectively enhancing geometry consistency and\nperceptual alignment. During testing, our UniFuture can easily predict\nhigh-consistency future image-depth pairs by only using the current image as\ninput. Extensive experiments on the nuScenes dataset demonstrate that UniFuture\noutperforms specialized models on future generation and perception tasks,\nhighlighting the advantages of a unified, structurally-aware world model. The\nproject page is at https://github.com/dk-liang/UniFuture.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13739", "pdf": "https://arxiv.org/pdf/2503.13739", "abs": "https://arxiv.org/abs/2503.13739", "authors": ["Keqi Chen", "Vinkle Srivastav", "Didier Mutter", "Nicolas Padoy"], "title": "Learning from Synchronization: Self-Supervised Uncalibrated Multi-View Person Association in Challenging Scenes", "categories": ["cs.CV"], "comment": "Accepted for CVPR 2025. Code:\n  https://github.com/CAMMA-public/Self-MVA", "summary": "Multi-view person association is a fundamental step towards multi-view\nanalysis of human activities. Although the person re-identification features\nhave been proven effective, they become unreliable in challenging scenes where\npersons share similar appearances. Therefore, cross-view geometric constraints\nare required for a more robust association. However, most existing approaches\nare either fully-supervised using ground-truth identity labels or require\ncalibrated camera parameters that are hard to obtain. In this work, we\ninvestigate the potential of learning from synchronization, and propose a\nself-supervised uncalibrated multi-view person association approach, Self-MVA,\nwithout using any annotations. Specifically, we propose a self-supervised\nlearning framework, consisting of an encoder-decoder model and a\nself-supervised pretext task, cross-view image synchronization, which aims to\ndistinguish whether two images from different views are captured at the same\ntime. The model encodes each person's unified geometric and appearance\nfeatures, and we train it by utilizing synchronization labels for supervision\nafter applying Hungarian matching to bridge the gap between instance-wise and\nimage-wise distances. To further reduce the solution space, we propose two\ntypes of self-supervised linear constraints: multi-view re-projection and\npairwise edge association. Extensive experiments on three challenging public\nbenchmark datasets (WILDTRACK, MVOR, and SOLDIERS) show that our approach\nachieves state-of-the-art results, surpassing existing unsupervised and\nfully-supervised approaches. Code is available at\nhttps://github.com/CAMMA-public/Self-MVA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13756", "pdf": "https://arxiv.org/pdf/2503.13756", "abs": "https://arxiv.org/abs/2503.13756", "authors": ["Yunpeng Shi", "Amit Singer", "Eric J. Verbeke"], "title": "Fast alignment of heterogeneous images in sliced Wasserstein distance", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Many applications of computer vision rely on the alignment of similar but\nnon-identical images. We present a fast algorithm for aligning heterogeneous\nimages based on optimal transport. Our approach combines the speed of fast\nFourier methods with the robustness of sliced probability metrics and allows us\nto efficiently compute the alignment between two $L \\times L$ images using the\nsliced 2-Wasserstein distance in $O(L^2 \\log L)$ operations. We show that our\nmethod is robust to translations, rotations and deformations in the images.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13816", "pdf": "https://arxiv.org/pdf/2503.13816", "abs": "https://arxiv.org/abs/2503.13816", "authors": ["Zhixuan Liu", "Haokun Zhu", "Rui Chen", "Jonathan Francis", "Soonmin Hwang", "Ji Zhang", "Jean Oh"], "title": "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel diffusion-based approach for generating\nprivacy-preserving digital twins of multi-room indoor environments from depth\nimages only. Central to our approach is a novel Multi-view Overlapped Scene\nAlignment with Implicit Consistency (MOSAIC) model that explicitly considers\ncross-view dependencies within the same scene in the probabilistic sense.\nMOSAIC operates through a novel inference-time optimization that avoids error\naccumulation common in sequential or single-room constraint in panorama-based\napproaches. MOSAIC scales to complex scenes with zero extra training and\nprovably reduces the variance during denoising processes when more overlapping\nviews are added, leading to improved generation quality. Experiments show that\nMOSAIC outperforms state-of-the-art baselines on image fidelity metrics in\nreconstructing complex multi-room environments. Project page is available at:\nhttps://mosaic-cmubig.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13821", "pdf": "https://arxiv.org/pdf/2503.13821", "abs": "https://arxiv.org/abs/2503.13821", "authors": ["Chi Hsuan Wu", "Kumar Ashutosh", "Kristen Grauman"], "title": "Stitch-a-Recipe: Video Demonstration from Multistep Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "When obtaining visual illustrations from text descriptions, today's methods\ntake a description with-a single text context caption, or an action\ndescription-and retrieve or generate the matching visual context. However,\nprior work does not permit visual illustration of multistep descriptions, e.g.\na cooking recipe composed of multiple steps. Furthermore, simply handling each\nstep description in isolation would result in an incoherent demonstration. We\npropose Stitch-a-Recipe, a novel retrieval-based method to assemble a video\ndemonstration from a multistep description. The resulting video contains clips,\npossibly from different sources, that accurately reflect all the step\ndescriptions, while being visually coherent. We formulate a training pipeline\nthat creates large-scale weakly supervised data containing diverse and novel\nrecipes and injects hard negatives that promote both correctness and coherence.\nValidated on in-the-wild instructional videos, Stitch-a-Recipe achieves\nstate-of-the-art performance, with quantitative gains up to 24% as well as\ndramatic wins in a human preference study.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13836", "pdf": "https://arxiv.org/pdf/2503.13836", "abs": "https://arxiv.org/abs/2503.13836", "authors": ["Seokhyeon Hong", "Chaelin Kim", "Serin Yoon", "Junghyun Nam", "Sihun Cha", "Junyong Noh"], "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "CVPR 2025; Project page\n  https://seokhyeonhong.github.io/projects/salad/", "summary": "Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13859", "pdf": "https://arxiv.org/pdf/2503.13859", "abs": "https://arxiv.org/abs/2503.13859", "authors": ["Jinseok Bae", "Inwoo Hwang", "Young Yoon Lee", "Ziyu Guo", "Joseph Liu", "Yizhak Ben-Shabat", "Young Min Kim", "Mubbasir Kapadia"], "title": "Less is More: Improving Motion Diffusion Models with Sparse Keyframes", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in motion diffusion models have led to remarkable progress in\ndiverse motion generation tasks, including text-to-motion synthesis. However,\nexisting approaches represent motions as dense frame sequences, requiring the\nmodel to process redundant or less informative frames. The processing of dense\nanimation frames imposes significant training complexity, especially when\nlearning intricate distributions of large motion datasets even with modern\nneural architectures. This severely limits the performance of generative motion\nmodels for downstream tasks. Inspired by professional animators who mainly\nfocus on sparse keyframes, we propose a novel diffusion framework explicitly\ndesigned around sparse and geometrically meaningful keyframes. Our method\nreduces computation by masking non-keyframes and efficiently interpolating\nmissing frames. We dynamically refine the keyframe mask during inference to\nprioritize informative frames in later diffusion steps. Extensive experiments\nshow that our approach consistently outperforms state-of-the-art methods in\ntext alignment and motion realism, while also effectively maintaining high\nperformance at significantly fewer diffusion steps. We further validate the\nrobustness of our framework by using it as a generative prior and adapting it\nto different downstream tasks. Source code and pre-trained models will be\nreleased upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13862", "pdf": "https://arxiv.org/pdf/2503.13862", "abs": "https://arxiv.org/abs/2503.13862", "authors": ["Jiaqi Yang", "Wenting Chen", "Xiaohan Xing", "Sean He", "Xiaoling Luo", "Xinheng Lyu", "Linlin Shen", "Guoping Qiu"], "title": "HySurvPred: Multimodal Hyperbolic Embedding with Angle-Aware Hierarchical Contrastive Learning and Uncertainty Constraints for Survival Prediction", "categories": ["cs.CV", "cs.LG"], "comment": "submitted to IJCAI2025", "summary": "Multimodal learning that integrates histopathology images and genomic data\nholds great promise for cancer survival prediction. However, existing methods\nface key limitations: 1) They rely on multimodal mapping and metrics in\nEuclidean space, which cannot fully capture the hierarchical structures in\nhistopathology (among patches from different resolutions) and genomics data\n(from genes to pathways). 2) They discretize survival time into independent\nrisk intervals, which ignores its continuous and ordinal nature and fails to\nachieve effective optimization. 3) They treat censorship as a binary indicator,\nexcluding censored samples from model optimization and not making full use of\nthem. To address these challenges, we propose HySurvPred, a novel framework for\nsurvival prediction that integrates three key modules: Multimodal Hyperbolic\nMapping (MHM), Angle-aware Ranking-based Contrastive Loss (ARCL) and\nCensor-Conditioned Uncertainty Constraint (CUC). Instead of relying on\nEuclidean space, we design the MHM module to explore the inherent hierarchical\nstructures within each modality in hyperbolic space. To better integrate\nmultimodal features in hyperbolic space, we introduce the ARCL module, which\nuses ranking-based contrastive learning to preserve the ordinal nature of\nsurvival time, along with the CUC module to fully explore the censored data.\nExtensive experiments demonstrate that our method outperforms state-of-the-art\nmethods on five benchmark datasets. The source code is to be released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13946", "pdf": "https://arxiv.org/pdf/2503.13946", "abs": "https://arxiv.org/abs/2503.13946", "authors": ["Kang Yang", "Tianci Bu", "Lantao Li", "Chunxu Li", "Yongcai Wang", "Deying Li"], "title": "Is Discretization Fusion All You Need for Collaborative Perception?", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception in multi-agent system enhances overall perceptual\ncapabilities by facilitating the exchange of complementary information among\nagents. Current mainstream collaborative perception methods rely on discretized\nfeature maps to conduct fusion, which however, lacks flexibility in extracting\nand transmitting the informative features and can hardly focus on the\ninformative features during fusion. To address these problems, this paper\nproposes a novel Anchor-Centric paradigm for Collaborative Object detection\n(ACCO). It avoids grid precision issues and allows more flexible and efficient\nanchor-centric communication and fusion. ACCO is composed by three main\ncomponents: (1) Anchor featuring block (AFB) that targets to generate anchor\nproposals and projects prepared anchor queries to image features. (2) Anchor\nconfidence generator (ACG) is designed to minimize communication by selecting\nonly the features in the confident anchors to transmit. (3) A local-global\nfusion module, in which local fusion is anchor alignment-based fusion (LAAF)\nand global fusion is conducted by spatial-aware cross-attention (SACA). LAAF\nand SACA run in multi-layers, so agents conduct anchor-centric fusion\niteratively to adjust the anchor proposals. Comprehensive experiments are\nconducted to evaluate ACCO on OPV2V and Dair-V2X datasets, which demonstrate\nACCO's superiority in reducing the communication volume, and in improving the\nperception range and detection performances. Code can be found at:\n\\href{https://github.com/sidiangongyuan/ACCO}{https://github.com/sidiangongyuan/ACCO}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13947", "pdf": "https://arxiv.org/pdf/2503.13947", "abs": "https://arxiv.org/abs/2503.13947", "authors": ["Sayak Nag", "Udita Ghosh", "Sarosij Bose", "Calvin-Khang Ta", "Jiachen Li", "Amit K Roy Chowdhury"], "title": "Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Scene Graph Generation (SGG) aims to represent visual scenes by identifying\nobjects and their pairwise relationships, providing a structured understanding\nof image content. However, inherent challenges like long-tailed class\ndistributions and prediction variability necessitate uncertainty quantification\nin SGG for its practical viability. In this paper, we introduce a novel\nConformal Prediction (CP) based framework, adaptive to any existing SGG method,\nfor quantifying their predictive uncertainty by constructing well-calibrated\nprediction sets over their generated scene graphs. These scene graph prediction\nsets are designed to achieve statistically rigorous coverage guarantees.\nAdditionally, to ensure these prediction sets contain the most practically\ninterpretable scene graphs, we design an effective MLLM-based post-processing\nstrategy for selecting the most visually and semantically plausible scene\ngraphs within these prediction sets. We show that our proposed approach can\nproduce diverse possible scene graphs from an image, assess the reliability of\nSGG methods, and improve overall SGG performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14074", "pdf": "https://arxiv.org/pdf/2503.14074", "abs": "https://arxiv.org/abs/2503.14074", "authors": ["Shengping Zhang", "Xiaoyu Han", "Weigang Zhang", "Xiangyuan Lan", "Hongxun Yao", "Qingming Huang"], "title": "Limb-Aware Virtual Try-On Network with Progressive Clothing Warping", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM). The code is\n  available at https://github.com/aipixel/PL-VTONv2", "summary": "Image-based virtual try-on aims to transfer an in-shop clothing image to a\nperson image. Most existing methods adopt a single global deformation to\nperform clothing warping directly, which lacks fine-grained modeling of in-shop\nclothing and leads to distorted clothing appearance. In addition, existing\nmethods usually fail to generate limb details well because they are limited by\nthe used clothing-agnostic person representation without referring to the limb\ntextures of the person image. To address these problems, we propose Limb-aware\nVirtual Try-on Network named PL-VTON, which performs fine-grained clothing\nwarping progressively and generates high-quality try-on results with realistic\nlimb details. Specifically, we present Progressive Clothing Warping (PCW) that\nexplicitly models the location and size of in-shop clothing and utilizes a\ntwo-stage alignment strategy to progressively align the in-shop clothing with\nthe human body. Moreover, a novel gravity-aware loss that considers the fit of\nthe person wearing clothing is adopted to better handle the clothing edges.\nThen, we design Person Parsing Estimator (PPE) with a non-limb target parsing\nmap to semantically divide the person into various regions, which provides\nstructural constraints on the human body and therefore alleviates texture\nbleeding between clothing and body regions. Finally, we introduce Limb-aware\nTexture Fusion (LTF) that focuses on generating realistic details in limb\nregions, where a coarse try-on result is first generated by fusing the warped\nclothing image with the person image, then limb textures are further fused with\nthe coarse result under limb-aware guidance to refine limb details. Extensive\nexperiments demonstrate that our PL-VTON outperforms the state-of-the-art\nmethods both qualitatively and quantitatively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14140", "pdf": "https://arxiv.org/pdf/2503.14140", "abs": "https://arxiv.org/abs/2503.14140", "authors": ["Zining Wang", "Tongkun Guan", "Pei Fu", "Chen Duan", "Qianyi Jiang", "Zhentao Guo", "Shan Guo", "Junfeng Luo", "Wei Shen", "Xiaokang Yang"], "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "dimension"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14150", "pdf": "https://arxiv.org/pdf/2503.14150", "abs": "https://arxiv.org/abs/2503.14150", "authors": ["Yihang Zhou", "Ruige Kong", "Zhengsen Xu", "Linlin Xu", "Sibo Cheng"], "title": "Comparative and Interpretative Analysis of CNN and Transformer Models in Predicting Wildfire Spread Using Remote Sensing Data", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14295", "pdf": "https://arxiv.org/pdf/2503.14295", "abs": "https://arxiv.org/abs/2503.14295", "authors": ["Baiqin Wang", "Xiangyu Zhu", "Fan Shen", "Hao Xu", "Zhen Lei"], "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14358", "pdf": "https://arxiv.org/pdf/2503.14358", "abs": "https://arxiv.org/abs/2503.14358", "authors": ["Chao Wang", "Giulio Franzese", "Alessandro Finamore", "Pietro Michiardi"], "title": "RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "to appear at ICLR 2025 Workshop on Deep Generative Model in Machine\n  Learning: Theory, Principle and Efficacy", "summary": "Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14494", "pdf": "https://arxiv.org/pdf/2503.14494", "abs": "https://arxiv.org/abs/2503.14494", "authors": ["Inkyu Shin", "Chenglin Yang", "Liang-Chieh Chen"], "title": "Deeply Supervised Flow-Based Generative Models", "categories": ["cs.CV"], "comment": "Project website at https://deepflow-project.github.io/", "summary": "Flow based generative models have charted an impressive path across multiple\nvisual generation tasks by adhering to a simple principle: learning velocity\nrepresentations of a linear interpolant. However, we observe that training\nvelocity solely from the final layer output underutilizes the rich inter layer\nrepresentations, potentially impeding model convergence. To address this\nlimitation, we introduce DeepFlow, a novel framework that enhances velocity\nrepresentation through inter layer communication. DeepFlow partitions\ntransformer layers into balanced branches with deep supervision and inserts a\nlightweight Velocity Refiner with Acceleration (VeRA) block between adjacent\nbranches, which aligns the intermediate velocity features within transformer\nblocks. Powered by the improved deep supervision via the internal velocity\nalignment, DeepFlow converges 8 times faster on ImageNet with equivalent\nperformance and further reduces FID by 2.6 while halving training time compared\nto previous flow based models without a classifier free guidance. DeepFlow also\noutperforms baselines in text to image generation tasks, as evidenced by\nevaluations on MSCOCO and zero shot GenEval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13469", "pdf": "https://arxiv.org/pdf/2503.13469", "abs": "https://arxiv.org/abs/2503.13469", "authors": ["Ivan Sviridov", "Konstantin Egorov"], "title": "Conditional Electrocardiogram Generation Using Hierarchical Variational Autoencoders", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "10 pages, 6 figures, 7 tables", "summary": "Cardiovascular diseases (CVDs) are disorders impacting the heart and\ncirculatory system. These disorders are the foremost and continuously\nescalating cause of mortality worldwide. One of the main tasks when working\nwith CVDs is analyzing and identifying pathologies on a 12-lead\nelectrocardiogram (ECG) with a standard 10-second duration. Using machine\nlearning (ML) in automatic ECG analysis increases CVD diagnostics'\navailability, speed, and accuracy. However, the most significant difficulty in\ndeveloping ML models is obtaining a sufficient training dataset. Due to the\nlimitations of medical data usage, such as expensiveness, errors, the ambiguity\nof labels, imbalance of classes, and privacy issues, utilizing synthetic\nsamples depending on specific pathologies bypasses these restrictions and\nimproves algorithm quality. Existing solutions for the conditional generation\nof ECG signals are mainly built on Generative Adversarial Networks (GANs), and\nonly a few papers consider the architectures based on Variational Autoencoders\n(VAEs), showing comparable results in recent works. This paper proposes the\npublicly available conditional Nouveau VAE model for ECG signal generation\n(cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We\nprovide an extensive comparison of the proposed model on various practical\ndownstream tasks, including transfer learning scenarios showing an area under\nthe receiver operating characteristic (AUROC) increase up to 2% surpassing\nGAN-like competitors.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.13896", "pdf": "https://arxiv.org/pdf/2503.13896", "abs": "https://arxiv.org/abs/2503.13896", "authors": ["Yi Yang", "Xuran Zhao", "H. Charles Zhao", "Shumin Yuan", "Samuel M. Bateman", "Tiffany A. Huang", "Chris Beall", "Will Maddern"], "title": "Evaluating Global Geo-alignment for Precision Learned Autonomous Vehicle Localization using Aerial Data", "categories": ["cs.RO", "cs.CV", "I.2.9"], "comment": "8 pages, 7 figures, accepted by International Conference on Robotics\n  and Automation (ICRA) 2025", "summary": "Recently there has been growing interest in the use of aerial and satellite\nmap data for autonomous vehicles, primarily due to its potential for\nsignificant cost reduction and enhanced scalability. Despite the advantages,\naerial data also comes with challenges such as a sensor-modality gap and a\nviewpoint difference gap. Learned localization methods have shown promise for\novercoming these challenges to provide precise metric localization for\nautonomous vehicles. Most learned localization methods rely on coarsely aligned\nground truth, or implicit consistency-based methods to learn the localization\ntask -- however, in this paper we find that improving the alignment between\naerial data and autonomous vehicle sensor data at training time is critical to\nthe performance of a learning-based localization system. We compare two data\nalignment methods using a factor graph framework and, using these methods, we\nthen evaluate the effects of closely aligned ground truth on learned\nlocalization accuracy through ablation studies. Finally, we evaluate a learned\nlocalization system using the data alignment methods on a comprehensive\n(1600km) autonomous vehicle dataset and demonstrate localization error below\n0.3m and 0.5$^{\\circ}$ sufficient for autonomous vehicle applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14040", "pdf": "https://arxiv.org/pdf/2503.14040", "abs": "https://arxiv.org/abs/2503.14040", "authors": ["Binjie Liu", "Lina Liu", "Sanyi Zhang", "Songen Gu", "Yihao Zhi", "Tianyi Zhu", "Lei Yang", "Long Ye"], "title": "MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": null, "summary": "This work focuses on full-body co-speech gesture generation. Existing methods\ntypically employ an autoregressive model accompanied by vector-quantized tokens\nfor gesture generation, which results in information loss and compromises the\nrealism of the generated gestures. To address this, inspired by the natural\ncontinuity of real-world human motion, we propose MAG, a novel multi-modal\naligned framework for high-quality and diverse co-speech gesture synthesis\nwithout relying on discrete tokenization. Specifically, (1) we introduce a\nmotion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages\npre-trained WavCaps' text and audio embeddings to enhance both semantic and\nrhythmic alignment with motion, ultimately producing more realistic gestures.\n(2) Building on this, we propose a multimodal masked autoregressive model\n(MMAG) that enables autoregressive modeling in continuous motion embeddings\nthrough diffusion without vector quantization. To further ensure multi-modal\nconsistency, MMAG incorporates a hybrid granularity audio-text fusion block,\nwhich serves as conditioning for diffusion process. Extensive experiments on\ntwo benchmark datasets demonstrate that MAG achieves stateof-the-art\nperformance both quantitatively and qualitatively, producing highly realistic\nand diverse co-speech gestures.The code will be released to facilitate future\nresearch.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14094", "pdf": "https://arxiv.org/pdf/2503.14094", "abs": "https://arxiv.org/abs/2503.14094", "authors": ["Roman Denkin", "Orcun Goksel"], "title": "Image-Based Metrics in Ultrasound for Estimation of Global Speed-of-Sound", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Accurate speed-of-sound (SoS) estimation is crucial for ultrasound image\nformation, yet conventional systems often rely on an assumed value for imaging.\nWhile several methods exist for SoS estimation, they typically depend on\ncomplex physical models of acoustic propagation. We propose to leverage\nconventional image analysis techniques and metrics, as a novel and simple\napproach to estimate tissue SoS. We study eleven metrics in three categories\nfor assessing image quality, image similarity and multi-frame variation, by\ntesting them in numerical simulations and phantom experiments. Among\nsingle-frame image quality metrics, conventional Focus and our proposed\nSmoothed Threshold Tenengrad metrics achieved satisfactory accuracy, however\nonly when applied to compounded images. Image quality metrics were largely\nsurpassed by various image comparison metrics, which exhibited errors\nconsistently under 8 m/s even applied to a single pair of images. Particularly,\nMean Square Error is a computationally efficient alternative for global\nestimation. Mutual Information and Correlation are found to be robust to\nprocessing small image segments, making them suitable, e.g., for multi-layer\nSoS estimation. The above metrics do not require access to raw channel data as\nthey can operate on post-beamformed data, and in the case of image quality\nmetrics they can operate on B-mode images, given that the beamforming SoS can\nbe controlled for beamforming using a multitude of values. These image analysis\nbased SoS estimation methods offer a computationally efficient and\ndata-accessible alternative to conventional physics-based methods, with\npotential extensions to layered or local SoS imaging.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14229", "pdf": "https://arxiv.org/pdf/2503.14229", "abs": "https://arxiv.org/abs/2503.14229", "authors": ["Yifei Dong", "Fengyi Wu", "Qi He", "Heng Li", "Minghan Li", "Zebang Cheng", "Yuxuan Zhou", "Jingdong Sun", "Qi Dai", "Zhi-Qi Cheng", "Alexander G Hauptmann"], "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "27 pages, website: https://ha-vln-project.vercel.app/", "summary": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-19.jsonl"}
{"id": "2503.14395", "pdf": "https://arxiv.org/pdf/2503.14395", "abs": "https://arxiv.org/abs/2503.14395", "authors": ["Jing Wang", "Ruirui Liu", "Yu Lei", "Michael J. Baine", "Tian Liu", "Yang Lei"], "title": "Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical\nfor real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We\npresent a weakly supervised spatial implicit neural representation (SINR)\nmethod to address modality differences and pelvic anatomy challenges.\n  Methods: The framework uses sparse surface supervision from MRI/US\nsegmentations instead of dense intensity matching. SINR models deformations as\ncontinuous spatial functions, with patient-specific surface priors guiding a\nstationary velocity field for biologically plausible deformations. Validation\nincluded 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases,\nevaluated via Dice similarity coefficient (DSC), mean surface distance (MSD),\nand 95% Hausdorff distance (HD95).\n  Results: The proposed method achieved robust registration. For the public\ndataset, prostate DSC was $0.93 \\pm 0.05$, MSD $0.87 \\pm 0.10$ mm, and HD95\n$1.58 \\pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC\n$0.88 \\pm 0.09$, MSD $1.21 \\pm 0.38$ mm, and HD95 $2.09 \\pm 1.48$ mm. Bladder\nand rectum performance was lower due to ultrasound's limited field of view.\nVisual assessments confirmed accurate alignment with minimal discrepancies.\n  Conclusion: This study introduces a novel weakly supervised SINR-based\napproach for 3D MRI-US deformable registration. By leveraging sparse surface\nsupervision and spatial priors, it achieves accurate, robust, and\ncomputationally efficient registration, enhancing real-time image guidance in\nHDR prostate brachytherapy and improving treatment precision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-19.jsonl"}
