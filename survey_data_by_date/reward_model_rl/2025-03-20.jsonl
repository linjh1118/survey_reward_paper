{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14554", "pdf": "https://arxiv.org/pdf/2503.14554", "abs": "https://arxiv.org/abs/2503.14554", "authors": ["Ali Parsaee", "Fahim Shahriar", "Chuxin He", "Ruiqing Tan"], "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference", "summary": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "comparison"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14554", "pdf": "https://arxiv.org/pdf/2503.14554", "abs": "https://arxiv.org/abs/2503.14554", "authors": ["Ali Parsaee", "Fahim Shahriar", "Chuxin He", "Ruiqing Tan"], "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference", "summary": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "comparison"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15003", "pdf": "https://arxiv.org/pdf/2503.15003", "abs": "https://arxiv.org/abs/2503.15003", "authors": ["Amr Keleg"], "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?", "categories": ["cs.CL"], "comment": "Accepted to the C3NLP workshop (Co-located with NAACL 2025)", "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14824", "pdf": "https://arxiv.org/pdf/2503.14824", "abs": "https://arxiv.org/abs/2503.14824", "authors": ["Zikun Zhou", "Yushuai Sun", "Wenjie Pei", "Xin Li", "Yaowei Wang"], "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning", "categories": ["cs.CV"], "comment": null, "summary": "The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14868", "pdf": "https://arxiv.org/pdf/2503.14868", "abs": "https://arxiv.org/abs/2503.14868", "authors": ["Hoigi Seo", "Wongi Jeong", "Kyungryeol Lee", "Se Young Chun"], "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14910", "pdf": "https://arxiv.org/pdf/2503.14910", "abs": "https://arxiv.org/abs/2503.14910", "authors": ["Jingyi Liao", "Xun Xu", "Yongyi Su", "Rong-Cheng Tu", "Yifan Liu", "Dacheng Tao", "Xulei Yang"], "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15484", "pdf": "https://arxiv.org/pdf/2503.15484", "abs": "https://arxiv.org/abs/2503.15484", "authors": ["Taylor Sorensen", "Pushkar Mishra", "Roma Patel", "Michael Henry Tessler", "Michiel Bakker", "Georgina Evans", "Iason Gabriel", "Noah Goodman", "Verena Rieser"], "title": "Value Profiles for Encoding Human Variation", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Àlex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14975", "pdf": "https://arxiv.org/pdf/2503.14975", "abs": "https://arxiv.org/abs/2503.14975", "authors": ["Zihan Cao", "Yu Zhong", "Liang-Jian Deng"], "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15005", "pdf": "https://arxiv.org/pdf/2503.15005", "abs": "https://arxiv.org/abs/2503.15005", "authors": ["Shengqiong Wu", "Hao Fei", "Tat-Seng Chua"], "title": "Universal Scene Graph Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Àlex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15361", "pdf": "https://arxiv.org/pdf/2503.15361", "abs": "https://arxiv.org/abs/2503.15361", "authors": ["Qingsen Yan", "Tao Hu", "Genggeng Chen", "Wei Dong", "Yanning Zhang"], "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14637", "pdf": "https://arxiv.org/pdf/2503.14637", "abs": "https://arxiv.org/abs/2503.14637", "authors": ["Merkourios Simos", "Alberto Silvio Chiappa", "Alexander Mathis"], "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15352", "pdf": "https://arxiv.org/pdf/2503.15352", "abs": "https://arxiv.org/abs/2503.15352", "authors": ["Abhi Kamboj", "Minh N. Do"], "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15003", "pdf": "https://arxiv.org/pdf/2503.15003", "abs": "https://arxiv.org/abs/2503.15003", "authors": ["Amr Keleg"], "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?", "categories": ["cs.CL"], "comment": "Accepted to the C3NLP workshop (Co-located with NAACL 2025)", "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14824", "pdf": "https://arxiv.org/pdf/2503.14824", "abs": "https://arxiv.org/abs/2503.14824", "authors": ["Zikun Zhou", "Yushuai Sun", "Wenjie Pei", "Xin Li", "Yaowei Wang"], "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning", "categories": ["cs.CV"], "comment": null, "summary": "The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14868", "pdf": "https://arxiv.org/pdf/2503.14868", "abs": "https://arxiv.org/abs/2503.14868", "authors": ["Hoigi Seo", "Wongi Jeong", "Kyungryeol Lee", "Se Young Chun"], "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14910", "pdf": "https://arxiv.org/pdf/2503.14910", "abs": "https://arxiv.org/abs/2503.14910", "authors": ["Jingyi Liao", "Xun Xu", "Yongyi Su", "Rong-Cheng Tu", "Yifan Liu", "Dacheng Tao", "Xulei Yang"], "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15484", "pdf": "https://arxiv.org/pdf/2503.15484", "abs": "https://arxiv.org/abs/2503.15484", "authors": ["Taylor Sorensen", "Pushkar Mishra", "Roma Patel", "Michael Henry Tessler", "Michiel Bakker", "Georgina Evans", "Iason Gabriel", "Noah Goodman", "Verena Rieser"], "title": "Value Profiles for Encoding Human Variation", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Àlex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14975", "pdf": "https://arxiv.org/pdf/2503.14975", "abs": "https://arxiv.org/abs/2503.14975", "authors": ["Zihan Cao", "Yu Zhong", "Liang-Jian Deng"], "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15005", "pdf": "https://arxiv.org/pdf/2503.15005", "abs": "https://arxiv.org/abs/2503.15005", "authors": ["Shengqiong Wu", "Hao Fei", "Tat-Seng Chua"], "title": "Universal Scene Graph Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["Àlex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15361", "pdf": "https://arxiv.org/pdf/2503.15361", "abs": "https://arxiv.org/abs/2503.15361", "authors": ["Qingsen Yan", "Tao Hu", "Genggeng Chen", "Wei Dong", "Yanning Zhang"], "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14637", "pdf": "https://arxiv.org/pdf/2503.14637", "abs": "https://arxiv.org/abs/2503.14637", "authors": ["Merkourios Simos", "Alberto Silvio Chiappa", "Alexander Mathis"], "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
{"id": "2503.15352", "pdf": "https://arxiv.org/pdf/2503.15352", "abs": "https://arxiv.org/abs/2503.15352", "authors": ["Abhi Kamboj", "Minh N. Do"], "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-20.jsonl"}
