{"id": "2503.16094", "pdf": "https://arxiv.org/pdf/2503.16094", "abs": "https://arxiv.org/abs/2503.16094", "authors": ["Reem I. Masoud", "Martin Ferianc", "Philip Treleaven", "Miguel Rodrigues"], "title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15880", "pdf": "https://arxiv.org/pdf/2503.15880", "abs": "https://arxiv.org/abs/2503.15880", "authors": ["Yunan Wang", "Jijie Li", "Bo-Wen Zhang", "Liangdong Wang", "Guang Liu"], "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "Alpaca"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15784", "pdf": "https://arxiv.org/pdf/2503.15784", "abs": "https://arxiv.org/abs/2503.15784", "authors": ["Parham Saremi", "Amar Kumar", "Mohammed Mohammed", "Zahra TehraniNasab", "Tal Arbel"], "title": "RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Foundation Models (VLFM) have shown a tremendous increase in\nperformance in terms of generating high-resolution, photorealistic natural\nimages. While VLFMs show a rich understanding of semantic content across\nmodalities, they often struggle with fine-grained alignment tasks that require\nprecise correspondence between image regions and textual descriptions a\nlimitation in medical imaging, where accurate localization and detection of\nclinical features are essential for diagnosis and analysis. To address this\nissue, we propose a multi-stage architecture where a pre-trained VLFM provides\na cursory semantic understanding, while a reinforcement learning (RL) algorithm\nrefines the alignment through an iterative process that optimizes for\nunderstanding semantic context. The reward signal is designed to align the\nsemantic information of the text with synthesized images. We demonstrate the\neffectiveness of our method on a medical imaging skin dataset where the\ngenerated images exhibit improved generation quality and alignment with prompt\nover the fine-tuned Stable Diffusion. We also show that the synthesized samples\ncould be used to improve disease classifier performance for underrepresented\nsubgroups through augmentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15783", "pdf": "https://arxiv.org/pdf/2503.15783", "abs": "https://arxiv.org/abs/2503.15783", "authors": ["Tsunehiko Tanaka", "Edgar Simo-Serra"], "title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15639", "pdf": "https://arxiv.org/pdf/2503.15639", "abs": "https://arxiv.org/abs/2503.15639", "authors": ["Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal", "Cheng-Lin Liu"], "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15879", "pdf": "https://arxiv.org/pdf/2503.15879", "abs": "https://arxiv.org/abs/2503.15879", "authors": ["DongGeon Lee", "Ahjeong Park", "Hyeri Lee", "Hyeonseo Nam", "Yunho Maeng"], "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to NAACL 2025 SRW", "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https://github.com/TeamNLP/Typed-RAG}{https://github.com/TeamNLP/Typed-RAG}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering", "aspect-based"], "score": 4}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15888", "pdf": "https://arxiv.org/pdf/2503.15888", "abs": "https://arxiv.org/abs/2503.15888", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Yilong Xu", "Junfeng Fang", "Lingrui Mei", "Xueqi Cheng"], "title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15952", "pdf": "https://arxiv.org/pdf/2503.15952", "abs": "https://arxiv.org/abs/2503.15952", "authors": ["Chen Li", "Nazhou Liu", "Kai Yang"], "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning", "categories": ["cs.CL"], "comment": "This is an unfinished version and will be updated. We aim to share\n  some findings", "summary": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15676", "pdf": "https://arxiv.org/pdf/2503.15676", "abs": "https://arxiv.org/abs/2503.15676", "authors": ["Cédric Vincent", "Taehyoung Kim", "Henri Meeß"], "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation from RGB cameras is essential to the perception of\nautonomous flying vehicles. The stability of predictions through the captured\nvideos is paramount to their reliability and, by extension, to the\ntrustworthiness of the agents. In this paper, we propose a lightweight video\nsemantic segmentation approach-suited to onboard real-time inference-achieving\nhigh temporal consistency on aerial data through Semantic Similarity\nPropagation across frames. SSP temporally propagates the predictions of an\nefficient image segmentation model with global registration alignment to\ncompensate for camera movements. It combines the current estimation and the\nprior prediction with linear interpolation using weights computed from the\nfeatures similarities of the two frames. Because data availability is a\nchallenge in this domain, we propose a consistency-aware Knowledge Distillation\ntraining procedure for sparsely labeled datasets with few annotations. Using a\nlarge image segmentation model as a teacher to train the efficient SSP, we\nleverage the strong correlations between labeled and unlabeled frames in the\nsame training videos to obtain high-quality supervision on all frames. KD-SSP\nobtains a significant temporal consistency increase over the base image\nsegmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,\nwith higher accuracy and comparable inference speed. On these aerial datasets,\nKD-SSP provides a superior segmentation quality and inference speed trade-off\nthan other video methods proposed for general applications and shows\nconsiderably higher consistency. The code will be made publicly available upon\nacceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16131", "pdf": "https://arxiv.org/pdf/2503.16131", "abs": "https://arxiv.org/abs/2503.16131", "authors": ["Feiyang Li", "Yingjian Chen", "Haoran Liu", "Rui Yang", "Han Yuan", "Yuang Jiang", "Tianxiao Li", "Edison Marrese Taylor", "Hossein Rouhizadeh", "Yusuke Iwasawa", "Douglas Teodoro", "Yutaka Matsuo", "Irene Li"], "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16252", "pdf": "https://arxiv.org/pdf/2503.16252", "abs": "https://arxiv.org/abs/2503.16252", "authors": ["Zhaowei Liu", "Xin Guo", "Fangqi Lou", "Lingfeng Zeng", "Jinyi Niu", "Zixuan Wang", "Jiajie Xu", "Weige Cai", "Ziwei Yang", "Xueqian Zhao", "Chao Li", "Sheng Xu", "Dezhi Chen", "Yun Chen", "Zuo Bai", "Liwen Zhang"], "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16419", "pdf": "https://arxiv.org/pdf/2503.16419", "abs": "https://arxiv.org/abs/2503.16419", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", "Shaochen", "Zhong", "Hanjie Chen", "Xia Hu"], "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "categories": ["cs.CL"], "comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15521", "pdf": "https://arxiv.org/pdf/2503.15521", "abs": "https://arxiv.org/abs/2503.15521", "authors": ["Loukas Triantafyllopoulos", "Dimitris Kalles"], "title": "From Divergence to Consensus: Evaluating the Role of Large Language Models in Facilitating Agreement through Adaptive Strategies", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "32 pages, 5 figures, 4 tables", "summary": "Achieving consensus in group decision-making often involves overcoming\nsignificant challenges, particularly in reconciling diverse perspectives and\nmitigating biases that hinder agreement. Traditional methods relying on human\nfacilitators are often constrained by scalability and efficiency, especially in\nlarge-scale, fast-paced discussions. To address these challenges, this study\nproposes a novel framework employing large language models (LLMs) as automated\nfacilitators within a custom-built multi-user chat system. Leveraging cosine\nsimilarity as a core metric, this approach evaluates the ability of three\nstate-of-the-art LLMs- ChatGPT 4.0, Mistral Large 2, and AI21 Jamba Instruct-\nto synthesize consensus proposals that align with participants' viewpoints.\nUnlike conventional techniques, the system integrates adaptive facilitation\nstrategies, including clarifying misunderstandings, summarizing discussions,\nand proposing compromises, enabling the LLMs to iteratively refine consensus\nproposals based on user feedback. Experimental results demonstrate the\nsuperiority of ChatGPT 4.0, which achieves higher alignment with participant\nopinions, requiring fewer iterations to reach consensus compared to its\ncounterparts. Moreover, analysis reveals the nuanced performance of the models\nacross various sustainability-focused discussion topics, such as climate\naction, quality education, good health and well-being, and access to clean\nwater and sanitation. These findings highlight the transformative potential of\nLLM-driven facilitation for improving collective decision-making processes and\nunderscore the importance of advancing evaluation metrics and cross-cultural\nadaptability in future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15948", "pdf": "https://arxiv.org/pdf/2503.15948", "abs": "https://arxiv.org/abs/2503.15948", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Alexander Panchenko", "Vasily Konovalov"], "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025", "summary": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15931", "pdf": "https://arxiv.org/pdf/2503.15931", "abs": "https://arxiv.org/abs/2503.15931", "authors": ["Sidi Yang", "Binxiao Huang", "Yulun Zhang", "Dahai Yu", "Yujiu Yang", "Ngai Wong"], "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16219", "pdf": "https://arxiv.org/pdf/2503.16219", "abs": "https://arxiv.org/abs/2503.16219", "authors": ["Quy-Anh Dang", "Chris Ngo"], "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15940", "pdf": "https://arxiv.org/pdf/2503.15940", "abs": "https://arxiv.org/abs/2503.15940", "authors": ["Yaxiong Chen", "Chuang Du", "Chunlei Li", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report Generation", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Automated radiology report generation aims to expedite the tedious and\nerror-prone reporting process for radiologists. While recent works have made\nprogress, learning to align medical images and textual findings remains\nchallenging due to the relative scarcity of labeled medical data. For example,\ndatasets for this task are much smaller than those used for image captioning in\ncomputer vision. In this work, we propose to transfer representations from\nCLIP, a large-scale pre-trained vision-language model, to better capture\ncross-modal semantics between images and texts. However, directly applying CLIP\nis suboptimal due to the domain gap between natural images and radiology. To\nenable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter\nmodules that are incorporated into CLIP and fine-tuned on the target task while\nkeeping base parameters fixed. The adapters are distributed across modalities\nand their interaction to enhance vision-language alignment. Experiments on two\npublic datasets demonstrate the effectiveness of our approach, advancing\nstate-of-the-art in radiology report generation. The proposed transfer learning\nframework provides a means of harnessing semantic knowledge from large-scale\npre-trained models to tackle data-scarce medical vision-language tasks. Code is\navailable at https://github.com/chauncey-tow/MRG-CLIP.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15948", "pdf": "https://arxiv.org/pdf/2503.15948", "abs": "https://arxiv.org/abs/2503.15948", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Alexander Panchenko", "Vasily Konovalov"], "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025", "summary": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15949", "pdf": "https://arxiv.org/pdf/2503.15949", "abs": "https://arxiv.org/abs/2503.15949", "authors": ["Yaxiong Chen", "Minghong Wei", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Referring medical image segmentation targets delineating lesions indicated by\ntextual descriptions. Aligning visual and textual cues is challenging due to\ntheir distinct data properties. Inspired by large-scale pre-trained\nvision-language models, we propose CausalCLIPSeg, an end-to-end framework for\nreferring medical image segmentation that leverages CLIP. Despite not being\ntrained on medical data, we enforce CLIP's rich semantic space onto the medical\ndomain by a tailored cross-modal decoding method to achieve text-to-pixel\nalignment. Furthermore, to mitigate confounding bias that may cause the model\nto learn spurious correlations instead of meaningful causal relationships,\nCausalCLIPSeg introduces a causal intervention module which self-annotates\nconfounders and excavates causal features from inputs for segmentation\njudgments. We also devise an adversarial min-max game to optimize causal\nfeatures while penalizing confounding ones. Extensive experiments demonstrate\nthe state-of-the-art performance of our proposed method. Code is available at\nhttps://github.com/WUTCM-Lab/CausalCLIPSeg.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16064", "pdf": "https://arxiv.org/pdf/2503.16064", "abs": "https://arxiv.org/abs/2503.16064", "authors": ["Qiang Zou", "Shuli Cheng", "Jiayi Chen"], "title": "PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.MM"], "comment": "Accepted by CVPR2025", "summary": "Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16171", "pdf": "https://arxiv.org/pdf/2503.16171", "abs": "https://arxiv.org/abs/2503.16171", "authors": ["Soham Roy", "Abhishek Mishra", "Shirish Karande", "Murari Mandal"], "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Modern text-to-image generative models can inadvertently reproduce\ncopyrighted content memorized in their training data, raising serious concerns\nabout potential copyright infringement. We introduce Guardians of Generation, a\nmodel agnostic inference time framework for dynamic copyright shielding in AI\nimage generation. Our approach requires no retraining or modification of the\ngenerative model weights, instead integrating seamlessly with existing\ndiffusion pipelines. It augments the generation process with an adaptive\nguidance mechanism comprising three components: a detection module, a prompt\nrewriting module, and a guidance adjustment module. The detection module\nmonitors user prompts and intermediate generation steps to identify features\nindicative of copyrighted content before they manifest in the final output. If\nsuch content is detected, the prompt rewriting mechanism dynamically transforms\nthe user's prompt by sanitizing or replacing references that could trigger\ncopyrighted material while preserving the prompt's intended semantics. The\nadaptive guidance module adaptively steers the diffusion process away from\nflagged content by modulating the model's sampling trajectory. Together, these\ncomponents form a robust shield that enables a tunable balance between\npreserving creative fidelity and ensuring copyright compliance. We validate our\nmethod on a variety of generative models such as Stable Diffusion, SDXL, and\nFlux, demonstrating substantial reductions in copyrighted content generation\nwith negligible impact on output fidelity or alignment with user intent. This\nwork provides a practical, plug-and-play safeguard for generative image models,\nenabling more responsible deployment under real-world copyright constraints.\nSource code is available at: https://respailab.github.io/gog", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "inference time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16188", "pdf": "https://arxiv.org/pdf/2503.16188", "abs": "https://arxiv.org/abs/2503.16188", "authors": ["Ming Li", "Shitian Zhao", "Jike Zhong", "Yuxiang Lai", "Kaipeng Zhang"], "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning", "categories": ["cs.CV"], "comment": "Preprint, work in progress", "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16247", "pdf": "https://arxiv.org/pdf/2503.16247", "abs": "https://arxiv.org/abs/2503.16247", "authors": ["Max Gutbrod", "David Rauber", "Danilo Weber Nunes", "Christoph Palm"], "title": "OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The growing reliance on Artificial Intelligence (AI) in critical domains such\nas healthcare demands robust mechanisms to ensure the trustworthiness of these\nsystems, especially when faced with unexpected or anomalous inputs. This paper\nintroduces the Open Medical Imaging Benchmarks for Out-Of-Distribution\nDetection (OpenMIBOOD), a comprehensive framework for evaluating\nout-of-distribution (OOD) detection methods specifically in medical imaging\ncontexts. OpenMIBOOD includes three benchmarks from diverse medical domains,\nencompassing 14 datasets divided into covariate-shifted in-distribution,\nnear-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these\nbenchmarks, providing a standardized reference to advance the development and\nfair comparison of OOD detection methods. Results reveal that findings from\nbroad-scale OOD benchmarks in natural image domains do not translate to medical\napplications, underscoring the critical need for such benchmarks in the medical\nfield. By mitigating the risk of exposing AI models to inputs outside their\ntraining distribution, OpenMIBOOD aims to support the advancement of reliable\nand trustworthy AI systems in healthcare. The repository is available at\nhttps://github.com/remic-othr/OpenMIBOOD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16397", "pdf": "https://arxiv.org/pdf/2503.16397", "abs": "https://arxiv.org/abs/2503.16397", "authors": ["Nikita Starodubcev", "Denis Kuznedelev", "Artem Babenko", "Dmitry Baranchuk"], "title": "Scale-wise Distillation of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16399", "pdf": "https://arxiv.org/pdf/2503.16399", "abs": "https://arxiv.org/abs/2503.16399", "authors": ["Chen Chen", "Zhirui Wang", "Taowei Sheng", "Yi Jiang", "Yundu Li", "Peirui Cheng", "Luning Zhang", "Kaiqiang Chen", "Yanfeng Hu", "Xue Yang", "Xian Sun"], "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16418", "pdf": "https://arxiv.org/pdf/2503.16418", "abs": "https://arxiv.org/abs/2503.16418", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/InfiniteYou/ Code and\n  model: https://github.com/bytedance/InfiniteYou", "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15558", "pdf": "https://arxiv.org/pdf/2503.15558", "abs": "https://arxiv.org/abs/2503.15558", "authors": ["NVIDIA", ":", "Alisson Azzolini", "Hannah Brandon", "Prithvijit Chattopadhyay", "Huayu Chen", "Jinju Chu", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Francesco Ferroni", "Rama Govindaraju", "Jinwei Gu", "Siddharth Gururani", "Imad El Hanafi", "Zekun Hao", "Jacob Huffman", "Jingyi Jin", "Brendan Johnson", "Rizwan Khan", "George Kurian", "Elena Lantz", "Nayeon Lee", "Zhaoshuo Li", "Xuan Li", "Tsung-Yi Lin", "Yen-Chen Lin", "Ming-Yu Liu", "Andrew Mathau", "Yun Ni", "Lindsey Pavao", "Wei Ping", "David W. Romero", "Misha Smelyanskiy", "Shuran Song", "Lyne Tchapmi", "Andrew Z. Wang", "Boxin Wang", "Haoxiang Wang", "Fangyin Wei", "Jiashu Xu", "Yao Xu", "Xiaodong Yang", "Zhuolin Yang", "Xiaohui Zeng", "Zhe Zhang"], "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.15781", "pdf": "https://arxiv.org/pdf/2503.15781", "abs": "https://arxiv.org/abs/2503.15781", "authors": ["Yuci Han", "Charles Toth", "Alper Yilmaz"], "title": "UAS Visual Navigation in Large and Unseen Environments via a Meta Agent", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The aim of this work is to develop an approach that enables Unmanned Aerial\nSystem (UAS) to efficiently learn to navigate in large-scale urban environments\nand transfer their acquired expertise to novel environments. To achieve this,\nwe propose a meta-curriculum training scheme. First, meta-training allows the\nagent to learn a master policy to generalize across tasks. The resulting model\nis then fine-tuned on the downstream tasks. We organize the training curriculum\nin a hierarchical manner such that the agent is guided from coarse to fine\ntowards the target task. In addition, we introduce Incremental Self-Adaptive\nReinforcement learning (ISAR), an algorithm that combines the ideas of\nincremental learning and meta-reinforcement learning (MRL). In contrast to\ntraditional reinforcement learning (RL), which focuses on acquiring a policy\nfor a specific task, MRL aims to learn a policy with fast transfer ability to\nnovel tasks. However, the MRL training process is time consuming, whereas our\nproposed ISAR algorithm achieves faster convergence than the conventional MRL\nalgorithm. We evaluate the proposed methodologies in simulated environments and\ndemonstrate that using this training philosophy in conjunction with the ISAR\nalgorithm significantly improves the convergence speed for navigation in\nlarge-scale cities and the adaptation proficiency in novel environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16264", "pdf": "https://arxiv.org/pdf/2503.16264", "abs": "https://arxiv.org/abs/2503.16264", "authors": ["Dounia Hammou", "Yancheng Cai", "Pavan Madhusudanarao", "Christos G. Bampis", "Rafał K. Mantiuk"], "title": "Do image and video quality metrics model low-level human vision?", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to\npredict the perceived quality of the evaluated content and are often claimed to\nbe \"perceptual\". Yet, few metrics directly model human visual perception, and\nmost rely on hand-crafted formulas or training datasets to achieve alignment\nwith perceptual data. In this paper, we propose a set of tests for\nfull-reference quality metrics that examine their ability to model several\naspects of low-level human vision: contrast sensitivity, contrast masking, and\ncontrast matching. The tests are meant to provide additional scrutiny for newly\nproposed metrics. We use our tests to analyze 33 existing image and video\nquality metrics and find their strengths and weaknesses, such as the ability of\nLPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in\nthis task. We further find that the popular SSIM metric overemphasizes\ndifferences in high spatial frequencies, but its multi-scale counterpart,\nMS-SSIM, addresses this shortcoming. Such findings cannot be easily made using\nexisting evaluation protocols.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-23.jsonl"}
{"id": "2503.16309", "pdf": "https://arxiv.org/pdf/2503.16309", "abs": "https://arxiv.org/abs/2503.16309", "authors": ["Vivek Gopalakrishnan", "Neel Dey", "David-Dimitris Chlorogiannis", "Andrew Abumoussa", "Anna M. Larson", "Darren B. Orbach", "Sarah Frisken", "Polina Golland"], "title": "Rapid patient-specific neural networks for intraoperative X-ray to volume registration", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "The integration of artificial intelligence in image-guided interventions\nholds transformative potential, promising to extract 3D geometric and\nquantitative information from conventional 2D imaging modalities during complex\nprocedures. Achieving this requires the rapid and precise alignment of 2D\nintraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,\nMRI). However, current 2D/3D registration methods fail across the broad\nspectrum of procedures dependent on X-ray guidance: traditional optimization\ntechniques require custom parameter tuning for each subject, whereas neural\nnetworks trained on small datasets do not generalize to new patients or require\nlabor-intensive manual annotations, increasing clinical burden and precluding\napplication to new anatomical targets. To address these challenges, we present\nxvr, a fully automated framework for training patient-specific neural networks\nfor 2D/3D registration. xvr uses physics-based simulation to generate abundant\nhigh-quality training data from a patient's own preoperative volumetric\nimaging, thereby overcoming the inherently limited ability of supervised models\nto generalize to new patients and procedures. Furthermore, xvr requires only 5\nminutes of training per patient, making it suitable for emergency interventions\nas well as planned procedures. We perform the largest evaluation of a 2D/3D\nregistration algorithm on real X-ray data to date and find that xvr robustly\ngeneralizes across a diverse dataset comprising multiple anatomical structures,\nimaging modalities, and hospitals. Across surgical tasks, xvr achieves\nsubmillimeter-accurate registration at intraoperative speeds, improving upon\nexisting methods by an order of magnitude. xvr is released as open-source\nsoftware freely available at https://github.com/eigenvivek/xvr.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-23.jsonl"}
