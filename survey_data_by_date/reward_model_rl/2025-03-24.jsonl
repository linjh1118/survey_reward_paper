{"id": "2503.17046", "pdf": "https://arxiv.org/pdf/2503.17046", "abs": "https://arxiv.org/abs/2503.17046", "authors": ["Dongsheng Yang", "Qianying Liu", "Wataru Sato", "Takashi Minato", "Chaoran Liu", "Shin'ya Nishida"], "title": "HAPI: A Model for Learning Robot Facial Expressions from Human Preferences", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Automatic robotic facial expression generation is crucial for human-robot\ninteraction, as handcrafted methods based on fixed joint configurations often\nyield rigid and unnatural behaviors. Although recent automated techniques\nreduce the need for manual tuning, they tend to fall short by not adequately\nbridging the gap between human preferences and model predictions-resulting in a\ndeficiency of nuanced and realistic expressions due to limited degrees of\nfreedom and insufficient perceptual integration. In this work, we propose a\nnovel learning-to-rank framework that leverages human feedback to address this\ndiscrepancy and enhanced the expressiveness of robotic faces. Specifically, we\nconduct pairwise comparison annotations to collect human preference data and\ndevelop the Human Affective Pairwise Impressions (HAPI) model, a Siamese\nRankNet-based approach that refines expression evaluation. Results obtained via\nBayesian Optimization and online expression survey on a 35-DOF android platform\ndemonstrate that our approach produces significantly more realistic and\nsocially resonant expressions of Anger, Happiness, and Surprise than those\ngenerated by baseline and expert-designed methods. This confirms that our\nframework effectively bridges the gap between human preferences and model\npredictions while robustly aligning robotic expression generation with human\naffective responses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference", "comparison", "pairwise"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "human preference"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17126", "pdf": "https://arxiv.org/pdf/2503.17126", "abs": "https://arxiv.org/abs/2503.17126", "authors": ["John Joon Young Chung", "Vishakh Padmakumar", "Melissa Roemmele", "Yuqian Sun", "Max Kreminski"], "title": "Modifying Large Language Model Post-Training for Diverse Creative Writing", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As creative writing tasks do not have singular correct answers, large\nlanguage models (LLMs) trained to perform these tasks should be able to\ngenerate diverse valid outputs. However, LLM post-training often focuses on\nimproving generation quality but neglects to facilitate output diversity.\nHence, in creative writing generation, we investigate post-training approaches\nto promote both output diversity and quality. Our core idea is to include\ndeviation -- the degree of difference between a training sample and all other\nsamples with the same prompt -- in the training objective to facilitate\nlearning from rare high-quality instances. By adopting our approach to direct\npreference optimization (DPO) and odds ratio preference optimization (ORPO), we\ndemonstrate that we can promote the output diversity of trained models while\nminimally decreasing quality. Our best model with 8B parameters could achieve\non-par diversity as a human-created dataset while having output quality similar\nto the best instruction-tuned models we examined, GPT-4o and DeepSeek-R1. We\nfurther validate our approaches with a human evaluation, an ablation, and a\ncomparison to an existing diversification approach, DivPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16929", "pdf": "https://arxiv.org/pdf/2503.16929", "abs": "https://arxiv.org/abs/2503.16929", "authors": ["Shicheng Li", "Lei Li", "Kun Ouyang", "Shuhuai Ren", "Yuanxin Liu", "Yuanxing Zhang", "Fuzheng Zhang", "Lingpeng Kong", "Qi Liu", "Xu Sun"], "title": "TEMPO: Temporal Preference Optimization of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved significant success by\nleveraging a two-stage paradigm: pretraining on large-scale video-text data for\nvision-language alignment, followed by supervised fine-tuning (SFT) for\ntask-specific capabilities. However, existing approaches struggle with temporal\nreasoning due to weak temporal correspondence in the data and reliance on the\nnext-token prediction paradigm during training. To address these limitations,\nwe propose TEMPO (TEMporal Preference Optimization), a systematic framework\nthat enhances Video LLMs' temporal reasoning capabilities through Direct\nPreference Optimization (DPO). To facilitate this, we introduce an automated\npreference data generation pipeline that systematically constructs preference\npairs by selecting videos that are rich in temporal information, designing\nvideo-specific perturbation strategies, and finally evaluating model responses\non clean and perturbed video inputs. Our temporal alignment features two key\ninnovations: curriculum learning which that progressively increases\nperturbation difficulty to improve model robustness and adaptability; and\n``Pre-SFT Alignment'', applying preference optimization before instruction\ntuning to prioritize fine-grained temporal comprehension. Extensive experiments\ndemonstrate that our approach consistently improves Video LLM performance\nacross multiple benchmarks with a relatively small set of self-generated DPO\ndata. We further analyze the transferability of DPO data across architectures\nand the role of difficulty scheduling in optimization. Our findings highlight\nour TEMPO as a scalable and efficient complement to SFT-based methods, paving\nthe way for developing reliable Video LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17003", "pdf": "https://arxiv.org/pdf/2503.17003", "abs": "https://arxiv.org/abs/2503.17003", "authors": ["Jian Guan", "Junfei Wu", "Jia-Nan Li", "Chuanqi Cheng", "Wei Wu"], "title": "A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir transition to real-world applications reveals a critical limitation: the\ninability to adapt to individual preferences while maintaining alignment with\nuniversal human values. Current alignment techniques adopt a one-size-fits-all\napproach that fails to accommodate users' diverse backgrounds and needs. This\npaper presents the first comprehensive survey of personalized alignment-a\nparadigm that enables LLMs to adapt their behavior within ethical boundaries\nbased on individual preferences. We propose a unified framework comprising\npreference memory management, personalized generation, and feedback-based\nalignment, systematically analyzing implementation approaches and evaluating\ntheir effectiveness across various scenarios. By examining current techniques,\npotential risks, and future challenges, this survey provides a structured\nfoundation for developing more adaptable and ethically-aligned LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16867", "pdf": "https://arxiv.org/pdf/2503.16867", "abs": "https://arxiv.org/abs/2503.16867", "authors": ["Kaisi Guan", "Zhengfeng Lai", "Yuchong Sun", "Peng Zhang", "Wei Liu", "Kieran Liu", "Meng Cao", "Ruihua Song"], "title": "ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering", "categories": ["cs.CV"], "comment": null, "summary": "Precisely evaluating semantic alignment between text prompts and generated\nvideos remains a challenge in Text-to-Video (T2V) Generation. Existing\ntext-to-video alignment metrics like CLIPScore only generate coarse-grained\nscores without fine-grained alignment details, failing to align with human\npreference. To address this limitation, we propose ETVA, a novel Evaluation\nmethod of Text-to-Video Alignment via fine-grained question generation and\nanswering. First, a multi-agent system parses prompts into semantic scene\ngraphs to generate atomic questions. Then we design a knowledge-augmented\nmulti-stage reasoning framework for question answering, where an auxiliary LLM\nfirst retrieves relevant common-sense knowledge (e.g., physical laws), and then\nvideo LLM answers the generated questions through a multi-stage reasoning\nmechanism. Extensive experiments demonstrate that ETVA achieves a Spearman's\ncorrelation coefficient of 58.47, showing a much higher correlation with human\njudgment than existing metrics which attain only 31.0. We also construct a\ncomprehensive benchmark specifically designed for text-to-video alignment\nevaluation, featuring 2k diverse prompts and 12k atomic questions spanning 10\ncategories. Through a systematic evaluation of 15 existing text-to-video\nmodels, we identify their key capabilities and limitations, paving the way for\nnext-generation T2V generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16921", "pdf": "https://arxiv.org/pdf/2503.16921", "abs": "https://arxiv.org/abs/2503.16921", "authors": ["Lingfan Zhang", "Chen Liu", "Chengming Xu", "Kai Hu", "Donghao Luo", "Chengjie Wang", "Yanwei Fu", "Yuan Yao"], "title": "When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the field of image generation has witnessed significant\nadvancements, particularly in fine-tuning methods that align models with\nuniversal human preferences. This paper explores the critical role of\npreference data in the training process of diffusion models, particularly in\nthe context of Diffusion-DPO and its subsequent adaptations. We investigate the\ncomplexities surrounding universal human preferences in image generation,\nhighlighting the subjective nature of these preferences and the challenges\nposed by minority samples in preference datasets. Through pilot experiments, we\ndemonstrate the existence of minority samples and their detrimental effects on\nmodel performance. We propose Adaptive-DPO -- a novel approach that\nincorporates a minority-instance-aware metric into the DPO objective. This\nmetric, which includes intra-annotator confidence and inter-annotator\nstability, distinguishes between majority and minority samples. We introduce an\nAdaptive-DPO loss function which improves the DPO loss in two ways: enhancing\nthe model's learning of majority labels while mitigating the negative impact of\nminority samples. Our experiments demonstrate that this method effectively\nhandles both synthetic minority data and real-world preference data, paving the\nway for more effective training methodologies in image generation tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16480", "pdf": "https://arxiv.org/pdf/2503.16480", "abs": "https://arxiv.org/abs/2503.16480", "authors": ["Yara Kyrychenko", "Jon Roozenbeek", "Brandon Davidson", "Sander van der Linden", "Ramit Debnath"], "title": "Human Preferences for Constructive Interactions in Language Model Alignment", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "1 Figure, 1 Table, 11 pages", "summary": "As large language models (LLMs) enter the mainstream, aligning them to foster\nconstructive dialogue rather than exacerbate societal divisions is critical.\nUsing an individualized and multicultural alignment dataset of over 7,500\nconversations of individuals from 74 countries engaging with 21 LLMs, we\nexamined how linguistic attributes linked to constructive interactions are\nreflected in human preference data used for training AI. We found that users\nconsistently preferred well-reasoned and nuanced responses while rejecting\nthose high in personal storytelling. However, users who believed that AI should\nreflect their values tended to place less preference on reasoning in LLM\nresponses and more on curiosity. Encouragingly, we observed that users could\nset the tone for how constructive their conversation would be, as LLMs mirrored\nlinguistic attributes, including toxicity, in user queries.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16505", "pdf": "https://arxiv.org/pdf/2503.16505", "abs": "https://arxiv.org/abs/2503.16505", "authors": ["Dimitris Tsirmpas", "Ion Androutsopoulos", "John Pavlopoulos"], "title": "Scalable Evaluation of Online Moderation Strategies via Synthetic Simulations", "categories": ["cs.HC", "cs.CL", "cs.LG", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 9 figures", "summary": "Despite the ever-growing importance of online moderation, there has been no\nlarge-scale study evaluating the effectiveness of alternative moderation\nstrategies. This is largely due to the lack of appropriate datasets, and the\ndifficulty of getting human discussants, moderators, and evaluators involved in\nmultiple experiments. In this paper, we propose a methodology for leveraging\nsynthetic experiments performed exclusively by Large Language Models (LLMs) to\ninitially bypass the need for human participation in experiments involving\nonline moderation. We evaluate six LLM moderation configurations; two currently\nused real-life moderation strategies (guidelines issued for human moderators\nfor online moderation and real-life facilitation), two baseline strategies\n(guidelines elicited for LLM alignment work, and LLM moderation with minimal\nprompting) a baseline with no moderator at all, as well as our own proposed\nstrategy inspired by a Reinforcement Learning (RL) formulation of the problem.\nWe find that our own moderation strategy significantly outperforms established\nmoderation guidelines, as well as out-of-the-box LLM moderation. We also find\nthat smaller LLMs, with less intensive instruction-tuning, can create more\nvaried discussions than larger models. In order to run these experiments, we\ncreate and release an efficient, purpose-built, open-source Python framework,\ndubbed \"SynDisco\" to easily simulate hundreds of discussions using LLM\nuser-agents and moderators. Additionally, we release the Virtual Moderation\nDataset (VMD), a large dataset of LLM-generated and LLM-annotated discussions,\ngenerated by three families of open-source LLMs accompanied by an exploratory\nanalysis of the dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16629", "pdf": "https://arxiv.org/pdf/2503.16629", "abs": "https://arxiv.org/abs/2503.16629", "authors": ["Julian Ziegler", "Patrick Frenzel", "Mirco Fuchs"], "title": "Utilizing Reinforcement Learning for Bottom-Up part-wise Reconstruction of 2D Wire-Frame Projections", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to RLDM 2025", "summary": "This work concerns itself with the task of reconstructing all edges of an\narbitrary 3D wire-frame model projected to an image plane. We explore a\nbottom-up part-wise procedure undertaken by an RL agent to segment and\nreconstruct these 2D multipart objects. The environment's state is represented\nas a four-colour image, where different colours correspond to background, a\ntarget edge, a reconstruction line, and the overlap of both. At each step, the\nagent can transform the reconstruction line within a four-dimensional action\nspace or terminate the episode using a specific termination action. To\ninvestigate the impact of reward function formulations, we tested episodic and\nincremental rewards, as well as combined approaches. Empirical results\ndemonstrated that the latter yielded the most effective training performance.\nTo further enhance efficiency and stability, we introduce curriculum learning\nstrategies. First, an action-based curriculum was implemented, where the agent\nwas initially restricted to a reduced action space, being able to only perform\nthree of the five possible actions, before progressing to the full action\nspace. Second, we test a task-based curriculum, where the agent first solves a\nsimplified version of the problem before being presented with the full, more\ncomplex task. This second approach produced promising results, as the agent not\nonly successfully transitioned from learning the simplified task to mastering\nthe full task, but in doing so gained significant performance. This study\ndemonstrates the potential of an iterative RL wire-frame reconstruction in two\ndimensions. By combining optimized reward function formulations with curriculum\nlearning strategies, we achieved significant improvements in training success.\nThe proposed methodology provides an effective framework for solving similar\ntasks and represents a promising direction for future research in the field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16525", "pdf": "https://arxiv.org/pdf/2503.16525", "abs": "https://arxiv.org/abs/2503.16525", "authors": ["Huan Yang", "Renji Zhang", "Deyu Zhang"], "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large Language Model Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16528", "pdf": "https://arxiv.org/pdf/2503.16528", "abs": "https://arxiv.org/abs/2503.16528", "authors": ["Heng Ping", "Shixuan Li", "Peiyu Zhang", "Anzhe Cheng", "Shukai Duan", "Nikos Kanakaris", "Xiongye Xiao", "Wei Yang", "Shahin Nazarian", "Andrei Irimia", "Paul Bogdan"], "title": "HDLCoRe: A Training-Free Framework for Mitigating Hallucinations in LLM-Generated HDL", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities in code generation tasks. However, when applied to hardware\ndescription languages (HDL), these models exhibit significant limitations due\nto data scarcity, resulting in hallucinations and incorrect code generation. To\naddress these challenges, we propose HDLCoRe, a training-free framework that\nenhances LLMs' HDL generation capabilities through prompt engineering\ntechniques and retrieval-augmented generation (RAG). Our approach consists of\ntwo main components: (1) an HDL-aware Chain-of-Thought (CoT) prompting\ntechnique with self-verification that classifies tasks by complexity and type,\nincorporates domain-specific knowledge, and guides LLMs through step-by-step\nself-simulation for error correction; and (2) a two-stage heterogeneous RAG\nsystem that addresses formatting inconsistencies through key component\nextraction and efficiently retrieves relevant HDL examples through sequential\nfiltering and re-ranking. HDLCoRe eliminates the need for model fine-tuning\nwhile substantially improving LLMs' HDL generation capabilities. Experimental\nresults demonstrate that our framework achieves superior performance on the\nRTLLM2.0 benchmark, significantly reducing hallucinations and improving both\nsyntactic and functional correctness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "code generation"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16544", "pdf": "https://arxiv.org/pdf/2503.16544", "abs": "https://arxiv.org/abs/2503.16544", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Causal Discovery and Counterfactual Reasoning to Optimize Persuasive Dialogue Policies", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages, 8 figures", "summary": "Tailoring persuasive conversations to users leads to more effective\npersuasion. However, existing dialogue systems often struggle to adapt to\ndynamically evolving user states. This paper presents a novel method that\nleverages causal discovery and counterfactual reasoning for optimizing system\npersuasion capability and outcomes. We employ the Greedy Relaxation of the\nSparsest Permutation (GRaSP) algorithm to identify causal relationships between\nuser and system utterance strategies, treating user strategies as states and\nsystem strategies as actions. GRaSP identifies user strategies as causal\nfactors influencing system responses, which inform Bidirectional Conditional\nGenerative Adversarial Networks (BiCoGAN) in generating counterfactual\nutterances for the system. Subsequently, we use the Dueling Double Deep\nQ-Network (D3QN) model to utilize counterfactual data to determine the best\npolicy for selecting system utterances. Our experiments with the\nPersuasionForGood dataset show measurable improvements in persuasion outcomes\nusing our approach over baseline methods. The observed increase in cumulative\nrewards and Q-values highlights the effectiveness of causal discovery in\nenhancing counterfactual reasoning and optimizing reinforcement learning\npolicies for online dialogue systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16707", "pdf": "https://arxiv.org/pdf/2503.16707", "abs": "https://arxiv.org/abs/2503.16707", "authors": ["Jinlong Li", "Cristiano Saltori", "Fabio Poiesi", "Nicu Sebe"], "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "The lack of a large-scale 3D-text corpus has led recent works to distill\nopen-vocabulary knowledge from vision-language models (VLMs). owever, these\nmethods typically rely on a single VLM to align the feature spaces of 3D models\nwithin a common language space, which limits the potential of 3D models to\nleverage the diverse spatial and semantic capabilities encapsulated in various\nfoundation models. In this paper, we propose Cross-modal and Uncertainty-aware\nAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, the\nfirst model to integrate multiple foundation models-such as CLIP, DINOv2, and\nStable Diffusion-into 3D scene understanding. We further introduce a\ndeterministic uncertainty estimation to adaptively distill and harmonize the\nheterogeneous 2D feature embeddings from these models. Our method addresses two\nkey challenges: (1) incorporating semantic priors from VLMs alongside the\ngeometric knowledge of spatially-aware vision foundation models, and (2) using\na novel deterministic uncertainty estimation to capture model-specific\nuncertainties across diverse semantic and geometric sensitivities, helping to\nreconcile heterogeneous representations during training. Extensive experiments\non ScanNetV2 and Matterport3D demonstrate that our method not only advances\nopen-vocabulary segmentation but also achieves robust cross-domain alignment\nand competitive spatial perception capabilities. The code will be available at\n\\href{https://github.com/TyroneLi/CUA_O3D}{CUA_O3D}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16674", "pdf": "https://arxiv.org/pdf/2503.16674", "abs": "https://arxiv.org/abs/2503.16674", "authors": ["Molly Kennedy", "Ayyoob Imani", "Timo Spinde", "Hinrich Sch√ºtze"], "title": "Through the LLM Looking Glass: A Socratic Self-Assessment of Donkeys, Elephants, and Markets", "categories": ["cs.CL"], "comment": null, "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\neight widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via self-assessment. By using self-assessment, the\nstudy aims to directly measure the models' biases rather than relying on\nexternal interpretations, thereby minimizing subjective judgments about media\nbias. Our results reveal a consistent preference of Democratic over Republican\npositions across all models. Conversely, in economic topics, biases vary among\nWestern LLMs, while those developed in China lean more strongly toward\nsocialism.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16832", "pdf": "https://arxiv.org/pdf/2503.16832", "abs": "https://arxiv.org/abs/2503.16832", "authors": ["Ali Shah Ali", "Syed Ahmed Mahmood", "Mubin Saeed", "Andrey Konin", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Joint Self-Supervised Video Alignment and Action Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel approach for simultaneous self-supervised video\nalignment and action segmentation based on a unified optimal transport\nframework. In particular, we first tackle self-supervised video alignment by\ndeveloping a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior, which trains efficiently on GPUs and needs only a few\niterations for solving the optimal transport problem. Our single-task method\nachieves the state-of-the-art performance on multiple video alignment\nbenchmarks and outperforms VAVA, which relies on a traditional Kantorovich\noptimal transport formulation with an optimality prior. Furthermore, we extend\nour approach by proposing a unified optimal transport framework for joint\nself-supervised video alignment and action segmentation, which requires\ntraining and storing a single model and saves both time and memory consumption\nas compared to two different single-task models. Extensive evaluations on\nseveral video alignment and action segmentation datasets demonstrate that our\nmulti-task method achieves comparable video alignment yet superior action\nsegmentation results over previous methods in video alignment and action\nsegmentation respectively. Finally, to the best of our knowledge, this is the\nfirst work to unify video alignment and action segmentation into a single\nmodel.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16916", "pdf": "https://arxiv.org/pdf/2503.16916", "abs": "https://arxiv.org/abs/2503.16916", "authors": ["Xiaoyong Chen", "Yong Guo", "Jiaming Liang", "Sitong Zhuang", "Runhao Zeng", "Xiping Hu"], "title": "Temporal Action Detection Model Compression by Progressive Block Drop", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Temporal action detection (TAD) aims to identify and localize action\ninstances in untrimmed videos, which is essential for various video\nunderstanding tasks. However, recent improvements in model performance, driven\nby larger feature extractors and datasets, have led to increased computational\ndemands. This presents a challenge for applications like autonomous driving and\nrobotics, which rely on limited computational resources. While existing channel\npruning methods can compress these models, reducing the number of channels\noften hinders the parallelization efficiency of GPU, due to the inefficient\nmultiplication between small matrices. Instead of pruning channels, we propose\na Progressive Block Drop method that reduces model depth while retaining layer\nwidth. In this way, we still use large matrices for computation but reduce the\nnumber of multiplications. Our approach iteratively removes redundant blocks in\ntwo steps: first, we drop blocks with minimal impact on model performance; and\nsecond, we employ a parameter-efficient cross-depth alignment technique,\nfine-tuning the pruned model to restore model accuracy. Our method achieves a\n25% reduction in computational overhead on two TAD benchmarks (THUMOS14 and\nActivityNet-1.3) to achieve lossless compression. More critically, we\nempirically show that our method is orthogonal to channel pruning methods and\ncan be combined with it to yield further efficiency gains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17239", "pdf": "https://arxiv.org/pdf/2503.17239", "abs": "https://arxiv.org/abs/2503.17239", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Farhan Ahmed", "Syed Zawad", "Holger Boche"], "title": "SafeMERGE: Preserving Safety Alignment in Fine-Tuned Large Language Models via Selective Layer-Wise Model Merging", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on downstream tasks can\ninadvertently erode their safety alignment, even for benign fine-tuning\ndatasets. We address this challenge by proposing SafeMERGE, a post-fine-tuning\nframework that preserves safety while maintaining task utility. It achieves\nthis by selectively merging fine-tuned and safety-aligned model layers only\nwhen those deviate from safe behavior, measured by a cosine similarity\ncriterion. We evaluate SafeMERGE against other fine-tuning- and\npost-fine-tuning-stage approaches for Llama-2-7B-Chat and Qwen-2-7B-Instruct\nmodels on GSM8K and PubMedQA tasks while exploring different merging\nstrategies. We find that SafeMERGE consistently reduces harmful outputs\ncompared to other baselines without significantly sacrificing performance,\nsometimes even enhancing it. The results suggest that our selective,\nsubspace-guided, and per-layer merging method provides an effective safeguard\nagainst the inadvertent loss of safety in fine-tuned LLMs while outperforming\nsimpler post-fine-tuning-stage defenses.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17287", "pdf": "https://arxiv.org/pdf/2503.17287", "abs": "https://arxiv.org/abs/2503.17287", "authors": ["Mingyang Song", "Mao Zheng", "Zheng Li", "Wenjie Yang", "Xuan Luo", "Yue Pan", "Feng Zhang"], "title": "FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose \\textbf{\\textsc{FastCuRL}}, a simple yet efficient\n\\textbf{Cu}rriculum \\textbf{R}einforcement \\textbf{L}earning approach with\ncontext window extending strategy to accelerate the reinforcement learning\ntraining efficiency for R1-like reasoning models while enhancing their\nperformance in tackling complex reasoning tasks with long chain-of-thought\nrationales, particularly with a 1.5B parameter language model.\n\\textbf{\\textsc{FastCuRL}} consists of two main procedures: length-aware\ntraining data segmentation and context window extension training. Specifically,\nthe former first splits the original training data into three different levels\nby the input prompt length, and then the latter leverages segmented training\ndatasets with a progressively increasing context window length to train the\nreasoning model. Experimental results demonstrate that\n\\textbf{\\textsc{FastCuRL}}-1.5B-Preview surpasses DeepScaleR-1.5B-Preview\nacross all five datasets (including MATH 500, AIME 2024, AMC 2023, Minerva\nMath, and OlympiadBench) while only utilizing 50\\% of training steps.\nFurthermore, all training stages for FastCuRL-1.5B-Preview are completed using\njust a single node with 8 GPUs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16945", "pdf": "https://arxiv.org/pdf/2503.16945", "abs": "https://arxiv.org/abs/2503.16945", "authors": ["Ibtissam Saadi", "Abdenour Hadid", "Douglas W. Cunningham", "Abdelmalik Taleb-Ahmed", "Yassin El Hillali"], "title": "PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP offer promising solutions for Dynamic\nFacial Expression Recognition (DFER) but face challenges such as inefficient\nfull fine-tuning, high complexity, and poor alignment between textual and\nvisual representations. Additionally, existing methods struggle with\nineffective temporal modeling. To address these issues, we propose PE-CLIP, a\nparameter-efficient fine-tuning (PEFT) framework that adapts CLIP for DFER\nwhile significantly reducing trainable parameters while maintaining high\naccuracy. PE-CLIP introduces two specialized adapters: a Temporal Dynamic\nAdapter (TDA) and a Shared Adapter (ShA). The TDA is a GRU-based module with\ndynamic scaling that captures sequential dependencies while emphasizing\ninformative temporal features and suppressing irrelevant variations. The ShA is\na lightweight adapter that refines representations within both textual and\nvisual encoders, ensuring consistency and efficiency. Additionally, we\nintegrate Multi-modal Prompt Learning (MaPLe), introducing learnable prompts\nfor visual and action unit-based textual inputs, enhancing semantic alignment\nbetween modalities and enabling efficient CLIP adaptation for dynamic tasks. We\nevaluate PE-CLIP on two benchmark datasets, DFEW and FERV39K, achieving\ncompetitive performance compared to state-of-the-art methods while requiring\nfewer trainable parameters. By balancing efficiency and accuracy, PE-CLIP sets\na new benchmark in resource-efficient DFER. The source code of the proposed\nPE-CLIP will be publicly available at https://github.com/Ibtissam-SAADI/PE-CLIP .", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16970", "pdf": "https://arxiv.org/pdf/2503.16970", "abs": "https://arxiv.org/abs/2503.16970", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Depth completion involves predicting dense depth maps from sparse LiDAR\ninputs. However, sparse depth annotations from sensors limit the availability\nof dense supervision, which is necessary for learning detailed geometric\nfeatures. In this paper, we propose a two-stage knowledge distillation\nframework that leverages powerful monocular foundation models to provide dense\nsupervision for depth completion. In the first stage, we introduce a\npre-training strategy that generates diverse training data from natural images,\nwhich distills geometric knowledge to depth completion. Specifically, we\nsimulate LiDAR scans by utilizing monocular depth and mesh reconstruction,\nthereby creating training data without requiring ground-truth depth. Besides,\nmonocular depth estimation suffers from inherent scale ambiguity in real-world\nsettings. To address this, in the second stage, we employ a scale- and\nshift-invariant loss (SSI Loss) to learn real-world scales when fine-tuning on\nreal-world datasets. Our two-stage distillation framework enables depth\ncompletion models to harness the strengths of monocular foundation models.\nExperimental results demonstrate that models trained with our two-stage\ndistillation framework achieve state-of-the-art performance, ranking\n\\textbf{first place} on the KITTI benchmark. Code is available at\nhttps://github.com/Sharpiless/DMD3C", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16463", "pdf": "https://arxiv.org/pdf/2503.16463", "abs": "https://arxiv.org/abs/2503.16463", "authors": ["Zhoujian Sun", "Ziyi Liu", "Cheng Luo", "Jiebin Chu", "Zhengxing Huang"], "title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "30 pages", "summary": "Recent advances in large language models (LLMs) have shown promising results\nin medical diagnosis, with some studies indicating superior performance\ncompared to human physicians in specific scenarios. However, the diagnostic\ncapabilities of LLMs are often overestimated, as their performance\nsignificantly deteriorates in interactive diagnostic settings that require\nactive information gathering. This study investigates the underlying mechanisms\nbehind the performance degradation phenomenon and proposes a solution. We\nidentified that the primary deficiency of LLMs lies in the initial diagnosis\nphase, particularly in information-gathering efficiency and initial diagnosis\nformation, rather than in the subsequent differential diagnosis phase. To\naddress this limitation, we developed a plug-and-play method enhanced (PPME)\nLLM agent, leveraging over 3.5 million electronic medical records from Chinese\nand American healthcare facilities. Our approach integrates specialized models\nfor initial disease diagnosis and inquiry into the history of the present\nillness, trained through supervised and reinforcement learning techniques. The\nexperimental results indicate that the PPME LLM achieved over 30% improvement\ncompared to baselines. The final diagnostic accuracy of the PPME LLM in\ninteractive diagnostic scenarios approached levels comparable to those achieved\nusing complete clinical data. These findings suggest a promising potential for\ndeveloping autonomous diagnostic systems, although further validation studies\nare needed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17027", "pdf": "https://arxiv.org/pdf/2503.17027", "abs": "https://arxiv.org/abs/2503.17027", "authors": ["Ziteng Cui", "Jianfei Yang", "Tatsuya Harada"], "title": "RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images and A Benchmark", "categories": ["cs.CV"], "comment": "23 pages, 17 figures, extension of ECCV 2024 work: arXiv:2408.14802", "summary": "In the computer vision community, the preference for pre-training visual\nmodels has largely shifted toward sRGB images due to their ease of acquisition\nand compact storage. However, camera RAW images preserve abundant physical\ndetails across diverse real-world scenarios. Despite this, most existing visual\nperception methods that utilize RAW data directly integrate image signal\nprocessing (ISP) stages with subsequent network modules, often overlooking\npotential synergies at the model level. Building on recent advances in\nadapter-based methodologies in both NLP and computer vision, we propose\nRAW-Adapter, a novel framework that incorporates learnable ISP modules as\ninput-level adapters to adjust RAW inputs. At the same time, it employs\nmodel-level adapters to seamlessly bridge ISP processing with high-level\ndownstream architectures. Moreover, RAW-Adapter serves as a general framework\napplicable to various computer vision frameworks.\n  Furthermore, we introduce RAW-Bench, which incorporates 17 types of RAW-based\ncommon corruptions, including lightness degradations, weather effects,\nblurriness, camera imaging degradations, and variations in camera color\nresponse. Using this benchmark, we systematically compare the performance of\nRAW-Adapter with state-of-the-art (SOTA) ISP methods and other RAW-based\nhigh-level vision algorithms. Additionally, we propose a RAW-based data\naugmentation strategy to further enhance RAW-Adapter's performance and improve\nits out-of-domain (OOD) generalization ability. Extensive experiments\nsubstantiate the effectiveness and efficiency of RAW-Adapter, highlighting its\nrobust performance across diverse scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16718", "pdf": "https://arxiv.org/pdf/2503.16718", "abs": "https://arxiv.org/abs/2503.16718", "authors": ["Massa Baali", "Xiang Li", "Hao Chen", "Rita Singh", "Bhiksha Raj"], "title": "CAARMA: Class Augmentation with Adversarial Mixup Regularization", "categories": ["cs.SD", "cs.CL", "cs.LG"], "comment": null, "summary": "Speaker verification is a typical zero-shot learning task, where inference of\nunseen classes is performed by comparing embeddings of test instances to known\nexamples. The models performing inference must hence naturally generate\nembeddings that cluster same-class instances compactly, while maintaining\nseparation across classes. In order to learn to do so, they are typically\ntrained on a large number of classes (speakers), often using specialized\nlosses. However real-world speaker datasets often lack the class diversity\nneeded to effectively learn this in a generalizable manner. We introduce\nCAARMA, a class augmentation framework that addresses this problem by\ngenerating synthetic classes through data mixing in the embedding space,\nexpanding the number of training classes. To ensure the authenticity of the\nsynthetic classes we adopt a novel adversarial refinement mechanism that\nminimizes categorical distinctions between synthetic and real classes. We\nevaluate CAARMA on multiple speaker verification tasks, as well as other\nrepresentative zero-shot comparison-based speech analysis tasks and obtain\nconsistent improvements: our framework demonstrates a significant improvement\nof 8\\% over all baseline models. Code for CAARMA will be released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16875", "pdf": "https://arxiv.org/pdf/2503.16875", "abs": "https://arxiv.org/abs/2503.16875", "authors": ["Jiangcheng Qin", "Xueyuan Zhang", "Baisong Liu", "Jiangbo Qian", "Yangyang Wang"], "title": "Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation", "categories": ["cs.IR", "cs.CL", "cs.DC"], "comment": null, "summary": "Accurately predicting click-through rates (CTR) under stringent privacy\nconstraints poses profound challenges, particularly when user-item interactions\nare sparse and fragmented across domains. Conventional cross-domain CTR (CCTR)\nmethods frequently assume homogeneous feature spaces and rely on centralized\ndata sharing, neglecting complex inter-domain discrepancies and the subtle\ntrade-offs imposed by privacy-preserving protocols. Here, we present Federated\nCross-Domain CTR Prediction with Large Language Model Augmentation\n(FedCCTR-LM), a federated framework engineered to address these limitations by\nsynchronizing data augmentation, representation disentanglement, and adaptive\nprivacy protection. Our approach integrates three core innovations. First, the\nPrivacy-Preserving Augmentation Network (PrivAugNet) employs large language\nmodels to enrich user and item representations and expand interaction\nsequences, mitigating data sparsity and feature incompleteness. Second, the\nIndependent Domain-Specific Transformer with Contrastive Learning (IDST-CL)\nmodule disentangles domain-specific and shared user preferences, employing\nintra-domain representation alignment (IDRA) and crossdomain representation\ndisentanglement (CDRD) to refine the learned embeddings and enhance knowledge\ntransfer across domains. Finally, the Adaptive Local Differential Privacy\n(AdaLDP) mechanism dynamically calibrates noise injection to achieve an optimal\nbalance between rigorous privacy guarantees and predictive accuracy. Empirical\nevaluations on four real-world datasets demonstrate that FedCCTR-LM\nsubstantially outperforms existing baselines, offering robust,\nprivacy-preserving, and generalizable cross-domain CTR prediction in\nheterogeneous, federated environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17080", "pdf": "https://arxiv.org/pdf/2503.17080", "abs": "https://arxiv.org/abs/2503.17080", "authors": ["Gensheng Pei", "Tao Chen", "Yujia Wang", "Xinhao Cai", "Xiangbo Shu", "Tianfei Zhou", "Yazhou Yao"], "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection", "categories": ["cs.CV"], "comment": "accepted by CVPR 2025", "summary": "The CLIP model has demonstrated significant advancements in aligning visual\nand language modalities through large-scale pre-training on image-text pairs,\nenabling strong zero-shot classification and retrieval capabilities on various\ndomains. However, CLIP's training remains computationally intensive, with high\ndemands on both data processing and memory. To address these challenges, recent\nmasking strategies have emerged, focusing on the selective removal of image\npatches to improve training efficiency. Although effective, these methods often\ncompromise key semantic information, resulting in suboptimal alignment between\nvisual features and text descriptions. In this work, we present a concise yet\neffective approach called Patch Generation-to-Selection to enhance CLIP's\ntraining efficiency while preserving critical semantic content. Our method\nintroduces a gradual masking process in which a small set of candidate patches\nis first pre-selected as potential mask regions. Then, we apply Sobel edge\ndetection across the entire image to generate an edge mask that prioritizes the\nretention of the primary object areas. Finally, similarity scores between the\ncandidate mask patches and their neighboring patches are computed, with optimal\ntransport normalization refining the selection process to ensure a balanced\nsimilarity matrix. Our approach, CLIP-PGS, sets new state-of-the-art results in\nzero-shot classification and retrieval tasks, achieving superior performance in\nrobustness evaluation and language compositionality benchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17142", "pdf": "https://arxiv.org/pdf/2503.17142", "abs": "https://arxiv.org/abs/2503.17142", "authors": ["Davide Berasi", "Matteo Farina", "Massimiliano Mancini", "Elisa Ricci", "Nicola Strisciuglio"], "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Camera-ready version for CVPR 2025 (with Supp.Mat.)", "summary": "Vision-Language Models (VLMs) learn a shared feature space for text and\nimages, enabling the comparison of inputs of different modalities. While prior\nworks demonstrated that VLMs organize natural language representations into\nregular structures encoding composite meanings, it remains unclear if\ncompositional patterns also emerge in the visual embedding space. In this work,\nwe investigate compositionality in the image domain, where the analysis of\ncompositional properties is challenged by noise and sparsity of visual data. We\naddress these problems and propose a framework, called Geodesically\nDecomposable Embeddings (GDE), that approximates image representations with\ngeometry-aware compositional structures in the latent space. We demonstrate\nthat visual embeddings of pre-trained VLMs exhibit a compositional arrangement,\nand evaluate the effectiveness of this property in the tasks of compositional\nclassification and group robustness. GDE achieves stronger performance in\ncompositional classification compared to its counterpart method that assumes\nlinear geometry of the latent space. Notably, it is particularly effective for\ngroup robustness, where we achieve higher results than task-specific solutions.\nOur results indicate that VLMs can automatically develop a human-like form of\ncompositional reasoning in the visual domain, making their underlying processes\nmore interpretable. Code is available at\nhttps://github.com/BerasiDavide/vlm_image_compositionality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17168", "pdf": "https://arxiv.org/pdf/2503.17168", "abs": "https://arxiv.org/abs/2503.17168", "authors": ["Alexandra Arzberger", "Ramin Tavakoli Kolagari"], "title": "Hi-ALPS -- An Experimental Robustness Quantification of Six LiDAR-based Object Detection Systems for Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Light Detection and Ranging (LiDAR) is an essential sensor technology for\nautonomous driving as it can capture high-resolution 3D data. As 3D object\ndetection systems (OD) can interpret such point cloud data, they play a key\nrole in the driving decisions of autonomous vehicles. Consequently, such 3D OD\nmust be robust against all types of perturbations and must therefore be\nextensively tested. One approach is the use of adversarial examples, which are\nsmall, sometimes sophisticated perturbations in the input data that change,\ni.e., falsify, the prediction of the OD. These perturbations are carefully\ndesigned based on the weaknesses of the OD. The robustness of the OD cannot be\nquantified with adversarial examples in general, because if the OD is\nvulnerable to a given attack, it is unclear whether this is due to the\nrobustness of the OD or whether the attack algorithm produces particularly\nstrong adversarial examples. The contribution of this work is Hi-ALPS --\nHierarchical Adversarial-example-based LiDAR Perturbation Level System, where\nhigher robustness of the OD is required to withstand the perturbations as the\nperturbation levels increase. In doing so, the Hi-ALPS levels successively\nimplement a heuristic followed by established adversarial example approaches.\nIn a series of comprehensive experiments using Hi-ALPS, we quantify the\nrobustness of six state-of-the-art 3D OD under different types of\nperturbations. The results of the experiments show that none of the OD is\nrobust against all Hi-ALPS levels; an important factor for the ranking is that\nhuman observers can still correctly recognize the perturbed objects, as the\nrespective perturbations are small. To increase the robustness of the OD, we\ndiscuss the applicability of state-of-the-art countermeasures. In addition, we\nderive further suggestions for countermeasures based on our experimental\nresults.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17212", "pdf": "https://arxiv.org/pdf/2503.17212", "abs": "https://arxiv.org/abs/2503.17212", "authors": ["Matthew Kenely", "Dylan Seychell", "Carl James Debono", "Chris Porter"], "title": "A Deep Learning Framework for Visual Attention Prediction and Analysis of News Interfaces", "categories": ["cs.CV", "cs.HC"], "comment": "This is a preprint submitted to the 2025 IEEE Conference on\n  Artificial Intelligence (CAI)", "summary": "News outlets' competition for attention in news interfaces has highlighted\nthe need for demographically-aware saliency prediction models. Despite recent\nadvancements in saliency detection applied to user interfaces (UI), existing\ndatasets are limited in size and demographic representation. We present a deep\nlearning framework that enhances the SaRa (Saliency Ranking) model with\nDeepGaze IIE, improving Salient Object Ranking (SOR) performance by 10.7%. Our\nframework optimizes three key components: saliency map generation, grid segment\nscoring, and map normalization. Through a two-fold experiment using\neye-tracking (30 participants) and mouse-tracking (375 participants aged\n13--70), we analyze attention patterns across demographic groups. Statistical\nanalysis reveals significant age-based variations (p < 0.05, {\\epsilon^2} =\n0.042), with older users (36--70) engaging more with textual content and\nyounger users (13--35) interacting more with images. Mouse-tracking data\nclosely approximates eye-tracking behavior (sAUC = 0.86) and identifies UI\nelements that immediately stand out, validating its use in large-scale studies.\nWe conclude that saliency studies should prioritize gathering data from a\nlarger, demographically representative sample and report exact demographic\ndistributions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17352", "pdf": "https://arxiv.org/pdf/2503.17352", "abs": "https://arxiv.org/abs/2503.17352", "authors": ["Yihe Deng", "Hritik Bansal", "Fan Yin", "Nanyun Peng", "Wei Wang", "Kai-Wei Chang"], "title": "OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement", "categories": ["cs.CV", "cs.CL"], "comment": "23 pages, 11 figures, 8 tables", "summary": "Recent advancements demonstrated by DeepSeek-R1 have shown that complex\nreasoning abilities in large language models (LLMs), including sophisticated\nbehaviors such as self-verification and self-correction, can be achieved by RL\nwith verifiable rewards and significantly improves model performance on\nchallenging tasks such as AIME. Motivated by these findings, our study\ninvestigates whether similar reasoning capabilities can be successfully\nintegrated into large vision-language models (LVLMs) and assesses their impact\non challenging multimodal reasoning tasks. We consider an approach that\niteratively leverages supervised fine-tuning (SFT) on lightweight training data\nand Reinforcement Learning (RL) to further improve model generalization.\nInitially, reasoning capabilities were distilled from pure-text R1 models by\ngenerating reasoning steps using high-quality captions of the images sourced\nfrom diverse visual datasets. Subsequently, iterative RL training further\nenhance reasoning skills, with each iteration's RL-improved model generating\nrefined SFT datasets for the next round. This iterative process yielded\nOpenVLThinker, a LVLM exhibiting consistently improved reasoning performance on\nchallenging benchmarks such as MathVista, MathVerse, and MathVision,\ndemonstrating the potential of our strategy for robust vision-language\nreasoning. The code, model and data are held at\nhttps://github.com/yihedeng9/OpenVLThinker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16543", "pdf": "https://arxiv.org/pdf/2503.16543", "abs": "https://arxiv.org/abs/2503.16543", "authors": ["Hanae Elmekki", "Saidul Islam", "Ahmed Alagha", "Hani Sami", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Shakti Singh", "Azzam Mourad"], "title": "Comprehensive Review of Reinforcement Learning for Medical Ultrasound Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "89 pages, 23 figures", "summary": "Medical Ultrasound (US) imaging has seen increasing demands over the past\nyears, becoming one of the most preferred imaging modalities in clinical\npractice due to its affordability, portability, and real-time capabilities.\nHowever, it faces several challenges that limit its applicability, such as\noperator dependency, variability in interpretation, and limited resolution,\nwhich are amplified by the low availability of trained experts. This calls for\nthe need of autonomous systems that are capable of reducing the dependency on\nhumans for increased efficiency and throughput. Reinforcement Learning (RL)\ncomes as a rapidly advancing field under Artificial Intelligence (AI) that\nallows the development of autonomous and intelligent agents that are capable of\nexecuting complex tasks through rewarded interactions with their environments.\nExisting surveys on advancements in the US scanning domain predominantly focus\non partially autonomous solutions leveraging AI for scanning guidance, organ\nidentification, plane recognition, and diagnosis. However, none of these\nsurveys explore the intersection between the stages of the US process and the\nrecent advancements in RL solutions. To bridge this gap, this review proposes a\ncomprehensive taxonomy that integrates the stages of the US process with the RL\ndevelopment pipeline. This taxonomy not only highlights recent RL advancements\nin the US domain but also identifies unresolved challenges crucial for\nachieving fully autonomous US systems. This work aims to offer a thorough\nreview of current research efforts, highlighting the potential of RL in\nbuilding autonomous US solutions while identifying limitations and\nopportunities for further advancements in this field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16842", "pdf": "https://arxiv.org/pdf/2503.16842", "abs": "https://arxiv.org/abs/2503.16842", "authors": ["Basar Demir", "Soumitri Chattopadhyay", "Thomas Hastings Greer", "Boqi Chen", "Marc Niethammer"], "title": "Downstream Analysis of Foundational Medical Vision Models for Disease Progression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical vision foundational models are used for a wide variety of tasks,\nincluding medical image segmentation and registration. This work evaluates the\nability of these models to predict disease progression using a simple linear\nprobe. We hypothesize that intermediate layer features of segmentation models\ncapture structural information, while those of registration models encode\nknowledge of change over time. Beyond demonstrating that these features are\nuseful for disease progression prediction, we also show that registration model\nfeatures do not require spatially aligned input images. However, for\nsegmentation models, spatial alignment is essential for optimal performance.\nOur findings highlight the importance of spatial alignment and the utility of\nfoundation model features for image registration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16872", "pdf": "https://arxiv.org/pdf/2503.16872", "abs": "https://arxiv.org/abs/2503.16872", "authors": ["Xuan Wang", "Siyuan Liang", "Dongping Liao", "Han Fang", "Aishan Liu", "Xiaochun Cao", "Yu-liang Lu", "Ee-Chien Chang", "Xitong Gao"], "title": "Lie Detector: Unified Backdoor Detection via Cross-Examination Framework", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Institutions with limited data and computing resources often outsource model\ntraining to third-party providers in a semi-honest setting, assuming adherence\nto prescribed training protocols with pre-defined learning paradigm (e.g.,\nsupervised or semi-supervised learning). However, this practice can introduce\nsevere security risks, as adversaries may poison the training data to embed\nbackdoors into the resulting model. Existing detection approaches predominantly\nrely on statistical analyses, which often fail to maintain universally accurate\ndetection accuracy across different learning paradigms. To address this\nchallenge, we propose a unified backdoor detection framework in the semi-honest\nsetting that exploits cross-examination of model inconsistencies between two\nindependent service providers. Specifically, we integrate central kernel\nalignment to enable robust feature similarity measurements across different\nmodel architectures and learning paradigms, thereby facilitating precise\nrecovery and identification of backdoor triggers. We further introduce backdoor\nfine-tuning sensitivity analysis to distinguish backdoor triggers from\nadversarial perturbations, substantially reducing false positives. Extensive\nexperiments demonstrate that our method achieves superior detection\nperformance, improving accuracy by 5.4%, 1.6%, and 11.9% over SoTA baselines\nacross supervised, semi-supervised, and autoregressive learning tasks,\nrespectively. Notably, it is the first to effectively detect backdoors in\nmultimodal large language models, further highlighting its broad applicability\nand advancing secure deep learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.16965", "pdf": "https://arxiv.org/pdf/2503.16965", "abs": "https://arxiv.org/abs/2503.16965", "authors": ["Zhe Hu", "Jing Li", "Yu Yin"], "title": "When Words Outperform Vision: VLMs Can Self-Improve Via Text-Only Training For Human-Centered Decision Making", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Embodied decision-making is fundamental for AI agents operating in real-world\nenvironments. While Visual Language Models (VLMs) have advanced this\ncapability, they still struggle with complex decisions, particularly in\nhuman-centered situations that require deep reasoning about human needs and\nvalues. In this study, we systematically evaluate open-sourced VLMs on\nmultimodal human-centered decision-making tasks. We find that LLMs receiving\nonly textual descriptions unexpectedly outperform their VLM counterparts of\nsimilar scale that process actual images, suggesting that visual alignment may\nhinder VLM abilities. To address this challenge, we propose a novel text-only\ntraining approach with synthesized textual data. This method strengthens VLMs'\nlanguage components and transfers the learned abilities to multimodal\ninference, eliminating the need for expensive image-text paired data.\nFurthermore, we show that VLMs can achieve substantial performance gains\nthrough self-improvement, using training data generated by their LLM\ncounterparts rather than relying on larger teacher models like GPT-4. Our\nfindings establish a more efficient and scalable approach to enhancing VLMs'\nhuman-centered decision-making capabilities, opening new avenues for optimizing\nVLMs through self-improvement mechanisms.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17211", "pdf": "https://arxiv.org/pdf/2503.17211", "abs": "https://arxiv.org/abs/2503.17211", "authors": ["Zilin Dai", "Lehong Wang", "Fangzhou Lin", "Yidong Wang", "Zhigang Li", "Kazunori D Yamada", "Ziming Zhang", "Wang Lu"], "title": "A Language Anchor-Guided Method for Robust Noisy Domain Generalization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-24.jsonl"}
{"id": "2503.17340", "pdf": "https://arxiv.org/pdf/2503.17340", "abs": "https://arxiv.org/abs/2503.17340", "authors": ["Congyi Fan", "Jian Guan", "Xuanjia Zhao", "Dongli Xu", "Youtian Lin", "Tong Ye", "Pengming Feng", "Haiwei Pan"], "title": "Align Your Rhythm: Generating Highly Aligned Dance Poses with Gating-Enhanced Rhythm-Aware Feature Representation", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": "10 pages, 6 figures", "summary": "Automatically generating natural, diverse and rhythmic human dance movements\ndriven by music is vital for virtual reality and film industries. However,\ngenerating dance that naturally follows music remains a challenge, as existing\nmethods lack proper beat alignment and exhibit unnatural motion dynamics. In\nthis paper, we propose Danceba, a novel framework that leverages gating\nmechanism to enhance rhythm-aware feature representation for music-driven dance\ngeneration, which achieves highly aligned dance poses with enhanced rhythmic\nsensitivity. Specifically, we introduce Phase-Based Rhythm Extraction (PRE) to\nprecisely extract rhythmic information from musical phase data, capitalizing on\nthe intrinsic periodicity and temporal structures of music. Additionally, we\npropose Temporal-Gated Causal Attention (TGCA) to focus on global rhythmic\nfeatures, ensuring that dance movements closely follow the musical rhythm. We\nalso introduce Parallel Mamba Motion Modeling (PMMM) architecture to separately\nmodel upper and lower body motions along with musical features, thereby\nimproving the naturalness and diversity of generated dance movements. Extensive\nexperiments confirm that Danceba outperforms state-of-the-art methods,\nachieving significantly better rhythmic alignment and motion diversity. Project\npage: https://danceba.github.io/ .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-24.jsonl"}
