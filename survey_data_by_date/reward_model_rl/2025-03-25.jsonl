{"id": "2503.18013", "pdf": "https://arxiv.org/pdf/2503.18013", "abs": "https://arxiv.org/abs/2503.18013", "authors": ["Yufei Zhan", "Yousong Zhu", "Shurong Zheng", "Hongyin Zhao", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "title": "Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Project in development. Github:\n  https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1", "summary": "Large Vision-Language Models (LVLMs) typically follow a two-stage training\nparadigm-pretraining and supervised fine-tuning. Recently, preference\noptimization, derived from the language domain, has emerged as an effective\npost-training reinforcement strategy to enhance capabilities of LVLMs. However,\nconstructing high-quality human-annotated preference data and developing robust\nreward models to mimic these preferences are both costly and challenging.\nMotivated by this observation, we propose Vision-R1, a novel vision-guided\nR1-like reinforcement learning algorithm for LVLMs that rewards models with\ndefinitive vision feedback. It only leverages curated instruction data,\neliminating the need for specialized reward models and handcrafted preference\ndatasets. We incorporate a criterion-driven reward function that further\nintegrates multi-dimensional feedback to evaluate model completions\ncomprehensively based on the vision task logic. Furthermore, we introduce a\nprogressive rule refinement strategy that dynamically adjusts the reward\ncriteria during training, enabling continuous model improvement and mitigating\nreward hacking. Extensive experiments on both in-distribution and\nout-of-distribution benchmarks demonstrate that fine-tuning the 7B LVLMs with\nVision-R1 achieves consistent performance gains, with even up to 50%\nimprovement and surpassing the state-of-the-art 10x size model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "preference", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional", "criteria"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18454", "pdf": "https://arxiv.org/pdf/2503.18454", "abs": "https://arxiv.org/abs/2503.18454", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xierui Wang", "Xiaoyin Xu", "Min Zhang"], "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR2025", "summary": "Without using explicit reward, direct preference optimization (DPO) employs\npaired human preference data to fine-tune generative models, a method that has\ngarnered considerable attention in large language models (LLMs). However,\nexploration of aligning text-to-image (T2I) diffusion models with human\npreferences remains limited. In comparison to supervised fine-tuning, existing\nmethods that align diffusion model suffer from low training efficiency and\nsubpar generation quality due to the long Markov chain process and the\nintractability of the reverse process. To address these limitations, we\nintroduce DDIM-InPO, an efficient method for direct preference alignment of\ndiffusion models. Our approach conceptualizes diffusion model as a single-step\ngenerative model, allowing us to fine-tune the outputs of specific latent\nvariables selectively. In order to accomplish this objective, we first assign\nimplicit rewards to any latent variable directly via a reparameterization\ntechnique. Then we construct an Inversion technique to estimate appropriate\nlatent variables for preference optimization. This modification process enables\nthe diffusion model to only fine-tune the outputs of latent variables that have\na strong correlation with the preference dataset. Experimental results indicate\nthat our DDIM-InPO achieves state-of-the-art performance with just 400 steps of\nfine-tuning, surpassing all preference aligning baselines for T2I diffusion\nmodels in human preference evaluation tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "preference dataset", "human preference", "correlation"], "score": 5}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17662", "pdf": "https://arxiv.org/pdf/2503.17662", "abs": "https://arxiv.org/abs/2503.17662", "authors": ["Ke Ji", "Yixin Lian", "Linxu Li", "Jingsheng Gao", "Weiyuan Li", "Bin Dai"], "title": "Enhancing Persona Consistency for LLMs' Role-Playing using Persona-Aware Contrastive Learning", "categories": ["cs.CL"], "comment": "18 pages, 4 figures", "summary": "In recent years, large language models (LLMs) have achieved breakthrough\nprogress in many dialogue generation tasks. However, their lack of emotion and\nfine-grained role awareness limits the model's ability to provide personalized\nand diverse interactions further. Current methods face high costs in collecting\nhigh-quality annotated data for scenarios such as role-playing, and traditional\nhuman alignment methods are difficult to deploy due to the inherent diversity\nof model behavior in role-playing scenarios. Inspired by the alignment of\nmodels for safety behaviors through RLHF (Reinforcement Learning from Human\nFeedback), in this paper, we revisit model role-playing behavior from the\nperspective of persona alignment and propose a novel annotation-free framework\nnamed \\textbf{\\underline{P}}ersona-Aware \\textbf{\\underline{C}}ontrastive\n\\textbf{\\underline{L}}earning (PCL) to align LLMs' behavior during\nrole-playing, enhancing the model's role consistency. Specifically, we first\ndesign a role chain method to encourage the model to self-question based on the\nrole characteristics and dialogue context to adjust personality consistency.\nThen, we further enhance the model's role-playing strategy through iterative\ncontrastive learning between the use of role characteristics and not.\nExperiments on both black-box and white-box LLMs show that LLMs equipped with\nPCL significantly outperform vanilla LLMs under automatic evaluation methods\n(CharEval \\& GPT-4) and human expert evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "alignment", "human alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "safety", "consistency", "dialogue", "fine-grained"], "score": 6}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17965", "pdf": "https://arxiv.org/pdf/2503.17965", "abs": "https://arxiv.org/abs/2503.17965", "authors": ["Beining Xu", "Arkaitz Zubiaga"], "title": "Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "14 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance on a\nrange of downstream NLP tasks by generating text that closely resembles human\nwriting. However, the ease of achieving this similarity raises concerns from\npotential malicious uses at scale by bad actors, as LLM-generated text becomes\nincreasingly difficult to discern from human text. Although detection methods\nhave been developed to address this issue, bad actors can further manipulate\nLLM-generated texts to make them less detectable. In this work, we study how\nfurther editing texts with Reinforcement Learning from Human Feedback (RLHF),\nwhich aligns model outputs with human preferences, affects (a) the quality of\ngenerated texts for two tasks, and (b) the performance of LLM-generated text\ndetectors, looking at both training-based and zero-shot detection methods.\nAlthough RLHF improves the quality of LLM-generated texts, we find that it also\ntends to produce more detectable, lengthy, and repetitive outputs.\nAdditionally, we observe that training-based detectors are vulnerable to short\ntexts and to texts that incorporate code, whereas zero-shot detectors exhibit\ngreater robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "testbed"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17882", "pdf": "https://arxiv.org/pdf/2503.17882", "abs": "https://arxiv.org/abs/2503.17882", "authors": ["Shengyun Si", "Xinpeng Wang", "Guangyao Zhai", "Nassir Navab", "Barbara Plank"], "title": "Think Before Refusal : Triggering Safety Reflection in LLMs to Mitigate False Refusal Behavior", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 23 figures", "summary": "Recent advancements in large language models (LLMs) have demonstrated that\nfine-tuning and human alignment can render LLMs harmless. In practice, such\n\"harmlessness\" behavior is mainly achieved by training models to reject harmful\nrequests, such as \"Explain how to burn down my neighbor's house\", where the\nmodel appropriately declines to respond. However, this approach can\ninadvertently result in false refusal, where models reject benign queries as\nwell, such as \"Tell me how to kill a Python process\". In this work, we\ndemonstrate that prompting safety reflection before generating a response can\nmitigate false refusal behavior. Building on this finding, we introduce the\nThink-Before-Refusal (TBR) schema and conduct safety-aware instruction\nfine-tuning incorporating safety reflection. In an ablation study across 15\npre-trained models, we show that models fine-tuned with safety reflection\nsignificantly reduce false refusal behavior while maintaining safety and\noverall performance compared to those fine-tuned without safety reflection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["harmlessness", "safety"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17660", "pdf": "https://arxiv.org/pdf/2503.17660", "abs": "https://arxiv.org/abs/2503.17660", "authors": ["Kun Li", "Jianhui Wang", "Miao Zhang", "Xueqian Wang"], "title": "OMR-Diffusion:Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Intent Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Generative AI has significantly advanced text-driven image generation, but it\nstill faces challenges in producing outputs that consistently align with\nevolving user preferences and intents, particularly in multi-turn dialogue\nscenarios. In this research, We present a Visual Co-Adaptation (VCA) framework\nthat incorporates human-in-the-loop feedback, utilizing a well-trained reward\nmodel specifically designed to closely align with human preferences. Using a\ndiverse multi-turn dialogue dataset, the framework applies multiple reward\nfunctions (such as diversity, consistency, and preference feedback) to refine\nthe diffusion model through LoRA, effectively optimizing image generation based\non user input. We also constructed multi-round dialogue datasets with prompts\nand image pairs that well-fit user intent. Experiments show the model achieves\n508 wins in human evaluation, outperforming DALL-E 3 (463 wins) and others. It\nalso achieves 3.4 rounds in dialogue efficiency (vs. 13.7 for DALL-E 3) and\nexcels in metrics like LPIPS (0.15) and BLIP (0.59). Various experiments\ndemonstrate the effectiveness of the proposed method over state-of-the-art\nbaselines, with significant improvements in image consistency and alignment\nwith user intent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "dialogue"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17669", "pdf": "https://arxiv.org/pdf/2503.17669", "abs": "https://arxiv.org/abs/2503.17669", "authors": ["Yuheng Feng", "Jianhui Wang", "Kun Li", "Sida Li", "Tianyu Shi", "Haoyue Han", "Miao Zhang", "Xueqian Wang"], "title": "TDRI: Two-Phase Dialogue Refinement and Co-Adaptation for Interactive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Although text-to-image generation technologies have made significant\nadvancements, they still face challenges when dealing with ambiguous prompts\nand aligning outputs with user intent.Our proposed framework, TDRI (Two-Phase\nDialogue Refinement and Co-Adaptation), addresses these issues by enhancing\nimage generation through iterative user interaction. It consists of two phases:\nthe Initial Generation Phase, which creates base images based on user prompts,\nand the Interactive Refinement Phase, which integrates user feedback through\nthree key modules. The Dialogue-to-Prompt (D2P) module ensures that user\nfeedback is effectively transformed into actionable prompts, which improves the\nalignment between user intent and model input. By evaluating generated outputs\nagainst user expectations, the Feedback-Reflection (FR) module identifies\ndiscrepancies and facilitates improvements. In an effort to ensure consistently\nhigh-quality results, the Adaptive Optimization (AO) module fine-tunes the\ngeneration process by balancing user preferences and maintaining prompt\nfidelity. Experimental results show that TDRI outperforms existing methods by\nachieving 33.6% human preference, compared to 6.2% for GPT-4 augmentation, and\nthe highest CLIP and BLIP alignment scores (0.338 and 0.336, respectively). In\niterative feedback tasks, user satisfaction increased to 88% after 8 rounds,\nwith diminishing returns beyond 6 rounds. Furthermore, TDRI has been found to\nreduce the number of iterations and improve personalization in the creation of\nfashion products. TDRI exhibits a strong potential for a wide range of\napplications in the creative and industrial domains, as it streamlines the\ncreative process and improves alignment with user preferences", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "dialogue"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18089", "pdf": "https://arxiv.org/pdf/2503.18089", "abs": "https://arxiv.org/abs/2503.18089", "authors": ["Javad SeraJ", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti"], "title": "$D^2LoRA$: Data-Driven LoRA Initialization for Low Resource Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Tuning large language models is essential for optimizing their performance\nacross diverse applications, particularly in scenarios with limited data\navailability. Tuning large language models in scarce data scenarios is crucial,\nparticularly given that the convergence speed of the LoRA method is lower than\nthat of full fine-tuning. In this paper, we present an analysis of\npost-training methods including Supervised Fine-Tuning (SFT), Direct Preference\nOptimization (DPO), and Odds Ratio Preference Optimization (ORPO) within the\ncontext of task-specific learning using the LoRA method. Next we introduce\n$D^2LoRA$, a data-driven approach for initializing LoRA metrics that enhances\ntraining efficiency, especially in limited-data settings. Our experiments\ncompare $D^2LoRA$ with vanilla LoRA in terms of performance and catastrophic\nforgetting under extremely data-constrained conditions. The results demonstrate\nthat $D^2LoRA$ achieves a 1% improvement GSM8K benchmark and a 2-point\nimprovement in ROUGE score in title generation tasks. $D^2LoRA$ facilitates the\nadaptation of LLMs to multiple tasks even when task-specific data is scarce,\nthereby reducing training expenses and offering data cost.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18432", "pdf": "https://arxiv.org/pdf/2503.18432", "abs": "https://arxiv.org/abs/2503.18432", "authors": ["Junsong Li", "Jie Zhou", "Yutao Yang", "Bihao Zhan", "Qianjun Pan", "Yuyang Ding", "Qin Chen", "Jiang Bo", "Xin Lin", "Liang He"], "title": "Teaching LLMs for Step-Level Automatic Math Correction via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic math correction aims to check students' solutions to mathematical\nproblems via artificial intelligence technologies. Most existing studies focus\non judging the final answer at the problem level, while they ignore detailed\nfeedback on each step in a math problem-solving process, which requires\nabilities of semantic understanding and reasoning. In this paper, we propose a\nreinforcement learning (RL)-based method to boost large language model (LLM)\nfor step-level automatic math correction, named StepAMC. Particularly, we\nconvert the step-level automatic math correction within the text classification\ntask into an RL problem to enhance the reasoning capabilities of LLMs. Then, we\ndesign a space-constrained policy network to improve the stability of RL. Then,\nwe introduce a fine-grained reward network to convert the binary human feedback\ninto a continuous value. We conduct extensive experiments over two benchmark\ndatasets and the results show that our model outperforms the eleven strong\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17928", "pdf": "https://arxiv.org/pdf/2503.17928", "abs": "https://arxiv.org/abs/2503.17928", "authors": ["Zefeng Zhang", "Hengzhu Tang", "Jiawei Sheng", "Zhenyu Zhang", "Yiming Ren", "Zhenyang Li", "Dawei Yin", "Duohe Ma", "Tingwen Liu"], "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Multimodal Large Language Models excel in various tasks, yet often struggle\nwith modality bias, where the model tends to rely heavily on a single modality\nand overlook critical information in other modalities, which leads to incorrect\nfocus and generating irrelevant responses. In this paper, we propose using the\nparadigm of preference optimization to solve the modality bias problem,\nincluding RLAIFVBias, a debiased preference optimization dataset, and a Noise\nAware Preference Optimization algorithm. Specifically, we first construct the\ndataset by introducing perturbations to reduce the informational content of\ncertain modalities, compelling the model to rely on a specific modality when\ngenerating negative responses. To address the inevitable noise in automatically\nconstructed data, we combine the noise robust Mean Absolute Error with the\nBinary Cross Entropy in Direct Preference Optimization by a negative Box Cox\ntransformation, and dynamically adjust the algorithm noise robustness based on\nthe evaluated noise levels in the data. Extensive experiments validate our\napproach, demonstrating not only its effectiveness in mitigating modality bias\nbut also its significant role in minimizing hallucinations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17489", "pdf": "https://arxiv.org/pdf/2503.17489", "abs": "https://arxiv.org/abs/2503.17489", "authors": ["Shu Pu", "Yaochen Wang", "Dongping Chen", "Yuhang Chen", "Guohao Wang", "Qi Qin", "Zhongyi Zhang", "Zhiyuan Zhang", "Zetong Zhou", "Shuang Gong", "Yi Gui", "Yao Wan", "Philip S. Yu"], "title": "Judge Anything: MLLM as a Judge Across Any Modality", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating generative foundation models on open-ended multimodal\nunderstanding (MMU) and generation (MMG) tasks across diverse modalities (e.g.,\nimages, audio, video) poses significant challenges due to the complexity of\ncross-modal interactions. To this end, the idea of utilizing Multimodal LLMs\n(MLLMs) as automated judges has emerged, with encouraging results in assessing\nvision-language understanding tasks. Moving further, this paper extends\nMLLM-as-a-Judge across modalities to a unified manner by introducing two\nbenchmarks, TaskAnything and JudgeAnything, to respectively evaluate the\noverall performance and judging capabilities of MLLMs across any-to-any\nmodality tasks. Specifically, TaskAnything evaluates the MMU and MMG\ncapabilities across 15 any-to-any modality categories, employing 1,500 queries\ncurated from well-established benchmarks. Furthermore, JudgeAnything evaluates\nthe judging capabilities of 5 advanced (e.g., GPT-4o and Gemini-2.0-Flash) from\nthe perspectives of Pair Comparison and Score Evaluation, providing a\nstandardized testbed that incorporates human judgments and detailed rubrics.\nOur extensive experiments reveal that while these MLLMs show promise in\nassessing MMU (i.e., achieving an average of 66.55% in Pair Comparison setting\nand 42.79% in Score Evaluation setting), they encounter significant challenges\nwith MMG tasks (i.e., averaging only 53.37% in Pair Comparison setting and\n30.05% in Score Evaluation setting), exposing cross-modality biases and\nhallucination issues. To address this, we present OmniArena, an automated\nplatform for evaluating omni-models and multimodal reward models. Our work\nhighlights the need for fairer evaluation protocols and stronger alignment with\nhuman preferences. The source code and dataset are publicly available at:\nhttps://urrealhero.github.io/judgeanythingweb/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "testbed"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17493", "pdf": "https://arxiv.org/pdf/2503.17493", "abs": "https://arxiv.org/abs/2503.17493", "authors": ["Aidos Konyspay", "Pakizar Shamoi", "Malika Ziyada", "Zhusup Smambayev"], "title": "Meme Similarity and Emotion Detection using Multimodal Analysis", "categories": ["cs.CV"], "comment": "Have been submitted to IEEE for consideration", "summary": "Internet memes are a central element of online culture, blending images and\ntext. While substantial research has focused on either the visual or textual\ncomponents of memes, little attention has been given to their interplay. This\ngap raises a key question: What methodology can effectively compare memes and\nthe emotions they elicit? Our study employs a multimodal methodological\napproach, analyzing both the visual and textual elements of memes.\nSpecifically, we perform a multimodal CLIP (Contrastive Language-Image\nPre-training) model for grouping similar memes based on text and visual content\nembeddings, enabling robust similarity assessments across modalities. Using the\nReddit Meme Dataset and Memotion Dataset, we extract low-level visual features\nand high-level semantic features to identify similar meme pairs. To validate\nthese automated similarity assessments, we conducted a user study with 50\nparticipants, asking them to provide yes/no responses regarding meme similarity\nand their emotional reactions. The comparison of experimental results with\nhuman judgments showed a 67.23\\% agreement, suggesting that the computational\napproach aligns well with human perception. Additionally, we implemented a\ntext-based classifier using the DistilBERT model to categorize memes into one\nof six basic emotions. The results indicate that anger and joy are the dominant\nemotions in memes, with motivational memes eliciting stronger emotional\nresponses. This research contributes to the study of multimodal memes,\nenhancing both language-based and visual approaches to analyzing and improving\nonline visual communication and user experiences. Furthermore, it provides\ninsights for better content moderation strategies in online platforms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "agreement"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17753", "pdf": "https://arxiv.org/pdf/2503.17753", "abs": "https://arxiv.org/abs/2503.17753", "authors": ["Hojun Cho", "Donghu Kim", "Soyoung Yang", "Chan Lee", "Hunjoo Lee", "Jaegul Choo"], "title": "Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Language agents powered by large language models (LLMs) face significant\ndeployment challenges in resource-constrained environments, particularly for\nspecialized domains and less-common languages. This paper presents Tox-chat, a\nKorean chemical toxicity information agent devised within these limitations. We\npropose two key innovations: a context-efficient architecture that reduces\ntoken consumption through hierarchical section search, and a scenario-based\ndialogue generation methodology that effectively distills tool-using\ncapabilities from larger models. Experimental evaluations demonstrate that our\nfine-tuned 8B parameter model substantially outperforms both untuned models and\nbaseline approaches, in terms of DB faithfulness and preference. Our work\noffers valuable insights for researchers developing domain-specific language\nagents under practical constraints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17755", "pdf": "https://arxiv.org/pdf/2503.17755", "abs": "https://arxiv.org/abs/2503.17755", "authors": ["Sharan Maiya", "Yinhong Liu", "Ramit Debnath", "Anna Korhonen"], "title": "Improving Preference Extraction In LLMs By Identifying Latent Knowledge Through Classifying Probes", "categories": ["cs.CL", "cs.LG"], "comment": "preprint, submitted to ACL ARR 2025, 21 pages, 23 figures", "summary": "Large Language Models (LLMs) are often used as automated judges to evaluate\ntext, but their effectiveness can be hindered by various unintentional biases.\nWe propose using linear classifying probes, trained by leveraging differences\nbetween contrasting pairs of prompts, to directly access LLMs' latent knowledge\nand extract more accurate preferences. Through extensive experiments using\nmodels of varying size from four different families and six diverse datasets\nassessing text quality evaluation and common sense reasoning, we demonstrate\nthat both supervised and unsupervised probing approaches consistently\noutperform traditional generation-based judgement while maintaining similar\ncomputational costs. These probes generalise under domain shifts and can even\noutperform finetuned evaluators with the same training data size. Our results\nsuggest linear probing offers an accurate, robust and computationally efficient\napproach for LLM-as-judge tasks while providing interpretable insights into how\nmodels encode judgement-relevant knowledge. Our data and code will be openly\nreleased in the future.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17932", "pdf": "https://arxiv.org/pdf/2503.17932", "abs": "https://arxiv.org/abs/2503.17932", "authors": ["Xunguang Wang", "Wenxuan Wang", "Zhenlan Ji", "Zongjie Li", "Pingchuan Ma", "Daoyuan Wu", "Shuai Wang"], "title": "STShield: Single-Token Sentinel for Real-Time Jailbreak Detection in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "11 pages", "summary": "Large Language Models (LLMs) have become increasingly vulnerable to jailbreak\nattacks that circumvent their safety mechanisms. While existing defense methods\neither suffer from adaptive attacks or require computationally expensive\nauxiliary models, we present STShield, a lightweight framework for real-time\njailbroken judgement. STShield introduces a novel single-token sentinel\nmechanism that appends a binary safety indicator to the model's response\nsequence, leveraging the LLM's own alignment capabilities for detection. Our\nframework combines supervised fine-tuning on normal prompts with adversarial\ntraining using embedding-space perturbations, achieving robust detection while\npreserving model utility. Extensive experiments demonstrate that STShield\nsuccessfully defends against various jailbreak attacks, while maintaining the\nmodel's performance on legitimate queries. Compared to existing approaches,\nSTShield achieves superior defense performance with minimal computational\noverhead, making it a practical solution for real-world LLM deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17651", "pdf": "https://arxiv.org/pdf/2503.17651", "abs": "https://arxiv.org/abs/2503.17651", "authors": ["Zhuo Tao", "Liang Li", "Qi Chen", "Yunbin Tu", "Zheng-Jun Zha", "Ming-Hsuan Yang", "Yuankai Qi", "Qingming Huang"], "title": "Collaborative Temporal Consistency Learning for Point-supervised Natural Language Video Localization", "categories": ["cs.CV"], "comment": "Under review", "summary": "Natural language video localization (NLVL) is a crucial task in video\nunderstanding that aims to localize the target moment in videos specified by a\ngiven language description. Recently, a point-supervised paradigm has been\npresented to address this task, requiring only a single annotated frame within\nthe target moment rather than complete temporal boundaries. Compared with the\nfully-supervised paradigm, it offers a balance between localization accuracy\nand annotation cost. However, due to the absence of complete annotation, it is\nchallenging to align the video content with language descriptions, consequently\nhindering accurate moment prediction. To address this problem, we propose a new\nCOllaborative Temporal consistEncy Learning (COTEL) framework that leverages\nthe synergy between saliency detection and moment localization to strengthen\nthe video-language alignment. Specifically, we first design a frame- and a\nsegment-level Temporal Consistency Learning (TCL) module that models semantic\nalignment across frame saliencies and sentence-moment pairs. Then, we design a\ncross-consistency guidance scheme, including a Frame-level Consistency Guidance\n(FCG) and a Segment-level Consistency Guidance (SCG), that enables the two\ntemporal consistency learning paths to reinforce each other mutually. Further,\nwe introduce a Hierarchical Contrastive Alignment Loss (HCAL) to\ncomprehensively align the video and text query. Extensive experiments on two\nbenchmarks demonstrate that our method performs favorably against SoTA\napproaches. We will release all the source codes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17657", "pdf": "https://arxiv.org/pdf/2503.17657", "abs": "https://arxiv.org/abs/2503.17657", "authors": ["Yumeng Ren", "Yaofang Liu", "Aitor Artola", "Laurent Mertz", "Raymond H. Chan", "Jean-michel Morel"], "title": "Efficient Diffusion Training through Parallelization with Truncated Karhunen-Loève Expansion", "categories": ["cs.CV", "I.2.0; I.4.0"], "comment": "12 pages, 9 figures", "summary": "Diffusion denoising models have become a popular approach for image\ngeneration, but they often suffer from slow convergence during training. In\nthis paper, we identify that this slow convergence is partly due to the\ncomplexity of the Brownian motion driving the forward-time process. To address\nthis, we represent the Brownian motion using the Karhunen-Lo\\`eve expansion,\ntruncating it to a limited number of eigenfunctions. We propose a novel\nordinary differential equation with augmented random initials, termed KL\ndiffusion, as a new forward-time process for training and sampling. By\ndeveloping an appropriate denoising loss function, we facilitate the\nintegration of our KL-diffusion into existing denoising-based models. Using the\nwidely adopted DDIM framework as our baseline ensures a fair comparison, as our\nmodifications focus solely on the forward process and loss function, leaving\nthe network architecture and sampling methods unchanged. Our method\nsignificantly outperforms baseline diffusion models, achieving convergence\nspeeds that are twice faster to reach the best FID score of the baseline and\nultimately yielding much lower FID scores. Notably, our approach allows for\nhighly parallelized computation, requires no additional learnable parameters,\nand can be flexibly integrated into existing diffusion methods. The code will\nbe made publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17668", "pdf": "https://arxiv.org/pdf/2503.17668", "abs": "https://arxiv.org/abs/2503.17668", "authors": ["Usha Kumari", "Shuvendu Rana"], "title": "3D Modeling: Camera Movement Estimation and path Correction for SFM Model using the Combination of Modified A-SIFT and Stereo System", "categories": ["cs.CV"], "comment": null, "summary": "Creating accurate and efficient 3D models poses significant challenges,\nparticularly in addressing large viewpoint variations, computational\ncomplexity, and alignment discrepancies. Efficient camera path generation can\nhelp resolve these issues. In this context, a modified version of the Affine\nScale-Invariant Feature Transform (ASIFT) is proposed to extract more matching\npoints with reduced computational overhead, ensuring an adequate number of\ninliers for precise camera rotation angle estimation. Additionally, a novel\ntwo-camera-based rotation correction model is introduced to mitigate small\nrotational errors, further enhancing accuracy. Furthermore, a stereo\ncamera-based translation estimation and correction model is implemented to\ndetermine camera movement in 3D space by altering the Structure From Motion\n(SFM) model. Finally, the novel combination of ASIFT and two camera-based SFM\nmodels provides an accurate camera movement trajectory in 3D space.\nExperimental results show that the proposed camera movement approach achieves\n99.9% accuracy compared to the actual camera movement path and outperforms\nstate-of-the-art camera path estimation methods. By leveraging this accurate\ncamera path, the system facilitates the creation of precise 3D models, making\nit a robust solution for applications requiring high fidelity and efficiency in\n3D reconstruction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18008", "pdf": "https://arxiv.org/pdf/2503.18008", "abs": "https://arxiv.org/abs/2503.18008", "authors": ["Kyuyoung Kim", "Jinwoo Shin", "Jaehyung Kim"], "title": "Personalized Language Models via Privacy-Preserving Evolutionary Model Merging", "categories": ["cs.CL", "cs.NE"], "comment": "Preprint", "summary": "Personalization in large language models (LLMs) seeks to tailor models to\nindividual user or user group preferences. Prompt-based methods augment queries\nwith user preference information, whereas training-based methods directly\nencode preferences into model parameters for more effective personalization.\nDespite achieving some success in personalizing LLMs, prior methods often fail\nto directly optimize task-specific metrics and lack explicit\nprivacy-preservation mechanisms. To address these limitations, we propose\nPrivacy-Preserving Model Merging via Evolutionary Algorithms (PriME), a novel\napproach to personalization that employs gradient-free methods to directly\noptimize task-specific metrics while preserving user privacy. By incorporating\nprivacy preservation into optimization, PriME produces a personalized module\nthat effectively captures the target user's preferences while minimizing the\nprivacy risks for the users sharing their private information. Experiments on\nthe LaMP benchmark show that PriME outperforms both prompt-based and\ntraining-based methods, achieving up to a 45% performance improvement over the\nprior art. Further analysis shows that PriME achieves a significantly better\nprivacy-utility trade-off, highlighting the potential of evolutionary\napproaches for privacy-preserving LLM personalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17675", "pdf": "https://arxiv.org/pdf/2503.17675", "abs": "https://arxiv.org/abs/2503.17675", "authors": ["Shulei Wang", "Wang Lin", "Hai Huang", "Hanting Wang", "Sihang Cai", "WenKang Han", "Tao Jin", "Jingyuan Chen", "Jiacheng Sun", "Jieming Zhu", "Zhou Zhao"], "title": "Towards Transformer-Based Aligned Generation with Self-Coherence Guidance", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "We introduce a novel, training-free approach for enhancing alignment in\nTransformer-based Text-Guided Diffusion Models (TGDMs). Existing TGDMs often\nstruggle to generate semantically aligned images, particularly when dealing\nwith complex text prompts or multi-concept attribute binding challenges.\nPrevious U-Net-based methods primarily optimized the latent space, but their\ndirect application to Transformer-based architectures has shown limited\neffectiveness. Our method addresses these challenges by directly optimizing\ncross-attention maps during the generation process. Specifically, we introduce\nSelf-Coherence Guidance, a method that dynamically refines attention maps using\nmasks derived from previous denoising steps, ensuring precise alignment without\nadditional training. To validate our approach, we constructed more challenging\nbenchmarks for evaluating coarse-grained attribute binding, fine-grained\nattribute binding, and style binding. Experimental results demonstrate the\nsuperior performance of our method, significantly surpassing other\nstate-of-the-art methods across all evaluated tasks. Our code is available at\nhttps://scg-diffusion.github.io/scg-diffusion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18076", "pdf": "https://arxiv.org/pdf/2503.18076", "abs": "https://arxiv.org/abs/2503.18076", "authors": ["Somnath Roy", "Padharthi Sreekar", "Srivatsa Narasimha", "Anubhav Anand"], "title": "A Multi-Model Adaptation of Speculative Decoding for Classification", "categories": ["cs.CL"], "comment": null, "summary": "The current study introduces a novel adaptation of speculative decoding,\nrepurposed from generation to classification tasks. We propose a multi-model\nframework employing up to three lightweight worker models and a single, more\nrobust judge model analogous to draft models and target model, respectively, in\nspeculative decoding. The worker models, tasked with the bulk of the\ncomputation, independently predict discrete class labels for a given input.\nWhen majority worker models agree on a label, it is accepted as the final\nlabel, optimizing efficiency by bypassing the computationally expensive judge\nmodel. In cases of disagreement, the judge model intervenes to resolve the\nlabel. This approach minimizes redundant computation, leverages the redundancy\nof multiple workers for confidence, and confines the judge model's role to\nchallenging cases, offering a practical balance of efficiency and accuracy. Our\nanalysis suggests that smaller out of the box instruction/chat finetuned worker\nmodels with 3 billion parameters (hereafter, 3B) demonstrate a level of\nalignment with judge models comparable to that of larger finetuned worker\nmodels with 7 billion parameters (hereafter, 7B) across both simple and higher\norder reasoning tasks. The top performing 3B worker model pair achieve an\nagreement rate of approximately 80-83% for sentiment and around 50-80% for\nsimilar ticket when compared to judge models. Additionally, 3B worker models\nprovide a speedup ranging from 2.8x to 9x relative to the judge models, while\n7B worker model combinations achieve a speedup ranging from 1.28x to 0.28x", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18174", "pdf": "https://arxiv.org/pdf/2503.18174", "abs": "https://arxiv.org/abs/2503.18174", "authors": ["Weronika Łajewska", "Krisztian Balog"], "title": "GINGER: Grounded Information Nugget-Based Generation of Responses", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. To address them, we\npropose a modular pipeline for grounded response generation that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. The multistage pipeline encompasses nugget detection,\nclustering, ranking, top cluster summarization, and fluency enhancement. It\nguarantees grounding in specific facts, facilitates source attribution, and\nensures maximum information inclusion within length constraints. Extensive\nexperiments on the TREC RAG'24 dataset evaluated with the AutoNuggetizer\nframework demonstrate that GINGER achieves state-of-the-art performance on this\nbenchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "summarization"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17782", "pdf": "https://arxiv.org/pdf/2503.17782", "abs": "https://arxiv.org/abs/2503.17782", "authors": ["Hyungyu Choi", "Young Kyun Jang", "Chanho Eom"], "title": "GOAL: Global-local Object Alignment Learning", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Vision-language models like CLIP have shown impressive capabilities in\naligning images and text, but they often struggle with lengthy and detailed\ntext descriptions because of their training focus on short and concise\ncaptions. We present GOAL (Global-local Object Alignment Learning), a novel\nfine-tuning method that enhances CLIP's ability to handle lengthy text by\nleveraging both global and local semantic alignments between image and lengthy\ntext. Our approach consists of two key components: Local Image-Sentence\nMatching (LISM), which identifies corresponding pairs between image segments\nand descriptive sentences, and Token Similarity-based Learning (TSL), which\nefficiently propagates local element attention through these matched pairs.\nEvaluating GOAL on three new benchmarks for image-lengthy text retrieval, we\ndemonstrate significant improvements over baseline CLIP fine-tuning,\nestablishing a simple yet effective approach for adapting CLIP to detailed\ntextual descriptions. Through extensive experiments, we show that our method's\nfocus on local semantic alignment alongside global context leads to more\nnuanced and representative embeddings, particularly beneficial for tasks\nrequiring fine-grained understanding of lengthy text descriptions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17788", "pdf": "https://arxiv.org/pdf/2503.17788", "abs": "https://arxiv.org/abs/2503.17788", "authors": ["Gaoge Han", "Yongkang Cheng", "Zhe Chen", "Shaoli Huang", "Tongliang Liu"], "title": "Aligning Foundation Model Priors and Diffusion-Based Hand Interactions for Occlusion-Resistant Two-Hand Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Two-hand reconstruction from monocular images faces persistent challenges due\nto complex and dynamic hand postures and occlusions, causing significant\ndifficulty in achieving plausible interaction alignment. Existing approaches\nstruggle with such alignment issues, often resulting in misalignment and\npenetration artifacts. To tackle this, we propose a novel framework that\nattempts to precisely align hand poses and interactions by synergistically\nintegrating foundation model-driven 2D priors with diffusion-based interaction\nrefinement for occlusion-resistant two-hand reconstruction. First, we introduce\na Fusion Alignment Encoder that learns to align fused multimodal priors\nkeypoints, segmentation maps, and depth cues from foundation models during\ntraining. This provides robust structured guidance, further enabling efficient\ninference without foundation models at test time while maintaining high\nreconstruction accuracy. Second, we employ a two-hand diffusion model\nexplicitly trained to transform interpenetrated poses into plausible,\nnon-penetrated interactions, leveraging gradient-guided denoising to correct\nartifacts and ensure realistic spatial relations. Extensive evaluations\ndemonstrate that our method achieves state-of-the-art performance on\nInterHand2.6M, FreiHAND, and HIC datasets, significantly advancing occlusion\nhandling and interaction robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17794", "pdf": "https://arxiv.org/pdf/2503.17794", "abs": "https://arxiv.org/abs/2503.17794", "authors": ["Ketan Suhaas Saichandran", "Xavier Thomas", "Prakhar Kaushik", "Deepti Ghadiyaram"], "title": "Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generative models often struggle with long prompts detailing\ncomplex scenes, diverse objects with distinct visual characteristics and\nspatial relationships. In this work, we propose SCoPE (Scheduled interpolation\nof Coarse-to-fine Prompt Embeddings), a training-free method to improve\ntext-to-image alignment by progressively refining the input prompt in a\ncoarse-to-fine-grained manner. Given a detailed input prompt, we first\ndecompose it into multiple sub-prompts which evolve from describing broad scene\nlayout to highly intricate details. During inference, we interpolate between\nthese sub-prompts and thus progressively introduce finer-grained details into\nthe generated image. Our training-free plug-and-play approach significantly\nenhances prompt alignment, achieves an average improvement of up to +4% in\nVisual Question Answering (VQA) scores over the Stable Diffusion baselines on\n85% of the prompts from the GenAI-Bench dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18250", "pdf": "https://arxiv.org/pdf/2503.18250", "abs": "https://arxiv.org/abs/2503.18250", "authors": ["Jong Myoung Kim", "Young-Jun_Lee", "Ho-Jin Choi", "Sangkeun Jung"], "title": "PAD: Towards Efficient Data Generation for Transfer Learning Using Phrase Alignment", "categories": ["cs.CL"], "comment": "Preparing for conference", "summary": "Transfer learning leverages the abundance of English data to address the\nscarcity of resources in modeling non-English languages, such as Korean. In\nthis study, we explore the potential of Phrase Aligned Data (PAD) from\nstandardized Statistical Machine Translation (SMT) to enhance the efficiency of\ntransfer learning. Through extensive experiments, we demonstrate that PAD\nsynergizes effectively with the syntactic characteristics of the Korean\nlanguage, mitigating the weaknesses of SMT and significantly improving model\nperformance. Moreover, we reveal that PAD complements traditional data\nconstruction methods and enhances their effectiveness when combined. This\ninnovative approach not only boosts model performance but also suggests a\ncost-efficient solution for resource-scarce languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18471", "pdf": "https://arxiv.org/pdf/2503.18471", "abs": "https://arxiv.org/abs/2503.18471", "authors": ["Calvin Bao", "Yow-Ting Shiue", "Marine Carpuat", "Joel Chan"], "title": "Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "26 pages, 8 tables, 6 figures", "summary": "Scholars often explore literature outside of their home community of study.\nThis exploration process is frequently hampered by field-specific jargon. Past\ncomputational work often focuses on supporting translation work by removing\njargon through simplification and summarization; here, we explore a different\napproach that preserves jargon as useful bridges to new conceptual spaces.\nSpecifically, we cast different scholarly domains as different language-using\ncommunities, and explore how to adapt techniques from unsupervised\ncross-lingual alignment of word embeddings to explore conceptual alignments\nbetween domain-specific word embedding spaces.We developed a prototype\ncross-domain search engine that uses aligned domain-specific embeddings to\nsupport conceptual exploration, and tested this prototype in two case studies.\nWe discuss qualitative insights into the promises and pitfalls of this approach\nto translation work, and suggest design insights for future interfaces that\nprovide computational support for cross-domain information seeking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18603", "pdf": "https://arxiv.org/pdf/2503.18603", "abs": "https://arxiv.org/abs/2503.18603", "authors": ["Jong Myoung Kim", "Young-Jun Lee", "Ho-Jin Choi", "Sangkeun Jung"], "title": "LANGALIGN: Enhancing Non-English Language Models via Cross-Lingual Embedding Alignment", "categories": ["cs.CL"], "comment": "now preparing", "summary": "While Large Language Models have gained attention, many service developers\nstill rely on embedding-based models due to practical constraints. In such\ncases, the quality of fine-tuning data directly impacts performance, and\nEnglish datasets are often used as seed data for training non-English models.\nIn this study, we propose LANGALIGN, which enhances target language processing\nby aligning English embedding vectors with those of the target language at the\ninterface between the language model and the task header. Experiments on\nKorean, Japanese, and Chinese demonstrate that LANGALIGN significantly improves\nperformance across all three languages. Additionally, we show that LANGALIGN\ncan be applied in reverse to convert target language data into a format that an\nEnglish-based model can process.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18646", "pdf": "https://arxiv.org/pdf/2503.18646", "abs": "https://arxiv.org/abs/2503.18646", "authors": ["Zhen-Song Chen", "Hong-Wei Ding", "Xian-Jia Wang", "Witold Pedrycz"], "title": "ZeroLM: Data-Free Transformer Architecture Search for Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Neural architecture search (NAS) provides a systematic framework for\nautomating the design of neural network architectures, yet its widespread\nadoption is hindered by prohibitive computational requirements. Existing\nzero-cost proxy methods, while reducing search overhead, demonstrate inadequate\nperformance in architecture ranking tasks, particularly for Transformer-based\nmodels where they often underperform simple parameter counting metrics. Current\nautomated proxy discovery approaches suffer from extended search times,\nsusceptibility to data overfitting, and structural complexity. This paper\nintroduces a novel zero-cost proxy methodology that quantifies model capacity\nthrough efficient weight statistics computation while decomposing Transformer\narchitectures into functionally distinct sub-modules, thereby optimizing the\nbalance of their contributions to overall performance. Our comprehensive\nevaluation demonstrates the superiority of this approach, achieving a\nSpearman's rho of 0.76 and Kendall's tau of 0.53 on the FlexiBERT benchmark.\nThe proposed method exhibits exceptional computational efficiency while\nmaintaining robust performance across diverse NAS benchmark tasks, offering a\npractical solution for large-scale architecture search.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18010", "pdf": "https://arxiv.org/pdf/2503.18010", "abs": "https://arxiv.org/abs/2503.18010", "authors": ["Thomas Dagès", "Simon Weber", "Ya-Wei Eileen Lin", "Ronen Talmon", "Daniel Cremers", "Michael Lindenbaum", "Alfred M. Bruckstein", "Ron Kimmel"], "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding", "categories": ["cs.CV"], "comment": "Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) 2025", "summary": "Dimensionality reduction is a fundamental task that aims to simplify complex\ndata by reducing its feature dimensionality while preserving essential\npatterns, with core applications in data analysis and visualisation. To\npreserve the underlying data structure, multi-dimensional scaling (MDS) methods\nfocus on preserving pairwise dissimilarities, such as distances. They optimise\nthe embedding to have pairwise distances as close as possible to the data\ndissimilarities. However, the current standard is limited to embedding data in\nRiemannian manifolds. Motivated by the lack of asymmetry in the Riemannian\nmetric of the embedding space, this paper extends the MDS problem to a natural\nasymmetric generalisation of Riemannian manifolds called Finsler manifolds.\nInspired by Euclidean space, we define a canonical Finsler space for embedding\nasymmetric data. Due to its simplicity with respect to geodesics, data\nrepresentation in this space is both intuitive and simple to analyse. We\ndemonstrate that our generalisation benefits from the same theoretical\nconvergence guarantees. We reveal the effectiveness of our Finsler embedding\nacross various types of non-symmetric data, highlighting its value in\napplications such as data visualisation, dimensionality reduction, directed\ngraph embedding, and link prediction.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17553", "pdf": "https://arxiv.org/pdf/2503.17553", "abs": "https://arxiv.org/abs/2503.17553", "authors": ["Humza Nusrat", "Bing Luo", "Ryan Hall", "Joshua Kim", "Hassan Bagher-Ebadian", "Anthony Doemer", "Benjamin Movsas", "Kundan Thind"], "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent", "categories": ["physics.med-ph", "cs.AI", "cs.CL", "cs.ET", "cs.HC"], "comment": "19 pages, 5 figures, preprint", "summary": "Radiotherapy treatment planning is a complex and time-intensive process,\noften impacted by inter-planner variability and subjective decision-making. To\naddress these challenges, we introduce Dose Optimization Language Agent (DOLA),\nan autonomous large language model (LLM)-based agent designed for optimizing\nradiotherapy treatment plans while rigorously protecting patient privacy. DOLA\nintegrates the LLaMa3.1 LLM directly with a commercial treatment planning\nsystem, utilizing chain-of-thought prompting, retrieval-augmented generation\n(RAG), and reinforcement learning (RL). Operating entirely within secure local\ninfrastructure, this agent eliminates external data sharing. We evaluated DOLA\nusing a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in\n20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and\noptimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.\nThe 70B model demonstrated significantly improved performance, achieving\napproximately 16.4% higher final scores than the 8B model. The RAG approach\noutperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated\nconvergence, highlighting the synergy of retrieval-based memory and\nreinforcement learning. Optimal temperature hyperparameter analysis identified\n0.4 as providing the best balance between exploration and exploitation. This\nproof of concept study represents the first successful deployment of locally\nhosted LLM agents for autonomous optimization of treatment plans within a\ncommercial radiotherapy planning system. By extending human-machine interaction\nthrough interpretable natural language reasoning, DOLA offers a scalable and\nprivacy-conscious framework, with significant potential for clinical\nimplementation and workflow improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18094", "pdf": "https://arxiv.org/pdf/2503.18094", "abs": "https://arxiv.org/abs/2503.18094", "authors": ["Fei Li", "Wenxuan Liu", "Jingjing Chen", "Ruixu Zhang", "Yuran Wang", "Xian Zhong", "Zheng Wang"], "title": "Anomize: Better Open Vocabulary Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18135", "pdf": "https://arxiv.org/pdf/2503.18135", "abs": "https://arxiv.org/abs/2503.18135", "authors": ["Jiaxin Huang", "Runnan Chen", "Ziwen Li", "Zhengqing Gao", "Xiao He", "Yandong Guo", "Mingming Gong", "Tongliang Liu"], "title": "MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning segmentation aims to segment target objects in complex scenes based\non human intent and spatial reasoning. While recent multimodal large language\nmodels (MLLMs) have demonstrated impressive 2D image reasoning segmentation,\nadapting these capabilities to 3D scenes remains underexplored. In this paper,\nwe introduce MLLM-For3D, a simple yet effective framework that transfers\nknowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize\nMLLMs to generate multi-view pseudo segmentation masks and corresponding text\nembeddings, then unproject 2D masks into 3D space and align them with the text\nembeddings. The primary challenge lies in the absence of 3D context and spatial\nconsistency across multiple views, causing the model to hallucinate objects\nthat do not exist and fail to target objects consistently. Training the 3D\nmodel with such irrelevant objects leads to performance degradation. To address\nthis, we introduce a spatial consistency strategy to enforce that segmentation\nmasks remain coherent in the 3D space, effectively capturing the geometry of\nthe scene. Moreover, we develop a Token-for-Query approach for multimodal\nsemantic alignment, enabling consistent identification of the same object\nacross different views. Extensive evaluations on various challenging indoor\nscene benchmarks demonstrate that, even without any labeled 3D training data,\nMLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively\ninterpreting user intent, understanding 3D scenes, and reasoning about spatial\nrelationships.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18147", "pdf": "https://arxiv.org/pdf/2503.18147", "abs": "https://arxiv.org/abs/2503.18147", "authors": ["Ke Niu", "Yuwen Chen", "Haiyang Yu", "Zhuofan Chen", "Xianghui Que", "Bin Li", "Xiangyang Xue"], "title": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive Hierarchical Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "annotation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18320", "pdf": "https://arxiv.org/pdf/2503.18320", "abs": "https://arxiv.org/abs/2503.18320", "authors": ["Dong Jing", "Nanyi Fei", "Zhiwu Lu"], "title": "Bridging Writing Manner Gap in Visual Instruction Tuning by Creating LLM-aligned Instructions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of Large Multi-modal Models (LMMs), the instruction quality\nduring the visual instruction tuning stage significantly influences the\nperformance of modality alignment. In this paper, we assess the instruction\nquality from a unique perspective termed \\textbf{Writing Manner}, which\nencompasses the selection of vocabulary, grammar and sentence structure to\nconvey specific semantics. We argue that there exists a substantial writing\nmanner gap between the visual instructions and the base Large Language Models\n(LLMs) within LMMs. This gap forces the pre-trained base LLMs to deviate from\ntheir original writing styles, leading to capability degradation of both base\nLLMs and LMMs. To bridge the writing manner gap while preserving the original\nsemantics, we propose directly leveraging the base LLM to align the writing\nmanner of soft-format visual instructions with that of the base LLM itself,\nresulting in novel LLM-aligned instructions. The manual writing manner\nevaluation results demonstrate that our approach successfully minimizes the\nwriting manner gap. By utilizing LLM-aligned instructions, the baseline models\nLLaVA-7B and QwenVL demonstrate enhanced resistance to hallucinations and\nnon-trivial comprehensive improvements across all $15$ visual and language\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18155", "pdf": "https://arxiv.org/pdf/2503.18155", "abs": "https://arxiv.org/abs/2503.18155", "authors": ["Kelly O. Marshall", "Omid Poursaeed", "Sergiu Oprea", "Amit Kumar", "Anushrut Jignasu", "Chinmay Hegde", "Yilei Li", "Rakesh Ranjan"], "title": "Decorum: A Language-Based Approach For Style-Conditioned Synthesis of Indoor 3D Scenes", "categories": ["cs.CV"], "comment": null, "summary": "3D indoor scene generation is an important problem for the design of digital\nand real-world environments. To automate this process, a scene generation model\nshould be able to not only generate plausible scene layouts, but also take into\nconsideration visual features and style preferences. Existing methods for this\ntask exhibit very limited control over these attributes, only allowing text\ninputs in the form of simple object-level descriptions or pairwise spatial\nrelationships. Our proposed method Decorum enables users to control the scene\ngeneration process with natural language by adopting language-based\nrepresentations at each stage. This enables us to harness recent advancements\nin Large Language Models (LLMs) to model language-to-language mappings. In\naddition, we show that using a text-based representation allows us to select\nfurniture for our scenes using a novel object retrieval method based on\nmultimodal LLMs. Evaluations on the benchmark 3D-FRONT dataset show that our\nmethods achieve improvements over existing work in text-conditioned scene\nsynthesis and object retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18211", "pdf": "https://arxiv.org/pdf/2503.18211", "abs": "https://arxiv.org/abs/2503.18211", "authors": ["Zhengyuan Li", "Kai Cheng", "Anindita Ghosh", "Uttaran Bhattacharya", "Liangyan Gui", "Aniket Bera"], "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction", "categories": ["cs.CV"], "comment": "Project URL: https://github.com/lzhyu/SimMotionEdit", "summary": "Text-based 3D human motion editing is a critical yet challenging task in\ncomputer vision and graphics. While training-free approaches have been\nexplored, the recent release of the MotionFix dataset, which includes\nsource-text-motion triplets, has opened new avenues for training, yielding\npromising results. However, existing methods struggle with precise control,\noften leading to misalignment between motion semantics and language\ninstructions. In this paper, we introduce a related task, motion similarity\nprediction, and propose a multi-task training paradigm, where we train the\nmodel jointly on motion editing and motion similarity prediction to foster the\nlearning of semantically meaningful representations. To complement this task,\nwe design an advanced Diffusion-Transformer-based architecture that separately\nhandles motion similarity prediction and motion editing. Extensive experiments\ndemonstrate the state-of-the-art performance of our approach in both editing\nalignment and fidelity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18227", "pdf": "https://arxiv.org/pdf/2503.18227", "abs": "https://arxiv.org/abs/2503.18227", "authors": ["Yiheng Zhong", "Zihong Luo", "Chengzhi Liu", "Feilong Tang", "Zelin Peng", "Ming Hu", "Yingzhen Hu", "Jionglong Su", "Zongyuan Geand", "Imran Razzak"], "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our anonymous code is released at\nhttps://github.com/logan-0623/PG-SAM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18254", "pdf": "https://arxiv.org/pdf/2503.18254", "abs": "https://arxiv.org/abs/2503.18254", "authors": ["Lukas Uzolas", "Elmar Eisemann", "Petr Kellnhofer"], "title": "Surface-Aware Distilled 3D Semantic Features", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Many 3D tasks such as pose alignment, animation, motion transfer, and 3D\nreconstruction rely on establishing correspondences between 3D shapes. This\nchallenge has recently been approached by matching of semantic features from\npre-trained vision models. However, despite their power, these features\nstruggle to differentiate instances of the same semantic class such as \"left\nhand\" versus \"right hand\" which leads to substantial mapping errors. To solve\nthis, we learn a surface-aware embedding space that is robust to these\nambiguities. Importantly, our approach is self-supervised and requires only a\nsmall number of unpaired training meshes to infer features for new 3D shapes at\ntest time. We achieve this by introducing a contrastive loss that preserves the\nsemantic content of the features distilled from foundational models while\ndisambiguating features located far apart on the shape's surface. We observe\nsuperior performance in correspondence matching benchmarks and enable\ndownstream applications including in-part segmentation, pose alignment, and\nmotion transfer. The project site is available at\nhttps://lukas.uzolas.com/SurfaceAware3DFeaturesSite.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18892", "pdf": "https://arxiv.org/pdf/2503.18892", "abs": "https://arxiv.org/abs/2503.18892", "authors": ["Weihao Zeng", "Yuzhen Huang", "Qian Liu", "Wei Liu", "Keqing He", "Zejun Ma", "Junxian He"], "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can\nnaturally emerge through a simple reinforcement learning (RL) framework with\nrule-based rewards, where the training may directly start from the base\nmodels-a paradigm referred to as zero RL training. Most recent efforts to\nreproduce zero RL training have primarily focused on the Qwen2.5 model series,\nwhich may not be representative as we find the base models already exhibit\nstrong instruction-following and self-reflection abilities. In this work, we\ninvestigate zero RL training across 10 diverse base models, spanning different\nfamilies and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B,\nQwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several\nkey design strategies-such as adjusting format reward and controlling query\ndifficulty-we achieve substantial improvements in both reasoning accuracy and\nresponse length across most settings. However, by carefully monitoring the\ntraining dynamics, we observe that different base models exhibit distinct\npatterns during training. For instance, the increased response length does not\nalways correlate with the emergence of certain cognitive behaviors such as\nverification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for\nthe first time in small models not from the Qwen family. We share the key\ndesigns that enable successful zero RL training, along with our findings and\npractices. To facilitate further research, we open-source the code, models, and\nanalysis tools.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18941", "pdf": "https://arxiv.org/pdf/2503.18941", "abs": "https://arxiv.org/abs/2503.18941", "authors": ["Hongru Cai", "Yongqi Li", "Ruifeng Yuan", "Wenjie Wang", "Zhen Zhang", "Wenjie Li", "Tat-Seng Chua"], "title": "Exploring Training and Inference Scaling Laws in Generative Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Generative retrieval has emerged as a novel paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers.\nAlthough promising, the mechanisms that underpin its performance and\nscalability remain largely unclear. We conduct a systematic investigation of\ntraining and inference scaling laws in generative retrieval, exploring how\nmodel size, training data scale, and inference-time compute jointly influence\nretrieval performance. To address the lack of suitable metrics, we propose a\nnovel evaluation measure inspired by contrastive entropy and generation loss,\nproviding a continuous performance signal that enables robust comparisons\nacross diverse generative retrieval methods. Our experiments show that\nn-gram-based methods demonstrate strong alignment with both training and\ninference scaling laws, especially when paired with larger LLMs. Furthermore,\nincreasing inference computation yields substantial performance gains,\nrevealing that generative retrieval can significantly benefit from higher\ncompute budgets at inference. Across these settings, LLaMA models consistently\noutperform T5 models, suggesting a particular advantage for larger decoder-only\nmodels in generative retrieval. Taken together, our findings underscore that\nmodel sizes, data availability, and inference computation interact to unlock\nthe full potential of generative retrieval, offering new insights for designing\nand optimizing future systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18349", "pdf": "https://arxiv.org/pdf/2503.18349", "abs": "https://arxiv.org/abs/2503.18349", "authors": ["Zekai Deng", "Ye Shi", "Kaiyang Ji", "Lan Xu", "Shaoli Huang", "Jingya Wang"], "title": "Human-Object Interaction with Vision-Language Model Guided Relative Movement Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Human-Object Interaction (HOI) is vital for advancing simulation, animation,\nand robotics, enabling the generation of long-term, physically plausible\nmotions in 3D environments. However, existing methods often fall short of\nachieving physics realism and supporting diverse types of interactions. To\naddress these challenges, this paper introduces a unified Human-Object\nInteraction framework that provides unified control over interactions with\nstatic scenes and dynamic objects using language commands. The interactions\nbetween human and object parts can always be described as the continuous stable\nRelative Movement Dynamics (RMD) between human and object parts. By leveraging\nthe world knowledge and scene perception capabilities of Vision-Language Models\n(VLMs), we translate language commands into RMD diagrams, which are used to\nguide goal-conditioned reinforcement learning for sequential interaction with\nobjects. Our framework supports long-horizon interactions among dynamic,\narticulated, and static objects. To support the training and evaluation of our\nframework, we present a new dataset named Interplay, which includes multi-round\ntask plans generated by VLMs, covering both static and dynamic HOI tasks.\nExtensive experiments demonstrate that our proposed framework can effectively\nhandle a wide range of HOI tasks, showcasing its ability to maintain long-term,\nmulti-round transitions. For more details, please refer to our project webpage:\nhttps://rmd-hoi.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18363", "pdf": "https://arxiv.org/pdf/2503.18363", "abs": "https://arxiv.org/abs/2503.18363", "authors": ["Wenyuan Zhang", "Yixiao Yang", "Han Huang", "Liang Han", "Kanle Shi", "Yu-Shen Liu"], "title": "MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project page:\n  https://wen-yuan-zhang.github.io/MonoInstance/", "summary": "Monocular depth priors have been widely adopted by neural rendering in\nmulti-view based tasks such as 3D reconstruction and novel view synthesis.\nHowever, due to the inconsistent prediction on each view, how to more\neffectively leverage monocular cues in a multi-view context remains a\nchallenge. Current methods treat the entire estimated depth map\nindiscriminately, and use it as ground truth supervision, while ignoring the\ninherent inaccuracy and cross-view inconsistency in monocular priors. To\nresolve these issues, we propose MonoInstance, a general approach that explores\nthe uncertainty of monocular depths to provide enhanced geometric priors for\nneural rendering and reconstruction. Our key insight lies in aligning each\nsegmented instance depths from multiple views within a common 3D space, thereby\ncasting the uncertainty estimation of monocular depths into a density measure\nwithin noisy point clouds. For high-uncertainty areas where depth priors are\nunreliable, we further introduce a constraint term that encourages the\nprojected instances to align with corresponding instance masks on nearby views.\nMonoInstance is a versatile strategy which can be seamlessly integrated into\nvarious multi-view neural rendering frameworks. Our experimental results\ndemonstrate that MonoInstance significantly improves the performance in both\nreconstruction and novel view synthesis under various benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18393", "pdf": "https://arxiv.org/pdf/2503.18393", "abs": "https://arxiv.org/abs/2503.18393", "authors": ["Xinhua Xu", "Hong Liu", "Jianbing Wu", "Jinfu Liu"], "title": "PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes", "categories": ["cs.CV"], "comment": null, "summary": "The integration of RGB and depth modalities significantly enhances the\naccuracy of segmenting complex indoor scenes, with depth data from RGB-D\ncameras playing a crucial role in this improvement. However, collecting an\nRGB-D dataset is more expensive than an RGB dataset due to the need for\nspecialized depth sensors. Aligning depth and RGB images also poses challenges\ndue to sensor positioning and issues like missing data and noise. In contrast,\nPseudo Depth (PD) from high-precision depth estimation algorithms can eliminate\nthe dependence on RGB-D sensors and alignment processes, as well as provide\neffective depth information and show significant potential in semantic\nsegmentation. Therefore, to explore the practicality of utilizing pseudo depth\ninstead of real depth for semantic segmentation, we design an RGB-PD\nsegmentation pipeline to integrate RGB and pseudo depth and propose a Pseudo\nDepth Aggregation Module (PDAM) for fully exploiting the informative clues\nprovided by the diverse pseudo depth maps. The PDAM aggregates multiple pseudo\ndepth maps into a single modality, making it easily adaptable to other RGB-D\nsegmentation methods. In addition, the pre-trained diffusion model serves as a\nstrong feature extractor for RGB segmentation tasks, but multi-modal\ndiffusion-based segmentation methods remain unexplored. Therefore, we present a\nPseudo Depth Diffusion Model (PDDM) that adopts a large-scale text-image\ndiffusion model as a feature extractor and a simple yet effective fusion\nstrategy to integrate pseudo depth. To verify the applicability of pseudo depth\nand our PDDM, we perform extensive experiments on the NYUv2 and SUNRGB-D\ndatasets. The experimental results demonstrate that pseudo depth can\neffectively enhance segmentation performance, and our PDDM achieves\nstate-of-the-art performance, outperforming other methods by +6.98 mIoU on\nNYUv2 and +2.11 mIoU on SUNRGB-D.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18406", "pdf": "https://arxiv.org/pdf/2503.18406", "abs": "https://arxiv.org/abs/2503.18406", "authors": ["Sherry X. Chen", "Misha Sra", "Pradeep Sen"], "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning", "categories": ["cs.CV"], "comment": "Computer Vision and Pattern Recognition 2025", "summary": "Although natural language instructions offer an intuitive way to guide\nautomated image editing, deep-learning models often struggle to achieve\nhigh-quality results, largely due to challenges in creating large, high-quality\ntraining datasets. Previous work has typically relied on text-toimage (T2I)\ngenerative models to produce pairs of original and edited images that simulate\nthe input/output of an instruction-guided image-editing model. However, these\nimage pairs often fail to align with the specified edit instructions due to the\nlimitations of T2I models, which negatively impacts models trained on such\ndatasets. To address this, we present Instruct-CLIP, a self-supervised method\nthat learns the semantic changes between original and edited images to refine\nand better align the instructions in existing datasets. Furthermore, we adapt\nInstruct-CLIP to handle noisy latent images and diffusion timesteps so that it\ncan be used to train latent diffusion models (LDMs) [19] and efficiently\nenforce alignment between the edit instruction and the image changes in latent\nspace at any step of the diffusion pipeline. We use Instruct-CLIP to correct\nthe InstructPix2Pix dataset and get over 120K refined samples we then use to\nfine-tune their model, guided by our novel Instruct-CLIP-based loss function.\nThe resulting model can produce edits that are more aligned with the given\ninstructions. Our code and dataset are available at\nhttps://github.com/SherryXTChen/Instruct-CLIP.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18407", "pdf": "https://arxiv.org/pdf/2503.18407", "abs": "https://arxiv.org/abs/2503.18407", "authors": ["Wencheng Zhu", "Yuexin Wang", "Hongxuan Li", "Pengfei Zhu", "Danqing Song", "Qinghua Hu"], "title": "VTD-CLIP: Video-to-Text Discretization via Prompting CLIP", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models bridge visual and linguistic understanding and have\nproven to be powerful for video recognition tasks. Existing approaches\nprimarily rely on parameter-efficient fine-tuning of image-text pre-trained\nmodels, yet they often suffer from limited interpretability and poor\ngeneralization due to inadequate temporal modeling. To address these, we\npropose a simple yet effective video-to-text discretization framework. Our\nmethod repurposes the frozen text encoder to construct a visual codebook from\nvideo class labels due to the many-to-one contrastive alignment between visual\nand textual embeddings in multimodal pretraining. This codebook effectively\ntransforms temporal visual data into textual tokens via feature lookups and\noffers interpretable video representations through explicit video modeling.\nThen, to enhance robustness against irrelevant or noisy frames, we introduce a\nconfidence-aware fusion module that dynamically weights keyframes by assessing\ntheir semantic relevance via the codebook. Furthermore, our method incorporates\nlearnable text prompts to conduct adaptive codebook updates. Extensive\nexperiments on HMDB-51, UCF-101, SSv2, and Kinetics-400 have validated the\nsuperiority of our approach, achieving more competitive improvements over\nstate-of-the-art methods. The code will be publicly available at\nhttps://github.com/isxinxin/VTD-CLIP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18414", "pdf": "https://arxiv.org/pdf/2503.18414", "abs": "https://arxiv.org/abs/2503.18414", "authors": ["Yuchuan Tian", "Hanting Chen", "Mengyu Zheng", "Yuchen Liang", "Chao Xu", "Yunhe Wang"], "title": "U-REPA: Aligning Diffusion U-Nets to ViTs", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "Representation Alignment (REPA) that aligns Diffusion Transformer (DiT)\nhidden-states with ViT visual encoders has proven highly effective in DiT\ntraining, demonstrating superior convergence properties, but it has not been\nvalidated on the canonical diffusion U-Net architecture that shows faster\nconvergence compared to DiTs. However, adapting REPA to U-Net architectures\npresents unique challenges: (1) different block functionalities necessitate\nrevised alignment strategies; (2) spatial-dimension inconsistencies emerge from\nU-Net's spatial downsampling operations; (3) space gaps between U-Net and ViT\nhinder the effectiveness of tokenwise alignment. To encounter these challenges,\nwe propose U-REPA, a representation alignment paradigm that bridges U-Net\nhidden states and ViT features as follows: Firstly, we propose via observation\nthat due to skip connection, the middle stage of U-Net is the best alignment\noption. Secondly, we propose upsampling of U-Net features after passing them\nthrough MLPs. Thirdly, we observe difficulty when performing tokenwise\nsimilarity alignment, and further introduces a manifold loss that regularizes\nthe relative similarity between samples. Experiments indicate that the\nresulting U-REPA could achieve excellent generation quality and greatly\naccelerates the convergence speed. With CFG guidance interval, U-REPA could\nreach $FID<1.5$ in 200 epochs or 1M iterations on ImageNet 256 $\\times$ 256,\nand needs only half the total epochs to perform better than REPA. Codes are\navailable at https://github.com/YuchuanTian/U-REPA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18446", "pdf": "https://arxiv.org/pdf/2503.18446", "abs": "https://arxiv.org/abs/2503.18446", "authors": ["Jinho Jeong", "Sangmin Han", "Jinwoo Kim", "Seon Joo Kim"], "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "In this paper, we propose LSRNA, a novel framework for higher-resolution\n(exceeding 1K) image generation using diffusion models by leveraging\nsuper-resolution directly in the latent space. Existing diffusion models\nstruggle with scaling beyond their training resolutions, often leading to\nstructural distortions or content repetition. Reference-based methods address\nthe issues by upsampling a low-resolution reference to guide higher-resolution\ngeneration. However, they face significant challenges: upsampling in latent\nspace often causes manifold deviation, which degrades output quality. On the\nother hand, upsampling in RGB space tends to produce overly smoothed outputs.\nTo overcome these limitations, LSRNA combines Latent space Super-Resolution\n(LSR) for manifold alignment and Region-wise Noise Addition (RNA) to enhance\nhigh-frequency details. Our extensive experiments demonstrate that integrating\nLSRNA outperforms state-of-the-art reference-based methods across various\nresolutions and metrics, while showing the critical role of latent space\nupsampling in preserving detail and sharpness. The code is available at\nhttps://github.com/3587jjh/LSRNA.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18469", "pdf": "https://arxiv.org/pdf/2503.18469", "abs": "https://arxiv.org/abs/2503.18469", "authors": ["Hao Ni", "Lianli Gao", "Pengpeng Zeng", "Heng Tao Shen", "Jingkuan Song"], "title": "CFReID: Continual Few-shot Person Re-Identification", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Real-world surveillance systems are dynamically evolving, requiring a person\nRe-identification model to continuously handle newly incoming data from various\ndomains. To cope with these dynamics, Lifelong ReID (LReID) has been proposed\nto learn and accumulate knowledge across multiple domains incrementally.\nHowever, LReID models need to be trained on large-scale labeled data for each\nunseen domain, which are typically inaccessible due to privacy and cost\nconcerns. In this paper, we propose a new paradigm called Continual Few-shot\nReID (CFReID), which requires models to be incrementally trained using few-shot\ndata and tested on all seen domains. Under few-shot conditions, CFREID faces\ntwo core challenges: 1) learning knowledge from few-shot data of unseen domain,\nand 2) avoiding catastrophic forgetting of seen domains. To tackle these two\nchallenges, we propose a Stable Distribution Alignment (SDA) framework from\nfeature distribution perspective. Specifically, our SDA is composed of two\nmodules, i.e., Meta Distribution Alignment (MDA) and Prototype-based Few-shot\nAdaptation (PFA). To support the study of CFReID, we establish an evaluation\nbenchmark for CFReID on five publicly available ReID datasets. Extensive\nexperiments demonstrate that our SDA can enhance the few-shot learning and\nanti-forgetting capabilities under few-shot conditions. Notably, our approach,\nusing only 5\\% of the data, i.e., 32 IDs, significantly outperforms LReID's\nstate-of-the-art performance, which requires 700 to 1,000 IDs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18470", "pdf": "https://arxiv.org/pdf/2503.18470", "abs": "https://arxiv.org/abs/2503.18470", "authors": ["Zhenyu Pan", "Han Liu"], "title": "MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse", "categories": ["cs.CV", "cs.AI"], "comment": "Working Paper", "summary": "We present MetaSpatial, the first reinforcement learning (RL)-based framework\ndesigned to enhance 3D spatial reasoning in vision-language models (VLMs),\nenabling real-time 3D scene generation without the need for hard-coded\noptimizations. MetaSpatial addresses two core challenges: (i) the lack of\ninternalized 3D spatial reasoning in VLMs, which limits their ability to\ngenerate realistic layouts, and (ii) the inefficiency of traditional supervised\nfine-tuning (SFT) for layout generation tasks, as perfect ground truth\nannotations are unavailable. Our key innovation is a multi-turn RL-based\noptimization mechanism that integrates physics-aware constraints and rendered\nimage evaluations, ensuring generated 3D layouts are coherent, physically\nplausible, and aesthetically consistent. Methodologically, MetaSpatial\nintroduces an adaptive, iterative reasoning process, where the VLM refines\nspatial arrangements over multiple turns by analyzing rendered outputs,\nimproving scene coherence progressively. Empirical evaluations demonstrate that\nMetaSpatial significantly enhances the spatial consistency and formatting\nstability of various scale models. Post-training, object placements are more\nrealistic, aligned, and functionally coherent, validating the effectiveness of\nRL for 3D spatial reasoning in metaverse, AR/VR, digital twins, and game\ndevelopment applications. Our code, data, and training pipeline are publicly\navailable at https://github.com/PzySeere/MetaSpatial.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18507", "pdf": "https://arxiv.org/pdf/2503.18507", "abs": "https://arxiv.org/abs/2503.18507", "authors": ["Luca Zanella", "Massimiliano Mancini", "Willi Menapace", "Sergey Tulyakov", "Yiming Wang", "Elisa Ricci"], "title": "Can Text-to-Video Generation help Video-Language Alignment?", "categories": ["cs.CV"], "comment": "CVPR 2025. Project website at https://lucazanella.github.io/synvita/", "summary": "Recent video-language alignment models are trained on sets of videos, each\nwith an associated positive caption and a negative caption generated by large\nlanguage models. A problem with this procedure is that negative captions may\nintroduce linguistic biases, i.e., concepts are seen only as negatives and\nnever associated with a video. While a solution would be to collect videos for\nthe negative captions, existing databases lack the fine-grained variations\nneeded to cover all possible negatives. In this work, we study whether\nsynthetic videos can help to overcome this issue. Our preliminary analysis with\nmultiple generators shows that, while promising on some tasks, synthetic videos\nharm the performance of the model on others. We hypothesize this issue is\nlinked to noise (semantic and visual) in the generated videos and develop a\nmethod, SynViTA, that accounts for those. SynViTA dynamically weights the\ncontribution of each synthetic video based on how similar its target caption is\nw.r.t. the real counterpart. Moreover, a semantic consistency loss makes the\nmodel focus on fine-grained differences across captions, rather than\ndifferences in video appearance. Experiments show that, on average, SynViTA\nimproves over existing methods on VideoCon test sets and SSv2-Temporal,\nSSv2-Events, and ATP-Hard benchmarks, being a first promising step for using\nsynthetic videos when learning video-language models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18567", "pdf": "https://arxiv.org/pdf/2503.18567", "abs": "https://arxiv.org/abs/2503.18567", "authors": ["Biwen Meng", "Xi Long", "Wanrong Yang", "Ruochen Liu", "Yi Tian", "Yalin Zheng", "Jingxin Liu"], "title": "Advancing Cross-Organ Domain Generalization with Test-Time Style Transfer and Diversity Enhancement", "categories": ["cs.CV"], "comment": "2025 IEEE International Symposium on Biomedical Imaging (ISBI)", "summary": "Deep learning has made significant progress in addressing challenges in\nvarious fields including computational pathology (CPath). However, due to the\ncomplexity of the domain shift problem, the performance of existing models will\ndegrade, especially when it comes to multi-domain or cross-domain tasks. In\nthis paper, we propose a Test-time style transfer (T3s) that uses a\nbidirectional mapping mechanism to project the features of the source and\ntarget domains into a unified feature space, enhancing the generalization\nability of the model. To further increase the style expression space, we\nintroduce a Cross-domain style diversification module (CSDM) to ensure the\northogonality between style bases. In addition, data augmentation and low-rank\nadaptation techniques are used to improve feature alignment and sensitivity,\nenabling the model to adapt to multi-domain inputs effectively. Our method has\ndemonstrated effectiveness on three unseen datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18623", "pdf": "https://arxiv.org/pdf/2503.18623", "abs": "https://arxiv.org/abs/2503.18623", "authors": ["Deepayan Das", "Davide Talon", "Yiming Wang", "Massimiliano Mancini", "Elisa Ricci"], "title": "Training-Free Personalization via Retrieval and Reasoning on Fingerprints", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) have lead to major improvements in multimodal\nreasoning, yet they still struggle to understand user-specific concepts.\nExisting personalization methods address this limitation but heavily rely on\ntraining procedures, that can be either costly or unpleasant to individual\nusers. We depart from existing work, and for the first time explore the\ntraining-free setting in the context of personalization. We propose a novel\nmethod, Retrieval and Reasoning for Personalization (R2P), leveraging internal\nknowledge of VLMs. First, we leverage VLMs to extract the concept fingerprint,\ni.e., key attributes uniquely defining the concept within its semantic class.\nWhen a query arrives, the most similar fingerprints are retrieved and scored\nvia chain-of-thought-reasoning. To reduce the risk of hallucinations, the\nscores are validated through cross-modal verification at the attribute level:\nin case of a discrepancy between the scores, R2P refines the concept\nassociation via pairwise multimodal matching, where the retrieved fingerprints\nand their images are directly compared with the query. We validate R2P on two\npublicly available benchmarks and a newly introduced dataset, Personal Concepts\nwith Visual Ambiguity (PerVA), for concept identification highlighting\nchallenges in visual ambiguity. R2P consistently outperforms state-of-the-art\napproaches on various downstream tasks across all benchmarks. Code will be\navailable upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18665", "pdf": "https://arxiv.org/pdf/2503.18665", "abs": "https://arxiv.org/abs/2503.18665", "authors": ["Bingchen Miao", "Yang Wu", "Minghe Gao", "Qifan Yu", "Wendong Bu", "Wenqiao Zhang", "Yunfei Li", "Siliang Tang", "Tat-Seng Chua", "Juncheng Li"], "title": "Boosting Virtual Agent Learning and Reasoning: A Step-wise, Multi-dimensional, and Generalist Reward Model with Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The development of Generalist Virtual Agents (GVAs) powered by Multimodal\nLarge Language Models (MLLMs) has shown significant promise in autonomous task\nexecution. However, current training paradigms face critical limitations,\nincluding reliance on outcome supervision and labor-intensive human\nannotations. To address these challenges, we propose Similar, a Step-wise\nMulti-dimensional Generalist Reward Model, which offers fine-grained signals\nfor agent training and can choose better action for inference-time scaling.\nSpecifically, we begin by systematically defining five dimensions for\nevaluating agent actions. Building on this framework, we design an MCTS-P\nalgorithm to automatically collect and annotate step-wise, five-dimensional\nagent execution data. Using this data, we train Similar with the Triple-M\nstrategy. Furthermore, we introduce the first benchmark in the virtual agent\ndomain for step-wise, multi-dimensional reward model training and evaluation,\nnamed SRM. This benchmark consists of two components: SRMTrain, which serves as\nthe training set for Similar, and SRMEval, a manually selected test set for\nevaluating the reward model. Experimental results demonstrate that Similar,\nthrough its step-wise, multi-dimensional assessment and synergistic gain,\nprovides GVAs with effective intermediate signals during both training and\ninference-time scaling. The code is available at\nhttps://github.com/Galery23/Similar-v1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional", "fine-grained"], "score": 4}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18673", "pdf": "https://arxiv.org/pdf/2503.18673", "abs": "https://arxiv.org/abs/2503.18673", "authors": ["Taeyeop Lee", "Bowen Wen", "Minjun Kang", "Gyuree Kang", "In So Kweon", "Kuk-Jin Yoon"], "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025, Project Page: https://taeyeop.com/any6d", "summary": "We introduce Any6D, a model-free framework for 6D object pose estimation that\nrequires only a single RGB-D anchor image to estimate both the 6D pose and size\nof unknown objects in novel scenes. Unlike existing methods that rely on\ntextured 3D models or multiple viewpoints, Any6D leverages a joint object\nalignment process to enhance 2D-3D alignment and metric scale estimation for\nimproved pose accuracy. Our approach integrates a render-and-compare strategy\nto generate and refine pose hypotheses, enabling robust performance in\nscenarios with occlusions, non-overlapping views, diverse lighting conditions,\nand large cross-environment variations. We evaluate our method on five\nchallenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O,\ndemonstrating its effectiveness in significantly outperforming state-of-the-art\nmethods for novel object pose estimation. Project page:\nhttps://taeyeop.com/any6d", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18725", "pdf": "https://arxiv.org/pdf/2503.18725", "abs": "https://arxiv.org/abs/2503.18725", "authors": ["Zimin Xia", "Alexandre Alahi"], "title": "FG$^2$: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel fine-grained cross-view localization method that estimates\nthe 3 Degrees of Freedom pose of a ground-level image in an aerial image of the\nsurroundings by matching fine-grained features between the two images. The pose\nis estimated by aligning a point plane generated from the ground image with a\npoint plane sampled from the aerial image. To generate the ground points, we\nfirst map ground image features to a 3D point cloud. Our method then learns to\nselect features along the height dimension to pool the 3D points to a\nBird's-Eye-View (BEV) plane. This selection enables us to trace which feature\nin the ground image contributes to the BEV representation. Next, we sample a\nset of sparse matches from computed point correspondences between the two point\nplanes and compute their relative pose using Procrustes alignment. Compared to\nthe previous state-of-the-art, our method reduces the mean localization error\nby 28% on the VIGOR cross-area test set. Qualitative results show that our\nmethod learns semantically consistent matches across ground and aerial views\nthrough weakly supervised learning from the camera pose.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained", "dimension"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18746", "pdf": "https://arxiv.org/pdf/2503.18746", "abs": "https://arxiv.org/abs/2503.18746", "authors": ["Yifei Zhang", "Chang Liu", "Jin Wei", "Xiaomeng Yang", "Yu Zhou", "Can Ma", "Xiangyang Ji"], "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Text images are unique in their dual nature, encompassing both visual and\nlinguistic information. The visual component encompasses structural and\nappearance-based features, while the linguistic dimension incorporates\ncontextual and semantic elements. In scenarios with degraded visual quality,\nlinguistic patterns serve as crucial supplements for comprehension,\nhighlighting the necessity of integrating both aspects for robust scene text\nrecognition (STR). Contemporary STR approaches often use language models or\nsemantic reasoning modules to capture linguistic features, typically requiring\nlarge-scale annotated datasets. Self-supervised learning, which lacks\nannotations, presents challenges in disentangling linguistic features related\nto the global context. Typically, sequence contrastive learning emphasizes the\nalignment of local features, while masked image modeling (MIM) tends to exploit\nlocal structures to reconstruct visual patterns, resulting in limited\nlinguistic knowledge. In this paper, we propose a Linguistics-aware Masked\nImage Modeling (LMIM) approach, which channels the linguistic information into\nthe decoding process of MIM through a separate branch. Specifically, we design\na linguistics alignment module to extract vision-independent features as\nlinguistic guidance using inputs with different visual appearances. As features\nextend beyond mere visual structures, LMIM must consider the global context to\nachieve reconstruction. Extensive experiments on various benchmarks\nquantitatively demonstrate our state-of-the-art performance, and attention\nvisualizations qualitatively show the simultaneous capture of both visual and\nlinguistic information.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18817", "pdf": "https://arxiv.org/pdf/2503.18817", "abs": "https://arxiv.org/abs/2503.18817", "authors": ["Jeonghyeon Kim", "Sangheum Hwang"], "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Prior research on out-of-distribution detection (OoDD) has primarily focused\non single-modality models. Recently, with the advent of large-scale pretrained\nvision-language models such as CLIP, OoDD methods utilizing such multi-modal\nrepresentations through zero-shot and prompt learning strategies have emerged.\nHowever, these methods typically involve either freezing the pretrained weights\nor only partially tuning them, which can be suboptimal for downstream datasets.\nIn this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve\nnotable OoDD performance. Despite some recent works demonstrating the impact of\nfine-tuning methods for OoDD, there remains significant potential for\nperformance improvement. We investigate the limitation of na\\\"ive fine-tuning\nmethods, examining why they fail to fully leverage the pretrained knowledge.\nOur empirical analysis suggests that this issue could stem from the modality\ngap within in-distribution (ID) embeddings. To address this, we propose a\ntraining objective that enhances cross-modal alignment by regularizing the\ndistances between image and text embeddings of ID data. This adjustment helps\nin better utilizing pretrained textual information by aligning similar\nsemantics from different modalities (i.e., text and image) more closely in the\nhyperspherical representation space. We theoretically demonstrate that the\nproposed regularization corresponds to the maximum likelihood estimation of an\nenergy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark\ndatasets, we show that our method, combined with post-hoc OoDD approaches\nleveraging pretrained knowledge (e.g., NegLabel), significantly outperforms\nexisting methods, achieving state-of-the-art OoDD performance and leading ID\naccuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18830", "pdf": "https://arxiv.org/pdf/2503.18830", "abs": "https://arxiv.org/abs/2503.18830", "authors": ["Zhengxian Wu", "Chuanrui Zhang", "Hangrui Xu", "Peng Jiao", "Haoqian Wang"], "title": "DAGait: Generalized Skeleton-Guided Data Alignment for Gait Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition is emerging as a promising and innovative area within the\nfield of computer vision, widely applied to remote person identification.\nAlthough existing gait recognition methods have achieved substantial success in\ncontrolled laboratory datasets, their performance often declines significantly\nwhen transitioning to wild datasets.We argue that the performance gap can be\nprimarily attributed to the spatio-temporal distribution inconsistencies\npresent in wild datasets, where subjects appear at varying angles, positions,\nand distances across the frames. To achieve accurate gait recognition in the\nwild, we propose a skeleton-guided silhouette alignment strategy, which uses\nprior knowledge of the skeletons to perform affine transformations on the\ncorresponding silhouettes.To the best of our knowledge, this is the first study\nto explore the impact of data alignment on gait recognition. We conducted\nextensive experiments across multiple datasets and network architectures, and\nthe results demonstrate the significant advantages of our proposed alignment\nstrategy.Specifically, on the challenging Gait3D dataset, our method achieved\nan average performance improvement of 7.9% across all evaluated networks.\nFurthermore, our method achieves substantial improvements on cross-domain\ndatasets, with accuracy improvements of up to 24.0%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18880", "pdf": "https://arxiv.org/pdf/2503.18880", "abs": "https://arxiv.org/abs/2503.18880", "authors": ["Hyeonggon Ryu", "Seongyu Kim", "Joon Son Chung", "Arda Senocak"], "title": "Seeing Speech and Sound: Distinguishing and Locating Audios in Visual Scenes", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025", "summary": "We present a unified model capable of simultaneously grounding both spoken\nlanguage and non-speech sounds within a visual scene, addressing key\nlimitations in current audio-visual grounding models. Existing approaches are\ntypically limited to handling either speech or non-speech sounds independently,\nor at best, together but sequentially without mixing. This limitation prevents\nthem from capturing the complexity of real-world audio sources that are often\nmixed. Our approach introduces a 'mix-and-separate' framework with audio-visual\nalignment objectives that jointly learn correspondence and disentanglement\nusing mixed audio. Through these objectives, our model learns to produce\ndistinct embeddings for each audio type, enabling effective disentanglement and\ngrounding across mixed audio sources. Additionally, we created a new dataset to\nevaluate simultaneous grounding of mixed audio sources, demonstrating that our\nmodel outperforms prior methods. Our approach also achieves comparable or\nbetter performance in standard segmentation and cross-modal retrieval tasks,\nhighlighting the benefits of our mix-and-separate approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18931", "pdf": "https://arxiv.org/pdf/2503.18931", "abs": "https://arxiv.org/abs/2503.18931", "authors": ["Yitong Chen", "Lingchen Meng", "Wujian Peng", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "CoMP: Continual Multimodal Pre-training for Vision Foundation Models", "categories": ["cs.CV"], "comment": "Code is available in https://github.com/SliMM-X/CoMP-MM", "summary": "Pre-trained Vision Foundation Models (VFMs) provide strong visual\nrepresentations for a wide range of applications. In this paper, we continually\npre-train prevailing VFMs in a multimodal manner such that they can\neffortlessly process visual inputs of varying sizes and produce visual\nrepresentations that are more aligned with language representations, regardless\nof their original pre-training process. To this end, we introduce CoMP, a\ncarefully designed multimodal pre-training pipeline. CoMP uses a Continual\nRotary Position Embedding to support native resolution continual pre-training,\nand an Alignment Loss between visual and textual features through language\nprototypes to align multimodal representations. By three-stage training, our\nVFMs achieve remarkable improvements not only in multimodal understanding but\nalso in other downstream tasks such as classification and segmentation.\nRemarkably, CoMP-SigLIP achieves scores of 66.7 on ChartQA and 75.9 on DocVQA\nwith a 0.5B LLM, while maintaining an 87.4% accuracy on ImageNet-1K and a 49.5\nmIoU on ADE20K under frozen chunk evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17482", "pdf": "https://arxiv.org/pdf/2503.17482", "abs": "https://arxiv.org/abs/2503.17482", "authors": ["Keyon Vafa", "Sarah Bentley", "Jon Kleinberg", "Sendhil Mullainathan"], "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "How should we evaluate the quality of generative models? Many existing\nmetrics focus on a model's producibility, i.e. the quality and breadth of\noutputs it can generate. However, the actual value from using a generative\nmodel stems not just from what it can produce but whether a user with a\nspecific goal can produce an output that satisfies that goal. We refer to this\nproperty as steerability. In this paper, we first introduce a mathematical\nframework for evaluating steerability independently from producibility.\nSteerability is more challenging to evaluate than producibility because it\nrequires knowing a user's goals. We address this issue by creating a benchmark\ntask that relies on one key idea: sample an output from a generative model and\nask users to reproduce it. We implement this benchmark in a large-scale user\nstudy of text-to-image models and large language models. Despite the ability of\nthese models to produce high-quality outputs, they all perform poorly on\nsteerabilty. This suggests that we need to focus on improving the steerability\nof generative models. We show such improvements are indeed possible: through\nreinforcement learning techniques, we create an alternative steering mechanism\nfor image models that achieves more than 2x improvement on this benchmark.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17551", "pdf": "https://arxiv.org/pdf/2503.17551", "abs": "https://arxiv.org/abs/2503.17551", "authors": ["Yu Sun", "Yin Li", "Ruixiao Sun", "Chunhui Liu", "Fangming Zhou", "Ze Jin", "Linjie Wang", "Xiang Shen", "Zhuolin Hao", "Hongyu Xiong"], "title": "Audio-Enhanced Vision-Language Modeling with Latent Space Broadening for High Quality Data Expansion", "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Transformer-based multimodal models are widely used in industrial-scale\nrecommendation, search, and advertising systems for content understanding and\nrelevance ranking. Enhancing labeled training data quality and cross-modal\nfusion significantly improves model performance, influencing key metrics such\nas quality view rates and ad revenue. High-quality annotations are crucial for\nadvancing content modeling, yet traditional statistical-based active learning\n(AL) methods face limitations: they struggle to detect overconfident\nmisclassifications and are less effective in distinguishing semantically\nsimilar items in deep neural networks. Additionally, audio information plays an\nincreasing role, especially in short-video platforms, yet most pre-trained\nmultimodal architectures primarily focus on text and images. While training\nfrom scratch across all three modalities is possible, it sacrifices the\nbenefits of leveraging existing pre-trained visual-language (VL) and audio\nmodels. To address these challenges, we propose kNN-based Latent Space\nBroadening (LSB) to enhance AL efficiency and Vision-Language Modeling with\nAudio Enhancement (VLMAE), a mid-fusion approach integrating audio into VL\nmodels. This system deployed in production systems, leading to significant\nbusiness gains.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17804", "pdf": "https://arxiv.org/pdf/2503.17804", "abs": "https://arxiv.org/abs/2503.17804", "authors": ["Xing Xie", "Jiawei Liu", "Huijie Fan", "Zhi Han", "Yandong Tang", "Liangqiong Qu"], "title": "DVG-Diffusion: Dual-View Guided Diffusion Model for CT Reconstruction from X-Rays", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Directly reconstructing 3D CT volume from few-view 2D X-rays using an\nend-to-end deep learning network is a challenging task, as X-ray images are\nmerely projection views of the 3D CT volume. In this work, we facilitate\ncomplex 2D X-ray image to 3D CT mapping by incorporating new view synthesis,\nand reduce the learning difficulty through view-guided feature alignment.\nSpecifically, we propose a dual-view guided diffusion model (DVG-Diffusion),\nwhich couples a real input X-ray view and a synthesized new X-ray view to\njointly guide CT reconstruction. First, a novel view parameter-guided encoder\ncaptures features from X-rays that are spatially aligned with CT. Next, we\nconcatenate the extracted dual-view features as conditions for the latent\ndiffusion model to learn and refine the CT latent representation. Finally, the\nCT latent representation is decoded into a CT volume in pixel space. By\nincorporating view parameter guided encoding and dual-view guided CT\nreconstruction, our DVG-Diffusion can achieve an effective balance between high\nfidelity and perceptual quality for CT reconstruction. Experimental results\ndemonstrate our method outperforms state-of-the-art methods. Based on\nexperiments, the comprehensive analysis and discussions for views and\nreconstruction are also presented.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.17914", "pdf": "https://arxiv.org/pdf/2503.17914", "abs": "https://arxiv.org/abs/2503.17914", "authors": ["Jianjian Yin", "Tao Chen", "Gensheng Pei", "Yazhou Yao", "Liqiang Nie", "Xiansheng Hua"], "title": "Semi-supervised Semantic Segmentation with Multi-Constraint Consistency Learning", "categories": ["cs.MM", "cs.CV"], "comment": "accepted by IEEE Transactions on Multimedia", "summary": "Consistency regularization has prevailed in semi-supervised semantic\nsegmentation and achieved promising performance. However, existing methods\ntypically concentrate on enhancing the Image-augmentation based Prediction\nconsistency and optimizing the segmentation network as a whole, resulting in\ninsufficient utilization of potential supervisory information. In this paper,\nwe propose a Multi-Constraint Consistency Learning (MCCL) approach to\nfacilitate the staged enhancement of the encoder and decoder. Specifically, we\nfirst design a feature knowledge alignment (FKA) strategy to promote the\nfeature consistency learning of the encoder from image-augmentation. Our FKA\nencourages the encoder to derive consistent features for strongly and weakly\naugmented views from the perspectives of point-to-point alignment and\nprototype-based intra-class compactness. Moreover, we propose a self-adaptive\nintervention (SAI) module to increase the discrepancy of aligned intermediate\nfeature representations, promoting Feature-perturbation based Prediction\nconsistency learning. Self-adaptive feature masking and noise injection are\ndesigned in an instance-specific manner to perturb the features for robust\nlearning of the decoder. Experimental results on Pascal VOC2012 and Cityscapes\ndatasets demonstrate that our proposed MCCL achieves new state-of-the-art\nperformance. The source code and models are made available at\nhttps://github.com/NUST-Machine-Intelligence-Laboratory/MCCL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
{"id": "2503.18162", "pdf": "https://arxiv.org/pdf/2503.18162", "abs": "https://arxiv.org/abs/2503.18162", "authors": ["Hui Xue", "Sarah M. Hooper", "Iain Pierce", "Rhodri H. Davies", "John Stairs", "Joseph Naegele", "Adrienne E. Campbell-Washburn", "Charlotte Manisty", "James C. Moon", "Thomas A. Treibel", "Peter Kellman", "Michael S. Hansen"], "title": "SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation", "categories": ["physics.med-ph", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "To develop and evaluate a new deep learning MR denoising method that\nleverages quantitative noise distribution information from the reconstruction\nprocess to improve denoising performance and generalization.\n  This retrospective study trained 14 different transformer and convolutional\nmodels with two backbone architectures on a large dataset of 2,885,236 images\nfrom 96,605 cardiac retro-gated cine complex series acquired at 3T. The\nproposed training scheme, termed SNRAware, leverages knowledge of the MRI\nreconstruction process to improve denoising performance by simulating large,\nhigh quality, and diverse synthetic datasets, and providing quantitative\ninformation about the noise distribution to the model. In-distribution testing\nwas performed on a hold-out dataset of 3000 samples with performance measured\nusing PSNR and SSIM, with ablation comparison without the noise augmentation.\nOut-of-distribution tests were conducted on cardiac real-time cine, first-pass\ncardiac perfusion, and neuro and spine MRI, all acquired at 1.5T, to test model\ngeneralization across imaging sequences, dynamically changing contrast,\ndifferent anatomies, and field strengths. The best model found in the\nin-distribution test generalized well to out-of-distribution samples,\ndelivering 6.5x and 2.9x CNR improvement for real-time cine and perfusion\nimaging, respectively. Further, a model trained with 100% cardiac cine data\ngeneralized well to a T1 MPRAGE neuro 3D scan and T2 TSE spine MRI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-25.jsonl"}
