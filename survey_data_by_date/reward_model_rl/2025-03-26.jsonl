{"id": "2503.19523", "pdf": "https://arxiv.org/pdf/2503.19523", "abs": "https://arxiv.org/abs/2503.19523", "authors": ["Xin Cai"], "title": "One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In this article, we primarily examine a variety of RL-based and RL-free\nmethods designed to address Reinforcement Learning from Human Feedback (RLHF)\nand Large Reasoning Models (LRMs). We begin with a concise overview of the\ntypical steps involved in RLHF and LRMs. Next, we reinterpret several RL-based\nand RL-free algorithms through the perspective of neural structured bandit\nprediction, providing a clear conceptual framework that uncovers a deeper\nconnection between these seemingly distinct approaches. Following this, we\nbriefly review some core principles of reinforcement learning, drawing\nattention to an often-overlooked aspect in existing RLHF studies. This leads to\na detailed derivation of the standard RLHF objective within a full RL context,\ndemonstrating its equivalence to neural structured bandit prediction. Finally,\nby reinvestigating the principles behind Proximal Policy Optimization (PPO), we\npinpoint areas needing adjustment, which culminates in the introduction of the\nGeneralized Reinforce Optimization (GRO) framework, seamlessly integrating\nRL-based and RL-free methods in RLHF. We look forward to the community's\nefforts to empirically validate GRO and invite constructive feedback.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "PPO", "proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 7}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991", "abs": "https://arxiv.org/abs/2503.18991", "authors": ["Ruoxi Cheng", "Shuirong Cao"], "title": "SRMIR: Shadow Reward Models Based on Introspective Reasoning for LLM Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences and values is\nvital for application. However, current alignment methods face three main\nlimitations: (1) reliance on costly human annotation; (2) alignment tax; (3)\nshallow alignment vulnerable to jailbreak attacks. Additionally, current\nalignment datasets often suffer from uneven distributions, leading to\noverrepresentation of some topics and neglect of others. To address these\nissues, we propose SRMIR (Shadow Reward Models Based on Introspective\nReasoning), inspired by shadow models in membership inference attacks. We first\nconstruct a balanced safety Chain of Draft (CoD) dataset across $7$ harmful\ntypes with structured prompt leveraging the introspective reasoning\ncapabilities of LLMs, then train a set of specialized reward models to guide\npolicy optimization through Group Relative Policy Optimization (GRPO). We apply\ntwo strategies, linear combination and categorized approach, to integrate\nshadow reward models for policy optimization. By comparison, we find that the\nlatter achieves superior alignment despite higher computational costs.\nExperiments across several LLMs demonstrate SRMIR significantly outperforms\nexisting methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "comparison", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "safety"], "score": 3}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19598", "pdf": "https://arxiv.org/pdf/2503.19598", "abs": "https://arxiv.org/abs/2503.19598", "authors": ["Giovanni Franco Gabriel Marraffini", "Andrés Cotton", "Noe Fabian Hsueh", "Axel Fridman", "Juan Wisznia", "Luciano Del Corro"], "title": "The Greatest Good Benchmark: Measuring LLMs' Alignment with Utilitarian Moral Dilemmas", "categories": ["cs.CL"], "comment": null, "summary": "The question of how to make decisions that maximise the well-being of all\npersons is very relevant to design language models that are beneficial to\nhumanity and free from harm. We introduce the Greatest Good Benchmark to\nevaluate the moral judgments of LLMs using utilitarian dilemmas. Our analysis\nacross 15 diverse LLMs reveals consistently encoded moral preferences that\ndiverge from established moral theories and lay population moral standards.\nMost LLMs have a marked preference for impartial beneficence and rejection of\ninstrumental harm. These findings showcase the 'artificial moral compass' of\nLLMs, offering insights into their moral alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19711", "pdf": "https://arxiv.org/pdf/2503.19711", "abs": "https://arxiv.org/abs/2503.19711", "authors": ["Sian Gooding", "Lucia Lopez-Rivilla", "Edward Grefenstette"], "title": "Writing as a testbed for open ended agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Open-ended tasks are particularly challenging for LLMs due to the vast\nsolution space, demanding both expansive exploration and adaptable strategies,\nespecially when success lacks a clear, objective definition. Writing, with its\nvast solution space and subjective evaluation criteria, provides a compelling\ntestbed for studying such problems. In this paper, we investigate the potential\nof LLMs to act as collaborative co-writers, capable of suggesting and\nimplementing text improvements autonomously. We analyse three prominent LLMs -\nGemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action\ndiversity, human alignment, and iterative improvement capabilities impact\noverall performance. This work establishes a framework for benchmarking\nautonomous writing agents and, more broadly, highlights fundamental challenges\nand potential solutions for building systems capable of excelling in diverse\nopen-ended domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "testbed", "criteria"], "score": 3}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19622", "pdf": "https://arxiv.org/pdf/2503.19622", "abs": "https://arxiv.org/abs/2503.19622", "authors": ["Hongcheng Gao", "Jiashu Qu", "Jingyi Tang", "Baolong Bi", "Yue Liu", "Hongyu Chen", "Li Liang", "Li Su", "Qingming Huang"], "title": "Exploring Hallucination of Large Multimodal Models in Video Understanding: Benchmark, Analysis and Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "The hallucination of large multimodal models (LMMs), providing responses that\nappear correct but are actually incorrect, limits their reliability and\napplicability. This paper aims to study the hallucination problem of LMMs in\nvideo modality, which is dynamic and more challenging compared to static\nmodalities like images and text. From this motivation, we first present a\ncomprehensive benchmark termed HAVEN for evaluating hallucinations of LMMs in\nvideo understanding tasks. It is built upon three dimensions, i.e.,\nhallucination causes, hallucination aspects, and question formats, resulting in\n6K questions. Then, we quantitatively study 7 influential factors on\nhallucinations, e.g., duration time of videos, model sizes, and model\nreasoning, via experiments of 16 LMMs on the presented benchmark. In addition,\ninspired by recent thinking models like OpenAI o1, we propose a video-thinking\nmodel to mitigate the hallucinations of LMMs via supervised reasoning\nfine-tuning (SRFT) and direct preference optimization (TDPO)-- where SRFT\nenhances reasoning capabilities while TDPO reduces hallucinations in the\nthinking process. Extensive experiments and analyses demonstrate the\neffectiveness. Remarkably, it improves the baseline by 7.65% in accuracy on\nhallucination evaluation and reduces the bias score by 4.5%. The code and data\nare public at https://github.com/Hongcheng-Gao/HAVEN.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability", "accuracy"], "score": 4}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19041", "pdf": "https://arxiv.org/pdf/2503.19041", "abs": "https://arxiv.org/abs/2503.19041", "authors": ["Kangwei Liu", "Mengru Wang", "Yujie Luo", "Lin Yuan", "Mengshu Sun", "Ningyu Zhang", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Huajun Chen"], "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19123", "pdf": "https://arxiv.org/pdf/2503.19123", "abs": "https://arxiv.org/abs/2503.19123", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Yeyun Gong"], "title": "Overcoming Vocabulary Mismatch: Vocabulary-agnostic Teacher Guided Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Using large teacher models to guide the training of smaller student models\nhas become the prevailing paradigm for efficient and effective learning.\nHowever, vocabulary mismatches between teacher and student language models pose\nsignificant challenges in language modeling, resulting in divergent token\nsequences and output distributions. To overcome these limitations, we propose\nVocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel\napproach that bridges the gap caused by vocabulary mismatch through two key\nmethods: (1) Token-level Lexical Alignment, which aligns token sequences across\nmismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss\nof teacher model to guide effective student training. We demonstrate its\neffectiveness in language modeling with 1B student model using various 7B\nteacher models with different vocabularies. Notably, with\nQwen2.5-Math-Instruct, a teacher model sharing only about 6% of its vocabulary\nwith TinyLlama, VocAgnoLM achieves a 46% performance improvement compared to\nnaive continual pretraining. Furthermore, we demonstrate that VocAgnoLM\nconsistently benefits from stronger teacher models, providing a robust solution\nto vocabulary mismatches in language modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19186", "pdf": "https://arxiv.org/pdf/2503.19186", "abs": "https://arxiv.org/abs/2503.19186", "authors": ["Parisa Mollaei", "Amir Barati Farimani"], "title": "Protein Structure-Function Relationship: A Kernel-PCA Approach for Reaction Coordinate Identification", "categories": ["cs.CL", "q-bio.QM"], "comment": "28 pages, 10 figures", "summary": "In this study, we propose a Kernel-PCA model designed to capture\nstructure-function relationships in a protein. This model also enables ranking\nof reaction coordinates according to their impact on protein properties. By\nleveraging machine learning techniques, including Kernel and principal\ncomponent analysis (PCA), our model uncovers meaningful patterns in\nhigh-dimensional protein data obtained from molecular dynamics (MD)\nsimulations. The effectiveness of our model in accurately identifying reaction\ncoordinates has been demonstrated through its application to a G\nprotein-coupled receptor. Furthermore, this model utilizes a network-based\napproach to uncover correlations in the dynamic behavior of residues associated\nwith a specific protein property. These findings underscore the potential of\nour model as a powerful tool for protein structure-function analysis and\nvisualization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19011", "pdf": "https://arxiv.org/pdf/2503.19011", "abs": "https://arxiv.org/abs/2503.19011", "authors": ["Yifei Feng", "Mingxin Yang", "Shuhui Yang", "Sheng Zhang", "Jiaao Yu", "Zibo Zhao", "Yuhong Liu", "Jie Jiang", "Chunchao Guo"], "title": "RomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Painting textures for existing geometries is a critical yet labor-intensive\nprocess in 3D asset generation. Recent advancements in text-to-image (T2I)\nmodels have led to significant progress in texture generation. Most existing\nresearch approaches this task by first generating images in 2D spaces using\nimage diffusion models, followed by a texture baking process to achieve UV\ntexture. However, these methods often struggle to produce high-quality textures\ndue to inconsistencies among the generated multi-view images, resulting in\nseams and ghosting artifacts. In contrast, 3D-based texture synthesis methods\naim to address these inconsistencies, but they often neglect 2D diffusion model\npriors, making them challenging to apply to real-world objects To overcome\nthese limitations, we propose RomanTex, a multiview-based texture generation\nframework that integrates a multi-attention network with an underlying 3D\nrepresentation, facilitated by our novel 3D-aware Rotary Positional Embedding.\nAdditionally, we incorporate a decoupling characteristic in the multi-attention\nblock to enhance the model's robustness in image-to-texture task, enabling\nsemantically-correct back-view synthesis. Furthermore, we introduce a\ngeometry-related Classifier-Free Guidance (CFG) mechanism to further improve\nthe alignment with both geometries and images. Quantitative and qualitative\nevaluations, along with comprehensive user studies, demonstrate that our method\nachieves state-of-the-art results in texture quality and consistency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19586", "pdf": "https://arxiv.org/pdf/2503.19586", "abs": "https://arxiv.org/abs/2503.19586", "authors": ["Hanlin Wu", "Xufeng Duan", "Zhenguang Cai"], "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment", "categories": ["cs.CL", "q-bio.NC"], "comment": "Accepted by the 14th edition of the Workshop on Cognitive Modeling\n  and Computational Linguistics (CMCL 2025)", "summary": "Voice-based AI development faces unique challenges in processing both\nlinguistic and paralinguistic information. This study compares how large\naudio-language models (LALMs) and humans integrate speaker characteristics\nduring speech comprehension, asking whether LALMs process\nspeaker-contextualized language in ways that parallel human cognitive\nmechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing\npatterns with human EEG responses. Using surprisal and entropy metrics from the\nmodels, we analyzed their sensitivity to speaker-content incongruency across\nsocial stereotype violations (e.g., a man claiming to regularly get manicures)\nand biological knowledge violations (e.g., a man claiming to be pregnant).\nResults revealed that Qwen2-Audio exhibited increased surprisal for\nspeaker-incongruent content and its surprisal values significantly predicted\nhuman N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker\ncharacteristics. Importantly, neither model replicated the human-like\nprocessing distinction between social violations (eliciting N400 effects) and\nbiological violations (eliciting P600 effects). These findings reveal both the\npotential and limitations of current LALMs in processing speaker-contextualized\nlanguage, and suggest differences in social-linguistic processing mechanisms\nbetween humans and LALMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19633", "pdf": "https://arxiv.org/pdf/2503.19633", "abs": "https://arxiv.org/abs/2503.19633", "authors": ["Han Zhao", "Haotian Wang", "Yiping Peng", "Sitong Zhao", "Xiaoyu Tian", "Shuaiting Chen", "Yunjie Ji", "Xiangang Li"], "title": "1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training", "categories": ["cs.CL"], "comment": null, "summary": "The AM-DeepSeek-R1-Distilled is a large-scale dataset with thinking traces\nfor general reasoning tasks, composed of high-quality and challenging reasoning\nproblems. These problems are collected from a multitude of open-source\ndatasets, subjected to semantic deduplication and meticulous cleaning to\neliminate test set contamination. All responses within the dataset are\ndistilled from reasoning models (predominantly DeepSeek-R1) and have undergone\nrigorous verification procedures. Mathematical problems are validated by\nchecking against reference answers, code problems are verified using test\ncases, and other tasks are evaluated with the aid of a reward model. The\nAM-Distill-Qwen-32B model, which was trained through only simple Supervised\nFine-Tuning (SFT) using this batch of data, outperformed the\nDeepSeek-R1-Distill-Qwen-32B model on four benchmarks: AIME2024, MATH-500,\nGPQA-Diamond, and LiveCodeBench. Additionally, the AM-Distill-Qwen-72B model\nsurpassed the DeepSeek-R1-Distill-Llama-70B model on all benchmarks as well. We\nare releasing these 1.4 million problems and their corresponding responses to\nthe research community with the objective of fostering the development of\npowerful reasoning-oriented Large Language Models (LLMs). The dataset was\npublished in\n\\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19668", "pdf": "https://arxiv.org/pdf/2503.19668", "abs": "https://arxiv.org/abs/2503.19668", "authors": ["Fredy Alejandro Mendoza López", "Jefferson Rodriguez", "Fabio Martínez"], "title": "A multitask transformer to sign language translation using motion gesture primitives", "categories": ["cs.CL"], "comment": "32 pages, 10 tables, 13 figures", "summary": "The absence of effective communication the deaf population represents the\nmain social gap in this community. Furthermore, the sign language, main deaf\ncommunication tool, is unlettered, i.e., there is no formal written\nrepresentation. In consequence, main challenge today is the automatic\ntranslation among spatiotemporal sign representation and natural text language.\nRecent approaches are based on encoder-decoder architectures, where the most\nrelevant strategies integrate attention modules to enhance non-linear\ncorrespondences, besides, many of these approximations require complex training\nand architectural schemes to achieve reasonable predictions, because of the\nabsence of intermediate text projections. However, they are still limited by\nthe redundant background information of the video sequences. This work\nintroduces a multitask transformer architecture that includes a gloss learning\nrepresentation to achieve a more suitable translation. The proposed approach\nalso includes a dense motion representation that enhances gestures and includes\nkinematic information, a key component in sign language. From this\nrepresentation it is possible to avoid background information and exploit the\ngeometry of the signs, in addition, it includes spatiotemporal representations\nthat facilitate the alignment between gestures and glosses as an intermediate\ntextual representation. The proposed approach outperforms the state-of-the-art\nevaluated on the CoL-SLTD dataset, achieving a BLEU-4 of 72,64% in split 1, and\na BLEU-4 of 14,64% in split 2. Additionally, the strategy was validated on the\nRWTH-PHOENIX-Weather 2014 T dataset, achieving a competitive BLEU-4 of 11,58%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19828", "pdf": "https://arxiv.org/pdf/2503.19828", "abs": "https://arxiv.org/abs/2503.19828", "authors": ["Athiya Deviyani", "Fernando Diaz"], "title": "Contextual Metric Meta-Evaluation by Measuring Local Metric Accuracy", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 (Findings)", "summary": "Meta-evaluation of automatic evaluation metrics -- assessing evaluation\nmetrics themselves -- is crucial for accurately benchmarking natural language\nprocessing systems and has implications for scientific inquiry, production\nmodel development, and policy enforcement. While existing approaches to metric\nmeta-evaluation focus on general statements about the absolute and relative\nquality of metrics across arbitrary system outputs, in practice, metrics are\napplied in highly contextual settings, often measuring the performance for a\nhighly constrained set of system outputs. For example, we may only be\ninterested in evaluating a specific model or class of models. We introduce a\nmethod for contextual metric meta-evaluation by comparing the local metric\naccuracy of evaluation metrics. Across translation, speech recognition, and\nranking tasks, we demonstrate that the local metric accuracies vary both in\nabsolute value and relative effectiveness as we shift across evaluation\ncontexts. This observed variation highlights the importance of adopting\ncontext-specific metric evaluations over global ones.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19855", "pdf": "https://arxiv.org/pdf/2503.19855", "abs": "https://arxiv.org/abs/2503.19855", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yunjie Ji", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs), such as OpenAI-o1 and\nDeepSeek-R1, have demonstrated the effectiveness of test-time scaling, where\nextended reasoning processes substantially enhance model performance. Despite\nthis, current models are constrained by limitations in handling long texts and\nreinforcement learning (RL) training efficiency. To address these issues, we\npropose a simple yet effective test-time scaling approach Multi-round Thinking.\nThis method iteratively refines model reasoning by leveraging previous answers\nas prompts for subsequent rounds. Extensive experiments across multiple models,\nincluding QwQ-32B and DeepSeek-R1, consistently show performance improvements\non various benchmarks such as AIME 2024, MATH-500, GPQA-diamond, and\nLiveCodeBench. For instance, the accuracy of QwQ-32B improved from 80.3% (Round\n1) to 82.1% (Round 2) on the AIME 2024 dataset, while DeepSeek-R1 showed a\nsimilar increase from 79.7% to 82.0%. These results confirm that Multi-round\nThinking is a broadly applicable, straightforward approach to achieving stable\nenhancements in model performance, underscoring its potential for future\ndevelopments in test-time scaling techniques. The key prompt: {Original\nquestion prompt} The assistant's previous answer is: <answer> {last round\nanswer} </answer>, and please re-answer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "o1"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19092", "pdf": "https://arxiv.org/pdf/2503.19092", "abs": "https://arxiv.org/abs/2503.19092", "authors": ["Krisztian Balog", "Donald Metzler", "Zhen Qin"], "title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly integral to information\nretrieval (IR), powering ranking, evaluation, and AI-assisted content creation.\nThis widespread adoption necessitates a critical examination of potential\nbiases arising from the interplay between these LLM-based components. This\npaper synthesizes existing research and presents novel experiment designs that\nexplore how LLM-based rankers and assistants influence LLM-based judges. We\nprovide the first empirical evidence of LLM judges exhibiting significant bias\ntowards LLM-based rankers. Furthermore, we observe limitations in LLM judges'\nability to discern subtle system performance differences. Contrary to some\nprevious findings, our preliminary study does not find evidence of bias against\nAI-generated content. These results highlight the need for a more holistic view\nof the LLM-driven information ecosystem. To this end, we offer initial\nguidelines and a research agenda to ensure the reliable use of LLMs in IR\nevaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19262", "pdf": "https://arxiv.org/pdf/2503.19262", "abs": "https://arxiv.org/abs/2503.19262", "authors": ["Ruiyi Wang", "Yushuo Zheng", "Zicheng Zhang", "Chunyi Li", "Shuaicheng Liu", "Guangtao Zhai", "Xiaohong Liu"], "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Existing real-world image dehazing methods primarily attempt to fine-tune\npre-trained models or adapt their inference procedures, thus heavily relying on\nthe pre-trained models and associated training data. Moreover, restoring\nheavily distorted information under dense haze requires generative diffusion\nmodels, whose potential in dehazing remains underutilized partly due to their\nlengthy sampling processes. To address these limitations, we introduce a novel\nhazing-dehazing pipeline consisting of a Realistic Hazy Image Generation\nframework (HazeGen) and a Diffusion-based Dehazing framework (DiffDehaze).\nSpecifically, HazeGen harnesses robust generative diffusion priors of\nreal-world hazy images embedded in a pre-trained text-to-image diffusion model.\nBy employing specialized hybrid training and blended sampling strategies,\nHazeGen produces realistic and diverse hazy images as high-quality training\ndata for DiffDehaze. To alleviate the inefficiency and fidelity concerns\nassociated with diffusion-based methods, DiffDehaze adopts an Accelerated\nFidelity-Preserving Sampling process (AccSamp). The core of AccSamp is the\nTiled Statistical Alignment Operation (AlignOp), which can provide a clean and\nfaithful dehazing estimate within a small fraction of sampling steps to reduce\ncomplexity and enable effective fidelity guidance. Extensive experiments\ndemonstrate the superior dehazing performance and visual quality of our\napproach over existing methods. The code is available at\nhttps://github.com/ruiyi-w/Learning-Hazing-to-Dehazing.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19470", "pdf": "https://arxiv.org/pdf/2503.19470", "abs": "https://arxiv.org/abs/2503.19470", "authors": ["Mingyang Chen", "Tianpeng Li", "Haoze Sun", "Yijie Zhou", "Chenzheng Zhu", "Fan Yang", "Zenan Zhou", "Weipeng Chen", "Haofen Wang", "Jeff Z. Pan", "Wen Zhang", "Huajun Chen"], "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning,\nexemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating\nreasoning with external search processes remains challenging, especially for\ncomplex multi-hop questions requiring multiple retrieval steps. We propose\nReSearch, a novel framework that trains LLMs to Reason with Search via\nreinforcement learning without using any supervised data on reasoning steps.\nOur approach treats search operations as integral components of the reasoning\nchain, where when and how to perform searches is guided by text-based thinking,\nand search results subsequently influence further reasoning. We train ReSearch\non Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct\nextensive experiments. Despite being trained on only one dataset, our models\ndemonstrate strong generalizability across various benchmarks. Analysis reveals\nthat ReSearch naturally elicits advanced reasoning capabilities such as\nreflection and self-correction during the reinforcement learning process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1", "self-correction"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19311", "pdf": "https://arxiv.org/pdf/2503.19311", "abs": "https://arxiv.org/abs/2503.19311", "authors": ["Weizhi Chen", "Jingbo Chen", "Yupeng Deng", "Jiansheng Chen", "Yuman Feng", "Zhihao Xi", "Diyou Liu", "Kai Li", "Yu Meng"], "title": "LRSCLIP: A Vision-Language Foundation Model for Aligning Remote Sensing Image with Longer Text", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 12 figures", "summary": "This study addresses the technical bottlenecks in handling long text and the\n\"hallucination\" issue caused by insufficient short text information in remote\nsensing vision-language foundation models (VLFM). We propose a novel\nvision-language foundation model, LRSCLIP, and a multimodal dataset, LRS2M. The\nmain contributions are as follows: (1) By integrating multi-source remote\nsensing data and adopting a large language model labeling strategy, we\nconstruct the LRS2M dataset, which contains 2 million image-text pairs,\nproviding both short and long texts for the first time, thus solving the\nproblem of semantic granularity limitations in existing datasets; (2) The\ndesign of the LRSCLIP architecture based on Long-CLIP's KPS module, which\nextends CLIP's text processing capacity and achieves fine-grained cross-modal\nfeature alignment through a dual-text loss weighting mechanism. Experimental\nresults show that LRSCLIP improves retrieval accuracy by 10\\%-20\\% over the\nLong-CLIP baseline in the zero-shot long-text cross-modal retrieval task. For\nthe zero-shot short-text cross-modal retrieval task, LRSCLIP achieves\nimprovements over the current best model, GeoRSCLIP, with increases of 0.17\\%,\n0.67\\%, and 0.92\\% in Text to Image R@1, Image to Text R@1, and mR on RSITMD,\nrespectively, and 0.04\\%, 2.93\\%, and 1.28\\% on RSICD. In the zero-shot image\nclassification task (average accuracy=75.75\\%) and semantic localization task\n(Rmi=0.7653), LRSCLIP achieves state-of-the-art performance. These results\nvalidate the dual advantages of fine-grained semantic understanding and global\nfeature matching in LRSCLIP. This work provides a new benchmark model and data\nsupport for remote sensing multimodal learning. The related code has been open\nsource and is available at https://github.com/MitsuiChen14/LRSCLIP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19391", "pdf": "https://arxiv.org/pdf/2503.19391", "abs": "https://arxiv.org/abs/2503.19391", "authors": ["Zhiying Song", "Lei Yang", "Fuxi Wen", "Jun Li"], "title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception", "categories": ["cs.CV", "cs.MA"], "comment": "Accepted to CVPR 2025", "summary": "Cooperative perception presents significant potential for enhancing the\nsensing capabilities of individual vehicles, however, inter-agent latency\nremains a critical challenge. Latencies cause misalignments in both spatial and\nsemantic features, complicating the fusion of real-time observations from the\nego vehicle with delayed data from others. To address these issues, we propose\nTraF-Align, a novel framework that learns the flow path of features by\npredicting the feature-level trajectory of objects from past observations up to\nthe ego vehicle's current time. By generating temporally ordered sampling\npoints along these paths, TraF-Align directs attention from the current-time\nquery to relevant historical features along each trajectory, supporting the\nreconstruction of current-time features and promoting semantic interaction\nacross multiple frames. This approach corrects spatial misalignment and ensures\nsemantic consistency across agents, effectively compensating for motion and\nachieving coherent feature fusion. Experiments on two real-world datasets,\nV2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for\nasynchronous cooperative perception.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19404", "pdf": "https://arxiv.org/pdf/2503.19404", "abs": "https://arxiv.org/abs/2503.19404", "authors": ["Jiaqi Liao", "Yuwei Niu", "Fanqing Meng", "Hao Li", "Changyao Tian", "Yinuo Du", "Yuwen Xiong", "Dianqi Li", "Xizhou Zhu", "Li Yuan", "Jifeng Dai", "Yu Cheng"], "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings", "categories": ["cs.CV"], "comment": "The code and weights will be open-sourced. Project page:\n  https://LangBridge.github.io/", "summary": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://LangBridge.github.io/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19405", "pdf": "https://arxiv.org/pdf/2503.19405", "abs": "https://arxiv.org/abs/2503.19405", "authors": ["Mingxiao Tu", "Hoijoon Jung", "Alireza Moghadam", "Jineel Raythatha", "Lachlan Allan", "Jeremy Hsu", "Andre Kyme", "Jinman Kim"], "title": "Multi-modal 3D Pose and Shape Estimation with Computed Tomography", "categories": ["cs.CV"], "comment": null, "summary": "In perioperative care, precise in-bed 3D patient pose and shape estimation\n(PSE) can be vital in optimizing patient positioning in preoperative planning,\nenabling accurate overlay of medical images for augmented reality-based\nsurgical navigation, and mitigating risks of prolonged immobility during\nrecovery. Conventional PSE methods relying on modalities such as RGB-D,\ninfrared, or pressure maps often struggle with occlusions caused by bedding and\ncomplex patient positioning, leading to inaccurate estimation that can affect\nclinical outcomes. To address these challenges, we present the first\nmulti-modal in-bed patient 3D PSE network that fuses detailed geometric\nfeatures extracted from routinely acquired computed tomography (CT) scans with\ndepth maps (mPSE-CT). mPSE-CT incorporates a shape estimation module that\nutilizes probabilistic correspondence alignment, a pose estimation module with\na refined neural network, and a final parameters mixing module. This\nmulti-modal network robustly reconstructs occluded body regions and enhances\nthe accuracy of the estimated 3D human mesh model. We validated mPSE-CT using\nproprietary whole-body rigid phantom and volunteer datasets in clinical\nscenarios. mPSE-CT outperformed the best-performing prior method by 23% and\n49.16% in pose and shape estimation respectively, demonstrating its potential\nfor improving clinical outcomes in challenging perioperative environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19457", "pdf": "https://arxiv.org/pdf/2503.19457", "abs": "https://arxiv.org/abs/2503.19457", "authors": ["Juntao Jian", "Xiuping Liu", "Zixuan Chen", "Manyi Li", "Jian Liu", "Ruizhen Hu"], "title": "G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation", "categories": ["cs.CV", "cs.RO"], "comment": "11 pages, 5 figures", "summary": "Recent advances in dexterous grasping synthesis have demonstrated significant\nprogress in producing reasonable and plausible grasps for many task purposes.\nBut it remains challenging to generalize to unseen object categories and\ndiverse task instructions. In this paper, we propose G-DexGrasp, a\nretrieval-augmented generation approach that can produce high-quality dexterous\nhand configurations for unseen object categories and language-based task\ninstructions. The key is to retrieve generalizable grasping priors, including\nthe fine-grained contact part and the affordance-related distribution of\nrelevant grasping instances, for the following synthesis pipeline.\nSpecifically, the fine-grained contact part and affordance act as generalizable\nguidance to infer reasonable grasping configurations for unseen objects with a\ngenerative model, while the relevant grasping distribution plays as\nregularization to guarantee the plausibility of synthesized grasps during the\nsubsequent refinement optimization. Our comparison experiments validate the\neffectiveness of our key designs for generalization and demonstrate the\nremarkable performance against the existing approaches. Project page:\nhttps://g-dexgrasp.github.io/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19503", "pdf": "https://arxiv.org/pdf/2503.19503", "abs": "https://arxiv.org/abs/2503.19503", "authors": ["Juncen Guo", "Xiaoguang Zhu", "Liangyu Teng", "Hao Yang", "Jing Liu", "Yang Liu", "Liang Song"], "title": "Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Class-incremental Learning (CIL) enables the model to incrementally absorb\nknowledge from new classes and build a generic classifier across all previously\nencountered classes. When the model optimizes with new classes, the knowledge\nof previous classes is inevitably erased, leading to catastrophic forgetting.\nAddressing this challenge requires making a trade-off between retaining old\nknowledge and accommodating new information. However, this balancing process\noften requires sacrificing some information, which can lead to a partial loss\nin the model's ability to discriminate between classes. To tackle this issue,\nwe design the adaptive weighted parameter fusion with Contrastive\nLanguage-Image Pre-training (CLIP), which not only takes into account the\nvariability of the data distribution of different tasks, but also retains all\nthe effective information of the parameter matrix to the greatest extent. In\naddition, we introduce a balance factor that can balance the data distribution\nalignment and distinguishability of adjacent tasks. Experimental results on\nseveral traditional benchmarks validate the superiority of the proposed method.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19508", "pdf": "https://arxiv.org/pdf/2503.19508", "abs": "https://arxiv.org/abs/2503.19508", "authors": ["Kartik Jangra", "Aman Kumar Singh", "Yashwani Mann", "Geetanjali Rathee"], "title": "Improved Alignment of Modalities in Large Vision Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in vision-language models have achieved remarkable\nresults in making language models understand vision inputs. However, a unified\napproach to align these models across diverse tasks such as image captioning\nand visual question answering remains a challenge. Existing methods either\nrequire very big language models or very big datasets which is not efficient in\nutilizing existing models. This paper addresses this gap and devises a training\nstrategy of auto-regressive vision-language models, to unify vision-language\ntasks like image-captioning and visual question answering. We propose four\ntraining stages for aligning the vision model with the language model, in other\nwords, the language model is given an ability to process visual inputs. We also\ndevise different attention masks for training transformer-based language models\nthat improve the quality of visual features. Further, we introduce some\nfindings, 1) the attention mask should not be applied on visual inputs, 2) the\nLanguage model converges faster on AI- generated data, 3) More work should be\ndone in the alignment stage during the pre-training of the model, 4) the model\ncan easily adapt to any downstream tasks like visual question answering on\nhealthcare datasets like PathVQA. After training the model for one epoch for\nall the stages, it outperforms large models like VILA-13 billion models on\ncommon benchmarks like CIDEr scores on COCO and Flickr30k datasets and achieves\nvery close scores to GIT-2 on the same dataset despite being a much smaller\nmodel trained on a much smaller dataset. All of the training is done using best\npractices available like multi- GPU parallel training, lower-precision training\nwith 16-bit float numbers, faster attention (SDPA), and gradient accumulation,\nand completed the training within 12 hours.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19661", "pdf": "https://arxiv.org/pdf/2503.19661", "abs": "https://arxiv.org/abs/2503.19661", "authors": ["Rupak Bose", "Chinedu Innocent Nwoye", "Aditya Bhat", "Nicolas Padoy"], "title": "CoSimGen: Controllable Diffusion Model for Simultaneous Image and Mask Generation", "categories": ["cs.CV"], "comment": "15 pages, 14 figure, 2 tables, project page at\n  https://camma-public.github.io/endogen/cosimgen", "summary": "The acquisition of annotated datasets with paired images and segmentation\nmasks is a critical challenge in domains such as medical imaging, remote\nsensing, and computer vision. Manual annotation demands significant resources,\nfaces ethical constraints, and depends heavily on domain expertise. Existing\ngenerative models often target single-modality outputs, either images or\nsegmentation masks, failing to address the need for high-quality, simultaneous\nimage-mask generation. Additionally, these models frequently lack adaptable\nconditioning mechanisms, restricting control over the generated outputs and\nlimiting their applicability for dataset augmentation and rare scenario\nsimulation. We propose CoSimGen, a diffusion-based framework for controllable\nsimultaneous image and mask generation. Conditioning is intuitively achieved\nthrough (1) text prompts grounded in class semantics, (2) spatial embedding of\ncontext prompts to provide spatial coherence, and (3) spectral embedding of\ntimestep information to model noise levels during diffusion. To enhance\ncontrollability and training efficiency, the framework incorporates contrastive\ntriplet loss between text and class embeddings, alongside diffusion and\nadversarial losses. Initial low-resolution outputs 128 x 128 are super-resolved\nto 512 x 512, producing high-fidelity images and masks with strict adherence to\nconditions. We evaluate CoSimGen on metrics such as FID, KID, LPIPS, Class FID,\nPositive predicted value for image fidelity and semantic alignment of generated\nsamples over 4 diverse datasets. CoSimGen achieves state-of-the-art performance\nacross all datasets, achieving the lowest KID of 0.11 and LPIPS of 0.53 across\ndatasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19706", "pdf": "https://arxiv.org/pdf/2503.19706", "abs": "https://arxiv.org/abs/2503.19706", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025 Camera-ready", "summary": "View-invariant representation learning from egocentric (first-person, ego)\nand exocentric (third-person, exo) videos is a promising approach toward\ngeneralizing video understanding systems across multiple viewpoints. However,\nthis area has been underexplored due to the substantial differences in\nperspective, motion patterns, and context between ego and exo views. In this\npaper, we propose a novel masked ego-exo modeling that promotes both causal\ntemporal dynamics and cross-view alignment, called Bootstrap Your Own Views\n(BYOV), for fine-grained view-invariant video representation learning from\nunpaired ego-exo videos. We highlight the importance of capturing the\ncompositional nature of human actions as a basis for robust cross-view\nunderstanding. Specifically, self-view masking and cross-view masking\npredictions are designed to learn view-invariant and powerful representations\nconcurrently. Experimental results demonstrate that our BYOV significantly\nsurpasses existing approaches with notable gains across all metrics in four\ndownstream ego-exo video tasks. The code is available at\nhttps://github.com/park-jungin/byov.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19739", "pdf": "https://arxiv.org/pdf/2503.19739", "abs": "https://arxiv.org/abs/2503.19739", "authors": ["Pihai Sun", "Junjun Jiang", "Yuanqi Yao", "Youyu Chen", "Wenbo Zhao", "Kui Jiang", "Xianming Liu"], "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via Frequency-Decoupled Alignment and Degradation-Robust Fusion", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth.Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19777", "pdf": "https://arxiv.org/pdf/2503.19777", "abs": "https://arxiv.org/abs/2503.19777", "authors": ["Vladan Stojnić", "Yannis Kalantidis", "Jiří Matas", "Giorgos Tolias"], "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose a training-free method for open-vocabulary semantic segmentation\nusing Vision-and-Language Models (VLMs). Our approach enhances the initial\nper-patch predictions of VLMs through label propagation, which jointly\noptimizes predictions by incorporating patch-to-patch relationships. Since VLMs\nare primarily optimized for cross-modal alignment and not for intra-modal\nsimilarity, we use a Vision Model (VM) that is observed to better capture these\nrelationships. We address resolution limitations inherent to patch-based\nencoders by applying label propagation at the pixel level as a refinement step,\nsignificantly improving segmentation accuracy near class boundaries. Our\nmethod, called LPOSS+, performs inference over the entire image, avoiding\nwindow-based processing and thereby capturing contextual interactions across\nthe full image. LPOSS+ achieves state-of-the-art performance among\ntraining-free methods, across a diverse set of datasets. Code:\nhttps://github.com/vladan-stojnic/LPOSS", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19881", "pdf": "https://arxiv.org/pdf/2503.19881", "abs": "https://arxiv.org/abs/2503.19881", "authors": ["Tianhao Qi", "Jianlong Yuan", "Wanquan Feng", "Shancheng Fang", "Jiawei Liu", "SiYu Zhou", "Qian He", "Hongtao Xie", "Yongdong Zhang"], "title": "Mask$^2$DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Sora has unveiled the immense potential of the Diffusion Transformer (DiT)\narchitecture in single-scene video generation. However, the more challenging\ntask of multi-scene video generation, which offers broader applications,\nremains relatively underexplored. To bridge this gap, we propose Mask$^2$DiT, a\nnovel approach that establishes fine-grained, one-to-one alignment between\nvideo segments and their corresponding text annotations. Specifically, we\nintroduce a symmetric binary mask at each attention layer within the DiT\narchitecture, ensuring that each text annotation applies exclusively to its\nrespective video segment while preserving temporal coherence across visual\ntokens. This attention mechanism enables precise segment-level\ntextual-to-visual alignment, allowing the DiT architecture to effectively\nhandle video generation tasks with a fixed number of scenes. To further equip\nthe DiT architecture with the ability to generate additional scenes based on\nexisting ones, we incorporate a segment-level conditional mask, which\nconditions each newly generated segment on the preceding video segments,\nthereby enabling auto-regressive scene extension. Both qualitative and\nquantitative experiments confirm that Mask$^2$DiT excels in maintaining visual\nconsistency across segments while ensuring semantic alignment between each\nsegment and its corresponding text description. Our project page is\nhttps://tianhao-qi.github.io/Mask2DiTProject.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19904", "pdf": "https://arxiv.org/pdf/2503.19904", "abs": "https://arxiv.org/abs/2503.19904", "authors": ["Zihang Lai", "Andrea Vedaldi"], "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025. Project website: zlai0.github.io/TrackTention", "summary": "Temporal consistency is critical in video prediction to ensure that outputs\nare coherent and free of artifacts. Traditional methods, such as temporal\nattention and 3D convolution, may struggle with significant object motion and\nmay not capture long-range temporal dependencies in dynamic scenes. To address\nthis gap, we propose the Tracktention Layer, a novel architectural component\nthat explicitly integrates motion information using point tracks, i.e.,\nsequences of corresponding points across frames. By incorporating these motion\ncues, the Tracktention Layer enhances temporal alignment and effectively\nhandles complex object motions, maintaining consistent feature representations\nover time. Our approach is computationally efficient and can be seamlessly\nintegrated into existing models, such as Vision Transformers, with minimal\nmodification. It can be used to upgrade image-only models to state-of-the-art\nvideo ones, sometimes outperforming models natively designed for video\nprediction. We demonstrate this on video depth prediction and video\ncolorization, where models augmented with the Tracktention Layer exhibit\nsignificantly improved temporal consistency compared to baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19912", "pdf": "https://arxiv.org/pdf/2503.19912", "abs": "https://arxiv.org/abs/2503.19912", "authors": ["Xiang Xu", "Lingdong Kong", "Hui Shuai", "Wenwei Zhang", "Liang Pan", "Kai Chen", "Ziwei Liu", "Qingshan Liu"], "title": "SuperFlow++: Enhanced Spatiotemporal Consistency for Cross-Modal Data Pretraining", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Preprint; 15 pages, 6 figures, 10 tables; Code at\n  https://github.com/Xiangxu-0103/SuperFlow", "summary": "LiDAR representation learning has emerged as a promising approach to reducing\nreliance on costly and labor-intensive human annotations. While existing\nmethods primarily focus on spatial alignment between LiDAR and camera sensors,\nthey often overlook the temporal dynamics critical for capturing motion and\nscene continuity in driving scenarios. To address this limitation, we propose\nSuperFlow++, a novel framework that integrates spatiotemporal cues in both\npretraining and downstream tasks using consecutive LiDAR-camera pairs.\nSuperFlow++ introduces four key components: (1) a view consistency alignment\nmodule to unify semantic information across camera views, (2) a dense-to-sparse\nconsistency regularization mechanism to enhance feature robustness across\nvarying point cloud densities, (3) a flow-based contrastive learning approach\nthat models temporal relationships for improved scene understanding, and (4) a\ntemporal voting strategy that propagates semantic information across LiDAR\nscans to improve prediction consistency. Extensive evaluations on 11\nheterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms\nstate-of-the-art methods across diverse tasks and driving conditions.\nFurthermore, by scaling both 2D and 3D backbones during pretraining, we uncover\nemergent properties that provide deeper insights into developing scalable 3D\nfoundation models. With strong generalizability and computational efficiency,\nSuperFlow++ establishes a new benchmark for data-efficient LiDAR-based\nperception in autonomous driving. The code is publicly available at\nhttps://github.com/Xiangxu-0103/SuperFlow", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19914", "pdf": "https://arxiv.org/pdf/2503.19914", "abs": "https://arxiv.org/abs/2503.19914", "authors": ["Sangwon Beak", "Hyeonwoo Kim", "Hanbyul Joo"], "title": "Learning 3D Object Spatial Relationships from Pre-trained 2D Diffusion Models", "categories": ["cs.CV"], "comment": "Project Page: https://tlb-miss.github.io/oor/", "summary": "We present a method for learning 3D spatial relationships between object\npairs, referred to as object-object spatial relationships (OOR), by leveraging\nsynthetically generated 3D samples from pre-trained 2D diffusion models. We\nhypothesize that images synthesized by 2D diffusion models inherently capture\nplausible and realistic OOR cues, enabling efficient ways to collect a 3D\ndataset to learn OOR for various unbounded object categories. Our approach\nbegins by synthesizing diverse images that capture plausible OOR cues, which we\nthen uplift into 3D samples. Leveraging our diverse collection of plausible 3D\nsamples for the object pairs, we train a score-based OOR diffusion model to\nlearn the distribution of their relative spatial relationships. Additionally,\nwe extend our pairwise OOR to multi-object OOR by enforcing consistency across\npairwise relations and preventing object collisions. Extensive experiments\ndemonstrate the robustness of our method across various object-object spatial\nrelationships, along with its applicability to real-world 3D scene arrangement\ntasks using the OOR diffusion model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19916", "pdf": "https://arxiv.org/pdf/2503.19916", "abs": "https://arxiv.org/abs/2503.19916", "authors": ["Lingdong Kong", "Dongyue Lu", "Xiang Xu", "Lai Xing Ng", "Wei Tsang Ooi", "Benoit R. Cottereau"], "title": "EventFly: Event Camera Perception from Ground to the Sky", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025; 30 pages, 8 figures, 16 tables; Project Page at\n  https://event-fly.github.io/", "summary": "Cross-platform adaptation in event-based dense perception is crucial for\ndeploying event cameras across diverse settings, such as vehicles, drones, and\nquadrupeds, each with unique motion dynamics, viewpoints, and class\ndistributions. In this work, we introduce EventFly, a framework for robust\ncross-platform adaptation in event camera perception. Our approach comprises\nthree key components: i) Event Activation Prior (EAP), which identifies\nhigh-activation regions in the target domain to minimize prediction entropy,\nfostering confident, domain-adaptive predictions; ii) EventBlend, a data-mixing\nstrategy that integrates source and target event voxel grids based on\nEAP-driven similarity and density maps, enhancing feature alignment; and iii)\nEventMatch, a dual-discriminator technique that aligns features from source,\ntarget, and blended domains for better domain-invariant learning. To\nholistically assess cross-platform adaptation abilities, we introduce EXPo, a\nlarge-scale benchmark with diverse samples across vehicle, drone, and quadruped\nplatforms. Extensive experiments validate our effectiveness, demonstrating\nsubstantial gains over popular adaptation methods. We hope this work can pave\nthe way for more adaptive, high-performing event perception across diverse and\ncomplex environments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19041", "pdf": "https://arxiv.org/pdf/2503.19041", "abs": "https://arxiv.org/abs/2503.19041", "authors": ["Kangwei Liu", "Mengru Wang", "Yujie Luo", "Lin Yuan", "Mengshu Sun", "Ningyu Zhang", "Lei Liang", "Zhiqiang Zhang", "Jun Zhou", "Huajun Chen"], "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often undermines their previously established safety alignment. To\nmitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, which comprises two simple, low-resource, and effective\ndata-driven methods that modify training data by previewing partial answer\nprefixes. Both methods aim to preserve the model's inherent safety mechanisms\nby minimizing perturbations to initial token distributions. Comprehensive\nexperiments demonstrate that LookAhead Tuning effectively maintains model\nsafety without sacrificing robust performance on downstream tasks. Our findings\nposition LookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs. Code is released at\nhttps://github.com/zjunlp/LookAheadTuning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19757", "pdf": "https://arxiv.org/pdf/2503.19757", "abs": "https://arxiv.org/abs/2503.19757", "authors": ["Zhi Hou", "Tianyi Zhang", "Yuwen Xiong", "Haonan Duan", "Hengjun Pu", "Ronglei Tong", "Chengyang Zhao", "Xizhou Zhu", "Yu Qiao", "Jifeng Dai", "Yuntao Chen"], "title": "Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy", "categories": ["cs.RO", "cs.CV"], "comment": "Preprint; https://robodita.github.io;", "summary": "While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19860", "pdf": "https://arxiv.org/pdf/2503.19860", "abs": "https://arxiv.org/abs/2503.19860", "authors": ["Junzhi Ning", "Dominic Marshall", "Yijian Gao", "Xiaodan Xing Yang Nan", "Yingying Fang", "Sheng Zhang", "Matthieu Komorowski", "Guang Yang"], "title": "Unpaired Translation of Chest X-ray Images for Lung Opacity Diagnosis via Adaptive Activation Masks and Cross-Domain Alignment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Chest X-ray radiographs (CXRs) play a pivotal role in diagnosing and\nmonitoring cardiopulmonary diseases. However, lung opac- ities in CXRs\nfrequently obscure anatomical structures, impeding clear identification of lung\nborders and complicating the localization of pathology. This challenge\nsignificantly hampers segmentation accuracy and precise lesion identification,\nwhich are crucial for diagnosis. To tackle these issues, our study proposes an\nunpaired CXR translation framework that converts CXRs with lung opacities into\ncounterparts without lung opacities while preserving semantic features. Central\nto our approach is the use of adaptive activation masks to selectively modify\nopacity regions in lung CXRs. Cross-domain alignment ensures translated CXRs\nwithout opacity issues align with feature maps and prediction labels from a\npre-trained CXR lesion classifier, facilitating the interpretability of the\ntranslation process. We validate our method using RSNA, MIMIC-CXR-JPG and JSRT\ndatasets, demonstrating superior translation quality through lower Frechet\nInception Distance (FID) and Kernel Inception Distance (KID) scores compared to\nexisting meth- ods (FID: 67.18 vs. 210.4, KID: 0.01604 vs. 0.225). Evaluation\non RSNA opacity, MIMIC acute respiratory distress syndrome (ARDS) patient CXRs\nand JSRT CXRs show our method enhances segmentation accuracy of lung borders\nand improves lesion classification, further underscoring its potential in\nclinical settings (RSNA: mIoU: 76.58% vs. 62.58%, Sensitivity: 85.58% vs.\n77.03%; MIMIC ARDS: mIoU: 86.20% vs. 72.07%, Sensitivity: 92.68% vs. 86.85%;\nJSRT: mIoU: 91.08% vs. 85.6%, Sensitivity: 97.62% vs. 95.04%). Our approach\nadvances CXR imaging analysis, especially in investigating segmentation impacts\nthrough image translation techniques.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-26.jsonl"}
{"id": "2503.19868", "pdf": "https://arxiv.org/pdf/2503.19868", "abs": "https://arxiv.org/abs/2503.19868", "authors": ["Sungyeon Kim", "Xinliang Zhu", "Xiaofan Lin", "Muhammet Bastan", "Douglas Gray", "Suha Kwak"], "title": "GENIUS: A Generative Framework for Universal Multimodal Search", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Generative retrieval is an emerging approach in information retrieval that\ngenerates identifiers (IDs) of target data based on a query, providing an\nefficient alternative to traditional embedding-based retrieval methods.\nHowever, existing models are task-specific and fall short of embedding-based\nretrieval in performance. This paper proposes GENIUS, a universal generative\nretrieval framework supporting diverse tasks across multiple modalities and\ndomains. At its core, GENIUS introduces modality-decoupled semantic\nquantization, transforming multimodal data into discrete IDs encoding both\nmodality and semantics. Moreover, to enhance generalization, we propose a query\naugmentation that interpolates between a query and its target, allowing GENIUS\nto adapt to varied query forms. Evaluated on the M-BEIR benchmark, it surpasses\nprior generative methods by a clear margin. Unlike embedding-based retrieval,\nGENIUS consistently maintains high retrieval speed across database size, with\ncompetitive performance across multiple benchmarks. With additional re-ranking,\nGENIUS often achieves results close to those of embedding-based methods while\npreserving efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-26.jsonl"}
