{"id": "2503.20194", "pdf": "https://arxiv.org/pdf/2503.20194", "abs": "https://arxiv.org/abs/2503.20194", "authors": ["Zhouhong Gu", "Xingzhou Chen", "Xiaoran Shi", "Tao Wang", "Suhang Zheng", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "PPO", "policy optimization", "DPO"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference learning", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "helpfulness", "harmlessness", "safety", "accuracy"], "score": 5}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "preference learning", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "helpfulness", "harmlessness", "safety", "accuracy"], "score": 5}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19948", "pdf": "https://arxiv.org/pdf/2503.19948", "abs": "https://arxiv.org/abs/2503.19948", "authors": ["Alexander Gambashidze", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "o1"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "ranking"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20309", "pdf": "https://arxiv.org/pdf/2503.20309", "abs": "https://arxiv.org/abs/2503.20309", "authors": ["Zitian Wang", "Yue Liao", "Kang Rong", "Fengyun Rao", "Yibo Yang", "Si Liu"], "title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20783", "pdf": "https://arxiv.org/pdf/2503.20783", "abs": "https://arxiv.org/abs/2503.20783", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20752", "pdf": "https://arxiv.org/pdf/2503.20752", "abs": "https://arxiv.org/abs/2503.20752", "authors": ["Huajie Tan", "Yuheng Ji", "Xiaoshuai Hao", "Minglan Lin", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 22 figures", "summary": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation.cExperimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20047", "pdf": "https://arxiv.org/pdf/2503.20047", "abs": "https://arxiv.org/abs/2503.20047", "authors": ["Yu Xin", "Gorkem Can Ates", "Kuang Gong", "Wei Shao"], "title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.19979", "pdf": "https://arxiv.org/pdf/2503.19979", "abs": "https://arxiv.org/abs/2503.19979", "authors": ["Enora Rice", "Ali Marashian", "Hannah Haynie", "Katharina von der Wense", "Alexis Palmer"], "title": "Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Workshop Language Models for Underserved\n  Communities", "summary": "Cross-lingual transfer learning is an invaluable tool for overcoming data\nscarcity, yet selecting a suitable transfer language remains a challenge. The\nprecise roles of linguistic typology, training data, and model architecture in\ntransfer language choice are not fully understood. We take a holistic approach,\nexamining how both dataset-specific and fine-grained typological features\ninfluence transfer language selection for part-of-speech tagging, considering\ntwo different sources for morphosyntactic features. While previous work\nexamines these dynamics in the context of bilingual biLSTMS, we extend our\nanalysis to a more modern transfer learning pipeline: zero-shot prediction with\npretrained multilingual models. We train a series of transfer language ranking\nsystems and examine how different feature inputs influence ranker performance\nacross architectures. Word overlap, type-token ratio, and genealogical distance\nemerge as top features across all architectures. Our findings reveal that a\ncombination of typological and dataset-dependent features leads to the best\nrankings, and that good performance can be obtained with either feature group\non its own.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20062", "pdf": "https://arxiv.org/pdf/2503.20062", "abs": "https://arxiv.org/abs/2503.20062", "authors": ["Jinsook Lee", "AJ Alvero", "Thorsten Joachims", "René Kizilcec"], "title": "Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays", "categories": ["cs.CL"], "comment": "48 pages, 10 figures, 6 tables", "summary": "People are increasingly using technologies equipped with large language\nmodels (LLM) to write texts for formal communication, which raises two\nimportant questions at the intersection of technology and society: Who do LLMs\nwrite like (model alignment); and can LLMs be prompted to change who they write\nlike (model steerability). We investigate these questions in the high-stakes\ncontext of undergraduate admissions at a selective university by comparing\nlexical and sentence variation between essays written by 30,000 applicants to\ntwo types of LLM-generated essays: one prompted with only the essay question\nused by the human applicants; and another with additional demographic\ninformation about each applicant. We consistently find that both types of\nLLM-generated essays are linguistically distinct from human-authored essays,\nregardless of the specific model and analytical approach. Further, prompting a\nspecific sociodemographic identity is remarkably ineffective in aligning the\nmodel with the linguistic patterns observed in human writing from this identity\ngroup. This holds along the key dimensions of sex, race, first-generation\nstatus, and geographic location. The demographically prompted and unprompted\nsynthetic texts were also more similar to each other than to the human text,\nmeaning that prompting did not alleviate homogenization. These issues of model\nalignment and steerability in current LLMs raise concerns about the use of LLMs\nin high-stakes contexts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20110", "pdf": "https://arxiv.org/pdf/2503.20110", "abs": "https://arxiv.org/abs/2503.20110", "authors": ["Pin-Jie Lin", "Rishab Balasubramanian", "Fengyuan Liu", "Nikhil Kandpal", "Tu Vu"], "title": "Efficient Model Development through Fine-tuning Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 4 figures, 13 tables", "summary": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20198", "pdf": "https://arxiv.org/pdf/2503.20198", "abs": "https://arxiv.org/abs/2503.20198", "authors": ["Alex Jinpeng Wang", "Linjie Li", "Zhengyuan Yang", "Lijuan Wang", "Min Li"], "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~\\cite{sd3} and GPT4o~\\cite{gpt4o} with DALL-E\n3~\\cite{dalle3} in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20207", "pdf": "https://arxiv.org/pdf/2503.20207", "abs": "https://arxiv.org/abs/2503.20207", "authors": ["Peiyuan Ni", "Chee Meng Chew", "Marcelo H. Ang Jr.", "Gregory S. Chirikjian"], "title": "Reasoning and Learning a Perceptual Metric for Self-Training of Reflective Objects in Bin-Picking with a Low-cost Camera", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages, 10 figures", "summary": "Bin-picking of metal objects using low-cost RGB-D cameras often suffers from\nsparse depth information and reflective surface textures, leading to errors and\nthe need for manual labeling. To reduce human intervention, we propose a\ntwo-stage framework consisting of a metric learning stage and a self-training\nstage. Specifically, to automatically process data captured by a low-cost\ncamera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that\noptimizes pose hypotheses under depth, collision, and boundary constraints. To\nfurther refine pose candidates, we adopt a Symmetry-aware Lie-group based\nBayesian Gaussian Mixture Model (SaL-BGMM), integrated with the\nExpectation-Maximization (EM) algorithm, for symmetry-aware filtering.\nAdditionally, we propose a Weighted Ranking Information Noise Contrastive\nEstimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from\nreconstructed data, supporting self-training on untrained or even unseen\nobjects. Experimental results show that our approach outperforms several\nstate-of-the-art methods on both the ROBI dataset and our newly introduced\nSelf-ROBI dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20641", "pdf": "https://arxiv.org/pdf/2503.20641", "abs": "https://arxiv.org/abs/2503.20641", "authors": ["Han Wu", "Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Xiaojin Fu", "Xiongwei Han", "Xing Li", "Hui-Ling Zhen", "Tao Zhong", "Mingxuan Yuan"], "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "categories": ["cs.CL"], "comment": "Work in progress; technical report", "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20228", "pdf": "https://arxiv.org/pdf/2503.20228", "abs": "https://arxiv.org/abs/2503.20228", "authors": ["Xiao Lin", "Manoj Acharya", "Anirban Roy", "Susmit Jha"], "title": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20289", "pdf": "https://arxiv.org/pdf/2503.20289", "abs": "https://arxiv.org/abs/2503.20289", "authors": ["Kaifan Sun", "Bingchen Yang", "Peter Wonka", "Jun Xiao", "Haiyong Jiang"], "title": "RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship Triples into the Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "The generation of indoor furniture layouts has significant applications in\naugmented reality, smart homes, and architectural design. Successful furniture\narrangement requires proper physical relationships (e.g., collision avoidance)\nand spacing relationships between furniture and their functional zones to be\nrespected. However, manually defined relationships are almost always incomplete\nand can produce unrealistic layouts. This work instead extracts spacing\nrelationships automatically based on a hierarchical analysis and adopts the\nDelaunay Triangulation to produce important triple relationships. Compared to\npairwise relationship modeling, triple relationships account for interactions\nand space utilization among multiple objects. To this end, we introduce\nRelTriple, a novel approach that enhances furniture distribution by learning\nspacing relationships between objects and regions. We formulate triple\nrelationships as object-to-object (O2O) losses and object-to-region (O2R)\nlosses and integrate them directly into the training process of generative\ndiffusion. Our approach consistently improves over existing state-of-the-art\nmethods in visual results evaluation metrics on unconditional layout\ngeneration, floorplan-conditioned layout generation, and scene rearrangement,\nachieving at least 12% on the introduced spatial relationship metric and\nsuperior spatial coherence and practical usability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20429", "pdf": "https://arxiv.org/pdf/2503.20429", "abs": "https://arxiv.org/abs/2503.20429", "authors": ["Guilherme Fernandes", "Vasco Ramos", "Regev Cohen", "Idan Szpektor", "João Magalhães"], "title": "Latent Beam Diffusion Models for Decoding Image Sequences", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20472", "pdf": "https://arxiv.org/pdf/2503.20472", "abs": "https://arxiv.org/abs/2503.20472", "authors": ["Yucheng Suo", "Fan Ma", "Linchao Zhu", "Tianyi Wang", "Fengyun Rao", "Yi Yang"], "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20662", "pdf": "https://arxiv.org/pdf/2503.20662", "abs": "https://arxiv.org/abs/2503.20662", "authors": ["Sadaf Khademi", "Mehran Shabanpour", "Reza Taleei", "Anastasia Oikonomou", "Arash Mohammadi"], "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20673", "pdf": "https://arxiv.org/pdf/2503.20673", "abs": "https://arxiv.org/abs/2503.20673", "authors": ["Yinan Sun", "Xiongkuo Min", "Zicheng Zhang", "Yixuan Gao", "Yuqin Cao", "Guangtao Zhai"], "title": "Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20782", "pdf": "https://arxiv.org/pdf/2503.20782", "abs": "https://arxiv.org/abs/2503.20782", "authors": ["Yan-Bo Lin", "Kevin Lin", "Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Chung-Ching Lin", "Xiaofei Wang", "Gedas Bertasius", "Lijuan Wang"], "title": "Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Project page: https://genjib.github.io/project_page/AVED/index.html", "summary": "In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https://genjib.github.io/project_page/AVED/index.html", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20308", "pdf": "https://arxiv.org/pdf/2503.20308", "abs": "https://arxiv.org/abs/2503.20308", "authors": ["Lee Chae-Yeon", "Oh Hyun-Bin", "Han EunGi", "Kim Sung-Bin", "Suekyeong Nam", "Tae-Hyun Oh"], "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-03-27.jsonl"}
{"id": "2503.20653", "pdf": "https://arxiv.org/pdf/2503.20653", "abs": "https://arxiv.org/abs/2503.20653", "authors": ["Antoine Schieb", "Bilal Hadjadji", "Daniel Tshokola Mweze", "Natalia Fernanda Valderrama", "Valentin Derangère", "Laurent Arnould", "Sylvain Ladoire", "Alain Lalande", "Louis-Oscar Morel", "Nathan Vinçon"], "title": "UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-03-27.jsonl"}
