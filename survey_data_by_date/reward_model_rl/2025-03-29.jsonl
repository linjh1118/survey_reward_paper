{"id": "2503.20995", "pdf": "https://arxiv.org/pdf/2503.20995", "abs": "https://arxiv.org/abs/2503.20995", "authors": ["Xiaomin Li", "Xupeng Chen", "Jingxuan Fan", "Eric Hanchen Jiang", "Mingye Gao"], "title": "Multi-head Reward Aggregation Guided by Entropy", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with safety guidelines typically\ninvolves reinforcement learning from human feedback (RLHF), relying on\nhuman-generated preference annotations. However, assigning consistent overall\nquality ratings is challenging, prompting recent research to shift towards\ndetailed evaluations based on multiple specific safety criteria. This paper\nuncovers a consistent observation: safety rules characterized by high rating\nentropy are generally less reliable in identifying responses preferred by\nhumans. Leveraging this finding, we introduce ENCORE, a straightforward\nentropy-guided approach that composes multi-head rewards by downweighting rules\nexhibiting high rating entropy. Theoretically, we demonstrate that rules with\nelevated entropy naturally receive minimal weighting in the Bradley-Terry\noptimization framework, justifying our entropy-based penalization. Through\nextensive experiments on RewardBench safety tasks, our method significantly\nsurpasses several competitive baselines, including random weighting, uniform\nweighting, single-head Bradley-Terry models, and LLM-based judging methods. Our\nproposed approach is training-free, broadly applicable to various datasets, and\nmaintains interpretability, offering a practical and effective solution for\nmulti-attribute reward modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "criteria"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21720", "pdf": "https://arxiv.org/pdf/2503.21720", "abs": "https://arxiv.org/abs/2503.21720", "authors": ["Souradip Chakraborty", "Sujay Bhatt", "Udari Madhushani Sehwag", "Soumya Suvra Ghosal", "Jiahao Qiu", "Mengdi Wang", "Dinesh Manocha", "Furong Huang", "Alec Koppel", "Sumitra Ganesh"], "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICLR 2025", "summary": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy\ndeployment in applications. Reinforcement learning from human feedback (RLHF)\nhas emerged as an effective technique to align LLMs to human preferences and\nbroader utilities, but it requires updating billions of model parameters, which\nis computationally expensive. Controlled Decoding, by contrast, provides a\nmechanism for aligning a model at inference time without retraining. However,\nsingle-agent decoding approaches often struggle to adapt to diverse tasks due\nto the complexity and variability inherent in these tasks. To strengthen the\ntest-time performance w.r.t the target task, we propose a mixture of\nagent-based decoding strategies leveraging the existing off-the-shelf aligned\nLLM policies. Treating each prior policy as an agent in the spirit of mixture\nof agent collaboration, we develop a decoding method that allows for\ninference-time alignment through a token-level selection strategy among\nmultiple agents. For each token, the most suitable LLM is dynamically chosen\nfrom a pool of models based on a long-term utility metric. This\npolicy-switching mechanism ensures optimal model selection at each step,\nenabling efficient collaboration and alignment among LLMs during decoding.\nTheoretical analysis of our proposed algorithm establishes optimal performance\nwith respect to the target task represented via a target reward for the given\noff-the-shelf models. We conduct comprehensive empirical evaluations with\nopen-source aligned models on diverse tasks and preferences, which demonstrates\nthe merits of this approach over single-agent decoding baselines. Notably,\nCollab surpasses the current SoTA decoding strategy, achieving an improvement\nof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "inference time"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 5}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21295", "pdf": "https://arxiv.org/pdf/2503.21295", "abs": "https://arxiv.org/abs/2503.21295", "authors": ["Shuaijie She", "Junxiao Liu", "Yifeng Liu", "Jiajun Chen", "Xin Huang", "Shujian Huang"], "title": "R-PRM: Reasoning-Driven Process Reward Modeling", "categories": ["cs.CL"], "comment": "The project is available at https://github.com/NJUNLP/R-PRM", "summary": "Large language models (LLMs) inevitably make mistakes when performing\nstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emerged\nas a promising solution by evaluating each reasoning step. However, existing\nPRMs typically output evaluation scores directly, limiting both learning\nefficiency and evaluation accuracy, which is further exacerbated by the\nscarcity of annotated data. To address these issues, we propose\nReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage stronger\nLLMs to generate seed data from limited annotations, effectively bootstrapping\nour model's reasoning capabilities and enabling comprehensive step-by-step\nevaluation. Second, we further enhance performance through preference\noptimization, without requiring additional annotated data. Third, we introduce\ninference-time scaling to fully harness the model's reasoning potential.\nExtensive experiments demonstrate R-PRM's effectiveness: on ProcessBench and\nPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,\nrespectively. When applied to guide mathematical reasoning, R-PRM achieves\nconsistent accuracy improvements of over 8.5 points across six challenging\ndatasets. Further analysis reveals that R-PRM exhibits more comprehensive\nevaluation and stronger generalization capabilities, thereby highlighting its\nsignificant potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "mathematical reasoning"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21749", "pdf": "https://arxiv.org/pdf/2503.21749", "abs": "https://arxiv.org/abs/2503.21749", "authors": ["Shitian Zhao", "Qilong Wu", "Xinyue Li", "Bo Zhang", "Ming Li", "Qi Qin", "Dongyang Liu", "Kaipeng Zhang", "Hongsheng Li", "Yu Qiao", "Peng Gao", "Bin Fu", "Zhen Li"], "title": "LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis", "categories": ["cs.CV"], "comment": "Project page: https://zhaoshitian.github.io/lexart/", "summary": "We introduce LeX-Art, a comprehensive suite for high-quality text-image\nsynthesis that systematically bridges the gap between prompt expressiveness and\ntext rendering fidelity. Our approach follows a data-centric paradigm,\nconstructing a high-quality data synthesis pipeline based on Deepseek-R1 to\ncurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined\n1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,\na robust prompt enrichment model, and train two text-to-image models, LeX-FLUX\nand LeX-Lumina, achieving state-of-the-art text rendering performance. To\nsystematically evaluate visual text generation, we introduce LeX-Bench, a\nbenchmark that assesses fidelity, aesthetics, and alignment, complemented by\nPairwise Normalized Edit Distance (PNED), a novel metric for robust text\naccuracy evaluation. Experiments demonstrate significant improvements, with\nLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX\noutperforming baselines in color (+3.18%), positional (+4.45%), and font\naccuracy (+3.81%). Our codes, models, datasets, and demo are publicly\navailable.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20801", "pdf": "https://arxiv.org/pdf/2503.20801", "abs": "https://arxiv.org/abs/2503.20801", "authors": ["Tao Meng", "Shuo Shan", "Hongen Shao", "Yuntao Shou", "Wei Ai", "Keqin Li"], "title": "SE-GNN: Seed Expanded-Aware Graph Neural Network with Iterative Optimization for Semi-supervised Entity Alignment", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Entity alignment aims to use pre-aligned seed pairs to find other equivalent\nentities from different knowledge graphs (KGs) and is widely used in graph\nfusion-related fields. However, as the scale of KGs increases, manually\nannotating pre-aligned seed pairs becomes difficult. Existing research utilizes\nentity embeddings obtained by aggregating single structural information to\nidentify potential seed pairs, thus reducing the reliance on pre-aligned seed\npairs. However, due to the structural heterogeneity of KGs, the quality of\npotential seed pairs obtained using only a single structural information is not\nideal. In addition, although existing research improves the quality of\npotential seed pairs through semi-supervised iteration, they underestimate the\nimpact of embedding distortion produced by noisy seed pairs on the alignment\neffect. In order to solve the above problems, we propose a seed expanded-aware\ngraph neural network with iterative optimization for semi-supervised entity\nalignment, named SE-GNN. First, we utilize the semantic attributes and\nstructural features of entities, combined with a conditional filtering\nmechanism, to obtain high-quality initial potential seed pairs. Next, we\ndesigned a local and global awareness mechanism. It introduces initial\npotential seed pairs and combines local and global information to obtain a more\ncomprehensive entity embedding representation, which alleviates the impact of\nKGs structural heterogeneity and lays the foundation for the optimization of\ninitial potential seed pairs. Then, we designed the threshold nearest neighbor\nembedding correction strategy. It combines the similarity threshold and the\nbidirectional nearest neighbor method as a filtering mechanism to select\niterative potential seed pairs and also uses an embedding correction strategy\nto eliminate the embedding distortion.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20919", "pdf": "https://arxiv.org/pdf/2503.20919", "abs": "https://arxiv.org/abs/2503.20919", "authors": ["Yupei Li", "Qiyang Sun", "Sunil Munthumoduku Krishna Murthy", "Emran Alturki", "Björn W. Schuller"], "title": "GatedxLSTM: A Multimodal Affective Computing Approach for Emotion Recognition in Conversations", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Affective Computing (AC) is essential for advancing Artificial General\nIntelligence (AGI), with emotion recognition serving as a key component.\nHowever, human emotions are inherently dynamic, influenced not only by an\nindividual's expressions but also by interactions with others, and\nsingle-modality approaches often fail to capture their full dynamics.\nMultimodal Emotion Recognition (MER) leverages multiple signals but\ntraditionally relies on utterance-level analysis, overlooking the dynamic\nnature of emotions in conversations. Emotion Recognition in Conversation (ERC)\naddresses this limitation, yet existing methods struggle to align multimodal\nfeatures and explain why emotions evolve within dialogues. To bridge this gap,\nwe propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly\nconsiders voice and transcripts of both the speaker and their conversational\npartner(s) to identify the most influential sentences driving emotional shifts.\nBy integrating Contrastive Language-Audio Pretraining (CLAP) for improved\ncross-modal alignment and employing a gating mechanism to emphasise emotionally\nimpactful utterances, GatedxLSTM enhances both interpretability and\nperformance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion\npredictions by modelling contextual dependencies. Experiments on the IEMOCAP\ndataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA)\nperformance among open-source methods in four-class emotion classification.\nThese results validate its effectiveness for ERC applications and provide an\ninterpretability analysis from a psychological perspective.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20880", "pdf": "https://arxiv.org/pdf/2503.20880", "abs": "https://arxiv.org/abs/2503.20880", "authors": ["Amaya Gallagher-Syed", "Henry Senior", "Omnia Alwazzan", "Elena Pontarini", "Michele Bombardieri", "Costantino Pitzalis", "Myles J. Lewis", "Michael R. Barnes", "Luca Rossi", "Gregory Slabaugh"], "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "q-bio.TO"], "comment": "Accepted for publication at CVPR 2025", "summary": "The development of biologically interpretable and explainable models remains\na key challenge in computational pathology, particularly for multistain\nimmunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable\ngraph neural network architecture for whole slide image (WSI) classification\nthat leverages both spatial and semantic features across multiple stains. At\nits core, BioX-CPath introduces a novel Stain-Aware Attention Pooling (SAAP)\nmodule that generates biologically meaningful, stain-aware patient embeddings.\nOur approach achieves state-of-the-art performance on both Rheumatoid Arthritis\nand Sjogren's Disease multistain datasets. Beyond performance metrics,\nBioX-CPath provides interpretable insights through stain attention scores,\nentropy measures, and stain interaction scores, that permit measuring model\nalignment with known pathological mechanisms. This biological grounding,\ncombined with strong classification performance, makes BioX-CPath particularly\nsuitable for clinical applications where interpretability is key. Source code\nand documentation can be found at: https://github.com/AmayaGS/BioX-CPath.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21011", "pdf": "https://arxiv.org/pdf/2503.21011", "abs": "https://arxiv.org/abs/2503.21011", "authors": ["Ana Ma", "Derek Powell"], "title": "Can Large Language Models Predict Associations Among Human Attitudes?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior work has shown that large language models (LLMs) can predict human\nattitudes based on other attitudes, but this work has largely focused on\npredictions from highly similar and interrelated attitudes. In contrast, human\nattitudes are often strongly associated even across disparate and dissimilar\ntopics. Using a novel dataset of human responses toward diverse attitude\nstatements, we found that a frontier language model (GPT-4o) was able to\nrecreate the pairwise correlations among individual attitudes and to predict\nindividuals' attitudes from one another. Crucially, in an advance over prior\nwork, we tested GPT-4o's ability to predict in the absence of\nsurface-similarity between attitudes, finding that while surface similarity\nimproves prediction accuracy, the model was still highly-capable of generating\nmeaningful social inferences between dissimilar attitudes. Altogether, our\nfindings indicate that LLMs capture crucial aspects of the deeper, latent\nstructure of human belief systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21061", "pdf": "https://arxiv.org/pdf/2503.21061", "abs": "https://arxiv.org/abs/2503.21061", "authors": ["Mehraveh Javan Roshtkhari", "Matthew Toews", "Marco Pedersoli"], "title": "Neural Architecture Search by Learning a Hierarchical Search Space", "categories": ["cs.CV"], "comment": null, "summary": "Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiable\nsearch related problems such as adversarial games. However, the performance of\nsuch approach highly depends on the order of the nodes that are considered at\neach branching of the tree. If the first branches cannot distinguish between\npromising and deceiving configurations for the final task, the efficiency of\nthe search is exponentially reduced. In Neural Architecture Search (NAS), as\nonly the final architecture matters, the visiting order of the branching can be\noptimized to improve learning. In this paper, we study the application of MCTS\nto NAS for image classification. We analyze several sampling methods and\nbranching alternatives for MCTS and propose to learn the branching by\nhierarchical clustering of architectures based on their similarity. The\nsimilarity is measured by the pairwise distance of output vectors of\narchitectures. Extensive experiments on two challenging benchmarks on CIFAR10\nand ImageNet show that MCTS, if provided with a good branching hierarchy, can\nyield promising solutions more efficiently than other approaches for NAS\nproblems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["MCTS"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21073", "pdf": "https://arxiv.org/pdf/2503.21073", "abs": "https://arxiv.org/abs/2503.21073", "authors": ["Andrew Lee", "Melanie Weber", "Fernanda Viégas", "Martin Wattenberg"], "title": "Shared Global and Local Geometry of Language Model Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Researchers have recently suggested that models share common representations.\nIn this work, we find that the token embeddings of language models exhibit\ncommon geometric structure. First, we find ``global'' similarities: token\nembeddings often share similar relative orientations. Next, we characterize\nlocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) by\ndefining a simple measure for the intrinsic dimension of each token embedding.\nOur intrinsic dimension measure demonstrates that token embeddings lie on a\nlower dimensional manifold. We qualitatively show that tokens with lower\nintrinsic dimensions often have semantically coherent clusters, while those\nwith higher intrinsic dimensions do not. Both characterizations allow us to\nfind similarities in the local geometry of token embeddings. Perhaps most\nsurprisingly, we find that alignment in token embeddings persists through the\nhidden states of language models, allowing us to develop an application for\ninterpretability. Namely, we empirically demonstrate that steering vectors from\none language model can be transferred to another, despite the two models having\ndifferent dimensions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21106", "pdf": "https://arxiv.org/pdf/2503.21106", "abs": "https://arxiv.org/abs/2503.21106", "authors": ["Gus G. Xia"], "title": "Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations", "categories": ["cs.CL", "68T27, 91E45", "I.2.0; I.2.4; F.4.1"], "comment": "12 pages, 2 figures. Part I of a multi-part position paper on a new\n  theory of mind", "summary": "This paper introduces function alignment, a novel theory of mind and\nintelligence that is both intuitively compelling and structurally grounded. It\nexplicitly models how meaning, interpretation, and analogy emerge from\ninteractions among layered representations, forming a coherent framework\ncapable not only of modeling minds but also of serving as a blueprint for\nbuilding them. One of the key theoretical insights derived from function\nalignment is bounded interpretability, which provides a unified explanation for\npreviously fragmented ideas in cognitive science, such as bounded rationality,\nsymbol grounding, and analogy-making. Beyond modeling, the function alignment\nframework bridges disciplines often kept apart, linking computational\narchitecture, psychological theory, and even contemplative traditions such as\nZen. Rather than building on any philosophical systems, it offers a structural\nfoundation upon which multiple ways of understanding the mind may be\nreconstructed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21082", "pdf": "https://arxiv.org/pdf/2503.21082", "abs": "https://arxiv.org/abs/2503.21082", "authors": ["Jinjie Mai", "Wenxuan Zhu", "Haozhe Liu", "Bing Li", "Cheng Zheng", "Jürgen Schmidhuber", "Bernard Ghanem"], "title": "Can Video Diffusion Model Reconstruct 4D Geometry?", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video is\nan important yet challenging problem. Conventional multiview geometry-based\napproaches often struggle with dynamic motion, whereas recent learning-based\nmethods either require specialized 4D representation or sophisticated\noptimization. In this paper, we present Sora3R, a novel framework that taps\ninto the rich spatiotemporal priors of large-scale video diffusion models to\ndirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stage\npipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuring\ncompatibility between the geometry and video latent spaces; (2) we finetune a\ndiffusion backbone in combined video and pointmap latent space to generate\ncoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforward\nmanner, requiring no external modules (e.g., depth, optical flow, or\nsegmentation) or iterative global alignment. Extensive experiments demonstrate\nthat Sora3R reliably recovers both camera poses and detailed scene geometry,\nachieving performance on par with state-of-the-art methods for dynamic 4D\nreconstruction across diverse scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21248", "pdf": "https://arxiv.org/pdf/2503.21248", "abs": "https://arxiv.org/abs/2503.21248", "authors": ["Yujie Liu", "Zonglin Yang", "Tong Xie", "Jinjie Ni", "Ben Gao", "Yuqiang Li", "Shixiang Tang", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21393", "pdf": "https://arxiv.org/pdf/2503.21393", "abs": "https://arxiv.org/abs/2503.21393", "authors": ["Rohitash Chandra", "Aryan Chaudhary", "Yeshwanth Rayavarapu"], "title": "An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study about the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT and Google Translate. In this study, we address this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts that have been\nwell translated by experts and use LLMs to generate their translations to\nEnglish, and then we provide a comparison with selected expert (human)\ntranslations. Our findings suggest that while LLMs have made significant\nprogress in translation accuracy, challenges remain in preserving sentiment and\nsemantic integrity, especially in figurative and philosophical contexts. The\nsentiment analysis revealed that GPT-4o and GPT-3.5 are better at preserving\nthe sentiments for the Bhagavad Gita (Sanskrit-English) translations when\ncompared to Google Translate. We observed a similar trend for the case of Tamas\n(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performs\nsimilarly to GPT-3.5 in the translation in terms of sentiments for the three\nlanguages. We found that LLMs are generally better at translation for capturing\nsentiments when compared to Google Translate.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21504", "pdf": "https://arxiv.org/pdf/2503.21504", "abs": "https://arxiv.org/abs/2503.21504", "authors": ["Yuxue Hu", "Junsong Li", "Meixuan Chen", "Dongyu Su", "Tongguan Wang", "Ying Sha"], "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21613", "pdf": "https://arxiv.org/pdf/2503.21613", "abs": "https://arxiv.org/abs/2503.21613", "authors": ["Javier Coronado-Blázquez"], "title": "Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach", "categories": ["cs.CL"], "comment": "22 pages, 6 figures", "summary": "We study the ability of large language models (LLMs) to generate\ncomprehensive and accurate book summaries solely from their internal knowledge,\nwithout recourse to the original text. Employing a diverse set of books and\nmultiple LLM architectures, we examine whether these models can synthesize\nmeaningful narratives that align with established human interpretations.\nEvaluation is performed with a LLM-as-a-judge paradigm: each AI-generated\nsummary is compared against a high-quality, human-written summary via a\ncross-model assessment, where all participating LLMs evaluate not only their\nown outputs but also those produced by others. This methodology enables the\nidentification of potential biases, such as the proclivity for models to favor\ntheir own summarization style over others. In addition, alignment between the\nhuman-crafted and LLM-generated summaries is quantified using ROUGE and\nBERTScore metrics, assessing the depth of grammatical and semantic\ncorrespondence. The results reveal nuanced variations in content representation\nand stylistic preferences among the models, highlighting both strengths and\nlimitations inherent in relying on internal knowledge for summarization tasks.\nThese findings contribute to a deeper understanding of LLM internal encodings\nof factual information and the dynamics of cross-model evaluation, with\nimplications for the development of more robust natural language generative\nsystems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "summarization"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21259", "pdf": "https://arxiv.org/pdf/2503.21259", "abs": "https://arxiv.org/abs/2503.21259", "authors": ["Wencheng Han", "Dongqian Guo", "Xiao Chen", "Pang Lyu", "Yi Jin", "Jianbing Shen"], "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data", "categories": ["cs.CV"], "comment": null, "summary": "Metal artifacts in CT slices have long posed challenges in medical\ndiagnostics. These artifacts degrade image quality, resulting in suboptimal\nvisualization and complicating the accurate interpretation of tissues adjacent\nto metal implants. To address these issues, we introduce the Latent Gemstone\nSpectral Imaging (GSI) Alignment Framework, which effectively reduces metal\nartifacts while avoiding the introduction of noise information. Our work is\nbased on a key finding that even artifact-affected ordinary CT sequences\ncontain sufficient information to discern detailed structures. The challenge\nlies in the inability to clearly represent this information. To address this\nissue, we developed an Alignment Framework that adjusts the representation of\nordinary CT images to match GSI CT sequences. GSI is an advanced imaging\ntechnique using multiple energy levels to mitigate artifacts caused by metal\nimplants. By aligning the representation to GSI data, we can effectively\nsuppress metal artifacts while clearly revealing detailed structure, without\nintroducing extraneous information into CT sequences. To facilitate the\napplication, we propose a new dataset, Artifacts-GSI, captured from real\npatients with metal implants, and establish a new benchmark based on this\ndataset. Experimental results show that our method significantly reduces metal\nartifacts and greatly enhances the readability of CT slices. All our code and\ndata are available at: https://um-lab.github.io/GSI-MAR/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21729", "pdf": "https://arxiv.org/pdf/2503.21729", "abs": "https://arxiv.org/abs/2503.21729", "authors": ["Zhicheng Lee", "Shulin Cao", "Jinxin Liu", "Jiajie Zhang", "Weichuan Liu", "Xiaoyin Che", "Lei Hou", "Juanzi Li"], "title": "ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely\nprimarily on parametric knowledge, limiting factual accuracy. While recent\nworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,\nthey suffer from overthinking and lack robustness in reasoning, reducing their\neffectiveness in question answering (QA) tasks. To address this, we propose\nReaRAG, a factuality-enhanced reasoning model that explores diverse queries\nwithout excessive iterations. Our solution includes a novel data construction\nframework with an upper bound on the reasoning chain length. Specifically, we\nfirst leverage an LRM to generate deliberate thinking, then select an action\nfrom a predefined action space (Search and Finish). For Search action, a query\nis executed against the RAG engine, where the result is returned as observation\nto guide reasoning steps later. This process iterates until a Finish action is\nchosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach\noutperforms existing baselines on multi-hop QA. Further analysis highlights its\nstrong reflective ability to recognize errors and refine its reasoning\ntrajectory. Our study enhances LRMs' factuality while effectively integrating\nrobust reasoning for Retrieval-Augmented Generation (RAG).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20807", "pdf": "https://arxiv.org/pdf/2503.20807", "abs": "https://arxiv.org/abs/2503.20807", "authors": ["Pin-Yu Chen", "Han Shen", "Payel Das", "Tianyi Chen"], "title": "Fundamental Safety-Capability Trade-offs in Fine-tuning Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "The first two authors contribute equally to this work and are listed\n  in alphabetical order", "summary": "Fine-tuning Large Language Models (LLMs) on some task-specific datasets has\nbeen a primary use of LLMs. However, it has been empirically observed that this\napproach to enhancing capability inevitably compromises safety, a phenomenon\nalso known as the safety-capability trade-off in LLM fine-tuning. This paper\npresents a theoretical framework for understanding the interplay between safety\nand capability in two primary safety-aware LLM fine-tuning strategies,\nproviding new insights into the effects of data similarity, context overlap,\nand alignment loss landscape. Our theoretical results characterize the\nfundamental limits of the safety-capability trade-off in LLM fine-tuning, which\nare also validated by numerical experiments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.20871", "pdf": "https://arxiv.org/pdf/2503.20871", "abs": "https://arxiv.org/abs/2503.20871", "authors": ["Silin Gao", "Sheryl Mathew", "Li Mi", "Sepideh Mamooler", "Mengjie Zhao", "Hiromi Wakaki", "Yuki Mitsufuji", "Syrielle Montariol", "Antoine Bosselut"], "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2025)", "summary": "Visual narrative generation transforms textual narratives into sequences of\nimages illustrating the content of the text. However, generating visual\nnarratives that are faithful to the input text and self-consistent across\ngenerated images remains an open challenge, due to the lack of knowledge\nconstraints used for planning the stories. In this work, we propose a new\nbenchmark, VinaBench, to address this challenge. Our benchmark annotates the\nunderlying commonsense and discourse constraints in visual narrative samples,\noffering systematic scaffolds for learning the implicit strategies of visual\nstorytelling. Based on the incorporated narrative constraints, we further\npropose novel metrics to closely evaluate the consistency of generated\nnarrative images and the alignment of generations with the input textual\nnarrative. Our results across three generative vision models demonstrate that\nlearning with VinaBench's knowledge constraints effectively improves the\nfaithfulness and cohesion of generated visual narratives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21683", "pdf": "https://arxiv.org/pdf/2503.21683", "abs": "https://arxiv.org/abs/2503.21683", "authors": ["Hui Wang"], "title": "LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, large language models (LLMs) have shown significant\nadvancements in natural language processing (NLP), with strong capa-bilities in\ngeneration, comprehension, and rea-soning. These models have found applications\nin education, intelligent decision-making, and gaming. However, effectively\nutilizing LLMs for strategic planning and decision-making in the game of Gomoku\nremains a challenge. This study aims to develop a Gomoku AI system based on\nLLMs, simulating the human learning process of playing chess. The system is\nde-signed to understand and apply Gomoku strat-egies and logic to make rational\ndecisions. The research methods include enabling the model to \"read the board,\"\n\"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while\nen-hancing its abilities through self-play and rein-forcement learning. The\nresults demonstrate that this approach significantly improves the se-lection of\nmove positions, resolves the issue of generating illegal positions, and reduces\npro-cess time through parallel position evaluation. After extensive self-play\ntraining, the model's Gomoku-playing capabilities have been notably enhanced.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21581", "pdf": "https://arxiv.org/pdf/2503.21581", "abs": "https://arxiv.org/abs/2503.21581", "authors": ["Liuyue Xie", "Jiancong Guo", "Ozan Cakmakci", "Andre Araujo", "Laszlo A. Jeni", "Zhiheng Jia"], "title": "AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate camera calibration is a fundamental task for 3D perception,\nespecially when dealing with real-world, in-the-wild environments where complex\noptical distortions are common. Existing methods often rely on pre-rectified\nimages or calibration patterns, which limits their applicability and\nflexibility. In this work, we introduce a novel framework that addresses these\nchallenges by jointly modeling camera intrinsic and extrinsic parameters using\na generic ray camera model. Unlike previous approaches, AlignDiff shifts focus\nfrom semantic to geometric features, enabling more accurate modeling of local\ndistortions. We propose AlignDiff, a diffusion model conditioned on geometric\npriors, enabling the simultaneous estimation of camera distortions and scene\ngeometry. To enhance distortion prediction, we incorporate edge-aware\nattention, focusing the model on geometric features around image edges, rather\nthan semantic content. Furthermore, to enhance generalizability to real-world\ncaptures, we incorporate a large database of ray-traced lenses containing over\nthree thousand samples. This database characterizes the distortion inherent in\na diverse variety of lens forms. Our experiments demonstrate that the proposed\nmethod significantly reduces the angular error of estimated ray bundles by ~8.2\ndegrees and overall calibration accuracy, outperforming existing approaches on\nchallenging, real-world datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21622", "pdf": "https://arxiv.org/pdf/2503.21622", "abs": "https://arxiv.org/abs/2503.21622", "authors": ["Lars Heckler-Kram", "Jan-Hendrik Neudeck", "Ulla Scheler", "Rebecca König", "Carsten Steger"], "title": "The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": "paper under review; dataset first released for the VAND3.0 challenge\n  @ CVPR 2025 https://sites.google.com/view/vand30cvpr2025/challenge", "summary": "In recent years, performance on existing anomaly detection benchmarks like\nMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with\nstate-of-the-art models often competing in the range of less than one\npercentage point. This lack of discriminatory power prevents a meaningful\ncomparison of models and thus hinders progress of the field, especially when\nconsidering the inherent stochastic nature of machine learning results. We\npresent MVTec AD 2, a collection of eight anomaly detection scenarios with more\nthan 8000 high-resolution images. It comprises challenging and highly relevant\nindustrial inspection use cases that have not been considered in previous\ndatasets, including transparent and overlapping objects, dark-field and back\nlight illumination, objects with high variance in the normal data, and\nextremely small defects. We provide comprehensive evaluations of\nstate-of-the-art methods and show that their performance remains below 60%\naverage AU-PRO. Additionally, our dataset provides test scenarios with lighting\ncondition changes to assess the robustness of methods under real-world\ndistribution shifts. We host a publicly accessible evaluation server that holds\nthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).\nAll image data is available at\nhttps://www.mvtec.com/company/research/datasets/mvtec-ad-2.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21695", "pdf": "https://arxiv.org/pdf/2503.21695", "abs": "https://arxiv.org/abs/2503.21695", "authors": ["Jiahe Qian", "Yaoyu Fang", "Jinkui Hao", "Bo Zhou"], "title": "AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 tables, 2 figures", "summary": "Accurate segmentation of cell nuclei in histopathology images is essential\nfor numerous biomedical research and clinical applications. However, existing\ncell nucleus segmentation methods only consider a single dataset (i.e., primary\ndomain), while neglecting to leverage supplementary data from diverse sources\n(i.e., auxiliary domains) to reduce overfitting and enhance the performance.\nAlthough incorporating multiple datasets could alleviate overfitting, it often\nexacerbates performance drops caused by domain shifts. In this work, we\nintroduce Adversarial Multi-domain Alignment of Segment Anything Model\n(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these\nobstacles through two key innovations. First, we propose a Conditional Gradient\nReversal Layer (CGRL), a multi-domain alignment module that harmonizes features\nfrom diverse domains to promote domain-invariant representation learning while\npreserving crucial discriminative features for the primary dataset. Second, we\naddress SAM's inherent low-resolution output by designing a High-Resolution\nDecoder (HR-Decoder), which directly produces fine-grained segmentation maps in\norder to capture intricate nuclei boundaries in high-resolution histology\nimages. To the best of our knowledge, this is the first attempt to adapt SAM\nfor multi-dataset learning with application to histology nuclei segmentation.\nWe validate our method on several publicly available datasets, demonstrating\nconsistent and significant improvements over state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21721", "pdf": "https://arxiv.org/pdf/2503.21721", "abs": "https://arxiv.org/abs/2503.21721", "authors": ["Jaywon Koo", "Jefferson Hernandez", "Moayed Haji-Ali", "Ziyan Yang", "Vicente Ordonez"], "title": "Evaluating Text-to-Image Synthesis with a Conditional Fréchet Distance", "categories": ["cs.CV"], "comment": null, "summary": "Evaluating text-to-image synthesis is challenging due to misalignment between\nestablished metrics and human preferences. We propose cFreD, a metric based on\nthe notion of Conditional Fr\\'echet Distance that explicitly accounts for both\nvisual fidelity and text-prompt alignment. Existing metrics such as Inception\nScore (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either\nimage quality or image-text alignment but not both which limits their\ncorrelation with human preferences. Scoring models explicitly trained to\nreplicate human preferences require constant updates and may not generalize to\nnovel generation techniques or out-of-domain inputs. Through extensive\nexperiments across multiple recently proposed text-to-image models and diverse\nprompt datasets, we demonstrate that cFreD exhibits a higher correlation with\nhuman judgments compared to statistical metrics, including metrics trained with\nhuman preferences. Our findings validate cFreD as a robust, future-proof metric\nfor the systematic evaluation of text-to-image models, standardizing\nbenchmarking in this rapidly evolving field. We release our evaluation toolkit\nand benchmark in the appendix.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "correlation"], "score": 3}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21745", "pdf": "https://arxiv.org/pdf/2503.21745", "abs": "https://arxiv.org/abs/2503.21745", "authors": ["Yuhan Zhang", "Mengchen Zhang", "Tong Wu", "Tengfei Wang", "Gordon Wetzstein", "Dahua Lin", "Ziwei Liu"], "title": "3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "3D generation is experiencing rapid advancements, while the development of 3D\nevaluation has not kept pace. How to keep automatic evaluation equitably\naligned with human perception has become a well-recognized challenge. Recent\nadvances in the field of language and image generation have explored human\npreferences and showcased respectable fitting ability. However, the 3D domain\nstill lacks such a comprehensive preference dataset over generative models. To\nmitigate this absence, we develop 3DGen-Arena, an integrated platform in a\nbattle manner. Then, we carefully design diverse text and image prompts and\nleverage the arena platform to gather human preferences from both public users\nand expert annotators, resulting in a large-scale multi-dimension human\npreference dataset 3DGen-Bench. Using this dataset, we further train a\nCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,\n3DGen-Eval. These two models innovatively unify the quality evaluation of\ntext-to-3D and image-to-3D generation, and jointly form our automated\nevaluation system with their respective strengths. Extensive experiments\ndemonstrate the efficacy of our scoring model in predicting human preferences,\nexhibiting a superior correlation with human ranks compared to existing\nmetrics. We believe that our 3DGen-Bench dataset and automated evaluation\nsystem will foster a more equitable evaluation in the field of 3D generation,\nfurther promoting the development of 3D generative models and their downstream\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "correlation", "dimension"], "score": 6}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21755", "pdf": "https://arxiv.org/pdf/2503.21755", "abs": "https://arxiv.org/abs/2503.21755", "authors": ["Dian Zheng", "Ziqi Huang", "Hongbo Liu", "Kai Zou", "Yinan He", "Fan Zhang", "Yuanhan Zhang", "Jingwen He", "Wei-Shi Zheng", "Yu Qiao", "Ziwei Liu"], "title": "VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness", "categories": ["cs.CV"], "comment": "Equal contributions from first two authors. Project page:\n  https://vchitect.github.io/VBench-2.0-project/ Code:\n  https://github.com/Vchitect/VBench", "summary": "Video generation has advanced significantly, evolving from producing\nunrealistic outputs to generating videos that appear visually convincing and\ntemporally coherent. To evaluate these video generative models, benchmarks such\nas VBench have been developed to assess their faithfulness, measuring factors\nlike per-frame aesthetics, temporal consistency, and basic prompt adherence.\nHowever, these aspects mainly represent superficial faithfulness, which focus\non whether the video appears visually convincing rather than whether it adheres\nto real-world principles. While recent models perform increasingly well on\nthese metrics, they still struggle to generate videos that are not just\nvisually plausible but fundamentally realistic. To achieve real \"world models\"\nthrough video generation, the next frontier lies in intrinsic faithfulness to\nensure that generated videos adhere to physical laws, commonsense reasoning,\nanatomical correctness, and compositional integrity. Achieving this level of\nrealism is essential for applications such as AI-assisted filmmaking and\nsimulated world modeling. To bridge this gap, we introduce VBench-2.0, a\nnext-generation benchmark designed to automatically evaluate video generative\nmodels for their intrinsic faithfulness. VBench-2.0 assesses five key\ndimensions: Human Fidelity, Controllability, Creativity, Physics, and\nCommonsense, each further broken down into fine-grained capabilities. Tailored\nfor individual dimensions, our evaluation framework integrates generalists such\nas state-of-the-art VLMs and LLMs, and specialists, including anomaly detection\nmethods proposed for video generation. We conduct extensive annotations to\nensure alignment with human judgment. By pushing beyond superficial\nfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new\nstandard for the next generation of video generative models in pursuit of\nintrinsic faithfulness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21766", "pdf": "https://arxiv.org/pdf/2503.21766", "abs": "https://arxiv.org/abs/2503.21766", "authors": ["Haolin Liu", "Xiaohang Zhan", "Zizheng Yan", "Zhongjin Luo", "Yuxin Wen", "Xiaoguang Han"], "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025. Homepage:\n  https://haolinliu97.github.io/Stable-Score/", "summary": "Establishing character shape correspondence is a critical and fundamental\ntask in computer vision and graphics, with diverse applications including\nre-topology, attribute transfer, and shape interpolation. Current dominant\nfunctional map methods, while effective in controlled scenarios, struggle in\nreal situations with more complex challenges such as non-isometric shape\ndiscrepancies. In response, we revisit registration-for-correspondence methods\nand tap their potential for more stable shape correspondence estimation. To\novercome their common issues including unstable deformations and the necessity\nfor careful pre-alignment or high-quality initial 3D correspondences, we\nintroduce Stable-SCore: A Stable Registration-based Framework for 3D Shape\nCorrespondence. We first re-purpose a foundation model for 2D character\ncorrespondence that ensures reliable and stable 2D mappings. Crucially, we\npropose a novel Semantic Flow Guided Registration approach that leverages 2D\ncorrespondence to guide mesh deformations. Our framework significantly\nsurpasses existing methods in challenging scenarios, and brings possibilities\nfor a wide array of real applications, as demonstrated in our results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21770", "pdf": "https://arxiv.org/pdf/2503.21770", "abs": "https://arxiv.org/abs/2503.21770", "authors": ["Anand Bhattad", "Konpat Preechakul", "Alexei A. Efros"], "title": "Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting", "categories": ["cs.CV"], "comment": "project page: https://visualjenga.github.io/", "summary": "This paper proposes a novel scene understanding task called Visual Jenga.\nDrawing inspiration from the game Jenga, the proposed task involves\nprogressively removing objects from a single image until only the background\nremains. Just as Jenga players must understand structural dependencies to\nmaintain tower stability, our task reveals the intrinsic relationships between\nscene elements by systematically exploring which objects can be removed while\npreserving scene coherence in both physical and geometric sense. As a starting\npoint for tackling the Visual Jenga task, we propose a simple, data-driven,\ntraining-free approach that is surprisingly effective on a range of real-world\nimages. The principle behind our approach is to utilize the asymmetry in the\npairwise relationships between objects within a scene and employ a large\ninpainting model to generate a set of counterfactuals to quantify the\nasymmetry.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21772", "pdf": "https://arxiv.org/pdf/2503.21772", "abs": "https://arxiv.org/abs/2503.21772", "authors": ["Zilin Xiao", "Pavel Suma", "Ayush Sachdeva", "Hao-Jen Wang", "Giorgos Kordopatis-Zilos", "Giorgos Tolias", "Vicente Ordonez"], "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We introduce LOCORE, Long-Context Re-ranker, a model that takes as input\nlocal descriptors corresponding to an image query and a list of gallery images\nand outputs similarity scores between the query and each gallery image. This\nmodel is used for image retrieval, where typically a first ranking is performed\nwith an efficient similarity measure, and then a shortlist of top-ranked images\nis re-ranked based on a more fine-grained similarity measure. Compared to\nexisting methods that perform pair-wise similarity estimation with local\ndescriptors or list-wise re-ranking with global descriptors, LOCORE is the\nfirst method to perform list-wise re-ranking with local descriptors. To achieve\nthis, we leverage efficient long-context sequence models to effectively capture\nthe dependencies between query and gallery images at the local-descriptor\nlevel. During testing, we process long shortlists with a sliding window\nstrategy that is tailored to overcome the context size limitations of sequence\nmodels. Our approach achieves superior performance compared with other\nre-rankers on established image retrieval benchmarks of landmarks (ROxf and\nRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)\nwhile having comparable latency to the pair-wise local descriptor re-rankers.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21776", "pdf": "https://arxiv.org/pdf/2503.21776", "abs": "https://arxiv.org/abs/2503.21776", "authors": ["Kaituo Feng", "Kaixiong Gong", "Bohao Li", "Zonghao Guo", "Yibing Wang", "Tianshuo Peng", "Benyou Wang", "Xiangyu Yue"], "title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "categories": ["cs.CV"], "comment": "Project page: https://github.com/tulerfeng/Video-R1", "summary": "Inspired by DeepSeek-R1's success in eliciting reasoning abilities through\nrule-based reinforcement learning (RL), we introduce Video-R1 as the first\nattempt to systematically explore the R1 paradigm for eliciting video reasoning\nwithin multimodal large language models (MLLMs). However, directly applying RL\ntraining with the GRPO algorithm to video reasoning presents two primary\nchallenges: (i) a lack of temporal modeling for video reasoning, and (ii) the\nscarcity of high-quality video-reasoning data. To address these issues, we\nfirst propose the T-GRPO algorithm, which encourages models to utilize temporal\ninformation in videos for reasoning. Additionally, instead of relying solely on\nvideo data, we incorporate high-quality image-reasoning data into the training\nprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold start\nand Video-R1-260k for RL training, both comprising image and video data.\nExperimental results demonstrate that Video-R1 achieves significant\nimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as\nwell as on general video benchmarks including MVBench and TempCompass, etc.\nNotably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning\nbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All\ncodes, models, data are released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21088", "pdf": "https://arxiv.org/pdf/2503.21088", "abs": "https://arxiv.org/abs/2503.21088", "authors": ["Haoming Xu", "Shuxun Wang", "Yanqiu Zhao", "Yi Zhong", "Ziyan Jiang", "Ningyuan Zhao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
{"id": "2503.21504", "pdf": "https://arxiv.org/pdf/2503.21504", "abs": "https://arxiv.org/abs/2503.21504", "authors": ["Yuxue Hu", "Junsong Li", "Meixuan Chen", "Dongyu Su", "Tongguan Wang", "Ying Sha"], "title": "Keyword-Oriented Multimodal Modeling for Euphemism Identification", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Euphemism identification deciphers the true meaning of euphemisms, such as\nlinking \"weed\" (euphemism) to \"marijuana\" (target keyword) in illicit texts,\naiding content moderation and combating underground markets. While existing\nmethods are primarily text-based, the rise of social media highlights the need\nfor multimodal analysis, incorporating text, images, and audio. However, the\nlack of multimodal datasets for euphemisms limits further research. To address\nthis, we regard euphemisms and their corresponding target keywords as keywords\nand first introduce a keyword-oriented multimodal corpus of euphemisms\n(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), including\ntext, images, and speech. We further propose a keyword-oriented multimodal\neuphemism identification method (KOM-EI), which uses cross-modal feature\nalignment and dynamic fusion modules to explicitly utilize the visual and audio\nfeatures of the keywords for efficient euphemism identification. Extensive\nexperiments demonstrate that KOM-EI outperforms state-of-the-art models and\nlarge language models, and show the importance of our multimodal datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-29.jsonl"}
