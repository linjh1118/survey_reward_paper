{"id": "2503.21819", "pdf": "https://arxiv.org/pdf/2503.21819", "abs": "https://arxiv.org/abs/2503.21819", "authors": ["Xuying Li", "Zhuo Li", "Yuji Kosuga", "Victor Bian"], "title": "Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human values and safety\nconstraints is challenging, especially when objectives like helpfulness,\ntruthfulness, and avoidance of harm conflict. Reinforcement Learning from Human\nFeedback (RLHF) has achieved notable success in steering models, but is complex\nand can be unstable. Recent approaches such as Direct Preference Optimization\n(DPO) simplify preference-based fine-tuning but may introduce bias or trade-off\ncertain objectives~\\cite{dpo}. In this work, we propose a Group Relative Policy\nOptimization (GRPO) framework with a multi-label reward regression model to\nachieve safe and aligned language generation. The GRPO algorithm optimizes a\npolicy by comparing groups of sampled responses, eliminating the need for a\nseparate value critic and improving training efficiency~\\cite{grpo}. We train a\nreward model to predict multiple alignment scores (e.g., safety, helpfulness,\netc.), which are combined into a single reward signal. We provide a theoretical\nderivation for using this learned multi-aspect reward within GRPO and discuss\nits advantages and limitations. Empirically, our approach improves all the\nsafety and quality metrics evaluated in language generation tasks on model\nscales (0.5B, 7B, and 14B parameters), demonstrating a robust balance of\nobjectives. We compare GRPO to PPO-based RLHF and DPO, highlighting that GRPO\nachieves alignment with significantly lower computational cost and explicit\nmulti-objective handling. \\textbf{We will open-source all trained models at\nhttps://huggingface.co/hydroxai.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "PPO", "reinforcement learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 8}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "truthfulness", "safety"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22677", "pdf": "https://arxiv.org/pdf/2503.22677", "abs": "https://arxiv.org/abs/2503.22677", "authors": ["Ruining Li", "Chuanxia Zheng", "Christian Rupprecht", "Andrea Vedaldi"], "title": "DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://ruiningli.com/dso", "summary": "Most 3D object generators focus on aesthetic quality, often neglecting\nphysical constraints necessary in applications. One such constraint is that the\n3D object should be self-supporting, i.e., remains balanced under gravity.\nPrior approaches to generating stable 3D objects used differentiable physics\nsimulators to optimize geometry at test-time, which is slow, unstable, and\nprone to local optima. Inspired by the literature on aligning generative models\nto external feedback, we propose Direct Simulation Optimization (DSO), a\nframework to use the feedback from a (non-differentiable) simulator to increase\nthe likelihood that the 3D generator outputs stable 3D objects directly. We\nconstruct a dataset of 3D objects labeled with a stability score obtained from\nthe physics simulator. We can then fine-tune the 3D generator using the\nstability score as the alignment metric, via direct preference optimization\n(DPO) or direct reward optimization (DRO), a novel objective, which we\nintroduce, to align diffusion models without requiring pairwise preferences.\nOur experiments show that the fine-tuned feed-forward generator, using either\nDPO or DRO objective, is much faster and more likely to produce stable objects\nthan test-time optimization. Notably, the DSO framework works even without any\nground-truth 3D objects for training, allowing the 3D generator to self-improve\nby automatically collecting simulation feedback on its own outputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22679", "pdf": "https://arxiv.org/pdf/2503.22679", "abs": "https://arxiv.org/abs/2503.22679", "authors": ["Weiqi Li", "Xuanyu Zhang", "Shijie Zhao", "Yabin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang"], "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Image quality assessment (IQA) focuses on the perceptual visual quality of\nimages, playing a crucial role in downstream tasks such as image\nreconstruction, compression, and generation. The rapid advancement of\nmulti-modal large language models (MLLMs) has significantly broadened the scope\nof IQA, moving toward comprehensive image quality understanding that\nincorporates content analysis, degradation perception, and comparison reasoning\nbeyond mere numerical scoring. Previous MLLM-based methods typically either\ngenerate numerical scores lacking interpretability or heavily rely on\nsupervised fine-tuning (SFT) using large-scale annotated datasets to provide\ndescriptive assessments, limiting their flexibility and applicability. In this\npaper, we propose Q-Insight, a reinforcement learning-based model built upon\ngroup relative policy optimization (GRPO), which demonstrates strong visual\nreasoning capability for image quality understanding while requiring only a\nlimited amount of rating scores and degradation labels. By jointly optimizing\nscore regression and degradation perception tasks with carefully designed\nreward functions, our approach effectively exploits their mutual benefits for\nenhanced performance. Extensive experiments demonstrate that Q-Insight\nsubstantially outperforms existing state-of-the-art methods in both score\nregression and degradation perception tasks, while exhibiting impressive\nzero-shot generalization to comparison reasoning tasks. Code will be available\nat https://github.com/lwq20020127/Q-Insight.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "comparison"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22182", "pdf": "https://arxiv.org/pdf/2503.22182", "abs": "https://arxiv.org/abs/2503.22182", "authors": ["Jianghao Lin", "Peng Du", "Jiaqi Liu", "Weite Li", "Yong Yu", "Weinan Zhang", "Yang Cao"], "title": "Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "E-commerce has revolutionized retail, yet its traditional workflows remain\ninefficient, with significant time and resource costs tied to product design\nand manufacturing inventory. This paper introduces a novel system deployed at\nAlibaba that leverages AI-generated items (AIGI) to address these challenges\nwith personalized text-to-image generation for e-commercial product design.\nAIGI enables an innovative business mode called \"sell it before you make it\",\nwhere merchants can design fashion items and generate photorealistic images\nwith digital models based on textual descriptions. Only when the items have\nreceived a certain number of orders, do the merchants start to produce them,\nwhich largely reduces reliance on physical prototypes and thus accelerates time\nto market. For such a promising application, we identify the underlying key\nscientific challenge, i.e., capturing the users' group-level personalized\npreferences towards multiple generated candidate images. To this end, we\npropose a Personalized Group-Level Preference Alignment Framework for Diffusion\nModels (i.e., PerFusion). We first design PerFusion Reward Model for user\npreference estimation with a feature-crossing-based personalized plug-in. Then\nwe develop PerFusion with a personalized adaptive network to model diverse\npreferences across users, and meanwhile derive the group-level preference\noptimization objective to capture the comparative behaviors among multiple\ncandidates. Both offline and online experiments demonstrate the effectiveness\nof our proposed algorithm. The AI-generated items have achieved over 13%\nrelative improvements for both click-through rate and conversion rate compared\nto their human-designed counterparts, validating the revolutionary potential of\nAI-generated items for e-commercial platforms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22115", "pdf": "https://arxiv.org/pdf/2503.22115", "abs": "https://arxiv.org/abs/2503.22115", "authors": ["Yazhou Zhang", "Qimeng Liu", "Qiuchi Li", "Peng Zhang", "Jing Qin"], "title": "Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Evaluating the value alignment of large language models (LLMs) has\ntraditionally relied on single-sentence adversarial prompts, which directly\nprobe models with ethically sensitive or controversial questions. However, with\nthe rapid advancements in AI safety techniques, models have become increasingly\nadept at circumventing these straightforward tests, limiting their\neffectiveness in revealing underlying biases and ethical stances. To address\nthis limitation, we propose an upgraded value alignment benchmark that moves\nbeyond single-sentence prompts by incorporating multi-turn dialogues and\nnarrative-based scenarios. This approach enhances the stealth and adversarial\nnature of the evaluation, making it more robust against superficial safeguards\nimplemented in modern LLMs. We design and implement a dataset that includes\nconversational traps and ethically ambiguous storytelling, systematically\nassessing LLMs' responses in more nuanced and context-rich settings.\nExperimental results demonstrate that this enhanced methodology can effectively\nexpose latent biases that remain undetected in traditional single-shot\nevaluations. Our findings highlight the necessity of contextual and dynamic\ntesting for value alignment in LLMs, paving the way for more sophisticated and\nrealistic assessments of AI ethics and safety.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22303", "pdf": "https://arxiv.org/pdf/2503.22303", "abs": "https://arxiv.org/abs/2503.22303", "authors": ["Magdalena Kaiser", "Gerhard Weikum"], "title": "Preference-based Learning with Retrieval Augmented Generation for Conversational Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "WWW 2025 Short Paper, 5 pages", "summary": "Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22265", "pdf": "https://arxiv.org/pdf/2503.22265", "abs": "https://arxiv.org/abs/2503.22265", "authors": ["Haomin Zhang", "Chang Liu", "Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 5 figures", "summary": "Currently, high-quality, synchronized audio is synthesized using various\nmulti-modal joint learning frameworks, leveraging video and optional text\ninputs. In the video-to-audio benchmarks, video-to-audio quality, semantic\nalignment, and audio-visual synchronization are effectively achieved. However,\nin real-world scenarios, speech and audio often coexist in videos\nsimultaneously, and the end-to-end generation of synchronous speech and audio\ngiven video and text conditions are not well studied. Therefore, we propose an\nend-to-end multi-modal generation framework that simultaneously produces speech\nand audio based on video and text conditions. Furthermore, the advantages of\nvideo-to-audio (V2A) models for generating speech from videos remain unclear.\nThe proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, a\ntext-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF)\nmodule. In the evaluation, the proposed end-to-end framework achieves\nstate-of-the-art performance on the video-audio benchmark, video-speech\nbenchmark, and text-speech benchmark. In detail, our framework achieves\ncomparable results in the comparison with state-of-the-art models for the\nvideo-audio and text-speech benchmarks, and surpassing state-of-the-art models\nin the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM\n78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to\n7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbing\nsettings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22200", "pdf": "https://arxiv.org/pdf/2503.22200", "abs": "https://arxiv.org/abs/2503.22200", "authors": ["Haomin Zhang", "Sizhe Shan", "Haoyu Wang", "Zihao Chen", "Xiulong Liu", "Chaofan Ding", "Xinhan Di"], "title": "Enhance Generation Quality of Flow Matching V2A Model via Multi-Step CoT-Like Guidance and Combined Preference Optimization", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "10 pages, 4 figures", "summary": "Creating high-quality sound effects from videos and text prompts requires\nprecise alignment between visual and audio domains, both semantically and\ntemporally, along with step-by-step guidance for professional audio generation.\nHowever, current state-of-the-art video-guided audio generation models often\nfall short of producing high-quality audio for both general and specialized use\ncases. To address this challenge, we introduce a multi-stage, multi-modal,\nend-to-end generative framework with Chain-of-Thought-like (CoT-like) guidance\nlearning, termed Chain-of-Perform (CoP). First, we employ a transformer-based\nnetwork architecture designed to achieve CoP guidance, enabling the generation\nof both general and professional audio. Second, we implement a multi-stage\ntraining framework that follows step-by-step guidance to ensure the generation\nof high-quality sound effects. Third, we develop a CoP multi-modal dataset,\nguided by video, to support step-by-step sound effects generation. Evaluation\nresults highlight the advantages of the proposed multi-stage CoP generative\nframework compared to the state-of-the-art models on a variety of datasets,\nwith FAD 0.79 to 0.74 (+6.33%), CLIP 16.12 to 17.70 (+9.80%) on VGGSound,\nSI-SDR 1.98dB to 3.35dB (+69.19%), MOS 2.94 to 3.49(+18.71%) on PianoYT-2h, and\nSI-SDR 2.22dB to 3.21dB (+44.59%), MOS 3.07 to 3.42 (+11.40%) on Piano-10h.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21813", "pdf": "https://arxiv.org/pdf/2503.21813", "abs": "https://arxiv.org/abs/2503.21813", "authors": ["Zhangcheng Qiang"], "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding LLM Hallucinations in Ontology Matching Systems", "categories": ["cs.CL", "cs.IR"], "comment": "10 pages, 4 figures, 3 tables, 2 prompt templates", "summary": "Hallucinations are inevitable in downstream tasks using large language models\n(LLMs). While addressing hallucinations becomes a substantial challenge for\nLLM-based ontology matching (OM) systems, we introduce a new benchmark dataset\ncalled OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching)\ndatasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing\nhallucinations of different LLMs performing OM tasks. These OM-specific\nhallucinations are carefully classified into two primary categories and six\nsub-categories. We showcase the usefulness of the dataset in constructing the\nLLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21888", "pdf": "https://arxiv.org/pdf/2503.21888", "abs": "https://arxiv.org/abs/2503.21888", "authors": ["Zeyad Alghamdi", "Tharindu Kumarage", "Garima Agrawal", "Mansooreh Karami", "Ibrahim Almuteb", "Huan Liu"], "title": "RedditESS: A Mental Health Social Support Interaction Dataset -- Understanding Effective Social Support to Refine AI-Driven Support Tools", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective mental health support is crucial for alleviating psychological\ndistress. While large language model (LLM)-based assistants have shown promise\nin mental health interventions, existing research often defines \"effective\"\nsupport primarily in terms of empathetic acknowledgments, overlooking other\nessential dimensions such as informational guidance, community validation, and\ntangible coping strategies. To address this limitation and better understand\nwhat constitutes effective support, we introduce RedditESS, a novel real-world\ndataset derived from Reddit posts, including supportive comments and original\nposters' follow-up responses. Grounded in established social science theories,\nwe develop an ensemble labeling mechanism to annotate supportive comments as\neffective or not and perform qualitative assessments to ensure the reliability\nof the annotations. Additionally, we demonstrate the practical utility of\nRedditESS by using it to guide LLM alignment toward generating more\ncontext-sensitive and genuinely helpful supportive responses. By broadening the\nunderstanding of effective support, our study paves the way for advanced\nAI-driven mental health interventions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21910", "pdf": "https://arxiv.org/pdf/2503.21910", "abs": "https://arxiv.org/abs/2503.21910", "authors": ["Karima Kadaoui", "Hanin Atwany", "Hamdan Al-Ali", "Abdelrahman Mohamed", "Ali Mekky", "Sergei Tilga", "Natalia Fedorova", "Ekaterina Artemova", "Hanan Aldarmaki", "Yova Kementchedjhieva"], "title": "JEEM: Vision-Language Understanding in Four Arabic Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce JEEM, a benchmark designed to evaluate Vision-Language Models\n(VLMs) on visual understanding across four Arabic-speaking countries: Jordan,\nThe Emirates, Egypt, and Morocco. JEEM includes the tasks of image captioning\nand visual question answering, and features culturally rich and regionally\ndiverse content. This dataset aims to assess the ability of VLMs to generalize\nacross dialects and accurately interpret cultural elements in visual contexts.\nIn an evaluation of five prominent open-source Arabic VLMs and GPT-4V, we find\nthat the Arabic VLMs consistently underperform, struggling with both visual\nunderstanding and dialect-specific generation. While GPT-4V ranks best in this\ncomparison, the model's linguistic competence varies across dialects, and its\nvisual understanding capabilities lag behind. This underscores the need for\nmore inclusive models and the value of culturally-diverse evaluation paradigms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "question answering"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21839", "pdf": "https://arxiv.org/pdf/2503.21839", "abs": "https://arxiv.org/abs/2503.21839", "authors": ["Haolong Yan", "Kaijun Tan", "Yeqing Shen", "Xin Huang", "Zheng Ge", "Xiangyu Zhang", "Si Li", "Daxin Jiang"], "title": "M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We investigate a critical yet under-explored question in Large\nVision-Language Models (LVLMs): Do LVLMs genuinely comprehend interleaved\nimage-text in the document? Existing document understanding benchmarks often\nassess LVLMs using question-answer formats, which are information-sparse and\ndifficult to guarantee the coverage of long-range dependencies. To address this\nissue, we introduce a novel and challenging Multimodal Document Summarization\nBenchmark (M-DocSum-Bench), which comprises 500 high-quality arXiv papers,\nalong with interleaved multimodal summaries aligned with human preferences.\nM-DocSum-Bench is a reference-based generation task and necessitates the\ngeneration of interleaved image-text summaries using provided reference images,\nthereby simultaneously evaluating capabilities in understanding, reasoning,\nlocalization, and summarization within complex multimodal document scenarios.\nTo facilitate this benchmark, we develop an automated framework to construct\nsummaries and propose a fine-grained evaluation method called M-DocEval.\nMoreover, we further develop a robust summarization baseline, i.e.,\nM-DocSum-7B, by progressive two-stage training with diverse instruction and\npreference data. The extensive results on our M-DocSum-Bench reveal that the\nleading LVLMs struggle to maintain coherence and accurately integrate\ninformation within long and interleaved contexts, often exhibiting confusion\nbetween similar images and a lack of robustness. Notably, M-DocSum-7B achieves\nstate-of-the-art performance compared to larger and closed-source models\n(including GPT-4o, Gemini Pro, Claude-3.5-Sonnet and Qwen2.5-VL-72B, etc.),\ndemonstrating the potential of LVLMs for improved interleaved image-text\nunderstanding. The code, data, and models are available at\nhttps://github.com/stepfun-ai/M-DocSum-Bench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "summarization", "fine-grained"], "score": 4}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21843", "pdf": "https://arxiv.org/pdf/2503.21843", "abs": "https://arxiv.org/abs/2503.21843", "authors": ["Hanyu Liu", "Siyao Li", "Ying Yu", "Yixuan Jiang", "Hang Xiao", "Jingxi Long", "Haotian Tang"], "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) is a fundamental technology for numerous\nhuman - centered intelligent applications. Although deep learning methods have\nbeen utilized to accelerate feature extraction, issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment remain largely\nunresolved. The aim of this paper is to address issues such as multimodal data\nmixing, activity heterogeneity, and complex model deployment in sensor-based\nhuman activity recognition. We propose a spatiotemporal attention modal\ndecomposition alignment fusion strategy to tackle the problem of the mixed\ndistribution of sensor data. Key discriminative features of activities are\ncaptured through cross-modal spatio-temporal disentangled representation, and\ngradient modulation is combined to alleviate data heterogeneity. In addition, a\nwearable deployment simulation system is constructed. We conducted experiments\non a large number of public datasets, demonstrating the effectiveness of the\nmodel.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21851", "pdf": "https://arxiv.org/pdf/2503.21851", "abs": "https://arxiv.org/abs/2503.21851", "authors": ["Alessandro Conti", "Massimiliano Mancini", "Enrico Fini", "Yiming Wang", "Paolo Rota", "Elisa Ricci"], "title": "On Large Multimodal Models as Open-World Image Classifiers", "categories": ["cs.CV"], "comment": "23 pages, 13 figures, code is available at\n  https://github.com/altndrr/lmms-owc", "summary": "Traditional image classification requires a predefined list of semantic\ncategories. In contrast, Large Multimodal Models (LMMs) can sidestep this\nrequirement by classifying images directly using natural language (e.g.,\nanswering the prompt \"What is the main object in the image?\"). Despite this\nremarkable capability, most existing studies on LMM classification performance\nare surprisingly limited in scope, often assuming a closed-world setting with a\npredefined set of categories. In this work, we address this gap by thoroughly\nevaluating LMM classification performance in a truly open-world setting. We\nfirst formalize the task and introduce an evaluation protocol, defining various\nmetrics to assess the alignment between predicted and ground truth classes. We\nthen evaluate 13 models across 10 benchmarks, encompassing prototypical,\nnon-prototypical, fine-grained, and very fine-grained classes, demonstrating\nthe challenges LMMs face in this task. Further analyses based on the proposed\nmetrics reveal the types of errors LMMs make, highlighting challenges related\nto granularity and fine-grained capabilities, showing how tailored prompting\nand reasoning can alleviate them.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22051", "pdf": "https://arxiv.org/pdf/2503.22051", "abs": "https://arxiv.org/abs/2503.22051", "authors": ["Zeeshan Ahmed", "Frank Seide", "Zhe Liu", "Rastislav Rabatin", "Jachym Kolar", "Niko Moritz", "Ruiming Xie", "Simone Merello", "Christian Fuegen"], "title": "Non-Monotonic Attention-based Read/Write Policy Learning for Simultaneous Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Simultaneous or streaming machine translation generates translation while\nreading the input stream. These systems face a quality/latency trade-off,\naiming to achieve high translation quality similar to non-streaming models with\nminimal latency. We propose an approach that efficiently manages this\ntrade-off. By enhancing a pretrained non-streaming model, which was trained\nwith a seq2seq mechanism and represents the upper bound in quality, we convert\nit into a streaming model by utilizing the alignment between source and target\ntokens. This alignment is used to learn a read/write decision boundary for\nreliable translation generation with minimal input. During training, the model\nlearns the decision boundary through a read/write policy module, employing\nsupervised learning on the alignment points (pseudo labels). The read/write\npolicy module, a small binary classification unit, can control the\nquality/latency trade-off during inference. Experimental results show that our\nmodel outperforms several strong baselines and narrows the gap with the\nnon-streaming baseline model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22074", "pdf": "https://arxiv.org/pdf/2503.22074", "abs": "https://arxiv.org/abs/2503.22074", "authors": ["Chuan-Wei Kuo", "Siyu Chen", "Chenqi Yan", "Yu Yang Fredrik Liu"], "title": "Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for specialized scientific\ndomains such as materials science, yet adapting them efficiently and accurately\nto domain-specific knowledge remains challenging due to limited data and high\nknowledge density. We propose a two-stage framework that combines structured\nmodel compression with a scientific fine-tuning regimen to address this\nchallenge. In the compression stage, we decompose the LLM's weight matrices\ninto local low-rank \"rank blocks\" and arrange these blocks in a Penrose-like\nnon-periodic tiling pattern. Each block is then compacted via spectral\ntransformations (e.g., discrete cosine or Fourier transforms), and a\nKullback-Leibler (KL) divergence-based alignment loss preserves the\ndistributional similarity between the compressed model's representations and\nthose of the original full model. In the adaptation stage, the compressed model\nis further tuned using a human-like scientific reading protocol: it processes\ntechnical materials science documents section by section, engaging in a\nstructured question-and-answer routine for each section. This section-wise Q&A\nfine-tuning strategy extracts explicit reasoning traces and gradually injects\ndomain knowledge, while minimizing catastrophic forgetting of the model's\ngeneral language capabilities. By balancing efficient compression with targeted\nadaptation, our two-stage approach enables precise specialization of LLMs to\nhigh-value domains under data-scarce conditions. We present this principled yet\nexploratory pipeline and outline its potential for advancing materials science\nknowledge integration, laying the groundwork for comprehensive empirical\nevaluation in future work.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21999", "pdf": "https://arxiv.org/pdf/2503.21999", "abs": "https://arxiv.org/abs/2503.21999", "authors": ["Tony Tran", "Bin Hu"], "title": "FACETS: Efficient Once-for-all Object Detection via Constrained Iterative Search", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 6 figures", "summary": "Neural Architecture Search (NAS) for deep learning object detection\nframeworks typically involves multiple modules, each performing distinct tasks.\nThese modules contribute to a vast search space, resulting in searches that can\ntake several GPU hours or even days, depending on the complexity of the search\nspace. This makes joint optimization both challenging and computationally\nexpensive. Furthermore, satisfying target device constraints across modules\nadds additional complexity to the optimization process. To address these\nchallenges, we propose \\textbf{FACETS}, e\\textbf{\\underline{F}}ficient\nOnce-for-\\textbf{\\underline{A}}ll Object Detection via\n\\textbf{\\underline{C}}onstrained\nit\\textbf{\\underline{E}}ra\\textbf{\\underline{T}}ive\\textbf{\\underline{S}}earch,\na novel unified iterative NAS method that refines the architecture of all\nmodules in a cyclical manner. FACETS leverages feedback from previous\niterations, alternating between fixing one module's architecture and optimizing\nthe others. This approach reduces the overall search space while preserving\ninterdependencies among modules and incorporates constraints based on the\ntarget device's computational budget. In a controlled comparison against\nprogressive and single-module search strategies, FACETS achieves architectures\nwith up to $4.75\\%$ higher accuracy twice as fast as progressive search\nstrategies in earlier stages, while still being able to achieve a global\noptimum. Moreover, FACETS demonstrates the ability to iteratively refine the\nsearch space, producing better performing architectures over time. The refined\nsearch space yields candidates with a mean accuracy up to $27\\%$ higher than\nglobal search and $5\\%$ higher than progressive search methods via random\nsampling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22019", "pdf": "https://arxiv.org/pdf/2503.22019", "abs": "https://arxiv.org/abs/2503.22019", "authors": ["Earl Ranario", "Lars Lundqvist", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "AGILE: A Diffusion-Based Attention-Guided Image and Label Translation for Efficient Cross-Domain Plant Trait Identification", "categories": ["cs.CV"], "comment": null, "summary": "Semantically consistent cross-domain image translation facilitates the\ngeneration of training data by transferring labels across different domains,\nmaking it particularly useful for plant trait identification in agriculture.\nHowever, existing generative models struggle to maintain object-level accuracy\nwhen translating images between domains, especially when domain gaps are\nsignificant. In this work, we introduce AGILE (Attention-Guided Image and Label\nTranslation for Efficient Cross-Domain Plant Trait Identification), a\ndiffusion-based framework that leverages optimized text embeddings and\nattention guidance to semantically constrain image translation. AGILE utilizes\npretrained diffusion models and publicly available agricultural datasets to\nimprove the fidelity of translated images while preserving critical object\nsemantics. Our approach optimizes text embeddings to strengthen the\ncorrespondence between source and target images and guides attention maps\nduring the denoising process to control object placement. We evaluate AGILE on\ncross-domain plant datasets and demonstrate its effectiveness in generating\nsemantically accurate translated images. Quantitative experiments show that\nAGILE enhances object detection performance in the target domain while\nmaintaining realism and consistency. Compared to prior image translation\nmethods, AGILE achieves superior semantic alignment, particularly in\nchallenging cases where objects vary significantly or domain gaps are\nsubstantial.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22081", "pdf": "https://arxiv.org/pdf/2503.22081", "abs": "https://arxiv.org/abs/2503.22081", "authors": ["Ziyue Huang", "Hongxi Yan", "Qiqi Zhan", "Shuai Yang", "Mingming Zhang", "Chenkai Zhang", "YiMing Lei", "Zeming Liu", "Qingjie Liu", "Yunhong Wang"], "title": "A Survey on Remote Sensing Foundation Models: From Vision to Multimodality", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of remote sensing foundation models, particularly\nvision and multimodal models, has significantly enhanced the capabilities of\nintelligent geospatial data interpretation. These models combine various data\nmodalities, such as optical, radar, and LiDAR imagery, with textual and\ngeographic information, enabling more comprehensive analysis and understanding\nof remote sensing data. The integration of multiple modalities allows for\nimproved performance in tasks like object detection, land cover classification,\nand change detection, which are often challenged by the complex and\nheterogeneous nature of remote sensing data. However, despite these\nadvancements, several challenges remain. The diversity in data types, the need\nfor large-scale annotated datasets, and the complexity of multimodal fusion\ntechniques pose significant obstacles to the effective deployment of these\nmodels. Moreover, the computational demands of training and fine-tuning\nmultimodal models require significant resources, further complicating their\npractical application in remote sensing image interpretation tasks. This paper\nprovides a comprehensive review of the state-of-the-art in vision and\nmultimodal foundation models for remote sensing, focusing on their\narchitecture, training methods, datasets and application scenarios. We discuss\nthe key challenges these models face, such as data alignment, cross-modal\ntransfer learning, and scalability, while also identifying emerging research\ndirections aimed at overcoming these limitations. Our goal is to provide a\nclear understanding of the current landscape of remote sensing foundation\nmodels and inspire future research that can push the boundaries of what these\nmodels can achieve in real-world applications. The list of resources collected\nby the paper can be found in the\nhttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22154", "pdf": "https://arxiv.org/pdf/2503.22154", "abs": "https://arxiv.org/abs/2503.22154", "authors": ["Jae-Young Yim", "Dongwook Kim", "Jae-Young Sim"], "title": "Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "We should collect large amount of data to train deep neural networks for\nvarious applications. Recently, the dataset distillation for images and texts\nhas been attracting a lot of attention, that reduces the original dataset to a\nsynthetic dataset while preserving essential task-relevant information.\nHowever, 3D point clouds distillation is almost unexplored due to the\nchallenges of unordered structures of points. In this paper, we propose a novel\ndistribution matching-based dataset distillation method for 3D point clouds\nthat jointly optimizes the geometric structures of synthetic dataset as well as\nthe orientations of synthetic models. To ensure the consistent feature\nalignment between different 3D point cloud models, we devise a permutation\ninvariant distribution matching loss with the sorted feature vectors. We also\nemploy learnable rotation angles to transform each syntheic model according to\nthe optimal orientation best representing the original feature distribution.\nExtensive experimental results on widely used four benchmark datasets,\nincluding ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate that\nthe proposed method consistently outperforms the existing methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22168", "pdf": "https://arxiv.org/pdf/2503.22168", "abs": "https://arxiv.org/abs/2503.22168", "authors": ["Woojung Han", "Yeonkyung Lee", "Chanyoung Kim", "Kwanghyun Park", "Seong Jae Hwang"], "title": "Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Diffusion-based text-to-image (T2I) models have recently excelled in\nhigh-quality image generation, particularly in a training-free manner, enabling\ncost-effective adaptability and generalization across diverse tasks. However,\nwhile the existing methods have been continuously focusing on several\nchallenges, such as \"missing objects\" and \"mismatched attributes,\" another\ncritical issue of \"mislocated objects\" remains where generated spatial\npositions fail to align with text prompts. Surprisingly, ensuring such\nseemingly basic functionality remains challenging in popular T2I models due to\nthe inherent difficulty of imposing explicit spatial guidance via text forms.\nTo address this, we propose STORM (Spatial Transport Optimization by\nRepositioning Attention Map), a novel training-free approach for spatially\ncoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),\nrooted in optimal transport theory, to dynamically adjust object attention maps\nfor precise spatial adherence, supported by a Spatial Transport (ST) Cost\nfunction that enhances spatial understanding. Our analysis shows that\nintegrating spatial awareness is most effective in the early denoising stages,\nwhile later phases refine details. Extensive experiments demonstrate that STORM\nsurpasses existing methods, effectively mitigating mislocated objects while\nimproving missing and mismatched attributes, setting a new benchmark for\nspatial alignment in T2I synthesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21902", "pdf": "https://arxiv.org/pdf/2503.21902", "abs": "https://arxiv.org/abs/2503.21902", "authors": ["Hamed Babaei Giglou", "Jennifer D'Souza", "Oliver Karras", "Sören Auer"], "title": "OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 3 figures. Accepted for the ESWC 2025 Resource Track", "summary": "Ontology Alignment (OA) is fundamental for achieving semantic\ninteroperability across diverse knowledge systems. We present OntoAligner, a\ncomprehensive, modular, and robust Python toolkit for ontology alignment,\ndesigned to address current limitations with existing tools faced by\npractitioners. Existing tools are limited in scalability, modularity, and ease\nof integration with recent AI advances. OntoAligner provides a flexible\narchitecture integrating existing lightweight OA techniques such as fuzzy\nmatching but goes beyond by supporting contemporary methods with\nretrieval-augmented generation and large language models for OA. The framework\nprioritizes extensibility, enabling researchers to integrate custom alignment\nalgorithms and datasets. This paper details the design principles,\narchitecture, and implementation of the OntoAligner, demonstrating its utility\nthrough benchmarks on standard OA tasks. Our evaluation highlights\nOntoAligner's ability to handle large-scale ontologies efficiently with few\nlines of code while delivering high alignment quality. By making OntoAligner\nopen-source, we aim to provide a resource that fosters innovation and\ncollaboration within the OA community, empowering researchers and practitioners\nwith a toolkit for reproducible OA research and real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22172", "pdf": "https://arxiv.org/pdf/2503.22172", "abs": "https://arxiv.org/abs/2503.22172", "authors": ["Minho Park", "Sunghyun Park", "Jungsoo Lee", "Hyojin Park", "Kyuwoong Hwang", "Fatih Porikli", "Jaegul Choo", "Sungha Choi"], "title": "Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of data scarcity in semantic segmentation\nby generating datasets through text-to-image (T2I) generation models, reducing\nimage acquisition and labeling costs. Segmentation dataset generation faces two\nkey challenges: 1) aligning generated samples with the target domain and 2)\nproducing informative samples beyond the training data. Fine-tuning T2I models\ncan help generate samples aligned with the target domain. However, it often\noverfits and memorizes training data, limiting their ability to generate\ndiverse and well-aligned samples. To overcome these issues, we propose\nConcept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectively\nidentifies and updates only the weights associated with necessary concepts\n(e.g., style or viewpoint) for domain alignment while preserving the pretrained\nknowledge of the T2I model to produce informative samples. We demonstrate its\neffectiveness in generating datasets for urban-scene segmentation,\noutperforming baseline and state-of-the-art methods in in-domain (few-shot and\nfully-supervised) settings, as well as in domain generalization tasks,\nespecially under challenging conditions such as adverse weather and varying\nillumination, further highlighting its superiority.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22179", "pdf": "https://arxiv.org/pdf/2503.22179", "abs": "https://arxiv.org/abs/2503.22179", "authors": ["Dailan He", "Xiahong Wang", "Shulun Wang", "Guanglu Song", "Bingqi Ma", "Hao Shao", "Yu Liu", "Hongsheng Li"], "title": "High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Face swapping aims to seamlessly transfer a source facial identity onto a\ntarget while preserving target attributes such as pose and expression.\nDiffusion models, known for their superior generative capabilities, have\nrecently shown promise in advancing face-swapping quality. This paper addresses\ntwo key challenges in diffusion-based face swapping: the prioritized\npreservation of identity over target attributes and the inherent conflict\nbetween identity and attribute conditioning. To tackle these issues, we\nintroduce an identity-constrained attribute-tuning framework for face swapping\nthat first ensures identity preservation and then fine-tunes for attribute\nalignment, achieved through a decoupled condition injection. We further enhance\nfidelity by incorporating identity and adversarial losses in a post-training\nrefinement stage. Our proposed identity-constrained diffusion-based\nface-swapping model outperforms existing methods in both qualitative and\nquantitative evaluations, demonstrating superior identity similarity and\nattribute consistency, achieving a new state-of-the-art performance in\nhigh-fidelity face swapping.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22193", "pdf": "https://arxiv.org/pdf/2503.22193", "abs": "https://arxiv.org/abs/2503.22193", "authors": ["Yang Liu", "Feixiang Liu", "Jiale Du", "Xinbo Gao", "Jungong Han"], "title": "Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks and supervised learning have achieved\nremarkable success in various fields but are limited by the need for large\nannotated datasets. Few-shot learning (FSL) addresses this limitation by\nenabling models to generalize from only a few labeled examples. Transductive\nfew-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeled\ndata, though it faces challenges like the hubness problem. To overcome these\nlimitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC)\nMethod, which addresses the key challenges in few-shot learning through three\ninnovative contributions. First, we introduce a decentralized covariance matrix\nto mitigate the hubness problem, ensuring a more uniform distribution of\nembeddings. Second, our method combines local alignment and global uniformity\nthrough adaptive weighting and nonlinear transformation, balancing intra-class\nclustering with inter-class separation. Third, we employ a Variational Sinkhorn\nFew-Shot Classifier to optimize the distances between samples and class\nprototypes, enhancing classification accuracy and robustness. These combined\ninnovations allow the UMMEC method to achieve superior performance with minimal\nlabeled data. Our UMMEC method significantly improves classification\nperformance with minimal labeled data, advancing the state-of-the-art in TFSL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22402", "pdf": "https://arxiv.org/pdf/2503.22402", "abs": "https://arxiv.org/abs/2503.22402", "authors": ["Yizhang Zhu", "Runzhi Jiang", "Boyan Li", "Nan Tang", "Yuyu Luo"], "title": "EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures, 3 tables", "summary": "Text-to-SQL automatically translates natural language queries to SQL,\nallowing non-technical users to retrieve data from databases without\nspecialized SQL knowledge. Despite the success of advanced LLM-based\nText-to-SQL approaches on leaderboards, their unsustainable computational\ncosts--often overlooked--stand as the \"elephant in the room\" in current\nleaderboard-driven research, limiting their economic practicability for\nreal-world deployment and widespread adoption. To tackle this, we exploratively\npropose EllieSQL, a complexity-aware routing framework that assigns queries to\nsuitable SQL generation pipelines based on estimated complexity. We investigate\nmultiple routers to direct simple queries to efficient approaches while\nreserving computationally intensive methods for complex cases. Drawing from\neconomics, we introduce the Token Elasticity of Performance (TEP) metric,\ncapturing cost-efficiency by quantifying the responsiveness of performance\ngains relative to token investment in SQL generation. Experiments show that\ncompared to always using the most advanced methods in our study, EllieSQL with\nthe Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising\nperformance on Bird development set, achieving more than a 2x boost in TEP over\nnon-routing approaches. This not only advances the pursuit of cost-efficient\nText-to-SQL but also invites the community to weigh resource efficiency\nalongside performance, contributing to progress in sustainable Text-to-SQL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22194", "pdf": "https://arxiv.org/pdf/2503.22194", "abs": "https://arxiv.org/abs/2503.22194", "authors": ["Yunhong Min", "Daehyeon Choi", "Kyeongmin Yeo", "Jihyun Lee", "Minhyuk Sung"], "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://origen2025.github.io", "summary": "We introduce ORIGEN, the first zero-shot method for 3D orientation grounding\nin text-to-image generation across multiple objects and diverse categories.\nWhile previous work on spatial grounding in image generation has mainly focused\non 2D positioning, it lacks control over 3D orientation. To address this, we\npropose a reward-guided sampling approach using a pretrained discriminative\nmodel for 3D orientation estimation and a one-step text-to-image generative\nflow model. While gradient-ascent-based optimization is a natural choice for\nreward-based guidance, it struggles to maintain image realism. Instead, we\nadopt a sampling-based approach using Langevin dynamics, which extends gradient\nascent by simply injecting random noise--requiring just a single additional\nline of code. Additionally, we introduce adaptive time rescaling based on the\nreward function to accelerate convergence. Our experiments show that ORIGEN\noutperforms both training-based and test-time guidance methods across\nquantitative metrics and user studies.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22218", "pdf": "https://arxiv.org/pdf/2503.22218", "abs": "https://arxiv.org/abs/2503.22218", "authors": ["Wenjie Liu", "Zhongliang Liu", "Xiaoyan Yang", "Man Sha", "Yang Li"], "title": "ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 14 figures", "summary": "3D scene stylization approaches based on Neural Radiance Fields (NeRF)\nachieve promising results by optimizing with Nearest Neighbor Feature Matching\n(NNFM) loss. However, NNFM loss does not consider global style information. In\naddition, the implicit representation of NeRF limits their fine-grained control\nover the resulting scenes. In this paper, we introduce ABC-GS, a novel\nframework based on 3D Gaussian Splatting to achieve high-quality 3D style\ntransfer. To this end, a controllable matching stage is designed to achieve\nprecise alignment between scene content and style features through segmentation\nmasks. Moreover, a style transfer loss function based on feature alignment is\nproposed to ensure that the outcomes of style transfer accurately reflect the\nglobal style of the reference image. Furthermore, the original geometric\ninformation of the scene is preserved with the depth loss and Gaussian\nregularization terms. Extensive experiments show that our ABC-GS provides\ncontrollability of style transfer and achieves stylization results that are\nmore faithfully aligned with the global style of the chosen artistic reference.\nOur homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22281", "pdf": "https://arxiv.org/pdf/2503.22281", "abs": "https://arxiv.org/abs/2503.22281", "authors": ["Xuan Loc Pham", "Mathias Prokop", "Bram van Ginneken", "Alessa Hering"], "title": "Divide to Conquer: A Field Decomposition Approach for Multi-Organ Whole-Body CT Image Registration", "categories": ["cs.CV"], "comment": null, "summary": "Image registration is an essential technique for the analysis of Computed\nTomography (CT) images in clinical practice. However, existing methodologies\nare predominantly tailored to a specific organ of interest and often exhibit\nlower performance on other organs, thus limiting their generalizability and\napplicability. Multi-organ registration addresses these limitations, but the\nsimultaneous alignment of multiple organs with diverse shapes, sizes and\nlocations requires a highly complex deformation field with a multi-layer\ncomposition of individual deformations. This study introduces a novel field\ndecomposition approach to address the high complexity of deformations in\nmulti-organ whole-body CT image registration. The proposed method is trained\nand evaluated on a longitudinal dataset of 691 patients, each with two CT\nimages obtained at distinct time points. These scans fully encompass the\nthoracic, abdominal, and pelvic regions. Two baseline registration methods are\nselected for this study: one based on optimization techniques and another based\non deep learning. Experimental results demonstrate that the proposed approach\noutperforms baseline methods in handling complex deformations in multi-organ\nwhole-body CT image registration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22285", "pdf": "https://arxiv.org/pdf/2503.22285", "abs": "https://arxiv.org/abs/2503.22285", "authors": ["Bin Zhang", "Jinggang Chen", "Xiaoyang Qu", "Guokuan Li", "Kai Lu", "Jiguang Wan", "Jing Xiao", "Jianzong Wang"], "title": "RUNA: Object-level Out-of-Distribution Detection via Regional Uncertainty Alignment of Multimodal Representations", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Enabling object detectors to recognize out-of-distribution (OOD) objects is\nvital for building reliable systems. A primary obstacle stems from the fact\nthat models frequently do not receive supervisory signals from unfamiliar data,\nleading to overly confident predictions regarding OOD objects. Despite previous\nprogress that estimates OOD uncertainty based on the detection model and\nin-distribution (ID) samples, we explore using pre-trained vision-language\nrepresentations for object-level OOD detection. We first discuss the\nlimitations of applying image-level CLIP-based OOD detection methods to\nobject-level scenarios. Building upon these insights, we propose RUNA, a novel\nframework that leverages a dual encoder architecture to capture rich contextual\ninformation and employs a regional uncertainty alignment mechanism to\ndistinguish ID from OOD objects effectively. We introduce a few-shot\nfine-tuning approach that aligns region-level semantic representations to\nfurther improve the model's capability to discriminate between similar objects.\nOur experiments show that RUNA substantially surpasses state-of-the-art methods\nin object-level OOD detection, particularly in challenging scenarios with\ndiverse and complex object instances.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22291", "pdf": "https://arxiv.org/pdf/2503.22291", "abs": "https://arxiv.org/abs/2503.22291", "authors": ["Bin Zhang", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "As object detectors are increasingly deployed as black-box cloud services or\npre-trained models with restricted access to the original training data, the\nchallenge of zero-shot object-level out-of-distribution (OOD) detection arises.\nThis task becomes crucial in ensuring the reliability of detectors in\nopen-world settings. While existing methods have demonstrated success in\nimage-level OOD detection using pre-trained vision-language models like CLIP,\ndirectly applying such models to object-level OOD detection presents challenges\ndue to the loss of contextual information and reliance on image-level\nalignment. To tackle these challenges, we introduce a new method that leverages\nvisual prompts and text-augmented in-distribution (ID) space construction to\nadapt CLIP for zero-shot object-level OOD detection. Our method preserves\ncritical contextual information and improves the ability to differentiate\nbetween ID and OOD objects, achieving competitive performance across different\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22359", "pdf": "https://arxiv.org/pdf/2503.22359", "abs": "https://arxiv.org/abs/2503.22359", "authors": ["Jiahao Xia", "Min Xu", "Wenjian Huang", "Jianguo Zhang", "Haimin Zhang", "Chunxia Xiao"], "title": "Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment", "categories": ["cs.CV"], "comment": "24 Pages, 9 Figures", "summary": "Despite the similar structures of human faces, existing face alignment\nmethods cannot learn unified knowledge from multiple datasets with different\nlandmark annotations. The limited training samples in a single dataset commonly\nresult in fragile robustness in this field. To mitigate knowledge discrepancies\namong different datasets and train a task-agnostic unified face alignment\n(TUFA) framework, this paper presents a strategy to unify knowledge from\nmultiple datasets. Specifically, we calculate a mean face shape for each\ndataset. To explicitly align these mean shapes on an interpretable plane based\non their semantics, each shape is then incorporated with a group of semantic\nalignment embeddings. The 2D coordinates of these aligned shapes can be viewed\nas the anchors of the plane. By encoding them into structure prompts and\nfurther regressing the corresponding facial landmarks using image features, a\nmapping from the plane to the target faces is finally established, which\nunifies the learning target of different datasets. Consequently, multiple\ndatasets can be utilized to boost the generalization ability of the model. The\nsuccessful mitigation of discrepancies also enhances the efficiency of\nknowledge transferring to a novel dataset, significantly boosts the performance\nof few-shot face alignment. Additionally, the interpretable plane endows TUFA\nwith a task-agnostic characteristic, enabling it to locate landmarks unseen\nduring training in a zero-shot manner. Extensive experiments are carried on\nseven benchmarks and the results demonstrate an impressive improvement in face\nalignment brought by knowledge discrepancies mitigation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22399", "pdf": "https://arxiv.org/pdf/2503.22399", "abs": "https://arxiv.org/abs/2503.22399", "authors": ["Ada Gorgun", "Bernt Schiele", "Jonas Fischer"], "title": "VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/adagorgun/VITAL", "summary": "Neural networks are widely adopted to solve complex and challenging tasks.\nEspecially in high-stakes decision-making, understanding their reasoning\nprocess is crucial, yet proves challenging for modern deep networks. Feature\nvisualization (FV) is a powerful tool to decode what information neurons are\nresponding to and hence to better understand the reasoning behind such\nnetworks. In particular, in FV we generate human-understandable images that\nreflect the information detected by neurons of interest. However, current\nmethods often yield unrecognizable visualizations, exhibiting repetitive\npatterns and visual artifacts that are hard to understand for a human. To\naddress these problems, we propose to guide FV through statistics of real image\nfeatures combined with measures of relevant network flow to generate\nprototypical images. Our approach yields human-understandable visualizations\nthat both qualitatively and quantitatively improve over state-of-the-art FVs\nacross various architectures. As such, it can be used to decode which\ninformation the network uses, complementing mechanistic circuits that identify\nwhere it is encoded. Code is available at: https://github.com/adagorgun/VITAL", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22417", "pdf": "https://arxiv.org/pdf/2503.22417", "abs": "https://arxiv.org/abs/2503.22417", "authors": ["David Fischinger", "Martin Boyer"], "title": "DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection", "categories": ["cs.CV"], "comment": "Published at the 25th Irish Machine Vision and Image Processing\n  Conference (IMVIP) --- Proceedings:\n  https://iprcs.github.io/pdf/IMVIP2023_Proceeding.pdf --- Dataset download:\n  https://zenodo.org/records/7326540/files/DF2023_train.zip\n  https://zenodo.org/records/7326540/files/DF2023_val.zip Kaggle:\n  https://www.kaggle.com/datasets/davidfischinger/df2023-digital-forensics-2023-dataset/data", "summary": "The deliberate manipulation of public opinion, especially through altered\nimages, which are frequently disseminated through online social networks, poses\na significant danger to society. To fight this issue on a technical level we\nsupport the research community by releasing the Digital Forensics 2023 (DF2023)\ntraining and validation dataset, comprising one million images from four major\nforgery categories: splicing, copy-move, enhancement and removal. This dataset\nenables an objective comparison of network architectures and can significantly\nreduce the time and effort of researchers preparing datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22462", "pdf": "https://arxiv.org/pdf/2503.22462", "abs": "https://arxiv.org/abs/2503.22462", "authors": ["Krispin Wandel", "Hesheng Wang"], "title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Poster:\n  https://cvpr.thecvf.com/virtual/2025/poster/32799", "summary": "Semantic correspondence made tremendous progress through the recent\nadvancements of large vision models (LVM). While these LVMs have been shown to\nreliably capture local semantics, the same can currently not be said for\ncapturing global geometric relationships between semantic object regions. This\nproblem leads to unreliable performance for semantic correspondence between\nimages with extreme view variation. In this work, we aim to leverage monocular\ndepth estimates to capture these geometric relationships for more robust and\ndata-efficient semantic correspondence. First, we introduce a simple but\neffective method to build 3D object-class representations from monocular depth\nestimates and LVM features using a sparsely annotated image correspondence\ndataset. Second, we formulate an alignment energy that can be minimized using\ngradient descent to obtain an alignment between the 3D object-class\nrepresentation and the object-class instance in the input RGB-image. Our method\nachieves state-of-the-art matching accuracy in multiple categories on the\nchallenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10\npoints on three categories and overall by 3.3 points from 85.6% to 88.9%.\nAdditional resources and code are available at https://dub.sh/semalign3d.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22577", "pdf": "https://arxiv.org/pdf/2503.22577", "abs": "https://arxiv.org/abs/2503.22577", "authors": ["Iñigo Pikabea", "Iñaki Lacunza", "Oriol Pareras", "Carlos Escolano", "Aitor Gonzalez-Agirre", "Javier Hernando", "Marta Villegas"], "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rapid advancements in Visual Language Models (VLMs) have transformed\nmultimodal understanding but are often constrained by generating English\nresponses regardless of the input language. This phenomenon has been termed as\nImage-induced Fidelity Loss (IFL) and stems from limited multimodal\nmultilingual training data. To address this, we propose a continuous\nmultilingual integration strategy that injects text-only multilingual data\nduring visual instruction tuning, preserving the language model's original\nmultilingual capabilities. Extensive evaluations demonstrate that our approach\nsignificantly improves linguistic fidelity across languages without degradation\nin visual performance. We also explore model merging, which improves language\nfidelity but comes at the cost of visual performance. In contrast, our core\nmethod achieves robust multilingual alignment without trade-offs, offering a\nscalable and effective path to mitigating IFL for global VLM adoption.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.21860", "pdf": "https://arxiv.org/pdf/2503.21860", "abs": "https://arxiv.org/abs/2503.21860", "authors": ["Kailin Li", "Puhao Li", "Tengyu Liu", "Yuyang Li", "Siyuan Huang"], "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Human hands play a central role in interacting, motivating increasing\nresearch in dexterous robotic manipulation. Data-driven embodied AI algorithms\ndemand precise, large-scale, human-like manipulation sequences, which are\nchallenging to obtain with conventional reinforcement learning or real-world\nteleoperation. To address this, we introduce ManipTrans, a novel two-stage\nmethod for efficiently transferring human bimanual skills to dexterous robotic\nhands in simulation. ManipTrans first pre-trains a generalist trajectory\nimitator to mimic hand motion, then fine-tunes a specific residual module under\ninteraction constraints, enabling efficient learning and accurate execution of\ncomplex bimanual tasks. Experiments show that ManipTrans surpasses\nstate-of-the-art methods in success rate, fidelity, and efficiency. Leveraging\nManipTrans, we transfer multiple hand-object datasets to robotic hands,\ncreating DexManipNet, a large-scale dataset featuring previously unexplored\ntasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K\nepisodes of robotic manipulation and is easily extensible, facilitating further\npolicy training for dexterous hands and enabling real-world deployments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22138", "pdf": "https://arxiv.org/pdf/2503.22138", "abs": "https://arxiv.org/abs/2503.22138", "authors": ["Changchang Sun", "Gaowen Liu", "Charles Fleming", "Yan Yan"], "title": "Enhancing Dance-to-Music Generation via Negative Conditioning Latent Diffusion Model", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "Conditional diffusion models have gained increasing attention since their\nimpressive results for cross-modal synthesis, where the strong alignment\nbetween conditioning input and generated output can be achieved by training a\ntime-conditioned U-Net augmented with cross-attention mechanism. In this paper,\nwe focus on the problem of generating music synchronized with rhythmic visual\ncues of the given dance video. Considering that bi-directional guidance is more\nbeneficial for training a diffusion model, we propose to enhance the quality of\ngenerated music and its synchronization with dance videos by adopting both\npositive rhythmic information and negative ones (PN-Diffusion) as conditions,\nwhere a dual diffusion and reverse processes is devised. Specifically, to train\na sequential multi-modal U-Net structure, PN-Diffusion consists of a noise\nprediction objective for positive conditioning and an additional noise\nprediction objective for negative conditioning. To accurately define and select\nboth positive and negative conditioning, we ingeniously utilize temporal\ncorrelations in dance videos, capturing positive and negative rhythmic cues by\nplaying them forward and backward, respectively. Through subjective and\nobjective evaluations of input-output correspondence in terms of dance-music\nbeat alignment and the quality of generated music, experimental results on the\nAIST++ and TikTok dance video datasets demonstrate that our model outperforms\nSOTA dance-to-music generation models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22176", "pdf": "https://arxiv.org/pdf/2503.22176", "abs": "https://arxiv.org/abs/2503.22176", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Kalyan Sivasailam", "Anandakumar D", "Keerthana R", "Mounigasri M", "Abilaasha G", "Kishore Prasath Venkatesh"], "title": "A Multi-Site Study on AI-Driven Pathology Detection and Osteoarthritis Grading from Knee X-Ray", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "15 pages, 2 figures", "summary": "Introduction: Bone health disorders like osteoarthritis and osteoporosis pose\nmajor global health challenges, often leading to delayed diagnoses due to\nlimited diagnostic tools. This study presents an AI-powered system that\nanalyzes knee X-rays to detect key pathologies, including joint space\nnarrowing, sclerosis, osteophytes, tibial spikes, alignment issues, and soft\ntissue anomalies. It also grades osteoarthritis severity, enabling timely,\npersonalized treatment.\n  Study Design: The research used 1.3 million knee X-rays from a multi-site\nIndian clinical trial across government, private, and SME hospitals. The\ndataset ensured diversity in demographics, imaging equipment, and clinical\nsettings. Rigorous annotation and preprocessing yielded high-quality training\ndatasets for pathology-specific models like ResNet15 for joint space narrowing\nand DenseNet for osteoarthritis grading.\n  Performance: The AI system achieved strong diagnostic accuracy across diverse\nimaging environments. Pathology-specific models excelled in precision, recall,\nand NPV, validated using Mean Squared Error (MSE), Intersection over Union\n(IoU), and Dice coefficient. Subgroup analyses across age, gender, and\nmanufacturer variations confirmed generalizability for real-world applications.\n  Conclusion: This scalable, cost-effective solution for bone health\ndiagnostics demonstrated robust performance in a multi-site trial. It holds\npromise for widespread adoption, especially in resource-limited healthcare\nsettings, transforming bone health management and enabling proactive patient\ncare.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22208", "pdf": "https://arxiv.org/pdf/2503.22208", "abs": "https://arxiv.org/abs/2503.22208", "authors": ["Yunming Liang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepSound-V1: Start to Think Step-by-Step in the Audio Generation from Videos", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "11 pages, 6 figures", "summary": "Currently, high-quality, synchronized audio is synthesized from video and\noptional text inputs using various multi-modal joint learning frameworks.\nHowever, the precise alignment between the visual and generated audio domains\nremains far from satisfactory. One key factor is the lack of sufficient\ntemporal and semantic alignment annotations in open-source video-audio and\ntext-audio benchmarks. Therefore, we propose a framework for audio generation\nfrom videos, leveraging the internal chain-of-thought (CoT) of a multi-modal\nlarge language model (MLLM) to enable step-by-step reasoning without requiring\nadditional annotations. Additionally, a corresponding multi-modal reasoning\ndataset is constructed to facilitate the learning of initial reasoning in audio\ngeneration. In the experiments, we demonstrate the effectiveness of the\nproposed framework in reducing misalignment (voice-over) in generated audio and\nachieving competitive performance compared to various state-of-the-art models.\nThe evaluation results show that the proposed method outperforms\nstate-of-the-art approaches across multiple metrics. Specifically, the F DP\naSST indicator is reduced by up to 10.07%, the F DP AN N s indicator by up to\n11.62%, and the F DV GG indicator by up to 38.61%. Furthermore, the IS\nindicator improves by up to 4.95%, the IB-score indicator increases by up to\n6.39%, and the DeSync indicator is reduced by up to 0.89%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-03-31.jsonl"}
{"id": "2503.22496", "pdf": "https://arxiv.org/pdf/2503.22496", "abs": "https://arxiv.org/abs/2503.22496", "authors": ["Luke Rowe", "Roger Girgis", "Anthony Gosselin", "Liam Paull", "Christopher Pal", "Felix Heide"], "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments", "categories": ["cs.RO", "cs.CV"], "comment": "CVPR 2025", "summary": "We introduce Scenario Dreamer, a fully data-driven generative simulator for\nautonomous vehicle planning that generates both the initial traffic scene -\ncomprising a lane graph and agent bounding boxes - and closed-loop agent\nbehaviours. Existing methods for generating driving simulation environments\nencode the initial traffic scene as a rasterized image and, as such, require\nparameter-heavy networks that perform unnecessary computation due to many empty\npixels in the rasterized scene. Moreover, we find that existing methods that\nemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamer\ninstead employs a novel vectorized latent diffusion model for initial scene\ngeneration that directly operates on the vectorized scene elements and an\nautoregressive Transformer for data-driven agent behaviour simulation. Scenario\nDreamer additionally supports scene extrapolation via diffusion inpainting,\nenabling the generation of unbounded simulation environments. Extensive\nexperiments show that Scenario Dreamer outperforms existing generative\nsimulators in realism and efficiency: the vectorized scene-generation base\nmodel achieves superior generation quality with around 2x fewer parameters, 6x\nlower generation latency, and 10x fewer GPU training hours compared to the\nstrongest baseline. We confirm its practical utility by showing that\nreinforcement learning planning agents are more challenged in Scenario Dreamer\nenvironments than traditional non-generative simulation environments,\nespecially on long and adversarial driving environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-03-31.jsonl"}
