{"id": "2503.23459", "pdf": "https://arxiv.org/pdf/2503.23459", "abs": "https://arxiv.org/abs/2503.23459", "authors": ["Chenglong Lu", "Shen Liang", "Xuewei Wang", "Wei Wang"], "title": "Reinforcement Learning-based Token Pruning in Vision Transformers: A Markov Game Approach", "categories": ["cs.CV"], "comment": "Accepted by IEEE International Conference on Multimedia & Expo (ICME)\n  2025", "summary": "Vision Transformers (ViTs) have computational costs scaling quadratically\nwith the number of tokens, calling for effective token pruning policies. Most\nexisting policies are handcrafted, lacking adaptivity to varying inputs.\nMoreover, they fail to consider the sequential nature of token pruning across\nmultiple layers. In this work, for the first time (as far as we know), we\nexploit Reinforcement Learning (RL) to data-adaptively learn a pruning policy.\nFormulating token pruning as a sequential decision-making problem, we model it\nas a Markov Game and utilize Multi-Agent Proximal Policy Optimization (MAPPO)\nwhere each agent makes an individualized pruning decision for a single token.\nWe also develop reward functions that enable simultaneous collaboration and\ncompetition of these agents to balance efficiency and accuracy. On the\nwell-known ImageNet-1k dataset, our method improves the inference speed by up\nto 44% while incurring only a negligible accuracy drop of 0.4%. The source code\nis available at https://github.com/daashuai/rl4evit.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22948", "pdf": "https://arxiv.org/pdf/2503.22948", "abs": "https://arxiv.org/abs/2503.22948", "authors": ["Tianyang Xu", "Xiaoze Liu", "Feijie Wu", "Xiaoqian Wang", "Jing Gao"], "title": "SUV: Scalable Large Language Model Copyright Compliance with Regularized Selective Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing by\nlearning from massive datasets, yet this rapid progress has also drawn legal\nscrutiny, as the ability to unintentionally generate copyrighted content has\nalready prompted several prominent lawsuits. In this work, we introduce SUV\n(Selective Unlearning for Verbatim data), a selective unlearning framework\ndesigned to prevent LLM from memorizing copyrighted content while preserving\nits overall utility. In detail, the proposed method constructs a dataset that\ncaptures instances of copyrighted infringement cases by the targeted LLM. With\nthe dataset, we unlearn the content from the LLM by means of Direct Preference\nOptimization (DPO), which replaces the verbatim copyrighted content with\nplausible and coherent alternatives. Since DPO may hinder the LLM's performance\nin other unrelated tasks, we integrate gradient projection and Fisher\ninformation regularization to mitigate the degradation. We validate our\napproach using a large-scale dataset of 500 famous books (predominantly\ncopyrighted works) and demonstrate that SUV significantly reduces verbatim\nmemorization with negligible impact on the performance on unrelated tasks.\nExtensive experiments on both our dataset and public benchmarks confirm the\nscalability and efficacy of our approach, offering a promising solution for\nmitigating copyright risks in real-world LLM applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23371", "pdf": "https://arxiv.org/pdf/2503.23371", "abs": "https://arxiv.org/abs/2503.23371", "authors": ["Jeonghyun Ko", "Gyeongyun Park", "Donghoon Lee", "Kyunam Lee"], "title": "FeRG-LLM : Feature Engineering by Reason Generation Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 Findings", "summary": "One of the key tasks in machine learning for tabular data is feature\nengineering. Although it is vital for improving the performance of models, it\ndemands considerable human expertise and deep domain knowledge, making it\nlabor-intensive endeavor. To address this issue, we propose a novel framework,\n\\textbf{FeRG-LLM} (\\textbf{Fe}ature engineering by \\textbf{R}eason\n\\textbf{G}eneration \\textbf{L}arge \\textbf{L}anguage \\textbf{M}odels), a large\nlanguage model designed to automatically perform feature engineering at an\n8-billion-parameter scale. We have constructed two-stage conversational\ndialogues that enable language models to analyze machine learning tasks and\ndiscovering new features, exhibiting their Chain-of-Thought (CoT) capabilities.\nWe use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct\nPreference Optimization (DPO) to receive feedback improving quality of new\nfeatures and the model's performance. Our experiments show that FeRG-LLM\nperforms comparably to or better than Llama 3.1 70B on most datasets, while\nusing fewer resources and achieving reduced inference time. It outperforms\nother studies in classification tasks and performs well in regression tasks.\nMoreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API\ncosts when generating features, it can be deployed locally, addressing security\nconcerns.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23427", "pdf": "https://arxiv.org/pdf/2503.23427", "abs": "https://arxiv.org/abs/2503.23427", "authors": ["Wenhan Liu", "Xinyu Ma", "Yutao Zhu", "Lixin Su", "Shuaiqiang Wang", "Dawei Yin", "Zhicheng Dou"], "title": "CoRanking: Collaborative Ranking with Small and Large Ranking Agents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated superior listwise ranking\nperformance. However, their superior performance often relies on large-scale\nparameters (\\eg, GPT-4) and a repetitive sliding window process, which\nintroduces significant efficiency challenges. In this paper, we propose\n\\textbf{CoRanking}, a novel collaborative ranking framework that combines small\nand large ranking models for efficient and effective ranking. CoRanking first\nemploys a small-size reranker to pre-rank all the candidate passages, bringing\nrelevant ones to the top part of the list (\\eg, top-20). Then, the LLM listwise\nreranker is applied to only rerank these top-ranked passages instead of the\nwhole list, substantially enhancing overall ranking efficiency. Although more\nefficient, previous studies have revealed that the LLM listwise reranker have\nsignificant positional biases on the order of input passages. Directly feed the\ntop-ranked passages from small reranker may result in the sub-optimal\nperformance of LLM listwise reranker. To alleviate this problem, we introduce a\npassage order adjuster trained via reinforcement learning, which reorders the\ntop passages from the small reranker to align with the LLM's preferences of\npassage order. Extensive experiments on three IR benchmarks demonstrate that\nCoRanking significantly improves efficiency (reducing ranking latency by about\n70\\%) while achieving even better effectiveness compared to using only the LLM\nlistwise reranker.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23777", "pdf": "https://arxiv.org/pdf/2503.23777", "abs": "https://arxiv.org/abs/2503.23777", "authors": ["Jiangnan Li", "Thuy-Trang Vu", "Christian Herold", "Amirhossein Tebbifakhr", "Shahram Khadivi", "Gholamreza Haffari"], "title": "CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Naive joint training of large language models (LLMs) for multilingual\npreference alignment can suffer from negative interference. This is a known\nissue in multilingual training, where conflicting objectives degrade overall\nperformance. However, the impact of this phenomenon in the context of\nmultilingual preference alignment remains largely underexplored. To address\nthis issue, we propose CONGRAD, a scalable and effective filtering method that\nselects high-quality preference samples with minimal gradient conflicts across\nlanguages. Our method leverages gradient surgery to retain samples aligned with\nan aggregated multilingual update direction. Additionally, we incorporate a\nsublinear gradient compression strategy that reduces memory overhead during\ngradient accumulation. We integrate CONGRAD into self-rewarding framework and\nevaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that\nCONGRAD consistently outperforms strong baselines in both seen and unseen\nlanguages, with minimal alignment tax.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23829", "pdf": "https://arxiv.org/pdf/2503.23829", "abs": "https://arxiv.org/abs/2503.23829", "authors": ["Yi Su", "Dian Yu", "Linfeng Song", "Juntao Li", "Haitao Mi", "Zhaopeng Tu", "Min Zhang", "Dong Yu"], "title": "Expanding RL with Verifiable Rewards Across Diverse Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) with verifiable rewards (RLVR) has shown\npromising results in mathematical reasoning and coding tasks where\nwell-structured reference answers are available. However, its applicability to\nbroader domains remains underexplored. In this work, we study the extension of\nRLVR to more diverse domains such as medicine, chemistry, psychology, and\neconomics. We observe high agreement in binary judgments across different large\nlanguage models (LLMs) when objective reference answers exist, which challenges\nthe necessity of large-scale annotation for training domain-specific reward\nmodels. To address the limitations of binary rewards when handling unstructured\nreference answers, we further incorporate model-based soft scoring into RLVR to\nimprove its flexibility. Our experiments show that a distilled generative\nreward model can serve as an effective cross-domain verifier, providing\nreliable reward signals for RL without requiring domain-specific annotations.\nBy fine-tuning a base 7B model using various RL algorithms against our reward\nmodel, we obtain policies that outperform state-of-the-art open-source aligned\nLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large\nmargin, across domains in free-form answer settings. This also strengthens\nRLVR's robustness and scalability, highlighting its potential for real-world\napplications with noisy or weak labels.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "agreement", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24290", "pdf": "https://arxiv.org/pdf/2503.24290", "abs": "https://arxiv.org/abs/2503.24290", "authors": ["Jingcheng Hu", "Yinmin Zhang", "Qi Han", "Daxin Jiang", "Xiangyu Zhang", "Heung-Yeung Shum"], "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce Open-Reasoner-Zero, the first open source implementation of\nlarge-scale reasoning-oriented RL training focusing on scalability, simplicity\nand accessibility. Through extensive experiments, we demonstrate that a\nminimalist approach, vanilla PPO with GAE ($\\lambda=1$, $\\gamma=1$) and\nstraightforward rule-based rewards, without any KL regularization, is\nsufficient to scale up both response length and benchmark performance, similar\nto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as\nDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on\nAIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating\nremarkable efficiency -- requiring only a tenth of the training steps, compared\nto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our\nsource code, parameter settings, training data, and model weights across\nvarious sizes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24121", "pdf": "https://arxiv.org/pdf/2503.24121", "abs": "https://arxiv.org/abs/2503.24121", "authors": ["Valentin Boussot", "Cédric Hémon", "Jean-Claude Nunes", "Jason Downling", "Simon Rouzé", "Caroline Lafond", "Anaïs Barateau", "Jean-Louis Dillenseger"], "title": "IMPACT: A Generic Semantic Loss for Multimodal Medical Image Registration", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI). This is a preprint version and has not been\n  peer-reviewed", "summary": "Image registration is fundamental in medical imaging, enabling precise\nalignment of anatomical structures for diagnosis, treatment planning,\nimage-guided treatment or longitudinal monitoring. This work introduces IMPACT\n(Image Metric with Pretrained model-Agnostic Comparison for Transmodality\nregistration), a generic semantic similarity metric designed for seamless\nintegration into diverse image registration frameworks (such as Elastix and\nVoxelmorph). It compares deep learning-based features extracted from medical\nimages without requiring task-specific training, ensuring broad applicability\nacross various modalities. By leveraging the features of the large-scale\npretrained TotalSegmentator models and the ability to integrate Segment\nAnything Model (SAM) and other large-scale segmentation networks, this approach\noffers significant advantages. It provides robust, scalable, and efficient\nsolutions for multimodal image registration. The IMPACT loss was evaluated on\nfive challenging registration tasks involving thoracic CT/CBCT, and pelvic\nMR/CT datasets. Quantitative metrics, such as Target Registration Error and\nDice Similarity Coefficient, demonstrated significant improvements in\nanatomical alignment compared to baseline methods. Qualitative analyses further\nconfirmed the increased robustness of the proposed metric in the face of noise,\nartifacts, and modality variations. IMPACT's versatility and efficiency make it\na valuable tool for advancing registration performance in clinical and research\napplications, addressing critical challenges in multimodal medical imaging.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24376", "pdf": "https://arxiv.org/pdf/2503.24376", "abs": "https://arxiv.org/abs/2503.24376", "authors": ["Yi Chen", "Yuying Ge", "Rui Wang", "Yixiao Ge", "Lu Qiu", "Ying Shan", "Xihui Liu"], "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1", "summary": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "chain of thought"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22828", "pdf": "https://arxiv.org/pdf/2503.22828", "abs": "https://arxiv.org/abs/2503.22828", "authors": ["Alexander Gurung", "Mirella Lapata"], "title": "Learning to Reason for Long-Form Story Generation", "categories": ["cs.CL"], "comment": null, "summary": "Generating high-quality stories spanning thousands of tokens requires\ncompetency across a variety of skills, from tracking plot and character arcs to\nkeeping a consistent and engaging style. Due to the difficulty of sourcing\nlabeled datasets and precise quality measurements, most work using large\nlanguage models (LLMs) for long-form story generation uses combinations of\nhand-designed prompting techniques to elicit author-like behavior. This is a\nmanual process that is highly dependent on the specific story-generation task.\nMotivated by the recent success of applying RL with Verifiable Rewards to\ndomains like math and coding, we propose a general story-generation task\n(Next-Chapter Prediction) and a reward formulation (Verified Rewards via\nCompletion Likelihood Improvement) that allows us to use an unlabeled book\ndataset as a learning signal for reasoning. We learn to reason over a story's\ncondensed information and generate a detailed plan for the next chapter. Our\nreasoning is evaluated via the chapters it helps a story-generator create, and\ncompared against non-trained and supervised finetuning (SFT) baselines.\nPairwise human judgments reveal the chapters our learned reasoning produces are\npreferred across almost all metrics, and the effect is more pronounced in Scifi\nand Fantasy genres.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22881", "pdf": "https://arxiv.org/pdf/2503.22881", "abs": "https://arxiv.org/abs/2503.22881", "authors": ["Lauren Shrack", "Timm Haucke", "Antoine Salaün", "Arjun Subramonian", "Sara Beery"], "title": "Pairwise Matching of Intermediate Representations for Fine-grained Explainability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The differences between images belonging to fine-grained categories are often\nsubtle and highly localized, and existing explainability techniques for deep\nlearning models are often too diffuse to provide useful and interpretable\nexplanations. We propose a new explainability method (PAIR-X) that leverages\nboth intermediate model activations and backpropagated relevance scores to\ngenerate fine-grained, highly-localized pairwise visual explanations. We use\nanimal and building re-identification (re-ID) as a primary case study of our\nmethod, and we demonstrate qualitatively improved results over a diverse set of\nexplainability baselines on 35 public re-ID datasets. In interviews, animal\nre-ID experts were in unanimous agreement that PAIR-X was an improvement over\nexisting baselines for deep model explainability, and suggested that its\nvisualizations would be directly applicable to their work. We also propose a\nnovel quantitative evaluation metric for our method, and demonstrate that\nPAIR-X visualizations appear more plausible for correct image matches than\nincorrect ones even when the model similarity score for the pairs is the same.\nBy improving interpretability, PAIR-X enables humans to better distinguish\ncorrect and incorrect matches. Our code is available at:\nhttps://github.com/pairx-explains/pairx", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22906", "pdf": "https://arxiv.org/pdf/2503.22906", "abs": "https://arxiv.org/abs/2503.22906", "authors": ["Heng Yu", "Juze Zhang", "Changan Chen", "Tiange Xiang", "Yusu Fang", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "SocialGen: Modeling Multi-Human Social Interaction with Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Human interactions in everyday life are inherently social, involving\nengagements with diverse individuals across various contexts. Modeling these\nsocial interactions is fundamental to a wide range of real-world applications.\nIn this paper, we introduce SocialGen, the first unified motion-language model\ncapable of modeling interaction behaviors among varying numbers of individuals,\nto address this crucial yet challenging problem. Unlike prior methods that are\nlimited to two-person interactions, we propose a novel social motion\nrepresentation that supports tokenizing the motions of an arbitrary number of\nindividuals and aligning them with the language space. This alignment enables\nthe model to leverage rich, pretrained linguistic knowledge to better\nunderstand and reason about human social behaviors. To tackle the challenges of\ndata scarcity, we curate a comprehensive multi-human interaction dataset,\nSocialX, enriched with textual annotations. Leveraging this dataset, we\nestablish the first comprehensive benchmark for multi-human interaction tasks.\nOur method achieves state-of-the-art performance across motion-language tasks,\nsetting a new standard for multi-human interaction modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23053", "pdf": "https://arxiv.org/pdf/2503.23053", "abs": "https://arxiv.org/abs/2503.23053", "authors": ["Hongjia Liu", "Jinlong Li"], "title": "A Training-free LLM Framework with Interaction between Contextually Related Subtasks in Solving Complex Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities in solving\ncomplex tasks. Recent work has explored decomposing such tasks into subtasks\nwith independent contexts. However, some contextually related subtasks may\nencounter information loss during execution, leading to redundant operations or\nexecution failures. To address this issue, we propose a training-free framework\nwith an interaction mechanism, which enables a subtask to query specific\ninformation or trigger certain actions in completed subtasks by sending\nrequests. To implement interaction, we introduce a subtask trajectory memory to\nenable resumption of completed subtasks upon receiving interaction requests.\nAdditionally, we propose a new action during execution, which generates a\nconcise and precise description of execution process and outcomes of a subtask,\nto assist subsequent subtasks in determining interaction targets and requests.\nWe evaluate our framework on interactive decision-making task WebShop and\nmulti-hop question answering HotpotQA, with GPT-3.5 and GPT-4, and comparison\nresults show that our framework outperforms the state-of-the-art training-free\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23091", "pdf": "https://arxiv.org/pdf/2503.23091", "abs": "https://arxiv.org/abs/2503.23091", "authors": ["Yige Chen", "Zelong Li", "Changbing Yang", "Cindy Zhang", "Amandisa Cady", "Ai Ka Lee", "Zejiao Zeng", "Haihua Pan", "Jungyeul Park"], "title": "Parsing Through Boundaries in Chinese Word Segmentation", "categories": ["cs.CL"], "comment": "Submitted to ACL2025 System Demonstration", "summary": "Chinese word segmentation is a foundational task in natural language\nprocessing (NLP), with far-reaching effects on syntactic analysis. Unlike\nalphabetic languages like English, Chinese lacks explicit word boundaries,\nmaking segmentation both necessary and inherently ambiguous. This study\nhighlights the intricate relationship between word segmentation and syntactic\nparsing, providing a clearer understanding of how different segmentation\nstrategies shape dependency structures in Chinese. Focusing on the Chinese GSD\ntreebank, we analyze multiple word boundary schemes, each reflecting distinct\nlinguistic and computational assumptions, and examine how they influence the\nresulting syntactic structures. To support detailed comparison, we introduce an\ninteractive web-based visualization tool that displays parsing outcomes across\nsegmentation methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23213", "pdf": "https://arxiv.org/pdf/2503.23213", "abs": "https://arxiv.org/abs/2503.23213", "authors": ["Diana Bolanos", "Mohammadmehdi Ataei", "Daniele Grandi", "Kosa Goucher-Lambert"], "title": "RECALL-MM: A Multimodal Dataset of Consumer Product Recalls for Risk Analysis using Computational Methods and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Product recalls provide valuable insights into potential risks and hazards\nwithin the engineering design process, yet their full potential remains\nunderutilized. In this study, we curate data from the United States Consumer\nProduct Safety Commission (CPSC) recalls database to develop a multimodal\ndataset, RECALL-MM, that informs data-driven risk assessment using historical\ninformation, and augment it using generative methods. Patterns in the dataset\nhighlight specific areas where improved safety measures could have significant\nimpact. We extend our analysis by demonstrating interactive clustering maps\nthat embed all recalls into a shared latent space based on recall descriptions\nand product names. Leveraging these data-driven tools, we explore three case\nstudies to demonstrate the dataset's utility in identifying product risks and\nguiding safer design decisions. The first two case studies illustrate how\ndesigners can visualize patterns across recalled products and situate new\nproduct ideas within the broader recall landscape to proactively anticipate\nhazards. In the third case study, we extend our approach by employing a large\nlanguage model (LLM) to predict potential hazards based solely on product\nimages. This demonstrates the model's ability to leverage visual context to\nidentify risk factors, revealing strong alignment with historical recall data\nacross many hazard categories. However, the analysis also highlights areas\nwhere hazard prediction remains challenging, underscoring the importance of\nrisk awareness throughout the design process. Collectively, this work aims to\nbridge the gap between historical recall data and future product safety,\npresenting a scalable, data-driven approach to safer engineering design.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23030", "pdf": "https://arxiv.org/pdf/2503.23030", "abs": "https://arxiv.org/abs/2503.23030", "authors": ["Huajie Jiang", "Zhengxian Li", "Xiaohan Yu", "Yongli Hu", "Baocai Yin", "Jian Yang", "Yuankai Qi"], "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Generalized zero-shot learning aims to recognize both seen and unseen classes\nwith the help of semantic information that is shared among different classes.\nIt inevitably requires consistent visual-semantic alignment. Existing\napproaches fine-tune the visual backbone by seen-class data to obtain\nsemantic-related visual features, which may cause overfitting on seen classes\nwith a limited number of training images. This paper proposes a novel visual\nand semantic prompt collaboration framework, which utilizes prompt tuning\ntechniques for efficient feature adaptation. Specifically, we design a visual\nprompt to integrate the visual information for discriminative feature learning\nand a semantic prompt to integrate the semantic formation for visualsemantic\nalignment. To achieve effective prompt information integration, we further\ndesign a weak prompt fusion mechanism for the shallow layers and a strong\nprompt fusion mechanism for the deep layers in the network. Through the\ncollaboration of visual and semantic prompts, we can obtain discriminative\nsemantic-related features for generalized zero-shot image recognition.\nExtensive experiments demonstrate that our framework consistently achieves\nfavorable performance in both conventional zero-shot learning and generalized\nzero-shot learning benchmarks compared to other state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23039", "pdf": "https://arxiv.org/pdf/2503.23039", "abs": "https://arxiv.org/abs/2503.23039", "authors": ["Zijun Ding", "Mingdie Xiong", "Congcong Zhu", "Jingrun Chen"], "title": "STSA: Spatial-Temporal Semantic Alignment for Visual Dubbing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Existing audio-driven visual dubbing methods have achieved great success.\nDespite this, we observe that the semantic ambiguity between spatial and\ntemporal domains significantly degrades the synthesis stability for the dynamic\nfaces. We argue that aligning the semantic features from spatial and temporal\ndomains is a promising approach to stabilizing facial motion. To achieve this,\nwe propose a Spatial-Temporal Semantic Alignment (STSA) method, which\nintroduces a dual-path alignment mechanism and a differentiable semantic\nrepresentation. The former leverages a Consistent Information Learning (CIL)\nmodule to maximize the mutual information at multiple scales, thereby reducing\nthe manifold differences between spatial and temporal domains. The latter\nutilizes probabilistic heatmap as ambiguity-tolerant guidance to avoid the\nabnormal dynamics of the synthesized faces caused by slight semantic jittering.\nExtensive experimental results demonstrate the superiority of the proposed\nSTSA, especially in terms of image quality and synthesis stability. Pre-trained\nweights and inference code are available at\nhttps://github.com/SCAILab-USTC/STSA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23305", "pdf": "https://arxiv.org/pdf/2503.23305", "abs": "https://arxiv.org/abs/2503.23305", "authors": ["Kenneth J. Sible", "David Chiang"], "title": "Using Source-Side Confidence Estimation for Reliable Translation into Unfamiliar Languages", "categories": ["cs.CL", "cs.LG"], "comment": "7 pages, 5 figures, 1 table. Submitted to ACL 2025 System\n  Demonstrations", "summary": "We present an interactive machine translation (MT) system designed for users\nwho are not proficient in the target language. It aims to improve\ntrustworthiness and explainability by identifying potentially mistranslated\nwords and allowing the user to intervene to correct mistranslations. However,\nconfidence estimation in machine translation has traditionally focused on the\ntarget side. Whereas the conventional approach to source-side confidence\nestimation would have been to project target word probabilities to the source\nside via word alignments, we propose a direct, alignment-free approach that\nmeasures how sensitive the target word probabilities are to changes in the\nsource embeddings. Experimental results show that our method outperforms\ntraditional alignment-based methods at detection of mistranslations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23306", "pdf": "https://arxiv.org/pdf/2503.23306", "abs": "https://arxiv.org/abs/2503.23306", "authors": ["Youxiang Zhu", "Ruochen Li", "Danqing Wang", "Daniel Haehn", "Xiaohui Liang"], "title": "Focus Directions Make Your Language Models Pay More Attention to Relevant Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Long-context large language models (LLMs) are prone to be distracted by\nirrelevant contexts. The reason for distraction remains poorly understood. In\nthis paper, we first identify the contextual heads, a special group of\nattention heads that control the overall attention of the LLM. Then, we\ndemonstrate that distraction arises when contextual heads fail to allocate\nsufficient attention to relevant contexts and can be mitigated by increasing\nattention to these contexts. We further identify focus directions, located at\nthe key and query activations of these heads, which enable them to allocate\nmore attention to relevant contexts without explicitly specifying which context\nis relevant. We comprehensively evaluate the effect of focus direction on\nvarious long-context tasks and find out focus directions could help to mitigate\nthe poor task alignment of the long-context LLMs. We believe our findings could\npromote further research on long-context LLM alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23105", "pdf": "https://arxiv.org/pdf/2503.23105", "abs": "https://arxiv.org/abs/2503.23105", "authors": ["Yifan Xu", "Vineet Kamat", "Carol Menassa"], "title": "Open-Vocabulary Semantic Segmentation with Uncertainty Alignment for Robotic Scene Understanding in Indoor Building Environments", "categories": ["cs.CV"], "comment": "32 pages, 7 figures", "summary": "The global rise in the number of people with physical disabilities, in part\ndue to improvements in post-trauma survivorship and longevity, has amplified\nthe demand for advanced assistive technologies to improve mobility and\nindependence. Autonomous assistive robots, such as smart wheelchairs, require\nrobust capabilities in spatial segmentation and semantic recognition to\nnavigate complex built environments effectively. Place segmentation involves\ndelineating spatial regions like rooms or functional areas, while semantic\nrecognition assigns semantic labels to these regions, enabling accurate\nlocalization to user-specific needs. Existing approaches often utilize deep\nlearning; however, these close-vocabulary detection systems struggle to\ninterpret intuitive and casual human instructions. Additionally, most existing\nmethods ignore the uncertainty of the scene recognition problem, leading to low\nsuccess rates, particularly in ambiguous and complex environments. To address\nthese challenges, we propose an open-vocabulary scene semantic segmentation and\ndetection pipeline leveraging Vision Language Models (VLMs) and Large Language\nModels (LLMs). Our approach follows a 'Segment Detect Select' framework for\nopen-vocabulary scene classification, enabling adaptive and intuitive\nnavigation for assistive robots in built environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23383", "pdf": "https://arxiv.org/pdf/2503.23383", "abs": "https://arxiv.org/abs/2503.23383", "authors": ["Xuefeng Li", "Haoyang Zou", "Pengfei Liu"], "title": "ToRL: Scaling Tool-Integrated RL", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for\ntraining large language models (LLMs) to autonomously use computational tools\nvia reinforcement learning. Unlike supervised fine-tuning, ToRL allows models\nto explore and discover optimal strategies for tool use. Experiments with\nQwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\%\naccuracy on AIME~24, surpassing reinforcement learning without tool integration\nby 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%.\nFurther analysis reveals emergent behaviors such as strategic tool invocation,\nself-regulation of ineffective code, and dynamic adaptation between\ncomputational and analytical reasoning, all arising purely through\nreward-driven learning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23131", "pdf": "https://arxiv.org/pdf/2503.23131", "abs": "https://arxiv.org/abs/2503.23131", "authors": ["Alexander Vogel", "Omar Moured", "Yufan Chen", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "RefChartQA: Grounding Visual Answer on Chart Images through Instruction Tuning", "categories": ["cs.CV"], "comment": "All models and code will be publicly available at\n  https://github.com/moured/RefChartQA", "summary": "Recently, Vision Language Models (VLMs) have increasingly emphasized document\nvisual grounding to achieve better human-computer interaction, accessibility,\nand detailed understanding. However, its application to visualizations such as\ncharts remains under-explored due to the inherent complexity of interleaved\nvisual-numerical relationships in chart images. Existing chart understanding\nmethods primarily focus on answering questions without explicitly identifying\nthe visual elements that support their predictions. To bridge this gap, we\nintroduce RefChartQA, a novel benchmark that integrates Chart Question\nAnswering (ChartQA) with visual grounding, enabling models to refer elements at\nmultiple granularities within chart images. Furthermore, we conduct a\ncomprehensive evaluation by instruction-tuning 5 state-of-the-art VLMs across\ndifferent categories. Our experiments demonstrate that incorporating spatial\nawareness via grounding improves response accuracy by over 15%, reducing\nhallucinations, and improving model reliability. Additionally, we identify key\nfactors influencing text-spatial alignment, such as architectural improvements\nin TinyChart, which leverages a token-merging module for enhanced feature\nfusion. Our dataset is open-sourced for community development and further\nadvancements. All models and code will be publicly available at\nhttps://github.com/moured/RefChartQA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23214", "pdf": "https://arxiv.org/pdf/2503.23214", "abs": "https://arxiv.org/abs/2503.23214", "authors": ["Vincent Gbouna Zakka", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Action Recognition in Real-World Ambient Assisted Living Environment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing ageing population and their preference to maintain independence\nby living in their own homes require proactive strategies to ensure safety and\nsupport. Ambient Assisted Living (AAL) technologies have emerged to facilitate\nageing in place by offering continuous monitoring and assistance within the\nhome. Within AAL technologies, action recognition plays a crucial role in\ninterpreting human activities and detecting incidents like falls, mobility\ndecline, or unusual behaviours that may signal worsening health conditions.\nHowever, action recognition in practical AAL applications presents challenges,\nincluding occlusions, noisy data, and the need for real-time performance. While\nadvancements have been made in accuracy, robustness to noise, and computation\nefficiency, achieving a balance among them all remains a challenge. To address\nthis challenge, this paper introduces the Robust and Efficient Temporal\nConvolution network (RE-TCN), which comprises three main elements: Adaptive\nTemporal Weighting (ATW), Depthwise Separable Convolutions (DSC), and data\naugmentation techniques. These elements aim to enhance the model's accuracy,\nrobustness against noise and occlusion, and computational efficiency within\nreal-world AAL contexts. RE-TCN outperforms existing models in terms of\naccuracy, noise and occlusion robustness, and has been validated on four\nbenchmark datasets: NTU RGB+D 60, Northwestern-UCLA, SHREC'17, and DHG-14/28.\nThe code is publicly available at: https://github.com/Gbouna/RE-TCN", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23220", "pdf": "https://arxiv.org/pdf/2503.23220", "abs": "https://arxiv.org/abs/2503.23220", "authors": ["Marc-Antoine Lavoie", "Anas Mahmoud", "Steven L. Waslander"], "title": "Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object Detection", "categories": ["cs.CV"], "comment": "16 pages (8 main), 5 figures, accepted at CVPR 2025", "summary": "The current state-of-the-art methods in domain adaptive object detection\n(DAOD) use Mean Teacher self-labelling, where a teacher model, directly derived\nas an exponential moving average of the student model, is used to generate\nlabels on the target domain which are then used to improve both models in a\npositive loop. This couples learning and generating labels on the target\ndomain, and other recent works also leverage the generated labels to add\nadditional domain alignment losses. We believe this coupling is brittle and\nexcessively constrained: there is no guarantee that a student trained only on\nsource data can generate accurate target domain labels and initiate the\npositive feedback loop, and much better target domain labels can likely be\ngenerated by using a large pretrained network that has been exposed to much\nmore data. Vision foundational models are exactly such models, and they have\nshown impressive task generalization capabilities even when frozen. We want to\nleverage these models for DAOD and introduce DINO Teacher, which consists of\ntwo components. First, we train a new labeller on source data only using a\nlarge frozen DINOv2 backbone and show it generates more accurate labels than\nMean Teacher. Next, we align the student's source and target image patch\nfeatures with those from a DINO encoder, driving source and target\nrepresentations closer to the generalizable DINO representation. We obtain\nstate-of-the-art performance on multiple DAOD datasets. Code available at\nhttps://github.com/TRAILab/DINO_Teacher", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23234", "pdf": "https://arxiv.org/pdf/2503.23234", "abs": "https://arxiv.org/abs/2503.23234", "authors": ["Alessio Borgi", "Luca Maiano", "Irene Amerini"], "title": "Z-SASLM: Zero-Shot Style-Aligned SLI Blending Latent Manipulation", "categories": ["cs.CV"], "comment": "Accepted to the CVPR 2025 Workshop AI for Creative Visual Content\n  Generation Editing and Understanding", "summary": "We introduce Z-SASLM, a Zero-Shot Style-Aligned SLI (Spherical Linear\nInterpolation) Blending Latent Manipulation pipeline that overcomes the\nlimitations of current multi-style blending methods. Conventional approaches\nrely on linear blending, assuming a flat latent space leading to suboptimal\nresults when integrating multiple reference styles. In contrast, our framework\nleverages the non-linear geometry of the latent space by using SLI Blending to\ncombine weighted style representations. By interpolating along the geodesic on\nthe hypersphere, Z-SASLM preserves the intrinsic structure of the latent space,\nensuring high-fidelity and coherent blending of diverse styles - all without\nthe need for fine-tuning. We further propose a new metric, Weighted Multi-Style\nDINO ViT-B/8, designed to quantitatively evaluate the consistency of the\nblended styles. While our primary focus is on the theoretical and practical\nadvantages of SLI Blending for style manipulation, we also demonstrate its\neffectiveness in a multi-modal content fusion setting through comprehensive\nexperimental studies. Experimental results show that Z-SASLM achieves enhanced\nand robust style alignment. The implementation code can be found at:\nhttps://github.com/alessioborgi/Z-SASLM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23307", "pdf": "https://arxiv.org/pdf/2503.23307", "abs": "https://arxiv.org/abs/2503.23307", "authors": ["Cong Wei", "Bo Sun", "Haoyu Ma", "Ji Hou", "Felix Juefei-Xu", "Zecheng He", "Xiaoliang Dai", "Luxin Zhang", "Kunpeng Li", "Tingbo Hou", "Animesh Sinha", "Peter Vajda", "Wenhu Chen"], "title": "MoCha: Towards Movie-Grade Talking Character Synthesis", "categories": ["cs.CV"], "comment": "https://congwei1230.github.io/MoCha/", "summary": "Recent advancements in video generation have achieved impressive motion\nrealism, yet they often overlook character-driven storytelling, a crucial task\nfor automated film, animation generation. We introduce Talking Characters, a\nmore realistic task to generate talking character animations directly from\nspeech and text. Unlike talking head, Talking Characters aims at generating the\nfull portrait of one or more characters beyond the facial region. In this\npaper, we propose MoCha, the first of its kind to generate talking characters.\nTo ensure precise synchronization between video and speech, we propose a\nspeech-video window attention mechanism that effectively aligns speech and\nvideo tokens. To address the scarcity of large-scale speech-labeled video\ndatasets, we introduce a joint training strategy that leverages both\nspeech-labeled and text-labeled video data, significantly improving\ngeneralization across diverse character actions. We also design structured\nprompt templates with character tags, enabling, for the first time,\nmulti-character conversation with turn-based dialogue-allowing AI-generated\ncharacters to engage in context-aware conversations with cinematic coherence.\nExtensive qualitative and quantitative evaluations, including human preference\nstudies and benchmark comparisons, demonstrate that MoCha sets a new standard\nfor AI-generated cinematic storytelling, achieving superior realism,\nexpressiveness, controllability and generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23330", "pdf": "https://arxiv.org/pdf/2503.23330", "abs": "https://arxiv.org/abs/2503.23330", "authors": ["Hongxiang Jiang", "Jihao Yin", "Qixiong Wang", "Jiaqi Feng", "Guo Chen"], "title": "EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nimpressive results in various visual tasks. However, in remote sensing (RS),\nhigh resolution and small proportion of objects pose challenges to existing\nMLLMs, which struggle with object-centric tasks, particularly in precise\nlocalization and fine-grained attribute description for each object. These RS\nMLLMs have not yet surpassed classical visual perception models, as they only\nprovide coarse image understanding, leading to limited gains in real-world\nscenarios. To address this gap, we establish EagleVision, an MLLM tailored for\nremote sensing that excels in object detection and attribute comprehension.\nEquipped with the Attribute Disentangle module, EagleVision learns\ndisentanglement vision tokens to express distinct attributes. To support\nobject-level visual-language alignment, we construct EVAttrs-95K, the first\nlarge-scale object attribute understanding dataset in RS for instruction\ntuning, along with a novel evaluation benchmark, EVBench. EagleVision achieves\nstate-of-the-art performance on both fine-grained object detection and object\nattribute understanding tasks, highlighting the mutual promotion between\ndetection and understanding capabilities in MLLMs. The code, model, data, and\ndemo will be available at https://github.com/XiangTodayEatsWhat/EagleVision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23331", "pdf": "https://arxiv.org/pdf/2503.23331", "abs": "https://arxiv.org/abs/2503.23331", "authors": ["Hongwei Zheng", "Han Li", "Wenrui Dai", "Ziyang Zheng", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR2025", "summary": "Existing 2D-to-3D human pose estimation (HPE) methods struggle with the\nocclusion issue by enriching information like temporal and visual cues in the\nlifting stage. In this paper, we argue that these methods ignore the limitation\nof the sparse skeleton 2D input representation, which fundamentally restricts\nthe 2D-to-3D lifting and worsens the occlusion issue. To address these, we\npropose a novel two-stage generative densification method, named Hierarchical\nPose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense\nposes from the original sparse 2D pose. Specifically, we first develop a\nmulti-scale skeleton tokenization module to quantize the highly dense 2D pose\ninto hierarchical tokens and propose a Skeleton-aware Alignment to strengthen\ntoken connections. We then develop a Hierarchical AutoRegressive Modeling\nscheme for hierarchical 2D pose generation. With generated hierarchical poses\nas inputs for 2D-to-3D lifting, the proposed method shows strong robustness in\noccluded scenarios and achieves state-of-the-art performance on the\nsingle-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame\nmethods while reducing parameter and computational complexity and can also\ncomplement them to further enhance performance and robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24190", "pdf": "https://arxiv.org/pdf/2503.24190", "abs": "https://arxiv.org/abs/2503.24190", "authors": ["Xiaomeng Ma", "Qihui Xu"], "title": "Implicit In-Context Learning: Evidence from Artificial Language Experiments", "categories": ["cs.CL"], "comment": null, "summary": "Humans acquire language through implicit learning, absorbing complex patterns\nwithout explicit awareness. While LLMs demonstrate impressive linguistic\ncapabilities, it remains unclear whether they exhibit human-like pattern\nrecognition during in-context learning at inferencing level. We adapted three\nclassic artificial language learning experiments spanning morphology,\nmorphosyntax, and syntax to systematically evaluate implicit learning at\ninferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.\nOur results reveal linguistic domain-specific alignment between models and\nhuman behaviors, o3-mini aligns better in morphology while both models align in\nsyntax.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23377", "pdf": "https://arxiv.org/pdf/2503.23377", "abs": "https://arxiv.org/abs/2503.23377", "authors": ["Kai Liu", "Wei Li", "Lai Chen", "Shengqiong Wu", "Yanhao Zheng", "Jiayi Ji", "Fan Zhou", "Rongxin Jiang", "Jiebo Luo", "Hao Fei", "Tat-Seng Chua"], "title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Work in progress. Homepage: https://javisdit.github.io/", "summary": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion\nTransformer designed for synchronized audio-video generation (JAVG). Built upon\nthe powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to\ngenerate high-quality audio and video content simultaneously from open-ended\nuser prompts. To ensure optimal synchronization, we introduce a fine-grained\nspatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal\nSynchronized Prior (HiST-Sypo) Estimator. This module extracts both global and\nfine-grained spatio-temporal priors, guiding the synchronization between the\nvisual and auditory components. Furthermore, we propose a new benchmark,\nJavisBench, consisting of 10,140 high-quality text-captioned sounding videos\nspanning diverse scenes and complex real-world scenarios. Further, we\nspecifically devise a robust metric for evaluating the synchronization between\ngenerated audio-video pairs in real-world complex content. Experimental results\ndemonstrate that JavisDiT significantly outperforms existing methods by\nensuring both high-quality generation and precise synchronization, setting a\nnew standard for JAVG tasks. Our code, model, and dataset will be made publicly\navailable at https://javisdit.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23407", "pdf": "https://arxiv.org/pdf/2503.23407", "abs": "https://arxiv.org/abs/2503.23407", "authors": ["Wei Zeng", "Xuebin Chang", "Jianghao Su", "Xiang Gu", "Jian Sun", "Zongben Xu"], "title": "GMapLatent: Geometric Mapping in Latent Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain generative models based on encoder-decoder AI architectures have\nattracted much attention in generating realistic images, where domain alignment\nis crucial for generation accuracy. Domain alignment methods usually deal\ndirectly with the initial distribution; however, mismatched or mixed clusters\ncan lead to mode collapse and mixture problems in the decoder, compromising\nmodel generalization capabilities. In this work, we innovate a cross-domain\nalignment and generation model that introduces a canonical latent space\nrepresentation based on geometric mapping to align the cross-domain latent\nspaces in a rigorous and precise manner, thus avoiding mode collapse and\nmixture in the encoder-decoder generation architectures. We name this model\nGMapLatent. The core of the method is to seamlessly align latent spaces with\nstrict cluster correspondence constraints using the canonical parameterizations\nof cluster-decorated latent spaces. We first (1) transform the latent space to\na canonical parameter domain by composing barycenter translation, optimal\ntransport merging and constrained harmonic mapping, and then (2) compute\ngeometric registration with cluster constraints over the canonical parameter\ndomains. This process realizes a bijective (one-to-one and onto) mapping\nbetween newly transformed latent spaces and generates a precise alignment of\ncluster pairs. Cross-domain generation is then achieved through the aligned\nlatent spaces embedded in the encoder-decoder pipeline. Experiments on\ngray-scale and color images validate the efficiency, efficacy and applicability\nof GMapLatent, and demonstrate that the proposed model has superior performance\nover existing models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24307", "pdf": "https://arxiv.org/pdf/2503.24307", "abs": "https://arxiv.org/abs/2503.24307", "authors": ["Arshia Kermani", "Veronica Perez-Rosas", "Vangelis Metsis"], "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23451", "pdf": "https://arxiv.org/pdf/2503.23451", "abs": "https://arxiv.org/abs/2503.23451", "authors": ["Aimira Baitieva", "Yacine Bouaouni", "Alexandre Briot", "Dick Ameln", "Souhaiel Khalfaoui", "Samet Akcay"], "title": "Beyond Academic Benchmarks: Critical Analysis and Best Practices for Visual Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is essential for automating visual inspection in\nmanufacturing. This field of computer vision is rapidly evolving, with\nincreasing attention towards real-world applications. Meanwhile, popular\ndatasets are typically produced in controlled lab environments with\nartificially created defects, unable to capture the diversity of real\nproduction conditions. New methods often fail in production settings, showing\nsignificant performance degradation or requiring impractical computational\nresources. This disconnect between academic results and industrial viability\nthreatens to misdirect visual anomaly detection research. This paper makes\nthree key contributions: (1) we demonstrate the importance of real-world\ndatasets and establish benchmarks using actual production data, (2) we provide\na fair comparison of existing SOTA methods across diverse tasks by utilizing\nmetrics that are valuable for practical applications, and (3) we present a\ncomprehensive analysis of recent advancements in this field by discussing\nimportant challenges and new perspectives for bridging the academia-industry\ngap. The code is publicly available at\nhttps://github.com/abc-125/viad-benchmark", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23452", "pdf": "https://arxiv.org/pdf/2503.23452", "abs": "https://arxiv.org/abs/2503.23452", "authors": ["Yuhang Yang", "Ke Fan", "Shangkun Sun", "Hongxiang Li", "Ailing Zeng", "FeiLin Han", "Wei Zhai", "Wei Liu", "Yang Cao", "Zheng-Jun Zha"], "title": "VideoGen-Eval: Agent-based System for Video Generation Evaluation", "categories": ["cs.CV"], "comment": "project:https://github.com/AILab-CVC/VideoGen-Eval", "summary": "The rapid advancement of video generation has rendered existing evaluation\nsystems inadequate for assessing state-of-the-art models, primarily due to\nsimple prompts that cannot showcase the model's capabilities, fixed evaluation\noperators struggling with Out-of-Distribution (OOD) cases, and misalignment\nbetween computed metrics and human preferences. To bridge the gap, we propose\nVideoGen-Eval, an agent evaluation system that integrates LLM-based content\nstructuring, MLLM-based content judgment, and patch tools designed for\ntemporal-dense dimensions, to achieve a dynamic, flexible, and expandable video\ngeneration evaluation. Additionally, we introduce a video generation benchmark\nto evaluate existing cutting-edge models and verify the effectiveness of our\nevaluation system. It comprises 700 structured, content-rich prompts (both T2V\nand I2V) and over 12,000 videos generated by 20+ models, among them, 8\ncutting-edge models are selected as quantitative evaluation for the agent and\nhuman. Extensive experiments validate that our proposed agent-based evaluation\nsystem demonstrates strong alignment with human preferences and reliably\ncompletes the evaluation, as well as the diversity and richness of the\nbenchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23456", "pdf": "https://arxiv.org/pdf/2503.23456", "abs": "https://arxiv.org/abs/2503.23456", "authors": ["Maofu Liu", "Xin Jiang", "Xiaokang Zhang"], "title": "CADFormer: Fine-Grained Cross-modal Alignment and Decoding Transformer for Referring Remote Sensing Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring Remote Sensing Image Segmentation (RRSIS) is a challenging task,\naiming to segment specific target objects in remote sensing (RS) images based\non a given language expression. Existing RRSIS methods typically employ\ncoarse-grained unidirectional alignment approaches to obtain multimodal\nfeatures, and they often overlook the critical role of language features as\ncontextual information during the decoding process. Consequently, these methods\nexhibit weak object-level correspondence between visual and language features,\nleading to incomplete or erroneous predicted masks, especially when handling\ncomplex expressions and intricate RS image scenes. To address these challenges,\nwe propose a fine-grained cross-modal alignment and decoding Transformer,\nCADFormer, for RRSIS. Specifically, we design a semantic mutual guidance\nalignment module (SMGAM) to achieve both vision-to-language and\nlanguage-to-vision alignment, enabling comprehensive integration of visual and\ntextual features for fine-grained cross-modal alignment. Furthermore, a\ntextual-enhanced cross-modal decoder (TCMD) is introduced to incorporate\nlanguage features during decoding, using refined textual information as context\nto enhance the relationship between cross-modal features. To thoroughly\nevaluate the performance of CADFormer, especially for inconspicuous targets in\ncomplex scenes, we constructed a new RRSIS dataset, called RRSIS-HR, which\nincludes larger high-resolution RS image patches and semantically richer\nlanguage expressions. Extensive experiments on the RRSIS-HR dataset and the\npopular RRSIS-D dataset demonstrate the effectiveness and superiority of\nCADFormer. Datasets and source codes will be available at\nhttps://github.com/zxk688.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22732", "pdf": "https://arxiv.org/pdf/2503.22732", "abs": "https://arxiv.org/abs/2503.22732", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Merouane Debbah"], "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "41 pages", "summary": "Recent generative reasoning breakthroughs have transformed how large language\nmodels (LLMs) tackle complex problems by dynamically retrieving and refining\ninformation while generating coherent, multi-step thought processes. Techniques\nsuch as inference-time scaling, reinforcement learning, supervised fine-tuning,\nand distillation have been successfully applied to models like DeepSeek-R1,\nOpenAI's o1 & o3, GPT-4o, Qwen-32B, and various Llama variants, resulting in\nenhanced reasoning capabilities. In this paper, we provide a comprehensive\nanalysis of the top 27 LLM models released between 2023 and 2025 (including\nmodels such as Mistral AI Small 3 24B, DeepSeek-R1, Search-o1, QwQ-32B, and\nphi-4). Then, we present an extensive overview of training methodologies that\nspans general training approaches, mixture-of-experts (MoE) and architectural\ninnovations, retrieval-augmented generation (RAG), chain-of-thought and\nself-improvement techniques, as well as test-time compute scaling,\ndistillation, and reinforcement learning (RL) methods. Finally, we discuss the\nkey challenges in advancing LLM capabilities, including improving multi-step\nreasoning without human supervision, overcoming limitations in chained tasks,\nbalancing structured prompts with flexibility, and enhancing long-context\nretrieval and external tool integration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "compute scaling", "test-time compute", "o1"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23461", "pdf": "https://arxiv.org/pdf/2503.23461", "abs": "https://arxiv.org/abs/2503.23461", "authors": ["Nikai Du", "Zhennan Chen", "Zhizhou Chen", "Shan Gao", "Xi Chen", "Zhengkai Jiang", "Jian Yang", "Ying Tai"], "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23463", "pdf": "https://arxiv.org/pdf/2503.23463", "abs": "https://arxiv.org/abs/2503.23463", "authors": ["Xingcheng Zhou", "Xuyuan Han", "Feng Yang", "Yunpu Ma", "Alois C. Knoll"], "title": "OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model", "categories": ["cs.CV"], "comment": null, "summary": "We present OpenDriveVLA, a Vision-Language Action (VLA) model designed for\nend-to-end autonomous driving. OpenDriveVLA builds upon open-source pre-trained\nlarge Vision-Language Models (VLMs) to generate reliable driving actions,\nconditioned on 3D environmental perception, ego vehicle states, and driver\ncommands. To bridge the modality gap between driving visual representations and\nlanguage embeddings, we propose a hierarchical vision-language alignment\nprocess, projecting both 2D and 3D structured visual tokens into a unified\nsemantic space. Besides, OpenDriveVLA models the dynamic relationships between\nthe ego vehicle, surrounding agents, and static road elements through an\nautoregressive agent-env-ego interaction process, ensuring both spatially and\nbehaviorally informed trajectory planning. Extensive experiments on the\nnuScenes dataset demonstrate that OpenDriveVLA achieves state-of-the-art\nresults across open-loop trajectory planning and driving-related\nquestion-answering tasks. Qualitative analyses further illustrate\nOpenDriveVLA's superior capability to follow high-level driving commands and\nrobustly generate trajectories under challenging scenarios, highlighting its\npotential for next-generation end-to-end autonomous driving. We will release\nour code to facilitate further research in this domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23495", "pdf": "https://arxiv.org/pdf/2503.23495", "abs": "https://arxiv.org/abs/2503.23495", "authors": ["Ashim Dahal", "Saydul Akbar Murad", "Nick Rahimi"], "title": "Embedding Shift Dissection on CLIP: Effects of Augmentations on VLM's Representation Learning", "categories": ["cs.CV"], "comment": "accepted at MIV at CVPR 2025", "summary": "Understanding the representation shift on Vision Language Models like CLIP\nunder different augmentations provides valuable insights on Mechanistic\nInterpretability. In this study, we show the shift on CLIP's embeddings on 9\ncommon augmentation techniques: noise, blur, color jitter, scale and rotate,\nflip, elastic and perspective transforms, random brightness and contrast, and\ncoarse dropout of pixel blocks. We scrutinize the embedding shifts under\nsimilarity on attention map, patch, edge, detail preservation, cosine\nsimilarity, L2 distance, pairwise distance and dendrogram clusters and provide\nqualitative analysis on sample images. Our findings suggest certain\naugmentations like noise, perspective transform and shift scaling have higher\ndegree of drastic impact on embedding shift. This study provides a concrete\nfoundation for future work on VLM's robustness for mechanical interpretation\nand adversarial data defense.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23508", "pdf": "https://arxiv.org/pdf/2503.23508", "abs": "https://arxiv.org/abs/2503.23508", "authors": ["Yuming Chen", "Jiangyan Feng", "Haodong Zhang", "Lijun Gong", "Feng Zhu", "Rui Zhao", "Qibin Hou", "Ming-Ming Cheng", "Yibing Song"], "title": "Re-Aligning Language to Visual Objects with an Agentic Workflow", "categories": ["cs.CV"], "comment": "33 pages, 20 figures, 17 tables, ICLR 2025", "summary": "Language-based object detection (LOD) aims to align visual objects with\nlanguage expressions. A large amount of paired data is utilized to improve LOD\nmodel generalizations. During the training process, recent studies leverage\nvision-language models (VLMs) to automatically generate human-like expressions\nfor visual objects, facilitating training data scaling up. In this process, we\nobserve that VLM hallucinations bring inaccurate object descriptions (e.g.,\nobject name, color, and shape) to deteriorate VL alignment quality. To reduce\nVLM hallucinations, we propose an agentic workflow controlled by an LLM to\nre-align language to visual objects via adaptively adjusting image and text\nprompts. We name this workflow Real-LOD, which includes planning, tool use, and\nreflection steps. Given an image with detected objects and VLM raw language\nexpressions, Real-LOD reasons its state automatically and arranges action based\non our neural symbolic designs (i.e., planning). The action will adaptively\nadjust the image and text prompts and send them to VLMs for object\nre-description (i.e., tool use). Then, we use another LLM to analyze these\nrefined expressions for feedback (i.e., reflection). These steps are conducted\nin a cyclic form to gradually improve language descriptions for re-aligning to\nvisual objects. We construct a dataset that contains a tiny amount of 0.18M\nimages with re-aligned language expression and train a prevalent LOD model to\nsurpass existing LOD methods by around 50% on the standard benchmarks. Our\nReal-LOD workflow, with automatic VL refinement, reveals a potential to\npreserve data quality along with scaling up data quantity, which further\nimproves LOD performance from a data-alignment perspective.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23534", "pdf": "https://arxiv.org/pdf/2503.23534", "abs": "https://arxiv.org/abs/2503.23534", "authors": ["Rafi Ibn Sultan", "Hui Zhu", "Chengyin Li", "Dongxiao Zhu"], "title": "BiPVL-Seg: Bidirectional Progressive Vision-Language Fusion with Global-Local Alignment for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation typically relies solely on visual data,\noverlooking the rich textual information clinicians use for diagnosis.\nVision-language models attempt to bridge this gap, but existing approaches\noften process visual and textual features independently, resulting in weak\ncross-modal alignment. Simple fusion techniques fail due to the inherent\ndifferences between spatial visual features and sequential text embeddings.\nAdditionally, medical terminology deviates from general language, limiting the\neffectiveness of off-the-shelf text encoders and further hindering\nvision-language alignment. We propose BiPVL-Seg, an end-to-end framework that\nintegrates vision-language fusion and embedding alignment through architectural\nand training innovations, where both components reinforce each other to enhance\nmedical image segmentation. BiPVL-Seg introduces bidirectional progressive\nfusion in the architecture, which facilitates stage-wise information exchange\nbetween vision and text encoders. Additionally, it incorporates global-local\ncontrastive alignment, a training objective that enhances the text encoder's\ncomprehension by aligning text and vision embeddings at both class and concept\nlevels. Extensive experiments on diverse medical imaging benchmarks across CT\nand MR modalities demonstrate BiPVL-Seg's superior performance when compared\nwith state-of-the-art methods in complex multi-class segmentation. Source code\nis available in this GitHub repository.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23538", "pdf": "https://arxiv.org/pdf/2503.23538", "abs": "https://arxiv.org/abs/2503.23538", "authors": ["Jiyeon Han", "Dahee Kwon", "Gayoung Lee", "Junho Kim", "Jaesik Choi"], "title": "Enhancing Creative Generation on Stable Diffusion-based Models", "categories": ["cs.CV"], "comment": "CVPR 2025 accepted paper", "summary": "Recent text-to-image generative models, particularly Stable Diffusion and its\ndistilled variants, have achieved impressive fidelity and strong text-image\nalignment. However, their creative capability remains constrained, as including\n`creative' in prompts seldom yields the desired results. This paper introduces\nC3 (Creative Concept Catalyst), a training-free approach designed to enhance\ncreativity in Stable Diffusion-based models. C3 selectively amplifies features\nduring the denoising process to foster more creative outputs. We offer\npractical guidelines for choosing amplification factors based on two main\naspects of creativity. C3 is the first study to enhance creativity in diffusion\nmodels without extensive computational costs. We demonstrate its effectiveness\nacross various Stable Diffusion-based models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23239", "pdf": "https://arxiv.org/pdf/2503.23239", "abs": "https://arxiv.org/abs/2503.23239", "authors": ["Reza Esfandiarpoor", "George Zerveas", "Ruochen Zhang", "Macton Mgonzo", "Carsten Eickhoff", "Stephen H. Bach"], "title": "Beyond Contrastive Learning: Synthetic Data Enables List-wise Training with Multiple Levels of Relevance", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": "Code: https://github.com/BatsResearch/sycl", "summary": "Recent advancements in large language models (LLMs) have allowed the\naugmentation of information retrieval (IR) pipelines with synthetic data in\nvarious ways. Yet, the main training paradigm remains: contrastive learning\nwith binary relevance labels and the InfoNCE loss, where one positive document\nis compared against one or more negatives. This objective treats all documents\nthat are not explicitly annotated as relevant on an equally negative footing,\nregardless of their actual degree of relevance, thus (a) missing subtle nuances\nthat are useful for ranking and (b) being susceptible to annotation noise. To\novercome this limitation, in this work we forgo real training documents and\nannotations altogether and use open-source LLMs to directly generate synthetic\ndocuments that answer real user queries according to several different levels\nof relevance. This fully synthetic ranking context of graduated relevance,\ntogether with an appropriate list-wise loss (Wasserstein distance), enables us\nto train dense retrievers in a way that better captures the ranking task.\nExperiments on various IR datasets show that our proposed approach outperforms\nconventional training with InfoNCE by a large margin. Without using any real\ndocuments for training, our dense retriever significantly outperforms the same\nretriever trained through self-supervision. More importantly, it matches the\nperformance of the same retriever trained on real, labeled training documents\nof the same dataset, while being more robust to distribution shift and clearly\noutperforming it when evaluated zero-shot on the BEIR dataset collection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23333", "pdf": "https://arxiv.org/pdf/2503.23333", "abs": "https://arxiv.org/abs/2503.23333", "authors": ["Jing Zhu", "Mingxuan Ju", "Yozen Liu", "Danai Koutra", "Neil Shah", "Tong Zhao"], "title": "Beyond Unimodal Boundaries: Generative Recommendation with Multimodal Semantics", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Generative recommendation (GR) has become a powerful paradigm in\nrecommendation systems that implicitly links modality and semantics to item\nrepresentation, in contrast to previous methods that relied on non-semantic\nitem identifiers in autoregressive models. However, previous research has\npredominantly treated modalities in isolation, typically assuming item content\nis unimodal (usually text). We argue that this is a significant limitation\ngiven the rich, multimodal nature of real-world data and the potential\nsensitivity of GR models to modality choices and usage. Our work aims to\nexplore the critical problem of Multimodal Generative Recommendation (MGR),\nhighlighting the importance of modality choices in GR nframeworks. We reveal\nthat GR models are particularly sensitive to different modalities and examine\nthe challenges in achieving effective GR when multiple modalities are\navailable. By evaluating design strategies for effectively leveraging multiple\nmodalities, we identify key challenges and introduce MGR-LF++, an enhanced late\nfusion framework that employs contrastive modality alignment and special tokens\nto denote different modalities, achieving a performance improvement of over 20%\ncompared to single-modality alternatives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23660", "pdf": "https://arxiv.org/pdf/2503.23660", "abs": "https://arxiv.org/abs/2503.23660", "authors": ["Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "Current movie dubbing technology can generate the desired voice from a given\nspeech prompt, ensuring good synchronization between speech and visuals while\naccurately conveying the intended emotions. However, in movie dubbing, key\naspects such as adapting to different dubbing styles, handling dialogue,\nnarration, and monologue effectively, and understanding subtle details like the\nage and gender of speakers, have not been well studied. To address this\nchallenge, we propose a framework of multi-modal large language model. First,\nit utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visual\ninputs to understand dubbing styles and fine-grained attributes. Second, it\ngenerates high-quality dubbing through large speech generation models, guided\nby multimodal conditions. Additionally, we have developed a movie dubbing\ndataset with CoT annotations. The evaluation results demonstrate a performance\nimprovement over state-of-the-art methods across multiple datasets. In\nparticular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from\n82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animation\ndataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 for\ndubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 and\nWER decreases from 52.69% to 23.20% for initial reasoning setting on proposed\nCoT-Movie-Dubbing dataset in the comparison with the state-of-the art models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "dialogue", "fine-grained"], "score": 4}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23487", "pdf": "https://arxiv.org/pdf/2503.23487", "abs": "https://arxiv.org/abs/2503.23487", "authors": ["Irtaza Khalid", "Amir Masoud Nourollah", "Steven Schockaert"], "title": "Benchmarking Systematic Relational Reasoning with Large Language and Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Submitted to ACL 2025", "summary": "Large Language Models (LLMs) have been found to struggle with systematic\nreasoning. Even on tasks where they appear to perform well, their performance\noften depends on shortcuts, rather than on genuine reasoning abilities, leading\nthem to collapse on out-of-distribution examples. Post-training strategies\nbased on reinforcement learning and chain-of-thought prompting have recently\nbeen hailed as a step change. However, little is still known about the\npotential of the resulting ``Large Reasoning Models'' (LRMs) beyond problem\nsolving in mathematics and programming, where finding genuine\nout-of-distribution problems can be difficult. In this paper, we focus on tasks\nthat require systematic reasoning about relational compositions, especially for\nqualitative spatial and temporal reasoning. These tasks allow us to control the\ndifficulty of problem instances, and measure in a precise way to what extent\nmodels can generalise. We find that that the considered LLMs and LRMs overall\nperform poorly overall, albeit better than random chance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24228", "pdf": "https://arxiv.org/pdf/2503.24228", "abs": "https://arxiv.org/abs/2503.24228", "authors": ["Saab Mansour", "Leonardo Perelli", "Lorenzo Mainetti", "George Davidson", "Stefano D'Amato"], "title": "PAARS: Persona Aligned Agentic Retail Shoppers", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24289", "pdf": "https://arxiv.org/pdf/2503.24289", "abs": "https://arxiv.org/abs/2503.24289", "authors": ["Jiacheng Lin", "Tian Wang", "Kun Qian"], "title": "Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We propose Rec-R1, a general reinforcement learning framework that bridges\nlarge language models (LLMs) with recommendation systems through closed-loop\noptimization. Unlike prompting and supervised fine-tuning (SFT), Rec-R1\ndirectly optimizes LLM generation using feedback from a fixed black-box\nrecommendation model, without relying on synthetic SFT data from proprietary\nmodels such as GPT-4o. This avoids the substantial cost and effort required for\ndata distillation. To verify the effectiveness of Rec-R1, we evaluate it on two\nrepresentative tasks: product search and sequential recommendation.\nExperimental results demonstrate that Rec-R1 not only consistently outperforms\nprompting- and SFT-based methods, but also achieves significant gains over\nstrong discriminative baselines, even when used with simple retrievers such as\nBM25. Moreover, Rec-R1 preserves the general-purpose capabilities of the LLM,\nunlike SFT, which often impairs instruction-following and reasoning. These\nfindings suggest Rec-R1 as a promising foundation for continual task-specific\nadaptation without catastrophic forgetting.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24370", "pdf": "https://arxiv.org/pdf/2503.24370", "abs": "https://arxiv.org/abs/2503.24370", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Prateek Mittal"], "title": "Effectively Controlling Reasoning Models through Thinking Intervention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23736", "pdf": "https://arxiv.org/pdf/2503.23736", "abs": "https://arxiv.org/abs/2503.23736", "authors": ["Lingyu Liu", "Yaxiong Wang", "Li Zhu", "Zhedong Zheng"], "title": "Every Painting Awakened: A Training-free Framework for Painting-to-Animation Generation", "categories": ["cs.CV", "cs.MM"], "comment": "The project is available at:\n  https://painting-animation.github.io/animation/", "summary": "We introduce a training-free framework specifically designed to bring\nreal-world static paintings to life through image-to-video (I2V) synthesis,\naddressing the persistent challenge of aligning these motions with textual\nguidance while preserving fidelity to the original artworks. Existing I2V\nmethods, primarily trained on natural video datasets, often struggle to\ngenerate dynamic outputs from static paintings. It remains challenging to\ngenerate motion while maintaining visual consistency with real-world paintings.\nThis results in two distinct failure modes: either static outputs due to\nlimited text-based motion interpretation or distorted dynamics caused by\ninadequate alignment with real-world artistic styles. We leverage the advanced\ntext-image alignment capabilities of pre-trained image models to guide the\nanimation process. Our approach introduces synthetic proxy images through two\nkey innovations: (1) Dual-path score distillation: We employ a dual-path\narchitecture to distill motion priors from both real and synthetic data,\npreserving static details from the original painting while learning dynamic\ncharacteristics from synthetic frames. (2) Hybrid latent fusion: We integrate\nhybrid features extracted from real paintings and synthetic proxy images via\nspherical linear interpolation in the latent space, ensuring smooth transitions\nand enhancing temporal consistency. Experimental evaluations confirm that our\napproach significantly improves semantic alignment with text prompts while\nfaithfully preserving the unique characteristics and integrity of the original\npaintings. Crucially, by achieving enhanced dynamic effects without requiring\nany model training or learnable parameters, our framework enables plug-and-play\nintegration with existing I2V methods, making it an ideal solution for\nanimating real-world paintings. More animated examples can be found on our\nproject website.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23844", "pdf": "https://arxiv.org/pdf/2503.23844", "abs": "https://arxiv.org/abs/2503.23844", "authors": ["Xuyang Li", "Chenyu Li", "Pedram Ghamisi", "Danfeng Hong"], "title": "FlexiMo: A Flexible Remote Sensing Foundation Model", "categories": ["cs.CV"], "comment": null, "summary": "The rapid expansion of multi-source satellite imagery drives innovation in\nEarth observation, opening unprecedented opportunities for Remote Sensing\nFoundation Models to harness diverse data. However, many existing models remain\nconstrained by fixed spatial resolutions and patch sizes, limiting their\nability to fully exploit the heterogeneous spatial characteristics inherent in\nsatellite imagery. To address these challenges, we propose FlexiMo, a flexible\nremote sensing foundation model that endows the pre-trained model with the\nflexibility to adapt to arbitrary spatial resolutions. Central to FlexiMo is a\nspatial resolution-aware module that employs a parameter-free alignment\nembedding mechanism to dynamically recalibrate patch embeddings based on the\ninput image's resolution and dimensions. This design not only preserves\ncritical token characteristics and ensures multi-scale feature fidelity but\nalso enables efficient feature extraction without requiring modifications to\nthe underlying network architecture. In addition, FlexiMo incorporates a\nlightweight channel adaptation module that leverages prior spectral information\nfrom sensors. This mechanism allows the model to process images with varying\nnumbers of channels while maintaining the data's intrinsic physical properties.\nExtensive experiments on diverse multimodal, multi-resolution, and multi-scale\ndatasets demonstrate that FlexiMo significantly enhances model generalization\nand robustness. In particular, our method achieves outstanding performance\nacross a range of downstream tasks, including scene classification, land cover\nclassification, urban building segmentation, and cloud detection. By enabling\nparameter-efficient and physically consistent adaptation, FlexiMo paves the way\nfor more adaptable and effective foundation models in real-world remote sensing\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23980", "pdf": "https://arxiv.org/pdf/2503.23980", "abs": "https://arxiv.org/abs/2503.23980", "authors": ["Yanbo Wang", "Yongtao Chen", "Chuan Cao", "Tianchen Deng", "Wentao Zhao", "Jingchuan Wang", "Weidong Chen"], "title": "SALT: A Flexible Semi-Automatic Labeling Tool for General LiDAR Point Clouds with Cross-Scene Adaptability and 4D Consistency", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose a flexible Semi-Automatic Labeling Tool (SALT) for general LiDAR\npoint clouds with cross-scene adaptability and 4D consistency. Unlike recent\napproaches that rely on camera distillation, SALT operates directly on raw\nLiDAR data, automatically generating pre-segmentation results. To achieve this,\nwe propose a novel zero-shot learning paradigm, termed data alignment, which\ntransforms LiDAR data into pseudo-images by aligning with the training\ndistribution of vision foundation models. Additionally, we design a\n4D-consistent prompting strategy and 4D non-maximum suppression module to\nenhance SAM2, ensuring high-quality, temporally consistent presegmentation.\nSALT surpasses the latest zero-shot methods by 18.4% PQ on SemanticKITTI and\nachieves nearly 40-50% of human annotator performance on our newly collected\nlow-resolution LiDAR data and on combined data from three LiDAR types,\nsignificantly boosting annotation efficiency. We anticipate that SALT's\nopen-sourcing will catalyze substantial expansion of current LiDAR datasets and\nlay the groundwork for the future development of LiDAR foundation models. Code\nis available at https://github.com/Cavendish518/SALT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24129", "pdf": "https://arxiv.org/pdf/2503.24129", "abs": "https://arxiv.org/abs/2503.24129", "authors": ["Dominik Schnaus", "Nikita Araslanov", "Daniel Cremers"], "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, Project page:\n  https://dominik-schnaus.github.io/itsamatch/", "summary": "The platonic representation hypothesis suggests that vision and language\nembeddings become more homogeneous as model and dataset sizes increase. In\nparticular, pairwise distances within each modality become more similar. This\nsuggests that as foundation models mature, it may become possible to match\nvision and language embeddings in a fully unsupervised fashion, i.e. without\nparallel data. We present the first feasibility study, and investigate\nconformity of existing vision and language foundation models in the context of\nunsupervised, or \"blind\", matching. First, we formulate unsupervised matching\nas a quadratic assignment problem and introduce a novel heuristic that\noutperforms previous solvers. We also develop a technique to find optimal\nmatching problems, for which a non-trivial match is very likely. Second, we\nconduct an extensive study deploying a range of vision and language models on\nfour datasets. Our analysis reveals that for many problem instances, vision and\nlanguage representations can be indeed matched without supervision. This\nfinding opens up the exciting possibility of embedding semantic knowledge into\nother modalities virtually annotation-free. As a proof of concept, we showcase\nan unsupervised classifier, which achieves non-trivial classification accuracy\nwithout any image-text annotation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24182", "pdf": "https://arxiv.org/pdf/2503.24182", "abs": "https://arxiv.org/abs/2503.24182", "authors": ["Yingrui Ji", "Xi Xiao", "Gaofei Chen", "Hao Xu", "Chenrui Ma", "Lijing Zhu", "Aokun Liang", "Jiansheng Chen"], "title": "CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success\nin cross-modal tasks such as zero-shot image classification and text-image\nretrieval by effectively aligning visual and textual representations. However,\nthe theoretical foundations underlying CLIP's strong generalization remain\nunclear. In this work, we address this gap by proposing the Cross-modal\nInformation Bottleneck (CIB) framework. CIB offers a principled interpretation\nof CLIP's contrastive learning objective as an implicit Information Bottleneck\noptimization. Under this view, the model maximizes shared cross-modal\ninformation while discarding modality-specific redundancies, thereby preserving\nessential semantic alignment across modalities. Building on this insight, we\nintroduce a Cross-modal Information Bottleneck Regularization (CIBR) method\nthat explicitly enforces these IB principles during training. CIBR introduces a\npenalty term to discourage modality-specific redundancy, thereby enhancing\nsemantic alignment between image and text features. We validate CIBR on\nextensive vision-language benchmarks, including zero-shot classification across\nseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.\nThe results show consistent performance gains over standard CLIP. These\nfindings provide the first theoretical understanding of CLIP's generalization\nthrough the IB lens. They also demonstrate practical improvements, offering\nguidance for future cross-modal representation learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.24270", "pdf": "https://arxiv.org/pdf/2503.24270", "abs": "https://arxiv.org/abs/2503.24270", "authors": ["Yuelei Li", "Hyunjin Kim", "Fangneng Zhan", "Ri-Zhao Qiu", "Mazeyu Ji", "Xiaojun Shan", "Xueyan Zou", "Paul Liang", "Hanspeter Pfister", "Xiaolong Wang"], "title": "Visual Acoustic Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.22713", "pdf": "https://arxiv.org/pdf/2503.22713", "abs": "https://arxiv.org/abs/2503.22713", "authors": ["Nooshin Bahador", "Milad Lankarany"], "title": "Chirp Localization via Fine-Tuned Transformer Model: A Proof-of-Concept Study", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "19 pages, 8 figures", "summary": "Spectrograms are pivotal in time-frequency signal analysis, widely used in\naudio processing and computational neuroscience. Chirp-like patterns in\nelectroencephalogram (EEG) spectrograms (marked by linear or exponential\nfrequency sweep) are key biomarkers for seizure dynamics, but automated tools\nfor their detection, localization, and feature extraction are lacking. This\nstudy bridges this gap by fine-tuning a Vision Transformer (ViT) model on\nsynthetic spectrograms, augmented with Low-Rank Adaptation (LoRA) to boost\nadaptability. We generated 100000 synthetic spectrograms with chirp parameters,\ncreating the first large-scale benchmark for chirp localization. These\nspectrograms mimic neural chirps using linear or exponential frequency sweep,\nGaussian noise, and smoothing. A ViT model, adapted for regression, predicted\nchirp parameters. LoRA fine-tuned the attention layers, enabling efficient\nupdates to the pre-trained backbone. Training used MSE loss and the AdamW\noptimizer, with a learning rate scheduler and early stopping to curb\noverfitting. Only three features were targeted: Chirp Start Time (Onset Time),\nChirp Start Frequency (Onset Frequency), and Chirp End Frequency (Offset\nFrequency). Performance was evaluated via Pearson correlation between predicted\nand actual labels. Results showed strong alignment: 0.9841 correlation for\nchirp start time, with stable inference times (137 to 140s) and minimal bias in\nerror distributions. This approach offers a tool for chirp analysis in EEG\ntime-frequency representation, filling a critical methodological void.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23042", "pdf": "https://arxiv.org/pdf/2503.23042", "abs": "https://arxiv.org/abs/2503.23042", "authors": ["M Rita Verdelho", "Alexandre Bernardino", "Catarina Barata"], "title": "MIL vs. Aggregation: Evaluating Patient-Level Survival Prediction Strategies Using Graph-Based Learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Oncologists often rely on a multitude of data, including whole-slide images\n(WSIs), to guide therapeutic decisions, aiming for the best patient outcome.\nHowever, predicting the prognosis of cancer patients can be a challenging task\ndue to tumor heterogeneity and intra-patient variability, and the complexity of\nanalyzing WSIs. These images are extremely large, containing billions of\npixels, making direct processing computationally expensive and requiring\nspecialized methods to extract relevant information. Additionally, multiple\nWSIs from the same patient may capture different tumor regions, some being more\ninformative than others. This raises a fundamental question: Should we use all\nWSIs to characterize the patient, or should we identify the most representative\nslide for prognosis? Our work seeks to answer this question by performing a\ncomparison of various strategies for predicting survival at the WSI and patient\nlevel. The former treats each WSI as an independent sample, mimicking the\nstrategy adopted in other works, while the latter comprises methods to either\naggregate the predictions of the several WSIs or automatically identify the\nmost relevant slide using multiple-instance learning (MIL). Additionally, we\nevaluate different Graph Neural Networks architectures under these strategies.\nWe conduct our experiments using the MMIST-ccRCC dataset, which comprises\npatients with clear cell renal cell carcinoma (ccRCC). Our results show that\nMIL-based selection improves accuracy, suggesting that choosing the most\nrepresentative slide benefits survival prediction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23179", "pdf": "https://arxiv.org/pdf/2503.23179", "abs": "https://arxiv.org/abs/2503.23179", "authors": ["Wiebke Heyer", "Yannic Elser", "Lennart Berkel", "Xinrui Song", "Xuanang Xu", "Pingkun Yan", "Xi Jia", "Zi Li", "Tony C. W. Mok", "BoWen LI", "Christian Staackmann", "Christoph Großbröhmer", "Alessa Hering", "Malte M. Sieren", "Mattias P. Heinrich"], "title": "OncoReg: Medical Image Registration for Oncological Challenges", "categories": ["eess.IV", "cs.CV"], "comment": "26 pages, 6 figures", "summary": "In modern cancer research, the vast volume of medical data generated is often\nunderutilised due to challenges related to patient privacy. The OncoReg\nChallenge addresses this issue by enabling researchers to develop and validate\nimage registration methods through a two-phase framework that ensures patient\nprivacy while fostering the development of more generalisable AI models. Phase\none involves working with a publicly available dataset, while phase two focuses\non training models on a private dataset within secure hospital networks.\nOncoReg builds upon the foundation established by the Learn2Reg Challenge by\nincorporating the registration of interventional cone-beam computed tomography\n(CBCT) with standard planning fan-beam CT (FBCT) images in radiotherapy.\nAccurate image registration is crucial in oncology, particularly for dynamic\ntreatment adjustments in image-guided radiotherapy, where precise alignment is\nnecessary to minimise radiation exposure to healthy tissues while effectively\ntargeting tumours. This work details the methodology and data behind the\nOncoReg Challenge and provides a comprehensive analysis of the competition\nentries and results. Findings reveal that feature extraction plays a pivotal\nrole in this registration task. A new method emerging from this challenge\ndemonstrated its versatility, while established approaches continue to perform\ncomparably to newer techniques. Both deep learning and classical approaches\nstill play significant roles in image registration, with the combination of\nmethods - particularly in feature extraction - proving most effective.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
{"id": "2503.23333", "pdf": "https://arxiv.org/pdf/2503.23333", "abs": "https://arxiv.org/abs/2503.23333", "authors": ["Jing Zhu", "Mingxuan Ju", "Yozen Liu", "Danai Koutra", "Neil Shah", "Tong Zhao"], "title": "Beyond Unimodal Boundaries: Generative Recommendation with Multimodal Semantics", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Generative recommendation (GR) has become a powerful paradigm in\nrecommendation systems that implicitly links modality and semantics to item\nrepresentation, in contrast to previous methods that relied on non-semantic\nitem identifiers in autoregressive models. However, previous research has\npredominantly treated modalities in isolation, typically assuming item content\nis unimodal (usually text). We argue that this is a significant limitation\ngiven the rich, multimodal nature of real-world data and the potential\nsensitivity of GR models to modality choices and usage. Our work aims to\nexplore the critical problem of Multimodal Generative Recommendation (MGR),\nhighlighting the importance of modality choices in GR nframeworks. We reveal\nthat GR models are particularly sensitive to different modalities and examine\nthe challenges in achieving effective GR when multiple modalities are\navailable. By evaluating design strategies for effectively leveraging multiple\nmodalities, we identify key challenges and introduce MGR-LF++, an enhanced late\nfusion framework that employs contrastive modality alignment and special tokens\nto denote different modalities, achieving a performance improvement of over 20%\ncompared to single-modality alternatives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-01.jsonl"}
