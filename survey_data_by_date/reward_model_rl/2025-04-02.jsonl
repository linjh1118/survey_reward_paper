{"id": "2504.00270", "pdf": "https://arxiv.org/pdf/2504.00270", "abs": "https://arxiv.org/abs/2504.00270", "authors": ["Tianqi", "Ding", "Dawei Xiang", "Yijiashun Qi", "Ze Yang", "Zunduo Zhao", "Tianyao Sun", "Pengbin Feng", "Haoyu Wang"], "title": "NeRF-Based defect detection", "categories": ["cs.CV"], "comment": "6 pages, 11 figures, 2025 2nd International Conference on Remote\n  Sensing, Mapping and Image Processing (RSMIP 2025)", "summary": "The rapid growth of industrial automation has highlighted the need for\nprecise and efficient defect detection in large-scale machinery. Traditional\ninspection techniques, involving manual procedures such as scaling tall\nstructures for visual evaluation, are labor-intensive, subjective, and often\nhazardous. To overcome these challenges, this paper introduces an automated\ndefect detection framework built on Neural Radiance Fields (NeRF) and the\nconcept of digital twins. The system utilizes UAVs to capture images and\nreconstruct 3D models of machinery, producing both a standard reference model\nand a current-state model for comparison. Alignment of the models is achieved\nthrough the Iterative Closest Point (ICP) algorithm, enabling precise point\ncloud analysis to detect deviations that signify potential defects. By\neliminating manual inspection, this method improves accuracy, enhances\noperational safety, and offers a scalable solution for defect detection. The\nproposed approach demonstrates great promise for reliable and efficient\nindustrial applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00883", "pdf": "https://arxiv.org/pdf/2504.00883", "abs": "https://arxiv.org/abs/2504.00883", "authors": ["Zhenyi Liao", "Qingsong Xie", "Yanhao Zhang", "Zijian Kong", "Haonan Lu", "Zhenyu Yang", "Zhijie Deng"], "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Increasing attention has been placed on improving the reasoning capacities of\nmulti-modal large language models (MLLMs). As the cornerstone for AI agents\nthat function in the physical realm, video-based visual-spatial intelligence\n(VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This\nwork conducts a first, in-depth study on improving the visual-spatial reasoning\nof MLLMs via R1-Zero-like training. Technically, we first identify that the\nvisual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models\ncannot be activated via Chain of Thought (CoT) prompts. We then incorporate\nGRPO training for improved visual-spatial reasoning, using the carefully\ncurated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation,\nwe identify the necessity to keep the KL penalty (even with a small value) in\nGRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from\nQwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o.\nMoreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves\nperformance comparable to that of the best open-source model\nLLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning\nand direct preference optimization baselines and observe strong performance\nsuperiority. The code and dataset will be available soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00470", "pdf": "https://arxiv.org/pdf/2504.00470", "abs": "https://arxiv.org/abs/2504.00470", "authors": ["Ruoyu Chen", "Siyuan Liang", "Jingzhi Li", "Shiming Liu", "Li Liu", "Hua Zhang", "Xiaochun Cao"], "title": "Less is More: Efficient Black-box Attribution via Minimal Interpretable Subset Selection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "To develop a trustworthy AI system, which aim to identify the input regions\nthat most influence the models decisions. The primary task of existing\nattribution methods lies in efficiently and accurately identifying the\nrelationships among input-prediction interactions. Particularly when the input\ndata is discrete, such as images, analyzing the relationship between inputs and\noutputs poses a significant challenge due to the combinatorial explosion. In\nthis paper, we propose a novel and efficient black-box attribution mechanism,\nLiMA (Less input is More faithful for Attribution), which reformulates the\nattribution of important regions as an optimization problem for submodular\nsubset selection. First, to accurately assess interactions, we design a\nsubmodular function that quantifies subset importance and effectively captures\ntheir impact on decision outcomes. Then, efficiently ranking input sub-regions\nby their importance for attribution, we improve optimization efficiency through\na novel bidirectional greedy search algorithm. LiMA identifies both the most\nand least important samples while ensuring an optimal attribution boundary that\nminimizes errors. Extensive experiments on eight foundation models demonstrate\nthat our method provides faithful interpretations with fewer regions and\nexhibits strong generalization, shows an average improvement of 36.3% in\nInsertion and 39.6% in Deletion. Our method also outperforms the naive greedy\nsearch in attribution efficiency, being 1.6 times faster. Furthermore, when\nexplaining the reasons behind model prediction errors, the average highest\nconfidence achieved by our method is, on average, 86.1% higher than that of\nstate-of-the-art attribution algorithms. The code is available at\nhttps://github.com/RuoyuChen10/LIMA.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "LIMA"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00016", "pdf": "https://arxiv.org/pdf/2504.00016", "abs": "https://arxiv.org/abs/2504.00016", "authors": ["Birger Moell", "Fredrik Sand Aronsson", "Sanian Akbar"], "title": "Medical Reasoning in LLMs: An In-Depth Analysis of DeepSeek R1", "categories": ["cs.CL"], "comment": null, "summary": "Integrating large language models (LLMs) like DeepSeek R1 into healthcare\nrequires rigorous evaluation of their reasoning alignment with clinical\nexpertise. This study assesses DeepSeek R1's medical reasoning against expert\npatterns using 100 MedQA clinical cases. The model achieved 93% diagnostic\naccuracy, demonstrating systematic clinical judgment through differential\ndiagnosis, guideline-based treatment selection, and integration of\npatient-specific factors. However, error analysis of seven incorrect cases\nrevealed persistent limitations: anchoring bias, challenges reconciling\nconflicting data, insufficient exploration of alternatives, overthinking,\nknowledge gaps, and premature prioritization of definitive treatment over\nintermediate care. Crucially, reasoning length correlated with accuracy -\nshorter responses (<5,000 characters) were more reliable, suggesting extended\nexplanations may signal uncertainty or rationalization of errors. While\nDeepSeek R1 exhibits foundational clinical reasoning capabilities, recurring\nflaws highlight critical areas for refinement, including bias mitigation,\nknowledge updates, and structured reasoning frameworks. These findings\nunderscore LLMs' potential to augment medical decision-making through\nartificial reasoning but emphasize the need for domain-specific validation,\ninterpretability safeguards, and confidence metrics (e.g., response length\nthresholds) to ensure reliability in real-world applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00025", "pdf": "https://arxiv.org/pdf/2504.00025", "abs": "https://arxiv.org/abs/2504.00025", "authors": ["Uwe Peters", "Benjamin Chin-Yee"], "title": "Generalization Bias in Large Language Model Summarization of Scientific Research", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Artificial intelligence chatbots driven by large language models (LLMs) have\nthe potential to increase public science literacy and support scientific\nresearch, as they can quickly summarize complex scientific information in\naccessible terms. However, when summarizing scientific texts, LLMs may omit\ndetails that limit the scope of research conclusions, leading to\ngeneralizations of results broader than warranted by the original study. We\ntested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA\n3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their\noriginal scientific texts. Even when explicitly prompted for accuracy, most\nLLMs produced broader generalizations of scientific results than those in the\noriginal texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing\nin 26 to 73% of cases. In a direct comparison of LLM-generated and\nhuman-authored science summaries, LLM summaries were nearly five times more\nlikely to contain broad generalizations (OR = 4.85, 95% CI [3.06, 7.70]).\nNotably, newer models tended to perform worse in generalization accuracy than\nearlier ones. Our results indicate a strong bias in many widely used LLMs\ntowards overgeneralizing scientific conclusions, posing a significant risk of\nlarge-scale misinterpretations of research findings. We highlight potential\nmitigation strategies, including lowering LLM temperature settings and\nbenchmarking LLMs for generalization accuracy.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00023", "pdf": "https://arxiv.org/pdf/2504.00023", "abs": "https://arxiv.org/abs/2504.00023", "authors": ["Niklas Rottmayer", "Claudia Redenbach"], "title": "A Novel Distance-Based Metric for Quality Assessment in Image Segmentation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The assessment of segmentation quality plays a fundamental role in the\ndevelopment, optimization, and comparison of segmentation methods which are\nused in a wide range of applications. With few exceptions, quality assessment\nis performed using traditional metrics, which are based on counting the number\nof erroneous pixels but do not capture the spatial distribution of errors.\nEstablished distance-based metrics such as the average Hausdorff distance are\ndifficult to interpret and compare for different methods and datasets. In this\npaper, we introduce the Surface Consistency Coefficient (SCC), a novel\ndistance-based quality metric that quantifies the spatial distribution of\nerrors based on their proximity to the surface of the structure. Through a\nrigorous analysis using synthetic data and real segmentation results, we\ndemonstrate the robustness and effectiveness of SCC in distinguishing errors\nnear the surface from those further away. At the same time, SCC is easy to\ninterpret and comparable across different structural contexts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00050", "pdf": "https://arxiv.org/pdf/2504.00050", "abs": "https://arxiv.org/abs/2504.00050", "authors": ["Nuo Chen", "Zhiyuan Hu", "Qingyun Zou", "Jiaying Wu", "Qian Wang", "Bryan Hooi", "Bingsheng He"], "title": "JudgeLRM: Large Reasoning Models as a Judge", "categories": ["cs.CL", "cs.AI"], "comment": "preprint", "summary": "The rise of Large Language Models (LLMs) as evaluators offers a scalable\nalternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for\njudges approaches often fall short in domains requiring complex reasoning. In\nthis work, we investigate whether LLM judges truly benefit from enhanced\nreasoning capabilities. Through a detailed analysis of reasoning requirements\nacross evaluation tasks, we reveal a negative correlation between SFT\nperformance gains and the proportion of reasoning-demanding samples -\nhighlighting the limitations of SFT in such scenarios. To address this, we\nintroduce JudgeLRM, a family of judgment-oriented LLMs trained using\nreinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM\nmodels consistently outperform both SFT-tuned and state-of-the-art reasoning\nmodels. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms\nDeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks\nrequiring deep reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "correlation"], "score": 3}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00241", "pdf": "https://arxiv.org/pdf/2504.00241", "abs": "https://arxiv.org/abs/2504.00241", "authors": ["Rabimba Karanjai", "Boris Shor", "Amanda Austin", "Ryan Kennedy", "Yang Lu", "Lei Xu", "Weidong Shi"], "title": "Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the use of Large Language Models (LLMs) to synthesize\npublic opinion data, addressing challenges in traditional survey methods like\ndeclining response rates and non-response bias. We introduce a novel technique:\nrole creation based on knowledge injection, a form of in-context learning that\nleverages RAG and specified personality profiles from the HEXACO model and\ndemographic information, and uses that for dynamically generated prompts. This\nmethod allows LLMs to simulate diverse opinions more accurately than existing\nprompt engineering approaches. We compare our results with pre-trained models\nwith standard few-shot prompts. Experiments using questions from the\nCooperative Election Study (CES) demonstrate that our role-creation approach\nsignificantly improves the alignment of LLM-generated opinions with real-world\nhuman survey responses, increasing answer adherence. In addition, we discuss\nchallenges, limitations and future research directions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00356", "pdf": "https://arxiv.org/pdf/2504.00356", "abs": "https://arxiv.org/abs/2504.00356", "authors": ["Ting Liu", "Siyuan Li"], "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "accepted to CVPR2025", "summary": "Recent advances in zero-shot referring image segmentation (RIS), driven by\nmodels such as the Segment Anything Model (SAM) and CLIP, have made substantial\nprogress in aligning visual and textual information. Despite these successes,\nthe extraction of precise and high-quality mask region representations remains\na critical challenge, limiting the full potential of RIS tasks. In this paper,\nwe introduce a training-free, hybrid global-local feature extraction approach\nthat integrates detailed mask-specific features with contextual information\nfrom the surrounding area, enhancing mask region representation. To further\nstrengthen alignment between mask regions and referring expressions, we propose\na spatial guidance augmentation strategy that improves spatial coherence, which\nis essential for accurately localizing described areas. By incorporating\nmultiple spatial cues, this approach facilitates more robust and precise\nreferring segmentation. Extensive experiments on standard RIS benchmarks\ndemonstrate that our method significantly outperforms existing zero-shot RIS\nmodels, achieving substantial performance gains. We believe our approach\nadvances RIS tasks and establishes a versatile framework for region-text\nalignment, offering broader implications for cross-modal understanding and\ninteraction. Code is available at https://github.com/fhgyuanshen/HybridGL .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00339", "pdf": "https://arxiv.org/pdf/2504.00339", "abs": "https://arxiv.org/abs/2504.00339", "authors": ["Hoang Hai Phan", "Nguyen Duc Minh Vu", "Nam Dang Phuong"], "title": "VNJPTranslate: A comprehensive pipeline for Vietnamese-Japanese translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Neural Machine Translation (NMT) driven by Transformer architectures has\nadvanced significantly, yet faces challenges with low-resource language pairs\nlike Vietnamese-Japanese (Vi-Ja). Issues include sparse parallel data and\nhandling linguistic/cultural nuances. Recent progress in Large Language Models\n(LLMs) with strong reasoning, often refined via Reinforcement Learning (RL),\nenables high-quality synthetic data generation. We introduce VNJPTranslate, a\npipeline designed to systematically address the Vi-Ja translation task. It\nfeatures a targeted data augmentation strategy using advanced LLMs with\nChain-of-Thought prompting for challenging segments identified via corpus\nanalysis. Subsequently, we employ efficient fine-tuning techniques (Unsloth\nwith QLoRA) on a capable, low-parameter autoregressive model (specifically, a\nfine-tuned version of the 1.8B parameter Sailor model, which is based on the\nQwen architecture) to create a practical and high-performing translation\nsystem. This integrated approach aims to improve Vi-Ja translation quality\nsignificantly over existing baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00396", "pdf": "https://arxiv.org/pdf/2504.00396", "abs": "https://arxiv.org/abs/2504.00396", "authors": ["Xiaole Xian", "Zhichao Liao", "Qingyu Li", "Wenyu Qin", "Pengfei Wan", "Weicheng Xie", "Long Zeng", "Linlin Shen", "Pingfa Feng"], "title": "SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning", "categories": ["cs.CV"], "comment": null, "summary": "While fine-tuning pre-trained Text-to-Image (T2I) models on portrait datasets\nenables attribute customization, existing methods suffer from Semantic\nPollution that compromises the original model's behavior and prevents\nincremental learning. To address this, we propose SPF-Portrait, a pioneering\nwork to purely understand customized semantics while eliminating semantic\npollution in text-driven portrait customization. In our SPF-Portrait, we\npropose a dual-path pipeline that introduces the original model as a reference\nfor the conventional fine-tuning path. Through contrastive learning, we ensure\nadaptation to target attributes and purposefully align other unrelated\nattributes with the original portrait. We introduce a novel Semantic-Aware Fine\nControl Map, which represents the precise response regions of the target\nsemantics, to spatially guide the alignment process between the contrastive\npaths. This alignment process not only effectively preserves the performance of\nthe original model but also avoids over-alignment. Furthermore, we propose a\nnovel response enhancement mechanism to reinforce the performance of target\nattributes, while mitigating representation discrepancy inherent in direct\ncross-modal supervision. Extensive experiments demonstrate that SPF-Portrait\nachieves state-of-the-art performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00409", "pdf": "https://arxiv.org/pdf/2504.00409", "abs": "https://arxiv.org/abs/2504.00409", "authors": ["Mohanakrishnan Hariharan"], "title": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have greatly improved their capability in\nperforming NLP tasks. However, deeper semantic understanding, contextual\ncoherence, and more subtle reasoning are still difficult to obtain. The paper\ndiscusses state-of-the-art methodologies that advance LLMs with more advanced\nNLU techniques, such as semantic parsing, knowledge integration, and contextual\nreinforcement learning. We analyze the use of structured knowledge graphs,\nretrieval-augmented generation (RAG), and fine-tuning strategies that match\nmodels with human-level understanding. Furthermore, we address the\nincorporation of transformer-based architectures, contrastive learning, and\nhybrid symbolic-neural methods that address problems like hallucinations,\nambiguity, and inconsistency in the factual perspectives involved in performing\ncomplex NLP tasks, such as question-answering text summarization and dialogue\ngeneration. Our findings show the importance of semantic precision for\nenhancing AI-driven language systems and suggest future research directions to\nbridge the gap between statistical language models and true natural language\nunderstanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "dialogue"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00476", "pdf": "https://arxiv.org/pdf/2504.00476", "abs": "https://arxiv.org/abs/2504.00476", "authors": ["Haobo Yuan", "Tao Zhang", "Xiangtai Li", "Lu Qi", "Zilong Huang", "Shilin Xu", "Jiashi Feng", "Ming-Hsuan Yang"], "title": "4th PVUW MeViS 3rd Place Report: Sa2VA", "categories": ["cs.CV"], "comment": "Technical Report, 4 pages, Code:\n  https://github.com/magic-research/Sa2VA", "summary": "Referring video object segmentation (RVOS) is a challenging task that\nrequires the model to segment the object in a video given the language\ndescription. MeViS is a recently proposed dataset that contains motion\nexpressions of the target objects, leading to a challenging benchmark, compared\nwith existing RVOS benchmarks. On the other hand, for referring expression\ntasks, a new trend is to adopt multi-modal large language model (MLLM) to\nachieve better image and text alignment. In this report, we show that with a\nsimple modification to the test time inference method on stronger MLLMs, we can\nlead to stronger results on MeVIS. In particular, we adopt the recent method\nSa2VA, a unified model for dense grounded understanding of both images and\nvideos. By enlarging the scope of key frames, without any further training, we\ncan achieve the 3rd place in the 4th PVUW workshop.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00478", "pdf": "https://arxiv.org/pdf/2504.00478", "abs": "https://arxiv.org/abs/2504.00478", "authors": ["Zhuohao Li", "Zhicheng Huang", "Wenchao Liu", "Zhuxing Zhang", "Jianming Miao"], "title": "FSSUWNet: Mitigating the Fragility of Pre-trained Models with Feature Enhancement for Few-Shot Semantic Segmentation in Underwater Images", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Semantic Segmentation (FSS), which focuses on segmenting new classes\nin images using only a limited number of annotated examples, has recently\nprogressed in data-scarce domains. However, in this work, we show that the\nexisting FSS methods often struggle to generalize to underwater environments.\nSpecifically, the prior features extracted by pre-trained models used as\nfeature extractors are fragile due to the unique challenges of underwater\nimages. To address this, we propose FSSUWNet, a tailored FSS framework for\nunderwater images with feature enhancement. FSSUWNet exploits the integration\nof complementary features, emphasizing both low-level and high-level image\ncharacteristics. In addition to employing a pre-trained model as the primary\nencoder, we propose an auxiliary encoder called Feature Enhanced Encoder which\nextracts complementary features to better adapt to underwater scene\ncharacteristics. Furthermore, a simple and effective Feature Alignment Module\naims to provide global prior knowledge and align low-level features with\nhigh-level features in dimensions. Given the scarcity of underwater images, we\nintroduce a cross-validation dataset version based on the Segmentation of\nUnderwater Imagery dataset. Extensive experiments on public underwater\nsegmentation datasets demonstrate that our approach achieves state-of-the-art\nperformance. For example, our method outperforms the previous best method by\n2.8% and 2.6% in terms of the mean Intersection over Union metric for 1-shot\nand 5-shot scenarios in the datasets, respectively. Our implementation is\navailable at https://github.com/lizhh268/FSSUWNet.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00490", "pdf": "https://arxiv.org/pdf/2504.00490", "abs": "https://arxiv.org/abs/2504.00490", "authors": ["Zetong Chen", "Yuzhuo Chen", "Hai Zhong", "Xu Qiao"], "title": "SCFANet: Style Distribution Constraint Feature Alignment Network For Pathological Staining Translation", "categories": ["cs.CV"], "comment": null, "summary": "Immunohistochemical (IHC) staining serves as a valuable technique for\ndetecting specific antigens or proteins through antibody-mediated\nvisualization. However, the IHC staining process is both time-consuming and\ncostly. To address these limitations, the application of deep learning models\nfor direct translation of cost-effective Hematoxylin and Eosin (H&E) stained\nimages into IHC stained images has emerged as an efficient solution.\nNevertheless, the conversion from H&E to IHC images presents significant\nchallenges, primarily due to alignment discrepancies between image pairs and\nthe inherent diversity in IHC staining style patterns. To overcome these\nchallenges, we propose the Style Distribution Constraint Feature Alignment\nNetwork (SCFANet), which incorporates two innovative modules: the Style\nDistribution Constrainer (SDC) and Feature Alignment Learning (FAL). The SDC\nensures consistency between the generated and target images' style\ndistributions while integrating cycle consistency loss to maintain structural\nconsistency. To mitigate the complexity of direct image-to-image translation,\nthe FAL module decomposes the end-to-end translation task into two subtasks:\nimage reconstruction and feature alignment. Furthermore, we ensure pathological\nconsistency between generated and target images by maintaining pathological\npattern consistency and Optical Density (OD) uniformity. Extensive experiments\nconducted on the Breast Cancer Immunohistochemical (BCI) dataset demonstrate\nthat our SCFANet model outperforms existing methods, achieving precise\ntransformation of H&E-stained images into their IHC-stained counterparts. The\nproposed approach not only addresses the technical challenges in H&E to IHC\nimage translation but also provides a robust framework for accurate and\nefficient stain conversion in pathological analysis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00752", "pdf": "https://arxiv.org/pdf/2504.00752", "abs": "https://arxiv.org/abs/2504.00752", "authors": ["Sameer Sadruddin", "Jennifer D'Souza", "Eleni Poupaki", "Alex Watkins", "Hamed Babaei Giglou", "Anisa Rula", "Bora Karasulu", "Sören Auer", "Adrie Mackus", "Erwin Kessels"], "title": "LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": "15 pages, 3 figures, to appear in the Extended Semantic Web\n  Conference (ESWC 2025) proceedings in the Resource track", "summary": "Extracting structured information from unstructured text is crucial for\nmodeling real-world processes, but traditional schema mining relies on\nsemi-structured data, limiting scalability. This paper introduces schema-miner,\na novel tool that combines large language models with human feedback to\nautomate and refine schema extraction. Through an iterative workflow, it\norganizes properties from text, incorporates expert input, and integrates\ndomain-specific ontologies for semantic depth. Applied to materials\nscience--specifically atomic layer deposition--schema-miner demonstrates that\nexpert-guided LLMs generate semantically rich schemas suitable for diverse\nreal-world applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00526", "pdf": "https://arxiv.org/pdf/2504.00526", "abs": "https://arxiv.org/abs/2504.00526", "authors": ["Xinrun Xu", "Qiuhong Zhang", "Jianwen Yang", "Zhanbiao Lian", "Jin Yan", "Zhiming Ding", "Shan Jiang"], "title": "High-Quality Pseudo-Label Generation Based on Visual Prompt Assisted Cloud Model Update", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN'25", "summary": "Generating high-quality pseudo-labels on the cloud is crucial for cloud-edge\nobject detection, especially in dynamic traffic monitoring where data\ndistributions evolve. Existing methods often assume reliable cloud models,\nneglecting potential errors or struggling with complex distribution shifts.\nThis paper proposes Cloud-Adaptive High-Quality Pseudo-label generation\n(CA-HQP), addressing these limitations by incorporating a learnable Visual\nPrompt Generator (VPG) and dual feature alignment into cloud model updates. The\nVPG enables parameter-efficient adaptation by injecting visual prompts,\nenhancing flexibility without extensive fine-tuning. CA-HQP mitigates domain\ndiscrepancies via two feature alignment techniques: global Domain Query Feature\nAlignment (DQFA) capturing scene-level shifts, and fine-grained Temporal\nInstance-Aware Feature Embedding Alignment (TIAFA) addressing instance\nvariations. Experiments on the Bellevue traffic dataset demonstrate that CA-HQP\nsignificantly improves pseudo-label quality compared to existing methods,\nleading to notable performance gains for the edge model and showcasing CA-HQP's\nadaptation effectiveness. Ablation studies validate each component (DQFA,\nTIAFA, VPG) and the synergistic effect of combined alignment strategies,\nhighlighting the importance of adaptive cloud updates and domain adaptation for\nrobust object detection in evolving scenarios. CA-HQP provides a promising\nsolution for enhancing cloud-edge object detection systems in real-world\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00543", "pdf": "https://arxiv.org/pdf/2504.00543", "abs": "https://arxiv.org/abs/2504.00543", "authors": ["Qi Zang", "Shuang Wang", "Dong Zhao", "Dou Quan", "Yang Hu", "Licheng Jiao"], "title": "Generalization-aware Remote Sensing Change Detection via Domain-agnostic Learning", "categories": ["cs.CV"], "comment": null, "summary": "Change detection has essential significance for the region's development, in\nwhich pseudo-changes between bitemporal images induced by imaging environmental\nfactors are key challenges. Existing transformation-based methods regard\npseudo-changes as a kind of style shift and alleviate it by transforming\nbitemporal images into the same style using generative adversarial networks\n(GANs). However, their efforts are limited by two drawbacks: 1) Transformed\nimages suffer from distortion that reduces feature discrimination. 2) Alignment\nhampers the model from learning domain-agnostic representations that degrades\nperformance on scenes with domain shifts from the training data. Therefore,\noriented from pseudo-changes caused by style differences, we present a\ngeneralizable domain-agnostic difference learning network (DonaNet). For the\ndrawback 1), we argue for local-level statistics as style proxies to assist\nagainst domain shifts. For the drawback 2), DonaNet learns domain-agnostic\nrepresentations by removing domain-specific style of encoded features and\nhighlighting the class characteristics of objects. In the removal, we propose a\ndomain difference removal module to reduce feature variance while preserving\ndiscriminative properties and propose its enhanced version to provide\npossibilities for eliminating more style by decorrelating the correlation\nbetween features. In the highlighting, we propose a cross-temporal\ngeneralization learning strategy to imitate latent domain shifts, thus enabling\nthe model to extract feature representations more robust to shifts actively.\nExtensive experiments conducted on three public datasets demonstrate that\nDonaNet outperforms existing state-of-the-art methods with a smaller model size\nand is more robust to domain shift.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00829", "pdf": "https://arxiv.org/pdf/2504.00829", "abs": "https://arxiv.org/abs/2504.00829", "authors": ["Yunjie Ji", "Sitong Zhao", "Xiaoyu Tian", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of Large Language Models (LLMs) with\nefficiency and scalability remains a fundamental challenge in artificial\nintelligence research. This paper presents a rigorous experimental\ninvestigation into how difficulty-aware staged reinforcement learning (RL)\nstrategies can substantially improve LLM reasoning performance. Through\nsystematic analysis, we demonstrate that strategically selecting training data\naccording to well-defined difficulty levels markedly enhances RL optimization.\nMoreover, we introduce a staged training methodology, progressively exposing\nmodels to increasingly challenging tasks, further amplifying reasoning\ncapabilities. Our findings reveal significant cross-domain benefits when\nsimultaneously training models on mathematical reasoning and code generation\ntasks. Notably, our proposed approach enables a 1.5B parameter model to achieve\nan accuracy of 42.3\\% on the AIME-2024 benchmark, 89.5\\% on the MATH-500\nbenchmark. These results underscore the efficacy of our method in advancing the\nreasoning proficiency of LLMs. We will open-source our datasets on GitHub and\nHugging Face.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "code generation", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00891", "pdf": "https://arxiv.org/pdf/2504.00891", "abs": "https://arxiv.org/abs/2504.00891", "authors": ["Jian Zhao", "Runze Liu", "Kaiyan Zhang", "Zhimu Zhou", "Junqi Gao", "Dong Li", "Jiafei Lyu", "Zhouyi Qian", "Biqing Qi", "Xiu Li", "Bowen Zhou"], "title": "GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shown that it is\npromising to utilize Process Reward Models (PRMs) as verifiers to enhance the\nperformance of LLMs. However, current PRMs face three key challenges: (1)\nlimited process supervision and generalization capabilities, (2) dependence on\nscalar value prediction without leveraging the generative abilities of LLMs,\nand (3) inability to scale the test-time compute of PRMs. In this work, we\nintroduce GenPRM, a generative process reward model that performs explicit\nChain-of-Thought (CoT) reasoning with code verification before providing\njudgment for each reasoning step. To obtain high-quality process supervision\nlabels and rationale data, we propose Relative Progress Estimation (RPE) and a\nrationale synthesis framework that incorporates code verification. Experimental\nresults on ProcessBench and several mathematical reasoning tasks show that\nGenPRM significantly outperforms prior PRMs with only 23K training data from\nMATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and\na 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally,\nGenPRM demonstrates strong abilities to serve as a critic model for policy\nmodel refinement. This work establishes a new paradigm for process supervision\nthat bridges the gap between PRMs and critic models in LLMs. Our code, model,\nand data will be available in https://ryanliu112.github.io/GenPRM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scale", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00640", "pdf": "https://arxiv.org/pdf/2504.00640", "abs": "https://arxiv.org/abs/2504.00640", "authors": ["Lanyun Zhu", "Tianrun Chen", "Qianxiong Xu", "Xuanyi Liu", "Deyi Ji", "Haiyang Wu", "De Wen Soh", "Jun Liu"], "title": "POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Existing LVLM-based reasoning segmentation methods often suffer from\nimprecise segmentation results and hallucinations in their text responses. This\npaper introduces POPEN, a novel framework designed to address these issues and\nachieve improved results. POPEN includes a preference-based optimization method\nto finetune the LVLM, aligning it more closely with human preferences and\nthereby generating better text responses and segmentation results.\nAdditionally, POPEN introduces a preference-based ensemble method for\ninference, which integrates multiple outputs from the LVLM using a\npreference-score-based attention mechanism for refinement. To better adapt to\nthe segmentation task, we incorporate several task-specific designs in our\nPOPEN framework, including a new approach for collecting segmentation\npreference data with a curriculum learning mechanism, and a novel preference\noptimization loss to refine the segmentation capability of the LVLM.\nExperiments demonstrate that our method achieves state-of-the-art performance\nin reasoning segmentation, exhibiting minimal hallucination in text responses\nand the highest segmentation accuracy compared to previous advanced methods\nlike LISA and PixelLM. Project page is https://lanyunzhu.site/POPEN/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00942", "pdf": "https://arxiv.org/pdf/2504.00942", "abs": "https://arxiv.org/abs/2504.00942", "authors": ["Anna Bavaresco", "Raquel Fernández"], "title": "Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?", "categories": ["cs.CL"], "comment": null, "summary": "A common assumption in Computational Linguistics is that text representations\nlearnt by multimodal models are richer and more human-like than those by\nlanguage-only models, as they are grounded in images or audio -- similar to how\nhuman language is grounded in real-world experiences. However, empirical\nstudies checking whether this is true are largely lacking. We address this gap\nby comparing word representations from contrastive multimodal models vs.\nlanguage-only ones in the extent to which they capture experiential information\n-- as defined by an existing norm-based 'experiential model' -- and align with\nhuman fMRI responses. Our results indicate that, surprisingly, language-only\nmodels are superior to multimodal ones in both respects. Additionally, they\nlearn more unique brain-relevant semantic information beyond that shared with\nthe experiential model. Overall, our study highlights the need to develop\ncomputational models that better integrate the complementary semantic\ninformation provided by multimodal data sources.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.01005", "pdf": "https://arxiv.org/pdf/2504.01005", "abs": "https://arxiv.org/abs/2504.01005", "authors": ["Nishad Singhi", "Hritik Bansal", "Arian Hosseini", "Aditya Grover", "Kai-Wei Chang", "Marcus Rohrbach", "Anna Rohrbach"], "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "29 pages", "summary": "Scaling test-time compute has emerged as a key strategy for enhancing the\nreasoning capabilities of large language models (LLMs), particularly in tasks\nlike mathematical problem-solving. A traditional approach, Self-Consistency\n(SC), generates multiple solutions to a problem and selects the most common\nanswer via majority voting. Another common method involves scoring each\nsolution with a reward model (verifier) and choosing the best one. Recent\nadvancements in Generative Reward Models (GenRM) reframe verification as a\nnext-token prediction task, enabling inference-time scaling along a new axis.\nSpecifically, GenRM generates multiple verification chains-of-thought to score\neach solution. Under a limited inference budget, this introduces a fundamental\ntrade-off: should you spend the budget on scaling solutions via SC or generate\nfewer solutions and allocate compute to verification via GenRM? To address\nthis, we evaluate GenRM against SC under a fixed inference budget.\nInterestingly, we find that SC is more compute-efficient than GenRM for most\npractical inference budgets across diverse models and datasets. For instance,\nGenRM first matches SC after consuming up to 8x the inference compute and\nrequires significantly more compute to outperform it. Furthermore, we derive\ninference scaling laws for the GenRM paradigm, revealing that compute-optimal\ninference favors scaling solution generation more aggressively than scaling the\nnumber of verifications. Our work provides practical guidance on optimizing\ntest-time scaling by balancing solution generation and verification. The code\nis available at https://github.com/nishadsinghi/sc-genrm-scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time", "scaling", "test-time compute", "inference compute"], "score": 5}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00882", "pdf": "https://arxiv.org/pdf/2504.00882", "abs": "https://arxiv.org/abs/2504.00882", "authors": ["Wei Zhou", "Yuyang Gao", "Xuanhe Zhou", "Guoliang Li"], "title": "CrackSQL: A Hybrid SQL Dialect Translation System Powered by Large Language Models", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Extension of our SIGMOD 2025 paper. Please refer to source code\n  available at: https://github.com/weAIDB/CrackSQL", "summary": "Dialect translation plays a key role in enabling seamless interaction across\nheterogeneous database systems. However, translating SQL queries between\ndifferent dialects (e.g., from PostgreSQL to MySQL) remains a challenging task\ndue to syntactic discrepancies and subtle semantic variations. Existing\napproaches including manual rewriting, rule-based systems, and large language\nmodel (LLM)-based techniques often involve high maintenance effort (e.g.,\ncrafting custom translation rules) or produce unreliable results (e.g., LLM\ngenerates non-existent functions), especially when handling complex queries. In\nthis demonstration, we present CrackSQL, the first hybrid SQL dialect\ntranslation system that combines rule and LLM-based methods to overcome these\nlimitations. CrackSQL leverages the adaptability of LLMs to minimize manual\nintervention, while enhancing translation accuracy by segmenting lengthy\ncomplex SQL via functionality-based query processing. To further improve\nrobustness, it incorporates a novel cross-dialect syntax embedding model for\nprecise syntax alignment, as well as an adaptive local-to-global translation\nstrategy that effectively resolves interdependent query operations. CrackSQL\nsupports three translation modes and offers multiple deployment and access\noptions including a web console interface, a PyPI package, and a command-line\nprompt, facilitating adoption across a variety of real-world use cases", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00996", "pdf": "https://arxiv.org/pdf/2504.00996", "abs": "https://arxiv.org/abs/2504.00996", "authors": ["Liangbin Xie", "Daniil Pakhomov", "Zhonghao Wang", "Zongze Wu", "Ziyan Chen", "Yuqian Zhou", "Haitian Zheng", "Zhifei Zhang", "Zhe Lin", "Jiantao Zhou", "Chao Dong"], "title": "TurboFill: Adapting Few-step Text-to-image Model for Fast Image Inpainting", "categories": ["cs.CV"], "comment": "Project webpage available at\n  https://liangbinxie.github.io/projects/TurboFill/", "summary": "This paper introduces TurboFill, a fast image inpainting model that enhances\na few-step text-to-image diffusion model with an inpainting adapter for\nhigh-quality and efficient inpainting. While standard diffusion models generate\nhigh-quality results, they incur high computational costs. We overcome this by\ntraining an inpainting adapter on a few-step distilled text-to-image model,\nDMD2, using a novel 3-step adversarial training scheme to ensure realistic,\nstructurally consistent, and visually harmonious inpainted regions. To evaluate\nTurboFill, we propose two benchmarks: DilationBench, which tests performance\nacross mask sizes, and HumanBench, based on human feedback for complex prompts.\nExperiments show that TurboFill outperforms both multi-step BrushNet and\nfew-step inpainting methods, setting a new benchmark for high-performance\ninpainting tasks. Our project page:\nhttps://liangbinxie.github.io/projects/TurboFill/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00999", "pdf": "https://arxiv.org/pdf/2504.00999", "abs": "https://arxiv.org/abs/2504.00999", "authors": ["Siyuan Li", "Luyuan Zhang", "Zedong Wang", "Juanxi Tian", "Cheng Tan", "Zicheng Liu", "Chang Yu", "Qingsong Xie", "Haonan Lu", "Haoqian Wang", "Zhen Lei"], "title": "MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR2025 (in process for more analysis and extension)", "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.01019", "pdf": "https://arxiv.org/pdf/2504.01019", "abs": "https://arxiv.org/abs/2504.01019", "authors": ["Pablo Ruiz-Ponce", "German Barquero", "Cristina Palmero", "Sergio Escalera", "José García-Rodríguez"], "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted - Project Page:\n  https://pabloruizponce.com/papers/MixerMDM", "summary": "Generating human motion guided by conditions such as textual descriptions is\nchallenging due to the need for datasets with pairs of high-quality motion and\ntheir corresponding conditions. The difficulty increases when aiming for finer\ncontrol in the generation. To that end, prior works have proposed to combine\nseveral motion diffusion models pre-trained on datasets with different types of\nconditions, thus allowing control with multiple conditions. However, the\nproposed merging strategies overlook that the optimal way to combine the\ngeneration processes might depend on the particularities of each pre-trained\ngenerative model and also the specific textual descriptions. In this context,\nwe introduce MixerMDM, the first learnable model composition technique for\ncombining pre-trained text-conditioned human motion diffusion models. Unlike\nprevious approaches, MixerMDM provides a dynamic mixing strategy that is\ntrained in an adversarial fashion to learn to combine the denoising process of\neach model depending on the set of conditions driving the generation. By using\nMixerMDM to combine single- and multi-person motion diffusion models, we\nachieve fine-grained control on the dynamics of every person individually, and\nalso on the overall interaction. Furthermore, we propose a new evaluation\ntechnique that, for the first time in this task, measures the interaction and\nindividual quality by computing the alignment between the mixed generated\nmotions and their conditions as well as the capabilities of MixerMDM to adapt\nthe mixing throughout the denoising process depending on the motions to mix.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-02.jsonl"}
{"id": "2503.24388", "pdf": "https://arxiv.org/pdf/2503.24388", "abs": "https://arxiv.org/abs/2503.24388", "authors": ["Zhonghan Zhao", "Wenwei Zhang", "Haian Huang", "Kuikun Liu", "Jianfei Gao", "Gaoang Wang", "Kai Chen"], "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
{"id": "2504.00772", "pdf": "https://arxiv.org/pdf/2504.00772", "abs": "https://arxiv.org/abs/2504.00772", "authors": ["TingJie Zhang", "HaiLin Liu"], "title": "Multi-Task Neural Architecture Search Using Architecture Embedding and Transfer Rank", "categories": ["cs.NE", "cs.CV"], "comment": null, "summary": "Multi-task neural architecture search (NAS) enables transferring\narchitectural knowledge among different tasks. However, ranking disorder\nbetween the source task and the target task degrades the architecture\nperformance on the downstream task. We propose KTNAS, an evolutionary\ncross-task NAS algorithm, to enhance transfer efficiency. Our data-agnostic\nmethod converts neural architectures into graphs and uses architecture\nembedding vectors for the subsequent architecture performance prediction. The\nconcept of transfer rank, an instance-based classifier, is introduced into\nKTNAS to address the performance degradation issue. We verify the search\nefficiency on NASBench-201 and transferability to various vision tasks on Micro\nTransNAS-Bench-101. The scalability of our method is demonstrated on DARTs\nsearch space including CIFAR-10/100, MNIST/Fashion-MNIST, MedMNIST.\nExperimental results show that KTNAS outperforms peer multi-task NAS algorithms\nin search efficiency and downstream task performance. Ablation studies\ndemonstrate the vital importance of transfer rank for transfer performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-02.jsonl"}
