{"id": "2504.01298", "pdf": "https://arxiv.org/pdf/2504.01298", "abs": "https://arxiv.org/abs/2504.01298", "authors": ["Shiyong Liu", "Zhihao Li", "Xiao Tang", "Jianzhuang Liu"], "title": "Direction-Aware Hybrid Representation Learning for 3D Hand Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop", "summary": "Most model-based 3D hand pose and shape estimation methods directly regress\nthe parametric model parameters from an image to obtain 3D joints under weak\nsupervision. However, these methods involve solving a complex optimization\nproblem with many local minima, making training difficult. To address this\nchallenge, we propose learning direction-aware hybrid features (DaHyF) that\nfuse implicit image features and explicit 2D joint coordinate features. This\nfusion is enhanced by the pixel direction information in the camera coordinate\nsystem to estimate pose, shape, and camera viewpoint. Our method directly\npredicts 3D hand poses with DaHyF representation and reduces jittering during\nmotion capture using prediction confidence based on contrastive learning. We\nevaluate our method on the FreiHAND dataset and show that it outperforms\nexisting state-of-the-art methods by more than 33% in accuracy. DaHyF also\nachieves the top ranking on both the HO3Dv2 and HO3Dv3 leaderboards for the\nmetric of Mean Joint Error (after scale and translation alignment). Compared to\nthe second-best results, the largest improvement observed is 10%. We also\ndemonstrate its effectiveness in real-time motion capture scenarios with hand\nposition variability, occlusion, and motion blur.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01274", "pdf": "https://arxiv.org/pdf/2504.01274", "abs": "https://arxiv.org/abs/2504.01274", "authors": ["Boseong Kim", "Debashis Das Chakladar", "Haejun Chung", "Ikbeom Jang"], "title": "BOLDSimNet: Examining Brain Network Similarity between Task and Resting-State fMRI", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "Traditional causal connectivity methods in task-based and resting-state\nfunctional magnetic resonance imaging (fMRI) face challenges in accurately\ncapturing directed information flow due to their sensitivity to noise and\ninability to model multivariate dependencies. These limitations hinder the\neffective comparison of brain networks between cognitive states, making it\ndifficult to analyze network reconfiguration during task and resting states. To\naddress these issues, we propose BOLDSimNet, a novel framework utilizing\nMultivariate Transfer Entropy (MTE) to measure causal connectivity and network\nsimilarity across different cognitive states. Our method groups functionally\nsimilar regions of interest (ROIs) rather than spatially adjacent nodes,\nimproving accuracy in network alignment. We applied BOLDSimNet to fMRI data\nfrom 40 healthy controls and found that children exhibited higher similarity\nscores between task and resting states compared to adolescents, indicating\nreduced variability in attention shifts. In contrast, adolescents showed more\ndifferences between task and resting states in the Dorsal Attention Network\n(DAN) and the Default Mode Network (DMN), reflecting enhanced network\nadaptability. These findings emphasize developmental variations in the\nreconfiguration of the causal brain network, showcasing BOLDSimNet's ability to\nquantify network similarity and identify attentional fluctuations between\ndifferent cognitive states.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01040", "pdf": "https://arxiv.org/pdf/2504.01040", "abs": "https://arxiv.org/abs/2504.01040", "authors": ["Ilir Tahiraj", "Jeremialie Swadiryus", "Felix Fent", "Markus Lienkamp"], "title": "Cal or No Cal? -- Real-Time Miscalibration Detection of LiDAR and Camera Sensors", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The goal of extrinsic calibration is the alignment of sensor data to ensure\nan accurate representation of the surroundings and enable sensor fusion\napplications. From a safety perspective, sensor calibration is a key enabler of\nautonomous driving. In the current state of the art, a trend from target-based\noffline calibration towards targetless online calibration can be observed.\nHowever, online calibration is subject to strict real-time and resource\nconstraints which are not met by state-of-the-art methods. This is mainly due\nto the high number of parameters to estimate, the reliance on geometric\nfeatures, or the dependence on specific vehicle maneuvers. To meet these\nrequirements and ensure the vehicle's safety at any time, we propose a\nmiscalibration detection framework that shifts the focus from the direct\nregression of calibration parameters to a binary classification of the\ncalibration state, i.e., calibrated or miscalibrated. Therefore, we propose a\ncontrastive learning approach that compares embedded features in a latent space\nto classify the calibration state of two different sensor modalities. Moreover,\nwe provide a comprehensive analysis of the feature embeddings and challenging\ncalibration errors that highlight the performance of our approach. As a result,\nour method outperforms the current state-of-the-art in terms of detection\nperformance, inference time, and resource demand. The code is open source and\navailable on https://github.com/TUMFTM/MiscalibrationDetection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01049", "pdf": "https://arxiv.org/pdf/2504.01049", "abs": "https://arxiv.org/abs/2504.01049", "authors": ["Bingxin Li"], "title": "SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual Question Answering", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal models integrating speech and vision hold significant potential\nfor advancing human-computer interaction, particularly in Speech-Based Visual\nQuestion Answering (SBVQA) where spoken questions about images require direct\naudio-visual understanding. Existing approaches predominantly focus on\ntext-visual integration, leaving speech-visual modality gaps underexplored due\nto their inherent heterogeneity. To this end, we introduce SViQA, a unified\nspeech-vision model that directly processes spoken questions without text\ntranscription. Building upon the LLaVA architecture, our framework bridges\nauditory and visual modalities through two key innovations: (1) end-to-end\nspeech feature extraction eliminating intermediate text conversion, and (2)\ncross-modal alignment optimization enabling effective fusion of speech signals\nwith visual content. Extensive experimental results on the SBVQA benchmark\ndemonstrate the proposed SViQA's state-of-the-art performance, achieving 75.62%\naccuracy, and competitive multimodal generalization. Leveraging speech-text\nmixed input boosts performance to 78.85%, a 3.23% improvement over pure speech\ninput, highlighting SViQA's enhanced robustness and effective cross-modal\nattention alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01296", "pdf": "https://arxiv.org/pdf/2504.01296", "abs": "https://arxiv.org/abs/2504.01296", "authors": ["Bairu Hou", "Yang Zhang", "Jiabao Ji", "Yujian Liu", "Kaizhi Qian", "Jacob Andreas", "Shiyu Chang"], "title": "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning", "categories": ["cs.CL"], "comment": "15 pages, 7 figures", "summary": "We present ThinkPrune, a simple yet effective method for pruning the thinking\nlength for long-thinking LLMs, which has been found to often produce\ninefficient and redundant thinking processes. Existing preliminary explorations\nof reducing thinking length primarily focus on forcing the thinking process to\nearly exit, rather than adapting the LLM to optimize and consolidate the\nthinking process, and therefore the length-performance tradeoff observed so far\nis sub-optimal. To fill this gap, ThinkPrune offers a simple solution that\ncontinuously trains the long-thinking LLMs via reinforcement learning (RL) with\nan added token limit, beyond which any unfinished thoughts and answers will be\ndiscarded, resulting in a zero reward. To further preserve model performance,\nwe introduce an iterative length pruning approach, where multiple rounds of RL\nare conducted, each with an increasingly more stringent token limit. We\nobserved that ThinkPrune results in a remarkable performance-length tradeoff --\non the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B\ncan be reduced by half with only 2% drop in performance. We also observed that\nafter pruning, the LLMs can bypass unnecessary steps while keeping the core\nreasoning process complete. Code is available at\nhttps://github.com/UCSB-NLP-Chang/ThinkPrune.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01317", "pdf": "https://arxiv.org/pdf/2504.01317", "abs": "https://arxiv.org/abs/2504.01317", "authors": ["Zhendong Tan", "Xingjun Zhang", "Chaoyi Hu", "Yancheng Pan", "Shaoxun Wang"], "title": "Adaptive Rectification Sampling for Test-Time Compute Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The newly released OpenAI-o1 and DeepSeek-R1 have demonstrated that test-time\nscaling can significantly improve model performance, especially in complex\ntasks such as logical reasoning. Common test-time scaling methods involve\ngenerating more chain of thoughts (CoTs) or longer CoTs with self-correction.\nHowever, while self-correction can improve performance, it may lead to\nsignificant token waste and reduce readability of the CoT if the reasoning\nsteps are already correct. To demonstrate that large language models (LLMs) can\nrectify errors at a more fine-grained level, we propose Adaptive Rectification\nSampling (AR-Sampling), which can guide the LLMs to self-correction at the\nappropriate step. AR-Sampling leverages a process-supervised reward model (PRM)\nas a verifier and constructed trigger sentences to guide the model in adaptive\nstep-level rethinking. Through the experiments on GSM8K and MATH500, it\nindicate that our approach enables the models to rethink in more fine-grained\nlevel, improving the accuracy of solutions, while generating a reasonable\nnumber of additional tokens.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "compute scaling", "test-time compute", "o1", "self-correction"], "score": 6}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01342", "pdf": "https://arxiv.org/pdf/2504.01342", "abs": "https://arxiv.org/abs/2504.01342", "authors": ["Jungyeul Park"], "title": "Foundations and Evaluations in NLP", "categories": ["cs.CL"], "comment": null, "summary": "This memoir explores two fundamental aspects of Natural Language Processing\n(NLP): the creation of linguistic resources and the evaluation of NLP system\nperformance. Over the past decade, my work has focused on developing a\nmorpheme-based annotation scheme for the Korean language that captures\nlinguistic properties from morphology to semantics. This approach has achieved\nstate-of-the-art results in various NLP tasks, including part-of-speech\ntagging, dependency parsing, and named entity recognition. Additionally, this\nwork provides a comprehensive analysis of segmentation granularity and its\ncritical impact on NLP system performance. In parallel with linguistic resource\ndevelopment, I have proposed a novel evaluation framework, the jp-algorithm,\nwhich introduces an alignment-based method to address challenges in\npreprocessing tasks like tokenization and sentence boundary detection (SBD).\nTraditional evaluation methods assume identical tokenization and sentence\nlengths between gold standards and system outputs, limiting their applicability\nto real-world data. The jp-algorithm overcomes these limitations, enabling\nrobust end-to-end evaluations across a variety of NLP tasks. It enhances\naccuracy and flexibility by incorporating linear-time alignment while\npreserving the complexity of traditional evaluation metrics. This memoir\nprovides key insights into the processing of morphologically rich languages,\nsuch as Korean, while offering a generalizable framework for evaluating diverse\nend-to-end NLP systems. My contributions lay the foundation for future\ndevelopments, with broader implications for multilingual resource development\nand system evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01220", "pdf": "https://arxiv.org/pdf/2504.01220", "abs": "https://arxiv.org/abs/2504.01220", "authors": ["Banafsheh Adami", "Nima Karimian"], "title": "rPPG-SysDiaGAN: Systolic-Diastolic Feature Localization in rPPG Using Generative Adversarial Network with Multi-Domain Discriminator", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) offers a novel approach to noninvasive\nmonitoring of vital signs, such as respiratory rate, utilizing a camera.\nAlthough several supervised and self-supervised methods have been proposed,\nthey often fail to accurately reconstruct the PPG signal, particularly in\ndistinguishing between systolic and diastolic components. Their primary focus\ntends to be solely on extracting heart rate, which may not accurately represent\nthe complete PPG signal. To address this limitation, this paper proposes a\nnovel deep learning architecture using Generative Adversarial Networks by\nintroducing multi-discriminators to extract rPPG signals from facial videos.\nThese discriminators focus on the time domain, the frequency domain, and the\nsecond derivative of the original time domain signal. The discriminator\nintegrates four loss functions: variance loss to mitigate local minima caused\nby noise; dynamic time warping loss to address local minima induced by\nalignment and sequences of variable lengths; Sparsity Loss for heart rate\nadjustment, and Variance Loss to ensure a uniform distribution across the\ndesired frequency domain and time interval between systolic and diastolic\nphases of the PPG signal.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01420", "pdf": "https://arxiv.org/pdf/2504.01420", "abs": "https://arxiv.org/abs/2504.01420", "authors": ["Athena Wen", "Tanush Patil", "Ansh Saxena", "Yicheng Fu", "Sean O'Brien", "Kevin Zhu"], "title": "FAIRE: Assessing Racial and Gender Bias in AI-Driven Resume Evaluations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In an era where AI-driven hiring is transforming recruitment practices,\nconcerns about fairness and bias have become increasingly important. To explore\nthese issues, we introduce a benchmark, FAIRE (Fairness Assessment In Resume\nEvaluation), to test for racial and gender bias in large language models (LLMs)\nused to evaluate resumes across different industries. We use two methods-direct\nscoring and ranking-to measure how model performance changes when resumes are\nslightly altered to reflect different racial or gender identities. Our findings\nreveal that while every model exhibits some degree of bias, the magnitude and\ndirection vary considerably. This benchmark provides a clear way to examine\nthese differences and offers valuable insights into the fairness of AI-based\nhiring tools. It highlights the urgent need for strategies to reduce bias in\nAI-driven recruitment. Our benchmark code and dataset are open-sourced at our\nrepository:\nhttps://github.com/athenawen/FAIRE-Fairness-Assessment-In-Resume-Evaluation.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01321", "pdf": "https://arxiv.org/pdf/2504.01321", "abs": "https://arxiv.org/abs/2504.01321", "authors": ["Chunhui Zhang", "Li Liu", "Jialin Gao", "Xin Sun", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint submitted to Elsevier.\n  https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Transformer has recently demonstrated great potential in improving\nvision-language (VL) tracking algorithms. However, most of the existing VL\ntrackers rely on carefully designed mechanisms to perform the multi-stage\nmulti-modal fusion. Additionally, direct multi-modal fusion without alignment\nignores distribution discrepancy between modalities in feature space,\npotentially leading to suboptimal representations. In this work, we propose\nCOST, a contrastive one-stage transformer fusion framework for VL tracking,\naiming to learn semantically consistent and unified VL representations.\nSpecifically, we introduce a contrastive alignment strategy that maximizes\nmutual information (MI) between a video and its corresponding language\ndescription. This enables effective cross-modal alignment, yielding\nsemantically consistent features in the representation space. By leveraging a\nvisual-linguistic transformer, we establish an efficient multi-modal fusion and\nreasoning mechanism, empirically demonstrating that a simple stack of\ntransformer encoders effectively enables unified VL representations. Moreover,\nwe contribute a newly collected VL tracking benchmark dataset for small object\ntracking, named VL-SOT500, with bounding boxes and language descriptions. Our\ndataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated\nto evaluating generic and high-speed small object tracking, respectively. Small\nobject tracking is notoriously challenging due to weak appearance and limited\nfeatures, and this dataset is, to the best of our knowledge, the first to\nexplore the usage of language cues to enhance visual representation for small\nobject tracking. Extensive experiments demonstrate that COST achieves\nstate-of-the-art performance on five existing VL tracking datasets, as well as\non our proposed VL-SOT500 dataset. Source codes and dataset will be made\npublicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01386", "pdf": "https://arxiv.org/pdf/2504.01386", "abs": "https://arxiv.org/abs/2504.01386", "authors": ["Junjie Wu", "Jiangtao Xie", "Zhaolin Zhang", "Qilong Wang", "Qinghua Hu", "Peihua Li", "Sen Xu"], "title": "DALIP: Distribution Alignment-based Language-Image Pre-Training for Domain-Specific Data", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Recently, Contrastive Language-Image Pre-training (CLIP) has shown promising\nperformance in domain-specific data (e.g., biology), and has attracted\nincreasing research attention. Existing works generally focus on collecting\nextensive domain-specific data and directly tuning the original CLIP models.\nIntuitively, such a paradigm takes no full consideration of the characteristics\nlying in domain-specific data (e.g., fine-grained nature of biological data)\nand so limits model capability, while mostly losing the original ability of\nCLIP in the general domain. In this paper, we propose a Distribution\nAlignment-based Language-Image Pre-Training (DALIP) method for biological data.\nSpecifically, DALIP optimizes CLIP models by matching the similarity between\nfeature distribution of image-text pairs instead of the original [cls] token,\nwhich can capture rich yet effective information inherent in image-text pairs\nas powerful representations, and so better cope with fine-grained nature of\nbiological data. Particularly, our DALIP efficiently approximates feature\ndistribution via its first- and second-order statistics, while presenting a\nMulti-head Brownian Distance Covariance (MBDC) module to acquire second-order\nstatistics of token features efficiently. Furthermore, we collect a new dataset\nfor plant domain (e.g., specific data in biological domain) comprising 10M\nplant data with 3M general-domain data (namely PlantMix-13M) according to data\nmixing laws. Extensive experiments show that DALIP clearly outperforms existing\nCLIP counterparts in biological domain, while well generalizing to remote\nsensing and medical imaging domains. Besides, our PlantMix-13M dataset further\nboosts performance of DALIP in plant domain, while preserving model ability in\ngeneral domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01698", "pdf": "https://arxiv.org/pdf/2504.01698", "abs": "https://arxiv.org/abs/2504.01698", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Jiajun Song", "Lifeng Fan", "Wei Wang"], "title": "ToM-RL: Reinforcement Learning Unlocks Theory of Mind in Small LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in rule-based reinforcement learning (RL), applied during\nthe post-training phase of large language models (LLMs), have significantly\nenhanced their capabilities in structured reasoning tasks such as mathematics\nand logical inference. However, the effectiveness of RL in social reasoning,\nparticularly in Theory of Mind (ToM), the ability to infer others' mental\nstates, remains largely unexplored. In this study, we demonstrate that RL\nmethods effectively unlock ToM reasoning capabilities even in small-scale LLMs\n(0.5B to 7B parameters). Using a modest dataset comprising 3200 questions\nacross diverse scenarios, our RL-trained 7B model achieves 84.50\\% accuracy on\nthe Hi-ToM benchmark, surpassing models like GPT-4o and DeepSeek-v3 despite\nsignificantly fewer parameters. While smaller models ($\\leq$3B parameters)\nsuffer from reasoning collapse, larger models (7B parameters) maintain stable\nperformance through consistent belief tracking. Additionally, our RL-based\nmodels demonstrate robust generalization to higher-order, out-of-distribution\nToM problems, novel textual presentations, and previously unseen datasets.\nThese findings highlight RL's potential to enhance social cognitive reasoning,\nbridging the gap between structured problem-solving and nuanced social\ninference in LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01428", "pdf": "https://arxiv.org/pdf/2504.01428", "abs": "https://arxiv.org/abs/2504.01428", "authors": ["Zhuangzhuang Chen", "Hualiang Wang", "Chubin Ou", "Xiaomeng Li"], "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical coherence tomography angiography (OCTA) shows its great importance in\nimaging microvascular networks by providing accurate 3D imaging of blood\nvessels, but it relies upon specialized sensors and expensive devices. For this\nreason, previous works show the potential to translate the readily available 3D\nOptical Coherence Tomography (OCT) images into 3D OCTA images. However,\nexisting OCTA translation methods directly learn the mapping from the OCT\ndomain to the OCTA domain in continuous and infinite space with guidance from\nonly a single view, i.e., the OCTA project map, resulting in suboptimal\nresults. To this end, we propose the multi-view Tri-alignment framework for OCT\nto OCTA 3D image translation in discrete and finite space, named MuTri. In the\nfirst stage, we pre-train two vector-quantized variational auto-encoder (VQ-\nVAE) by reconstructing 3D OCT and 3D OCTA data, providing semantic prior for\nsubsequent multi-view guidances. In the second stage, our multi-view\ntri-alignment facilitates another VQVAE model to learn the mapping from the OCT\ndomain to the OCTA domain in discrete and finite space. Specifically, a\ncontrastive-inspired semantic alignment is proposed to maximize the mutual\ninformation with the pre-trained models from OCT and OCTA views, to facilitate\ncodebook learning. Meanwhile, a vessel structure alignment is proposed to\nminimize the structure discrepancy with the pre-trained models from the OCTA\nproject map view, benefiting from learning the detailed vessel structure\ninformation. We also collect the first large-scale dataset, namely, OCTA2024,\nwhich contains a pair of OCT and OCTA volumes from 846 subjects.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01801", "pdf": "https://arxiv.org/pdf/2504.01801", "abs": "https://arxiv.org/abs/2504.01801", "authors": ["Zhijun Wang", "Jiahuan Li", "Hao Zhou", "Rongxiang Weng", "Jingang Wang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Shujian Huang"], "title": "Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable multilingual capabilities\ndespite the extreme language imbalance in the pre-training data. In this paper,\nwe closely examine the reasons behind this phenomenon, focusing on the\npre-training corpus. We find that the existence of code-switching, alternating\nbetween different languages within a context, is key to multilingual\ncapabilities. We conduct an analysis to investigate code-switching in the\npre-training corpus, examining its presence and categorizing it into four types\nwithin two quadrants. We then assess its impact on multilingual performance.\nThese types of code-switching data are unbalanced in proportions and\ndemonstrate different effects on facilitating language transfer. To better\nexplore the power of code-switching for language alignment during pre-training,\nwe investigate the strategy of synthetic code-switching. We continuously scale\nup the synthetic code-switching data and observe remarkable improvements in\nboth benchmarks and representation space. Extensive experiments indicate that\nincorporating synthetic code-switching data enables better language alignment\nand generalizes well to high, medium, and low-resource languages with\npre-training corpora of varying qualities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01452", "pdf": "https://arxiv.org/pdf/2504.01452", "abs": "https://arxiv.org/abs/2504.01452", "authors": ["Encheng Su", "Hu Cao", "Alois Knoll"], "title": "BiSeg-SAM: Weakly-Supervised Post-Processing Framework for Boosting Binary Segmentation in Segment Anything Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Bioinformatics and Biomedicine\n  (BIBM)", "summary": "Accurate segmentation of polyps and skin lesions is essential for diagnosing\ncolorectal and skin cancers. While various segmentation methods for polyps and\nskin lesions using fully supervised deep learning techniques have been\ndeveloped, the pixel-level annotation of medical images by doctors is both\ntime-consuming and costly. Foundational vision models like the Segment Anything\nModel (SAM) have demonstrated superior performance; however, directly applying\nSAM to medical segmentation may not yield satisfactory results due to the lack\nof domain-specific medical knowledge. In this paper, we propose BiSeg-SAM, a\nSAM-guided weakly supervised prompting and boundary refinement network for the\nsegmentation of polyps and skin lesions. Specifically, we fine-tune SAM\ncombined with a CNN module to learn local features. We introduce a WeakBox with\ntwo functions: automatically generating box prompts for the SAM model and using\nour proposed Multi-choice Mask-to-Box (MM2B) transformation for rough\nmask-to-box conversion, addressing the mismatch between coarse labels and\nprecise predictions. Additionally, we apply scale consistency (SC) loss for\nprediction scale alignment. Our DetailRefine module enhances boundary precision\nand segmentation accuracy by refining coarse predictions using a limited amount\nof ground truth labels. This comprehensive approach enables BiSeg-SAM to\nachieve excellent multi-task segmentation performance. Our method demonstrates\nsignificant superiority over state-of-the-art (SOTA) methods when tested on\nfive polyp datasets and one skin cancer dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01476", "pdf": "https://arxiv.org/pdf/2504.01476", "abs": "https://arxiv.org/abs/2504.01476", "authors": ["Junlong Ren", "Hao Wang"], "title": "Enhanced Cross-modal 3D Retrieval via Tri-modal Reconstruction", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "Cross-modal 3D retrieval is a critical yet challenging task, aiming to\nachieve bi-directional retrieval between 3D and text modalities. Current\nmethods predominantly rely on a certain 3D representation (e.g., point cloud),\nwith few exploiting the 2D-3D consistency and complementary relationships,\nwhich constrains their performance. To bridge this gap, we propose to adopt\nmulti-view images and point clouds to jointly represent 3D shapes, facilitating\ntri-modal alignment (i.e., image, point, text) for enhanced cross-modal 3D\nretrieval. Notably, we introduce tri-modal reconstruction to improve the\ngeneralization ability of encoders. Given point features, we reconstruct image\nfeatures under the guidance of text features, and vice versa. With well-aligned\npoint cloud and multi-view image features, we aggregate them as multimodal\nembeddings through fine-grained 2D-3D fusion to enhance geometric and semantic\nunderstanding. Recognizing the significant noise in current datasets where many\n3D shapes and texts share similar semantics, we employ hard negative\ncontrastive training to emphasize harder negatives with greater significance,\nleading to robust discriminative embeddings. Extensive experiments on the\nText2Shape dataset demonstrate that our method significantly outperforms\nprevious state-of-the-art methods in both shape-to-text and text-to-shape\nretrieval tasks by a substantial margin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01903", "pdf": "https://arxiv.org/pdf/2504.01903", "abs": "https://arxiv.org/abs/2504.01903", "authors": ["Zijun Wang", "Haoqin Tu", "Yuhan Wang", "Juncheng Wu", "Jieru Mei", "Brian R. Bartoldson", "Bhavya Kailkhura", "Cihang Xie"], "title": "STAR-1: Safer Alignment of Reasoning LLMs with 1K Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset\nspecifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built\non three core principles -- diversity, deliberative reasoning, and rigorous\nfiltering -- STAR-1 aims to address the critical needs for safety alignment in\nLRMs. Specifically, we begin by integrating existing open-source safety\ndatasets from diverse sources. Then, we curate safety policies to generate\npolicy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based\nsafety scoring system to select training examples aligned with best practices.\nExperimental results show that fine-tuning LRMs with STAR-1 leads to an average\n40% improvement in safety performance across four benchmarks, while only\nincurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability\nmeasured across five reasoning tasks. Extensive ablation studies further\nvalidate the importance of our design principles in constructing STAR-1 and\nanalyze its efficacy across both LRMs and traditional LLMs. Our project page is\nhttps://ucsc-vlaa.github.io/STAR-1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01515", "pdf": "https://arxiv.org/pdf/2504.01515", "abs": "https://arxiv.org/abs/2504.01515", "authors": ["Zixuan Wang", "Duo Peng", "Feng Chen", "Yuwei Yang", "Yinjie Lei"], "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Conditional image synthesis is a crucial task with broad applications, such\nas artistic creation and virtual reality. However, current generative methods\nare often task-oriented with a narrow scope, handling a restricted condition\nwith constrained applicability. In this paper, we propose a novel approach that\ntreats conditional image synthesis as the modular combination of diverse\nfundamental condition units. Specifically, we divide conditions into three\nprimary units: text, layout, and drag. To enable effective control over these\nconditions, we design a dedicated alignment module for each. For the text\ncondition, we introduce a Dense Concept Alignment (DCA) module, which achieves\ndense visual-text alignment by drawing on diverse textual concepts. For the\nlayout condition, we propose a Dense Geometry Alignment (DGA) module to enforce\ncomprehensive geometric constraints that preserve the spatial configuration.\nFor the drag condition, we introduce a Dense Motion Alignment (DMA) module to\napply multi-level motion regularization, ensuring that each pixel follows its\ndesired trajectory without visual artifacts. By flexibly inserting and\ncombining these alignment modules, our framework enhances the model's\nadaptability to diverse conditional generation tasks and greatly expands its\napplication range. Extensive experiments demonstrate the superior performance\nof our framework across a variety of conditions, including textual description,\nsegmentation mask (bounding box), drag manipulation, and their combinations.\nCode is available at https://github.com/ZixuanWang0525/DADG.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01931", "pdf": "https://arxiv.org/pdf/2504.01931", "abs": "https://arxiv.org/abs/2504.01931", "authors": ["Souradip Chakraborty", "Mohammadreza Pourreza", "Ruoxi Sun", "Yiwen Song", "Nino Scherrer", "Jindong Gu", "Furong Huang", "Amrit Singh Bedi", "Ahmad Beirami", "Hamid Palangi", "Tomas Pfister"], "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection", "categories": ["cs.CL"], "comment": null, "summary": "While AI agents have shown remarkable performance at various tasks, they\nstill struggle with complex multi-modal applications, structured generation and\nstrategic planning. Improvements via standard fine-tuning is often impractical,\nas solving agentic tasks usually relies on black box API access without control\nover model parameters. Inference-time methods such as Best-of-N (BON) sampling\noffer a simple yet effective alternative to improve performance. However, BON\nlacks iterative feedback integration mechanism. Hence, we propose Iterative\nAgent Decoding (IAD) which combines iterative refinement with dynamic candidate\nevaluation and selection guided by a verifier. IAD differs in how feedback is\ndesigned and integrated, specifically optimized to extract maximal signal from\nreward scores. We conduct a detailed comparison of baselines across key metrics\non Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms\nbaselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and\nwithout LLM judges) and 8--10% gains on Webshop across multiple metrics. To\nbetter understand the source of IAD's gains, we perform controlled experiments\nto disentangle the effect of adaptive feedback from stochastic sampling, and\nfind that IAD's improvements are primarily driven by verifier-guided\nrefinement, not merely sampling diversity. We also show that both IAD and BON\nexhibit inference-time scaling with increased compute when guided by an optimal\nverifier. Our analysis highlights the critical role of verifier quality in\neffective inference-time optimization and examines the impact of noisy and\nsparse rewards on scaling behavior. Together, these findings offer key insights\ninto the trade-offs and principles of effective inference-time optimization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "iterative refinement"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01943", "pdf": "https://arxiv.org/pdf/2504.01943", "abs": "https://arxiv.org/abs/2504.01943", "authors": ["Wasi Uddin Ahmad", "Sean Narenthiran", "Somshubra Majumdar", "Aleksander Ficek", "Siddhartha Jain", "Jocelyn Huang", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Since the advent of reasoning-based large language models, many have found\ngreat success from distilling reasoning capabilities into student models. Such\ntechniques have significantly bridged the gap between reasoning and standard\nLLMs on coding tasks. Despite this, much of the progress on distilling\nreasoning models remains locked behind proprietary datasets or lacks details on\ndata curation, filtering and subsequent training. To address this, we construct\na superior supervised fine-tuning (SFT) dataset that we use to achieve\nstate-of-the-art coding capability results in models of various sizes. Our\ndistilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on\nCodeContests, surpassing alternatives trained with reinforcement learning. We\nthen perform analysis on the data sources used to construct our dataset, the\nimpact of code execution filtering, and the importance of instruction/solution\ndiversity. We observe that execution filtering negatively affected benchmark\naccuracy, leading us to prioritize instruction diversity over solution\ncorrectness. Finally, we also analyze the token efficiency and reasoning\npatterns utilized by these models. We will open-source these datasets and\ndistilled models to the community.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01591", "pdf": "https://arxiv.org/pdf/2504.01591", "abs": "https://arxiv.org/abs/2504.01591", "authors": ["Adriano Fragomeni", "Dima Damen", "Michael Wray"], "title": "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Video retrieval requires aligning visual content with corresponding natural\nlanguage descriptions. In this paper, we introduce Modality Auxiliary Concepts\nfor Video Retrieval (MAC-VR), a novel approach that leverages modality-specific\ntags -- automatically extracted from foundation models -- to enhance video\nretrieval. We propose to align modalities in a latent space, along with\nlearning and aligning auxiliary latent concepts, derived from the features of a\nvideo and its corresponding caption. We introduce these auxiliary concepts to\nimprove the alignment of visual and textual latent concepts, and so are able to\ndistinguish concepts from one other. We conduct extensive experiments on five\ndiverse datasets: MSR-VTT, DiDeMo, TGIF, Charades and YouCook2. The\nexperimental results consistently demonstrate that modality-specific tags\nimprove cross-modal alignment, outperforming current state-of-the-art methods\nacross three datasets and performing comparably or better across the other two.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01205", "pdf": "https://arxiv.org/pdf/2504.01205", "abs": "https://arxiv.org/abs/2504.01205", "authors": ["Nicholas Clark", "Hua Shen", "Bill Howe", "Tanushree Mitra"], "title": "Epistemic Alignment: A Mediating Framework for User-LLM Knowledge Delivery", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "LLMs increasingly serve as tools for knowledge acquisition, yet users cannot\neffectively specify how they want information presented. When users request\nthat LLMs \"cite reputable sources,\" \"express appropriate uncertainty,\" or\n\"include multiple perspectives,\" they discover that current interfaces provide\nno structured way to articulate these preferences. The result is prompt sharing\nfolklore: community-specific copied prompts passed through trust relationships\nrather than based on measured efficacy. We propose the Epistemic Alignment\nFramework, a set of ten challenges in knowledge transmission derived from the\nphilosophical literature of epistemology, concerning issues such as evidence\nquality assessment and calibration of testimonial reliance. The framework\nserves as a structured intermediary between user needs and system capabilities,\ncreating a common vocabulary to bridge the gap between what users want and what\nsystems deliver. Through a thematic analysis of custom prompts and\npersonalization strategies shared on online communities where these issues are\nactively discussed, we find users develop elaborate workarounds to address each\nof the challenges. We then apply our framework to two prominent model\nproviders, OpenAI and Anthropic, through content analysis of their documented\npolicies and product features. Our analysis shows that while these providers\nhave partially addressed the challenges we identified, they fail to establish\nadequate mechanisms for specifying epistemic preferences, lack transparency\nabout how preferences are implemented, and offer no verification tools to\nconfirm whether preferences were followed. For AI developers, the Epistemic\nAlignment Framework offers concrete guidance for supporting diverse approaches\nto knowledge; for users, it works toward information delivery that aligns with\ntheir specific needs rather than defaulting to one-size-fits-all approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01403", "pdf": "https://arxiv.org/pdf/2504.01403", "abs": "https://arxiv.org/abs/2504.01403", "authors": ["Ming Pang", "Chunyuan Yuan", "Xiaoyu He", "Zheng Fang", "Donghao Xie", "Fanyi Qu", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Zheng Luo", "Jingping Shao"], "title": "Generative Retrieval and Alignment Model: A New Paradigm for E-commerce Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by WWW2025", "summary": "Traditional sparse and dense retrieval methods struggle to leverage general\nworld knowledge and often fail to capture the nuanced features of queries and\nproducts. With the advent of large language models (LLMs), industrial search\nsystems have started to employ LLMs to generate identifiers for product\nretrieval. Commonly used identifiers include (1) static/semantic IDs and (2)\nproduct term sets. The first approach requires creating a product ID system\nfrom scratch, missing out on the world knowledge embedded within LLMs. While\nthe second approach leverages this general knowledge, the significant\ndifference in word distribution between queries and products means that\nproduct-based identifiers often do not align well with user search queries,\nleading to missed product recalls. Furthermore, when queries contain numerous\nattributes, these algorithms generate a large number of identifiers, making it\ndifficult to assess their quality, which results in low overall recall\nefficiency.\n  To address these challenges, this paper introduces a novel e-commerce\nretrieval paradigm: the Generative Retrieval and Alignment Model (GRAM). GRAM\nemploys joint training on text information from both queries and products to\ngenerate shared text identifier codes, effectively bridging the gap between\nqueries and products. This approach not only enhances the connection between\nqueries and products but also improves inference efficiency. The model uses a\nco-alignment strategy to generate codes optimized for maximizing retrieval\nefficiency. Additionally, it introduces a query-product scoring mechanism to\ncompare product values across different codes, further boosting retrieval\nefficiency. Extensive offline and online A/B testing demonstrates that GRAM\nsignificantly outperforms traditional models and the latest generative\nretrieval models, confirming its effectiveness and practicality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01641", "pdf": "https://arxiv.org/pdf/2504.01641", "abs": "https://arxiv.org/abs/2504.01641", "authors": ["Zhixin Cheng", "Jiacheng Deng", "Xinjun Li", "Baoqun Yin", "Tianzhu Zhang"], "title": "Bridge 2D-3D: Uncertainty-aware Hierarchical Registration Network with Domain Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI2025accept", "summary": "The method for image-to-point cloud registration typically determines the\nrigid transformation using a coarse-to-fine pipeline. However, directly and\nuniformly matching image patches with point cloud patches may lead to focusing\non incorrect noise patches during matching while ignoring key ones. Moreover,\ndue to the significant differences between image and point cloud modalities, it\nmay be challenging to bridge the domain gap without specific improvements in\ndesign. To address the above issues, we innovatively propose the\nUncertainty-aware Hierarchical Matching Module (UHMM) and the Adversarial Modal\nAlignment Module (AMAM). Within the UHMM, we model the uncertainty of critical\ninformation in image patches and facilitate multi-level fusion interactions\nbetween image and point cloud features. In the AMAM, we design an adversarial\napproach to reduce the domain gap between image and point cloud. Extensive\nexperiments and ablation studies on RGB-D Scene V2 and 7-Scenes benchmarks\ndemonstrate the superiority of our method, making it a state-of-the-art\napproach for image-to-point cloud registration tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01550", "pdf": "https://arxiv.org/pdf/2504.01550", "abs": "https://arxiv.org/abs/2504.01550", "authors": ["Ashkan Yousefpour", "Taeheon Kim", "Ryan S. Kwon", "Seungbeen Lee", "Wonje Jeung", "Seungju Han", "Alvin Wan", "Harrison Ngan", "Youngjae Yu", "Jonghyun Choi"], "title": "Representation Bending for Large Language Model Safety", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools, but their\ninherent safety risks - ranging from harmful content generation to broader\nsocietal harms - pose significant challenges. These risks can be amplified by\nthe recent adversarial attacks, fine-tuning vulnerabilities, and the increasing\ndeployment of LLMs in high-stakes environments. Existing safety-enhancing\ntechniques, such as fine-tuning with human feedback or adversarial training,\nare still vulnerable as they address specific threats and often fail to\ngeneralize across unseen attacks, or require manual system-level defenses. This\npaper introduces RepBend, a novel approach that fundamentally disrupts the\nrepresentations underlying harmful behaviors in LLMs, offering a scalable\nsolution to enhance (potentially inherent) safety. RepBend brings the idea of\nactivation steering - simple vector arithmetic for steering model's behavior\nduring inference - to loss-based fine-tuning. Through extensive evaluation,\nRepBend achieves state-of-the-art performance, outperforming prior methods such\nas Circuit Breaker, RMU, and NPO, with up to 95% reduction in attack success\nrates across diverse jailbreak benchmarks, all with negligible reduction in\nmodel usability and general capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01627", "pdf": "https://arxiv.org/pdf/2504.01627", "abs": "https://arxiv.org/abs/2504.01627", "authors": ["Lena Schmidt", "Oshin Sharma", "Chris Marshall", "Sonia Garcia Gonzalez Moral"], "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Introduction: Horizon scanning in healthcare assesses early signals of\ninnovation, crucial for timely adoption. Current horizon scanning faces\nchallenges in efficient information retrieval and analysis, especially from\nunstructured sources like news, presenting a need for innovative tools.\nMethodology: The study introduces SCANAR and AIDOC, open-source Python-based\ntools designed to improve horizon scanning. SCANAR automates the retrieval and\nprocessing of news articles, offering functionalities such as de-duplication\nand unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to\nreorder textual data based on relevancy, employing neural networks for semantic\nsimilarity, and subsequently prioritizing likely relevant entries for human\nreview. Results: Twelve internal datasets from horizon scans and four external\nbenchmarking datasets were used. SCANAR improved retrieval efficiency by\nautomating processes previously dependent on manual labour. AIDOC displayed\nwork-saving potential, achieving around 62% reduction in manual review efforts\nat 95% recall. Comparative analysis with benchmarking data showed AIDOC's\nperformance was similar to existing systematic review automation tools, though\nperformance varied depending on dataset characteristics. A smaller case-study\non our news datasets shows the potential of ensembling large language models\nwithin the active-learning process for faster detection of relevant articles\nacross news datasets. Conclusion: The validation indicates that SCANAR and\nAIDOC show potential to enhance horizon scanning efficiency by streamlining\ndata retrieval and prioritisation. These tools may alleviate methodological\nlimitations and allow broader, swifter horizon scans. Further studies are\nsuggested to optimize these models and to design new workflows and validation\nprocesses that integrate large language models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01818", "pdf": "https://arxiv.org/pdf/2504.01818", "abs": "https://arxiv.org/abs/2504.01818", "authors": ["Sean MacAvaney", "Antonio Mallia", "Nicola Tonellotto"], "title": "Efficient Constant-Space Multi-Vector Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "ECIR 2025", "summary": "Multi-vector retrieval methods, exemplified by the ColBERT architecture, have\nshown substantial promise for retrieval by providing strong trade-offs in terms\nof retrieval latency and effectiveness. However, they come at a high cost in\nterms of storage since a (potentially compressed) vector needs to be stored for\nevery token in the input collection. To overcome this issue, we propose\nencoding documents to a fixed number of vectors, which are no longer\nnecessarily tied to the input tokens. Beyond reducing the storage costs, our\napproach has the advantage that document representations become of a fixed size\non disk, allowing for better OS paging management. Through experiments using\nthe MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a\nrepresentative multi-vector ranking model architecture, we find that passages\ncan be effectively encoded into a fixed number of vectors while retaining most\nof the original effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01668", "pdf": "https://arxiv.org/pdf/2504.01668", "abs": "https://arxiv.org/abs/2504.01668", "authors": ["Junjie Chen", "Yuecong Xu", "Haosheng Li", "Kemi Ding"], "title": "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages,6 figures", "summary": "3D point cloud semantic segmentation (PCSS) is a cornerstone for\nenvironmental perception in robotic systems and autonomous driving, enabling\nprecise scene understanding through point-wise classification. While\nunsupervised domain adaptation (UDA) mitigates label scarcity in PCSS, existing\nmethods critically overlook the inherent vulnerability to real-world\nperturbations (e.g., snow, fog, rain) and adversarial distortions. This work\nfirst identifies two intrinsic limitations that undermine current PCSS-UDA\nrobustness: (a) unsupervised features overlap from unaligned boundaries in\nshared-class regions and (b) feature structure erosion caused by\ndomain-invariant learning that suppresses target-specific patterns. To address\nthe proposed problems, we propose a tripartite framework consisting of: 1) a\nrobustness evaluation model quantifying resilience against adversarial\nattack/corruption types through robustness metrics; 2) an invertible attention\nalignment module (IAAM) enabling bidirectional domain mapping while preserving\ndiscriminative structure via attention-guided overlap suppression; and 3) a\ncontrastive memory bank with quality-aware contrastive learning that\nprogressively refines pseudo-labels with feature quality for more\ndiscriminative representations. Extensive experiments on\nSynLiDAR-to-SemanticPOSS adaptation demonstrate a maximum mIoU improvement of\n14.3\\% under adversarial attack.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01916", "pdf": "https://arxiv.org/pdf/2504.01916", "abs": "https://arxiv.org/abs/2504.01916", "authors": ["Mothilal Asokan", "Kebin Wu", "Fatima Albreiki"], "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01735", "pdf": "https://arxiv.org/pdf/2504.01735", "abs": "https://arxiv.org/abs/2504.01735", "authors": ["Chaohu Liu", "Tianyi Gui", "Yu Liu", "Linli Xu"], "title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently\nwitnessed remarkable advancements and are increasingly being deployed in\nreal-world applications. However, inheriting the sensitivity of visual neural\nnetworks, LVLMs remain vulnerable to adversarial attacks, which can result in\nerroneous or malicious outputs. While existing efforts utilize adversarial\nfine-tuning to enhance robustness, they often suffer from performance\ndegradation on clean inputs. In this paper, we proposes AdPO, a novel\nadversarial defense strategy for LVLMs based on preference optimization. For\nthe first time, we reframe adversarial training as a preference optimization\nproblem, aiming to enhance the model's preference for generating normal outputs\non clean inputs while rejecting the potential misleading outputs for\nadversarial examples. Notably, AdPO achieves this by solely modifying the image\nencoder, e.g., CLIP ViT, resulting in superior clean and adversarial\nperformance in a variety of downsream tasks. Considering that training involves\nlarge language models (LLMs), the computational cost increases significantly.\nWe validate that training on smaller LVLMs and subsequently transferring to\nlarger models can achieve competitive performance while maintaining efficiency\ncomparable to baseline methods. Our comprehensive experiments confirm the\neffectiveness of the proposed AdPO, which provides a novel perspective for\nfuture adversarial defense research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01805", "pdf": "https://arxiv.org/pdf/2504.01805", "abs": "https://arxiv.org/abs/2504.01805", "authors": ["Kun Ouyang"], "title": "Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Enhancing the spatial reasoning capabilities of Multi-modal Large Language\nModels (MLLMs) for video understanding is crucial yet challenging. We present\nSpatial-R1, a targeted approach involving two key contributions: the curation\nof SR, a new video spatial reasoning dataset from ScanNet with automatically\ngenerated QA pairs across seven task types, and the application of\nTask-Specific Group Relative Policy Optimization (GRPO) for fine-tuning. By\ntraining the Qwen2.5-VL-7B-Instruct model on SR using GRPO, Spatial-R1\nsignificantly advances performance on the VSI-Bench benchmark, achieving a\n7.4\\% gain over the baseline and outperforming strong contemporary models. This\nwork validates the effectiveness of specialized data curation and optimization\ntechniques for improving complex spatial reasoning in video MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01872", "pdf": "https://arxiv.org/pdf/2504.01872", "abs": "https://arxiv.org/abs/2504.01872", "authors": ["Jintao Zhang", "Zimin Xia", "Mingyue Dong", "Shuhan Shen", "Linwei Yue", "Xianwei Zheng"], "title": "CoMatcher: Multi-View Collaborative Feature Matching", "categories": ["cs.CV", "I.4.8; I.2.10; I.5.4"], "comment": "15 pages, 7 figures, to be published in CVPR 2025", "summary": "This paper proposes a multi-view collaborative matching strategy for reliable\ntrack construction in complex scenarios. We observe that the pairwise matching\nparadigms applied to image set matching often result in ambiguous estimation\nwhen the selected independent pairs exhibit significant occlusions or extreme\nviewpoint changes. This challenge primarily stems from the inherent uncertainty\nin interpreting intricate 3D structures based on limited two-view observations,\nas the 3D-to-2D projection leads to significant information loss. To address\nthis, we introduce CoMatcher, a deep multi-view matcher to (i) leverage\ncomplementary context cues from different views to form a holistic 3D scene\nunderstanding and (ii) utilize cross-view projection consistency to infer a\nreliable global solution. Building on CoMatcher, we develop a groupwise\nframework that fully exploits cross-view relationships for large-scale matching\ntasks. Extensive experiments on various complex scenarios demonstrate the\nsuperiority of our method over the mainstream two-view matching paradigm.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01886", "pdf": "https://arxiv.org/pdf/2504.01886", "abs": "https://arxiv.org/abs/2504.01886", "authors": ["Yanzhou Su", "Tianbin Li", "Jiyao Liu", "Chenglong Ma", "Junzhi Ning", "Cheng Tang", "Sibo Ju", "Jin Ye", "Pengcheng Chen", "Ming Hu", "Shixiang Tang", "Lihao Liu", "Bin Fu", "Wenqi Shao", "Xiaowei Hu", "Xiangwen Liao", "Yuanfeng Ji", "Junjun He"], "title": "GMAI-VL-R1: Harnessing Reinforcement Learning for Multimodal Medical Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in general medical AI have made significant strides, but\nexisting models often lack the reasoning capabilities needed for complex\nmedical decision-making. This paper presents GMAI-VL-R1, a multimodal medical\nreasoning model enhanced by reinforcement learning (RL) to improve its\nreasoning abilities. Through iterative training, GMAI-VL-R1 optimizes\ndecision-making, significantly boosting diagnostic accuracy and clinical\nsupport. We also develop a reasoning data synthesis method, generating\nstep-by-step reasoning data via rejection sampling, which further enhances the\nmodel's generalization. Experimental results show that after RL training,\nGMAI-VL-R1 excels in tasks such as medical image diagnosis and visual question\nanswering. While the model demonstrates basic memorization with supervised\nfine-tuning, RL is crucial for true generalization. Our work establishes new\nevaluation benchmarks and paves the way for future advancements in medical\nreasoning models. Code, data, and model will be released at\n\\href{https://github.com/uni-medical/GMAI-VL-R1}{this link}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01890", "pdf": "https://arxiv.org/pdf/2504.01890", "abs": "https://arxiv.org/abs/2504.01890", "authors": ["Shreyank N Gowda", "Boyan Gao", "Xiao Gu", "Xiaobo Jin"], "title": "Is Temporal Prompting All We Need For Limited Labeled Action Recognition?", "categories": ["cs.CV"], "comment": "Accepted in CVPR-W 2025", "summary": "Video understanding has shown remarkable improvements in recent years,\nlargely dependent on the availability of large scaled labeled datasets. Recent\nadvancements in visual-language models, especially based on contrastive\npretraining, have shown remarkable generalization in zero-shot tasks, helping\nto overcome this dependence on labeled datasets. Adaptations of such models for\nvideos, typically involve modifying the architecture of vision-language models\nto cater to video data. However, this is not trivial, since such adaptations\nare mostly computationally intensive and struggle with temporal modeling. We\npresent TP-CLIP, an adaptation of CLIP that leverages temporal visual prompting\nfor temporal adaptation without modifying the core CLIP architecture. This\npreserves its generalization abilities. TP-CLIP efficiently integrates into the\nCLIP architecture, leveraging its pre-trained capabilities for video data.\nExtensive experiments across various datasets demonstrate its efficacy in\nzero-shot and few-shot learning, outperforming existing approaches with fewer\nparameters and computational efficiency. In particular, we use just 1/3 the\nGFLOPs and 1/28 the number of tuneable parameters in comparison to recent\nstate-of-the-art and still outperform it by up to 15.8% depending on the task\nand dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01916", "pdf": "https://arxiv.org/pdf/2504.01916", "abs": "https://arxiv.org/abs/2504.01916", "authors": ["Mothilal Asokan", "Kebin Wu", "Fatima Albreiki"], "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "As a pioneering vision-language model, CLIP (Contrastive Language-Image\nPre-training) has achieved significant success across various domains and a\nwide range of downstream vision-language tasks. However, the text encoders in\npopular CLIP models are limited to processing only 77 text tokens, which\nconstrains their ability to effectively handle longer, detail-rich captions.\nAdditionally, CLIP models often struggle to effectively capture detailed visual\nand textual information, which hampers their performance on tasks that require\nfine-grained analysis. To address these limitations, we present a novel\napproach, \\textbf{FineLIP}, that extends the capabilities of CLIP. FineLIP\nenhances cross-modal text-image mapping by incorporating \\textbf{Fine}-grained\nalignment with \\textbf{L}onger text input within the CL\\textbf{IP}-style\nframework. FineLIP first extends the positional embeddings to handle longer\ntext, followed by the dynamic aggregation of local image and text tokens. The\naggregated results are then used to enforce fine-grained token-to-token\ncross-modal alignment. We validate our model on datasets with long, detailed\ncaptions across two tasks: zero-shot cross-modal retrieval and text-to-image\ngeneration. Quantitative and qualitative experimental results demonstrate the\neffectiveness of FineLIP, outperforming existing state-of-the-art approaches.\nFurthermore, comprehensive ablation studies validate the benefits of key design\nelements within FineLIP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01218", "pdf": "https://arxiv.org/pdf/2504.01218", "abs": "https://arxiv.org/abs/2504.01218", "authors": ["Piyush Nagasubramaniam", "Neeraj Karamchandani", "Chen Wu", "Sencun Zhu"], "title": "Prompting Forgetting: Unlearning in GANs via Textual Guidance", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "State-of-the-art generative models exhibit powerful image-generation\ncapabilities, introducing various ethical and legal challenges to service\nproviders hosting these models. Consequently, Content Removal Techniques (CRTs)\nhave emerged as a growing area of research to control outputs without\nfull-scale retraining. Recent work has explored the use of Machine Unlearning\nin generative models to address content removal. However, the focus of such\nresearch has been on diffusion models, and unlearning in Generative Adversarial\nNetworks (GANs) has remained largely unexplored. We address this gap by\nproposing Text-to-Unlearn, a novel framework that selectively unlearns concepts\nfrom pre-trained GANs using only text prompts, enabling feature unlearning,\nidentity unlearning, and fine-grained tasks like expression and multi-attribute\nremoval in models trained on human faces. Leveraging natural language\ndescriptions, our approach guides the unlearning process without requiring\nadditional datasets or supervised fine-tuning, offering a scalable and\nefficient solution. To evaluate its effectiveness, we introduce an automatic\nunlearning assessment method adapted from state-of-the-art image-text alignment\nmetrics, providing a comprehensive analysis of the unlearning methodology. To\nour knowledge, Text-to-Unlearn is the first cross-modal unlearning framework\nfor GANs, representing a flexible and efficient advancement in managing\ngenerative model behavior.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
{"id": "2504.01521", "pdf": "https://arxiv.org/pdf/2504.01521", "abs": "https://arxiv.org/abs/2504.01521", "authors": ["Jincheng Zhong", "Xiangcheng Zhang", "Jianmin Wang", "Mingsheng Long"], "title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advancements in diffusion models have revolutionized generative\nmodeling. However, the impressive and vivid outputs they produce often come at\nthe cost of significant model scaling and increased computational demands.\nConsequently, building personalized diffusion models based on off-the-shelf\nmodels has emerged as an appealing alternative. In this paper, we introduce a\nnovel perspective on conditional generation for transferring a pre-trained\nmodel. From this viewpoint, we propose *Domain Guidance*, a straightforward\ntransfer approach that leverages pre-trained knowledge to guide the sampling\nprocess toward the target domain. Domain Guidance shares a formulation similar\nto advanced classifier-free guidance, facilitating better domain alignment and\nhigher-quality generations. We provide both empirical and theoretical analyses\nof the mechanisms behind Domain Guidance. Our experimental results demonstrate\nits substantial effectiveness across various transfer benchmarks, achieving\nover a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$\ncompared to standard fine-tuning. Notably, existing fine-tuned models can\nseamlessly integrate Domain Guidance to leverage these benefits, without\nadditional training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-03.jsonl"}
