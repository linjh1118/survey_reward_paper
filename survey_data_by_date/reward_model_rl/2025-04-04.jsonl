{"id": "2504.02161", "pdf": "https://arxiv.org/pdf/2504.02161", "abs": "https://arxiv.org/abs/2504.02161", "authors": ["Zhen Meng", "Kan Chen", "Xiangmin Xu", "Erwin Jose Lopez Pulgarin", "Emma Li", "Philip G. Zhao", "David Flynn"], "title": "Preference-Driven Active 3D Scene Representation for Robotic Inspection in Nuclear Decommissioning", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025", "summary": "Active 3D scene representation is pivotal in modern robotics applications,\nincluding remote inspection, manipulation, and telepresence. Traditional\nmethods primarily optimize geometric fidelity or rendering accuracy, but often\noverlook operator-specific objectives, such as safety-critical coverage or\ntask-driven viewpoints. This limitation leads to suboptimal viewpoint\nselection, particularly in constrained environments such as nuclear\ndecommissioning. To bridge this gap, we introduce a novel framework that\nintegrates expert operator preferences into the active 3D scene representation\npipeline. Specifically, we employ Reinforcement Learning from Human Feedback\n(RLHF) to guide robotic path planning, reshaping the reward function based on\nexpert input. To capture operator-specific priorities, we conduct interactive\nchoice experiments that evaluate user preferences in 3D scene representation.\nWe validate our framework using a UR3e robotic arm for reactor tile inspection\nin a nuclear decommissioning scenario. Compared to baseline methods, our\napproach enhances scene representation while optimizing trajectory efficiency.\nThe RLHF-based policy consistently outperforms random selection, prioritizing\ntask-critical details. By unifying explicit 3D geometric modeling with implicit\nhuman-in-the-loop optimization, this work establishes a foundation for\nadaptive, safety-critical robotic perception systems, paving the way for\nenhanced automation in nuclear decommissioning, remote maintenance, and other\nhigh-risk environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02725", "pdf": "https://arxiv.org/pdf/2504.02725", "abs": "https://arxiv.org/abs/2504.02725", "authors": ["Kehua Feng", "Keyan Ding", "Jing Yu", "Menghan Li", "Yuhao Wang", "Tong Xu", "Xinda Wang", "Qiang Zhang", "Huajun Chen"], "title": "ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, yet their potential to generate harmful\ncontent poses critical safety challenges. Existing alignment methods often\nstruggle to cover diverse safety scenarios and remain vulnerable to adversarial\nattacks. In this work, we propose Ex-Ante Reasoning Preference Optimization\n(ERPO), a novel safety alignment framework that equips LLMs with explicit\npreemptive reasoning through Chain-of-Thought and provides clear evidence for\nsafety judgments by embedding predefined safety rules. Specifically, our\napproach consists of three stages: first, equipping the model with Ex-Ante\nreasoning through supervised fine-tuning (SFT) using a constructed reasoning\nmodule; second, enhancing safety, usefulness, and efficiency via Direct\nPreference Optimization (DPO); and third, mitigating inference latency with a\nlength-controlled iterative preference optimization strategy. Experiments on\nmultiple open-source LLMs demonstrate that ERPO significantly enhances safety\nperformance while maintaining response efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02327", "pdf": "https://arxiv.org/pdf/2504.02327", "abs": "https://arxiv.org/abs/2504.02327", "authors": ["Weibin Liao", "Xin Gao", "Tianyu Jia", "Rihong Qiu", "Yifan Zhu", "Yang Lin", "Xu Chu", "Junfeng Zhao", "Yasha Wang"], "title": "LearNAT: Learning NL2SQL with AST-guided Task Decomposition for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language to SQL (NL2SQL) has emerged as a critical task for enabling\nseamless interaction with databases. Recent advancements in Large Language\nModels (LLMs) have demonstrated remarkable performance in this domain. However,\nexisting NL2SQL methods predominantly rely on closed-source LLMs leveraging\nprompt engineering, while open-source models typically require fine-tuning to\nacquire domain-specific knowledge. Despite these efforts, open-source LLMs\nstruggle with complex NL2SQL tasks due to the indirect expression of user query\nobjectives and the semantic gap between user queries and database schemas.\nInspired by the application of reinforcement learning in mathematical\nproblem-solving to encourage step-by-step reasoning in LLMs, we propose LearNAT\n(Learning NL2SQL with AST-guided Task Decomposition), a novel framework that\nimproves the performance of open-source LLMs on complex NL2SQL tasks through\ntask decomposition and reinforcement learning. LearNAT introduces three key\ncomponents: (1) a Decomposition Synthesis Procedure that leverages Abstract\nSyntax Trees (ASTs) to guide efficient search and pruning strategies for task\ndecomposition, (2) Margin-aware Reinforcement Learning, which employs\nfine-grained step-level optimization via DPO with AST margins, and (3) Adaptive\nDemonstration Reasoning, a mechanism for dynamically selecting relevant\nexamples to enhance decomposition capabilities. Extensive experiments on two\nbenchmark datasets, Spider and BIRD, demonstrate that LearNAT enables a\n7B-parameter open-source LLM to achieve performance comparable to GPT-4, while\noffering improved efficiency and accessibility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "DPO"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02495", "pdf": "https://arxiv.org/pdf/2504.02495", "abs": "https://arxiv.org/abs/2504.02495", "authors": ["Zijun Liu", "Peiyi Wang", "Runxin Xu", "Shirong Ma", "Chong Ruan", "Peng Li", "Yang Liu", "Yu Wu"], "title": "Inference-Time Scaling for Generalist Reward Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, under review. 42 pages", "summary": "Reinforcement learning (RL) has been widely adopted in post-training for\nlarge language models (LLMs) at scale. Recently, the incentivization of\nreasoning capabilities in LLMs from RL indicates that $\\textit{proper learning\nmethods could enable effective inference-time scalability}$. A key challenge of\nRL is to obtain accurate reward signals for LLMs in various domains beyond\nverifiable questions or artificial rules. In this work, we investigate how to\nimprove reward modeling (RM) with more inference compute for general queries,\ni.e. the $\\textbf{inference-time scalability of generalist RM}$, and further,\nhow to improve the effectiveness of performance-compute scaling with proper\nlearning methods. For the RM approach, we adopt pointwise generative reward\nmodeling (GRM) to enable flexibility for different input types and potential\nfor inference-time scaling. For the learning method, we propose Self-Principled\nCritique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs\nthrough online RL, to generate principles adaptively and critiques accurately,\nresulting in $\\textbf{DeepSeek-GRM}$ models. Furthermore, for effective\ninference-time scaling, we use parallel sampling to expand compute usage, and\nintroduce a meta RM to guide voting process for better scaling performance.\nEmpirically, we show that SPCT significantly improves the quality and\nscalability of GRMs, outperforming existing methods and models in various RM\nbenchmarks without severe biases, and could achieve better performance compared\nto training-time scaling. DeepSeek-GRM still meets challenges in some tasks,\nwhich we believe can be addressed by future efforts in generalist reward\nsystems. The models will be released and open-sourced.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale", "compute scaling", "inference compute"], "score": 5}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02708", "pdf": "https://arxiv.org/pdf/2504.02708", "abs": "https://arxiv.org/abs/2504.02708", "authors": ["Nikhil Verma", "Manasa Bharadwaj"], "title": "The Hidden Space of Safety: Understanding Preference-Tuned LLMs in Multilingual context", "categories": ["cs.CL"], "comment": "14 pages, 11 Figures, 2 Tables, currently under review at ACL 2025", "summary": "Alignment tuning has enabled large language models to excel in reasoning,\ninstruction-following, and minimizing harmful generations. However, despite\ntheir widespread deployment, these models exhibit a monolingual bias, raising\nconcerns about the effectiveness of alignment across languages. Current\nalignment methods predominantly focus on English, leaving it unclear how\nalignment mechanism generalize to multilingual settings. To address this, we\nconduct a systematic analysis of distributional shifts in the embedding space\nof LLMs before and after alignment, uncovering its impact on model behavior\nacross diverse languages. We leverage the alignment-induced separation in\nsafety space as a quantitative tool to measure how alignment enforces safety\nconstraints. Our study evaluates seven LLMs using balanced toxicity datasets\nand parallel text-detoxification benchmarks, revealing substantial disparities\nin the latent representation space between high-resource and low-resource\nlanguages. These findings underscore the need for language-specific fine-tuning\nto ensure fair, reliable and robust multilingual alignment. Our insights\nprovide a foundation for developing truly safe multilingual LLMs, emphasizing\nthe urgency of addressing alignment gaps in underrepresented languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02061", "pdf": "https://arxiv.org/pdf/2504.02061", "abs": "https://arxiv.org/abs/2504.02061", "authors": ["Yuxin Guo", "Shuailei Ma", "Shijie Ma", "Xiaoyi Bao", "Chen-Wei Xie", "Kecheng Zheng", "Tingyu Weng", "Siyang Sun", "Yun Zheng", "Wei Zou"], "title": "Aligned Better, Listen Better for Audio-Visual Large Language Models", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ICLR 2025", "summary": "Audio is essential for multimodal video understanding. On the one hand, video\ninherently contains audio, which supplies complementary information to vision.\nBesides, video large language models (Video-LLMs) can encounter many\naudio-centric settings. However, existing Video-LLMs and Audio-Visual Large\nLanguage Models (AV-LLMs) exhibit deficiencies in exploiting audio information,\nleading to weak understanding and hallucinations. To solve the issues, we delve\ninto the model architecture and dataset. (1) From the architectural\nperspective, we propose a fine-grained AV-LLM, namely Dolphin. The concurrent\nalignment of audio and visual modalities in both temporal and spatial\ndimensions ensures a comprehensive and accurate understanding of videos.\nSpecifically, we devise an audio-visual multi-scale adapter for multi-scale\ninformation aggregation, which achieves spatial alignment. For temporal\nalignment, we propose audio-visual interleaved merging. (2) From the dataset\nperspective, we curate an audio-visual caption and instruction-tuning dataset,\ncalled AVU. It comprises 5.2 million diverse, open-ended data tuples (video,\naudio, question, answer) and introduces a novel data partitioning strategy.\nExtensive experiments show our model not only achieves remarkable performance\nin audio-visual understanding, but also mitigates potential hallucinations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02160", "pdf": "https://arxiv.org/pdf/2504.02160", "abs": "https://arxiv.org/abs/2504.02160", "authors": ["Shaojin Wu", "Mengqi Huang", "Wenxu Wu", "Yufeng Cheng", "Fei Ding", "Qian He"], "title": "Less-to-More Generalization: Unlocking More Controllability by In-Context Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/UNO Code and model:\n  https://github.com/bytedance/UNO", "summary": "Although subject-driven generation has been extensively explored in image\ngeneration due to its wide applications, it still has challenges in data\nscalability and subject expansibility. For the first challenge, moving from\ncurating single-subject datasets to multiple-subject ones and scaling them is\nparticularly difficult. For the second, most recent methods center on\nsingle-subject generation, making it hard to apply when dealing with\nmulti-subject scenarios. In this study, we propose a highly-consistent data\nsynthesis pipeline to tackle this challenge. This pipeline harnesses the\nintrinsic in-context generation capabilities of diffusion transformers and\ngenerates high-consistency multi-subject paired data. Additionally, we\nintroduce UNO, which consists of progressive cross-modal alignment and\nuniversal rotary position embedding. It is a multi-image conditioned\nsubject-to-image model iteratively trained from a text-to-image model.\nExtensive experiments show that our method can achieve high consistency while\nensuring controllability in both single-subject and multi-subject driven\ngeneration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02590", "pdf": "https://arxiv.org/pdf/2504.02590", "abs": "https://arxiv.org/abs/2504.02590", "authors": ["Kepu Zhang", "Guofu Xie", "Weijie Yu", "Mingyue Xu", "Xu Tang", "Yaxin Li", "Jun Xu"], "title": "LexPam: Legal Procedure Awareness-Guided Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The legal mathematical reasoning ability of LLMs is crucial when applying\nthem to real-world scenarios, as it directly affects the credibility of the\nLLM. While existing legal LLMs can perform general judicial question answering,\ntheir legal mathematical reasoning capabilities have not been trained.\nOpen-domain reasoning models, though able to generate detailed calculation\nsteps, do not follow the reasoning logic required for legal scenarios.\nAdditionally, there is currently a lack of legal mathematical reasoning\ndatasets to help validate and enhance LLMs' reasoning abilities in legal\ncontexts. To address these issues, we propose the first Chinese legal\nMathematical Reasoning Dataset, LexNum, which includes three common legal\nmathematical reasoning scenarios: economic compensation, work injury\ncompensation, and traffic accident compensation. Based on LexNum, we tested the\nperformance of existing legal LLMs and reasoning LLMs, and introduced LexPam, a\nreinforcement learning algorithm guided by legal procedural awareness to train\nLLMs, enhancing their mathematical reasoning abilities in legal scenarios.\nExperiments on tasks in the three legal scenarios show that the performance of\nexisting legal LLMs and reasoning models in legal mathematical reasoning tasks\nis unsatisfactory. LexPam can enhance the LLM's ability in these tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02328", "pdf": "https://arxiv.org/pdf/2504.02328", "abs": "https://arxiv.org/abs/2504.02328", "authors": ["Congpei Qiu", "Yanhao Wu", "Wei Ke", "Xiuxiu Bai", "Tong Zhang"], "title": "Refining CLIP's Spatial Awareness: A Visual-Centric Perspective", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) excels in global alignment\nwith language but exhibits limited sensitivity to spatial information, leading\nto strong performance in zero-shot classification tasks but underperformance in\ntasks requiring precise spatial understanding. Recent approaches have\nintroduced Region-Language Alignment (RLA) to enhance CLIP's performance in\ndense multimodal tasks by aligning regional visual representations with\ncorresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA\nsuffer from notable loss in spatial awareness, which is crucial for dense\nprediction tasks. To address this, we propose the Spatial Correlation\nDistillation (SCD) framework, which preserves CLIP's inherent spatial structure\nand mitigates the above degradation. To further enhance spatial correlations,\nwe introduce a lightweight Refiner that extracts refined correlations directly\nfrom CLIP before feeding them into SCD, based on an intriguing finding that\nCLIP naturally captures high-quality dense features. Together, these components\nform a robust distillation framework that enables CLIP ViTs to integrate both\nvisual-language and visual-centric improvements, achieving state-of-the-art\nresults across various open-vocabulary dense prediction benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02337", "pdf": "https://arxiv.org/pdf/2504.02337", "abs": "https://arxiv.org/abs/2504.02337", "authors": ["Ming-Jia Yang", "Yu-Xiao Guo", "Yang Liu", "Bin Zhou", "Xin Tong"], "title": "LPA3D: 3D Room-Level Scene Generation from In-the-Wild Images", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic, room-level indoor scenes with semantically plausible\nand detailed appearances from in-the-wild images is crucial for various\napplications in VR, AR, and robotics. The success of NeRF-based generative\nmethods indicates a promising direction to address this challenge. However,\nunlike their success at the object level, existing scene-level generative\nmethods require additional information, such as multiple views, depth images,\nor semantic guidance, rather than relying solely on RGB images. This is because\nNeRF-based methods necessitate prior knowledge of camera poses, which is\nchallenging to approximate for indoor scenes due to the complexity of defining\nalignment and the difficulty of globally estimating poses from a single image,\ngiven the unseen parts behind the camera. To address this challenge, we\nredefine global poses within the framework of Local-Pose-Alignment (LPA) -- an\nanchor-based multi-local-coordinate system that uses a selected number of\nanchors as the roots of these coordinates. Building on this foundation, we\nintroduce LPA-GAN, a novel NeRF-based generative approach that incorporates\nspecific modifications to estimate the priors of camera poses under LPA. It\nalso co-optimizes the pose predictor and scene generation processes. Our\nablation study and comparisons with straightforward extensions of NeRF-based\nobject generative methods demonstrate the effectiveness of our approach.\nFurthermore, visual comparisons with other techniques reveal that our method\nachieves superior view-to-view consistency and semantic normality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02733", "pdf": "https://arxiv.org/pdf/2504.02733", "abs": "https://arxiv.org/abs/2504.02733", "authors": ["Aryan Agrawal", "Lisa Alazraki", "Shahin Honarvar", "Marek Rei"], "title": "Enhancing LLM Robustness to Perturbed Instructions: An Empirical Study", "categories": ["cs.CL"], "comment": "Building Trust Workshop, ICLR 2025", "summary": "Large Language Models (LLMs) are highly vulnerable to input perturbations, as\neven a small prompt change may result in a substantially different output.\nExisting methods to enhance LLM robustness are primarily focused on perturbed\ndata samples, whereas improving resiliency to perturbations of task-level\ninstructions has remained relatively underexplored. In this work, we focus on\ncharacter- and word-level edits of task-specific instructions, which\nsubstantially degrade downstream performance. We experiment with a variety of\ntechniques to enhance the robustness of LLMs, including self-denoising and\nrepresentation alignment, testing different models (Llama 3 and Flan-T5),\ndatasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and\nrole-oriented). We find that, on average, self-denoising -- whether performed\nby a frozen LLM or a fine-tuned model -- achieves substantially higher\nperformance gains than alternative strategies, including more complex baselines\nsuch as ensembling and supervised methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02397", "pdf": "https://arxiv.org/pdf/2504.02397", "abs": "https://arxiv.org/abs/2504.02397", "authors": ["Boseung Jeong", "Jicheol Park", "Sungyeon Kim", "Suha Kwak"], "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Video-text retrieval, the task of retrieving videos based on a textual query\nor vice versa, is of paramount importance for video understanding and\nmultimodal information retrieval. Recent methods in this area rely primarily on\nvisual and textual features and often ignore audio, although it helps enhance\noverall comprehension of video content. Moreover, traditional models that\nincorporate audio blindly utilize the audio input regardless of whether it is\nuseful or not, resulting in suboptimal video representation. To address these\nlimitations, we propose a novel video-text retrieval framework, Audio-guided\nVIdeo representation learning with GATEd attention (AVIGATE), that effectively\nleverages audio cues through a gated attention mechanism that selectively\nfilters out uninformative audio signals. In addition, we propose an adaptive\nmargin-based contrastive loss to deal with the inherently unclear\npositive-negative relationship between video and text, which facilitates\nlearning better video-text alignment. Our extensive experiments demonstrate\nthat AVIGATE achieves state-of-the-art performance on all the public\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02436", "pdf": "https://arxiv.org/pdf/2504.02436", "abs": "https://arxiv.org/abs/2504.02436", "authors": ["Zhengcong Fei", "Debang Li", "Di Qiu", "Jiahua Wang", "Yikun Dou", "Rui Wang", "Jingtao Xu", "Mingyuan Fan", "Guibin Chen", "Yang Li", "Yahui Zhou"], "title": "SkyReels-A2: Compose Anything in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents SkyReels-A2, a controllable video generation framework\ncapable of assembling arbitrary visual elements (e.g., characters, objects,\nbackgrounds) into synthesized videos based on textual prompts while maintaining\nstrict consistency with reference images for each element. We term this task\nelements-to-video (E2V), whose primary challenges lie in preserving the\nfidelity of each reference element, ensuring coherent composition of the scene,\nand achieving natural outputs. To address these, we first design a\ncomprehensive data pipeline to construct prompt-reference-video triplets for\nmodel training. Next, we propose a novel image-text joint embedding model to\ninject multi-element representations into the generative process, balancing\nelement-specific consistency with global coherence and text alignment. We also\noptimize the inference pipeline for both speed and output stability. Moreover,\nwe introduce a carefully curated benchmark for systematic evaluation, i.e, A2\nBench. Experiments demonstrate that our framework can generate diverse,\nhigh-quality videos with precise element control. SkyReels-A2 is the first\nopen-source commercial grade model for the generation of E2V, performing\nfavorably against advanced closed-source commercial models. We anticipate\nSkyReels-A2 will advance creative applications such as drama and virtual\ne-commerce, pushing the boundaries of controllable video generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02605", "pdf": "https://arxiv.org/pdf/2504.02605", "abs": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "The task of issue resolving is to modify a codebase to generate a patch that\naddresses a given issue. However, existing benchmarks, such as SWE-bench, focus\nalmost exclusively on Python, making them insufficient for evaluating Large\nLanguage Models (LLMs) across diverse software ecosystems. To address this, we\nintroduce a multilingual issue-resolving benchmark, called Multi-SWE-bench,\ncovering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a\ntotal of 1,632 high-quality instances, which were carefully annotated from\n2,456 candidates by 68 expert annotators, ensuring that the benchmark can\nprovide an accurate and reliable evaluation. Based on Multi-SWE-bench, we\nevaluate a series of state-of-the-art models using three representative methods\n(Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with\nkey empirical insights. In addition, we launch a Multi-SWE-RL open-source\ncommunity, aimed at building large-scale reinforcement learning (RL) training\ndatasets for issue-resolving tasks. As an initial contribution, we release a\nset of 4,723 well-structured instances spanning seven programming languages,\nlaying a solid foundation for RL research in this domain. More importantly, we\nopen-source our entire data production pipeline, along with detailed tutorials,\nencouraging the open-source community to continuously contribute and expand the\ndataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL\ncommunity as catalysts for advancing RL toward its full potential, bringing us\none step closer to the dawn of AGI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02812", "pdf": "https://arxiv.org/pdf/2504.02812", "abs": "https://arxiv.org/abs/2504.02812", "authors": ["Van Nguyen Nguyen", "Stephen Tyree", "Andrew Guo", "Mederic Fourmy", "Anas Gouda", "Taeyeop Lee", "Sungphill Moon", "Hyeontae Son", "Lukas Ranftl", "Jonathan Tremblay", "Eric Brachmann", "Bertram Drost", "Vincent Lepetit", "Carsten Rother", "Stan Birchfield", "Jiri Matas", "Yann Labbe", "Martin Sundermeyer", "Tomas Hodan"], "title": "BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2403.09799", "summary": "We present the evaluation methodology, datasets and results of the BOP\nChallenge 2024, the sixth in a series of public competitions organized to\ncapture the state of the art in 6D object pose estimation and related tasks. In\n2024, our goal was to transition BOP from lab-like setups to real-world\nscenarios. First, we introduced new model-free tasks, where no 3D object models\nare available and methods need to onboard objects just from provided reference\nvideos. Second, we defined a new, more practical 6D object detection task where\nidentities of objects visible in a test image are not provided as input. Third,\nwe introduced new BOP-H3 datasets recorded with high-resolution sensors and\nAR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D\nmodels and onboarding videos to support both model-based and model-free tasks.\nParticipants competed on seven challenge tracks, each defined by a task, object\nonboarding setup, and dataset group. Notably, the best 2024 method for\nmodel-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher\naccuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only\n4% behind the best 2023 method for seen objects (GPose2023) although being\nsignificantly slower (24.9 vs 2.7s per image). A more practical 2024 method for\nthis task is Co-op which takes only 0.8s per image and is 25X faster and 13%\nmore accurate than GenFlow. Methods have a similar ranking on 6D detection as\non 6D localization but higher run time. On model-based 2D detection of unseen\nobjects, the best 2024 method (MUSE) achieves 21% relative improvement compared\nto the best 2023 method (CNOS). However, the 2D detection accuracy for unseen\nobjects is still noticealy (-53%) behind the accuracy for seen objects\n(GDet2023). The online evaluation system stays open and is available at\nhttp://bop.felk.cvut.cz/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02334", "pdf": "https://arxiv.org/pdf/2504.02334", "abs": "https://arxiv.org/abs/2504.02334", "authors": ["Boris Sukhovilov"], "title": "Determining Sphere Radius through Pairwise Distances", "categories": ["cs.CG", "cs.CV", "68U05 (Primary) 65D18 (Secondary)", "I.3.5; I.4.5"], "comment": "10 pages, we share the implementation of our method as open source\n  code at https://github.com/boris-sukhovilov/Sphere_Radius", "summary": "We propose a novel method for determining the radius of a spherical surface\nbased on the distances measured between points on this surface. We consider the\nmost general case of determining the radius when the distances are measured\nwith errors and the sphere has random deviations from its ideal shape. For the\nsolution, we used the minimally necessary four points and an arbitrary N number\nof points. We provide a new closed form solution for the radius of the sphere\nthrough the matrix of pairwise distances. We also determine the standard\ndeviation of the radius estimate caused by measurement errors and deviations of\nthe sphere from its ideal shape. We found optimal configurations of points on\nthe sphere that provide the minimum standard deviation of the radius estimate.\nThis paper describes our solution and provides all the mathematical\nderivations. We share the implementation of our method as open source code at\nhttps://github.com/boris-sukhovilov/Sphere_Radius.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02477", "pdf": "https://arxiv.org/pdf/2504.02477", "abs": "https://arxiv.org/abs/2504.02477", "authors": ["Xiaofeng Han", "Shunpeng Chen", "Zenghuang Fu", "Zhe Feng", "Lue Fan", "Dong An", "Changwei Wang", "Li Guo", "Weiliang Meng", "Xiaopeng Zhang", "Rongtao Xu", "Shibiao Xu"], "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision", "categories": ["cs.RO", "cs.CV"], "comment": "27 pages, 11 figures, survey paper submitted to Information Fusion", "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We systematically review the\napplications of multimodal fusion in key robotic vision tasks, including\nsemantic scene understanding, simultaneous localization and mapping (SLAM), 3D\nobject detection, navigation and localization, and robot manipulation. We\ncompare VLMs based on large language models (LLMs) with traditional multimodal\nfusion methods, analyzing their advantages, limitations, and synergies.\nAdditionally, we conduct an in-depth analysis of commonly used datasets,\nevaluating their applicability and challenges in real-world robotic scenarios.\nFurthermore, we identify critical research challenges such as cross-modal\nalignment, efficient fusion strategies, real-time deployment, and domain\nadaptation, and propose future research directions, including self-supervised\nlearning for robust multimodal representations, transformer-based fusion\narchitectures, and scalable multimodal frameworks. Through a comprehensive\nreview, comparative analysis, and forward-looking discussion, we provide a\nvaluable reference for advancing multimodal perception and interaction in\nrobotic vision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02587", "pdf": "https://arxiv.org/pdf/2504.02587", "abs": "https://arxiv.org/abs/2504.02587", "authors": ["Yan Ma", "Steffi Chern", "Xuyang Shen", "Yiran Zhong", "Pengfei Liu"], "title": "Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Code is public and available at: https://github.com/GAIR-NLP/MAYE", "summary": "Reinforcement learning (RL) has recently shown strong potential in improving\nthe reasoning capabilities of large language models and is now being actively\nextended to vision-language models (VLMs). However, existing RL applications in\nVLMs often rely on heavily engineered frameworks that hinder reproducibility\nand accessibility, while lacking standardized evaluation protocols, making it\ndifficult to compare results or interpret training dynamics. This work\nintroduces a transparent, from-scratch framework for RL in VLMs, offering a\nminimal yet functional four-step pipeline validated across multiple models and\ndatasets. In addition, a standardized evaluation scheme is proposed to assess\ntraining dynamics and reflective behaviors. Extensive experiments on visual\nreasoning tasks uncover key empirical findings: response length is sensitive to\nrandom seeds, reflection correlates with output length, and RL consistently\noutperforms supervised fine-tuning (SFT) in generalization, even with\nhigh-quality data. These findings, together with the proposed framework, aim to\nestablish a reproducible baseline and support broader engagement in RL-based\nVLM research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02628", "pdf": "https://arxiv.org/pdf/2504.02628", "abs": "https://arxiv.org/abs/2504.02628", "authors": ["Chu Han", "Bingchao Zhao", "Jiatai Lin", "Shanshan Lyu", "Longfei Wang", "Tianpeng Deng", "Cheng Lu", "Changhong Liang", "Hannah Y. Wen", "Xiaojing Guo", "Zhenwei Shi", "Zaiyi Liu"], "title": "Towards Computation- and Communication-efficient Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Despite the impressive performance across a wide range of applications,\ncurrent computational pathology models face significant diagnostic efficiency\nchallenges due to their reliance on high-magnification whole-slide image\nanalysis. This limitation severely compromises their clinical utility,\nespecially in time-sensitive diagnostic scenarios and situations requiring\nefficient data transfer. To address these issues, we present a novel\ncomputation- and communication-efficient framework called Magnification-Aligned\nGlobal-Local Transformer (MAGA-GLTrans). Our approach significantly reduces\ncomputational time, file transfer requirements, and storage overhead by\nenabling effective analysis using low-magnification inputs rather than\nhigh-magnification ones. The key innovation lies in our proposed magnification\nalignment (MAGA) mechanism, which employs self-supervised learning to bridge\nthe information gap between low and high magnification levels by effectively\naligning their feature representations. Through extensive evaluation across\nvarious fundamental CPath tasks, MAGA-GLTrans demonstrates state-of-the-art\nclassification performance while achieving remarkable efficiency gains: up to\n10.7 times reduction in computational time and over 20 times reduction in file\ntransfer and storage requirements. Furthermore, we highlight the versatility of\nour MAGA framework through two significant extensions: (1) its applicability as\na feature extractor to enhance the efficiency of any CPath architecture, and\n(2) its compatibility with existing foundation models and\nhistopathology-specific encoders, enabling them to process low-magnification\ninputs with minimal information loss. These advancements position MAGA-GLTrans\nas a particularly promising solution for time-sensitive applications,\nespecially in the context of intraoperative frozen section diagnosis where both\naccuracy and efficiency are paramount.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-04.jsonl"}
{"id": "2504.02797", "pdf": "https://arxiv.org/pdf/2504.02797", "abs": "https://arxiv.org/abs/2504.02797", "authors": ["Prashanth Chandran", "Agon Serifi", "Markus Gross", "Moritz BÃ¤cher"], "title": "Spline-based Transformers", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce Spline-based Transformers, a novel class of Transformer models\nthat eliminate the need for positional encoding. Inspired by workflows using\nsplines in computer animation, our Spline-based Transformers embed an input\nsequence of elements as a smooth trajectory in latent space. Overcoming\ndrawbacks of positional encoding such as sequence length extrapolation,\nSpline-based Transformers also provide a novel way for users to interact with\ntransformer latent spaces by directly manipulating the latent control points to\ncreate new latent trajectories and sequences. We demonstrate the superior\nperformance of our approach in comparison to conventional positional encoding\non a variety of datasets, ranging from synthetic 2D to large-scale real-world\ndatasets of images, 3D shapes, and animations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-04.jsonl"}
