{"id": "2504.03185", "pdf": "https://arxiv.org/pdf/2504.03185", "abs": "https://arxiv.org/abs/2504.03185", "authors": ["Jaymari Chua", "Chen Wang", "Lina Yao"], "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4; I.2.6; I.2.8"], "comment": null, "summary": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02894", "pdf": "https://arxiv.org/pdf/2504.02894", "abs": "https://arxiv.org/abs/2504.02894", "authors": ["Ahsan Bilal", "Beiyu Lin", "Mehdi Zaeifi"], "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been widely used for various tasks and\napplications. However, LLMs and fine-tuning are limited to the pre-trained\ndata. For example, ChatGPT's world knowledge until 2021 can be outdated or\ninaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation\n(RAG), is proposed to augment LLMs with additional, new, latest details and\ninformation to LLMs. While RAG offers the correct information, it may not best\npresent it, especially to different population groups with personalizations.\nReinforcement Learning from Human Feedback (RLHF) adapts to user needs by\naligning model responses with human preference through feedback loops. In\nreal-life applications, such as mental health problems, a dynamic and\nfeedback-based model would continuously adapt to new information and offer\npersonalized assistance due to complex factors fluctuating in a daily\nenvironment. Thus, we propose an Online Reinforcement Learning-based\nRetrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the\nresponding systems to mental health problems, such as stress, anxiety, and\ndepression. We use an open-source dataset collected from 2028 College Students\nwith 28 survey questions for each student to demonstrate the performance of our\nproposed system with the existing systems. Our system achieves superior\nperformance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,\nGemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life\napplications of LLMs for personalized services in the everyday environment. The\nresults will also help researchers in the fields of sociology, psychology, and\nneuroscience to align their theories more closely with the actual human daily\nenvironment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "dialogue"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03622", "pdf": "https://arxiv.org/pdf/2504.03622", "abs": "https://arxiv.org/abs/2504.03622", "authors": ["Zae Myung Kim", "Anand Ramachandran", "Farideh Tavazoee", "Joo-Kyung Kim", "Oleg Rokhlenko", "Dongyeop Kang"], "title": "Align to Structure: Aligning Large Language Models with Structural Information", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "policy optimization", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "fine-grained"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02949", "pdf": "https://arxiv.org/pdf/2504.02949", "abs": "https://arxiv.org/abs/2504.02949", "authors": ["Xianwei Zhuang", "Yuxin Xie", "Yufan Deng", "Dongchao Yang", "Liming Liang", "Jinghan Ru", "Yuguo Yin", "Yuexian Zou"], "title": "VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at: https://github.com/VARGPT-family/VARGPT-v1.1.\n  arXiv admin note: text overlap with arXiv:2501.12327", "summary": "In this work, we present VARGPT-v1.1, an advanced unified visual\nautoregressive model that builds upon our previous framework VARGPT. The model\npreserves the dual paradigm of next-token prediction for visual understanding\nand next-scale generation for image synthesis. Specifically, VARGPT-v1.1\nintegrates: (1) a novel training strategy combining iterative visual\ninstruction tuning with reinforcement learning through Direct Preference\nOptimization (DPO), (2) an expanded training corpus containing 8.3M\nvisual-generative instruction pairs, (3) an upgraded language model backbone\nusing Qwen2, (4) enhanced image generation resolution, and (5) emergent image\nediting capabilities without architectural modifications. These advancements\nenable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal\nunderstanding and text-to-image instruction-following tasks, demonstrating\nsignificant improvements in both comprehension and generation metrics. Notably,\nthrough visual instruction tuning, the model acquires image editing\nfunctionality while maintaining architectural consistency with its predecessor,\nrevealing the potential for unified visual understanding, generation, and\nediting. Our findings suggest that well-designed unified visual autoregressive\nmodels can effectively adopt flexible training strategies from large language\nmodels (LLMs), exhibiting promising scalability. The codebase and model weights\nare publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02882", "pdf": "https://arxiv.org/pdf/2504.02882", "abs": "https://arxiv.org/abs/2504.02882", "authors": ["Sunghee Jung", "Donghun Lee", "Shinbok Lee", "Gaeun Seo", "Daniel Lee", "Byeongil Ko", "Junrae Cho", "Kihyun Kim", "Eunggyun Kim", "Myeongcheol Shin"], "title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in\nreal-world applications, but face challenges in handling incomplete queries and\nout-of-scope requests. While existing approaches rely mainly on Supervised\nFine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method\nthat enhances TA-LLM's dialogue capabilities through Direct Preference\nOptimization. We model TA-LLM interactions as a Markov Decision Process with 5\ndistinct dialogue states and categorize user queries into 3 types based on\ntheir state transition trajectories. We automatically construct paired\ntrajectory datasets of correct and incorrect dialogue flows and introduce a\nspecialized objective loss for dialogue control. Our comprehensive evaluation\ndemonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in\ninformation gathering, 91% in tool call rejection) with substantial\nimprovements over baseline (44% and 9.6% respectively) while maintaining core\nfunctionality. Our approach opens new possibilities for developing TA-LLMs that\ncan handle diverse real-world scenarios without requiring additional expert\ndemonstrations or human labeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03612", "pdf": "https://arxiv.org/pdf/2504.03612", "abs": "https://arxiv.org/abs/2504.03612", "authors": ["Bingxiang He", "Wenbin Zhang", "Jiaxi Song", "Cheng Qian", "Zixuan Fu", "Bowen Sun", "Ning Ding", "Haiwen Hong", "Longtao Huang", "Hui Xue", "Ganqu Cui", "Wanxiang Che", "Zhiyuan Liu", "Maosong Sun"], "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response Pairs in Preference Dataset", "categories": ["cs.CL"], "comment": "29 pages, 11 figures", "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "annotation"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03043", "pdf": "https://arxiv.org/pdf/2504.03043", "abs": "https://arxiv.org/abs/2504.03043", "authors": ["Joel Sol", "Shadi Alijani", "Homayoun Najjaran"], "title": "Sliced Wasserstein Discrepancy in Disentangling Representation and Adaptation Networks for Unsupervised Domain Adaptation", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, submitted to IEEE conference", "summary": "This paper introduces DRANet-SWD, an extension of existing work that\ndisentangles content and style representations of images for unsupervised\ndomain adaptation (UDA). The approach builds upon DRANet by incorporating the\nsliced Wasserstein discrepancy (SWD) as a style loss instead of the traditional\nGram matrix loss. The potential advantages of SWD over the Gram matrix loss for\ncapturing style variations in domain adaptation are investigated. Experiments\nusing digit classification datasets and driving scenario segmentation validate\nthe method, demonstrating that DRANet-SWD enhances performance. Results\nindicate that SWD provides a more robust statistical comparison of feature\ndistributions, leading to better style adaptation. These findings highlight the\neffectiveness of SWD in refining feature alignment and improving domain\nadaptation tasks across these benchmarks. Our code can be found here.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02906", "pdf": "https://arxiv.org/pdf/2504.02906", "abs": "https://arxiv.org/abs/2504.02906", "authors": ["Zhihan Zhang", "Yixin Cao", "Lizi Liao"], "title": "Enhancing Chart-to-Code Generation in Multimodal Large Language Models via Iterative Dual Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 5 figures", "summary": "Chart-to-code generation, the process of converting chart images into\nexecutable plotting scripts, provides a lossless representation of chart\ninformation, requiring models to accurately capture and summarize all visual\nand structural elements. However, this remains a significant challenge for\nmultimodal large language models (MLLMs), which are not inherently well-aligned\nwith code generation tasks. To bridge this gap, we introduce Chart2Code, a\nnovel iterative dual preference learning framework designed to enhance MLLMs'\nchart-to-code generation capabilities through structured code variant\ngeneration and fine-grained dual reward signals. We validate Chart2Code across\nthree MLLMs and find that iterative preference learning consistently improves\nout-of-distribution chart-to-code generation quality. Throughout this process,\nour dual scoring method, which evaluates both the textual code structure and\nits visual representation, leads to greater performance improvements, even with\na reduced preference dataset size. Further analysis explores the key components\nof our framework and highlights the interplay between chart-to-code generation\nand broader chart reasoning, paving the way for future advancements in chart\ncomprehension.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "code generation", "fine-grained"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02953", "pdf": "https://arxiv.org/pdf/2504.02953", "abs": "https://arxiv.org/abs/2504.02953", "authors": ["Chen Cecilia Liu", "Anna Korhonen", "Iryna Gurevych"], "title": "Cultural Learning-Based Culture Adaptation of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Adapting large language models (LLMs) to diverse cultural values is a\nchallenging task, as existing LLMs often reflect the values of specific groups\nby default, and potentially causing harm to others. In this paper, we present\nCLCA, a novel framework for enhancing LLM alignment with cultural values based\non cultural learning. The framework leverages simulated social interactions to\ngenerate conversations in which LLMs engage in role-playing within culturally\nadapted social scenarios, capturing implicit cultural norms for model\nfine-tuning. CLCA improves cultural value alignment across various model\narchitectures measured using World Value Survey data, demonstrating the\neffectiveness of our proposed approach. Our results provide early evidence that\nunderstanding intent and social interactions can enhance cultural value\nadaptation in LLMs, highlighting the promise of training approaches based on\ncultural learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03206", "pdf": "https://arxiv.org/pdf/2504.03206", "abs": "https://arxiv.org/abs/2504.03206", "authors": ["Yanming Wan", "Jiaxing Wu", "Marwa Abdulhai", "Lior Shani", "Natasha Jaques"], "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["helpfulness", "safety", "accuracy", "dialogue"], "score": 4}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03640", "pdf": "https://arxiv.org/pdf/2504.03640", "abs": "https://arxiv.org/abs/2504.03640", "authors": ["Kate Sanders", "Benjamin Van Durme"], "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50, 68T37", "I.2.7"], "comment": "9 pages, preprint", "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02867", "pdf": "https://arxiv.org/pdf/2504.02867", "abs": "https://arxiv.org/abs/2504.02867", "authors": ["Hongliu Cao", "Ilias Driouich", "Robin Singh", "Eoin Thomas"], "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at SophiaSummit2024", "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\ndiverse domains, yet they still encounter challenges such as insufficient\ndomain-specific knowledge, biases, and hallucinations. This underscores the\nneed for robust evaluation methodologies to accurately assess LLM-based\napplications. Traditional evaluation methods, which rely on word overlap or\ntext embeddings, are inadequate for capturing the nuanced semantic information\nnecessary to evaluate dynamic, open-ended text generation. Recent research has\nexplored leveraging LLMs to mimic human reasoning and decision-making processes\nfor evaluation purposes known as LLM-as-a-judge framework. However, these\nexisting frameworks have two significant limitations. First, they lack the\nflexibility to adapt to different text styles, including various answer and\nground truth styles, thereby reducing their generalization performance. Second,\nthe evaluation scores produced by these frameworks are often skewed and hard to\ninterpret, showing a low correlation with human judgment. To address these\nchallenges, we propose a novel dynamic multi-agent system that automatically\ndesigns personalized LLM judges for various natural language generation\napplications. This system iteratively refines evaluation prompts and balances\nthe trade-off between the adaptive requirements of downstream tasks and the\nalignment with human perception. Our experimental results show that the\nproposed multi-agent LLM Judge framework not only enhances evaluation accuracy\ncompared to existing methods but also produces evaluation scores that better\nalign with human perception.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "accuracy"], "score": 3}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.02881", "pdf": "https://arxiv.org/pdf/2504.02881", "abs": "https://arxiv.org/abs/2504.02881", "authors": ["Nick Whitehouse", "Nicole Lincoln", "Stephanie Yiu", "Lizzie Catterson", "Rivindu Perera"], "title": "Better Bill GPT: Comparing Large Language Models against Legal Invoice Reviewers", "categories": ["cs.CL"], "comment": null, "summary": "Legal invoice review is a costly, inconsistent, and time-consuming process,\ntraditionally performed by Legal Operations, Lawyers or Billing Specialists who\nscrutinise billing compliance line by line. This study presents the first\nempirical comparison of Large Language Models (LLMs) against human invoice\nreviewers - Early-Career Lawyers, Experienced Lawyers, and Legal Operations\nProfessionals-assessing their accuracy, speed, and cost-effectiveness.\nBenchmarking state-of-the-art LLMs against a ground truth set by expert legal\nprofessionals, our empirically substantiated findings reveal that LLMs\ndecisively outperform humans across every metric. In invoice approval\ndecisions, LLMs achieve up to 92% accuracy, surpassing the 72% ceiling set by\nexperienced lawyers. On a granular level, LLMs dominate line-item\nclassification, with top models reaching F-scores of 81%, compared to just 43%\nfor the best-performing human group. Speed comparisons are even more striking -\nwhile lawyers take 194 to 316 seconds per invoice, LLMs are capable of\ncompleting reviews in as fast as 3.6 seconds. And cost? AI slashes review\nexpenses by 99.97%, reducing invoice processing costs from an average of $4.27\nper invoice for human invoice reviewers to mere cents. These results highlight\nthe evolving role of AI in legal spend management. As law firms and corporate\nlegal departments struggle with inefficiencies, this study signals a seismic\nshift: The era of LLM-powered legal spend management is not on the horizon, it\nhas arrived. The challenge ahead is not whether AI can perform as well as human\nreviewers, but how legal teams will strategically incorporate it, balancing\nautomation with human discretion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03026", "pdf": "https://arxiv.org/pdf/2504.03026", "abs": "https://arxiv.org/abs/2504.03026", "authors": ["Yiran Xu", "Siqi Xie", "Zhuofang Li", "Harris Shadmany", "Yinxiao Li", "Luciano Sbaiz", "Miaosen Wang", "Junjie Ke", "Jose Lezama", "Hang Qi", "Han Zhang", "Jesse Berent", "Ming-Hsuan Yang", "Irfan Essa", "Jia-Bin Huang", "Feng Yang"], "title": "HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations", "categories": ["cs.CV"], "comment": null, "summary": "Image retargeting aims to change the aspect-ratio of an image while\nmaintaining its content and structure with less visual artifacts. Existing\nmethods still generate many artifacts or fail to maintain original content or\nstructure. To address this, we introduce HALO, an end-to-end trainable solution\nfor image retargeting. Since humans are more sensitive to distortions in\nsalient areas than non-salient areas of an image, HALO decomposes the input\nimage into salient/non-salient layers and applies different wrapping fields to\ndifferent layers. To further minimize the structure distortion in the output\nimages, we propose perceptual structure similarity loss which measures the\nstructure similarity between input and output images and aligns with human\nperception. Both quantitative results and a user study on the RetargetMe\ndataset show that HALO achieves SOTA. Especially, our method achieves an 18.4%\nhigher user preference compared to the baselines on average.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03140", "pdf": "https://arxiv.org/pdf/2504.03140", "abs": "https://arxiv.org/abs/2504.03140", "authors": ["Xuran Ma", "Yexin Liu", "Yaofu Liu", "Xianfeng Wu", "Mingzhe Zheng", "Zihao Wang", "Ser-Nam Lim", "Harry Yang"], "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03151", "pdf": "https://arxiv.org/pdf/2504.03151", "abs": "https://arxiv.org/abs/2504.03151", "authors": ["Jing Bi", "Susan Liang", "Xiaofei Zhou", "Pinxin Liu", "Junjia Guo", "Yunlong Tang", "Luchuan Song", "Chao Huang", "Guangyu Sun", "Jinxi He", "Jiarui Wu", "Shu Yang", "Daoan Zhang", "Chen Chen", "Lianggong Bruce Wen", "Zhang Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03193", "pdf": "https://arxiv.org/pdf/2504.03193", "abs": "https://arxiv.org/abs/2504.03193", "authors": ["Xin Zhang", "Robby T. Tan"], "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained\ntraction in Domain Generalized Semantic Segmentation (DGSS) due to their strong\ngeneralization capabilities. However, existing DGSS methods often rely\nexclusively on either VFMs or VLMs, overlooking their complementary strengths.\nVFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g.,\nCLIP) provide robust text alignment but struggle with coarse granularity.\nDespite their complementary strengths, effectively integrating VFMs and VLMs\nwith attention mechanisms is challenging, as the increased patch tokens\ncomplicate long-sequence modeling. To address this, we propose MFuser, a novel\nMamba-based fusion framework that efficiently combines the strengths of VFMs\nand VLMs while maintaining linear scalability in sequence length. MFuser\nconsists of two key components: MVFuser, which acts as a co-adapter to jointly\nfine-tune the two models by capturing both sequential and spatial dynamics; and\nMTEnhancer, a hybrid attention-Mamba module that refines text embeddings by\nincorporating image priors. Our approach achieves precise feature locality and\nstrong text alignment without incurring significant computational overhead.\nExtensive experiments demonstrate that MFuser significantly outperforms\nstate-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and\n71.87 mIoU on real-to-real benchmarks. The code is available at\nhttps://github.com/devinxzhang/MFuser.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03198", "pdf": "https://arxiv.org/pdf/2504.03198", "abs": "https://arxiv.org/abs/2504.03198", "authors": ["Jiaxin Guo", "Wenzhen Dong", "Tianyu Huang", "Hao Ding", "Ziyi Wang", "Haomin Kuang", "Qi Dou", "Yun-Hui Liu"], "title": "Endo3R: Unified Online Reconstruction from Dynamic Monocular Endoscopic Video", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from monocular surgical videos can enhance surgeon's\nperception and therefore plays a vital role in various computer-assisted\nsurgery tasks. However, achieving scale-consistent reconstruction remains an\nopen challenge due to inherent issues in endoscopic videos, such as dynamic\ndeformations and textureless surfaces. Despite recent advances, current methods\neither rely on calibration or instrument priors to estimate scale, or employ\nSfM-like multi-stage pipelines, leading to error accumulation and requiring\noffline optimization. In this paper, we present Endo3R, a unified 3D foundation\nmodel for online scale-consistent reconstruction from monocular surgical video,\nwithout any priors or extra optimization. Our model unifies the tasks by\npredicting globally aligned pointmaps, scale-consistent video depths, and\ncamera parameters without any offline optimization. The core contribution of\nour method is expanding the capability of the recent pairwise reconstruction\nmodel to long-term incremental dynamic reconstruction by an uncertainty-aware\ndual memory mechanism. The mechanism maintains history tokens of both\nshort-term dynamics and long-term spatial consistency. Notably, to tackle the\nhighly dynamic nature of surgical scenes, we measure the uncertainty of tokens\nvia Sampson distance and filter out tokens with high uncertainty. Regarding the\nscarcity of endoscopic datasets with ground-truth depth and camera poses, we\nfurther devise a self-supervised mechanism with a novel dynamics-aware flow\nloss. Abundant experiments on SCARED and Hamlyn datasets demonstrate our\nsuperior performance in zero-shot surgical video depth prediction and camera\npose estimation with online efficiency. Project page:\nhttps://wrld.github.io/Endo3R/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03174", "pdf": "https://arxiv.org/pdf/2504.03174", "abs": "https://arxiv.org/abs/2504.03174", "authors": ["Abhishek Singhania", "Christophe Dupuy", "Shivam Mangale", "Amani Namboori"], "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs", "categories": ["cs.CL"], "comment": "Accepted at TrustNLP@NAACL 2025", "summary": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03380", "pdf": "https://arxiv.org/pdf/2504.03380", "abs": "https://arxiv.org/abs/2504.03380", "authors": ["Sanghwan Bae", "Jiwoo Hong", "Min Young Lee", "Hanbyul Kim", "JeongYeon Nam", "Donghyun Kwak"], "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03486", "pdf": "https://arxiv.org/pdf/2504.03486", "abs": "https://arxiv.org/abs/2504.03486", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "accuracy", "summarization"], "score": 5}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03602", "pdf": "https://arxiv.org/pdf/2504.03602", "abs": "https://arxiv.org/abs/2504.03602", "authors": ["Kai Lascheit", "Daniel Barath", "Marc Pollefeys", "Leonidas Guibas", "Francis Engelmann"], "title": "Robust Human Registration with Body Part Segmentation on Noisy Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Registering human meshes to 3D point clouds is essential for applications\nsuch as augmented reality and human-robot interaction but often yields\nimprecise results due to noise and background clutter in real-world data. We\nintroduce a hybrid approach that incorporates body-part segmentation into the\nmesh fitting process, enhancing both human pose estimation and segmentation\naccuracy. Our method first assigns body part labels to individual points, which\nthen guide a two-step SMPL-X fitting: initial pose and orientation estimation\nusing body part centroids, followed by global refinement of the point cloud\nalignment. Additionally, we demonstrate that the fitted human mesh can refine\nbody part labels, leading to improved segmentation. Evaluations on the\ncluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that\nour approach significantly outperforms prior methods in both pose estimation\nand segmentation accuracy. Code and results are available on our project\nwebsite: https://segfit.github.io", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
{"id": "2504.03160", "pdf": "https://arxiv.org/pdf/2504.03160", "abs": "https://arxiv.org/abs/2504.03160", "authors": ["Yuxiang Zheng", "Dayuan Fu", "Xiangkun Hu", "Xiaojie Cai", "Lyumanshan Ye", "Pengrui Lu", "Pengfei Liu"], "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["honesty"], "score": 1}}, "source_file": "2025-04-07.jsonl"}
