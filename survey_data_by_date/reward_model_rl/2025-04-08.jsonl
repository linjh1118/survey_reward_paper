{"id": "2504.03790", "pdf": "https://arxiv.org/pdf/2504.03790", "abs": "https://arxiv.org/abs/2504.03790", "authors": ["Gonçalo Faria", "Noah A. Smith"], "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Increasing test-time computation has emerged as a promising direction for\nimproving language model performance, particularly in scenarios where model\nfinetuning is impractical or impossible due to computational constraints or\nprivate model weights. However, existing test-time search methods using a\nreward model (RM) often degrade in quality as compute scales, due to the\nover-optimization of what are inherently imperfect reward proxies. We introduce\nQAlign, a new test-time alignment approach. As we scale test-time compute,\nQAlign converges to sampling from the optimal aligned distribution for each\nindividual prompt. By adopting recent advances in Markov chain Monte Carlo for\ntext generation, our method enables better-aligned outputs without modifying\nthe underlying model or even requiring logit access. We demonstrate the\neffectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and\nGSM-Symbolic) using a task-specific RM, showing consistent improvements over\nexisting test-time compute methods like best-of-n and majority voting.\nFurthermore, when applied with more realistic RMs trained on the Tulu 3\npreference dataset, QAlign outperforms direct preference optimization (DPO),\nbest-of-n, majority voting, and weighted majority voting on a diverse range of\ndatasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical\nsolution to aligning language models at test time using additional computation\nwithout degradation, our approach expands the limits of the capability that can\nbe obtained from off-the-shelf language models without further training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scale", "test-time compute"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294", "abs": "https://arxiv.org/abs/2504.05294", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 5 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment", "reward hacking"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04699", "pdf": "https://arxiv.org/pdf/2504.04699", "abs": "https://arxiv.org/abs/2504.04699", "authors": ["Martin Weyssow", "Chengran Yang", "Junkai Chen", "Yikun Li", "Huihui Huang", "Ratnadira Widyasari", "Han Wei Ang", "Frank Liauw", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "title": "R2Vul: Learning to Reason about Software Vulnerabilities with Reinforcement Learning and Structured Reasoning Distillation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown promising performance in software\nvulnerability detection (SVD), yet their reasoning capabilities remain\nunreliable. Existing approaches relying on chain-of-thought (CoT) struggle to\nprovide relevant and actionable security assessments. Additionally, effective\nSVD requires not only generating coherent reasoning but also differentiating\nbetween well-founded and misleading yet plausible security assessments, an\naspect overlooked in prior work. To this end, we introduce R2Vul, a novel\napproach that distills structured reasoning into small LLMs using reinforcement\nlearning from AI feedback (RLAIF). Through RLAIF, R2Vul enables LLMs to produce\nstructured, security-aware reasoning that is actionable and reliable while\nexplicitly learning to distinguish valid assessments from misleading ones. We\nevaluate R2Vul across five languages against SAST tools, CoT, instruction\ntuning, and classification-based baselines. Our results show that R2Vul with\nstructured reasoning distillation enables a 1.5B student LLM to rival larger\nmodels while improving generalization to out-of-distribution vulnerabilities.\nBeyond model improvements, we contribute a large-scale, multilingual preference\ndataset featuring structured reasoning to support future research in SVD.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "RLAIF", "AI feedback"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050", "abs": "https://arxiv.org/abs/2504.05050", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04753", "pdf": "https://arxiv.org/pdf/2504.04753", "abs": "https://arxiv.org/abs/2504.04753", "authors": ["Cheng Chen", "Jiacheng Wei", "Tianrun Chen", "Chi Zhang", "Xiaofeng Yang", "Shangzhan Zhang", "Bingchen Yang", "Chuan-Sheng Foo", "Guosheng Lin", "Qixing Huang", "Fayao Liu"], "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Creating CAD digital twins from the physical world is crucial for\nmanufacturing, design, and simulation. However, current methods typically rely\non costly 3D scanning with labor-intensive post-processing. To provide a\nuser-friendly design process, we explore the problem of reverse engineering\nfrom unconstrained real-world CAD images that can be easily captured by users\nof all experiences. However, the scarcity of real-world CAD data poses\nchallenges in directly training such models. To tackle these challenges, we\npropose CADCrafter, an image-to-parametric CAD model generation framework that\ntrains solely on synthetic textureless CAD data while testing on real-world\nimages. To bridge the significant representation disparity between images and\nparametric CAD models, we introduce a geometry encoder to accurately capture\ndiverse geometric features. Moreover, the texture-invariant properties of the\ngeometric features can also facilitate the generalization to real-world\nscenarios. Since compiling CAD parameter sequences into explicit CAD models is\na non-differentiable process, the network training inherently lacks explicit\ngeometric supervision. To impose geometric validity constraints, we employ\ndirect preference optimization (DPO) to fine-tune our model with the automatic\ncode checker feedback on CAD sequence quality. Furthermore, we collected a\nreal-world dataset, comprised of multi-view images and corresponding CAD\ncommand sequence pairs, to evaluate our method. Experimental results\ndemonstrate that our approach can robustly handle real unconstrained CAD\nimages, and even generalize to unseen general objects.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04736", "pdf": "https://arxiv.org/pdf/2504.04736", "abs": "https://arxiv.org/abs/2504.04736", "authors": ["Anna Goldie", "Azalia Mirhoseini", "Hao Zhou", "Irene Cai", "Christopher D. Manning"], "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning has been shown to improve the performance of large\nlanguage models. However, traditional approaches like RLHF or RLAIF treat the\nproblem as single-step. As focus shifts toward more complex reasoning and\nagentic tasks, language models must take multiple steps of text generation,\nreasoning and environment interaction before generating a solution. We propose\na synthetic data generation and RL methodology targeting multi-step\noptimization scenarios. This approach, called Step-Wise Reinforcement Learning\n(SWiRL), iteratively generates multi-step reasoning and tool use data, and then\nlearns from that data. It employs a simple step-wise decomposition that breaks\neach multi-step trajectory into multiple sub-trajectories corresponding to each\naction by the original model. It then applies synthetic data filtering and RL\noptimization on these sub-trajectories. We evaluated SWiRL on a number of\nmulti-step tool use, question answering, and mathematical reasoning tasks. Our\nexperiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%,\n14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA,\nMuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits\ngeneralization across tasks: for example, training only on HotPotQA (text\nquestion-answering) improves zero-shot performance on GSM8K (a math dataset) by\na relative 16.9%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "RLAIF"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03846", "pdf": "https://arxiv.org/pdf/2504.03846", "abs": "https://arxiv.org/abs/2504.03846", "authors": ["Wei-Lin Chen", "Zhepei Wei", "Xinyu Zhu", "Shi Feng", "Yu Meng"], "title": "Do LLM Evaluators Prefer Themselves for a Reason?", "categories": ["cs.CL"], "comment": "Preprint. 31 pages", "summary": "Large language models (LLMs) are increasingly used as automatic evaluators in\napplications such as benchmarking, reward modeling, and self-refinement. Prior\nwork highlights a potential self-preference bias where LLMs favor their own\ngenerated responses, a tendency often intensifying with model size and\ncapability. This raises a critical question: Is self-preference detrimental, or\ndoes it simply reflect objectively superior outputs from more capable models?\nDisentangling these has been challenging due to the usage of subjective tasks\nin previous studies. To address this, we investigate self-preference using\nverifiable benchmarks (mathematical reasoning, factual knowledge, code\ngeneration) that allow objective ground-truth assessment. This enables us to\ndistinguish harmful self-preference (favoring objectively worse responses) from\nlegitimate self-preference (favoring genuinely superior ones). We conduct\nlarge-scale experiments under controlled evaluation conditions across diverse\nmodel families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our\nfindings reveal three key insights: (1) Better generators are better judges --\nLLM evaluators' accuracy strongly correlates with their task performance, and\nmuch of the self-preference in capable models is legitimate. (2) Harmful\nself-preference persists, particularly when evaluator models perform poorly as\ngenerators on specific task instances. Stronger models exhibit more pronounced\nharmful bias when they err, though such incorrect generations are less\nfrequent. (3) Inference-time scaling strategies, such as generating a long\nChain-of-Thought before evaluation, effectively reduce the harmful\nself-preference. These results provide a more nuanced understanding of\nLLM-based evaluation and practical insights for improving its reliability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "scale"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04158", "pdf": "https://arxiv.org/pdf/2504.04158", "abs": "https://arxiv.org/abs/2504.04158", "authors": ["Yunlong Lin", "Zixu Lin", "Haoyu Chen", "Panwang Pan", "Chenxin Li", "Sixiang Chen", "Yeying Jin", "Wenbo Li", "Xinghao Ding"], "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration", "categories": ["cs.CV"], "comment": "25 pages, 15 figures", "summary": "Vision-centric perception systems struggle with unpredictable and coupled\nweather degradations in the wild. Current solutions are often limited, as they\neither depend on specific degradation priors or suffer from significant domain\ngaps. To enable robust and autonomous operation in real-world conditions, we\npropose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to\nmanage multiple expert restoration models. To further enhance system\nrobustness, reduce hallucinations, and improve generalizability in real-world\nadverse weather, JarvisIR employs a novel two-stage framework consisting of\nsupervised fine-tuning and human feedback alignment. Specifically, to address\nthe lack of paired data in real-world scenarios, the human feedback alignment\nenables the VLM to be fine-tuned effectively on large-scale real-world data in\nan unsupervised manner. To support the training and evaluation of JarvisIR, we\nintroduce CleanBench, a comprehensive dataset consisting of high-quality and\nlarge-scale instruction-responses pairs, including 150K synthetic entries and\n80K real entries. Extensive experiments demonstrate that JarvisIR exhibits\nsuperior decision-making and restoration capabilities. Compared with existing\nmethods, it achieves a 50% improvement in the average of all perception metrics\non CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04953", "pdf": "https://arxiv.org/pdf/2504.04953", "abs": "https://arxiv.org/abs/2504.04953", "authors": ["José Pombal", "Dongkeun Yoon", "Patrick Fernandes", "Ian Wu", "Seungone Kim", "Ricardo Rei", "Graham Neubig", "André F. T. Martins"], "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154", "abs": "https://arxiv.org/abs/2504.05154", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "title": "CARE: Aligning Language Models for Regional Cultural Awareness", "categories": ["cs.CL"], "comment": "24 pages", "summary": "Existing language models (LMs) often exhibit a Western-centric bias and\nstruggle to represent diverse cultural knowledge. Previous attempts to address\nthis rely on synthetic data and express cultural knowledge only in English. In\nthis work, we study whether a small amount of human-written, multilingual\ncultural preference data can improve LMs across various model families and\nsizes. We first introduce CARE, a multilingual resource of 24.1k responses with\nhuman preferences on 2,580 questions about Chinese and Arab cultures, all\ncarefully annotated by native speakers and offering more balanced coverage.\nUsing CARE, we demonstrate that cultural alignment improves existing LMs beyond\ngeneric resources without compromising general capabilities. Moreover, we\nevaluate the cultural awareness of LMs, native speakers, and retrieved web\ncontent when queried in different languages. Our experiment reveals regional\ndisparities among LMs, which may also be reflected in the documentation gap:\nnative speakers often take everyday cultural commonsense and social norms for\ngranted, while non-natives are more likely to actively seek out and document\nthem. CARE is publicly available at https://github.com/Guochry/CARE (we plan to\nadd Japanese data in the near future).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04740", "pdf": "https://arxiv.org/pdf/2504.04740", "abs": "https://arxiv.org/abs/2504.04740", "authors": ["Samarth Mishra", "Kate Saenko", "Venkatesh Saligrama"], "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947", "abs": "https://arxiv.org/abs/2504.03947", "authors": ["Chris Samarinas", "Hamed Zamani"], "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04596", "pdf": "https://arxiv.org/pdf/2504.04596", "abs": "https://arxiv.org/abs/2504.04596", "authors": ["Noga Ben Yoash", "Meni Brief", "Oded Ovadia", "Gil Shenderovitz", "Moshik Mishaeli", "Rachel Lemberg", "Eitam Sheetrit"], "title": "SECQUE: A Benchmark for Evaluating Real-World Financial Analysis Capabilities", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Benchmark available at:\n  https://huggingface.co/datasets/nogabenyoash/SecQue", "summary": "We introduce SECQUE, a comprehensive benchmark for evaluating large language\nmodels (LLMs) in financial analysis tasks. SECQUE comprises 565 expert-written\nquestions covering SEC filings analysis across four key categories: comparison\nanalysis, ratio calculation, risk assessment, and financial insight generation.\nTo assess model performance, we develop SECQUE-Judge, an evaluation mechanism\nleveraging multiple LLM-based judges, which demonstrates strong alignment with\nhuman evaluations. Additionally, we provide an extensive analysis of various\nmodels' performance on our benchmark. By making SECQUE publicly available, we\naim to facilitate further research and advancements in financial AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03906", "pdf": "https://arxiv.org/pdf/2504.03906", "abs": "https://arxiv.org/abs/2504.03906", "authors": ["Abhilekh Borah", "Hasnat Md Abdullah", "Kangda Wei", "Ruihong Huang"], "title": "CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "The rise of Large Language Models (LLMs) has raised questions about their\nability to understand climate-related contexts. Though climate change dominates\nsocial media, analyzing its multimodal expressions is understudied, and current\ntools have failed to determine whether LLMs amplify credible solutions or\nspread unsubstantiated claims. To address this, we introduce CliME (Climate\nChange Multimodal Evaluation), a first-of-its-kind multimodal dataset,\ncomprising 2579 Twitter and Reddit posts. The benchmark features a diverse\ncollection of humorous memes and skeptical posts, capturing how these formats\ndistill complex issues into viral narratives that shape public opinion and\npolicy discussions. To systematically evaluate LLM performance, we present the\nClimate Alignment Quotient (CAQ), a novel metric comprising five distinct\ndimensions: Articulation, Evidence, Resonance, Transition, and Specificity.\nAdditionally, we propose three analytical lenses: Actionability, Criticality,\nand Justice, to guide the assessment of LLM-generated climate discourse using\nCAQ. Our findings, based on the CAQ metric, indicate that while most evaluated\nLLMs perform relatively well in Criticality and Justice, they consistently\nunderperform on the Actionability axis. Among the models evaluated, Claude 3.7\nSonnet achieves the highest overall performance. We publicly release our CliME\ndataset and code to foster further research in this domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03807", "pdf": "https://arxiv.org/pdf/2504.03807", "abs": "https://arxiv.org/abs/2504.03807", "authors": ["Maliheh Toozandehjani", "Ali Mousavi", "Reza Taheri"], "title": "From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images", "categories": ["cs.CV"], "comment": "in Persian language", "summary": "The aim of image-based virtual try-on is to generate realistic images of\nindividuals wearing target garments, ensuring that the pose, body shape and\ncharacteristics of the target garment are accurately preserved. Existing\nmethods often fail to reproduce the fine details of target garments effectively\nand lack generalizability to new scenarios. In the proposed method, the\nperson's initial garment is completely removed. Subsequently, a precise warping\nis performed using the predicted keypoints to fully align the target garment\nwith the body structure and pose of the individual. Based on the warped\ngarment, a body segmentation map is more accurately predicted. Then, using an\nalignment-aware segment normalization, the misaligned areas between the warped\ngarment and the predicted garment region in the segmentation map are removed.\nFinally, the generator produces the final image with high visual quality,\nreconstructing the precise characteristics of the target garment, including its\noverall shape and texture. This approach emphasizes preserving garment\ncharacteristics and improving adaptability to various poses, providing better\ngeneralization for diverse applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03991", "pdf": "https://arxiv.org/pdf/2504.03991", "abs": "https://arxiv.org/abs/2504.03991", "authors": ["Siddharth Srikanth", "Varun Bhatt", "Boshen Zhang", "Werner Hager", "Charles Michael Lewis", "Katia P. Sycara", "Aaquib Tabrez", "Stefanos Nikolaidis"], "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04022", "pdf": "https://arxiv.org/pdf/2504.04022", "abs": "https://arxiv.org/abs/2504.04022", "authors": ["Essential AI", ":", "Darsh J Shah", "Peter Rushton", "Somanshu Singla", "Mohit Parmar", "Kurt Smith", "Yash Vanjani", "Ashish Vaswani", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Anthony Polloreno", "Ashish Tanwer", "Burhan Drak Sibai", "Divya S Mansingka", "Divya Shivaprasad", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Michael Callahan", "Michael Pust", "Mrinal Iyer", "Philip Monk", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Tim Romanski"], "title": "Rethinking Reflection in Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A language model's ability to reflect on its own reasoning provides a key\nadvantage for solving complex problems. While most recent research has focused\non how this ability develops during reinforcement learning, we show that it\nactually begins to emerge much earlier - during the model's pre-training. To\nstudy this, we introduce deliberate errors into chains-of-thought and test\nwhether the model can still arrive at the correct answer by recognizing and\ncorrecting these mistakes. By tracking performance across different stages of\npre-training, we observe that this self-correcting ability appears early and\nimproves steadily over time. For instance, an OLMo2-7B model pre-trained on 4\ntrillion tokens displays self-correction on our six self-reflection tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042", "abs": "https://arxiv.org/abs/2504.04042", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is a fundamental aspect of legal decision-making,\nenabling logical conclusions by connecting general legal principles with\nspecific case facts. Although existing large language models (LLMs) can\ngenerate responses to legal questions, they fail to perform explicit\nsyllogistic reasoning, often producing implicit and unstructured answers that\nlack explainability and trustworthiness. To address this limitation, we propose\nSyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic\nlegal reasoning. SyLeR integrates a tree-structured hierarchical retrieval\nmechanism to effectively combine relevant legal statutes and precedent cases,\nforming comprehensive major premises. This is followed by a two-stage\nfine-tuning process: supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning with a\nstructure-aware reward mechanism refines the ability of the model to generate\ndiverse logically sound and well-structured reasoning paths. We conducted\nextensive experiments across various dimensions, including in-domain and\ncross-domain user groups (legal laypersons and practitioners), multiple\nlanguages (Chinese and French), and different LLM backbones (legal-specific and\nopen-domain LLMs). The results show that SyLeR significantly improves response\naccuracy and consistently delivers explicit, explainable, and trustworthy legal\nreasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04010", "pdf": "https://arxiv.org/pdf/2504.04010", "abs": "https://arxiv.org/abs/2504.04010", "authors": ["Maksim Siniukov", "Di Chang", "Minh Tran", "Hongkun Gong", "Ashutosh Chaubey", "Mohammad Soleymani"], "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion", "categories": ["cs.CV", "cs.LG", "I.4.9"], "comment": "Project page: https://havent-invented.github.io/DiTaiListener", "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04025", "pdf": "https://arxiv.org/pdf/2504.04025", "abs": "https://arxiv.org/abs/2504.04025", "authors": ["Daniel Rivera", "Jacob Huddin", "Alexander Banerjee", "Rongzhen Zhang", "Brenda Mai", "Hanadi El Achi", "Jacob Armstrong", "Amer Wahed", "Andy Nguyen"], "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 6 figures, 1 table", "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04238", "pdf": "https://arxiv.org/pdf/2504.04238", "abs": "https://arxiv.org/abs/2504.04238", "authors": ["Yuheng Wu", "Wentao Guo", "Zirui Liu", "Heng Ji", "Zhaozhuo Xu", "Denghui Zhang"], "title": "Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in\nlarge language models (LLMs) from a mechanistic perspective, focusing on the\nrole of extremely sparse parameter patterns. We introduce a novel method to\nidentify ToM-sensitive parameters and reveal that perturbing as little as\n0.001% of these parameters significantly degrades ToM performance while also\nimpairing contextual localization and language understanding. To understand\nthis effect, we analyze their interaction with core architectural components of\nLLMs. Our findings demonstrate that these sensitive parameters are closely\nlinked to the positional encoding module, particularly in models using Rotary\nPosition Embedding (RoPE), where perturbations disrupt dominant-frequency\nactivations critical for contextual processing. Furthermore, we show that\nperturbing ToM-sensitive parameters affects LLM's attention mechanism by\nmodulating the angle between queries and keys under positional encoding. These\ninsights provide a deeper understanding of how LLMs acquire social reasoning\nabilities, bridging AI interpretability with cognitive science. Our results\nhave implications for enhancing model alignment, mitigating biases, and\nimproving AI systems designed for human interaction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04339", "pdf": "https://arxiv.org/pdf/2504.04339", "abs": "https://arxiv.org/abs/2504.04339", "authors": ["Peng Gao", "Yujian Lee", "Zailong Chen", "Hui zhang", "Xubo Liu", "Yiyang Hu", "Guquang Jing"], "title": "NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval", "categories": ["cs.CV"], "comment": "Has been accepted by ICASSP2025", "summary": "Composed Image Retrieval (CIR) seeks to find a target image using a\nmulti-modal query, which combines an image with modification text to pinpoint\nthe target. While recent CIR methods have shown promise, they mainly focus on\nexploring relationships between the query pairs (image and text) through data\naugmentation or model design. These methods often assume perfect alignment\nbetween queries and target images, an idealized scenario rarely encountered in\npractice. In reality, pairs are often partially or completely mismatched due to\nissues like inaccurate modification texts, low-quality target images, and\nannotation errors. Ignoring these mismatches leads to numerous False Positive\nPair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit\nand ultimately reducing its performance. To address this problem, we propose\nthe Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key\ncomponents: the Weight Compensation Block (WCB) and the Noise-pair Filter Block\n(NFB). The WCB coupled with diverse weight maps can ensure more stable token\nrepresentations of multi-modal queries and target images. Meanwhile, the NFB,\nin conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by\nevaluating loss distributions, and generates soft labels correspondingly,\nallowing for the design of the soft-label based Noise Contrastive Estimation\n(NCE) loss function. Consequently, the overall architecture helps to mitigate\nthe influence of mismatched and partially matched samples, with experimental\nresults demonstrating that NCL-CIR achieves exceptional performance on the\nbenchmark datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04348", "pdf": "https://arxiv.org/pdf/2504.04348", "abs": "https://arxiv.org/abs/2504.04348", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "annotation"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04473", "pdf": "https://arxiv.org/pdf/2504.04473", "abs": "https://arxiv.org/abs/2504.04473", "authors": ["Archana Sahu", "Plaban Kumar Bhowmick"], "title": "Directed Graph-alignment Approach for Identification of Gaps in Short Answers", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 11 figures", "summary": "In this paper, we have presented a method for identifying missing items known\nas gaps in the student answers by comparing them against the corresponding\nmodel answer/reference answers, automatically. The gaps can be identified at\nword, phrase or sentence level. The identified gaps are useful in providing\nfeedback to the students for formative assessment. The problem of gap\nidentification has been modelled as an alignment of a pair of directed graphs\nrepresenting a student answer and the corresponding model answer for a given\nquestion. To validate the proposed approach, the gap annotated student answers\nconsidering answers from three widely known datasets in the short answer\ngrading domain, namely, University of North Texas (UNT), SciEntsBank, and\nBeetle have been developed and this gap annotated student answers' dataset is\navailable at: https://github.com/sahuarchana7/gaps-answers-dataset. Evaluation\nmetrics used in the traditional machine learning tasks have been adopted to\nevaluate the task of gap identification. Though performance of the proposed\napproach varies across the datasets and the types of the answers, overall the\nperformance is observed to be promising.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04514", "pdf": "https://arxiv.org/pdf/2504.04514", "abs": "https://arxiv.org/abs/2504.04514", "authors": ["Yao Tao", "Yehui Tang", "Yun Wang", "Mingjian Zhu", "Hailin Hu", "Yunhe Wang"], "title": "Saliency-driven Dynamic Token Pruning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04534", "pdf": "https://arxiv.org/pdf/2504.04534", "abs": "https://arxiv.org/abs/2504.04534", "authors": ["Anantharaman Janakiraman", "Behnaz Ghoraani"], "title": "An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text summarization is crucial for mitigating information overload across\ndomains like journalism, medicine, and business. This research evaluates\nsummarization performance across 17 large language models (OpenAI, Google,\nAnthropic, open-source) using a novel multi-dimensional framework. We assessed\nmodels on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed,\nSAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using\nmetrics for factual consistency, semantic similarity, lexical overlap, and\nhuman-like quality, while also considering efficiency factors. Our findings\nreveal significant performance differences, with specific models excelling in\nfactual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and\nprocessing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash).\nPerformance varies dramatically by dataset, with models struggling on technical\ndomains but performing well on conversational content. We identified a critical\ntension between factual consistency (best at 50 tokens) and perceived quality\n(best at 150 tokens). Our analysis provides evidence-based recommendations for\ndifferent use cases, from high-stakes applications requiring factual accuracy\nto resource-constrained environments needing efficient processing. This\ncomprehensive approach enhances evaluation methodology by integrating quality\nmetrics with operational considerations, incorporating trade-offs between\naccuracy, efficiency, and cost-effectiveness to guide model selection for\nspecific applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "summarization", "multi-dimensional"], "score": 6}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04569", "pdf": "https://arxiv.org/pdf/2504.04569", "abs": "https://arxiv.org/abs/2504.04569", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations", "categories": ["cs.CL"], "comment": null, "summary": "In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "accuracy", "dialogue"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717", "abs": "https://arxiv.org/abs/2504.04717", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "After 136 days of meticulous preparation, we're thrilled to finally\n  share our comprehensive survey on llm multi-turn interactions with the\n  community!", "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dialogue"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04550", "pdf": "https://arxiv.org/pdf/2504.04550", "abs": "https://arxiv.org/abs/2504.04550", "authors": ["Alkesh Patel", "Vibhav Chitalia", "Yinfei Yang"], "title": "Advancing Egocentric Video Question Answering with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages", "summary": "Egocentric Video Question Answering (QA) requires models to handle\nlong-horizon temporal reasoning, first-person perspectives, and specialized\nchallenges like frequent camera movement. This paper systematically evaluates\nboth proprietary and open-source Multimodal Large Language Models (MLLMs) on\nQaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four\npopular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct)\nare assessed using zero-shot and fine-tuned approaches for both OpenQA and\nCloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in\nQaEgo4D, enabling more reliable comparison. Our results show that fine-tuned\nVideo-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art\nperformance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for\nOpenQA) and +13% accuracy (for CloseQA). We also present a thorough error\nanalysis, indicating the model's difficulty in spatial reasoning and\nfine-grained object recognition - key areas for future improvement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy", "question answering", "fine-grained"], "score": 5}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04963", "pdf": "https://arxiv.org/pdf/2504.04963", "abs": "https://arxiv.org/abs/2504.04963", "authors": ["Yuzhe Zhang", "Min Cen", "Hong Zhang"], "title": "Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "28pages, 3 figures. First submitted in Oct. 2023", "summary": "Distantly supervised named entity recognition (DS-NER) has been proposed to\nexploit the automatically labeled training data by external knowledge bases\ninstead of human annotations. However, it tends to suffer from a high false\nnegative rate due to the inherent incompleteness. To address this issue, we\npresent a novel approach called \\textbf{C}onstraint \\textbf{M}ulti-class\n\\textbf{P}ositive and \\textbf{U}nlabeled Learning (CMPU), which introduces a\nconstraint factor on the risk estimator of multiple positive classes. It\nsuggests that the constraint non-negative risk estimator is more robust against\noverfitting than previous PU learning methods with limited positive data. Solid\ntheoretical analysis on CMPU is provided to prove the validity of our approach.\nExtensive experiments on two benchmark datasets that were labeled using diverse\nexternal knowledge sources serve to demonstrate the superior performance of\nCMPU in comparison to existing DS-NER methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04976", "pdf": "https://arxiv.org/pdf/2504.04976", "abs": "https://arxiv.org/abs/2504.04976", "authors": ["Carlos Peláez-González", "Andrés Herrera-Poyatos", "Cristina Zuheros", "David Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera"], "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 5 figures", "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04633", "pdf": "https://arxiv.org/pdf/2504.04633", "abs": "https://arxiv.org/abs/2504.04633", "authors": ["Yanshu Li", "Hongyang He", "Yi Cao", "Qisen Cheng", "Xiang Fu", "Ruixiang Tang"], "title": "M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint, 28 pages, 10 figures, 15 tables", "summary": "Multimodal in-context learning (ICL) is a vital capability for Large\nVision-Language Models (LVLMs), allowing task adaptation via contextual prompts\nwithout parameter retraining. However, its application is hindered by the\ntoken-intensive nature of inputs and the high complexity of cross-modal\nfew-shot learning, which limits the expressive power of representation methods.\nTo tackle these challenges, we propose \\textbf{M2IV}, a method that substitutes\nexplicit demonstrations with learnable \\textbf{I}n-context \\textbf{V}ectors\ndirectly integrated into LVLMs. By exploiting the complementary strengths of\nmulti-head attention (\\textbf{M}HA) and multi-layer perceptrons (\\textbf{M}LP),\nM2IV achieves robust cross-modal fidelity and fine-grained semantic\ndistillation through training. This significantly enhances performance across\ndiverse LVLMs and tasks and scales efficiently to many-shot scenarios,\nbypassing the context window limitations. We also introduce \\textbf{VLibrary},\na repository for storing and retrieving M2IV, enabling flexible LVLM steering\nfor tasks like cross-modal alignment, customized generation and safety\nimprovement. Experiments across seven benchmarks and three LVLMs show that M2IV\nsurpasses Vanilla ICL and prior representation engineering approaches, with an\naverage accuracy gain of \\textbf{3.74\\%} over ICL with the same shot count,\nalongside substantial efficiency advantages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04722", "pdf": "https://arxiv.org/pdf/2504.04722", "abs": "https://arxiv.org/abs/2504.04722", "authors": ["Adnan Khan", "Alireza Choubineh", "Mai A. Shaaban", "Abbas Akkasi", "Majid Komeili"], "title": "TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment", "categories": ["cs.CV"], "comment": null, "summary": "Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss, as estimated by global\nprevalence data. However, traditional methods for creating these tactile\ngraphics are labor-intensive and struggle to meet demand. We introduce\nTactileNet, the first comprehensive dataset and AI-driven framework for\ngenerating tactile graphics using text-to-image Stable Diffusion (SD) models.\nBy integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes\nSD models to produce high-fidelity, guideline-compliant tactile graphics while\nreducing computational costs. Evaluations involving tactile experts show that\ngenerated graphics achieve 92.86% adherence to tactile standards and 100%\nalignment with natural images in posture and features. Our framework also\ndemonstrates scalability, generating 32,000 images (7,050 filtered for quality)\nacross 66 classes, with prompt editing enabling customizable outputs (e.g.,\nadding/removing details). Our work empowers designers to focus on refinement,\nsignificantly accelerating accessibility efforts. It underscores the\ntransformative potential of AI for social good, offering a scalable solution to\nbridge the accessibility gap in education and beyond.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05185", "pdf": "https://arxiv.org/pdf/2504.05185", "abs": "https://arxiv.org/abs/2504.05185", "authors": ["Mehdi Fatemi", "Banafsheh Rafiee", "Mingjie Tang", "Kartik Talamadupula"], "title": "Concise Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05211", "pdf": "https://arxiv.org/pdf/2504.05211", "abs": "https://arxiv.org/abs/2504.05211", "authors": ["Richard A. Blythe", "Casimir Fisch"], "title": "Exploiting individual differences to bootstrap communication", "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "comment": "13 pages including supplementary information, 3 figures", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03735", "pdf": "https://arxiv.org/pdf/2504.03735", "abs": "https://arxiv.org/abs/2504.03735", "authors": ["Erfan Shayegani", "G M Shahariar", "Sara Abdali", "Lei Yu", "Nael Abu-Ghazaleh", "Yue Dong"], "title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Multimodal Language Models (MMLMs) typically undergo post-training alignment\nto prevent harmful content generation. However, these alignment stages focus\nprimarily on the assistant role, leaving the user role unaligned, and stick to\na fixed input prompt structure of special tokens, leaving the model vulnerable\nwhen inputs deviate from these expectations. We introduce Role-Modality Attacks\n(RMA), a novel class of adversarial attacks that exploit role confusion between\nthe user and assistant and alter the position of the image token to elicit\nharmful outputs. Unlike existing attacks that modify query content, RMAs\nmanipulate the input structure without altering the query itself. We\nsystematically evaluate these attacks across multiple Vision Language Models\n(VLMs) on eight distinct settings, showing that they can be composed to create\nstronger adversarial prompts, as also evidenced by their increased projection\nin the negative refusal direction in the residual stream, a property observed\nin prior successful attacks. Finally, for mitigation, we propose an adversarial\ntraining approach that makes the model robust against input prompt\nperturbations. By training the model on a range of harmful and benign prompts\nall perturbed with different RMA settings, it loses its sensitivity to Role\nConfusion and Modality Manipulation attacks and is trained to only pay\nattention to the content of the query in the input prompt structure,\neffectively reducing Attack Success Rate (ASR) while preserving the model's\ngeneral utility.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04841", "pdf": "https://arxiv.org/pdf/2504.04841", "abs": "https://arxiv.org/abs/2504.04841", "authors": ["Sebastian Schmidt", "Julius Körner", "Dominik Fuchsgruber", "Stefano Gasperini", "Federico Tombari", "Stephan Günnemann"], "title": "Prior2Former -- Evidential Modeling of Mask Transformers for Assumption-Free Open-World Panoptic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In panoptic segmentation, individual instances must be separated within\nsemantic classes. As state-of-the-art methods rely on a pre-defined set of\nclasses, they struggle with novel categories and out-of-distribution (OOD)\ndata. This is particularly problematic in safety-critical applications, such as\nautonomous driving, where reliability in unseen scenarios is essential. We\naddress the gap between outstanding benchmark performance and reliability by\nproposing Prior2Former (P2F), the first approach for segmentation vision\ntransformers rooted in evidential learning. P2F extends the mask vision\ntransformer architecture by incorporating a Beta prior for computing model\nuncertainty in pixel-wise binary mask assignments. This design enables\nhigh-quality uncertainty estimation that effectively detects novel and OOD\nobjects enabling state-of-the-art anomaly instance segmentation and open-world\npanoptic segmentation. Unlike most segmentation models addressing unknown\nclasses, P2F operates without access to OOD data samples or contrastive\ntraining on void (i.e., unlabeled) classes, making it highly applicable in\nreal-world scenarios where such prior information is unavailable. Additionally,\nP2F can be flexibly applied to anomaly instance and panoptic segmentation.\nThrough comprehensive experiments on the Cityscapes, COCO, SegmentMeIfYouCan,\nand OoDIS datasets, we demonstrate the state-of-the-art performance of P2F. It\nachieves the highest ranking in the OoDIS anomaly instance benchmark among\nmethods not using OOD data in any way.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "reliability"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04842", "pdf": "https://arxiv.org/pdf/2504.04842", "abs": "https://arxiv.org/abs/2504.04842", "authors": ["Mengchao Wang", "Qiang Wang", "Fan Jiang", "Yaqi Fan", "Yunpeng Zhang", "Yonggang Qi", "Kun Zhao", "Mu Xu"], "title": "FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Creating a realistic animatable avatar from a single static portrait remains\nchallenging. Existing approaches often struggle to capture subtle facial\nexpressions, the associated global body movements, and the dynamic background.\nTo address these limitations, we propose a novel framework that leverages a\npretrained video diffusion transformer model to generate high-fidelity,\ncoherent talking portraits with controllable motion dynamics. At the core of\nour work is a dual-stage audio-visual alignment strategy. In the first stage,\nwe employ a clip-level training scheme to establish coherent global motion by\naligning audio-driven dynamics across the entire scene, including the reference\nportrait, contextual objects, and background. In the second stage, we refine\nlip movements at the frame level using a lip-tracing mask, ensuring precise\nsynchronization with audio signals. To preserve identity without compromising\nmotion flexibility, we replace the commonly used reference network with a\nfacial-focused cross-attention module that effectively maintains facial\nconsistency throughout the video. Furthermore, we integrate a motion intensity\nmodulation module that explicitly controls expression and body motion\nintensity, enabling controllable manipulation of portrait movements beyond mere\nlip motion. Extensive experimental results show that our proposed approach\nachieves higher quality with better realism, coherence, motion intensity, and\nidentity preservation. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04907", "pdf": "https://arxiv.org/pdf/2504.04907", "abs": "https://arxiv.org/abs/2504.04907", "authors": ["Hui Han", "Siyuan Li", "Jiaqi Chen", "Yiwen Yuan", "Yuling Wu", "Chak Tou Leong", "Hanwen Du", "Junchen Fu", "Youhua Li", "Jie Zhang", "Chi Zhang", "Li-jia Li", "Yongxin Ni"], "title": "Video-Bench: Human-Aligned Video Generation Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR'25", "summary": "Video generation assessment is essential for ensuring that generative models\nproduce visually realistic, high-quality videos while aligning with human\nexpectations. Current video generation benchmarks fall into two main\ncategories: traditional benchmarks, which use metrics and embeddings to\nevaluate generated video quality across multiple dimensions but often lack\nalignment with human judgments; and large language model (LLM)-based\nbenchmarks, though capable of human-like reasoning, are constrained by a\nlimited understanding of video quality metrics and cross-modal consistency. To\naddress these challenges and establish a benchmark that better aligns with\nhuman preferences, this paper introduces Video-Bench, a comprehensive benchmark\nfeaturing a rich prompt suite and extensive evaluation dimensions. This\nbenchmark represents the first attempt to systematically leverage MLLMs across\nall dimensions relevant to video generation assessment in generative models. By\nincorporating few-shot scoring and chain-of-query techniques, Video-Bench\nprovides a structured, scalable approach to generated video evaluation.\nExperiments on advanced models including Sora demonstrate that Video-Bench\nachieves superior alignment with human preferences across all dimensions.\nMoreover, in instances where our framework's assessments diverge from human\nevaluations, it consistently offers more objective and accurate insights,\nsuggesting an even greater potential advantage over traditional human judgment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05019", "pdf": "https://arxiv.org/pdf/2504.05019", "abs": "https://arxiv.org/abs/2504.05019", "authors": ["Ngoc Bui", "Hieu Trung Nguyen", "Shantanu Kumar", "Julian Theodore", "Weikang Qiu", "Viet Anh Nguyen", "Rex Ying"], "title": "Mixture-of-Personas Language Models for Population Simulation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Advances in Large Language Models (LLMs) paved the way for their emerging\napplications in various domains, such as human behavior simulations, where LLMs\ncould augment human-generated data in social science research and machine\nlearning model training. However, pretrained LLMs often fail to capture the\nbehavioral diversity of target populations due to the inherent variability\nacross individuals and groups. To address this, we propose \\textit{Mixture of\nPersonas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM\nresponses with the target population. MoP is a contextual mixture model, where\neach component is an LM agent characterized by a persona and an exemplar\nrepresenting subpopulation behaviors. The persona and exemplar are randomly\nchosen according to the learned mixing weights to elicit diverse LLM responses\nduring simulation. MoP is flexible, requires no model finetuning, and is\ntransferable across base models. Experiments for synthetic data generation show\nthat MoP outperforms competing methods in alignment and diversity metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05049", "pdf": "https://arxiv.org/pdf/2504.05049", "abs": "https://arxiv.org/abs/2504.05049", "authors": ["Shuai Chen", "Fanman Meng", "Haoran Wei", "Chenhao Wu", "Qingbo Wu", "Linfeng Xu", "Hongliang Li"], "title": "CMaP-SAM: Contraction Mapping Prior for SAM-driven Few-shot Segmentation", "categories": ["cs.CV"], "comment": "7 figures", "summary": "Few-shot segmentation (FSS) aims to segment new classes using few annotated\nimages. While recent FSS methods have shown considerable improvements by\nleveraging Segment Anything Model (SAM), they face two critical limitations:\ninsufficient utilization of structural correlations in query images, and\nsignificant information loss when converting continuous position priors to\ndiscrete point prompts. To address these challenges, we propose CMaP-SAM, a\nnovel framework that introduces contraction mapping theory to optimize position\npriors for SAM-driven few-shot segmentation. CMaP-SAM consists of three key\ncomponents: (1) a contraction mapping module that formulates position prior\noptimization as a Banach contraction mapping with convergence guarantees. This\nmodule iteratively refines position priors through pixel-wise structural\nsimilarity, generating a converged prior that preserves both semantic guidance\nfrom reference images and structural correlations in query images; (2) an\nadaptive distribution alignment module bridging continuous priors with SAM's\nbinary mask prompt encoder; and (3) a foreground-background decoupled\nrefinement architecture producing accurate final segmentation masks. Extensive\nexperiments demonstrate CMaP-SAM's effectiveness, achieving state-of-the-art\nperformance with 71.1 mIoU on PASCAL-$5^i$ and 56.1 on COCO-$20^i$ datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05216", "pdf": "https://arxiv.org/pdf/2504.05216", "abs": "https://arxiv.org/abs/2504.05216", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Xiaojie Sun", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "12 pages, 3 figures", "summary": "Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05170", "pdf": "https://arxiv.org/pdf/2504.05170", "abs": "https://arxiv.org/abs/2504.05170", "authors": ["Bonan Ding", "Jin Xie", "Jing Nie", "Jiale Cao"], "title": "SSLFusion: Scale & Space Aligned Latent Fusion Model for Multimodal 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2025", "summary": "Multimodal 3D object detection based on deep neural networks has indeed made\nsignificant progress. However, it still faces challenges due to the\nmisalignment of scale and spatial information between features extracted from\n2D images and those derived from 3D point clouds. Existing methods usually\naggregate multimodal features at a single stage. However, leveraging\nmulti-stage cross-modal features is crucial for detecting objects of various\nscales. Therefore, these methods often struggle to integrate features across\ndifferent scales and modalities effectively, thereby restricting the accuracy\nof detection. Additionally, the time-consuming Query-Key-Value-based\n(QKV-based) cross-attention operations often utilized in existing methods aid\nin reasoning the location and existence of objects by capturing non-local\ncontexts. However, this approach tends to increase computational complexity. To\naddress these challenges, we present SSLFusion, a novel Scale & Space Aligned\nLatent Fusion Model, consisting of a scale-aligned fusion strategy (SAF), a\n3D-to-2D space alignment module (SAM), and a latent cross-modal fusion module\n(LFM). SAF mitigates scale misalignment between modalities by aggregating\nfeatures from both images and point clouds across multiple levels. SAM is\ndesigned to reduce the inter-modal gap between features from images and point\nclouds by incorporating 3D coordinate information into 2D image features.\nAdditionally, LFM captures cross-modal non-local contexts in the latent space\nwithout utilizing the QKV-based attention operations, thus mitigating\ncomputational complexity. Experiments on the KITTI and DENSE datasets\ndemonstrate that our SSLFusion outperforms state-of-the-art methods. Our\napproach obtains an absolute gain of 2.15% in 3D AP, compared with the\nstate-of-art method GraphAlign on the moderate level of the KITTI test set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05201", "pdf": "https://arxiv.org/pdf/2504.05201", "abs": "https://arxiv.org/abs/2504.05201", "authors": ["Jared Frazier", "Tejas Sudharshan Mathai", "Jianfei Liu", "Angshuman Paul", "Ronald M. Summers"], "title": "3D Universal Lesion Detection and Tagging in CT with Self-Training", "categories": ["cs.CV", "cs.AI"], "comment": "Published at SPIE Medical Imaging 2023", "summary": "Radiologists routinely perform the tedious task of lesion localization,\nclassification, and size measurement in computed tomography (CT) studies.\nUniversal lesion detection and tagging (ULDT) can simultaneously help alleviate\nthe cumbersome nature of lesion measurement and enable tumor burden assessment.\nPrevious ULDT approaches utilize the publicly available DeepLesion dataset,\nhowever it does not provide the full volumetric (3D) extent of lesions and also\ndisplays a severe class imbalance. In this work, we propose a self-training\npipeline to detect 3D lesions and tag them according to the body part they\noccur in. We used a significantly limited 30\\% subset of DeepLesion to train a\nVFNet model for 2D lesion detection and tagging. Next, the 2D lesion context\nwas expanded into 3D, and the mined 3D lesion proposals were integrated back\ninto the baseline training data in order to retrain the model over multiple\nrounds. Through the self-training procedure, our VFNet model learned from its\nown predictions, detected lesions in 3D, and tagged them. Our results indicated\nthat our VFNet model achieved an average sensitivity of 46.9\\% at [0.125:8]\nfalse positives (FP) with a limited 30\\% data subset in comparison to the\n46.8\\% of an existing approach that used the entire DeepLesion dataset. To our\nknowledge, we are the first to jointly detect lesions in 3D and tag them\naccording to the body part label.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05227", "pdf": "https://arxiv.org/pdf/2504.05227", "abs": "https://arxiv.org/abs/2504.05227", "authors": ["Julio Silva-Rodríguez", "Jose Dolz", "Ismail Ben Ayed"], "title": "A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?", "categories": ["cs.CV"], "comment": "IPMI 2025. Code and weights: https://github.com/jusiro/DLILP", "summary": "Vision-language pre-training has recently gained popularity as it allows\nlearning rich feature representations using large-scale data sources. This\nparadigm has quickly made its way into the medical image analysis community. In\nparticular, there is an impressive amount of recent literature developing\nvision-language models for radiology. However, the available medical datasets\nwith image-text supervision are scarce, and medical concepts are fine-grained,\ninvolving expert knowledge that existing vision-language models struggle to\nencode. In this paper, we propose to take a prudent step back from the\nliterature and revisit supervised, unimodal pre-training, using fine-grained\nlabels instead. We conduct an extensive comparison demonstrating that unimodal\npre-training is highly competitive and better suited to integrating\nheterogeneous data sources. Our results also question the potential of recent\nvision-language models for open-vocabulary generalization, which have been\nevaluated using optimistic experimental settings. Finally, we study novel\nalternatives to better integrate fine-grained labels and noisy text\nsupervision.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05238", "pdf": "https://arxiv.org/pdf/2504.05238", "abs": "https://arxiv.org/abs/2504.05238", "authors": ["Zhekai Zhou", "Guibo Luo", "Mingzhi Chen", "Zhenyu Weng", "Yuesheng Zhu"], "title": "Federated Learning for Medical Image Classification: A Comprehensive Benchmark", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "The federated learning paradigm is wellsuited for the field of medical image\nanalysis, as it can effectively cope with machine learning on isolated\nmulticenter data while protecting the privacy of participating parties.\nHowever, current research on optimization algorithms in federated learning\noften focuses on limited datasets and scenarios, primarily centered around\nnatural images, with insufficient comparative experiments in medical contexts.\nIn this work, we conduct a comprehensive evaluation of several state-of-the-art\nfederated learning algorithms in the context of medical imaging. We conduct a\nfair comparison of classification models trained using various federated\nlearning algorithms across multiple medical imaging datasets. Additionally, we\nevaluate system performance metrics, such as communication cost and\ncomputational efficiency, while considering different federated learning\narchitectures. Our findings show that medical imaging datasets pose substantial\nchallenges for current federated learning optimization algorithms. No single\nalgorithm consistently delivers optimal performance across all medical\nfederated learning scenarios, and many optimization algorithms may underperform\nwhen applied to these datasets. Our experiments provide a benchmark and\nguidance for future research and application of federated learning in medical\nimaging contexts. Furthermore, we propose an efficient and robust method that\ncombines generative techniques using denoising diffusion probabilistic models\nwith label smoothing to augment datasets, widely enhancing the performance of\nfederated learning on classification tasks across various medical imaging\ndatasets. Our code will be released on GitHub, offering a reliable and\ncomprehensive benchmark for future federated learning studies in medical\nimaging.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05253", "pdf": "https://arxiv.org/pdf/2504.05253", "abs": "https://arxiv.org/abs/2504.05253", "authors": ["Ben Lonnqvist", "Elsa Scialom", "Abdulkadir Gokce", "Zehra Merchant", "Michael H. Herzog", "Martin Schrimpf"], "title": "Contour Integration Underlies Human-Like Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite the tremendous success of deep learning in computer vision, models\nstill fall behind humans in generalizing to new input distributions. Existing\nbenchmarks do not investigate the specific failure points of models by\nanalyzing performance under many controlled conditions. Our study\nsystematically dissects where and why models struggle with contour integration\n-- a hallmark of human vision -- by designing an experiment that tests object\nrecognition under various levels of object fragmentation. Humans (n=50) perform\nat high accuracy, even with few object contours present. This is in contrast to\nmodels which exhibit substantially lower sensitivity to increasing object\ncontours, with most of the over 1,000 models we tested barely performing above\nchance. Only at very large scales ($\\sim5B$ training dataset size) do models\nbegin to approach human performance. Importantly, humans exhibit an integration\nbias -- a preference towards recognizing objects made up of directional\nfragments over directionless fragments. We find that not only do models that\nshare this property perform better at our task, but that this bias also\nincreases with model training dataset size, and training models to exhibit\ncontour integration leads to high shape bias. Taken together, our results\nsuggest that contour integration is a hallmark of object vision that underlies\nobject recognition performance, and may be a mechanism learned from data at\nscale.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.05306", "pdf": "https://arxiv.org/pdf/2504.05306", "abs": "https://arxiv.org/abs/2504.05306", "authors": ["Kavana Venkatesh", "Connor Dunlop", "Pinar Yanardag"], "title": "CREA: A Collaborative Multi-Agent Framework for Creative Content Generation with Diffusion Models", "categories": ["cs.CV"], "comment": "Project URL: https://crea-diffusion.github.io", "summary": "Creativity in AI imagery remains a fundamental challenge, requiring not only\nthe generation of visually compelling content but also the capacity to add\nnovel, expressive, and artistically rich transformations to images. Unlike\nconventional editing tasks that rely on direct prompt-based modifications,\ncreative image editing demands an autonomous, iterative approach that balances\noriginality, coherence, and artistic intent. To address this, we introduce\nCREA, a novel multi-agent collaborative framework that mimics the human\ncreative process. Our framework leverages a team of specialized AI agents who\ndynamically collaborate to conceptualize, generate, critique, and enhance\nimages. Through extensive qualitative and quantitative evaluations, we\ndemonstrate that CREA significantly outperforms state-of-the-art methods in\ndiversity, semantic alignment, and creative transformation. By structuring\ncreativity as a dynamic, agentic process, CREA redefines the intersection of AI\nand art, paving the way for autonomous AI-driven artistic exploration,\ngenerative design, and human-AI co-creation. To the best of our knowledge, this\nis the first work to introduce the task of creative editing.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03736", "pdf": "https://arxiv.org/pdf/2504.03736", "abs": "https://arxiv.org/abs/2504.03736", "authors": ["Teodor Chiaburu", "Felix Bießmann", "Frank Haußer"], "title": "Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages, 10 figures, accepted at WCXAI 2025 Istanbul", "summary": "Understanding uncertainty in Explainable AI (XAI) is crucial for building\ntrust and ensuring reliable decision-making in Machine Learning models. This\npaper introduces a unified framework for quantifying and interpreting\nUncertainty in XAI by defining a general explanation function $e_{\\theta}(x,\nf)$ that captures the propagation of uncertainty from key sources:\nperturbations in input data and model parameters. By using both analytical and\nempirical estimates of explanation variance, we provide a systematic means of\nassessing the impact uncertainty on explanations. We illustrate the approach\nusing a first-order uncertainty propagation as the analytical estimator. In a\ncomprehensive evaluation across heterogeneous datasets, we compare analytical\nand empirical estimates of uncertainty propagation and evaluate their\nrobustness. Extending previous work on inconsistencies in explanations, our\nexperiments identify XAI methods that do not reliably capture and propagate\nuncertainty. Our findings underscore the importance of uncertainty-aware\nexplanations in high-stakes applications and offer new insights into the\nlimitations of current XAI methods. The code for the experiments can be found\nin our repository at https://github.com/TeodorChiaburu/UXAI", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.03782", "pdf": "https://arxiv.org/pdf/2504.03782", "abs": "https://arxiv.org/abs/2504.03782", "authors": ["Ramin Zarei Sabzevar", "Hamed Mohammadzadeh", "Tahmineh Tavakoli", "Ahad Harati"], "title": "A Study on Adversarial Robustness of Discriminative Prototypical Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deep neural networks demonstrate significant vulnerability to adversarial\nperturbations, posing risks for critical applications. Current adversarial\ntraining methods predominantly focus on robustness against attacks without\nexplicitly leveraging geometric structures in the latent space, usually\nresulting in reduced accuracy on the original clean data. To address these\nissues, we propose a novel adversarial training framework named Adversarial\nDeep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative\nprototype-based learning with adversarial training. Adv-DPNP uses unified class\nprototypes serving dual roles as classifier weights and robust anchors,\nenhancing both intra-class compactness and inter-class separation in the latent\nspace. Moreover, a novel dual-branch training mechanism maintains stable\nprototypes by updating them exclusively with clean data; while the feature\nextractor layers are learned using both clean and adversarial data to remain\ninvariant against adversarial perturbations. In addition, our approach utilizes\na composite loss function combining positive prototype alignment, negative\nprototype repulsion, and consistency regularization to further enhance\ndiscrimination, adversarial robustness, and clean accuracy. Extensive\nexperiments conducted on standard benchmark datasets confirm the effectiveness\nof Adv-DPNP compared to state-of-the-art methods, achieving higher clean\naccuracy and competitive robustness under adversarial perturbations and common\ncorruptions. Our code is available at https://github.com/fum-rpl/adv-dpnp", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04153", "pdf": "https://arxiv.org/pdf/2504.04153", "abs": "https://arxiv.org/abs/2504.04153", "authors": ["Yikai Wang", "Guangce Liu", "Xinzhou Wang", "Zilong Chen", "Jiafang Li", "Xin Liang", "Fuchun Sun", "Jun Zhu"], "title": "Video4DGen: Enhancing Video and 4D Generation through Mutual Optimization", "categories": ["cs.GR", "cs.CV"], "comment": "Published in TPAMI 2025. Code: https://github.com/yikaiw/Vidu4D,\n  Project page: https://video4dgen.github.io", "summary": "The advancement of 4D (i.e., sequential 3D) generation opens up new\npossibilities for lifelike experiences in various applications, where users can\nexplore dynamic objects or characters from any viewpoint. Meanwhile, video\ngenerative models are receiving particular attention given their ability to\nproduce realistic and imaginative frames. These models are also observed to\nexhibit strong 3D consistency, indicating the potential to act as world\nsimulators. In this work, we present Video4DGen, a novel framework that excels\nin generating 4D representations from single or multiple generated videos as\nwell as generating 4D-guided videos. This framework is pivotal for creating\nhigh-fidelity virtual contents that maintain both spatial and temporal\ncoherence. The 4D outputs generated by Video4DGen are represented using our\nproposed Dynamic Gaussian Surfels (DGS), which optimizes time-varying warping\nfunctions to transform Gaussian surfels (surface elements) from a static state\nto a dynamically warped state. We design warped-state geometric regularization\nand refinements on Gaussian surfels, to preserve the structural integrity and\nfine-grained appearance details. To perform 4D generation from multiple videos\nand capture representation across spatial, temporal, and pose dimensions, we\ndesign multi-video alignment, root pose optimization, and pose-guided frame\nsampling strategies. The leveraging of continuous warping fields also enables a\nprecise depiction of pose, motion, and deformation over per-video frames.\nFurther, to improve the overall fidelity from the observation of all camera\nposes, Video4DGen performs novel-view video generation guided by the 4D\ncontent, with the proposed confidence-filtered DGS to enhance the quality of\ngenerated sequences. With the ability of 4D and video generation, Video4DGen\noffers a powerful tool for applications in virtual reality, animation, and\nbeyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["sampling strategies"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04318", "pdf": "https://arxiv.org/pdf/2504.04318", "abs": "https://arxiv.org/abs/2504.04318", "authors": ["Mehmet Can Yavuz", "Berrin Yanikoglu"], "title": "Variational Self-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Submitted to NeurIPS 2025", "summary": "We present Variational Self-Supervised Learning (VSSL), a novel framework\nthat combines variational inference with self-supervised learning to enable\nefficient, decoder-free representation learning. Unlike traditional VAEs that\nrely on input reconstruction via a decoder, VSSL symmetrically couples two\nencoders with Gaussian outputs. A momentum-updated teacher network defines a\ndynamic, data-dependent prior, while the student encoder produces an\napproximate posterior from augmented views. The reconstruction term in the ELBO\nis replaced with a cross-view denoising objective, preserving the analytical\ntractability of Gaussian KL divergence. We further introduce cosine-based\nformulations of KL and log-likelihood terms to enhance semantic alignment in\nhigh-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and\nImageNet-100 show that VSSL achieves competitive or superior performance to\nleading self-supervised methods, including BYOL and MoCo V3. VSSL offers a\nscalable, probabilistically grounded approach to learning transferable\nrepresentations without generative reconstruction, bridging the gap between\nvariational modeling and modern self-supervised techniques.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04634", "pdf": "https://arxiv.org/pdf/2504.04634", "abs": "https://arxiv.org/abs/2504.04634", "authors": ["Foram Niravbhai Shah", "Parshwa Shah", "Muhammad Usama Saleem", "Ekkasit Pinyoanuntapong", "Pu Wang", "Hongfei Xue", "Ahmed Helmy"], "title": "DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in dance generation have enabled automatic synthesis of 3D\ndance motions. However, existing methods still struggle to produce\nhigh-fidelity dance sequences that simultaneously deliver exceptional realism,\nprecise dance-music synchronization, high motion diversity, and physical\nplausibility. Moreover, existing methods lack the flexibility to edit dance\nsequences according to diverse guidance signals, such as musical prompts, pose\nconstraints, action labels, and genre descriptions, significantly restricting\ntheir creative utility and adaptability. Unlike the existing approaches,\nDanceMosaic enables fast and high-fidelity dance generation, while allowing\nmultimodal motion editing. Specifically, we propose a multimodal masked motion\nmodel that fuses the text-to-motion model with music and pose adapters to learn\nprobabilistic mapping from diverse guidance signals to high-quality dance\nmotion sequences via progressive generative masking training. To further\nenhance the motion generation quality, we propose multimodal classifier-free\nguidance and inference-time optimization mechanism that further enforce the\nalignment between the generated motions and the multimodal guidance. Extensive\nexperiments demonstrate that our method establishes a new state-of-the-art\nperformance in dance generation, significantly advancing the quality and\neditability achieved by existing approaches.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
{"id": "2504.04704", "pdf": "https://arxiv.org/pdf/2504.04704", "abs": "https://arxiv.org/abs/2504.04704", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-08.jsonl"}
