{"id": "2504.05810", "pdf": "https://arxiv.org/pdf/2504.05810", "abs": "https://arxiv.org/abs/2504.05810", "authors": ["Xinpeng Ding", "Kui Zhang", "Jinahua Han", "Lanqing Hong", "Hang Xu", "Xiaomeng Li"], "title": "PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning", "categories": ["cs.CV"], "comment": null, "summary": "Direct Preference Optimization (DPO) helps reduce hallucinations in Video\nMultimodal Large Language Models (VLLMs), but its reliance on offline\npreference data limits adaptability and fails to capture true video-response\nmisalignment. We propose Video Direct Preference Optimization (VDPO), an online\npreference learning framework that eliminates the need for preference\nannotation by leveraging video augmentations to generate rejected samples while\nkeeping responses fixed. However, selecting effective augmentations is\nnon-trivial, as some clips may be semantically identical to the original under\nspecific prompts, leading to false rejections and disrupting alignment. To\naddress this, we introduce Prompt-aware Multi-instance Learning VDPO\n(PaMi-VDPO), which selects augmentations based on prompt context. Instead of a\nsingle rejection, we construct a candidate set of augmented clips and apply a\nclose-to-far selection strategy, initially ensuring all clips are semantically\nrelevant while then prioritizing the most prompt-aware distinct clip. This\nallows the model to better capture meaningful visual differences, mitigating\nhallucinations, while avoiding false rejections, and improving alignment.\nPaMi-VDPOseamlessly integrates into existing VLLMs without additional\nparameters, GPT-4/human supervision. With only 10k SFT data, it improves the\nbase model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining\nstable performance on general video benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05520", "pdf": "https://arxiv.org/pdf/2504.05520", "abs": "https://arxiv.org/abs/2504.05520", "authors": ["Taiwei Shi", "Yiyang Wu", "Linxin Song", "Tianyi Zhou", "Jieyu Zhao"], "title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning", "categories": ["cs.LG", "cs.CL"], "comment": "18 pages, 4 figures, 2 tables", "summary": "Reinforcement finetuning (RFT) has shown great potential for enhancing the\nmathematical reasoning capabilities of large language models (LLMs), but it is\noften sample- and compute-inefficient, requiring extensive training. In this\nwork, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a\nmethod that significantly improves both the efficiency and final accuracy of\nRFT through adaptive curriculum learning. AdaRFT dynamically adjusts the\ndifficulty of training problems based on the model's recent reward signals,\nensuring that the model consistently trains on tasks that are challenging but\nsolvable. This adaptive sampling strategy accelerates learning by maintaining\nan optimal difficulty range, avoiding wasted computation on problems that are\ntoo easy or too hard. AdaRFT requires only a lightweight extension to standard\nRFT algorithms like Proximal Policy Optimization (PPO), without modifying the\nreward function or model architecture. Experiments on competition-level math\ndatasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT\nsignificantly improves both training efficiency and reasoning performance. We\nevaluate AdaRFT across multiple data distributions and model sizes, showing\nthat it reduces the number of training steps by up to 2x and improves accuracy\nby a considerable margin, offering a more scalable and effective RFT framework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "PPO", "proximal policy optimization", "policy optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05535", "pdf": "https://arxiv.org/pdf/2504.05535", "abs": "https://arxiv.org/abs/2504.05535", "authors": ["M-A-P Team", "Siwei Wu", "Jincheng Ren", "Xinrun Du", "Shuyue Guo", "Xingwei Qu", "Yiming Liang", "Jie Liu", "Yunwen Li", "Tianyu Zheng", "Boyu Feng", "Huaqing Yuan", "Zenith Wang", "Jiaheng Liu", "Wenhao Huang", "Chenglin Cai", "Haoran Que", "Jian Yang", "Yuelin Bai", "Zekun Moore Wang", "Zhouliang Yu", "Qunshu Lin", "Ding Pan", "Yuchen Jiang", "Tiannan Wang", "Wangchunshu Zhou", "Shenzhi Wang", "Xingyuan Bu", "Minghao Liu", "Guoyin Wang", "Ge Zhang", "Chenghua Lin"], "title": "COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has achieved\nremarkable success. However, existing Chinese preference datasets are limited\nby small scale, narrow domain coverage, and lack of rigorous data validation.\nAdditionally, the reliance on human annotators for instruction and response\nlabeling significantly constrains the scalability of human preference datasets.\nTo address these challenges, we design an LLM-based Chinese preference dataset\nannotation pipeline with no human intervention. Specifically, we crawled and\ncarefully filtered 92k high-quality Chinese queries and employed 15 mainstream\nLLMs to generate and score chosen-rejected response pairs. Based on it, we\nintroduce COIG-P (Chinese Open Instruction Generalist - Preference), a\nhigh-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese\npreference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel,\nand Role. Building upon COIG-P, to reduce the overhead of using LLMs for\nscoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously\nconstructed a Chinese Reward Benchmark (CRBench). Evaluation results based on\nAlignBench \\citep{liu2024alignbenchbenchmarkingchinesealignment} show that that\nCOIG-P significantly outperforms other Chinese preference datasets, and it\nbrings significant performance improvements ranging from 2% to 12% for the\nQwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results\non CRBench demonstrate that our CRM has a strong and robust scoring ability. We\napply it to filter chosen-rejected response pairs in a test split of COIG-P,\nand our experiments show that it is comparable to GPT-4o in identifying\nlow-quality samples while maintaining efficiency and cost-effectiveness. Our\ncodes and data are released in\nhttps://github.com/multimodal-art-projection/COIG-P.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "human preference", "annotation"], "score": 6}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05831", "pdf": "https://arxiv.org/pdf/2504.05831", "abs": "https://arxiv.org/abs/2504.05831", "authors": ["Mingye Zhu", "Yi Liu", "Junbo Guo", "Quan Wang", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Robust Optimization for LLM Alignment under Distribution Shifts", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly rely on preference alignment\nmethods to steer outputs toward human values, yet these methods are often\nconstrained by the scarcity of high-quality human-annotated data. To tackle\nthis, recent approaches have turned to synthetic data generated by LLMs as a\nscalable alternative. However, synthetic data can introduce distribution\nshifts, compromising the nuanced human preferences that are essential for\ndesirable outputs. In this paper, we propose a novel distribution-aware\noptimization framework that improves preference alignment in the presence of\nsuch shifts. Our approach first estimates the likelihood ratios between the\ntarget and training distributions leveraging a learned classifier, then it\nminimizes the worst-case loss over data regions that reflect the target\nhuman-preferred distribution. By explicitly prioritizing the target\ndistribution during optimization, our method mitigates the adverse effects of\ndistributional variation and enhances the generation of responses that\nfaithfully reflect human values.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05599", "pdf": "https://arxiv.org/pdf/2504.05599", "abs": "https://arxiv.org/abs/2504.05599", "authors": ["Yi Peng", "Chris", "Xiaokun Wang", "Yichen Wei", "Jiangbo Pei", "Weijie Qiu", "Ai Jian", "Yunzhuo Hao", "Jiachun Pan", "Tianyidan Xie", "Li Ge", "Rongxian Zhuang", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Skywork R1V, a multimodal reasoning model extending the an\nR1-series Large language models (LLM) to visual modalities via an efficient\nmultimodal transfer method. Leveraging a lightweight visual projector, Skywork\nR1V facilitates seamless multimodal adaptation without necessitating retraining\nof either the foundational language model or the vision encoder. To strengthen\nvisual-text alignment, we propose a hybrid optimization strategy that combines\nIterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO), significantly enhancing cross-modal integration efficiency.\nAdditionally, we introduce an adaptive-length Chain-of-Thought distillation\napproach for reasoning data generation. This approach dynamically optimizes\nreasoning chain lengths, thereby enhancing inference efficiency and preventing\nexcessive reasoning overthinking. Empirical evaluations demonstrate that\nSkywork R1V, with only 38B parameters, delivers competitive performance,\nachieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista.\nMeanwhile, it maintains robust textual reasoning performance, evidenced by\nimpressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model\nweights have been publicly released to promote openness and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05956", "pdf": "https://arxiv.org/pdf/2504.05956", "abs": "https://arxiv.org/abs/2504.05956", "authors": ["SuBeen Lee", "WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures, 6 tables, Accepted to CVPR 2025 as Oral\n  Presentation", "summary": "Few-Shot Action Recognition (FSAR) aims to train a model with only a few\nlabeled video instances. A key challenge in FSAR is handling divergent\nnarrative trajectories for precise video matching. While the frame- and\ntuple-level alignment approaches have been promising, their methods heavily\nrely on pre-defined and length-dependent alignment units (e.g., frames or\ntuples), which limits flexibility for actions of varying lengths and speeds. In\nthis work, we introduce a novel TEmporal Alignment-free Matching (TEAM)\napproach, which eliminates the need for temporal units in action representation\nand brute-force alignment during matching. Specifically, TEAM represents each\nvideo with a fixed set of pattern tokens that capture globally discriminative\nclues within the video instance regardless of action length or speed, ensuring\nits flexibility. Furthermore, TEAM is inherently efficient, using token-wise\ncomparisons to measure similarity between videos, unlike existing methods that\nrely on pairwise comparisons for temporal alignment. Additionally, we propose\nan adaptation process that identifies and removes common information across\nclasses, establishing clear boundaries even between novel categories. Extensive\nexperiments demonstrate the effectiveness of TEAM. Codes are available at\ngithub.com/leesb7426/TEAM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05527", "pdf": "https://arxiv.org/pdf/2504.05527", "abs": "https://arxiv.org/abs/2504.05527", "authors": ["Despina Tomkou", "George Fatouros", "Andreas Andreou", "Georgios Makridis", "Fotis Liarokapis", "Dimitrios Dardanis", "Athanasios Kiourtis", "John Soldatos", "Dimosthenis Kyriazis"], "title": "Bridging Industrial Expertise and XR with LLM-Powered Conversational Agents", "categories": ["cs.CL", "cs.AI", "68T50, 68T40, 68U20, 68U35", "H.5.1; I.2.7; I.2.11; H.3.3; H.5.2; C.3"], "comment": "7 pages, 7 figures", "summary": "This paper introduces a novel integration of Retrieval-Augmented Generation\n(RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR)\ntechnologies to address knowledge transfer challenges in industrial\nenvironments. The proposed system embeds domain-specific industrial knowledge\ninto XR environments through a natural language interface, enabling hands-free,\ncontext-aware expert guidance for workers. We present the architecture of the\nproposed system consisting of an LLM Chat Engine with dynamic tool\norchestration and an XR application featuring voice-driven interaction.\nPerformance evaluation of various chunking strategies, embedding models, and\nvector databases reveals that semantic chunking, balanced embedding models, and\nefficient vector stores deliver optimal performance for industrial knowledge\nretrieval. The system's potential is demonstrated through early implementation\nin multiple industrial use cases, including robotic assembly, smart\ninfrastructure maintenance, and aerospace component servicing. Results indicate\npotential for enhancing training efficiency, remote assistance capabilities,\nand operational guidance in alignment with Industry 5.0's human-centric and\nresilient approach to industrial development.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05463", "pdf": "https://arxiv.org/pdf/2504.05463", "abs": "https://arxiv.org/abs/2504.05463", "authors": ["Sofian Chaybouti", "Walid Bousselham", "Moritz Wolter", "Hilde Kuehne"], "title": "REVEAL: Relation-based Video Representation Learning for Video-Question-Answering", "categories": ["cs.CV"], "comment": "18 pages, 7 figures", "summary": "Video-Question-Answering (VideoQA) comprises the capturing of complex visual\nrelation changes over time, remaining a challenge even for advanced Video\nLanguage Models (VLM), i.a., because of the need to represent the visual\ncontent to a reasonably sized input for those models. To address this problem,\nwe propose\n  RElation-based Video rEpresentAtion Learning (REVEAL), a framework designed\nto capture visual relation information by encoding them into structured,\ndecomposed representations. Specifically, inspired by spatiotemporal scene\ngraphs, we propose to encode video sequences as sets of relation triplets in\nthe form of (\\textit{subject-predicate-object}) over time via their language\nembeddings. To this end, we extract explicit relations from video captions and\nintroduce a Many-to-Many Noise Contrastive Estimation (MM-NCE) together with a\nQ-Former architecture to align an unordered set of video-derived queries with\ncorresponding text-based relation descriptions. At inference, the resulting\nQ-former produces an efficient token representation that can serve as input to\na VLM for VideoQA.\n  We evaluate the proposed framework on five challenging benchmarks: NeXT-QA,\nIntent-QA, STAR, VLEP, and TVQA. It shows that the resulting query-based video\nrepresentation is able to outperform global alignment-based CLS or patch token\nrepresentations and achieves competitive results against state-of-the-art\nmodels, particularly on tasks requiring temporal reasoning and relation\ncomprehension. The code and models will be publicly released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05508", "pdf": "https://arxiv.org/pdf/2504.05508", "abs": "https://arxiv.org/abs/2504.05508", "authors": ["Mo Zhou", "Josh Myers-Dean", "Danna Gurari"], "title": "PartStickers: Generating Parts of Objects for Rapid Prototyping", "categories": ["cs.CV"], "comment": "Accepted to CVPR CVEU workshop 2025", "summary": "Design prototyping involves creating mockups of products or concepts to\ngather feedback and iterate on ideas. While prototyping often requires specific\nparts of objects, such as when constructing a novel creature for a video game,\nexisting text-to-image methods tend to only generate entire objects. To address\nthis, we propose a novel task and method of ``part sticker generation\", which\nentails generating an isolated part of an object on a neutral background.\nExperiments demonstrate our method outperforms state-of-the-art baselines with\nrespect to realism and text alignment, while preserving object-level generation\ncapabilities. We publicly share our code and models to encourage community-wide\nprogress on this new task: https://partsticker.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05683", "pdf": "https://arxiv.org/pdf/2504.05683", "abs": "https://arxiv.org/abs/2504.05683", "authors": ["Subhankar Maity", "Aniket Deroy", "Sudeshna Sarkar"], "title": "Towards Smarter Hiring: Are Zero-Shot and Few-Shot Pre-trained LLMs Ready for HR Spoken Interview Transcript Analysis?", "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 24 figures", "summary": "This research paper presents a comprehensive analysis of the performance of\nprominent pre-trained large language models (LLMs), including GPT-4 Turbo,\nGPT-3.5 Turbo, text-davinci-003, text-babbage-001, text-curie-001,\ntext-ada-001, llama-2-7b-chat, llama-2-13b-chat, and llama-2-70b-chat, in\ncomparison to expert human evaluators in providing scores, identifying errors,\nand offering feedback and improvement suggestions to candidates during mock HR\n(Human Resources) interviews. We introduce a dataset called HURIT (Human\nResource Interview Transcripts), which comprises 3,890 HR interview transcripts\nsourced from real-world HR interview scenarios. Our findings reveal that\npre-trained LLMs, particularly GPT-4 Turbo and GPT-3.5 Turbo, exhibit\ncommendable performance and are capable of producing evaluations comparable to\nthose of expert human evaluators. Although these LLMs demonstrate proficiency\nin providing scores comparable to human experts in terms of human evaluation\nmetrics, they frequently fail to identify errors and offer specific actionable\nadvice for candidate performance improvement in HR interviews. Our research\nsuggests that the current state-of-the-art pre-trained LLMs are not fully\nconducive for automatic deployment in an HR interview assessment. Instead, our\nfindings advocate for a human-in-the-loop approach, to incorporate manual\nchecks for inconsistencies and provisions for improving feedback quality as a\nmore suitable strategy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05736", "pdf": "https://arxiv.org/pdf/2504.05736", "abs": "https://arxiv.org/abs/2504.05736", "authors": ["Yida Cai", "Kun Liang", "Sanwoo Lee", "Qinghan Wang", "Yunfang Wu"], "title": "Rank-Then-Score: Enhancing Large Language Models for Automated Essay Scoring", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "In recent years, large language models (LLMs) achieve remarkable success\nacross a variety of tasks. However, their potential in the domain of Automated\nEssay Scoring (AES) remains largely underexplored. Moreover, compared to\nEnglish data, the methods for Chinese AES is not well developed. In this paper,\nwe propose Rank-Then-Score (RTS), a fine-tuning framework based on large\nlanguage models to enhance their essay scoring capabilities. Specifically, we\nfine-tune the ranking model (Ranker) with feature-enriched data, and then feed\nthe output of the ranking model, in the form of a candidate score set, with the\nessay content into the scoring model (Scorer) to produce the final score.\nExperimental results on two benchmark datasets, HSK and ASAP, demonstrate that\nRTS consistently outperforms the direct prompting (Vanilla) method in terms of\naverage QWK across all LLMs and datasets, and achieves the best performance on\nChinese essay scoring using the HSK dataset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05747", "pdf": "https://arxiv.org/pdf/2504.05747", "abs": "https://arxiv.org/abs/2504.05747", "authors": ["Raymond Ng", "Thanh Ngan Nguyen", "Yuli Huang", "Ngee Chia Tai", "Wai Yi Leong", "Wei Qi Leong", "Xianbin Yong", "Jian Gang Ngui", "Yosephine Susanto", "Nicholas Cheng", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Adithya Venkatadri Hulagadri", "Kok Wai Teng", "Yeo Yeow Tong", "Bryan Siow", "Wei Yi Teo", "Wayne Lau", "Choon Meng Tan", "Brandon Ong", "Zhi Hao Ong", "Jann Railey Montalan", "Adwin Chan", "Sajeban Antonyrex", "Ren Lee", "Esther Choa", "David Ong Tat-Wee", "Bing Jie Darius Liu", "William Chandra Tjhi", "Erik Cambria", "Leslie Teo"], "title": "SEA-LION: Southeast Asian Languages in One Network", "categories": ["cs.CL"], "comment": "We released our model at\n  https://huggingface.co/collections/aisingapore/sea-lionv3-672589a39cdadd6a5b199581", "summary": "Recently, Large Language Models (LLMs) have dominated much of the artificial\nintelligence scene with their ability to process and generate natural\nlanguages. However, the majority of LLM research and development remains\nEnglish-centric, leaving low-resource languages such as those in the Southeast\nAsian (SEA) region under-represented. To address this representation gap, we\nintroduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge\nmultilingual LLMs designed for SEA languages. The SEA-LION family of LLMs\nsupports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese,\nMalay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages\nlarge-scale multilingual continued pre-training with a comprehensive\npost-training regime involving multiple stages of instruction fine-tuning,\nalignment, and model merging. Evaluation results on multilingual benchmarks\nindicate that our models achieve state-of-the-art performance across LLMs\nsupporting SEA languages. We open-source the models to benefit the wider SEA\ncommunity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05594", "pdf": "https://arxiv.org/pdf/2504.05594", "abs": "https://arxiv.org/abs/2504.05594", "authors": ["Qi Mao", "Lan Chen", "Yuchao Gu", "Mike Zheng Shou", "Ming-Hsuan Yang"], "title": "Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model", "categories": ["cs.CV"], "comment": "under review", "summary": "Balancing fidelity and editability is essential in text-based image editing\n(TIE), where failures commonly lead to over- or under-editing issues. Existing\nmethods typically rely on attention injections for structure preservation and\nleverage the inherent text alignment capabilities of pre-trained text-to-image\n(T2I) models for editability, but they lack explicit and unified mechanisms to\nproperly balance these two objectives. In this work, we introduce UnifyEdit, a\ntuning-free method that performs diffusion latent optimization to enable a\nbalanced integration of fidelity and editability within a unified framework.\nUnlike direct attention injections, we develop two attention-based constraints:\na self-attention (SA) preservation constraint for structural fidelity, and a\ncross-attention (CA) alignment constraint to enhance text alignment for\nimproved editability. However, simultaneously applying both constraints can\nlead to gradient conflicts, where the dominance of one constraint results in\nover- or under-editing. To address this challenge, we introduce an adaptive\ntime-step scheduler that dynamically adjusts the influence of these\nconstraints, guiding the diffusion latent toward an optimal balance. Extensive\nquantitative and qualitative experiments validate the effectiveness of our\napproach, demonstrating its superiority in achieving a robust balance between\nstructure preservation and text alignment across various editing tasks,\noutperforming other state-of-the-art methods. The source code will be available\nat https://github.com/CUC-MIPG/UnifyEdit.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05644", "pdf": "https://arxiv.org/pdf/2504.05644", "abs": "https://arxiv.org/abs/2504.05644", "authors": ["Yan Zhang", "Zhong Ji", "Changxu Meng", "Yanwei Pang", "Jungong Han"], "title": "iEBAKER: Improved Remote Sensing Image-Text Retrieval Framework via Eliminate Before Align and Keyword Explicit Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies focus on the Remote Sensing Image-Text Retrieval (RSITR),\nwhich aims at searching for the corresponding targets based on the given query.\nAmong these efforts, the application of Foundation Models (FMs), such as CLIP,\nto the domain of remote sensing has yielded encouraging outcomes. However,\nexisting FM based methodologies neglect the negative impact of weakly\ncorrelated sample pairs and fail to account for the key distinctions among\nremote sensing texts, leading to biased and superficial exploration of sample\npairs. To address these challenges, we propose an approach named iEBAKER (an\nImproved Eliminate Before Align strategy with Keyword Explicit Reasoning\nframework) for RSITR. Specifically, we propose an innovative Eliminate Before\nAlign (EBA) strategy to filter out the weakly correlated sample pairs, thereby\nmitigating their deviations from optimal embedding space during\nalignment.Further, two specific schemes are introduced from the perspective of\nwhether local similarity and global similarity affect each other. On this\nbasis, we introduce an alternative Sort After Reversed Retrieval (SAR)\nstrategy, aims at optimizing the similarity matrix via reverse retrieval.\nAdditionally, we incorporate a Keyword Explicit Reasoning (KER) module to\nfacilitate the beneficial impact of subtle key concept distinctions. Without\nbells and whistles, our approach enables a direct transition from FM to RSITR\ntask, eliminating the need for additional pretraining on remote sensing data.\nExtensive experiments conducted on three popular benchmark datasets demonstrate\nthat our proposed iEBAKER method surpasses the state-of-the-art models while\nrequiring less training data. Our source code will be released at\nhttps://github.com/zhangy0822/iEBAKER.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06011", "pdf": "https://arxiv.org/pdf/2504.06011", "abs": "https://arxiv.org/abs/2504.06011", "authors": ["Monojit Choudhury", "Shivam Chauhan", "Rocktim Jyoti Das", "Dhruv Sahnan", "Xudong Han", "Haonan Li", "Aaryamonvikram Singh", "Alok Anil Jadhav", "Utkarsh Agarwal", "Mukund Choudhary", "Debopriyo Banerjee", "Fajri Koto", "Junaid Bhat", "Awantika Shukla", "Samujjwal Ghosh", "Samta Kamboj", "Onkar Pandit", "Lalit Pradhan", "Rahul Pal", "Sunil Sahu", "Soundar Doraiswamy", "Parvez Mullah", "Ali El Filali", "Neha Sengupta", "Gokul Ramakrishnan", "Rituraj Joshi", "Gurpreet Gosal", "Avraham Sheinin", "Natalia Vassilieva", "Preslav Nakov"], "title": "Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi", "categories": ["cs.CL"], "comment": null, "summary": "Developing high-quality large language models (LLMs) for moderately resourced\nlanguages presents unique challenges in data availability, model adaptation,\nand evaluation. We introduce Llama-3-Nanda-10B-Chat, or Nanda for short, a\nstate-of-the-art Hindi-centric instruction-tuned generative LLM, designed to\npush the boundaries of open-source Hindi language models. Built upon\nLlama-3-8B, Nanda incorporates continuous pre-training with expanded\ntransformer blocks, leveraging the Llama Pro methodology. A key challenge was\nthe limited availability of high-quality Hindi text data; we addressed this\nthrough rigorous data curation, augmentation, and strategic bilingual training,\nbalancing Hindi and English corpora to optimize cross-linguistic knowledge\ntransfer. With 10 billion parameters, Nanda stands among the top-performing\nopen-source Hindi and multilingual models of similar scale, demonstrating\nsignificant advantages over many existing models. We provide an in-depth\ndiscussion of training strategies, fine-tuning techniques, safety alignment,\nand evaluation metrics, demonstrating how these approaches enabled Nanda to\nachieve state-of-the-art results. By open-sourcing Nanda, we aim to advance\nresearch in Hindi LLMs and support a wide range of real-world applications\nacross academia, industry, and public services.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06136", "pdf": "https://arxiv.org/pdf/2504.06136", "abs": "https://arxiv.org/abs/2504.06136", "authors": ["Movina Moses", "Mohab Elkaref", "James Barry", "Shinnosuke Tanaka", "Vishnudev Kuruvanthodi", "Nathan Herr", "Campbell D Watson", "Geeth De Mel"], "title": "QGen Studio: An Adaptive Question-Answer Generation, Training and Evaluation Platform", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present QGen Studio: an adaptive question-answer generation, training, and\nevaluation platform. QGen Studio enables users to leverage large language\nmodels (LLMs) to create custom question-answer datasets and fine-tune models on\nthis synthetic data. It features a dataset viewer and model explorer to\nstreamline this process. The dataset viewer provides key metrics and visualizes\nthe context from which the QA pairs are generated, offering insights into data\nquality. The model explorer supports model comparison, allowing users to\ncontrast the performance of their trained LLMs against other models, supporting\nperformance benchmarking and refinement. QGen Studio delivers an interactive,\nend-to-end solution for generating QA datasets and training scalable,\ndomain-adaptable models. The studio will be open-sourced soon, allowing users\nto deploy it locally.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05746", "pdf": "https://arxiv.org/pdf/2504.05746", "abs": "https://arxiv.org/abs/2504.05746", "authors": ["Zhihua Xu", "Tianshui Chen", "Zhijing Yang", "Siyuan Peng", "Keze Wang", "Liang Lin"], "title": "Exploiting Temporal Audio-Visual Correlation Embedding for Audio-Driven One-Shot Talking Head Animation", "categories": ["cs.CV"], "comment": "Accepted at TMM 2025", "summary": "The paramount challenge in audio-driven One-shot Talking Head Animation\n(ADOS-THA) lies in capturing subtle imperceptible changes between adjacent\nvideo frames. Inherently, the temporal relationship of adjacent audio clips is\nhighly correlated with that of the corresponding adjacent video frames,\noffering supplementary information that can be pivotal for guiding and\nsupervising talking head animations. In this work, we propose to learn\naudio-visual correlations and integrate the correlations to help enhance\nfeature representation and regularize final generation by a novel Temporal\nAudio-Visual Correlation Embedding (TAVCE) framework. Specifically, it first\nlearns an audio-visual temporal correlation metric, ensuring the temporal audio\nrelationships of adjacent clips are aligned with the temporal visual\nrelationships of corresponding adjacent video frames. Since the temporal audio\nrelationship contains aligned information about the visual frame, we first\nintegrate it to guide learning more representative features via a simple yet\neffective channel attention mechanism. During training, we also use the\nalignment correlations as an additional objective to supervise generating\nvisual frames. We conduct extensive experiments on several publicly available\nbenchmarks (i.e., HDTF, LRW, VoxCeleb1, and VoxCeleb2) to demonstrate its\nsuperiority over existing leading algorithms.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05779", "pdf": "https://arxiv.org/pdf/2504.05779", "abs": "https://arxiv.org/abs/2504.05779", "authors": ["Tao Lin", "Qingwang Wang", "Qiwei Liang", "Minghua Tang", "Yuxuan Sun"], "title": "FASR-Net: Unsupervised Shadow Removal Leveraging Inherent Frequency Priors", "categories": ["cs.CV"], "comment": null, "summary": "Shadow removal is challenging due to the complex interaction of geometry,\nlighting, and environmental factors. Existing unsupervised methods often\noverlook shadow-specific priors, leading to incomplete shadow recovery. To\naddress this issue, we propose a novel unsupervised Frequency Aware Shadow\nRemoval Network (FASR-Net), which leverages the inherent frequency\ncharacteristics of shadow regions. Specifically, the proposed Wavelet Attention\nDownsampling Module (WADM) integrates wavelet-based image decomposition and\ndeformable attention, effectively breaking down the image into frequency\ncomponents to enhance shadow details within specific frequency bands. We also\nintroduce several new loss functions for precise shadow-free image\nreproduction: a frequency loss to capture image component details, a\nbrightness-chromaticity loss that references the chromaticity of shadow-free\nregions, and an alignment loss to ensure smooth transitions between shadowed\nand shadow-free regions. Experimental results on the AISTD and SRD datasets\ndemonstrate that our method achieves superior shadow removal performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05789", "pdf": "https://arxiv.org/pdf/2504.05789", "abs": "https://arxiv.org/abs/2504.05789", "authors": ["Sarosij Bose", "Hannah Dela Cruz", "Arindam Dutta", "Elena Kokkoni", "Konstantinos Karydis", "Amit K. Roy-Chowdhury"], "title": "Leveraging Synthetic Adult Datasets for Unsupervised Infant Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ABAW@CVPR 2025", "summary": "Human pose estimation is a critical tool across a variety of healthcare\napplications. Despite significant progress in pose estimation algorithms\ntargeting adults, such developments for infants remain limited. Existing\nalgorithms for infant pose estimation, despite achieving commendable\nperformance, depend on fully supervised approaches that require large amounts\nof labeled data. These algorithms also struggle with poor generalizability\nunder distribution shifts. To address these challenges, we introduce SHIFT:\nLeveraging SyntHetic Adult Datasets for Unsupervised InFanT Pose Estimation,\nwhich leverages the pseudo-labeling-based Mean-Teacher framework to compensate\nfor the lack of labeled data and addresses distribution shifts by enforcing\nconsistency between the student and the teacher pseudo-labels. Additionally, to\npenalize implausible predictions obtained from the mean-teacher framework, we\nincorporate an infant manifold pose prior. To enhance SHIFT's self-occlusion\nperception ability, we propose a novel visibility consistency module for\nimproved alignment of the predicted poses with the original image. Extensive\nexperiments on multiple benchmarks show that SHIFT significantly outperforms\nexisting state-of-the-art unsupervised domain adaptation (UDA) pose estimation\nmethods by 5% and supervised infant pose estimation methods by a margin of 16%.\nThe project page is available at: https://sarosijbose.github.io/SHIFT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05795", "pdf": "https://arxiv.org/pdf/2504.05795", "abs": "https://arxiv.org/abs/2504.05795", "authors": ["Hao Zhang", "Yanping Zha", "Qingwei Zhuang", "Zhenfeng Shao", "Jiayi Ma"], "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions", "categories": ["cs.CV"], "comment": null, "summary": "Current image fusion methods struggle to adapt to real-world environments\nencompassing diverse degradations with spatially varying characteristics. To\naddress this challenge, we propose a robust fusion controller (RFC) capable of\nachieving degradation-aware image fusion through fine-grained language\ninstructions, ensuring its reliable application in adverse environments.\nSpecifically, RFC first parses language instructions to innovatively derive the\nfunctional condition and the spatial condition, where the former specifies the\ndegradation type to remove, while the latter defines its spatial coverage.\nThen, a composite control priori is generated through a multi-condition\ncoupling network, achieving a seamless transition from abstract language\ninstructions to latent control variables. Subsequently, we design a hybrid\nattention-based fusion network to aggregate multi-modal information, in which\nthe obtained composite control priori is deeply embedded to linearly modulate\nthe intermediate fused features. To ensure the alignment between language\ninstructions and control outcomes, we introduce a novel language-feature\nalignment loss, which constrains the consistency between feature-level gains\nand the composite control priori. Extensive experiments on publicly available\ndatasets demonstrate that our RFC is robust against various composite\ndegradations, particularly in highly challenging flare scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05605", "pdf": "https://arxiv.org/pdf/2504.05605", "abs": "https://arxiv.org/abs/2504.05605", "authors": ["Gejian Zhao", "Hanzhou Wu", "Xinpeng Zhang", "Athanasios V. Vasilakos"], "title": "ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs", "categories": ["cs.CR", "cs.CL"], "comment": "Zhao et al., 16 pages, 2025, uploaded by Hanzhou Wu, Shanghai\n  University", "summary": "Chain-of-Thought (CoT) enhances an LLM's ability to perform complex reasoning\ntasks, but it also introduces new security issues. In this work, we present\nShadowCoT, a novel backdoor attack framework that targets the internal\nreasoning mechanism of LLMs. Unlike prior token-level or prompt-based attacks,\nShadowCoT directly manipulates the model's cognitive reasoning path, enabling\nit to hijack multi-step reasoning chains and produce logically coherent but\nadversarial outcomes. By conditioning on internal reasoning states, ShadowCoT\nlearns to recognize and selectively disrupt key reasoning steps, effectively\nmounting a self-reflective cognitive attack within the target model. Our\napproach introduces a lightweight yet effective multi-stage injection pipeline,\nwhich selectively rewires attention pathways and perturbs intermediate\nrepresentations with minimal parameter overhead (only 0.15% updated). ShadowCoT\nfurther leverages reinforcement learning and reasoning chain pollution (RCP) to\nautonomously synthesize stealthy adversarial CoTs that remain undetectable to\nadvanced defenses. Extensive experiments across diverse reasoning benchmarks\nand LLMs show that ShadowCoT consistently achieves high Attack Success Rate\n(94.4%) and Hijacking Success Rate (88.4%) while preserving benign performance.\nThese results reveal an emergent class of cognition-level threats and highlight\nthe urgent need for defenses beyond shallow surface-level consistency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05731", "pdf": "https://arxiv.org/pdf/2504.05731", "abs": "https://arxiv.org/abs/2504.05731", "authors": ["Teng Shi", "Jun Xu", "Xiao Zhang", "Xiaoxue Zang", "Kai Zheng", "Yang Song", "Han Li"], "title": "Retrieval Augmented Generation with Collaborative Filtering for Personalized Text Generation", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Recently, the personalization of Large Language Models (LLMs) to generate\ncontent that aligns with individual user preferences has garnered widespread\nattention. Personalized Retrieval-Augmented Generation (RAG), which retrieves\nrelevant documents from the user's history to reflect their preferences and\nenhance LLM generation, is one commonly used approach for personalization.\nHowever, existing personalized RAG methods do not consider that the histories\nof similar users can also assist in personalized generation for the current\nuser, meaning that collaborative information between users can also benefit\npersonalized generation. Inspired by the application of collaborative filtering\nin recommender systems, we propose a method called CFRAG, which adapts\nCollaborative Filtering to RAG for personalized text generation. However, this\npresents two challenges: (1)~how to incorporate collaborative information\nwithout explicit user similarity labels? (2)~how to retrieve documents that\nsupport personalized LLM generation? For Challenge 1, we use contrastive\nlearning to train user embeddings to retrieve similar users and introduce\ncollaborative information. For Challenge 2, we design a personalized retriever\nand reranker to retrieve the top-$k$ documents from these users' histories. We\ntake into account the user's preference during retrieval and reranking. Then we\nleverage feedback from the LLM to fine-tune the personalized retriever and\nreranker, enabling them to retrieve documents that meet the personalized\ngeneration needs of the LLM. Experimental results on the Language Model\nPersonalization (LaMP) benchmark validate the effectiveness of CFRAG. Further\nanalysis confirms the importance of incorporating collaborative information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05862", "pdf": "https://arxiv.org/pdf/2504.05862", "abs": "https://arxiv.org/abs/2504.05862", "authors": ["Takehiro Takayanagi", "Kiyoshi Izumi", "Javier Sanz-Cruzado", "Richard McCreadie", "Iadh Ounis"], "title": "Are Generative AI Agents Effective Personalized Financial Advisors?", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "q-fin.CP"], "comment": null, "summary": "Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06196", "pdf": "https://arxiv.org/pdf/2504.06196", "abs": "https://arxiv.org/abs/2504.06196", "authors": ["Eric Wang", "Samuel Schmidgall", "Paul F. Jaeger", "Fan Zhang", "Rory Pilgrim", "Yossi Matias", "Joelle Barral", "David Fleet", "Shekoofeh Azizi"], "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Therapeutic development is a costly and high-risk endeavor that is often\nplagued by high failure rates. To address this, we introduce TxGemma, a suite\nof efficient, generalist large language models (LLMs) capable of therapeutic\nproperty prediction as well as interactive reasoning and explainability. Unlike\ntask-specific models, TxGemma synthesizes information from diverse sources,\nenabling broad application across the therapeutic development pipeline. The\nsuite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a\ncomprehensive dataset of small molecules, proteins, nucleic acids, diseases,\nand cell lines. Across 66 therapeutic development tasks, TxGemma achieved\nsuperior or comparable performance to the state-of-the-art generalist model on\n64 (superior on 45), and against state-of-the-art specialist models on 50\n(superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks,\nsuch as clinical trial adverse event prediction, requires less training data\nthan fine-tuning base LLMs, making TxGemma suitable for data-limited\napplications. Beyond these predictive capabilities, TxGemma features\nconversational models that bridge the gap between general LLMs and specialized\nproperty predictors. These allow scientists to interact in natural language,\nprovide mechanistic reasoning for predictions based on molecular structure, and\nengage in scientific discussions. Building on this, we further introduce\nAgentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that\nreasons, acts, manages diverse workflows, and acquires external domain\nknowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last\nExam benchmark (Chemistry & Biology) with 52.3% relative improvement over\no3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels\nwith improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over\no3-mini (high).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05925", "pdf": "https://arxiv.org/pdf/2504.05925", "abs": "https://arxiv.org/abs/2504.05925", "authors": ["Hao Du", "Bo Wu", "Yan Lu", "Zhendong Mao"], "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation", "categories": ["cs.CV"], "comment": "CVPR 2025. The first two authors contributed equally", "summary": "Vision-language temporal alignment is a crucial capability for human dynamic\nrecognition and cognition in real-world scenarios. While existing research\nfocuses on capturing vision-language relevance, it faces limitations due to\nbiased temporal distributions, imprecise annotations, and insufficient\ncompositionally. To achieve fair evaluation and comprehensive exploration, our\nobjective is to investigate and evaluate the ability of models to achieve\nalignment from a temporal perspective, specifically focusing on their capacity\nto synchronize visual scenarios with linguistic context in a temporally\ncoherent manner. As a preliminary step, we present the statistical analysis of\nexisting benchmarks and reveal the existing challenges from a decomposed\nperspective. To this end, we introduce SVLTA, the Synthetic Vision-Language\nTemporal Alignment derived via a well-designed and feasible control generation\nmethod within a simulation environment. The approach considers commonsense\nknowledge, manipulable action, and constrained filtering, which generates\nreasonable, diverse, and balanced data distributions for diagnostic\nevaluations. Our experiments reveal diagnostic insights through the evaluations\nin temporal question answering, distributional shift sensitiveness, and\ntemporal alignment adaptation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06116", "pdf": "https://arxiv.org/pdf/2504.06116", "abs": "https://arxiv.org/abs/2504.06116", "authors": ["Davide Sferrazza", "Gabriele Berton", "Gabriele Trivigno", "Carlo Masone"], "title": "To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition", "categories": ["cs.CV"], "comment": "CVPRW 2025", "summary": "Visual Place Recognition (VPR) is a critical task in computer vision,\ntraditionally enhanced by re-ranking retrieval results with image matching.\nHowever, recent advancements in VPR methods have significantly improved\nperformance, challenging the necessity of re-ranking. In this work, we show\nthat modern retrieval systems often reach a point where re-ranking can degrade\nresults, as current VPR datasets are largely saturated. We propose using image\nmatching as a verification step to assess retrieval confidence, demonstrating\nthat inlier counts can reliably predict when re-ranking is beneficial. Our\nfindings shift the paradigm of retrieval pipelines, offering insights for more\nrobust and adaptive VPR systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06144", "pdf": "https://arxiv.org/pdf/2504.06144", "abs": "https://arxiv.org/abs/2504.06144", "authors": ["Jihun Park", "Jongmin Gim", "Kyoungmin Lee", "Minseok Oh", "Minwoo Choi", "Jaeyeul Kim", "Woo Chool Park", "Sunghoon Im"], "title": "A Training-Free Style-aligned Image Generation with Scale-wise Autoregressive Model", "categories": ["cs.CV"], "comment": "17 pages, 15 figures", "summary": "We present a training-free style-aligned image generation method that\nleverages a scale-wise autoregressive model. While large-scale text-to-image\n(T2I) models, particularly diffusion-based methods, have demonstrated\nimpressive generation quality, they often suffer from style misalignment across\ngenerated image sets and slow inference speeds, limiting their practical\nusability. To address these issues, we propose three key components: initial\nfeature replacement to ensure consistent background appearance, pivotal feature\ninterpolation to align object placement, and dynamic style injection, which\nreinforces style consistency using a schedule function. Unlike previous methods\nrequiring fine-tuning or additional training, our approach maintains fast\ninference while preserving individual content details. Extensive experiments\nshow that our method achieves generation quality comparable to competing\napproaches, significantly improves style alignment, and delivers inference\nspeeds over six times faster than the fastest model.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06185", "pdf": "https://arxiv.org/pdf/2504.06185", "abs": "https://arxiv.org/abs/2504.06185", "authors": ["Vanessa Borst", "Timo Dittus", "Tassilo Dege", "Astrid Schmieder", "Samuel Kounev"], "title": "WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care", "categories": ["cs.CV", "cs.AI"], "comment": "Main paper: 17 pages; supplementary material: 16 pages; paper\n  submitted to the application track of the European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2025)", "summary": "Chronic wounds affect a large population, particularly the elderly and\ndiabetic patients, who often exhibit limited mobility and co-existing health\nconditions. Automated wound monitoring via mobile image capture can reduce\nin-person physician visits by enabling remote tracking of wound size. Semantic\nsegmentation is key to this process, yet wound segmentation remains\nunderrepresented in medical imaging research. To address this, we benchmark\nstate-of-the-art deep learning models from general-purpose vision, medical\nimaging, and top methods from public wound challenges. For fair comparison, we\nstandardize training, data augmentation, and evaluation, conducting\ncross-validationto minimize partitioning bias. We also assess real-world\ndeployment aspects, including generalization to an out-of-distribution wound\ndataset, computational efficiency, and interpretability. Additionally, we\npropose a reference object-based approach to convert AI-generated masks into\nclinically relevant wound size estimates, and evaluate this, along with mask\nquality, for the best models based on physician assessments. Overall, the\ntransformer-based TransNeXt showed the highest levels of generalizability.\nDespite variations in inference times, all models processed at least one image\nper second on the CPU, which is deemed adequate for the intended application.\nInterpretability analysis typically revealed prominent activations in wound\nregions, emphasizing focus on clinically relevant features. Expert evaluation\nshowed high mask approval for all analyzed models, with VWFormer and ConvNeXtS\nbackbone performing the best. Size retrieval accuracy was similar across\nmodels, and predictions closely matched expert annotations. Finally, we\ndemonstrate how our AI-driven wound size estimation framework, WoundAmbit, can\nbe integrated into a custom telehealth system. Our code will be made available\non GitHub upon publication.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06232", "pdf": "https://arxiv.org/pdf/2504.06232", "abs": "https://arxiv.org/abs/2504.06232", "authors": ["Jiazi Bu", "Pengyang Ling", "Yujie Zhou", "Pan Zhang", "Tong Wu", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention\nrecently due to their remarkable ability to deliver flexible visual creations.\nStill, high-resolution image synthesis presents formidable challenges due to\nthe scarcity and complexity of high-resolution content. To this end, we present\nHiFlow, a training-free and model-agnostic framework to unlock the resolution\npotential of pre-trained flow models. Specifically, HiFlow establishes a\nvirtual reference flow within the high-resolution space that effectively\ncaptures the characteristics of low-resolution flow information, offering\nguidance for high-resolution generation through three key aspects:\ninitialization alignment for low-frequency consistency, direction alignment for\nstructure preservation, and acceleration alignment for detail fidelity. By\nleveraging this flow-aligned guidance, HiFlow substantially elevates the\nquality of high-resolution image synthesis of T2I models and demonstrates\nversatility across their personalized variants. Extensive experiments validate\nHiFlow's superiority in achieving superior high-resolution image quality over\ncurrent state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.06264", "pdf": "https://arxiv.org/pdf/2504.06264", "abs": "https://arxiv.org/abs/2504.06264", "authors": ["Jisang Han", "Honggyu An", "Jaewoo Jung", "Takuya Narihira", "Junyoung Seo", "Kazumi Fukuda", "Chaehyun Kim", "Sunghwan Hong", "Yuki Mitsufuji", "Seungryong Kim"], "title": "D^2USt3R: Enhancing 3D Reconstruction with 4D Pointmaps for Dynamic Scenes", "categories": ["cs.CV"], "comment": "project page: https://cvlab-kaist.github.io/DDUSt3R/", "summary": "We address the task of 3D reconstruction in dynamic scenes, where object\nmotions degrade the quality of previous 3D pointmap regression methods, such as\nDUSt3R, originally designed for static 3D scene reconstruction. Although these\nmethods provide an elegant and powerful solution in static settings, they\nstruggle in the presence of dynamic motions that disrupt alignment based solely\non camera poses. To overcome this, we propose D^2USt3R that regresses 4D\npointmaps that simultaneiously capture both static and dynamic 3D scene\ngeometry in a feed-forward manner. By explicitly incorporating both spatial and\ntemporal aspects, our approach successfully encapsulates spatio-temporal dense\ncorrespondence to the proposed 4D pointmaps, enhancing downstream tasks.\nExtensive experimental evaluations demonstrate that our proposed approach\nconsistently achieves superior reconstruction performance across various\ndatasets featuring complex motions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05316", "pdf": "https://arxiv.org/pdf/2504.05316", "abs": "https://arxiv.org/abs/2504.05316", "authors": ["Yinan Zhou", "Yaxiong Wang", "Haokun Lin", "Chen Ma", "Li Zhu", "Zhedong Zheng"], "title": "Scale Up Composed Image Retrieval Learning via Modification Text Generation", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "12 pages, 8 figures", "summary": "Composed Image Retrieval (CIR) aims to search an image of interest using a\ncombination of a reference image and modification text as the query. Despite\nrecent advancements, this task remains challenging due to limited training data\nand laborious triplet annotation processes. To address this issue, this paper\nproposes to synthesize the training triplets to augment the training resource\nfor the CIR problem. Specifically, we commence by training a modification text\ngenerator exploiting large-scale multimodal models and scale up the CIR\nlearning throughout both the pretraining and fine-tuning stages. During\npretraining, we leverage the trained generator to directly create Modification\nText-oriented Synthetic Triplets(MTST) conditioned on pairs of images. For\nfine-tuning, we first synthesize reverse modification text to connect the\ntarget image back to the reference image. Subsequently, we devise a two-hop\nalignment strategy to incrementally close the semantic gap between the\nmultimodal pair and the target image. We initially learn an implicit prototype\nutilizing both the original triplet and its reversed version in a cycle manner,\nfollowed by combining the implicit prototype feature with the modification text\nto facilitate accurate alignment with the target image. Extensive experiments\nvalidate the efficacy of the generated triplets and confirm that our proposed\nmethodology attains competitive recall on both the CIRR and FashionIQ\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05684", "pdf": "https://arxiv.org/pdf/2504.05684", "abs": "https://arxiv.org/abs/2504.05684", "authors": ["Tri Ton", "Ji Woo Hong", "Chang D. Yoo"], "title": "TARO: Timestep-Adaptive Representation Alignment with Onset-Aware Conditioning for Synchronized Video-to-Audio Synthesis", "categories": ["cs.SD", "cs.AI", "cs.CV"], "comment": "10 pages, 6 figures", "summary": "This paper introduces Timestep-Adaptive Representation Alignment with\nOnset-Aware Conditioning (TARO), a novel framework for high-fidelity and\ntemporally coherent video-to-audio synthesis. Built upon flow-based\ntransformers, which offer stable training and continuous transformations for\nenhanced synchronization and audio quality, TARO introduces two key\ninnovations: (1) Timestep-Adaptive Representation Alignment (TRA), which\ndynamically aligns latent representations by adjusting alignment strength based\non the noise schedule, ensuring smooth evolution and improved fidelity, and (2)\nOnset-Aware Conditioning (OAC), which integrates onset cues that serve as sharp\nevent-driven markers of audio-relevant visual moments to enhance\nsynchronization with dynamic visual events. Extensive experiments on the\nVGGSound and Landscape datasets demonstrate that TARO outperforms prior\nmethods, achieving relatively 53\\% lower Frechet Distance (FD), 29% lower\nFrechet Audio Distance (FAD), and a 97.19% Alignment Accuracy, highlighting its\nsuperior audio quality and synchronization precision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-09.jsonl"}
{"id": "2504.05803", "pdf": "https://arxiv.org/pdf/2504.05803", "abs": "https://arxiv.org/abs/2504.05803", "authors": ["Yihuan Huang", "Jiajun Liu", "Yanzhen Ren", "Wuyang Liu", "Juhua Tang"], "title": "SE4Lip: Speech-Lip Encoder for Talking Head Synthesis to Solve Phoneme-Viseme Alignment Ambiguity", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Speech-driven talking head synthesis tasks commonly use general acoustic\nfeatures (such as HuBERT and DeepSpeech) as guided speech features. However, we\ndiscovered that these features suffer from phoneme-viseme alignment ambiguity,\nwhich refers to the uncertainty and imprecision in matching phonemes (speech)\nwith visemes (lip). To address this issue, we propose the Speech Encoder for\nLip (SE4Lip) to encode lip features from speech directly, aligning speech and\nlip features in the joint embedding space by a cross-modal alignment framework.\nThe STFT spectrogram with the GRU-based model is designed in SE4Lip to preserve\nthe fine-grained speech features. Experimental results show that SE4Lip\nachieves state-of-the-art performance in both NeRF and 3DGS rendering models.\nIts lip sync accuracy improves by 13.7% and 14.2% compared to the best baseline\nand produces results close to the ground truth videos.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-09.jsonl"}
