{"id": "2504.06659", "pdf": "https://arxiv.org/pdf/2504.06659", "abs": "https://arxiv.org/abs/2504.06659", "authors": ["Xiaohua Feng", "Yuyuan Li", "Huwei Ji", "Jiaming Zhang", "Li Zhang", "Tianyu Du", "Chaochao Chen"], "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages", "summary": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback", "reinforcement learning", "preference", "alignment"], "score": 5}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06020", "pdf": "https://arxiv.org/pdf/2504.06020", "abs": "https://arxiv.org/abs/2504.06020", "authors": ["Liyuan Mao", "Haoran Xu", "Amy Zhang", "Weinan Zhang", "Chenjia Bai"], "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work done during internships at Institute of Artificial Intelligence\n  (TeleAI), China Telecom", "summary": "A generalizable reward model is crucial in Reinforcement Learning from Human\nFeedback (RLHF) as it enables correctly evaluating unseen prompt-response\npairs. However, existing reward models lack this ability, as they are typically\ntrained by increasing the reward gap between chosen and rejected responses,\nwhile overlooking the prompts that the responses are conditioned on.\nConsequently, when the trained reward model is evaluated on prompt-response\npairs that lie outside the data distribution, neglecting the effect of prompts\nmay result in poor generalization of the reward model. To address this issue,\nwe decompose the reward value into two independent components: prompt-free\nreward and prompt-related reward. Prompt-free reward represents the evaluation\nthat is determined only by responses, while the prompt-related reward reflects\nthe reward that derives from both the prompt and the response. We extract these\ntwo components from an information-theoretic perspective, which requires no\nextra models. Subsequently, we propose a new reward learning algorithm by\nprioritizing data samples based on their prompt-free reward values. Through toy\nexamples, we demonstrate that the extracted prompt-free and prompt-related\nrewards effectively characterize two parts of the reward model. Further,\nstandard evaluations show that our method improves both the alignment\nperformance and the generalization capability of the reward model.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06562", "pdf": "https://arxiv.org/pdf/2504.06562", "abs": "https://arxiv.org/abs/2504.06562", "authors": ["Longguang Zhong", "Fanqi Wan", "Ziyi Yang", "Guosheng Liang", "Tianyuan Shi", "Xiaojun Quan"], "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06843", "pdf": "https://arxiv.org/pdf/2504.06843", "abs": "https://arxiv.org/abs/2504.06843", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Ioannis Arapakis"], "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07070", "pdf": "https://arxiv.org/pdf/2504.07070", "abs": "https://arxiv.org/abs/2504.07070", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06611", "pdf": "https://arxiv.org/pdf/2504.06611", "abs": "https://arxiv.org/abs/2504.06611", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Simon Osindero"], "title": "Wanting to be Understood", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand \\textit{and\nto be understood} even in the absence of extrinsic rewards. Through simulations\nof the perceptual crossing paradigm, we explore the effect of various internal\nreward functions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06958", "pdf": "https://arxiv.org/pdf/2504.06958", "abs": "https://arxiv.org/abs/2504.06958", "authors": ["Xinhao Li", "Ziang Yan", "Desen Meng", "Lu Dong", "Xiangyu Zeng", "Yinan He", "Yali Wang", "Yu Qiao", "Yi Wang", "Limin Wang"], "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06313", "pdf": "https://arxiv.org/pdf/2504.06313", "abs": "https://arxiv.org/abs/2504.06313", "authors": ["Abdulkareem Alsudais"], "title": "Analyzing How Text-to-Image Models Represent Nationalities in Everyday Tasks", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "The primary objective of this paper is to investigate how a popular\nText-to-Image (T2I) model represents people from 208 different nationalities\nwhen prompted to generate images of individuals performing typical everyday\ntasks. Two scenarios were developed, and images were generated based on input\nprompts that specified nationalities. The results show that in one scenario,\nthe majority of images, and in the other, a substantial portion, depict\nindividuals wearing traditional attire. This suggests that the model emphasizes\nsuch characteristics even when they are impractical for the given task. A\nstatistically significant relationship was observed between this representation\npattern and the regions associated with the specified countries. This indicates\nthat the issue disproportionately affects certain areas, particularly the\nMiddle East & North Africa and Sub-Saharan Africa. A notable association with\nincome groups was also found. CLIP was used to measure alignment scores between\ngenerated images and various prompts and captions. The findings indicate\nstatistically significant higher scores for images featuring individuals in\ntraditional attire in one scenario. The study also examined revised prompts\n(additional contextual information automatically added to the original input\nprompts) to assess their potential influence on how individuals are represented\nin the generated images, finding that the word \"traditional\" was commonly added\nto revised prompts. These findings provide valuable insights into how T2I\nmodels represent individuals from various countries and highlight potential\nareas for improvement in future models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06564", "pdf": "https://arxiv.org/pdf/2504.06564", "abs": "https://arxiv.org/abs/2504.06564", "authors": ["Qingcheng Zeng", "Weihao Xuan", "Leyang Cui", "Rob Voigt"], "title": "Do Reasoning Models Show Better Verbalized Calibration?", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["factuality", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06464", "pdf": "https://arxiv.org/pdf/2504.06464", "abs": "https://arxiv.org/abs/2504.06464", "authors": ["José A. Pilartes-Congo", "Matthew Kastl", "Michael J. Starek", "Marina Vicens-Miquel", "Philippe Tissot"], "title": "Implementation of a Zed 2i Stereo Camera for High-Frequency Shoreline Change and Coastal Elevation Monitoring", "categories": ["cs.CV"], "comment": "Published in IGARSS 2023 - 2023 IEEE International Geoscience and\n  Remote Sensing Symposium", "summary": "The increasing population, thus financial interests, in coastal areas have\nincreased the need to monitor coastal elevation and shoreline change. Though\nseveral resources exist to obtain this information, they often lack the\nrequired temporal resolution for short-term monitoring (e.g., every hour). To\naddress this issue, this study implements a low-cost ZED 2i stereo camera\nsystem and close-range photogrammetry to collect images for generating 3D point\nclouds, digital surface models (DSMs) of beach elevation, and georectified\nimagery at a localized scale and high temporal resolution. The main\ncontributions of this study are (i) intrinsic camera calibration, (ii)\ngeorectification and registration of acquired imagery and point cloud, (iii)\ngeneration of the DSM of the beach elevation, and (iv) a comparison of derived\nproducts against those from uncrewed aircraft system structure-from-motion\nphotogrammetry. Preliminary results show that despite its limitations, the ZED\n2i can provide the desired mapping products at localized and high temporal\nscales. The system achieved a mean reprojection error of 0.20 px, a point cloud\nregistration of 27 cm, a vertical error of 37.56 cm relative to ground truth,\nand georectification root mean square errors of 2.67 cm and 2.81 cm for x and\ny.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06486", "pdf": "https://arxiv.org/pdf/2504.06486", "abs": "https://arxiv.org/abs/2504.06486", "authors": ["Samuel Stevens", "S M Rayeed", "Jenna Kline"], "title": "Mind the Gap: Evaluating Vision Systems in Small Data Applications", "categories": ["cs.CV"], "comment": "4 pages (main text), 5 figures", "summary": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06650", "pdf": "https://arxiv.org/pdf/2504.06650", "abs": "https://arxiv.org/abs/2504.06650", "authors": ["Zijian Wang", "Chang Xu"], "title": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06606", "pdf": "https://arxiv.org/pdf/2504.06606", "abs": "https://arxiv.org/abs/2504.06606", "authors": ["Minghe Gao", "Xuqi Liu", "Zhongqi Yue", "Yang Wu", "Shuang Chen", "Juncheng Li", "Siliang Tang", "Fei Wu", "Tat-Seng Chua", "Yueting Zhuang"], "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06607", "pdf": "https://arxiv.org/pdf/2504.06607", "abs": "https://arxiv.org/abs/2504.06607", "authors": ["Onkar Krishna", "Hiroki Ohashi"], "title": "Visually Similar Pair Alignment for Robust Cross-Domain Object Detection", "categories": ["cs.CV"], "comment": "15 pages, Journal paper submission", "summary": "Domain gaps between training data (source) and real-world environments\n(target) often degrade the performance of object detection models. Most\nexisting methods aim to bridge this gap by aligning features across source and\ntarget domains but often fail to account for visual differences, such as color\nor orientation, in alignment pairs. This limitation leads to less effective\ndomain adaptation, as the model struggles to manage both domain-specific shifts\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\nfor the first time, using a custom-built dataset, that aligning visually\nsimilar pairs significantly improves domain adaptation. Based on this insight,\nwe propose a novel memory-based system to enhance domain alignment. This system\nstores precomputed features of foreground objects and background areas from the\nsource domain, which are periodically updated during training. By retrieving\nvisually similar source features for alignment with target foreground and\nbackground features, the model effectively addresses domain-specific\ndifferences while reducing the impact of visual variations. Extensive\nexperiments across diverse domain shift scenarios validate our method's\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06910", "pdf": "https://arxiv.org/pdf/2504.06910", "abs": "https://arxiv.org/abs/2504.06910", "authors": ["Sheng Lu", "Ilia Kuznetsov", "Iryna Gurevych"], "title": "Identifying Aspects in Peer Reviews", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06618", "pdf": "https://arxiv.org/pdf/2504.06618", "abs": "https://arxiv.org/abs/2504.06618", "authors": ["Zijun Lin", "M Ganesh Kumar", "Cheston Tan"], "title": "Human-like compositional learning of visually-grounded concepts using synthetic environments", "categories": ["cs.CV"], "comment": null, "summary": "The compositional structure of language enables humans to decompose complex\nphrases and map them to novel visual concepts, showcasing flexible\nintelligence. While several algorithms exhibit compositionality, they fail to\nelucidate how humans learn to compose concept classes and ground visual cues\nthrough trial and error. To investigate this multi-modal learning challenge, we\ndesigned a 3D synthetic environment in which an agent learns, via\nreinforcement, to navigate to a target specified by a natural language\ninstruction. These instructions comprise nouns, attributes, and critically,\ndeterminers, prepositions, or both. The vast array of word combinations\nheightens the compositional complexity of the visual grounding task, as\nnavigating to a blue cube above red spheres is not rewarded when the\ninstruction specifies navigating to \"some blue cubes below the red sphere\". We\nfirst demonstrate that reinforcement learning agents can ground determiner\nconcepts to visual targets but struggle with more complex prepositional\nconcepts. Second, we show that curriculum learning, a strategy humans employ,\nenhances concept learning efficiency, reducing the required training episodes\nby 15% in determiner environments and enabling agents to easily learn\nprepositional concepts. Finally, we establish that agents trained on determiner\nor prepositional concepts can decompose held-out test instructions and rapidly\nadapt their navigation policies to unseen visual object combinations.\nLeveraging synthetic environments, our findings demonstrate that multi-modal\nreinforcement learning agents can achieve compositional understanding of\ncomplex concept classes and highlight the efficacy of human-like learning\nstrategies in improving artificial systems' learning efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06627", "pdf": "https://arxiv.org/pdf/2504.06627", "abs": "https://arxiv.org/abs/2504.06627", "authors": ["Ludvig Dillén", "Per-Erik Forssén", "Johan Edstedt"], "title": "FACT: Multinomial Misalignment Classification for Point Cloud Registration", "categories": ["cs.CV", "cs.LG", "I.4.5; I.4.8; I.2.9; I.2.10"], "comment": "Accepted at SCIA 2025 (the Scandinavian Conference on Image Analysis\n  2025)", "summary": "We present FACT, a method for predicting alignment quality (i.e.,\nregistration error) of registered lidar point cloud pairs. This is useful e.g.\nfor quality assurance of large, automatically registered 3D models. FACT\nextracts local features from a registered pair and processes them with a point\ntransformer-based network to predict a misalignment class. We generalize prior\nwork that study binary alignment classification of registration errors, by\nrecasting it as multinomial misalignment classification. To achieve this, we\nintroduce a custom regression-by-classification loss function that combines the\ncross-entropy and Wasserstein losses, and demonstrate that it outperforms both\ndirect regression and prior binary classification. FACT successfully classifies\npoint-cloud pairs registered with both the classical ICP and GeoTransformer,\nwhile other choices, such as standard point-cloud-quality metrics and\nregistration residuals are shown to be poor choices for predicting\nmisalignment. On a synthetically perturbed point-cloud task introduced by the\nCorAl method, we show that FACT achieves substantially better performance than\nCorAl. Finally, we demonstrate how FACT can assist experts in correcting\nmisaligned point-cloud maps. Our code is available at\nhttps://github.com/LudvigDillen/FACT_for_PCMC.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07024", "pdf": "https://arxiv.org/pdf/2504.07024", "abs": "https://arxiv.org/abs/2504.07024", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Data Augmentation and Hyperparameter Tuning for Low-Resource MFA", "categories": ["cs.CL"], "comment": null, "summary": "A continued issue for those working with computational tools and endangered\nand under-resourced languages is the lower accuracy of results for languages\nwith smaller amounts of data. We attempt to ameliorate this issue by using data\naugmentation methods to increase corpus size, comparing augmentation to\nhyperparameter tuning for multilingual forced alignment. Unlike text\naugmentation methods, audio augmentation does not lead to substantially\nincreased performance. Hyperparameter tuning, on the other hand, results in\nsubstantial improvement without (for this amount of data) infeasible additional\ntraining time. For languages with small to medium amounts of training data,\nthis is a workable alternative to adapting models from high-resource languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.05804", "pdf": "https://arxiv.org/pdf/2504.05804", "abs": "https://arxiv.org/abs/2504.05804", "authors": ["Yiming Tang", "Yi Fan", "Chenxiao Yu", "Tiankai Yang", "Yue Zhao", "Xiyang Hu"], "title": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06271", "pdf": "https://arxiv.org/pdf/2504.06271", "abs": "https://arxiv.org/abs/2504.06271", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "title": "ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in question-answering (QA) tasks, and\nretrieval-augmented generation (RAG) enhances their precision by incorporating\nexternal evidence from diverse sources like web pages, databases, and knowledge\ngraphs. However, current RAG methods rely on agent-specific strategies for\nindividual data sources, posing challenges low-resource or black-box\nenvironments and complicates operations when evidence is fragmented across\nsources. To address these limitations, we propose ER-RAG, a framework that\nunifies evidence integration across heterogeneous data sources using the\nEntity-Relationship (ER) model. ER-RAG standardizes entity retrieval and\nrelationship querying through ER-based APIs with GET and JOIN operations. It\nemploys a two-stage generation process: first, a preference optimization module\nselects optimal sources; second, another module constructs API chains based on\nsource schemas. This unified approach allows efficient fine-tuning and seamless\nintegration across diverse data sources. ER-RAG demonstrated its effectiveness\nby winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving\nperformance on par with commercial RAG pipelines using an 8B LLM backbone. It\noutperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval\nby 5.5X.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06752", "pdf": "https://arxiv.org/pdf/2504.06752", "abs": "https://arxiv.org/abs/2504.06752", "authors": ["Rishbuh Parihar", "Vaibhav Agrawal", "Sachidanand VS", "R. Venkatesh Babu"], "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/compasscontrol", "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware \\textbf{compass} tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06303", "pdf": "https://arxiv.org/pdf/2504.06303", "abs": "https://arxiv.org/abs/2504.06303", "authors": ["Dang Nguyen", "Chenhao Tan"], "title": "On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages, 15 figures, 14 tables", "summary": "Understanding and mitigating biases is critical for the adoption of large\nlanguage models (LLMs) in high-stakes decision-making. We introduce Admissions\nand Hiring, decision tasks with hypothetical applicant profiles where a\nperson's race can be inferred from their name, as simplified test beds for\nracial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit\nstrong biases. Gemma grants admission to 26% more White than Black applicants,\nand LLaMA hires 60% more Asian than White applicants. We demonstrate that these\nbiases are resistant to prompt engineering: multiple prompting strategies all\nfail to promote fairness. In contrast, using distributed alignment search, we\ncan identify \"race subspaces\" within model activations and intervene on them to\ndebias model decisions. Averaging the representation across all races within\nthe subspaces reduces Gemma's bias by 37-57%. Finally, we examine the\ngeneralizability of Gemma's race subspaces, and find limited evidence for\ngeneralization, where changing the prompt format can affect the race\nrepresentation. Our work suggests mechanistic approaches may provide a\npromising venue for improving the fairness of LLMs, but a universal race\nrepresentation remains elusive.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06514", "pdf": "https://arxiv.org/pdf/2504.06514", "abs": "https://arxiv.org/abs/2504.06514", "authors": ["Chenrui Fan", "Ming Li", "Lichao Sun", "Tianyi Zhou"], "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06800", "pdf": "https://arxiv.org/pdf/2504.06800", "abs": "https://arxiv.org/abs/2504.06800", "authors": ["Danielle Cohen", "Hila Chefer", "Lior Wolf"], "title": "A Meaningful Perturbation Metric for Evaluating Explainability Methods", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable success, yet their\nwide adoption is often hindered by their opaque decision-making. To address\nthis, attribution methods have been proposed to assign relevance values to each\npart of the input. However, different methods often produce entirely different\nrelevance maps, necessitating the development of standardized metrics to\nevaluate them. Typically, such evaluation is performed through perturbation,\nwherein high- or low-relevance regions of the input image are manipulated to\nexamine the change in prediction. In this work, we introduce a novel approach,\nwhich harnesses image generation models to perform targeted perturbation.\nSpecifically, we focus on inpainting only the high-relevance pixels of an input\nimage to modify the model's predictions while preserving image fidelity. This\nis in contrast to existing approaches, which often produce out-of-distribution\nmodifications, leading to unreliable results. Through extensive experiments, we\ndemonstrate the effectiveness of our approach in generating meaningful rankings\nacross a wide range of models and attribution methods. Crucially, we establish\nthat the ranking produced by our metric exhibits significantly higher\ncorrelation with human preferences compared to existing approaches,\nunderscoring its potential for enhancing interpretability in DNNs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06801", "pdf": "https://arxiv.org/pdf/2504.06801", "abs": "https://arxiv.org/abs/2504.06801", "authors": ["Rishubh Parihar", "Srinjay Sarkar", "Sarthak Vora", "Jogendra Kundu", "R. Venkatesh Babu"], "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/monoplace3D", "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06835", "pdf": "https://arxiv.org/pdf/2504.06835", "abs": "https://arxiv.org/abs/2504.06835", "authors": ["Ziyi Wang", "Haoran Wu", "Yiming Rong", "Deyang Jiang", "Yixin Zhang", "Yunlong Zhao", "Shuang Xu", "Bo XU"], "title": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07086", "pdf": "https://arxiv.org/pdf/2504.07086", "abs": "https://arxiv.org/abs/2504.07086", "authors": ["Andreas Hochlehnert", "Hardik Bhatnagar", "Vishaal Udandarao", "Samuel Albanie", "Ameya Prabhu", "Matthias Bethge"], "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report", "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06897", "pdf": "https://arxiv.org/pdf/2504.06897", "abs": "https://arxiv.org/abs/2504.06897", "authors": ["Jiawei Mao", "Yuhan Wang", "Yucheng Tang", "Daguang Xu", "Kang Wang", "Yang Yang", "Zongwei Zhou", "Yuyin Zhou"], "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web", "summary": "This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06957", "pdf": "https://arxiv.org/pdf/2504.06957", "abs": "https://arxiv.org/abs/2504.06957", "authors": ["Marco Acerbis", "Nataša Sladoje", "Joakim Lindblad"], "title": "A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology", "categories": ["cs.CV"], "comment": "14 pages, 6 figures, SCIA2025", "summary": "Accurate and efficient cell detection is crucial in many biomedical image\nanalysis tasks. We evaluate the performance of several Deep Learning (DL)\nmethods for cell detection in Papanicolaou-stained cytological Whole Slide\nImages (WSIs), focusing on accuracy of predictions and computational\nefficiency. We examine recentoff-the-shelf algorithms as well as\ncustom-designed detectors, applying them to two datasets: the CNSeg Dataset and\nthe Oral Cancer (OC) Dataset. Our comparison includes well-established\nsegmentation methods such as StarDist, Cellpose, and the Segment Anything Model\n2 (SAM2), alongside centroid-based Fully Convolutional Regression Network\n(FCRN) approaches. We introduce a suitable evaluation metric to assess the\naccuracy of predictions based on the distance from ground truth positions. We\nalso explore the impact of dataset size and data augmentation techniques on\nmodel performance. Results show that centroid-based methods, particularly the\nImproved Fully Convolutional Regression Network (IFCRN) method, outperform\nsegmentation-based methods in terms of both detection accuracy and\ncomputational efficiency. This study highlights the potential of centroid-based\ndetectors as a preferred option for cell detection in resource-limited\nenvironments, offering faster processing times and lower GPU memory usage\nwithout compromising accuracy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07061", "pdf": "https://arxiv.org/pdf/2504.07061", "abs": "https://arxiv.org/abs/2504.07061", "authors": ["Shi Pan", "Jianan Chen", "Maria Secrier"], "title": "Teaching pathology foundation models to accurately predict gene expression with parameter efficient knowledge transfer", "categories": ["cs.CV"], "comment": null, "summary": "Gene expression profiling provides critical insights into cellular\nheterogeneity, biological processes and disease mechanisms. There has been an\nincreasing interest in computational approaches that can predict gene\nexpression directly from digitalized histopathology images. While image\nfoundation models have shown promise in a variety of pathology downstream\nanalysis, their performances on gene-expression prediction are still limited.\nExplicitly incorporating information from the transcriptomic models can help\nimage models to address domain shift, yet the fine-tuning and alignment of\nfoundation models can be expensive. In the work, we propose Parameter Efficient\nKnowledge trAnsfer (PEKA), a novel framework that leverages Block-Affine\nAdaptation and integrates knowledge distillation and structure alignment losses\nfor cross-modal knowledge transfer. We evaluated PEKA for gene expression\nprediction using multiple spatial transcriptomics datasets (comprising 206,123\nimage tiles with matched gene expression profiles) that encompassed various\ntypes of tissue. PEKA achieved at least 5\\% performance improvement over\nbaseline foundation models while also outperforming alternative\nparameter-efficient fine-tuning strategies. We will release the code, datasets\nand aligned models after peer-review to facilitate broader adoption and further\ndevelopment for parameter efficient model alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.07083", "pdf": "https://arxiv.org/pdf/2504.07083", "abs": "https://arxiv.org/abs/2504.07083", "authors": ["Mengchen Zhang", "Tong Wu", "Jing Tan", "Ziwei Liu", "Gordon Wetzstein", "Dahua Lin"], "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography", "categories": ["cs.CV"], "comment": null, "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06301", "pdf": "https://arxiv.org/pdf/2504.06301", "abs": "https://arxiv.org/abs/2504.06301", "authors": ["Mohsen Jenadeleh", "Jon Sneyers", "Panqi Jia", "Shima Mohammadi", "Joao Ascenso", "Dietmar Saupe"], "title": "Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 3 tables, submitted to QoMEX 2025", "summary": "Learning-based image compression methods have recently emerged as promising\nalternatives to traditional codecs, offering improved rate-distortion\nperformance and perceptual quality. JPEG AI represents the latest standardized\nframework in this domain, leveraging deep neural networks for high-fidelity\nimage reconstruction. In this study, we present a comprehensive subjective\nvisual quality assessment of JPEG AI-compressed images using the JPEG AIC-3\nmethodology, which quantifies perceptual differences in terms of Just\nNoticeable Difference (JND) units. We generated a dataset of 50 compressed\nimages with fine-grained distortion levels from five diverse sources. A\nlarge-scale crowdsourced experiment collected 96,200 triplet responses from 459\nparticipants. We reconstructed JND-based quality scales using a unified model\nbased on boosted and plain triplet comparisons. Additionally, we evaluated the\nalignment of multiple objective image quality metrics with human perception in\nthe high-fidelity range. The CVVDP metric achieved the overall highest\nperformance; however, most metrics including CVVDP were overly optimistic in\npredicting the quality of JPEG AI-compressed images. These findings emphasize\nthe necessity for rigorous subjective evaluations in the development and\nbenchmarking of modern image codecs, particularly in the high-fidelity range.\nAnother technical contribution is the introduction of the well-known\nMeng-Rosenthal-Rubin statistical test to the field of Quality of Experience\nresearch. This test can reliably assess the significance of difference in\nperformance of quality metrics in terms of correlation between metrics and\nground truth. The complete dataset, including all subjective scores, is\npublicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06479", "pdf": "https://arxiv.org/pdf/2504.06479", "abs": "https://arxiv.org/abs/2504.06479", "authors": ["Julian Nubert", "Turcan Tuna", "Jonas Frey", "Cesar Cadena", "Katherine J. Kuchenbecker", "Shehryar Khattak", "Marco Hutter"], "title": "Holistic Fusion: Task- and Setup-Agnostic Robot Localization and State Estimation with Factor Graphs", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": "21 pages, 25 figures, 9 tables, journal submission", "summary": "Seamless operation of mobile robots in challenging environments requires\nlow-latency local motion estimation (e.g., dynamic maneuvers) and accurate\nglobal localization (e.g., wayfinding). While most existing sensor-fusion\napproaches are designed for specific scenarios, this work introduces a flexible\nopen-source solution for task- and setup-agnostic multimodal sensor fusion that\nis distinguished by its generality and usability. Holistic Fusion formulates\nsensor fusion as a combined estimation problem of i) the local and global robot\nstate and ii) a (theoretically unlimited) number of dynamic context variables,\nincluding automatic alignment of reference frames; this formulation fits\ncountless real-world applications without any conceptual modifications. The\nproposed factor-graph solution enables the direct fusion of an arbitrary number\nof absolute, local, and landmark measurements expressed with respect to\ndifferent reference frames by explicitly including them as states in the\noptimization and modeling their evolution as random walks. Moreover, local\nsmoothness and consistency receive particular attention to prevent jumps in the\nrobot state belief. HF enables low-latency and smooth online state estimation\non typical robot hardware while simultaneously providing low-drift global\nlocalization at the IMU measurement rate. The efficacy of this released\nframework is demonstrated in five real-world scenarios on three robotic\nplatforms, each with distinct task requirements.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-10.jsonl"}
{"id": "2504.06961", "pdf": "https://arxiv.org/pdf/2504.06961", "abs": "https://arxiv.org/abs/2504.06961", "authors": ["Yu Qi", "Yuanchen Ju", "Tianming Wei", "Chi Chu", "Lawson L. S. Wong", "Huazhe Xu"], "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025 (Conference on Computer Vision and Pattern\n  Recognition)", "summary": "3D assembly tasks, such as furniture assembly and component fitting, play a\ncrucial role in daily life and represent essential capabilities for future home\nrobots. Existing benchmarks and datasets predominantly focus on assembling\ngeometric fragments or factory parts, which fall short in addressing the\ncomplexities of everyday object interactions and assemblies. To bridge this\ngap, we present 2BY2, a large-scale annotated dataset for daily pairwise\nobjects assembly, covering 18 fine-grained tasks that reflect real-life\nscenarios, such as plugging into sockets, arranging flowers in vases, and\ninserting bread into toasters. 2BY2 dataset includes 1,034 instances and 517\npairwise objects with pose and symmetry annotations, requiring approaches that\nalign geometric shapes while accounting for functional and spatial\nrelationships between objects. Leveraging the 2BY2 dataset, we propose a\ntwo-step SE(3) pose estimation method with equivariant features for assembly\nconstraints. Compared to previous shape assembly methods, our approach achieves\nstate-of-the-art performance across all 18 tasks in the 2BY2 dataset.\nAdditionally, robot experiments further validate the reliability and\ngeneralization ability of our method for complex 3D assembly tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "fine-grained"], "score": 3}}, "source_file": "2025-04-10.jsonl"}
