{"id": "2504.07957", "pdf": "https://arxiv.org/pdf/2504.07957", "abs": "https://arxiv.org/abs/2504.07957", "authors": ["Shengyuan Ding", "Shenxi Wu", "Xiangyu Zhao", "Yuhang Zang", "Haodong Duan", "Xiaoyi Dong", "Pan Zhang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "MM-IFEngine: Towards Multimodal Instruction Following", "categories": ["cs.CV"], "comment": null, "summary": "The Instruction Following (IF) ability measures how well Multi-modal Large\nLanguage Models (MLLMs) understand exactly what users are telling them and\nwhether they are doing it right. Existing multimodal instruction following\ntraining data is scarce, the benchmarks are simple with atomic instructions,\nand the evaluation strategies are imprecise for tasks demanding exact output\nconstraints. To address this, we present MM-IFEngine, an effective pipeline to\ngenerate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields\nlarge-scale, diverse, and high-quality training data MM-IFInstruct-23k, which\nis suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for\nDirect Preference Optimization (DPO). We further introduce MM-IFEval, a\nchallenging and diverse multi-modal instruction-following benchmark that\nincludes (1) both compose-level constraints for output responses and\nperception-level constraints tied to the input images, and (2) a comprehensive\nevaluation pipeline incorporating both rule-based assessment and judge model.\nWe conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on\nMM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF\nbenchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval\n(+12.3$\\%$). The full data and evaluation code will be released on\nhttps://github.com/SYuan03/MM-IFEngine.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07165", "pdf": "https://arxiv.org/pdf/2504.07165", "abs": "https://arxiv.org/abs/2504.07165", "authors": ["Yana Wei", "Liang Zhao", "Kangheng Lin", "En Yu", "Yuang Peng", "Runpei Dong", "Jianjian Sun", "Haoran Wei", "Zheng Ge", "Xiangyu Zhang", "Vishal M. Patel"], "title": "Perception in Reflection", "categories": ["cs.CV"], "comment": null, "summary": "We present a perception in reflection paradigm designed to transcend the\nlimitations of current large vision-language models (LVLMs), which are expected\nyet often fail to achieve perfect perception initially. Specifically, we\npropose Reflective Perception (RePer), a dual-model reflection mechanism that\nsystematically alternates between policy and critic models, enables iterative\nrefinement of visual perception. This framework is powered by Reflective\nPerceptual Learning (RPL), which reinforces intrinsic reflective capabilities\nthrough a methodically constructed visual reflection dataset and reflective\nunlikelihood training. Comprehensive experimental evaluation demonstrates\nRePer's quantifiable improvements in image understanding, captioning precision,\nand hallucination reduction. Notably, RePer achieves strong alignment between\nmodel attention patterns and human visual focus, while RPL optimizes\nfine-grained and free-form preference alignment. These advancements establish\nperception in reflection as a robust paradigm for future multimodal agents,\nparticularly in tasks requiring complex reasoning and multi-step manipulation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07942", "pdf": "https://arxiv.org/pdf/2504.07942", "abs": "https://arxiv.org/abs/2504.07942", "authors": ["Nico Catalano", "Stefano Samele", "Paolo Pertino", "Matteo Matteucci"], "title": "MARS: a Multimodal Alignment and Ranking System for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Current Few Shot Segmentation literature lacks a mask selection method that\ngoes beyond visual similarity between the query and example images, leading to\nsuboptimal predictions. We present MARS, a plug-and-play ranking system that\nleverages multimodal cues to filter and merge mask proposals robustly. Starting\nfrom a set of mask predictions for a single query image, we score, filter, and\nmerge them to improve results. Proposals are evaluated using multimodal scores\ncomputed at local and global levels. Extensive experiments on COCO-20i,\nPascal-5i, LVIS-92i, and FSS-1000 demonstrate that integrating all four scoring\ncomponents is crucial for robust ranking, validating our contribution. As MARS\ncan be effortlessly integrated with various mask proposal systems, we deploy it\nacross a wide range of top-performer methods and achieve new state-of-the-art\nresults on multiple existing benchmarks. Code will be available upon\nacceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07898", "pdf": "https://arxiv.org/pdf/2504.07898", "abs": "https://arxiv.org/abs/2504.07898", "authors": ["Qi Liu", "Jiaxin Mao", "Ji-Rong Wen"], "title": "How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective", "categories": ["cs.IR", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent studies have shown that large language models (LLMs) can assess\nrelevance and support information retrieval (IR) tasks such as document ranking\nand relevance judgment generation. However, the internal mechanisms by which\noff-the-shelf LLMs understand and operationalize relevance remain largely\nunexplored. In this paper, we systematically investigate how different LLM\nmodules contribute to relevance judgment through the lens of mechanistic\ninterpretability. Using activation patching techniques, we analyze the roles of\nvarious model components and identify a multi-stage, progressive process in\ngenerating either pointwise or pairwise relevance judgment. Specifically, LLMs\nfirst extract query and document information in the early layers, then process\nrelevance information according to instructions in the middle layers, and\nfinally utilize specific attention heads in the later layers to generate\nrelevance judgments in the required format. Our findings provide insights into\nthe mechanisms underlying relevance assessment in LLMs, offering valuable\nimplications for future research on leveraging LLMs for IR tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "pairwise"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07965", "pdf": "https://arxiv.org/pdf/2504.07965", "abs": "https://arxiv.org/abs/2504.07965", "authors": ["Lorenz Linhardt", "Tom Neuhäuser", "Lenka Tětková", "Oliver Eberle"], "title": "Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments", "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2025 Workshop on Representational Alignment (Re-Align)", "summary": "Small and mid-sized generative language models have gained increasing\nattention. Their size and availability make them amenable to being analyzed at\na behavioral as well as a representational level, allowing investigations of\nhow these levels interact. We evaluate 32 publicly available language models\nfor their representational and behavioral alignment with human similarity\njudgments on a word triplet task. This provides a novel evaluation setting to\nprobe semantic associations in language beyond common pairwise comparisons. We\nfind that (1) even the representations of small language models can achieve\nhuman-level alignment, (2) instruction-tuned model variants can exhibit\nsubstantially increased agreement, (3) the pattern of alignment across layers\nis highly model dependent, and (4) alignment based on models' behavioral\nresponses is highly dependent on model size, matching their representational\nalignment only for the largest evaluated models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07100", "pdf": "https://arxiv.org/pdf/2504.07100", "abs": "https://arxiv.org/abs/2504.07100", "authors": ["Abhay Gupta", "Jacob Cheung", "Philip Meng", "Shayan Sayyed", "Austen Liao", "Kevin Zhu", "Sean O'Brien"], "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "The diversity of human language, shaped by social, cultural, and regional\ninfluences, presents significant challenges for natural language processing\n(NLP) systems. Existing benchmarks often overlook intra-language variations,\nleaving speakers of non-standard dialects underserved. To address this gap, we\nintroduce EnDive (English Diversity), a benchmark that evaluates five\nwidely-used large language models (LLMs) across tasks in language\nunderstanding, algorithmic reasoning, mathematics, and logic. Our framework\ntranslates Standard American English datasets into five underrepresented\ndialects using few-shot prompting with verified examples from native speakers,\nand compare these translations against rule-based methods via fluency\nassessments, preference tests, and semantic similarity metrics. Human\nevaluations confirm high translation quality, with average scores of at least\n6.02/7 for faithfulness, fluency, and formality. By filtering out\nnear-identical translations, we create a challenging dataset that reveals\nsignificant performance disparities - models consistently underperform on\ndialectal inputs compared to Standard American English. EnDive thus advances\ndialect-aware NLP by uncovering model biases and promoting more equitable\nlanguage technologies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07113", "pdf": "https://arxiv.org/pdf/2504.07113", "abs": "https://arxiv.org/abs/2504.07113", "authors": ["Aly M. Kassem", "Bernhard Schölkopf", "Zhijing Jin"], "title": "How Robust Are Router-LLMs? Analysis of the Fragility of LLM Routing Capabilities", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Large language model (LLM) routing has emerged as a crucial strategy for\nbalancing computational costs with performance by dynamically assigning queries\nto the most appropriate model based on query complexity. Despite recent\nadvances showing that preference-data-based routers can outperform traditional\nmethods, current evaluation benchmarks remain limited. They largely focus on\ngeneral model capabilities while overlooking task-specific behaviors and\ncritical concerns such as privacy, safety, and potential backdoor\nvulnerabilities introduced through preference data. In response, we propose the\nDSC benchmark: Diverse, Simple, and Categorized, an evaluation framework that\ncategorizes router performance across a broad spectrum of query types,\nincluding coding, translation, mathematics, human instructions, general\nknowledge, and LLM jailbreaking. Additionally, it integrates privacy and safety\nassessments to reveal hidden risks. Our experiments on three preference-based\nrouters and two commercial counterparts demonstrate that while these systems\nimprove efficiency, they often make suboptimal, category-driven decisions. For\ninstance, a BERT-based router directs all coding and mathematics queries to the\nmost powerful LLM even when simpler models would suffice, while routing\njailbreaking attempts to weaker models, thereby elevating safety risks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07174", "pdf": "https://arxiv.org/pdf/2504.07174", "abs": "https://arxiv.org/abs/2504.07174", "authors": ["Mingxuan Li", "Hanchen Li", "Chenhao Tan"], "title": "HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 3 figures, code link:\n  https://github.com/ChicagoHAI/HypoEval-Gen", "summary": "Large language models (LLMs) have demonstrated great potential for automating\nthe evaluation of natural language generation. Previous frameworks of\nLLM-as-a-judge fall short in two ways: they either use zero-shot setting\nwithout consulting any human input, which leads to low alignment, or fine-tune\nLLMs on labeled data, which requires a non-trivial number of samples. Moreover,\nprevious methods often provide little reasoning behind automated evaluations.\nIn this paper, we propose HypoEval, Hypothesis-guided Evaluation framework,\nwhich first uses a small corpus of human evaluations to generate more detailed\nrubrics for human judgments and then incorporates a checklist-like approach to\ncombine LLM's assigned scores on each decomposed dimension to acquire overall\nscores. With only 30 human evaluations, HypoEval achieves state-of-the-art\nperformance in alignment with both human rankings (Spearman correlation) and\nhuman scores (Pearson correlation), on average outperforming G-Eval by 11.86%\nand fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human\nevaluations by 11.95%. Furthermore, we conduct systematic studies to assess the\nrobustness of HypoEval, highlighting its effectiveness as a reliable and\ninterpretable automated evaluation framework.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation", "dimension"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07315", "pdf": "https://arxiv.org/pdf/2504.07315", "abs": "https://arxiv.org/abs/2504.07315", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Multilingual MFA: Forced Alignment on Low-Resource Related Languages", "categories": ["cs.CL"], "comment": null, "summary": "We compare the outcomes of multilingual and crosslingual training for related\nand unrelated Australian languages with similar phonological inventories. We\nuse the Montreal Forced Aligner to train acoustic models from scratch and adapt\na large English model, evaluating results against seen data, unseen data (seen\nlanguage), and unseen data and language. Results indicate benefits of adapting\nthe English baseline model for previously unseen languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07360", "pdf": "https://arxiv.org/pdf/2504.07360", "abs": "https://arxiv.org/abs/2504.07360", "authors": ["Taibiao Zhao", "Xiaobing Chen", "Mingxuan Sun"], "title": "Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The adaptation of large language models (LLMs) to time series forecasting\nposes unique challenges, as time series data is continuous in nature, while\nLLMs operate on discrete tokens. Despite the success of LLMs in natural\nlanguage processing (NLP) and other structured domains, aligning time series\ndata with language-based representations while maintaining both predictive\naccuracy and interpretability remains a significant hurdle. Existing methods\nhave attempted to reprogram time series data into text-based forms, but these\noften fall short in delivering meaningful, interpretable results. In this\npaper, we propose a multi-level text alignment framework for time series\nforecasting using LLMs that not only improves prediction accuracy but also\nenhances the interpretability of time series representations. Our method\ndecomposes time series into trend, seasonal, and residual components, which are\nthen reprogrammed into component-specific text representations. We introduce a\nmulti-level alignment mechanism, where component-specific embeddings are\naligned with pre-trained word tokens, enabling more interpretable forecasts.\nExperiments on multiple datasets demonstrate that our method outperforms\nstate-of-the-art models in accuracy while providing good interpretability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416", "abs": "https://arxiv.org/abs/2504.07416", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language alignment in radiology. However, existing approaches struggle\nto effectively utilize complex radiology reports for learning, rely on\nlow-resolution images, and offer limited interpretability in attention\nmechanisms. To address these challenges, we introduce RadZero, a novel\nsimilarity-based cross-attention framework for vision-language alignment in\nradiology with zero-shot multi-task capability. RadZero leverages large\nlanguage models to extract minimal semantic sentences from radiology reports\nand employs a multi-positive contrastive learning strategy to effectively\ncapture relationships between images and multiple relevant textual\ndescriptions. It also utilizes a pre-trained vision encoder with additional\ntrainable Transformer layers, allowing efficient high-resolution image\nprocessing. By computing similarity between text embeddings and local image\npatch features, RadZero enables zero-shot inference with similarity probability\nfor classification and pixel-level cross-modal similarity maps for grounding\nand segmentation. Experimental results on public chest radiograph benchmarks\nshow that RadZero outperforms state-of-the-art methods in zero-shot\nclassification, grounding, and segmentation. Furthermore, cross-modal\nsimilarity map analysis highlights its potential for improving explainability\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\nRadZero's capability for open-vocabulary semantic segmentation, further\nvalidating its effectiveness in medical imaging.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07441", "pdf": "https://arxiv.org/pdf/2504.07441", "abs": "https://arxiv.org/abs/2504.07441", "authors": ["Huilin Yin", "Pengyu Wang", "Senmao Li", "Jun Yan", "Daniel Watzenig"], "title": "WS-DETR: Robust Water Surface Object Detection through Vision-Radar Fusion with Detection Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Robust object detection for Unmanned Surface Vehicles (USVs) in complex water\nenvironments is essential for reliable navigation and operation. Specifically,\nwater surface object detection faces challenges from blurred edges and diverse\nobject scales. Although vision-radar fusion offers a feasible solution,\nexisting approaches suffer from cross-modal feature conflicts, which negatively\naffect model robustness. To address this problem, we propose a robust\nvision-radar fusion model WS-DETR. In particular, we first introduce a\nMulti-Scale Edge Information Integration (MSEII) module to enhance edge\nperception and a Hierarchical Feature Aggregator (HiFA) to boost multi-scale\nobject detection in the encoder. Then, we adopt self-moving point\nrepresentations for continuous convolution and residual connection to\nefficiently extract irregular features under the scenarios of irregular point\ncloud data. To further mitigate cross-modal conflicts, an Adaptive Feature\nInteractive Fusion (AFIF) module is introduced to integrate visual and radar\nfeatures through geometric alignment and semantic fusion. Extensive experiments\non the WaterScenes dataset demonstrate that WS-DETR achieves state-of-the-art\n(SOTA) performance, maintaining its superiority even under adverse weather and\nlighting conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07440", "pdf": "https://arxiv.org/pdf/2504.07440", "abs": "https://arxiv.org/abs/2504.07440", "authors": ["Yixin Cao", "Jiahao Ying", "Yaoning Wang", "Xipeng Qiu", "Xuanjing Huang", "Yugang Jiang"], "title": "Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. In this paper, we analyze the core\nlimitations of traditional evaluation pipelines and propose a novel metric, the\nModel Utilization Index (MUI), which introduces mechanism interpretability\ntechniques to complement traditional performance metrics. MUI quantifies the\nextent to which a model leverages its capabilities to complete tasks. The core\nidea is that to assess an LLM's overall ability, we must evaluate not only its\ntask performance but also the effort expended to achieve the outcome. Our\nextensive experiments reveal an inverse relationship between MUI and\nperformance, from which we deduce a common trend observed in popular LLMs,\nwhich we term the Utility Law. Based on this, we derive four corollaries that\naddress key challenges, including training judgement, the issue of data\ncontamination, fairness in model comparison, and data diversity. We hope that\nour survey, novel metric, and utility law will foster mutual advancement in\nboth evaluation and mechanism interpretability. Our code can be found at\nhttps://github.com/ALEX-nlp/MUI-Eva.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07491", "pdf": "https://arxiv.org/pdf/2504.07491", "abs": "https://arxiv.org/abs/2504.07491", "authors": ["Kimi Team", "Angang Du", "Bohong Yin", "Bowei Xing", "Bowen Qu", "Bowen Wang", "Cheng Chen", "Chenlin Zhang", "Chenzhuang Du", "Chu Wei", "Congcong Wang", "Dehao Zhang", "Dikang Du", "Dongliang Wang", "Enming Yuan", "Enzhe Lu", "Fang Li", "Flood Sung", "Guangda Wei", "Guokun Lai", "Han Zhu", "Hao Ding", "Hao Hu", "Hao Yang", "Hao Zhang", "Haoning Wu", "Haotian Yao", "Haoyu Lu", "Heng Wang", "Hongcheng Gao", "Huabin Zheng", "Jiaming Li", "Jianlin Su", "Jianzhou Wang", "Jiaqi Deng", "Jiezhong Qiu", "Jin Xie", "Jinhong Wang", "Jingyuan Liu", "Junjie Yan", "Kun Ouyang", "Liang Chen", "Lin Sui", "Longhui Yu", "Mengfan Dong", "Mengnan Dong", "Nuo Xu", "Pengyu Cheng", "Qizheng Gu", "Runjie Zhou", "Shaowei Liu", "Sihan Cao", "Tao Yu", "Tianhui Song", "Tongtong Bai", "Wei Song", "Weiran He", "Weixiao Huang", "Weixin Xu", "Xiaokun Yuan", "Xingcheng Yao", "Xingzhe Wu", "Xinxing Zu", "Xinyu Zhou", "Xinyuan Wang", "Y. Charles", "Yan Zhong", "Yang Li", "Yangyang Hu", "Yanru Chen", "Yejie Wang", "Yibo Liu", "Yibo Miao", "Yidao Qin", "Yimin Chen", "Yiping Bao", "Yiqin Wang", "Yongsheng Kang", "Yuanxin Liu", "Yulun Du", "Yuxin Wu", "Yuzhi Wang", "Yuzi Yan", "Zaida Zhou", "Zhaowei Li", "Zhejun Jiang", "Zheng Zhang", "Zhilin Yang", "Zhiqi Huang", "Zihao Huang", "Zijia Zhao", "Ziwei Chen"], "title": "Kimi-VL Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE)\nvision-language model (VLM) that offers advanced multimodal reasoning,\nlong-context understanding, and strong agent capabilities - all while\nactivating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL\ndemonstrates strong performance across challenging domains: as a\ngeneral-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld),\nmatching flagship models. Furthermore, it exhibits remarkable capabilities\nacross diverse challenging vision language tasks, including college-level image\nand video comprehension, OCR, mathematical reasoning, and multi-image\nunderstanding. In comparative evaluations, it effectively competes with\ncutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and\nGemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also\nadvances in processing long contexts and perceiving clearly. With a 128K\nextended context window, Kimi-VL can process diverse long inputs, achieving\nimpressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its\nnative-resolution vision encoder, MoonViT, further allows it to see and\nunderstand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and\n34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common\ntasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant:\nKimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised\nfine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong\nlong-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8\non MathVision, and 71.3 on MathVista while maintaining the compact 2.8B\nactivated LLM parameters, setting a new standard for efficient multimodal\nthinking models. Code and models are publicly accessible at\nhttps://github.com/MoonshotAI/Kimi-VL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07527", "pdf": "https://arxiv.org/pdf/2504.07527", "abs": "https://arxiv.org/abs/2504.07527", "authors": ["Junjie Zhang", "Rushuai Yang", "Shunyu Liu", "Ting-En Lin", "Fei Huang", "Yi Chen", "Yongbin Li", "Dacheng Tao"], "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07532", "pdf": "https://arxiv.org/pdf/2504.07532", "abs": "https://arxiv.org/abs/2504.07532", "authors": ["Tuhin Chakrabarty", "Philippe Laban", "Chien-Sheng Wu"], "title": "AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Submission", "summary": "AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that\ncompetitive baselines, including state-of-the-art LLMs that excel at reasoning\ntasks, barely outperform random baselines on WQ. We then train specialized\nWriting Quality Reward Models (WQRM) of various sizes for writing quality\nassessment that demonstrate strong generalization on four out-of-distribution\ntest sets and 74% accuracy on the WQ benchmark. To further show WQRM's\npractical benefits during inference, we leverage additional test-time compute\nto generate and rank multiple candidate revisions, allowing us to select\nhigher-quality outputs from an initial draft. Human evaluation with 9\nexperienced writers confirm that WQRM-based selection produces writing samples\npreferred by experts 66% overall, and 72.2% when the reward gap is larger than\n1 point. We release our datasets and models to encourage community engagement\nwith writing quality assessment and development of AI writing systems better\naligned with human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07583", "pdf": "https://arxiv.org/pdf/2504.07583", "abs": "https://arxiv.org/abs/2504.07583", "authors": ["Patrick Fernandes", "Sweta Agrawal", "Emmanouil Zaranis", "André F. T. Martins", "Graham Neubig"], "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07556", "pdf": "https://arxiv.org/pdf/2504.07556", "abs": "https://arxiv.org/abs/2504.07556", "authors": ["Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao"], "title": "TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "While text-to-image (T2I) generation models have achieved remarkable progress\nin recent years, existing evaluation methodologies for vision-language\nalignment still struggle with the fine-grained semantic matching. Current\napproaches based on global similarity metrics often overlook critical\ntoken-level correspondences between textual descriptions and visual content. To\nthis end, we present TokenFocus-VQA, a novel evaluation framework that\nleverages Large Vision-Language Models (LVLMs) through visual question\nanswering (VQA) paradigm with position-specific probability optimization. Our\nkey innovation lies in designing a token-aware loss function that selectively\nfocuses on probability distributions at pre-defined vocabulary positions\ncorresponding to crucial semantic elements, enabling precise measurement of\nfine-grained semantical alignment. The proposed framework further integrates\nensemble learning techniques to aggregate multi-perspective assessments from\ndiverse LVLMs architectures, thereby achieving further performance enhancement.\nEvaluated on the NTIRE 2025 T2I Quality Assessment Challenge Track 1, our\nTokenFocus-VQA ranks 2nd place (0.8445, only 0.0001 lower than the 1st method)\non public evaluation and 2nd place (0.8426) on the official private test set,\ndemonstrating superiority in capturing nuanced text-image correspondences\ncompared to conventional evaluation methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07661", "pdf": "https://arxiv.org/pdf/2504.07661", "abs": "https://arxiv.org/abs/2504.07661", "authors": ["Xiaowu Zhang", "Hongfei Zhao", "Jingyi Hou", "Zhijie Liu"], "title": "Unveiling the Impact of Multimodal Features on Chinese Spelling Correction: From Analysis to Design", "categories": ["cs.CL"], "comment": null, "summary": "The Chinese Spelling Correction (CSC) task focuses on detecting and\ncorrecting spelling errors in sentences. Current research primarily explores\ntwo approaches: traditional multimodal pre-trained models and large language\nmodels (LLMs). However, LLMs face limitations in CSC, particularly\nover-correction, making them suboptimal for this task. While existing studies\nhave investigated the use of phonetic and graphemic information in multimodal\nCSC models, effectively leveraging these features to enhance correction\nperformance remains a challenge. To address this, we propose the Multimodal\nAnalysis for Character Usage (\\textbf{MACU}) experiment, identifying potential\nimprovements for multimodal correctison. Based on empirical findings, we\nintroduce \\textbf{NamBert}, a novel multimodal model for Chinese spelling\ncorrection. Experiments on benchmark datasets demonstrate NamBert's superiority\nover SOTA methods. We also conduct a comprehensive comparison between NamBert\nand LLMs, systematically evaluating their strengths and limitations in CSC. Our\ncode and model are available at https://github.com/iioSnail/NamBert.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07615", "pdf": "https://arxiv.org/pdf/2504.07615", "abs": "https://arxiv.org/abs/2504.07615", "authors": ["Haozhan Shen", "Peng Liu", "Jingcheng Li", "Chunxin Fang", "Yibo Ma", "Jiajia Liao", "Qiaoli Shen", "Zilun Zhang", "Kangjia Zhao", "Qianqian Zhang", "Ruochen Xu", "Tiancheng Zhao"], "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages", "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07685", "pdf": "https://arxiv.org/pdf/2504.07685", "abs": "https://arxiv.org/abs/2504.07685", "authors": ["Silvio Picinini", "Sheila Castilho"], "title": "Context-Aware Monolingual Human Evaluation of Machine Translation", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "This paper explores the potential of context-aware monolingual human\nevaluation for assessing machine translation (MT) when no source is given for\nreference. To this end, we compare monolingual with bilingual evaluations (with\nsource text), under two scenarios: the evaluation of a single MT system, and\nthe comparative evaluation of pairwise MT systems. Four professional\ntranslators performed both monolingual and bilingual evaluations by assigning\nratings and annotating errors, and providing feedback on their experience. Our\nfindings suggest that context-aware monolingual human evaluation achieves\ncomparable outcomes to human bilingual evaluations, and suggest the feasibility\nand potential of monolingual evaluation as an efficient approach to assessing\nMT.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07794", "pdf": "https://arxiv.org/pdf/2504.07794", "abs": "https://arxiv.org/abs/2504.07794", "authors": ["Alireza Salemi", "Chris Samarinas", "Hamed Zamani"], "title": "Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper studies the limitations of (retrieval-augmented) large language\nmodels (LLMs) in generating diverse and comprehensive responses, and introduces\nthe Plan-and-Refine (P&R) framework based on a two phase system design. In the\nglobal exploration phase, P&R generates a diverse set of plans for the given\ninput, where each plan consists of a list of diverse query aspects with\ncorresponding additional descriptions. This phase is followed by a local\nexploitation phase that generates a response proposal for the input query\nconditioned on each plan and iteratively refines the proposal for improving the\nproposal quality. Finally, a reward model is employed to select the proposal\nwith the highest factuality and coverage. We conduct our experiments based on\nthe ICAT evaluation methodology--a recent approach for answer factuality and\ncomprehensiveness evaluation. Experiments on the two diverse information\nseeking benchmarks adopted from non-factoid question answering and TREC search\nresult diversification tasks demonstrate that P&R significantly outperforms\nbaselines, achieving up to a 13.1% improvement on the ANTIQUE dataset and a\n15.41% improvement on the TREC dataset. Furthermore, a smaller scale user study\nconfirms the substantial efficacy of the P&R framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "factuality", "question answering"], "score": 4}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07803", "pdf": "https://arxiv.org/pdf/2504.07803", "abs": "https://arxiv.org/abs/2504.07803", "authors": ["Mattia Rengo", "Senad Beadini", "Domenico Alfano", "Roberto Abbruzzese"], "title": "A System for Comprehensive Assessment of RAG Frameworks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report, 7 pages, 2 figures, 1 table", "summary": "Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for\nenhancing the factual accuracy and contextual relevance of Large Language\nModels (LLMs) by integrating retrieval mechanisms. However, existing evaluation\nframeworks fail to provide a holistic black-box approach to assessing RAG\nsystems, especially in real-world deployment scenarios. To address this gap, we\nintroduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a\nmodular and flexible evaluation framework designed to benchmark deployed RAG\napplications systematically. SCARF provides an end-to-end, black-box evaluation\nmethodology, enabling a limited-effort comparison across diverse RAG\nframeworks. Our framework supports multiple deployment configurations and\nfacilitates automated testing across vector databases and LLM serving\nstrategies, producing a detailed performance report. Moreover, SCARF integrates\npractical considerations such as response coherence, providing a scalable and\nadaptable solution for researchers and industry professionals evaluating RAG\napplications. Using the REST APIs interface, we demonstrate how SCARF can be\napplied to real-world scenarios, showcasing its flexibility in assessing\ndifferent RAG frameworks and configurations. SCARF is available at GitHub\nrepository.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07853", "pdf": "https://arxiv.org/pdf/2504.07853", "abs": "https://arxiv.org/abs/2504.07853", "authors": ["Jiayin Zhao", "Zhenqi Fu", "Tao Yu", "Hui Qiao"], "title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light-Field Microscopy", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Light field microscopy (LFM) has gained significant attention due to its\nability to capture snapshot-based, large-scale 3D fluorescence images. However,\nexisting LFM reconstruction algorithms are highly sensitive to sensor noise or\nrequire hard-to-get ground-truth annotated data for training. To address these\nchallenges, this paper introduces V2V3D, an unsupervised view2view-based\nframework that establishes a new paradigm for joint optimization of image\ndenoising and 3D reconstruction in a unified architecture. We assume that the\nLF images are derived from a consistent 3D signal, with the noise in each view\nbeing independent. This enables V2V3D to incorporate the principle of\nnoise2noise for effective denoising. To enhance the recovery of high-frequency\ndetails, we propose a novel wave-optics-based feature alignment technique,\nwhich transforms the point spread function, used for forward propagation in\nwave optics, into convolution kernels specifically designed for feature\nalignment. Moreover, we introduce an LFM dataset containing LF images and their\ncorresponding 3D intensity volumes. Extensive experiments demonstrate that our\napproach achieves high computational efficiency and outperforms the other\nstate-of-the-art methods. These advancements position V2V3D as a promising\nsolution for 3D imaging under challenging conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07901", "pdf": "https://arxiv.org/pdf/2504.07901", "abs": "https://arxiv.org/abs/2504.07901", "authors": ["Hongcheng Guo", "Fei Zhao", "Shaosheng Cao", "Xinze Lyu", "Ziyan Liu", "Yue Wang", "Boyang Wang", "Zhoujun Li", "Chonggang Lu", "Zhe Xu", "Yao Hu"], "title": "Redefining Machine Translation on Social Network Services with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The globalization of social interactions has heightened the need for machine\ntranslation (MT) on Social Network Services (SNS), yet traditional models\nstruggle with culturally nuanced content like memes, slang, and pop culture\nreferences. While large language models (LLMs) have advanced general-purpose\ntranslation, their performance on SNS-specific content remains limited due to\ninsufficient specialized training data and evaluation benchmarks. This paper\nintroduces RedTrans, a 72B LLM tailored for SNS translation, trained on a novel\ndataset developed through three innovations: (1) Supervised Finetuning with\nDual-LLM Back-Translation Sampling, an unsupervised sampling method using\nLLM-based back-translation to select diverse data for large-scale finetuning;\n(2) Rewritten Preference Optimization (RePO), an algorithm that identifies and\ncorrects erroneous preference pairs through expert annotation, building\nreliable preference corpora; and (3) RedTrans-Bench, the first benchmark for\nSNS translation, evaluating phenomena like humor localization, emoji semantics,\nand meme adaptation. Experiments show RedTrans outperforms state-of-the-art\nLLMs. Besides, RedTrans has already been deployed in a real-world production\nenvironment, demonstrating that domain-specific adaptation, effectively bridges\nthe gap between generic and culturally grounded translation systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "annotation"], "score": 4}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07940", "pdf": "https://arxiv.org/pdf/2504.07940", "abs": "https://arxiv.org/abs/2504.07940", "authors": ["Rundong Luo", "Matthew Wallingford", "Ali Farhadi", "Noah Snavely", "Wei-Chiu Ma"], "title": "Beyond the Frame: Generating 360° Panoramic Videos from Perspective Videos", "categories": ["cs.CV"], "comment": "Project page: https://red-fairy.github.io/argus/", "summary": "360{\\deg} videos have emerged as a promising medium to represent our dynamic\nvisual world. Compared to the \"tunnel vision\" of standard cameras, their\nborderless field of view offers a more complete perspective of our\nsurroundings. While existing video models excel at producing standard videos,\ntheir ability to generate full panoramic videos remains elusive. In this paper,\nwe investigate the task of video-to-360{\\deg} generation: given a perspective\nvideo as input, our goal is to generate a full panoramic video that is\nconsistent with the original video. Unlike conventional video generation tasks,\nthe output's field of view is significantly larger, and the model is required\nto have a deep understanding of both the spatial layout of the scene and the\ndynamics of objects to maintain spatio-temporal consistency. To address these\nchallenges, we first leverage the abundant 360{\\deg} videos available online\nand develop a high-quality data filtering pipeline to curate pairwise training\ndata. We then carefully design a series of geometry- and motion-aware\noperations to facilitate the learning process and improve the quality of\n360{\\deg} video generation. Experimental results demonstrate that our model can\ngenerate realistic and coherent 360{\\deg} videos from in-the-wild perspective\nvideo. In addition, we showcase its potential applications, including video\nstabilization, camera viewpoint control, and interactive visual question\nanswering.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07954", "pdf": "https://arxiv.org/pdf/2504.07954", "abs": "https://arxiv.org/abs/2504.07954", "authors": ["En Yu", "Kangheng Lin", "Liang Zhao", "Jisheng Yin", "Yana Wei", "Yuang Peng", "Haoran Wei", "Jianjian Sun", "Chunrui Han", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang", "Jingyu Wang", "Wenbing Tao"], "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Github page: https://github.com/linkangheng/PR1", "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in MLLM post-training for perception\npolicy learning. While promising, our initial experiments reveal that\nincorporating a thinking process through RL does not consistently lead to\nperformance gains across all visual perception tasks. This leads us to delve\ninto the essential role of RL in the context of visual perception. In this\nwork, we return to the fundamentals and explore the effects of RL on different\nperception tasks. We observe that the perceptual complexity is a major factor\nin determining the effectiveness of RL. We also observe that reward design\nplays a crucial role in further approching the upper limit of model perception.\nTo leverage these findings, we propose Perception-R1, a scalable RL framework\nusing GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,\nPerception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on\nPageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing\na strong baseline for perception policy learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07416", "pdf": "https://arxiv.org/pdf/2504.07416", "abs": "https://arxiv.org/abs/2504.07416", "authors": ["Jonggwon Park", "Soobum Kim", "Byungmu Yoon", "Kyoyun Choi"], "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language alignment in radiology. However, existing approaches struggle\nto effectively utilize complex radiology reports for learning, rely on\nlow-resolution images, and offer limited interpretability in attention\nmechanisms. To address these challenges, we introduce RadZero, a novel\nsimilarity-based cross-attention framework for vision-language alignment in\nradiology with zero-shot multi-task capability. RadZero leverages large\nlanguage models to extract minimal semantic sentences from radiology reports\nand employs a multi-positive contrastive learning strategy to effectively\ncapture relationships between images and multiple relevant textual\ndescriptions. It also utilizes a pre-trained vision encoder with additional\ntrainable Transformer layers, allowing efficient high-resolution image\nprocessing. By computing similarity between text embeddings and local image\npatch features, RadZero enables zero-shot inference with similarity probability\nfor classification and pixel-level cross-modal similarity maps for grounding\nand segmentation. Experimental results on public chest radiograph benchmarks\nshow that RadZero outperforms state-of-the-art methods in zero-shot\nclassification, grounding, and segmentation. Furthermore, cross-modal\nsimilarity map analysis highlights its potential for improving explainability\nin vision-language alignment. Additionally, qualitative evaluation demonstrates\nRadZero's capability for open-vocabulary semantic segmentation, further\nvalidating its effectiveness in medical imaging.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07439", "pdf": "https://arxiv.org/pdf/2504.07439", "abs": "https://arxiv.org/abs/2504.07439", "authors": ["Qi Liu", "Haozhe Duan", "Yiqun Chen", "Quanfeng Lu", "Weiwei Sun", "Jiaxin Mao"], "title": "LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Utilizing large language models (LLMs) for document reranking has been a\npopular and promising research direction in recent years, many studies are\ndedicated to improving the performance and efficiency of using LLMs for\nreranking. Besides, it can also be applied in many real-world applications,\nsuch as search engines or retrieval-augmented generation. In response to the\ngrowing demand for research and application in practice, we introduce a unified\nframework, \\textbf{LLM4Ranking}, which enables users to adopt different ranking\nmethods using open-source or closed-source API-based LLMs. Our framework\nprovides a simple and extensible interface for document reranking with LLMs, as\nwell as easy-to-use evaluation and fine-tuning scripts for this task. We\nconducted experiments based on this framework and evaluated various models and\nmethods on several widely used datasets, providing reproducibility results on\nutilizing LLMs for document reranking. Our code is publicly available at\nhttps://github.com/liuqi6777/llm4ranking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07448", "pdf": "https://arxiv.org/pdf/2504.07448", "abs": "https://arxiv.org/abs/2504.07448", "authors": ["Juzheng Zhang", "Jiacheng You", "Ashwinee Panda", "Tom Goldstein"], "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "24 pages, 7 figures, 20 tables", "summary": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient\nfine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs\nnotable overhead and suffers from parameter interference in multi-task\nscenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet\neffective approach that freezes the projection matrices $A$ as random\nprojections and sparsifies the matrices $B$ using task-specific masks. This\ndesign substantially reduces the number of trainable parameters while\nmaintaining strong task performance. Moreover, LoRI minimizes cross-task\ninterference in adapter merging by leveraging the orthogonality between adapter\nsubspaces, and supports continual learning by using sparsity to mitigate\ncatastrophic forgetting. Extensive experiments across natural language\nunderstanding, mathematical reasoning, code generation, and safety alignment\ntasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT\nmethods, while using up to 95% fewer trainable parameters than LoRA. In\nmulti-task experiments, LoRI enables effective adapter merging and continual\nlearning with reduced cross-task interference. Code is available at:\nhttps://github.com/juzhengz/LoRI", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "code generation", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07615", "pdf": "https://arxiv.org/pdf/2504.07615", "abs": "https://arxiv.org/abs/2504.07615", "authors": ["Haozhan Shen", "Peng Liu", "Jingcheng Li", "Chunxin Fang", "Yibo Ma", "Jiajia Liao", "Qiaoli Shen", "Zilun Zhang", "Kangjia Zhao", "Qianqian Zhang", "Ruochen Xu", "Tiancheng Zhao"], "title": "VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages", "summary": "Recently DeepSeek R1 has shown that reinforcement learning (RL) can\nsubstantially improve the reasoning capabilities of Large Language Models\n(LLMs) through a simple yet effective design. The core of R1 lies in its\nrule-based reward formulation, which leverages tasks with deterministic\nground-truth answers to enable precise and stable reward computation. In the\nvisual domain, we similarly observe that a wide range of visual understanding\ntasks are inherently equipped with well-defined ground-truth annotations. This\nproperty makes them naturally compatible with rule-based reward mechanisms.\nMotivated by this observation, we investigate the extension of R1-style\nreinforcement learning to Vision-Language Models (VLMs), aiming to enhance\ntheir visual reasoning capabilities. To this end, we develop VLM-R1, a\ndedicated framework designed to harness RL for improving VLMs' performance on\ngeneral vision-language tasks. Using this framework, we further explore the\nfeasibility of applying RL to visual domain. Experimental results indicate that\nthe RL-based model not only delivers competitive performance on visual\nunderstanding tasks but also surpasses Supervised Fine-Tuning (SFT) in\ngeneralization ability. Furthermore, we conduct comprehensive ablation studies\nthat uncover a series of noteworthy insights, including the presence of reward\nhacking in object detection, the emergence of the \"OD aha moment\", the impact\nof training data quality, and the scaling behavior of RL across different model\nsizes. Through these analyses, we aim to deepen the understanding of how\nreinforcement learning enhances the capabilities of vision-language models, and\nwe hope our findings and open-source contributions will support continued\nprogress in the vision-language RL community. Our code and model are available\nat https://github.com/om-ai-lab/VLM-R1", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07961", "pdf": "https://arxiv.org/pdf/2504.07961", "abs": "https://arxiv.org/abs/2504.07961", "authors": ["Zeren Jiang", "Chuanxia Zheng", "Iro Laina", "Diane Larlus", "Andrea Vedaldi"], "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction", "categories": ["cs.CV", "I.4.5"], "comment": "16 pages, 5 figures, Project page: https://geo4d.github.io/", "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\nprior captured by such video models, Geo4D can be trained using only synthetic\ndata while generalizing well to real data in a zero-shot manner. Geo4D predicts\nseveral complementary geometric modalities, namely point, depth, and ray maps.\nIt uses a new multi-modal alignment algorithm to align and fuse these\nmodalities, as well as multiple sliding windows, at inference time, thus\nobtaining robust and accurate 4D reconstruction of long videos. Extensive\nexperiments across multiple benchmarks show that Geo4D significantly surpasses\nstate-of-the-art video depth estimation methods, including recent methods such\nas MonST3R, which are also designed to handle dynamic scenes.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07952", "pdf": "https://arxiv.org/pdf/2504.07952", "abs": "https://arxiv.org/abs/2504.07952", "authors": ["Mirac Suzgun", "Mert Yuksekgonul", "Federico Bianchi", "Dan Jurafsky", "James Zou"], "title": "Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory", "categories": ["cs.LG", "cs.CL"], "comment": "https://github.com/suzgunmirac/dynamic-cheatsheet", "summary": "Despite their impressive performance on complex tasks, current language\nmodels (LMs) typically operate in a vacuum: Each input query is processed\nseparately, without retaining insights from previous attempts. Here, we present\nDynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM\nwith a persistent, evolving memory. Rather than repeatedly re-discovering or\nre-committing the same solutions and mistakes, DC enables models to store and\nreuse accumulated strategies, code snippets, and general problem-solving\ninsights at inference time. This test-time learning enhances performance\nsubstantially across a range of tasks without needing explicit ground-truth\nlabels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than\ndoubled on AIME math exams once it began retaining algebraic insights across\nquestions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to\n99% after the model discovered and reused a Python-based solution. In tasks\nprone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o\nand Claude to reach near-perfect accuracy by recalling previously validated\ncode, whereas their baselines stagnated around 50%. Beyond arithmetic\nchallenges, DC yields notable accuracy gains on knowledge-demanding tasks.\nClaude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro\nproblems. Crucially, DC's memory is self-curated, focusing on concise,\ntransferable snippets rather than entire transcript. Unlike finetuning or\nstatic retrieval methods, DC adapts LMs' problem-solving skills on the fly,\nwithout modifying their underlying parameters. Overall, our findings present DC\nas a promising approach for augmenting LMs with persistent memory, bridging the\ndivide between isolated inference events and the cumulative, experience-driven\nlearning characteristic of human cognition.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07954", "pdf": "https://arxiv.org/pdf/2504.07954", "abs": "https://arxiv.org/abs/2504.07954", "authors": ["En Yu", "Kangheng Lin", "Liang Zhao", "Jisheng Yin", "Yana Wei", "Yuang Peng", "Haoran Wei", "Jianjian Sun", "Chunrui Han", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang", "Jingyu Wang", "Wenbing Tao"], "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Github page: https://github.com/linkangheng/PR1", "summary": "Inspired by the success of DeepSeek-R1, we explore the potential of\nrule-based reinforcement learning (RL) in MLLM post-training for perception\npolicy learning. While promising, our initial experiments reveal that\nincorporating a thinking process through RL does not consistently lead to\nperformance gains across all visual perception tasks. This leads us to delve\ninto the essential role of RL in the context of visual perception. In this\nwork, we return to the fundamentals and explore the effects of RL on different\nperception tasks. We observe that the perceptual complexity is a major factor\nin determining the effectiveness of RL. We also observe that reward design\nplays a crucial role in further approching the upper limit of model perception.\nTo leverage these findings, we propose Perception-R1, a scalable RL framework\nusing GRPO during MLLM post-training. With a standard Qwen2.5-VL-3B-Instruct,\nPerception-R1 achieves +4.2% on RefCOCO+, +17.9% on PixMo-Count, +4.2% on\nPageOCR, and notably, 31.9% AP on COCO2017 val for the first time, establishing\na strong baseline for perception policy learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07594", "pdf": "https://arxiv.org/pdf/2504.07594", "abs": "https://arxiv.org/abs/2504.07594", "authors": ["Xiaohao Liu", "Teng Tu", "Yunshan Ma", "Tat-Seng Chua"], "title": "Extending Visual Dynamics for Video-to-Music Generation", "categories": ["cs.MM", "cs.CV"], "comment": "Under review", "summary": "Music profoundly enhances video production by improving quality, engagement,\nand emotional resonance, sparking growing interest in video-to-music\ngeneration. Despite recent advances, existing approaches remain limited in\nspecific scenarios or undervalue the visual dynamics. To address these\nlimitations, we focus on tackling the complexity of dynamics and resolving\ntemporal misalignment between video and music representations. To this end, we\npropose DyViM, a novel framework to enhance dynamics modeling for\nvideo-to-music generation. Specifically, we extract frame-wise dynamics\nfeatures via a simplified motion encoder inherited from optical flow methods,\nfollowed by a self-attention module for aggregation within frames. These\ndynamic features are then incorporated to extend existing music tokens for\ntemporal alignment. Additionally, high-level semantics are conveyed through a\ncross-attention mechanism, and an annealing tuning strategy benefits to\nfine-tune well-trained music decoders efficiently, therefore facilitating\nseamless adaptation. Extensive experiments demonstrate DyViM's superiority over\nstate-of-the-art (SOTA) methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-12.jsonl"}
{"id": "2504.07677", "pdf": "https://arxiv.org/pdf/2504.07677", "abs": "https://arxiv.org/abs/2504.07677", "authors": ["Hye-Min Won", "Jieun Lee", "Jiyong Oh"], "title": "Localization Meets Uncertainty: Uncertainty-Aware Multi-Modal Localization", "categories": ["cs.RO", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Reliable localization is critical for robot navigation in complex indoor\nenvironments. In this paper, we propose an uncertainty-aware localization\nmethod that enhances the reliability of localization outputs without modifying\nthe prediction model itself. This study introduces a percentile-based rejection\nstrategy that filters out unreliable 3-DoF pose predictions based on aleatoric\nand epistemic uncertainties the network estimates. We apply this approach to a\nmulti-modal end-to-end localization that fuses RGB images and 2D LiDAR data,\nand we evaluate it across three real-world datasets collected using a\ncommercialized serving robot. Experimental results show that applying stricter\nuncertainty thresholds consistently improves pose accuracy. Specifically, the\nmean position error is reduced by 41.0%, 56.7%, and 69.4%, and the mean\norientation error by 55.6%, 65.7%, and 73.3%, when applying 90%, 80%, and 70%\nthresholds, respectively. Furthermore, the rejection strategy effectively\nremoves extreme outliers, resulting in better alignment with ground truth\ntrajectories. To the best of our knowledge, this is the first study to\nquantitatively demonstrate the benefits of percentile-based uncertainty\nrejection in multi-modal end-to-end localization tasks. Our approach provides a\npractical means to enhance the reliability and accuracy of localization systems\nin real-world deployments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-12.jsonl"}
