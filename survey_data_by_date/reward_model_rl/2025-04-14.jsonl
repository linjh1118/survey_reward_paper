{"id": "2504.08590", "pdf": "https://arxiv.org/pdf/2504.08590", "abs": "https://arxiv.org/abs/2504.08590", "authors": ["Nicola Horst", "Davide Mazzaccara", "Antonia Schmidt", "Michael Sullivan", "Filippo Momentè", "Luca Franceschetti", "Philipp Sadler", "Sherzod Hakimov", "Alberto Testoni", "Raffaella Bernardi", "Raquel Fernández", "Alexander Koller", "Oliver Lemon", "David Schlangen", "Mario Giulianelli", "Alessandro Suglia"], "title": "Playpen: An Environment for Exploring Learning Through Conversational Interaction", "categories": ["cs.CL"], "comment": "Source code: https://github.com/lm-playpen/playpen Please send\n  correspodence to: lm-playschool@googlegroups.com", "summary": "Are we running out of learning signal? Predicting the next word in an\nexisting text has turned out to be a powerful signal, at least at scale. But\nthere are signs that we are running out of this resource. In recent months,\ninteraction between learner and feedback-giver has come into focus, both for\n\"alignment\" (with a reward model judging the quality of instruction following\nattempts) and for improving \"reasoning\" (process- and outcome-based verifiers\njudging reasoning steps). In this paper, we explore to what extent synthetic\ninteraction in what we call Dialogue Games -- goal-directed and rule-governed\nactivities driven predominantly by verbal actions -- can provide a learning\nsignal, and how this signal can be used. We introduce an environment for\nproducing such interaction data (with the help of a Large Language Model as\ncounterpart to the learner model), both offline and online. We investigate the\neffects of supervised fine-tuning on this data, as well as reinforcement\nlearning setups such as DPO, and GRPO; showing that all of these approaches\nachieve some improvements in in-domain games, but only GRPO demonstrates the\nability to generalise to out-of-domain games as well as retain competitive\nperformance in reference-based tasks. We release the framework and the baseline\ntraining setups in the hope that this can foster research in this promising new\ndirection.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08542", "pdf": "https://arxiv.org/pdf/2504.08542", "abs": "https://arxiv.org/abs/2504.08542", "authors": ["Haoran Cheng", "Qide Dong", "Liang Peng", "Zhizhou Sha", "Weiguo Feng", "Jinghui Xie", "Zhao Song", "Shilei Wen", "Xiaofei He", "Boxi Wu"], "title": "Discriminator-Free Direct Preference Optimization for Video Diffusion", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2412.14167 by other authors", "summary": "Direct Preference Optimization (DPO), which aligns models with human\npreferences through win/lose data pairs, has achieved remarkable success in\nlanguage and image generation. However, applying DPO to video diffusion models\nfaces critical challenges: (1) Data inefficiency. Generating thousands of\nvideos per DPO iteration incurs prohibitive costs; (2) Evaluation uncertainty.\nHuman annotations suffer from subjective bias, and automated discriminators\nfail to detect subtle temporal artifacts like flickering or motion incoherence.\nTo address these, we propose a discriminator-free video DPO framework that: (1)\nUses original real videos as win cases and their edited versions (e.g.,\nreversed, shuffled, or noise-corrupted clips) as lose cases; (2) Trains video\ndiffusion models to distinguish and avoid artifacts introduced by editing. This\napproach eliminates the need for costly synthetic video comparisons, provides\nunambiguous quality signals, and enables unlimited training data expansion\nthrough simple editing operations. We theoretically prove the framework's\neffectiveness even when real videos and model-generated videos follow different\ndistributions. Experiments on CogVideoX demonstrate the efficiency of the\nproposed method.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08110", "pdf": "https://arxiv.org/pdf/2504.08110", "abs": "https://arxiv.org/abs/2504.08110", "authors": ["Muhammad Saif Ullah Khan", "Stephan Krauß", "Didier Stricker"], "title": "Towards Unconstrained 2D Pose Estimation of the Human Spine", "categories": ["cs.CV"], "comment": "Accepted for publication in CVPRW 2025", "summary": "We present SpineTrack, the first comprehensive dataset for 2D spine pose\nestimation in unconstrained settings, addressing a crucial need in sports\nanalytics, healthcare, and realistic animation. Existing pose datasets often\nsimplify the spine to a single rigid segment, overlooking the nuanced\narticulation required for accurate motion analysis. In contrast, SpineTrack\nannotates nine detailed spinal keypoints across two complementary subsets: a\nsynthetic set comprising 25k annotations created using Unreal Engine with\nbiomechanical alignment through OpenSim, and a real-world set comprising over\n33k annotations curated via an active learning pipeline that iteratively\nrefines automated annotations with human feedback. This integrated approach\nensures anatomically consistent labels at scale, even for challenging,\nin-the-wild images. We further introduce SpinePose, extending state-of-the-art\nbody pose estimators using knowledge distillation and an anatomical\nregularization strategy to jointly predict body and spine keypoints. Our\nexperiments in both general and sports-specific contexts validate the\neffectiveness of SpineTrack for precise spine pose estimation, establishing a\nrobust foundation for future research in advanced biomechanical analysis and 3D\nspine reconstruction in the wild.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08291", "pdf": "https://arxiv.org/pdf/2504.08291", "abs": "https://arxiv.org/abs/2504.08291", "authors": ["Junjia Huang", "Pengxiang Yan", "Jiyang Liu", "Jie Wu", "Zhao Wang", "Yitong Wang", "Liang Lin", "Guanbin Li"], "title": "DreamFuse: Adaptive Image Fusion with Diffusion Transformer", "categories": ["cs.CV"], "comment": "under review", "summary": "Image fusion seeks to seamlessly integrate foreground objects with background\nscenes, producing realistic and harmonious fused images. Unlike existing\nmethods that directly insert objects into the background, adaptive and\ninteractive fusion remains a challenging yet appealing task. It requires the\nforeground to adjust or interact with the background context, enabling more\ncoherent integration. To address this, we propose an iterative\nhuman-in-the-loop data generation pipeline, which leverages limited initial\ndata with diverse textual prompts to generate fusion datasets across various\nscenarios and interactions, including placement, holding, wearing, and style\ntransfer. Building on this, we introduce DreamFuse, a novel approach based on\nthe Diffusion Transformer (DiT) model, to generate consistent and harmonious\nfused images with both foreground and background information. DreamFuse employs\na Positional Affine mechanism to inject the size and position of the foreground\ninto the background, enabling effective foreground-background interaction\nthrough shared attention. Furthermore, we apply Localized Direct Preference\nOptimization guided by human feedback to refine DreamFuse, enhancing background\nconsistency and foreground harmony. DreamFuse achieves harmonious fusion while\ngeneralizing to text-driven attribute editing of the fused results.\nExperimental results demonstrate that our method outperforms state-of-the-art\napproaches across multiple metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08609", "pdf": "https://arxiv.org/pdf/2504.08609", "abs": "https://arxiv.org/abs/2504.08609", "authors": ["Julian Bäumler", "Louis Blöcher", "Lars-Joel Frey", "Xian Chen", "Markus Bayer", "Christian Reuter"], "title": "A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 4 figures, 4 tables", "summary": "The dissemination of online hate speech can have serious negative\nconsequences for individuals, online communities, and entire societies. This\nand the large volume of hateful online content prompted both practitioners',\ni.e., in content moderation or law enforcement, and researchers' interest in\nmachine learning models to automatically classify instances of hate speech.\nWhereas most scientific works address hate speech classification as a binary\ntask, practice often requires a differentiation into sub-types, e.g., according\nto target, severity, or legality, which may overlap for individual content.\nHence, researchers created datasets and machine learning models that approach\nhate speech classification in textual data as a multi-label problem. This work\npresents the first systematic and comprehensive survey of scientific literature\non this emerging research landscape in English (N=46). We contribute with a\nconcise overview of 28 datasets suited for training multi-label classification\nmodels that reveals significant heterogeneity regarding label-set, size,\nmeta-concept, annotation process, and inter-annotator agreement. Our analysis\nof 24 publications proposing suitable classification models further establishes\ninconsistency in evaluation and a preference for architectures based on\nBidirectional Encoder Representation from Transformers (BERT) and Recurrent\nNeural Networks (RNNs). We identify imbalanced training data, reliance on\ncrowdsourcing platforms, small and sparse datasets, and missing methodological\nalignment as critical open issues and formulate ten recommendations for\nresearch.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "annotation", "agreement", "inter-annotator agreement"], "score": 4}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08358", "pdf": "https://arxiv.org/pdf/2504.08358", "abs": "https://arxiv.org/abs/2504.08358", "authors": ["Jiarui Wang", "Huiyu Duan", "Yu Zhao", "Juntong Wang", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs) have significantly\nadvanced both text-to-image (T2I) generation and image-to-text (I2T)\ninterpretation. However, many generated images still suffer from issues related\nto perceptual quality and text-image alignment. Given the high cost and\ninefficiency of manual evaluation, an automatic metric that aligns with human\npreferences is desirable. To this end, we present EvalMi-50K, a comprehensive\ndataset and benchmark for evaluating large-multimodal image generation, which\nfeatures (i) comprehensive tasks, encompassing 2,100 extensive prompts across\n20 fine-grained task dimensions, and (ii) large-scale human-preference\nannotations, including 100K mean-opinion scores (MOSs) and 50K\nquestion-answering (QA) pairs annotated on 50,400 images generated from 24 T2I\nmodels. Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for\nevaluating large multimodal T2I generation from multiple dimensions including\nperception, text-image correspondence, and task-specific accuracy. Extensive\nexperimental results show that LMM4LMM achieves state-of-the-art performance on\nEvalMi-50K, and exhibits strong generalization ability on other AI-generated\nimage evaluation benchmark datasets, manifesting the generality of both the\nEvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be\nreleased at https://github.com/IntMeGroup/LMM4LMM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08003", "pdf": "https://arxiv.org/pdf/2504.08003", "abs": "https://arxiv.org/abs/2504.08003", "authors": ["Ning Li", "Jingran Zhang", "Justin Cui"], "title": "Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability", "categories": ["cs.CV"], "comment": "Early work, technical report", "summary": "OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image\ngeneration and editing, yet its ability to achieve world knowledge-informed\nsemantic synthesis--seamlessly integrating domain knowledge, contextual\nreasoning, and instruction adherence--remains unproven. In this study, we\nsystematically evaluate these capabilities across three critical dimensions:\n(1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3)\nPost-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong\ncapabilities in image generation and editing, our evaluation reveals GPT-4o's\npersistent limitations: the model frequently defaults to literal\ninterpretations of instructions, inconsistently applies knowledge constraints,\nand struggles with conditional reasoning tasks. These findings challenge\nprevailing assumptions about GPT-4o's unified understanding and generation\ncapabilities, exposing significant gaps in its dynamic knowledge integration.\nOur study calls for the development of more robust benchmarks and training\nstrategies that go beyond surface-level alignment, emphasizing context-aware\nand reasoning-grounded multimodal generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08211", "pdf": "https://arxiv.org/pdf/2504.08211", "abs": "https://arxiv.org/abs/2504.08211", "authors": ["Leo Kampen", "Carlos Rabat Villarreal", "Louis Yu", "Santu Karmaker", "Dongji Feng"], "title": "LLM for Comparative Narrative Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures, Appendix included", "summary": "In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08281", "pdf": "https://arxiv.org/pdf/2504.08281", "abs": "https://arxiv.org/abs/2504.08281", "authors": ["Vishal Gandhi", "Sagar Gandhi"], "title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08252", "pdf": "https://arxiv.org/pdf/2504.08252", "abs": "https://arxiv.org/abs/2504.08252", "authors": ["Travis Driver", "Andrew Vaughan", "Yang Cheng", "Adnan Ansar", "John Christian", "Panagiotis Tsiotras"], "title": "Stereophotoclinometry Revisited", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.06865", "summary": "Image-based surface reconstruction and characterization is crucial for\nmissions to small celestial bodies, as it informs mission planning, navigation,\nand scientific analysis. However, current state-of-the-practice methods, such\nas stereophotoclinometry (SPC), rely heavily on human-in-the-loop verification\nand high-fidelity a priori information. This paper proposes\nPhotoclinometry-from-Motion (PhoMo), a novel framework that incorporates\nphotoclinometry techniques into a keypoint-based structure-from-motion (SfM)\nsystem to estimate the surface normal and albedo at detected landmarks to\nimprove autonomous surface and shape characterization of small celestial bodies\nfrom in-situ imagery. In contrast to SPC, we forego the expensive maplet\nestimation step and instead use dense keypoint measurements and correspondences\nfrom an autonomous keypoint detection and matching method based on deep\nlearning. Moreover, we develop a factor graph-based approach allowing for\nsimultaneous optimization of the spacecraft's pose, landmark positions,\nSun-relative direction, and surface normals and albedos via fusion of Sun\nvector measurements and image keypoint measurements. The proposed framework is\nvalidated on real imagery taken by the Dawn mission to the asteroid 4 Vesta and\nthe minor planet 1 Ceres and compared against an SPC reconstruction, where we\ndemonstrate superior rendering performance compared to an SPC solution and\nprecise alignment to a stereophotogrammetry (SPG) solution without relying on\nany a priori camera pose and topography information or humans-in-the-loop.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08269", "pdf": "https://arxiv.org/pdf/2504.08269", "abs": "https://arxiv.org/abs/2504.08269", "authors": ["Qi Zhi Lim", "Chin Poo Lee", "Kian Ming Lim", "Kalaiarasi Sonai Muthu Anbananthen"], "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08543", "pdf": "https://arxiv.org/pdf/2504.08543", "abs": "https://arxiv.org/abs/2504.08543", "authors": ["Frances Laureano De Leon", "Yixiao Wang", "Yue Feng", "Mark G. Lee"], "title": "UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual and Cross-Lingual Emotion Detection", "categories": ["cs.CL"], "comment": "Accepted to appear in Proceedings of the 19th International Workshop\n  on Semantic Evaluation (SemEval-2025)", "summary": "Emotion detection in natural language processing is a challenging task due to\nthe complexity of human emotions and linguistic diversity. While significant\nprogress has been made in high-resource languages, emotion detection in\nlow-resource languages remains underexplored. In this work, we address\nmultilingual and cross-lingual emotion detection by leveraging adapter-based\nfine-tuning with multilingual pre-trained language models. Adapters introduce a\nsmall number of trainable parameters while keeping the pre-trained model\nweights fixed, offering a parameter-efficient approach to adaptation. We\nexperiment with different adapter tuning strategies, including task-only\nadapters, target-language-ready task adapters, and language-family-based\nadapters. Our results show that target-language-ready task adapters achieve the\nbest overall performance, particularly for low-resource African languages with\nour team ranking 7th for Tigrinya, and 8th for Kinyarwanda in Track A. In Track\nC, our system ranked 3rd for Amharic, and 4th for Oromo, Tigrinya, Kinyarwanda,\nHausa, and Igbo. Our approach outperforms large language models in 11 languages\nand matches their performance in four others, despite our models having\nsignificantly fewer parameters. Furthermore, we find that adapter-based models\nretain cross-linguistic transfer capabilities while requiring fewer\ncomputational resources compared to full fine-tuning for each language.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08269", "pdf": "https://arxiv.org/pdf/2504.08269", "abs": "https://arxiv.org/abs/2504.08269", "authors": ["Qi Zhi Lim", "Chin Poo Lee", "Kian Ming Lim", "Kalaiarasi Sonai Muthu Anbananthen"], "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The increasing availability of multimodal data across text, tables, and\nimages presents new challenges for developing models capable of complex\ncross-modal reasoning. Existing methods for Multimodal Multi-hop Question\nAnswering (MMQA) often suffer from limited reasoning capabilities, reliance on\nmodality conversion, and inadequate alignment between visual and textual\nrepresentations. To address these limitations, this paper introduces\nVision-Language Multimodal Transformer (VLMT), a unified architecture that\nintegrates a transformer-based vision encoder with a sequence-to-sequence\nlanguage model. VLMT employs a direct token-level injection mechanism to fuse\nvisual and textual inputs within a shared embedding space, eliminating the need\nfor intermediate projection layers. To enhance cross-modal alignment and\nreasoning, a three-stage pretraining strategy is proposed to progressively\nalign vision-language representations and improve the model's capacity for\nmultimodal understanding. Based on the pretrained backbone, two task-specific\nmodules are instantiated to form a two-stage MMQA framework: a multimodal\nreranker that predicts document relevance scores and utilizes a relative\nthreshold with top-k strategy for context retrieval, and a multimodal question\nanswering model that generates contextually grounded answers based on the\nretrieved evidence. Comprehensive experiments on two benchmark datasets\ndemonstrate the effectiveness of the proposed approach. On MultimodalQA\nvalidation set, VLMT-Large achieves 76.5% Exact Match and 80.1% F1,\noutperforming the previous state-of-the-art by +9.1% in Exact Match and +8.8%\nin F1. On WebQA, it attains a QA score of 47.6, surpassing prior models such as\nPERQA by +3.2. These results highlight VLMT's strong capabilities in multimodal\nreasoning and its potential to advance real-world information retrieval and\nquestion answering systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08641", "pdf": "https://arxiv.org/pdf/2504.08641", "abs": "https://arxiv.org/abs/2504.08641", "authors": ["Jialu Li", "Shoubin Yu", "Han Lin", "Jaemin Cho", "Jaehong Yoon", "Mohit Bansal"], "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally", "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08645", "pdf": "https://arxiv.org/pdf/2504.08645", "abs": "https://arxiv.org/abs/2504.08645", "authors": ["Alessio Lombardi", "Li Duan", "Ahmed Elnagar", "Ahmed Zaalouk", "Khalid Ismail", "Edlira Vakaj"], "title": "Title block detection and information extraction for enhanced building drawings search", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 8 figures, 1 table. Accepted for publication in the 2025\n  European Conference on Computing in Construction (EC3,\n  https://ec-3.org/conference2025/)", "summary": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "accuracy"], "score": 3}}, "source_file": "2025-04-14.jsonl"}
{"id": "2504.08675", "pdf": "https://arxiv.org/pdf/2504.08675", "abs": "https://arxiv.org/abs/2504.08675", "authors": ["Gokce Guven", "H. Fatih Ugurdag", "Hasan F. Ates"], "title": "X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image with Hybrid Neural Implicit Methods", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D bone reconstruction from a single planar X-ray remains a\nchallenge due to anatomical complexity and limited input data. We propose X2BR,\na hybrid neural implicit framework that combines continuous volumetric\nreconstruction with template-guided non-rigid registration. The core network,\nX2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays\nand predict high-fidelity 3D bone occupancy fields without relying on\nstatistical shape models. To further refine anatomical accuracy, X2BR\nintegrates a patient-specific template mesh, constructed using YOLOv9-based\ndetection and the SKEL biomechanical skeleton model. The coarse reconstruction\nis aligned to the template using geodesic-based coherent point drift, enabling\nanatomically consistent 3D bone volumes. Experimental results on a clinical\ndataset show that X2B achieves the highest numerical accuracy, with an IoU of\n0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines\nincluding X2V and D2IM-Net. Building on this, X2BR incorporates anatomical\npriors via YOLOv9-based bone detection and biomechanical template alignment,\nleading to reconstructions that, while slightly lower in IoU (0.875), offer\nsuperior anatomical realism, especially in rib curvature and vertebral\nalignment. This numerical accuracy vs. visual consistency trade-off between X2B\nand X2BR highlights the value of hybrid frameworks for clinically relevant 3D\nreconstructions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-04-14.jsonl"}
