{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "preference", "RLAIF", "AI feedback", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10329", "pdf": "https://arxiv.org/pdf/2504.10329", "abs": "https://arxiv.org/abs/2504.10329", "authors": ["Xingyu Lu", "Yuhang Hu", "YiFan Zhang", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Jinpeng Wang", "Bin Wen", "Chun Yuan", "Fan Yang", "Tingting Gao", "Di Zhang"], "title": "InstructEngine: Instruction-driven Text-to-Image Alignment", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning", "preference", "RLAIF", "AI feedback", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09643", "pdf": "https://arxiv.org/pdf/2504.09643", "abs": "https://arxiv.org/abs/2504.09643", "authors": ["Nikita Sorokin", "Ivan Sedykh", "Valentin Malykh"], "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking", "categories": ["cs.CL", "cs.IR", "cs.SE"], "comment": "Published at ECIR 2025", "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "PPO", "proximal policy optimization", "policy optimization", "ranking", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "code generation"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09643", "pdf": "https://arxiv.org/pdf/2504.09643", "abs": "https://arxiv.org/abs/2504.09643", "authors": ["Nikita Sorokin", "Ivan Sedykh", "Valentin Malykh"], "title": "Iterative Self-Training for Code Generation via Reinforced Re-Ranking", "categories": ["cs.CL", "cs.IR", "cs.SE"], "comment": "Published at ECIR 2025", "summary": "Generating high-quality code that solves complex programming tasks is\nchallenging, especially with current decoder-based models that produce highly\nstochastic outputs. In code generation, even minor errors can easily break the\nentire solution. Leveraging multiple sampled solutions can significantly\nimprove the overall output quality.\n  One effective way to enhance code generation is by pairing a code generation\nmodel with a reranker model, which selects the best solution from the generated\nsamples. We propose a novel iterative self-training approach for self-training\nreranker models using Proximal Policy Optimization (PPO), aimed at improving\nboth reranking accuracy and the overall code generation process. Unlike\ntraditional PPO approaches, where the focus is on optimizing a generative model\nwith a reward model, our approach emphasizes the development of a robust\nreward/reranking model. This model improves the quality of generated code\nthrough reranking and addresses problems and errors that the reward model might\noverlook during PPO alignment with the reranker. Our method iteratively refines\nthe training dataset by re-evaluating outputs, identifying high-scoring\nnegative examples, and incorporating them into the training loop, that boosting\nmodel performance.\n  Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter\nmodel outperforms a 33B model in code generation quality while being three\ntimes faster. Moreover, it achieves performance comparable to GPT-4 and\nsurpasses it in one programming language.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "PPO", "proximal policy optimization", "policy optimization", "ranking", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy", "code generation"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09420", "pdf": "https://arxiv.org/pdf/2504.09420", "abs": "https://arxiv.org/abs/2504.09420", "authors": ["Yutao Mou", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Current safety alignment techniques for large language models (LLMs) face two\nkey challenges: (1) under-generalization, which leaves models vulnerable to\nnovel jailbreak attacks, and (2) over-alignment, which leads to the excessive\nrefusal of benign instructions. Our preliminary investigation reveals semantic\noverlap between jailbreak/harmful queries and normal prompts in embedding\nspace, suggesting that more effective safety alignment requires a deeper\nsemantic understanding. This motivates us to incorporate safety-policy-driven\nreasoning into the alignment process. To this end, we propose the\nSafety-oriented Reasoning Optimization Framework (SaRO), which consists of two\nstages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize\nlong-chain reasoning through supervised fine-tuning, and (2) Safety-oriented\nReasoning Process Optimization (SRPO) that promotes safety reflection via\ndirect preference optimization (DPO). Extensive experiments demonstrate the\nsuperiority of SaRO over traditional alignment methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10044", "pdf": "https://arxiv.org/pdf/2504.10044", "abs": "https://arxiv.org/abs/2504.10044", "authors": ["Bingwen Zhu", "Yudong Jiang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Yidi Wu", "Huyang Sun", "Zuxuan Wu"], "title": "Aligning Anime Video Generation with Human Feedback", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 7 tables", "summary": "Anime video generation faces significant challenges due to the scarcity of\nanime data and unusual motion patterns, leading to issues such as motion\ndistortion and flickering artifacts, which result in misalignment with human\npreferences. Existing reward models, designed primarily for real-world videos,\nfail to capture the unique appearance and consistency requirements of anime. In\nthis work, we propose a pipeline to enhance anime video generation by\nleveraging human feedback for better alignment. Specifically, we construct the\nfirst multi-dimensional reward dataset for anime videos, comprising 30k\nhuman-annotated samples that incorporating human preferences for both visual\nappearance and visual consistency. Based on this, we develop AnimeReward, a\npowerful reward model that employs specialized vision-language models for\ndifferent evaluation dimensions to guide preference alignment. Furthermore, we\nintroduce Gap-Aware Preference Optimization (GAPO), a novel training method\nthat explicitly incorporates preference gaps into the optimization process,\nenhancing alignment performance and efficiency. Extensive experiment results\nshow that AnimeReward outperforms existing reward models, and the inclusion of\nGAPO leads to superior alignment in both quantitative benchmarks and human\nevaluations, demonstrating the effectiveness of our pipeline in enhancing anime\nvideo quality. Our dataset and code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "human feedback", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "multi-dimensional"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09420", "pdf": "https://arxiv.org/pdf/2504.09420", "abs": "https://arxiv.org/abs/2504.09420", "authors": ["Yutao Mou", "Yuxiao Luo", "Shikun Zhang", "Wei Ye"], "title": "SaRO: Enhancing LLM Safety through Reasoning-based Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Current safety alignment techniques for large language models (LLMs) face two\nkey challenges: (1) under-generalization, which leaves models vulnerable to\nnovel jailbreak attacks, and (2) over-alignment, which leads to the excessive\nrefusal of benign instructions. Our preliminary investigation reveals semantic\noverlap between jailbreak/harmful queries and normal prompts in embedding\nspace, suggesting that more effective safety alignment requires a deeper\nsemantic understanding. This motivates us to incorporate safety-policy-driven\nreasoning into the alignment process. To this end, we propose the\nSafety-oriented Reasoning Optimization Framework (SaRO), which consists of two\nstages: (1) Reasoning-style Warmup (RW) that enables LLMs to internalize\nlong-chain reasoning through supervised fine-tuning, and (2) Safety-oriented\nReasoning Process Optimization (SRPO) that promotes safety reflection via\ndirect preference optimization (DPO). Extensive experiments demonstrate the\nsuperiority of SaRO over traditional alignment methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10044", "pdf": "https://arxiv.org/pdf/2504.10044", "abs": "https://arxiv.org/abs/2504.10044", "authors": ["Bingwen Zhu", "Yudong Jiang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Yidi Wu", "Huyang Sun", "Zuxuan Wu"], "title": "Aligning Anime Video Generation with Human Feedback", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 7 tables", "summary": "Anime video generation faces significant challenges due to the scarcity of\nanime data and unusual motion patterns, leading to issues such as motion\ndistortion and flickering artifacts, which result in misalignment with human\npreferences. Existing reward models, designed primarily for real-world videos,\nfail to capture the unique appearance and consistency requirements of anime. In\nthis work, we propose a pipeline to enhance anime video generation by\nleveraging human feedback for better alignment. Specifically, we construct the\nfirst multi-dimensional reward dataset for anime videos, comprising 30k\nhuman-annotated samples that incorporating human preferences for both visual\nappearance and visual consistency. Based on this, we develop AnimeReward, a\npowerful reward model that employs specialized vision-language models for\ndifferent evaluation dimensions to guide preference alignment. Furthermore, we\nintroduce Gap-Aware Preference Optimization (GAPO), a novel training method\nthat explicitly incorporates preference gaps into the optimization process,\nenhancing alignment performance and efficiency. Extensive experiment results\nshow that AnimeReward outperforms existing reward models, and the inclusion of\nGAPO leads to superior alignment in both quantitative benchmarks and human\nevaluations, demonstrating the effectiveness of our pipeline in enhancing anime\nvideo quality. Our dataset and code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "human feedback", "preference", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "multi-dimensional"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09488", "pdf": "https://arxiv.org/pdf/2504.09488", "abs": "https://arxiv.org/abs/2504.09488", "authors": ["Jiashu Yang", "Ningning Wang", "Yian Zhao", "Chaoran Feng", "Junjia Du", "Hao Pang", "Zhirui Fang", "Xuxin Cheng"], "title": "Kongzi: A Historical Large Language Model with Fact Enhancement", "categories": ["cs.CL"], "comment": "22 pages, 12 figures", "summary": "The capabilities of the latest large language models (LLMs) have been\nextended from pure natural language understanding to complex reasoning tasks.\nHowever, current reasoning models often exhibit factual inaccuracies in longer\nreasoning chains, which poses challenges for historical reasoning and limits\nthe potential of LLMs in complex, knowledge-intensive tasks. Historical studies\nrequire not only the accurate presentation of factual information but also the\nability to establish cross-temporal correlations and derive coherent\nconclusions from fragmentary and often ambiguous sources. To address these\nchallenges, we propose Kongzi, a large language model specifically designed for\nhistorical analysis. Through the integration of curated, high-quality\nhistorical data and a novel fact-reinforcement learning strategy, Kongzi\ndemonstrates strong factual alignment and sophisticated reasoning depth.\nExtensive experiments on tasks such as historical question answering and\nnarrative generation demonstrate that Kongzi outperforms existing models in\nboth factual accuracy and reasoning depth. By effectively addressing the unique\nchallenges inherent in historical texts, Kongzi sets a new standard for the\ndevelopment of accurate and reliable LLMs in professional domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09895", "pdf": "https://arxiv.org/pdf/2504.09895", "abs": "https://arxiv.org/abs/2504.09895", "authors": ["Shuai Zhao", "Linchao Zhu", "Yi Yang"], "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10185", "pdf": "https://arxiv.org/pdf/2504.10185", "abs": "https://arxiv.org/abs/2504.10185", "authors": ["Soumyadeep Pal", "Changsheng Wang", "James Diffenderfer", "Bhavya Kailkhura", "Sijia Liu"], "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09881", "pdf": "https://arxiv.org/pdf/2504.09881", "abs": "https://arxiv.org/abs/2504.09881", "authors": ["Changwei Wang", "Shunpeng Chen", "Yukun Song", "Rongtao Xu", "Zherui Zhang", "Jiguang Zhang", "Haoran Yang", "Yu Zhang", "Kexue Fu", "Shide Du", "Zhiwei Xu", "Longxiang Gao", "Li Guo", "Shibiao Xu"], "title": "Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09488", "pdf": "https://arxiv.org/pdf/2504.09488", "abs": "https://arxiv.org/abs/2504.09488", "authors": ["Jiashu Yang", "Ningning Wang", "Yian Zhao", "Chaoran Feng", "Junjia Du", "Hao Pang", "Zhirui Fang", "Xuxin Cheng"], "title": "Kongzi: A Historical Large Language Model with Fact Enhancement", "categories": ["cs.CL"], "comment": "22 pages, 12 figures", "summary": "The capabilities of the latest large language models (LLMs) have been\nextended from pure natural language understanding to complex reasoning tasks.\nHowever, current reasoning models often exhibit factual inaccuracies in longer\nreasoning chains, which poses challenges for historical reasoning and limits\nthe potential of LLMs in complex, knowledge-intensive tasks. Historical studies\nrequire not only the accurate presentation of factual information but also the\nability to establish cross-temporal correlations and derive coherent\nconclusions from fragmentary and often ambiguous sources. To address these\nchallenges, we propose Kongzi, a large language model specifically designed for\nhistorical analysis. Through the integration of curated, high-quality\nhistorical data and a novel fact-reinforcement learning strategy, Kongzi\ndemonstrates strong factual alignment and sophisticated reasoning depth.\nExtensive experiments on tasks such as historical question answering and\nnarrative generation demonstrate that Kongzi outperforms existing models in\nboth factual accuracy and reasoning depth. By effectively addressing the unique\nchallenges inherent in historical texts, Kongzi sets a new standard for the\ndevelopment of accurate and reliable LLMs in professional domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09802", "pdf": "https://arxiv.org/pdf/2504.09802", "abs": "https://arxiv.org/abs/2504.09802", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Training Small Reasoning LLMs with Cognitive Preference Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs), such as OpenAI's\no1 and DeepSeek-R1, have seen substantial advancements through deep thinking.\nHowever, these enhancements come with significant resource demands,\nunderscoring the need to explore strategies to train effective reasoning LLMs\nwith far fewer parameters. A critical challenge is that smaller models have\ndifferent capacities and cognitive trajectories than their larger counterparts.\nHence, direct distillation of chain-of-thought (CoT) results from large LLMs to\nsmaller ones can be sometimes ineffective and requires a huge amount of\nannotated data. In this paper, we introduce a novel framework called\nCritique-Rethink-Verify (CRV), designed for training smaller yet powerful\nreasoning LLMs. Our CRV framework consists of multiple LLM agents, each\nspecializing in unique abilities: (i) critiquing the CoTs according to the\ncognitive capabilities of smaller models, (ii) rethinking and refining these\nCoTs based on the critiques, and (iii) verifying the correctness of the refined\nresults. We further propose the cognitive preference optimization (CogPO)\nalgorithm to enhance the reasoning abilities of smaller models by aligning\nthoughts of these models with their cognitive capacities. Comprehensive\nevaluations on challenging reasoning benchmarks demonstrate the efficacy of CRV\nand CogPO, which outperforms other training methods by a large margin.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09895", "pdf": "https://arxiv.org/pdf/2504.09895", "abs": "https://arxiv.org/abs/2504.09895", "authors": ["Shuai Zhao", "Linchao Zhu", "Yi Yang"], "title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large language models~(LLMs) are expected to be helpful, harmless, and\nhonest. In various alignment scenarios, such as general human preference,\nsafety, and confidence alignment, binary preference data collection and reward\nmodeling are resource-intensive but necessary for human preference\ntransferring. In this work, we explore using the similarity between sampled\ngenerations and high-quality reference answers as an alternative reward\nfunction for LLM alignment. Using similarity as a reward circumvents training\nreward models, and collecting a single reference answer potentially costs less\ntime than constructing binary preference pairs when multiple candidates are\navailable. Specifically, we develop \\textit{RefAlign}, a versatile\nREINFORCE-style alignment algorithm, which is free of reference and reward\nmodels. Instead, RefAlign utilizes BERTScore between sampled generations and\nhigh-quality reference answers as the surrogate reward. Beyond general human\npreference optimization, RefAlign can be readily extended to diverse scenarios,\nsuch as safety and confidence alignment, by incorporating the similarity reward\nwith task-related objectives. In various scenarios, {RefAlign} demonstrates\ncomparable performance to previous alignment methods while offering high\nefficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10185", "pdf": "https://arxiv.org/pdf/2504.10185", "abs": "https://arxiv.org/abs/2504.10185", "authors": ["Soumyadeep Pal", "Changsheng Wang", "James Diffenderfer", "Bhavya Kailkhura", "Sijia Liu"], "title": "LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language model unlearning has become a critical challenge in ensuring\nsafety and controlled model behavior by removing undesired data-model\ninfluences from the pretrained model while preserving general utility.\nSignificant recent efforts have been dedicated to developing LLM unlearning\nbenchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine\nUnlearning Six-way Evaluation), facilitating standardized unlearning\nperformance assessment and method comparison. Despite their usefulness, we\nuncover for the first time a novel coreset effect within these benchmarks.\nSpecifically, we find that LLM unlearning achieved with the original (full)\nforget set can be effectively maintained using a significantly smaller subset\n(functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even\nwhen selected at random. This suggests that LLM unlearning in these benchmarks\ncan be performed surprisingly easily, even in an extremely low-data regime. We\ndemonstrate that this coreset effect remains strong, regardless of the LLM\nunlearning method used, such as NPO (Negative Preference Optimization) and RMU\n(Representation Misdirection Unlearning), the popular ones in these benchmarks.\nThe surprisingly strong coreset effect is also robust across various data\nselection methods, ranging from random selection to more sophisticated\nheuristic approaches. We explain the coreset effect in LLM unlearning through a\nkeyword-based perspective, showing that keywords extracted from the forget set\nalone contribute significantly to unlearning effectiveness and indicating that\ncurrent unlearning is driven by a compact set of high-impact tokens rather than\nthe entire dataset. We further justify the faithfulness of coreset-unlearned\nmodels along additional dimensions, such as mode connectivity and robustness to\njailbreaking attacks. Codes are available at\nhttps://github.com/OPTML-Group/MU-Coreset.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10187", "pdf": "https://arxiv.org/pdf/2504.10187", "abs": "https://arxiv.org/abs/2504.10187", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "Deep Reasoning Translation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1/o3 and DeepSeek-R1) have shown\npromising performance in various complex tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation and taking cultural differences into account.\nThis task is still under-explored in deep reasoning LLMs. In this paper, we\nintroduce DeepTrans, a deep reasoning translation model that learns free\ntranslation via reinforcement learning. Specifically, we carefully build a\nreward model with pre-defined scoring criteria on both the translation results\nand the thought process. Given the source sentences, the reward model teaches\nthe deep translation model how to think and free-translate them during\nreinforcement learning. In this way, training DeepTrans does not need any\nlabeled translations, avoiding the human-intensive annotation or\nresource-intensive data synthesis. Experimental results show the effectiveness\nof DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance\nby 16.3% in literature translation, and outperforms strong deep reasoning\nbaselines as well as baselines that are fine-tuned with synthesized data.\nMoreover, we summarize the failures and interesting findings during our RL\nexploration. We hope this work could inspire other researchers in free\ntranslation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "criteria"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09881", "pdf": "https://arxiv.org/pdf/2504.09881", "abs": "https://arxiv.org/abs/2504.09881", "authors": ["Changwei Wang", "Shunpeng Chen", "Yukun Song", "Rongtao Xu", "Zherui Zhang", "Jiguang Zhang", "Haoran Yang", "Yu Zhang", "Kexue Fu", "Shide Du", "Zhiwei Xu", "Longxiang Gao", "Li Guo", "Shibiao Xu"], "title": "Focus on Local: Finding Reliable Discriminative Regions for Visual Place Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Visual Place Recognition (VPR) is aimed at predicting the location of a query\nimage by referencing a database of geotagged images. For VPR task, often fewer\ndiscriminative local regions in an image produce important effects while\nmundane background regions do not contribute or even cause perceptual aliasing\nbecause of easy overlap. However, existing methods lack precisely modeling and\nfull exploitation of these discriminative regions. In this paper, we propose\nthe Focus on Local (FoL) approach to stimulate the performance of image\nretrieval and re-ranking in VPR simultaneously by mining and exploiting\nreliable discriminative local regions in images and introducing\npseudo-correlation supervision. First, we design two losses,\nExtraction-Aggregation Spatial Alignment Loss (SAL) and Foreground-Background\nContrast Enhancement Loss (CEL), to explicitly model reliable discriminative\nlocal regions and use them to guide the generation of global representations\nand efficient re-ranking. Second, we introduce a weakly-supervised local\nfeature training strategy based on pseudo-correspondences obtained from\naggregating global features to alleviate the lack of local correspondences\nground truth for the VPR task. Third, we suggest an efficient re-ranking\npipeline that is efficiently and precisely based on discriminative region\nguidance. Finally, experimental results show that our FoL achieves the\nstate-of-the-art on multiple VPR benchmarks in both image retrieval and\nre-ranking stages and also significantly outperforms existing two-stage VPR\nmethods in terms of computational efficiency. Code and models are available at\nhttps://github.com/chenshunpeng/FoL", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09037", "pdf": "https://arxiv.org/pdf/2504.09037", "abs": "https://arxiv.org/abs/2504.09037", "authors": ["Zixuan Ke", "Fangkai Jiao", "Yifei Ming", "Xuan-Phi Nguyen", "Austin Xu", "Do Xuan Long", "Minzhi Li", "Chengwei Qin", "Peifeng Wang", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems", "categories": ["cs.AI", "cs.CL"], "comment": "72 pages, 6 figures", "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09946", "pdf": "https://arxiv.org/pdf/2504.09946", "abs": "https://arxiv.org/abs/2504.09946", "authors": ["Qian Wang", "Zhanzhi Lou", "Zhenheng Tang", "Nuo Chen", "Xuandong Zhao", "Wenxuan Zhang", "Dawn Song", "Bingsheng He"], "title": "Assessing Judging Bias in Large Reasoning Models: An Empirical Study", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have\ndemonstrated remarkable reasoning capabilities, raising important questions\nabout their biases in LLM-as-a-judge settings. We present a comprehensive\nbenchmark comparing judging biases between LLMs and LRMs across both subjective\npreference-alignment datasets and objective fact-based datasets. Through\ninvestigation of bandwagon, authority, position, and distraction biases, we\nuncover four key findings: (1) despite their advanced reasoning capabilities,\nLRMs remain susceptible to the above biases; (2) LRMs demonstrate better\nrobustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit\nnotable position bias, preferring options in later positions; and (4) we\nidentify a novel \"superficial reflection bias\" where phrases mimicking\nreasoning (e.g., \"wait, let me think...\") significantly influence model\njudgments. To address these biases, we design and evaluate three mitigation\nstrategies: specialized system prompts that reduce judging biases by up to 19\\%\nin preference alignment datasets and 14\\% in fact-related datasets, in-context\nlearning that provides up to 27\\% improvement on preference tasks but shows\ninconsistent results on factual tasks, and a self-reflection mechanism that\nreduces biases by up to 10\\% in preference datasets and 16\\% in fact-related\ndatasets, with self-reflection proving particularly effective for LRMs. Our\nwork provides crucial insights for developing more reliable LLM-as-a-Judge\nframeworks, especially as LRMs become increasingly deployed as automated\njudges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10458", "pdf": "https://arxiv.org/pdf/2504.10458", "abs": "https://arxiv.org/abs/2504.10458", "authors": ["Xiaobo Xia", "Run Luo"], "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents", "categories": ["cs.CV", "cs.CL", "cs.HC"], "comment": null, "summary": "Existing efforts in building Graphical User Interface (GUI) agents largely\nrely on the training paradigm of supervised fine-tuning on Large\nVision-Language Models (LVLMs). However, this approach not only demands\nextensive amounts of training data but also struggles to effectively understand\nGUI screenshots and generalize to unseen interfaces. The issue significantly\nlimits its application in real-world scenarios, especially for high-level\ntasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models\n(e.g., DeepSeek-R1), which efficiently enhances the problem-solving\ncapabilities of large language models in real-world settings, we propose \\name,\nthe first reinforcement learning framework designed to enhance the GUI\ncapabilities of LVLMs in high-level real-world task scenarios, through unified\naction space rule modeling. By leveraging a small amount of carefully curated\nhigh-quality data across multiple platforms (including Windows, Linux, MacOS,\nAndroid, and Web) and employing policy optimization algorithms such as Group\nRelative Policy Optimization (GRPO) to update the model, \\name achieves\nsuperior performance using only 0.02\\% of the data (3K vs. 13M) compared to\nprevious state-of-the-art methods like OS-Atlas across eight benchmarks\nspanning three different platforms (mobile, desktop, and web). These results\ndemonstrate the immense potential of reinforcement learning based on unified\naction space rule modeling in improving the execution capabilities of LVLMs for\nreal-world GUI agent tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10479", "pdf": "https://arxiv.org/pdf/2504.10479", "abs": "https://arxiv.org/abs/2504.10479", "authors": ["Jinguo Zhu", "Weiyun Wang", "Zhe Chen", "Zhaoyang Liu", "Shenglong Ye", "Lixin Gu", "Yuchen Duan", "Hao Tian", "Weijie Su", "Jie Shao", "Zhangwei Gao", "Erfei Cui", "Yue Cao", "Yangzhou Liu", "Weiye Xu", "Hao Li", "Jiahao Wang", "Han Lv", "Dengnian Chen", "Songze Li", "Yinan He", "Tan Jiang", "Jiapeng Luo", "Yi Wang", "Conghui He", "Botian Shi", "Xingcheng Zhang", "Wenqi Shao", "Junjun He", "Yingtong Xiong", "Wenwen Qu", "Peng Sun", "Penglong Jiao", "Lijun Wu", "Kaipeng Zhang", "Huipeng Deng", "Jiaye Ge", "Kai Chen", "Limin Wang", "Min Dou", "Lewei Lu", "Xizhou Zhu", "Tong Lu", "Dahua Lin", "Yu Qiao", "Jifeng Dai", "Wenhai Wang"], "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We introduce InternVL3, a significant advancement in the InternVL series\nfeaturing a native multimodal pre-training paradigm. Rather than adapting a\ntext-only large language model (LLM) into a multimodal large language model\n(MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and\nlinguistic capabilities from both diverse multimodal data and pure-text corpora\nduring a single pre-training stage. This unified training paradigm effectively\naddresses the complexities and alignment challenges commonly encountered in\nconventional post-hoc training pipelines for MLLMs. To further improve\nperformance and scalability, InternVL3 incorporates variable visual position\nencoding (V2PE) to support extended multimodal contexts, employs advanced\npost-training techniques such as supervised fine-tuning (SFT) and mixed\npreference optimization (MPO), and adopts test-time scaling strategies\nalongside an optimized training infrastructure. Extensive empirical evaluations\ndemonstrate that InternVL3 delivers superior performance across a wide range of\nmulti-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the\nMMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its\ncapabilities remain highly competitive with leading proprietary models,\nincluding ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also\nmaintaining strong pure-language proficiency. In pursuit of open-science\nprinciples, we will publicly release both the training data and model weights\nto foster further research and development in next-generation MLLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10000", "pdf": "https://arxiv.org/pdf/2504.10000", "abs": "https://arxiv.org/abs/2504.10000", "authors": ["Yanbo Wang", "Jiyang Guan", "Jian Liang", "Ran He"], "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025, codes in process", "summary": "Multi-modal large language models (MLLMs) have made significant progress, yet\ntheir safety alignment remains limited. Typically, current open-source MLLMs\nrely on the alignment inherited from their language module to avoid harmful\ngenerations. However, the lack of safety measures specifically designed for\nmulti-modal inputs creates an alignment gap, leaving MLLMs vulnerable to\nvision-domain attacks such as typographic manipulation. Current methods utilize\na carefully designed safety dataset to enhance model defense capability, while\nthe specific knowledge or patterns acquired from the high-quality dataset\nremain unclear. Through comparison experiments, we find that the alignment gap\nprimarily arises from data distribution biases, while image content, response\nquality, or the contrastive behavior of the dataset makes little contribution\nto boosting multi-modal safety. To further investigate this and identify the\nkey factors in improving MLLM safety, we propose finetuning MLLMs on a small\nset of benign instruct-following data with responses replaced by simple, clear\nrejection sentences. Experiments show that, without the need for\nlabor-intensive collection of high-quality malicious data, model safety can\nstill be significantly improved, as long as a specific fraction of rejection\ndata exists in the finetuning set, indicating the security alignment is not\nlost but rather obscured during multi-modal pretraining or instruction\nfinetuning. Simply correcting the underlying data bias could narrow the safety\ngap in the vision domain.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09033", "pdf": "https://arxiv.org/pdf/2504.09033", "abs": "https://arxiv.org/abs/2504.09033", "authors": ["Snigdha Agarwal", "Neelam Sinha"], "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 5 figures", "summary": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09039", "pdf": "https://arxiv.org/pdf/2504.09039", "abs": "https://arxiv.org/abs/2504.09039", "authors": ["Gen Li", "Yang Xiao", "Jie Ji", "Kaiyuan Deng", "Bo Hui", "Linke Guo", "Xiaolong Ma"], "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08820", "pdf": "https://arxiv.org/pdf/2504.08820", "abs": "https://arxiv.org/abs/2504.08820", "authors": ["Jing Yao", "Xiaoyuan Yi", "Jindong Wang", "Zhicheng Dou", "Xing Xie"], "title": "CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) more deeply integrate into human life across\nvarious regions, aligning them with pluralistic cultures is crucial for\nimproving user experience and mitigating cultural conflicts. Existing\napproaches develop culturally aligned LLMs primarily through fine-tuning with\nmassive carefully curated culture-specific corpora. Nevertheless, inspired by\nculture theories, we identify two key challenges faced by these datasets: (1)\nRepresentativeness: These corpora fail to fully capture the target culture's\ncore characteristics with redundancy, causing computation waste; (2)\nDistinctiveness: They struggle to distinguish the unique nuances of a given\nculture from shared patterns across other relevant ones, hindering precise\ncultural modeling. To handle these challenges, we introduce CAReDiO, a novel\ncultural data construction framework. Specifically, CAReDiO utilizes powerful\nLLMs to automatically generate cultural conversation data, where both the\nqueries and responses are further optimized by maximizing representativeness\nand distinctiveness. Using CAReDiO, we construct a small yet effective dataset,\ncovering five cultures, and compare it with several recent cultural corpora.\nExtensive experiments demonstrate that our method generates more effective data\nand enables cultural alignment with as few as 100 training samples, enhancing\nboth performance and efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838", "abs": "https://arxiv.org/abs/2504.08838", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Vithursan Thangarasa"], "title": "SD$^2$: Self-Distilled Sparse Drafters", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "21 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a $\\times$1.59 higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our\nresults highlight the potential of sparsity-aware fine-tuning and compression\nstrategies to improve LLM inference efficiency while maintaining alignment with\ntarget models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09097", "pdf": "https://arxiv.org/pdf/2504.09097", "abs": "https://arxiv.org/abs/2504.09097", "authors": ["Jeongwan On", "Kyeonghwan Gwak", "Gunyoung Kang", "Junuk Cha", "Soohyun Hwang", "Hyein Hwang", "Seungryul Baek"], "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem\nthat can find numerous applications. Despite recent advances, there is no\ncomprehensive pipeline yet for bimanual class-agnostic interaction\nreconstruction from a monocular RGB video, where two hands and an unknown\nobject are interacting with each other. Previous works tackled the limited\nhand-object interaction case, where object templates are pre-known or only one\nhand is involved in the interaction. The bimanual interaction reconstruction\nexhibits severe occlusions introduced by complex interactions between two hands\nand an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D\nGaussian Splatting), a method that reconstructs 3D Gaussians of hands and an\nunknown object from a monocular video. To robustly obtain object Gaussians\navoiding severe occlusions, we leverage prior knowledge of pre-trained\ndiffusion model with score distillation sampling (SDS) loss, to reconstruct\nunseen object parts. For hand Gaussians, we exploit the 3D priors of hand model\n(i.e., MANO) and share a single Gaussian for two hands to effectively\naccumulate hand 3D information, given limited views. To further consider the 3D\nalignment between hands and objects, we include the interacting-subjects\noptimization step during Gaussian optimization. Our method achieves the\nstate-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose\nestimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering\nquality (PSNR, SSIM, LPIPS), respectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09109", "pdf": "https://arxiv.org/pdf/2504.09109", "abs": "https://arxiv.org/abs/2504.09109", "authors": ["Ganxi Xu", "Jinyi Long", "Hanrui Wu", "Jia Zhang"], "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Brain decoding currently faces significant challenges in individual\ndifferences, modality alignment, and high-dimensional embeddings. To address\nindividual differences, researchers often use source subject data, which leads\nto issues such as privacy leakage and heavy data storage burdens. In modality\nalignment, current works focus on aligning the softmax probability distribution\nbut neglect the alignment of marginal probability distributions, resulting in\nmodality misalignment. Additionally, images and text are aligned separately\nwith fMRI without considering the complex interplay between images and text,\nleading to poor image reconstruction. Finally, the enormous dimensionality of\nCLIP embeddings causes significant computational costs. Although the\ndimensionality of CLIP embeddings can be reduced by ignoring the number of\npatches obtained from images and the number of tokens acquired from text, this\ncomes at the cost of a significant drop in model performance, creating a\ndilemma. To overcome these limitations, we propose a source-free domain\nadaptation-based brain decoding framework", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09196", "pdf": "https://arxiv.org/pdf/2504.09196", "abs": "https://arxiv.org/abs/2504.09196", "authors": ["Feng Lv", "Chunlong Xia", "Shuo Wang", "Huo Cao"], "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09191", "pdf": "https://arxiv.org/pdf/2504.09191", "abs": "https://arxiv.org/abs/2504.09191", "authors": ["Weilong Dong", "Peiguang Li", "Yu Tian", "Xinyi Zeng", "Fengdi Li", "Sirui Wang"], "title": "Feature-Aware Malicious Output Detection and Mitigation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has brought significant\nbenefits to various domains while introducing substantial risks. Despite being\nfine-tuned through reinforcement learning, LLMs lack the capability to discern\nmalicious content, limiting their defense against jailbreak. To address these\nsafety concerns, we propose a feature-aware method for harmful response\nrejection (FMM), which detects the presence of malicious features within the\nmodel's feature space and adaptively adjusts the model's rejection mechanism.\nBy employing a simple discriminator, we detect potential malicious traits\nduring the decoding phase. Upon detecting features indicative of toxic tokens,\nFMM regenerates the current token. By employing activation patching, an\nadditional rejection vector is incorporated during the subsequent token\ngeneration, steering the model towards a refusal response. Experimental results\ndemonstrate the effectiveness of our approach across multiple language models\nand diverse attack techniques, while crucially maintaining the models' standard\ngeneration capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09378", "pdf": "https://arxiv.org/pdf/2504.09378", "abs": "https://arxiv.org/abs/2504.09378", "authors": ["Kartik Ravisankar", "Hyojung Han", "Marine Carpuat"], "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "reliability"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09298", "pdf": "https://arxiv.org/pdf/2504.09298", "abs": "https://arxiv.org/abs/2504.09298", "authors": ["Tinh-Anh Nguyen-Nhu", "Huu-Loc Tran", "Nguyen-Khang Le", "Minh-Nhat Nguyen", "Tien-Huy Nguyen", "Hoang-Long Nguyen-Huu", "Huu-Phong Phan-Nguyen", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09472", "pdf": "https://arxiv.org/pdf/2504.09472", "abs": "https://arxiv.org/abs/2504.09472", "authors": ["Pooja Guhan", "Divya Kothandaraman", "Tsung-Wei Huang", "Guan-Ming Su", "Dinesh Manocha"], "title": "CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CamMimic, an innovative algorithm tailored for dynamic video\nediting needs. It is designed to seamlessly transfer the camera motion observed\nin a given reference video onto any scene of the user's choice in a zero-shot\nmanner without requiring any additional data. Our algorithm achieves this using\na two-phase strategy by leveraging a text-to-video diffusion model. In the\nfirst phase, we develop a multi-concept learning method using a combination of\nLoRA layers and an orthogonality loss to capture and understand the underlying\nspatial-temporal characteristics of the reference video as well as the spatial\nfeatures of the user's desired scene. The second phase proposes a unique\nhomography-based refinement strategy to enhance the temporal and spatial\nalignment of the generated video. We demonstrate the efficacy of our method\nthrough experiments conducted on a dataset containing combinations of diverse\nscenes and reference videos containing a variety of camera motions. In the\nabsence of an established metric for assessing camera motion transfer between\nunrelated scenes, we propose CameraScore, a novel metric that utilizes\nhomography representations to measure camera motion similarity between the\nreference and generated videos. Extensive quantitative and qualitative\nevaluations demonstrate that our approach generates high-quality,\nmotion-enhanced videos. Additionally, a user study reveals that 70.31% of\nparticipants preferred our method for scene preservation, while 90.45% favored\nit for motion transfer. We hope this work lays the foundation for future\nadvancements in camera motion transfer across different scenes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09498", "pdf": "https://arxiv.org/pdf/2504.09498", "abs": "https://arxiv.org/abs/2504.09498", "authors": ["Yue Yang", "Christoph Leuze", "Brian Hargreaves", "Bruce Daniel", "Fred Baik"], "title": "EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance", "categories": ["cs.CV"], "comment": null, "summary": "The use of Augmented Reality (AR) devices for surgical guidance has gained\nincreasing traction in the medical field. Traditional registration methods\noften rely on external fiducial markers to achieve high accuracy and real-time\nperformance. However, these markers introduce cumbersome calibration procedures\nand can be challenging to deploy in clinical settings. While commercial\nsolutions have attempted real-time markerless tracking using the native RGB\ncameras of AR devices, their accuracy remains questionable for medical\nguidance, primarily due to occlusions and significant outliers between the live\nsensor data and the preoperative target anatomy point cloud derived from MRI or\nCT scans. In this work, we present a markerless framework that relies only on\nthe depth sensor of AR devices and consists of two modules: a registration\nmodule for high-precision, outlier-robust target anatomy localization, and a\ntracking module for real-time pose estimation. The registration module\nintegrates depth sensor error correction, a human-in-the-loop region filtering\ntechnique, and a robust global alignment with curvature-aware feature sampling,\nfollowed by local ICP refinement, for markerless alignment of preoperative\nmodels with patient anatomy. The tracking module employs a fast and robust\nregistration algorithm that uses the initial pose from the registration module\nto estimate the target pose in real-time. We comprehensively evaluated the\nperformance of both modules through simulation and real-world measurements. The\nresults indicate that our markerless system achieves superior performance for\nregistration and comparable performance for tracking to industrial solutions.\nThe two-module design makes our system a one-stop solution for surgical\nprocedures where the target anatomy moves or stays static during surgery.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09506", "pdf": "https://arxiv.org/pdf/2504.09506", "abs": "https://arxiv.org/abs/2504.09506", "authors": ["Yanze Jiang", "Yanfeng Gu", "Xian Li"], "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial\nand spectral information of ground objects, offering excellent 3D perception\nand target recognition capabilities. Current approaches for generating HPCs\noften involve fusion techniques with hyperspectral images and LiDAR point\nclouds, which inevitably lead to geometric-spectral distortions due to fusion\nerrors and obstacle occlusions. These adverse effects limit their performance\nin downstream fine-grained tasks across multiple scenarios, particularly in\nairborne applications. To address these issues, we propose PiV-AHPC, a 3D\nobject detection network for airborne HPCs. To the best of our knowledge, this\nis the first attempt at this HPCs task. Specifically, we first develop a\npillar-voxel dual-branch encoder, where the former captures spectral and\nvertical structural features from HPCs to overcome spectral distortion, while\nthe latter emphasizes extracting accurate 3D spatial features from point\nclouds. A multi-level feature fusion mechanism is devised to enhance\ninformation interaction between the two branches, achieving neighborhood\nfeature alignment and channel-adaptive selection, thereby organically\nintegrating heterogeneous features and mitigating geometric distortion.\nExtensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC\npossesses state-of-the-art detection performance and high generalization\ncapability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09518", "pdf": "https://arxiv.org/pdf/2504.09518", "abs": "https://arxiv.org/abs/2504.09518", "authors": ["Ting Huang", "Zeyu Zhang", "Yemin Wang", "Hao Tang"], "title": "3D CoCa: Contrastive Learners are 3D Captioners", "categories": ["cs.CV"], "comment": null, "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09910", "pdf": "https://arxiv.org/pdf/2504.09910", "abs": "https://arxiv.org/abs/2504.09910", "authors": ["Yujing Wang", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Binghui Guo", "Hongwei Zheng", "Zhiming Zheng"], "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a promising technique for applying\nLLMs to proprietary domains. However, retrieved documents may contain sensitive\nknowledge, posing risks of privacy leakage in generative results. Thus,\neffectively erasing private information from retrieved documents is a key\nchallenge for RAG. Unlike traditional text anonymization, RAG should consider:\n(1) the inherent multi-document reasoning may face de-anonymization attacks;\n(2) private knowledge varies by scenarios, so users should be allowed to\ncustomize which information to erase; (3) preserving sufficient publicly\navailable knowledge for generation tasks. This paper introduces the privacy\nerasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which\neffectively removes user-defined private knowledge from documents while\npreserving sufficient public knowledge for generation. Specifically, we first\nconstruct a global knowledge graph to identify potential knowledge across\ndocuments, aiming to defend against de-anonymization attacks. Then we randomly\nsplit it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite\nthe retrieved documents excluding private triples. Finally, PPO algorithm\noptimizes the rewriting model to minimize private triples and maximize public\ntriples retention. Experiments on four QA datasets demonstrate that Eraser4RAG\nachieves superior erase performance than GPT-4o.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09588", "pdf": "https://arxiv.org/pdf/2504.09588", "abs": "https://arxiv.org/abs/2504.09588", "authors": ["Zhicong Wu", "Hongbin Xu", "Gang Xu", "Ping Nie", "Zhixin Yan", "Jinkai Zheng", "Liangqiong Qu", "Ming Li", "Liqiang Nie"], "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Generalizable Gaussian Splatting have enabled robust\n3D reconstruction from sparse input views by utilizing feed-forward Gaussian\nSplatting models, achieving superior cross-scene generalization. However, while\nmany methods focus on geometric consistency, they often neglect the potential\nof text-driven guidance to enhance semantic understanding, which is crucial for\naccurately reconstructing fine-grained details in complex scenes. To address\nthis limitation, we propose TextSplat--the first text-driven Generalizable\nGaussian Splatting framework. By employing a text-guided fusion of diverse\nsemantic cues, our framework learns robust cross-modal feature representations\nthat improve the alignment of geometric and semantic information, producing\nhigh-fidelity 3D reconstructions. Specifically, our framework employs three\nparallel modules to obtain complementary representations: the Diffusion Prior\nDepth Estimator for accurate depth information, the Semantic Aware Segmentation\nNetwork for detailed semantic information, and the Multi-View Interaction\nNetwork for refined cross-view features. Then, in the Text-Guided Semantic\nFusion Module, these representations are integrated via the text-guided and\nattention-based feature aggregation mechanism, resulting in enhanced 3D\nGaussian parameters enriched with detailed semantic cues. Experimental results\non various benchmark datasets demonstrate improved performance compared to\nexisting methods across multiple evaluation metrics, validating the\neffectiveness of our framework. The code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "reasoning model"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09700", "pdf": "https://arxiv.org/pdf/2504.09700", "abs": "https://arxiv.org/abs/2504.09700", "authors": ["Zijian Wu", "Shuojue Yang", "Yueming Jin", "Septimiu E Salcudean"], "title": "ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection", "categories": ["cs.CV"], "comment": null, "summary": "In robot-assisted laparoscopic radical prostatectomy (RALP), the location of\nthe instrument tip is important to register the ultrasound frame with the\nlaparoscopic camera frame. A long-standing limitation is that the instrument\ntip position obtained from the da Vinci API is inaccurate and requires hand-eye\ncalibration. Thus, directly computing the position of the tool tip in the\ncamera frame using the vision-based method becomes an attractive solution.\nBesides, surgical instrument tip detection is the key component of other tasks,\nlike surgical skill assessment and surgery automation. However, this task is\nchallenging due to the small size of the tool tip and the articulation of the\nsurgical instrument. Surgical instrument segmentation becomes relatively easy\ndue to the emergence of the Segmentation Foundation Model, i.e., Segment\nAnything. Based on this advancement, we explore the deep learning-based\nsurgical instrument tip detection approach that takes the part-level instrument\nsegmentation mask as input. Comparison experiments with a hand-crafted\nimage-processing approach demonstrate the superiority of the proposed method on\nsimulated and real datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10340", "pdf": "https://arxiv.org/pdf/2504.10340", "abs": "https://arxiv.org/abs/2504.10340", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "Jeremy C. Weiss"], "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families", "categories": ["cs.CL", "cs.AI"], "comment": "Machine Learning for Healthcare (MLHC 2025)", "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10356", "pdf": "https://arxiv.org/pdf/2504.10356", "abs": "https://arxiv.org/abs/2504.10356", "authors": ["Dieuwke Hupkes", "Nikolay Bogoychev"], "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages", "categories": ["cs.CL"], "comment": null, "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09789", "pdf": "https://arxiv.org/pdf/2504.09789", "abs": "https://arxiv.org/abs/2504.09789", "authors": ["Chao Liu", "Arash Vahdat"], "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporally consistent video-to-video generation is essential for applications\nof video diffusion models in areas such as sim-to-real, style-transfer, video\nupsampling, etc. In this paper, we propose a video diffusion framework that\nleverages temporally consistent noise to generate coherent video frames without\nspecialized modules or additional constraints. We show that the standard\ntraining objective of diffusion models, when applied with temporally consistent\nnoise, encourages the model to be equivariant to spatial transformations in\ninput video and noise. This enables our model to better follow motion patterns\nfrom the input video, producing aligned motion and high-fidelity frames.\nFurthermore, we extend our approach to 3D-consistent video generation by\nattaching noise as textures on 3D meshes, ensuring 3D consistency in\nsim-to-real applications. Experimental results demonstrate that our method\nsurpasses state-of-the-art baselines in motion alignment, 3D consistency, and\nvideo quality while requiring only a few sampling steps in practice.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09852", "pdf": "https://arxiv.org/pdf/2504.09852", "abs": "https://arxiv.org/abs/2504.09852", "authors": ["Boris Kriuk", "Simranjit Kaur Gill", "Shoaib Aslam", "Amir Fakhrutdinov"], "title": "GFT: Gradient Focal Transformer", "categories": ["cs.CV"], "comment": "11 pages, 3 tables, 5 figures", "summary": "Fine-Grained Image Classification (FGIC) remains a complex task in computer\nvision, as it requires models to distinguish between categories with subtle\nlocalized visual differences. Well-studied CNN-based models, while strong in\nlocal feature extraction, often fail to capture the global context required for\nfine-grained recognition, while more recent ViT-backboned models address FGIC\nwith attention-driven mechanisms but lack the ability to adaptively focus on\ntruly discriminative regions. TransFG and other ViT-based extensions introduced\npart-aware token selection to enhance attention localization, yet they still\nstruggle with computational efficiency, attention region selection flexibility,\nand detail-focus narrative in complex environments. This paper introduces GFT\n(Gradient Focal Transformer), a new ViT-derived framework created for FGIC\ntasks. GFT integrates the Gradient Attention Learning Alignment (GALA)\nmechanism to dynamically prioritize class-discriminative features by analyzing\nattention gradient flow. Coupled with a Progressive Patch Selection (PPS)\nstrategy, the model progressively filters out less informative regions,\nreducing computational overhead while enhancing sensitivity to fine details.\nGFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with\n93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By\nbridging global context and localized detail extraction, GFT sets a new\nbenchmark in fine-grained recognition, offering interpretable solutions for\nreal-world deployment scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10430", "pdf": "https://arxiv.org/pdf/2504.10430", "abs": "https://arxiv.org/abs/2504.10430", "authors": ["Minqian Liu", "Zhiyang Xu", "Xinyi Zhang", "Heajun An", "Sarvech Qadir", "Qi Zhang", "Pamela J. Wisniewski", "Jin-Hee Cho", "Sang Won Lee", "Ruoxi Jia", "Lifu Huang"], "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08745", "pdf": "https://arxiv.org/pdf/2504.08745", "abs": "https://arxiv.org/abs/2504.08745", "authors": ["Mert Yazan", "Suzan Verberne", "Frederik Situmeang"], "title": "Improving RAG for Personalization with Author Features and Contrastive Examples", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalization with retrieval-augmented generation (RAG) often fails to\ncapture fine-grained features of authors, making it hard to identify their\nunique traits. To enrich the RAG context, we propose providing Large Language\nModels (LLMs) with author-specific features, such as average sentiment polarity\nand frequently used words, in addition to past samples from the author's\nprofile. We introduce a new feature called Contrastive Examples: documents from\nother authors are retrieved to help LLM identify what makes an author's style\nunique in comparison to others. Our experiments show that adding a couple of\nsentences about the named entities, dependency patterns, and words a person\nuses frequently significantly improves personalized text generation. Combining\nfeatures with contrastive examples boosts the performance further, achieving a\nrelative 15% improvement over baseline RAG while outperforming the benchmarks.\nOur results show the value of fine-grained features for better personalization,\nwhile opening a new research dimension for including contrastive examples as a\ncomplement with RAG. We release our code publicly.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained", "dimension"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09925", "pdf": "https://arxiv.org/pdf/2504.09925", "abs": "https://arxiv.org/abs/2504.09925", "authors": ["Zheng Liu", "Mengjie Liu", "Jingzhou Chen", "Jingwei Xu", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08846", "pdf": "https://arxiv.org/pdf/2504.08846", "abs": "https://arxiv.org/abs/2504.08846", "authors": ["Mostafa Faghih Shojaei", "Rahul Gulati", "Benjamin A. Jasperson", "Shangshang Wang", "Simone Cimolato", "Dangli Cao", "Willie Neiswanger", "Krishna Garikipati"], "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09973", "pdf": "https://arxiv.org/pdf/2504.09973", "abs": "https://arxiv.org/abs/2504.09973", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Aitical/CPLIR", "summary": "All-in-one image restoration, addressing diverse degradation types with a\nunified model, presents significant challenges in designing task-specific\nprompts that effectively guide restoration across multiple degradation\nscenarios. While adaptive prompt learning enables end-to-end optimization, it\noften yields overlapping or redundant task representations. Conversely,\nexplicit prompts derived from pretrained classifiers enhance discriminability\nbut may discard critical visual information for reconstruction. To address\nthese limitations, we introduce Contrastive Prompt Learning (CPL), a novel\nframework that fundamentally enhances prompt-task alignment through two\ncomplementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently\ncaptures degradation-specific features while minimizing redundancy, and a\n\\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task\nboundaries by incorporating negative prompt samples across different\ndegradation types. Unlike previous approaches that focus primarily on\ndegradation classification, CPL optimizes the critical interaction between\nprompts and the restoration model itself. Extensive experiments across five\ncomprehensive benchmarks demonstrate that CPL consistently enhances\nstate-of-the-art all-in-one restoration models, achieving significant\nimprovements in both standard multi-task scenarios and challenging composite\ndegradation settings. Our framework establishes new state-of-the-art\nperformance while maintaining parameter efficiency, offering a principled\nsolution for unified image restoration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09271", "pdf": "https://arxiv.org/pdf/2504.09271", "abs": "https://arxiv.org/abs/2504.09271", "authors": ["Koustuv Saha", "Yoshee Jain", "Munmun De Choudhury"], "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10021", "pdf": "https://arxiv.org/pdf/2504.10021", "abs": "https://arxiv.org/abs/2504.10021", "authors": ["Nikolai Rhrich", "Alwin Hoffmann", "Richard Nordsieck", "Emilio Zarbali", "Alireza Javanmardi"], "title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Whereas in general computer vision, transformer-based architectures have\nquickly become the gold standard, microelectronics defect detection still\nheavily relies on convolutional neural networks (CNNs). We hypothesize that\nthis is due to the fact that a) transformers have an increased need for data\nand b) labelled image generation procedures for microelectronics are costly,\nand labelled data is therefore sparse. Whereas in other domains, pre-training\non large natural image datasets can mitigate this problem, in microelectronics\ntransfer learning is hindered due to the dissimilarity of domain data and\nnatural images. Therefore, we evaluate self pre-training, where models are\npre-trained on the target dataset, rather than another dataset. We propose a\nvision transformer (ViT) pre-training framework for defect detection in\nmicroelectronics based on masked autoencoders (MAE). In MAE, a large share of\nimage patches is masked and reconstructed by the model during pre-training. We\nperform pre-training and defect detection using a dataset of less than 10.000\nscanning acoustic microscopy (SAM) images labelled using transient thermal\nanalysis (TTA). Our experimental results show that our approach leads to\nsubstantial performance gains compared to a) supervised ViT, b) ViT pre-trained\non natural image datasets, and c) state-of-the-art CNN-based defect detection\nmodels used in the literature. Additionally, interpretability analysis reveals\nthat our self pre-trained models, in comparison to ViT baselines, correctly\nfocus on defect-relevant features such as cracks in the solder material. This\ndemonstrates that our approach yields fault-specific feature representations,\nmaking our self pre-trained models viable for real-world defect detection in\nmicroelectronics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09466", "pdf": "https://arxiv.org/pdf/2504.09466", "abs": "https://arxiv.org/abs/2504.09466", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yulin Hu", "Yang Deng", "An Zhang", "Xingyu Sui", "Xinyang Han", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender", "categories": ["cs.CR", "cs.CL"], "comment": "17 pages, 6 figures, 9 tables", "summary": "Despite extensive efforts in safety alignment, large language models (LLMs)\nremain vulnerable to jailbreak attacks. Activation steering offers a\ntraining-free defense method but relies on fixed steering coefficients,\nresulting in suboptimal protection and increased false rejections of benign\ninputs. To address this, we propose AdaSteer, an adaptive activation steering\nmethod that dynamically adjusts model behavior based on input characteristics.\nWe identify two key properties: Rejection Law (R-Law), which shows that\nstronger steering is needed for jailbreak inputs opposing the rejection\ndirection, and Harmfulness Law (H-Law), which differentiates adversarial and\nbenign inputs. AdaSteer steers input representations along both the Rejection\nDirection (RD) and Harmfulness Direction (HD), with adaptive coefficients\nlearned via logistic regression, ensuring robust jailbreak defense while\npreserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and\nQwen2.5 show that AdaSteer outperforms baseline methods across multiple\njailbreak attacks with minimal impact on utility. Our results highlight the\npotential of interpretable model internals for real-time, flexible safety\nenforcement in LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09479", "pdf": "https://arxiv.org/pdf/2504.09479", "abs": "https://arxiv.org/abs/2504.09479", "authors": ["Zhiqing Cui", "Jiahao Yuan", "Hanqing Wang", "Yanshu Li", "Chenxu Du", "Zhenglong Ding"], "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific Diagram Generation", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 14 figures", "summary": "Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "code generation"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09710", "pdf": "https://arxiv.org/pdf/2504.09710", "abs": "https://arxiv.org/abs/2504.09710", "authors": ["Zhenting Wang", "Guofeng Cui", "Kun Wan", "Wentian Zhao"], "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["AI feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09816", "pdf": "https://arxiv.org/pdf/2504.09816", "abs": "https://arxiv.org/abs/2504.09816", "authors": ["Quentin Fitte-Rey", "Matyas Amrouche", "Romain Deveaud"], "title": "Augmented Relevance Datasets with Fine-Tuned Small LLMs", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25", "summary": "Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10106", "pdf": "https://arxiv.org/pdf/2504.10106", "abs": "https://arxiv.org/abs/2504.10106", "authors": ["Marc Gutirrez-Prez", "Antonio Agudo"], "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": null, "summary": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10117", "pdf": "https://arxiv.org/pdf/2504.10117", "abs": "https://arxiv.org/abs/2504.10117", "authors": ["Peizheng Li", "Shuxiao Ding", "You Zhou", "Qingwen Zhang", "Onat Inak", "Larissa Triess", "Niklas Hanselmann", "Marius Cordts", "Andreas Zell"], "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\nrepresentation from sensor inputs while recognizing both known and unknown\nobjects. Transferring open-vocabulary knowledge from vision-language models\n(VLMs) offers a promising direction but remains challenging. However, methods\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\nby a predefined label space and lack general prediction capabilities. Direct\nalignment with pretrained image embeddings, on the other hand, fails to achieve\nreliable performance due to often inconsistent image and text representations\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\nprediction framework with adaptive grounding to handle diverse open-world\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\ntext embeddings, respectively, leveraging similarity-based grounding training\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\na space aligned with VLM-derived image embeddings, reducing modality gaps.\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\nin zero-shot and few-shot transfer while achieving state-of-the-art\nclosed-world self-supervised performance, surpassing prior methods by 4.09\nmIoU.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10179", "pdf": "https://arxiv.org/pdf/2504.10179", "abs": "https://arxiv.org/abs/2504.10179", "authors": ["Anwesha Mohanty", "Venkatesh Balavadhani Parthasarathy", "Arsalan Shahid"], "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance", "categories": ["cs.AI", "cs.CL", "cs.ET"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "code generation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10483", "pdf": "https://arxiv.org/pdf/2504.10483", "abs": "https://arxiv.org/abs/2504.10483", "authors": ["Xingjian Leng", "Jaskirat Singh", "Yunzhong Hou", "Zhenchang Xing", "Saining Xie", "Liang Zheng"], "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09209", "pdf": "https://arxiv.org/pdf/2504.09209", "abs": "https://arxiv.org/abs/2504.09209", "authors": ["Xiangyue Zhang", "Jianfang Li", "Jiaxu Zhang", "Jianqiang Ren", "Liefeng Bo", "Zhigang Tu"], "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": "12 pages, 12 figures", "summary": "Masked modeling framework has shown promise in co-speech motion generation.\nHowever, it struggles to identify semantically significant frames for effective\nmotion masking. In this work, we propose a speech-queried attention-based mask\nmodeling framework for co-speech motion generation. Our key insight is to\nleverage motion-aligned speech features to guide the masked motion modeling\nprocess, selectively masking rhythm-related and semantically expressive motion\nframes. Specifically, we first propose a motion-audio alignment module (MAM) to\nconstruct a latent motion-audio joint space. In this space, both low-level and\nhigh-level speech features are projected, enabling motion-aligned speech\nrepresentation using learnable speech queries. Then, a speech-queried attention\nmechanism (SQA) is introduced to compute frame-level attention scores through\ninteractions between motion keys and speech queries, guiding selective masking\ntoward motion frames with high attention scores. Finally, the motion-aligned\nspeech features are also injected into the generation network to facilitate\nco-speech motion generation. Qualitative and quantitative evaluations confirm\nthat our method outperforms existing state-of-the-art approaches, successfully\nproducing high-quality co-speech motion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10007", "pdf": "https://arxiv.org/pdf/2504.10007", "abs": "https://arxiv.org/abs/2504.10007", "authors": ["Jiani Ni", "He Zhao", "Jintong Gao", "Dandan Guo", "Hongyuan Zha"], "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration", "categories": ["cs.LG", "cs.CV"], "comment": "CVPR2025", "summary": "In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09033", "pdf": "https://arxiv.org/pdf/2504.09033", "abs": "https://arxiv.org/abs/2504.09033", "authors": ["Snigdha Agarwal", "Neelam Sinha"], "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels", "categories": ["cs.CV", "cs.AI"], "comment": "5 pages, 5 figures", "summary": "Deep Convolutional Neural Networks have consistently proven to achieve\nstate-of-the-art results on a lot of imaging tasks over the past years'\nmajority of which comprise of high-quality data. However, it is important to\nwork on low-resolution images since it could be a cheaper alternative for\nremote healthcare access where the primary need of automated pathology\nidentification models occurs. Medical diagnosis using low-resolution images is\nchallenging since critical details may not be easily identifiable. In this\npaper, we report classification results by experimenting on different input\nimage sizes of Chest X-rays to deep CNN models and discuss the feasibility of\nclassification on varying image sizes. We also leverage the noisy labels in the\ndataset by proposing a Randomized Flipping of labels techniques. We use an\nensemble of multi-label classification models on frontal and lateral studies.\nOur models are trained on 5 out of the 14 chest pathologies of the publicly\navailable CheXpert dataset. We incorporate techniques such as augmentation,\nregularization for model improvement and use class activation maps to visualize\nthe neural network's decision making. Comparison with classification results on\ndata from 200 subjects, obtained on the corresponding high-resolution images,\nreported in the original CheXpert paper, has been presented. For pathologies\nCardiomegaly, Consolidation and Edema, we obtain 3% higher accuracy with our\nmodel architecture.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09039", "pdf": "https://arxiv.org/pdf/2504.09039", "abs": "https://arxiv.org/abs/2504.09039", "authors": ["Gen Li", "Yang Xiao", "Jie Ji", "Kaiyuan Deng", "Bo Hui", "Linke Guo", "Xiaolong Ma"], "title": "Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have achieved remarkable success in\ngenerating high-quality images from textual prompts. However, their ability to\nstore vast amounts of knowledge raises concerns in scenarios where selective\nforgetting is necessary, such as removing copyrighted content, reducing biases,\nor eliminating harmful concepts. While existing unlearning methods can remove\ncertain concepts, they struggle with multi-concept forgetting due to\ninstability, residual knowledge persistence, and generation quality\ndegradation. To address these challenges, we propose \\textbf{Dynamic Mask\ncoupled with Concept-Aware Loss}, a novel unlearning framework designed for\nmulti-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask}\nmechanism adaptively updates gradient masks based on current optimization\nstates, allowing selective weight modifications that prevent interference with\nunrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly\nguides the unlearning process by enforcing semantic consistency through\nsuperclass alignment, while a regularization loss based on knowledge\ndistillation ensures that previously unlearned concepts remain forgotten during\nsequential unlearning. We conduct extensive experiments to evaluate our\napproach. Results demonstrate that our method outperforms existing unlearning\ntechniques in forgetting effectiveness, output fidelity, and semantic\ncoherence, particularly in multi-concept scenarios. Our work provides a\nprincipled and flexible framework for stable and high-fidelity unlearning in\ngenerative models. The code will be released publicly.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08820", "pdf": "https://arxiv.org/pdf/2504.08820", "abs": "https://arxiv.org/abs/2504.08820", "authors": ["Jing Yao", "Xiaoyuan Yi", "Jindong Wang", "Zhicheng Dou", "Xing Xie"], "title": "CAReDiO: Cultural Alignment of LLM via Representativeness and Distinctiveness Guided Data Optimization", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) more deeply integrate into human life across\nvarious regions, aligning them with pluralistic cultures is crucial for\nimproving user experience and mitigating cultural conflicts. Existing\napproaches develop culturally aligned LLMs primarily through fine-tuning with\nmassive carefully curated culture-specific corpora. Nevertheless, inspired by\nculture theories, we identify two key challenges faced by these datasets: (1)\nRepresentativeness: These corpora fail to fully capture the target culture's\ncore characteristics with redundancy, causing computation waste; (2)\nDistinctiveness: They struggle to distinguish the unique nuances of a given\nculture from shared patterns across other relevant ones, hindering precise\ncultural modeling. To handle these challenges, we introduce CAReDiO, a novel\ncultural data construction framework. Specifically, CAReDiO utilizes powerful\nLLMs to automatically generate cultural conversation data, where both the\nqueries and responses are further optimized by maximizing representativeness\nand distinctiveness. Using CAReDiO, we construct a small yet effective dataset,\ncovering five cultures, and compare it with several recent cultural corpora.\nExtensive experiments demonstrate that our method generates more effective data\nand enables cultural alignment with as few as 100 training samples, enhancing\nboth performance and efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838", "abs": "https://arxiv.org/abs/2504.08838", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Vithursan Thangarasa"], "title": "SD$^2$: Self-Distilled Sparse Drafters", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "21 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a $\\times$1.59 higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our\nresults highlight the potential of sparsity-aware fine-tuning and compression\nstrategies to improve LLM inference efficiency while maintaining alignment with\ntarget models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09097", "pdf": "https://arxiv.org/pdf/2504.09097", "abs": "https://arxiv.org/abs/2504.09097", "authors": ["Jeongwan On", "Kyeonghwan Gwak", "Gunyoung Kang", "Junuk Cha", "Soohyun Hwang", "Hyein Hwang", "Seungryul Baek"], "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reconstructing 3Ds of hand-object interaction (HOI) is a fundamental problem\nthat can find numerous applications. Despite recent advances, there is no\ncomprehensive pipeline yet for bimanual class-agnostic interaction\nreconstruction from a monocular RGB video, where two hands and an unknown\nobject are interacting with each other. Previous works tackled the limited\nhand-object interaction case, where object templates are pre-known or only one\nhand is involved in the interaction. The bimanual interaction reconstruction\nexhibits severe occlusions introduced by complex interactions between two hands\nand an object. To solve this, we first introduce BIGS (Bimanual Interaction 3D\nGaussian Splatting), a method that reconstructs 3D Gaussians of hands and an\nunknown object from a monocular video. To robustly obtain object Gaussians\navoiding severe occlusions, we leverage prior knowledge of pre-trained\ndiffusion model with score distillation sampling (SDS) loss, to reconstruct\nunseen object parts. For hand Gaussians, we exploit the 3D priors of hand model\n(i.e., MANO) and share a single Gaussian for two hands to effectively\naccumulate hand 3D information, given limited views. To further consider the 3D\nalignment between hands and objects, we include the interacting-subjects\noptimization step during Gaussian optimization. Our method achieves the\nstate-of-the-art accuracy on two challenging datasets, in terms of 3D hand pose\nestimation (MPJPE), 3D object reconstruction (CDh, CDo, F10), and rendering\nquality (PSNR, SSIM, LPIPS), respectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09109", "pdf": "https://arxiv.org/pdf/2504.09109", "abs": "https://arxiv.org/abs/2504.09109", "authors": ["Ganxi Xu", "Jinyi Long", "Hanrui Wu", "Jia Zhang"], "title": "Probability Distribution Alignment and Low-Rank Weight Decomposition for Source-Free Domain Adaptive Brain Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Brain decoding currently faces significant challenges in individual\ndifferences, modality alignment, and high-dimensional embeddings. To address\nindividual differences, researchers often use source subject data, which leads\nto issues such as privacy leakage and heavy data storage burdens. In modality\nalignment, current works focus on aligning the softmax probability distribution\nbut neglect the alignment of marginal probability distributions, resulting in\nmodality misalignment. Additionally, images and text are aligned separately\nwith fMRI without considering the complex interplay between images and text,\nleading to poor image reconstruction. Finally, the enormous dimensionality of\nCLIP embeddings causes significant computational costs. Although the\ndimensionality of CLIP embeddings can be reduced by ignoring the number of\npatches obtained from images and the number of tokens acquired from text, this\ncomes at the cost of a significant drop in model performance, creating a\ndilemma. To overcome these limitations, we propose a source-free domain\nadaptation-based brain decoding framework", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09196", "pdf": "https://arxiv.org/pdf/2504.09196", "abs": "https://arxiv.org/abs/2504.09196", "authors": ["Feng Lv", "Chunlong Xia", "Shuo Wang", "Huo Cao"], "title": "RT-DATR:Real-time Unsupervised Domain Adaptive Detection Transformer with Adversarial Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Despite domain-adaptive object detectors based on CNN and transformers have\nmade significant progress in cross-domain detection tasks, it is regrettable\nthat domain adaptation for real-time transformer-based detectors has not yet\nbeen explored. Directly applying existing domain adaptation algorithms has\nproven to be suboptimal. In this paper, we propose RT-DATR, a simple and\nefficient real-time domain adaptive detection transformer. Building on RT-DETR\nas our base detector, we first introduce a local object-level feature alignment\nmodule to significantly enhance the feature representation of domain invariance\nduring object transfer. Additionally, we introduce a scene semantic feature\nalignment module designed to boost cross-domain detection performance by\naligning scene semantic features. Finally, we introduced a domain query and\ndecoupled it from the object query to further align the instance feature\ndistribution within the decoder layer, reduce the domain gap, and maintain\ndiscriminative ability. Experimental results on various benchmarks demonstrate\nthat our method outperforms current state-of-the-art approaches. Our code will\nbe released soon.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09191", "pdf": "https://arxiv.org/pdf/2504.09191", "abs": "https://arxiv.org/abs/2504.09191", "authors": ["Weilong Dong", "Peiguang Li", "Yu Tian", "Xinyi Zeng", "Fengdi Li", "Sirui Wang"], "title": "Feature-Aware Malicious Output Detection and Mitigation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has brought significant\nbenefits to various domains while introducing substantial risks. Despite being\nfine-tuned through reinforcement learning, LLMs lack the capability to discern\nmalicious content, limiting their defense against jailbreak. To address these\nsafety concerns, we propose a feature-aware method for harmful response\nrejection (FMM), which detects the presence of malicious features within the\nmodel's feature space and adaptively adjusts the model's rejection mechanism.\nBy employing a simple discriminator, we detect potential malicious traits\nduring the decoding phase. Upon detecting features indicative of toxic tokens,\nFMM regenerates the current token. By employing activation patching, an\nadditional rejection vector is incorporated during the subsequent token\ngeneration, steering the model towards a refusal response. Experimental results\ndemonstrate the effectiveness of our approach across multiple language models\nand diverse attack techniques, while crucially maintaining the models' standard\ngeneration capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09378", "pdf": "https://arxiv.org/pdf/2504.09378", "abs": "https://arxiv.org/abs/2504.09378", "authors": ["Kartik Ravisankar", "Hyojung Han", "Marine Carpuat"], "title": "Can you map it to English? The Role of Cross-Lingual Alignment in Multilingual Performance of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) pre-trained predominantly on English text\nexhibit surprising multilingual capabilities, yet the mechanisms driving\ncross-lingual generalization remain poorly understood. This work investigates\nhow the alignment of representations for text written in different languages\ncorrelates with LLM performance on natural language understanding tasks and\ntranslation tasks, both at the language and the instance level. For this\npurpose, we introduce cross-lingual alignment metrics such as the\nDiscriminative Alignment Index (DALI) to quantify the alignment at an instance\nlevel for discriminative tasks. Through experiments on three natural language\nunderstanding tasks (Belebele, XStoryCloze, XCOPA), and machine translation, we\nfind that while cross-lingual alignment metrics strongly correlate with task\naccuracy at the language level, the sample-level alignment often fails to\ndistinguish correct from incorrect predictions, exposing alignment as a\nnecessary but insufficient condition for success.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09389", "pdf": "https://arxiv.org/pdf/2504.09389", "abs": "https://arxiv.org/abs/2504.09389", "authors": ["Vishakh Padmakumar", "Chen Yueh-Han", "Jane Pan", "Valerie Chen", "He He"], "title": "Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for ideation and\nscientific discovery, it is important to evaluate their ability to generate\nnovel output. Prior work evaluates novelty as the originality with respect to\ntraining data, but original outputs can be low quality. In contrast, non-expert\njudges may favor high-quality but memorized outputs, limiting the reliability\nof human preference as a metric. We propose a new novelty metric for LLM\ngenerations that balances originality and quality -- the harmonic mean of the\nfraction of \\ngrams unseen during training and a task-specific quality score.\nWe evaluate the novelty of generations from two families of open-data models\n(OLMo and Pythia) on three creative tasks: story completion, poetry writing,\nand creative tool use. We find that LLM generated text is less novel than human\nwritten text. To elicit more novel outputs, we experiment with various\ninference-time methods, which reveals a trade-off between originality and\nquality. While these methods can boost novelty, they do so by increasing\noriginality at the expense of quality. In contrast, increasing model size or\napplying post-training reliably shifts the Pareto frontier, highlighting that\nstarting with a stronger base model is a more effective way to improve novelty.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "reliability"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09258", "pdf": "https://arxiv.org/pdf/2504.09258", "abs": "https://arxiv.org/abs/2504.09258", "authors": ["Jianyu Wu", "Hao Yang", "Xinhua Zeng", "Guibing He", "Zhiyu Chen", "Zihui Li", "Xiaochuan Zhang", "Yangyang Ma", "Run Fang", "Yang Liu"], "title": "PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09398", "pdf": "https://arxiv.org/pdf/2504.09398", "abs": "https://arxiv.org/abs/2504.09398", "authors": ["Gaurav Kumar", "Murali Mohana Krishna Dandu"], "title": "Composable NLP Workflows for BERT-based Ranking and QA System", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, 6 tables", "summary": "There has been a lot of progress towards building NLP models that scale to\nmultiple tasks. However, real-world systems contain multiple components and it\nis tedious to handle cross-task interaction with varying levels of text\ngranularity. In this work, we built an end-to-end Ranking and\nQuestion-Answering (QA) system using Forte, a toolkit that makes composable NLP\npipelines. We utilized state-of-the-art deep learning models such as BERT,\nRoBERTa in our pipeline, evaluated the performance on MS-MARCO and Covid-19\ndatasets using metrics such as BLUE, MRR, F1 and compared the results of\nranking and QA systems with their corresponding benchmark results. The modular\nnature of our pipeline and low latency of reranker makes it easy to build\ncomplex NLP applications easily.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09298", "pdf": "https://arxiv.org/pdf/2504.09298", "abs": "https://arxiv.org/abs/2504.09298", "authors": ["Tinh-Anh Nguyen-Nhu", "Huu-Loc Tran", "Nguyen-Khang Le", "Minh-Nhat Nguyen", "Tien-Huy Nguyen", "Hoang-Long Nguyen-Huu", "Huu-Phong Phan-Nguyen", "Huy-Thach Pham", "Quan Nguyen", "Hoang M. Le", "Quang-Vinh Dinh"], "title": "A Lightweight Moment Retrieval System with Global Re-Ranking and Robust Adaptive Bidirectional Temporal Search", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of digital video content has posed critical challenges\nin moment-level video retrieval, where existing methodologies struggle to\nefficiently localize specific segments within an expansive video corpus.\nCurrent retrieval systems are constrained by computational inefficiencies,\ntemporal context limitations, and the intrinsic complexity of navigating video\ncontent. In this paper, we address these limitations through a novel\nInteractive Video Corpus Moment Retrieval framework that integrates a\nSuperGlobal Reranking mechanism and Adaptive Bidirectional Temporal Search\n(ABTS), strategically optimizing query similarity, temporal stability, and\ncomputational resources. By preprocessing a large corpus of videos using a\nkeyframe extraction model and deduplication technique through image hashing,\nour approach provides a scalable solution that significantly reduces storage\nrequirements while maintaining high localization precision across diverse video\nrepositories.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09696", "pdf": "https://arxiv.org/pdf/2504.09696", "abs": "https://arxiv.org/abs/2504.09696", "authors": ["Jixiao Zhang", "Chunsheng Zuo"], "title": "GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in R1-like reasoning models leveraging Group Relative Policy\nOptimization (GRPO) have significantly improved the performance of language\nmodels on mathematical reasoning tasks. However, current GRPO implementations\nencounter critical challenges, including reward sparsity due to binary accuracy\nmetrics, limited incentives for conciseness, and insufficient focus on complex\nreasoning tasks. To address these issues, we propose GRPO-LEAD, a suite of\nnovel enhancements tailored for mathematical reasoning. Specifically, GRPO-LEAD\nintroduces (1) a length-dependent accuracy reward to encourage concise and\nprecise solutions, (2) an explicit penalty mechanism for incorrect answers to\nsharpen decision boundaries, and (3) a difficulty-aware advantage reweighting\nstrategy that amplifies learning signals for challenging problems. Furthermore,\nwe systematically examine the impact of model scale and supervised fine-tuning\n(SFT) strategies, demonstrating that larger-scale base models and carefully\ncurated datasets significantly enhance reinforcement learning effectiveness.\nExtensive empirical evaluations and ablation studies confirm that GRPO-LEAD\nsubstantially mitigates previous shortcomings, resulting in language models\nthat produce more concise, accurate, and robust reasoning across diverse\nmathematical tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09472", "pdf": "https://arxiv.org/pdf/2504.09472", "abs": "https://arxiv.org/abs/2504.09472", "authors": ["Pooja Guhan", "Divya Kothandaraman", "Tsung-Wei Huang", "Guan-Ming Su", "Dinesh Manocha"], "title": "CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce CamMimic, an innovative algorithm tailored for dynamic video\nediting needs. It is designed to seamlessly transfer the camera motion observed\nin a given reference video onto any scene of the user's choice in a zero-shot\nmanner without requiring any additional data. Our algorithm achieves this using\na two-phase strategy by leveraging a text-to-video diffusion model. In the\nfirst phase, we develop a multi-concept learning method using a combination of\nLoRA layers and an orthogonality loss to capture and understand the underlying\nspatial-temporal characteristics of the reference video as well as the spatial\nfeatures of the user's desired scene. The second phase proposes a unique\nhomography-based refinement strategy to enhance the temporal and spatial\nalignment of the generated video. We demonstrate the efficacy of our method\nthrough experiments conducted on a dataset containing combinations of diverse\nscenes and reference videos containing a variety of camera motions. In the\nabsence of an established metric for assessing camera motion transfer between\nunrelated scenes, we propose CameraScore, a novel metric that utilizes\nhomography representations to measure camera motion similarity between the\nreference and generated videos. Extensive quantitative and qualitative\nevaluations demonstrate that our approach generates high-quality,\nmotion-enhanced videos. Additionally, a user study reveals that 70.31% of\nparticipants preferred our method for scene preservation, while 90.45% favored\nit for motion transfer. We hope this work lays the foundation for future\nadvancements in camera motion transfer across different scenes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09498", "pdf": "https://arxiv.org/pdf/2504.09498", "abs": "https://arxiv.org/abs/2504.09498", "authors": ["Yue Yang", "Christoph Leuze", "Brian Hargreaves", "Bruce Daniel", "Fred Baik"], "title": "EasyREG: Easy Depth-Based Markerless Registration and Tracking using Augmented Reality Device for Surgical Guidance", "categories": ["cs.CV"], "comment": null, "summary": "The use of Augmented Reality (AR) devices for surgical guidance has gained\nincreasing traction in the medical field. Traditional registration methods\noften rely on external fiducial markers to achieve high accuracy and real-time\nperformance. However, these markers introduce cumbersome calibration procedures\nand can be challenging to deploy in clinical settings. While commercial\nsolutions have attempted real-time markerless tracking using the native RGB\ncameras of AR devices, their accuracy remains questionable for medical\nguidance, primarily due to occlusions and significant outliers between the live\nsensor data and the preoperative target anatomy point cloud derived from MRI or\nCT scans. In this work, we present a markerless framework that relies only on\nthe depth sensor of AR devices and consists of two modules: a registration\nmodule for high-precision, outlier-robust target anatomy localization, and a\ntracking module for real-time pose estimation. The registration module\nintegrates depth sensor error correction, a human-in-the-loop region filtering\ntechnique, and a robust global alignment with curvature-aware feature sampling,\nfollowed by local ICP refinement, for markerless alignment of preoperative\nmodels with patient anatomy. The tracking module employs a fast and robust\nregistration algorithm that uses the initial pose from the registration module\nto estimate the target pose in real-time. We comprehensively evaluated the\nperformance of both modules through simulation and real-world measurements. The\nresults indicate that our markerless system achieves superior performance for\nregistration and comparable performance for tracking to industrial solutions.\nThe two-module design makes our system a one-stop solution for surgical\nprocedures where the target anatomy moves or stays static during surgery.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09506", "pdf": "https://arxiv.org/pdf/2504.09506", "abs": "https://arxiv.org/abs/2504.09506", "authors": ["Yanze Jiang", "Yanfeng Gu", "Xian Li"], "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial\nand spectral information of ground objects, offering excellent 3D perception\nand target recognition capabilities. Current approaches for generating HPCs\noften involve fusion techniques with hyperspectral images and LiDAR point\nclouds, which inevitably lead to geometric-spectral distortions due to fusion\nerrors and obstacle occlusions. These adverse effects limit their performance\nin downstream fine-grained tasks across multiple scenarios, particularly in\nairborne applications. To address these issues, we propose PiV-AHPC, a 3D\nobject detection network for airborne HPCs. To the best of our knowledge, this\nis the first attempt at this HPCs task. Specifically, we first develop a\npillar-voxel dual-branch encoder, where the former captures spectral and\nvertical structural features from HPCs to overcome spectral distortion, while\nthe latter emphasizes extracting accurate 3D spatial features from point\nclouds. A multi-level feature fusion mechanism is devised to enhance\ninformation interaction between the two branches, achieving neighborhood\nfeature alignment and channel-adaptive selection, thereby organically\nintegrating heterogeneous features and mitigating geometric distortion.\nExtensive experiments on two airborne HPCs datasets demonstrate that PiV-AHPC\npossesses state-of-the-art detection performance and high generalization\ncapability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09518", "pdf": "https://arxiv.org/pdf/2504.09518", "abs": "https://arxiv.org/abs/2504.09518", "authors": ["Ting Huang", "Zeyu Zhang", "Yemin Wang", "Hao Tang"], "title": "3D CoCa: Contrastive Learners are 3D Captioners", "categories": ["cs.CV"], "comment": null, "summary": "3D captioning, which aims to describe the content of 3D scenes in natural\nlanguage, remains highly challenging due to the inherent sparsity of point\nclouds and weak cross-modal alignment in existing methods. To address these\nchallenges, we propose 3D CoCa, a novel unified framework that seamlessly\ncombines contrastive vision-language learning with 3D caption generation in a\nsingle architecture. Our approach leverages a frozen CLIP vision-language\nbackbone to provide rich semantic priors, a spatially-aware 3D scene encoder to\ncapture geometric context, and a multi-modal decoder to generate descriptive\ncaptions. Unlike prior two-stage methods that rely on explicit object\nproposals, 3D CoCa jointly optimizes contrastive and captioning objectives in a\nshared feature space, eliminating the need for external detectors or\nhandcrafted proposals. This joint training paradigm yields stronger spatial\nreasoning and richer semantic grounding by aligning 3D and textual\nrepresentations. Extensive experiments on the ScanRefer and Nr3D benchmarks\ndemonstrate that 3D CoCa significantly outperforms current state-of-the-arts by\n10.2% and 5.76% in CIDEr at 0.5IoU, respectively. Code will be available at\nhttps://github.com/AIGeeksGroup/3DCoCa.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09528", "pdf": "https://arxiv.org/pdf/2504.09528", "abs": "https://arxiv.org/abs/2504.09528", "authors": ["Xing Zi", "Tengjun Ni", "Xianjing Fan", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "AeroLite: Tag-Guided Lightweight Generation of Aerial Image Captions", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate and automated captioning of aerial imagery is crucial for\napplications like environmental monitoring, urban planning, and disaster\nmanagement. However, this task remains challenging due to complex spatial\nsemantics and domain variability. To address these issues, we introduce\n\\textbf{AeroLite}, a lightweight, tag-guided captioning framework designed to\nequip small-scale language models (1--3B parameters) with robust and\ninterpretable captioning capabilities specifically for remote sensing images.\n\\textbf{AeroLite} leverages GPT-4o to generate a large-scale, semantically rich\npseudo-caption dataset by integrating multiple remote sensing benchmarks,\nincluding DLRSD, iSAID, LoveDA, WHU, and RSSCN7. To explicitly capture key\nsemantic elements such as orientation and land-use types, AeroLite employs\nnatural language processing techniques to extract relevant semantic tags. These\ntags are then learned by a dedicated multi-label CLIP encoder, ensuring precise\nsemantic predictions. To effectively fuse visual and semantic information, we\npropose a novel bridging multilayer perceptron (MLP) architecture, aligning\nsemantic tags with visual embeddings while maintaining minimal computational\noverhead. AeroLite's flexible design also enables seamless integration with\nvarious pretrained large language models. We adopt a two-stage LoRA-based\ntraining approach: the initial stage leverages our pseudo-caption dataset to\ncapture broad remote sensing semantics, followed by fine-tuning on smaller,\ncurated datasets like UCM and Sydney Captions to refine domain-specific\nalignment. Experimental evaluations demonstrate that AeroLite surpasses\nsignificantly larger models (e.g., 13B parameters) in standard captioning\nmetrics, including BLEU and METEOR, while maintaining substantially lower\ncomputational costs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09910", "pdf": "https://arxiv.org/pdf/2504.09910", "abs": "https://arxiv.org/abs/2504.09910", "authors": ["Yujing Wang", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Binghui Guo", "Hongwei Zheng", "Zhiming Zheng"], "title": "Learning to Erase Private Knowledge from Multi-Documents for Retrieval-Augmented Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a promising technique for applying\nLLMs to proprietary domains. However, retrieved documents may contain sensitive\nknowledge, posing risks of privacy leakage in generative results. Thus,\neffectively erasing private information from retrieved documents is a key\nchallenge for RAG. Unlike traditional text anonymization, RAG should consider:\n(1) the inherent multi-document reasoning may face de-anonymization attacks;\n(2) private knowledge varies by scenarios, so users should be allowed to\ncustomize which information to erase; (3) preserving sufficient publicly\navailable knowledge for generation tasks. This paper introduces the privacy\nerasure task for RAG and proposes Eraser4RAG, a private knowledge eraser which\neffectively removes user-defined private knowledge from documents while\npreserving sufficient public knowledge for generation. Specifically, we first\nconstruct a global knowledge graph to identify potential knowledge across\ndocuments, aiming to defend against de-anonymization attacks. Then we randomly\nsplit it into private and public sub-graphs, and fine-tune Flan-T5 to rewrite\nthe retrieved documents excluding private triples. Finally, PPO algorithm\noptimizes the rewriting model to minimize private triples and maximize public\ntriples retention. Experiments on four QA datasets demonstrate that Eraser4RAG\nachieves superior erase performance than GPT-4o.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09555", "pdf": "https://arxiv.org/pdf/2504.09555", "abs": "https://arxiv.org/abs/2504.09555", "authors": ["Jinhao Li", "Zijian Chen", "Runze Dong", "Tingzhu Chen", "Changbo Wang", "Guangtao Zhai"], "title": "Mitigating Long-tail Distribution in Oracle Bone Inscriptions: Dataset, Model, and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The oracle bone inscription (OBI) recognition plays a significant role in\nunderstanding the history and culture of ancient China. However, the existing\nOBI datasets suffer from a long-tail distribution problem, leading to biased\nperformance of OBI recognition models across majority and minority classes.\nWith recent advancements in generative models, OBI synthesis-based data\naugmentation has become a promising avenue to expand the sample size of\nminority classes. Unfortunately, current OBI datasets lack large-scale\nstructure-aligned image pairs for generative model training. To address these\nproblems, we first present the Oracle-P15K, a structure-aligned OBI dataset for\nOBI generation and denoising, consisting of 14,542 images infused with domain\nknowledge from OBI experts. Second, we propose a diffusion model-based pseudo\nOBI generator, called OBIDiff, to achieve realistic and controllable OBI\ngeneration. Given a clean glyph image and a target rubbing-style image, it can\neffectively transfer the noise style of the original rubbing to the glyph\nimage. Extensive experiments on OBI downstream tasks and user preference\nstudies show the effectiveness of the proposed Oracle-P15K dataset and\ndemonstrate that OBIDiff can accurately preserve inherent glyph structures\nwhile transferring authentic rubbing styles effectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09588", "pdf": "https://arxiv.org/pdf/2504.09588", "abs": "https://arxiv.org/abs/2504.09588", "authors": ["Zhicong Wu", "Hongbin Xu", "Gang Xu", "Ping Nie", "Zhixin Yan", "Jinkai Zheng", "Liangqiong Qu", "Ming Li", "Liqiang Nie"], "title": "TextSplat: Text-Guided Semantic Fusion for Generalizable Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Generalizable Gaussian Splatting have enabled robust\n3D reconstruction from sparse input views by utilizing feed-forward Gaussian\nSplatting models, achieving superior cross-scene generalization. However, while\nmany methods focus on geometric consistency, they often neglect the potential\nof text-driven guidance to enhance semantic understanding, which is crucial for\naccurately reconstructing fine-grained details in complex scenes. To address\nthis limitation, we propose TextSplat--the first text-driven Generalizable\nGaussian Splatting framework. By employing a text-guided fusion of diverse\nsemantic cues, our framework learns robust cross-modal feature representations\nthat improve the alignment of geometric and semantic information, producing\nhigh-fidelity 3D reconstructions. Specifically, our framework employs three\nparallel modules to obtain complementary representations: the Diffusion Prior\nDepth Estimator for accurate depth information, the Semantic Aware Segmentation\nNetwork for detailed semantic information, and the Multi-View Interaction\nNetwork for refined cross-view features. Then, in the Text-Guided Semantic\nFusion Module, these representations are integrated via the text-guided and\nattention-based feature aggregation mechanism, resulting in enhanced 3D\nGaussian parameters enriched with detailed semantic cues. Experimental results\non various benchmark datasets demonstrate improved performance compared to\nexisting methods across multiple evaluation metrics, validating the\neffectiveness of our framework. The code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10157", "pdf": "https://arxiv.org/pdf/2504.10157", "abs": "https://arxiv.org/abs/2504.10157", "authors": ["Xinnong Zhang", "Jiayu Lin", "Xinyi Mou", "Shiyue Yang", "Xiawei Liu", "Libo Sun", "Hanjia Lyu", "Yihang Yang", "Weihong Qi", "Yue Chen", "Guanying Li", "Ling Yan", "Yao Hu", "Siming Chen", "Yu Wang", "Jingxuan Huang", "Jiebo Luo", "Shiping Tang", "Libo Wu", "Baohua Zhou", "Zhongyu Wei"], "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users", "categories": ["cs.CL", "cs.CY"], "comment": "work in progress", "summary": "Social simulation is transforming traditional social science research by\nmodeling human behavior through interactions between virtual individuals and\ntheir environments. With recent advances in large language models (LLMs), this\napproach has shown growing potential in capturing individual differences and\npredicting group behaviors. However, existing methods face alignment challenges\nrelated to the environment, target users, interaction mechanisms, and\nbehavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven\nworld model for social simulation. Our framework features four powerful\nalignment components and a user pool of 10 million real individuals. To\nvalidate its effectiveness, we conducted large-scale simulation experiments\nacross three distinct domains: politics, news, and economics. Results\ndemonstrate that SocioVerse can reflect large-scale population dynamics while\nensuring diversity, credibility, and representativeness through standardized\nprocedures and minimal manual adjustments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09608", "pdf": "https://arxiv.org/pdf/2504.09608", "abs": "https://arxiv.org/abs/2504.09608", "authors": ["Xingke Song", "Xiaoying Yang", "Chenglin Yao", "Jianfeng Ren", "Ruibin Bai", "Xin Chen", "Xudong Jiang"], "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Solving jigsaw puzzles has been extensively studied. While most existing\nmodels focus on solving either small-scale puzzles or puzzles with no gap\nbetween fragments, solving large-scale puzzles with gaps presents distinctive\nchallenges in both image understanding and combinatorial optimization. To\ntackle these challenges, we propose a framework of Evolutionary Reinforcement\nLearning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of\nswapping actions for solving the puzzles. Specifically, to tackle the\nchallenges of perceiving the puzzle with gaps, a Multi-head Puzzle Perception\nNetwork (MPPN) with a shared encoder is designed, where multiple puzzlet heads\ncomprehensively perceive the local assembly status, and a discriminator head\nprovides a global assessment of the puzzle. To explore the large swapping\naction space efficiently, an Evolutionary Reinforcement Learning (EvoRL) agent\nis designed, where an actor recommends a set of suitable swapping actions from\na large action space based on the perceived puzzle status, a critic updates the\nactor using the estimated rewards and the puzzle status, and an evaluator\ncoupled with evolutionary strategies evolves the actions aligning with the\nhistorical assembly experience. The proposed ERL-MPP is comprehensively\nevaluated on the JPLEG-5 dataset with large gaps and the MIT dataset with\nlarge-scale puzzles. It significantly outperforms all state-of-the-art models\non both datasets.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10160", "pdf": "https://arxiv.org/pdf/2504.10160", "abs": "https://arxiv.org/abs/2504.10160", "authors": ["Zhaopeng Feng", "Shaosheng Cao", "Jiahan Ren", "Jiayuan Su", "Ruizhe Chen", "Yan Zhang", "Zhe Xu", "Yao Hu", "Jian Wu", "Zuozhu Liu"], "title": "MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress. Our code is available at\n  https://github.com/fzp0424/MT-R1-Zero", "summary": "Large-scale reinforcement learning (RL) methods have proven highly effective\nin enhancing the reasoning abilities of large language models (LLMs),\nparticularly for tasks with verifiable solutions such as mathematics and\ncoding. However, applying this idea to machine translation (MT), where outputs\nare flexibly formatted and difficult to automatically evaluate with explicit\nrules, remains underexplored. In this work, we introduce MT-R1-Zero, the first\nopen-source adaptation of the R1-Zero RL framework for MT without supervised\nfine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to\nguide LLMs towards improved translation quality via emergent reasoning. On the\nWMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive\nperformance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points.\nMeanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across\nall metrics, placing it on par with advanced proprietary models such as GPT-4o\nand Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves\nstate-of-the-art scores on semantic metrics. Moreover, our work exhibits strong\ngeneralization capabilities on out-of-distribution MT tasks, robustly\nsupporting multilingual and low-resource settings. Extensive analysis of model\nbehavior across different initializations and reward metrics offers pioneering\ninsight into the critical role of reward design, LLM adaptability, training\ndynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT.\nOur code is available at https://github.com/fzp0424/MT-R1-Zero.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09641", "pdf": "https://arxiv.org/pdf/2504.09641", "abs": "https://arxiv.org/abs/2504.09641", "authors": ["Xingjian Zhang", "Siwei Wen", "Wenjun Wu", "Lei Huang"], "title": "TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recently, improving the reasoning ability of large multimodal models (LMMs)\nthrough reinforcement learning has made great progress. However, most existing\nworks are based on highly reasoning-intensive datasets such as mathematics and\ncode, and researchers generally choose large-scale models as the foundation. We\nargue that exploring small-scale models' reasoning capabilities remains\nvaluable for researchers with limited computational resources. Moreover,\nenabling models to explain their reasoning processes on general\nquestion-answering datasets is equally meaningful. Therefore, we present the\nsmall-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video,\na traceably trained video understanding model with no more than 4B parameters,\nit not only demonstrates significantly improved reasoning and thinking\ncapabilities after using reinforcement learning on general Video-QA datasets,\nbut also exhibits the emergent characteristic of \"aha moments\". Furthermore, we\nshare a series of experimental findings, aiming to provide practical insights\nfor future exploration of video reasoning (thinking) abilities in small-scale\nmodels. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "reasoning model"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09700", "pdf": "https://arxiv.org/pdf/2504.09700", "abs": "https://arxiv.org/abs/2504.09700", "authors": ["Zijian Wu", "Shuojue Yang", "Yueming Jin", "Septimiu E Salcudean"], "title": "ToolTipNet: A Segmentation-Driven Deep Learning Baseline for Surgical Instrument Tip Detection", "categories": ["cs.CV"], "comment": null, "summary": "In robot-assisted laparoscopic radical prostatectomy (RALP), the location of\nthe instrument tip is important to register the ultrasound frame with the\nlaparoscopic camera frame. A long-standing limitation is that the instrument\ntip position obtained from the da Vinci API is inaccurate and requires hand-eye\ncalibration. Thus, directly computing the position of the tool tip in the\ncamera frame using the vision-based method becomes an attractive solution.\nBesides, surgical instrument tip detection is the key component of other tasks,\nlike surgical skill assessment and surgery automation. However, this task is\nchallenging due to the small size of the tool tip and the articulation of the\nsurgical instrument. Surgical instrument segmentation becomes relatively easy\ndue to the emergence of the Segmentation Foundation Model, i.e., Segment\nAnything. Based on this advancement, we explore the deep learning-based\nsurgical instrument tip detection approach that takes the part-level instrument\nsegmentation mask as input. Comparison experiments with a hand-crafted\nimage-processing approach demonstrate the superiority of the proposed method on\nsimulated and real datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10340", "pdf": "https://arxiv.org/pdf/2504.10340", "abs": "https://arxiv.org/abs/2504.10340", "authors": ["Shahriar Noroozizadeh", "Sayantan Kumar", "Jeremy C. Weiss"], "title": "Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families", "categories": ["cs.CL", "cs.AI"], "comment": "Machine Learning for Healthcare (MLHC 2025)", "summary": "Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings--extracted via an\nLLM-assisted annotation pipeline--serve as the primary input for prediction. We\nsystematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10356", "pdf": "https://arxiv.org/pdf/2504.10356", "abs": "https://arxiv.org/abs/2504.10356", "authors": ["Dieuwke Hupkes", "Nikolay Bogoychev"], "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages", "categories": ["cs.CL"], "comment": null, "summary": "We present MultiLoKo, a new benchmark for evaluating multilinguality in LLMs\ncovering 31 languages. MultiLoKo consists of three partitions: a main partition\nconsisting of 500 questions per language, separately sourced to be locally\nrelevant to the specific language, and two translated partitions, containing\nhuman-authored translations from 30 non-English languages to English and vice\nversa. For comparison, we also release corresponding machine-authored\ntranslations. The data is equally distributed over two splits: a dev split and\na blind, out-of-distribution test split. MultiLoKo can be used to study a\nvariety of questions regarding the multilinguality of LLMs as well as\nmeta-questions about multilingual benchmark creation. We compute MultiLoKo\nscores for 11 base and chat models marketed to be multilingual and study their\naverage performance, their performance parity across languages, how much their\nability to answer questions depends on the question language, and which\nlanguages are most difficult. None of the models we studied performs well on\nMultiLoKo, as indicated by low average scores as well as large differences\nbetween the best and worst scoring languages. Furthermore, we find a\nsubstantial effect of the question language, indicating sub-optimal knowledge\ntransfer between languages. Lastly, we find that using local vs\nEnglish-translated data can result in differences more than 20 points for the\nbest performing models, drastically change the estimated difficulty of some\nlanguages. For using machines instead of human translations, we find a weaker\neffect on ordering of language difficulty, a larger difference in model\nrankings, and a substantial drop in estimated performance for all models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09789", "pdf": "https://arxiv.org/pdf/2504.09789", "abs": "https://arxiv.org/abs/2504.09789", "authors": ["Chao Liu", "Arash Vahdat"], "title": "EquiVDM: Equivariant Video Diffusion Models with Temporally Consistent Noise", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporally consistent video-to-video generation is essential for applications\nof video diffusion models in areas such as sim-to-real, style-transfer, video\nupsampling, etc. In this paper, we propose a video diffusion framework that\nleverages temporally consistent noise to generate coherent video frames without\nspecialized modules or additional constraints. We show that the standard\ntraining objective of diffusion models, when applied with temporally consistent\nnoise, encourages the model to be equivariant to spatial transformations in\ninput video and noise. This enables our model to better follow motion patterns\nfrom the input video, producing aligned motion and high-fidelity frames.\nFurthermore, we extend our approach to 3D-consistent video generation by\nattaching noise as textures on 3D meshes, ensuring 3D consistency in\nsim-to-real applications. Experimental results demonstrate that our method\nsurpasses state-of-the-art baselines in motion alignment, 3D consistency, and\nvideo quality while requiring only a few sampling steps in practice.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09852", "pdf": "https://arxiv.org/pdf/2504.09852", "abs": "https://arxiv.org/abs/2504.09852", "authors": ["Boris Kriuk", "Simranjit Kaur Gill", "Shoaib Aslam", "Amir Fakhrutdinov"], "title": "GFT: Gradient Focal Transformer", "categories": ["cs.CV"], "comment": "11 pages, 3 tables, 5 figures", "summary": "Fine-Grained Image Classification (FGIC) remains a complex task in computer\nvision, as it requires models to distinguish between categories with subtle\nlocalized visual differences. Well-studied CNN-based models, while strong in\nlocal feature extraction, often fail to capture the global context required for\nfine-grained recognition, while more recent ViT-backboned models address FGIC\nwith attention-driven mechanisms but lack the ability to adaptively focus on\ntruly discriminative regions. TransFG and other ViT-based extensions introduced\npart-aware token selection to enhance attention localization, yet they still\nstruggle with computational efficiency, attention region selection flexibility,\nand detail-focus narrative in complex environments. This paper introduces GFT\n(Gradient Focal Transformer), a new ViT-derived framework created for FGIC\ntasks. GFT integrates the Gradient Attention Learning Alignment (GALA)\nmechanism to dynamically prioritize class-discriminative features by analyzing\nattention gradient flow. Coupled with a Progressive Patch Selection (PPS)\nstrategy, the model progressively filters out less informative regions,\nreducing computational overhead while enhancing sensitivity to fine details.\nGFT achieves SOTA accuracy on FGVC Aircraft, Food-101, and COCO datasets with\n93M parameters, outperforming ViT-based advanced FGIC models in efficiency. By\nbridging global context and localized detail extraction, GFT sets a new\nbenchmark in fine-grained recognition, offering interpretable solutions for\nreal-world deployment scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10430", "pdf": "https://arxiv.org/pdf/2504.10430", "abs": "https://arxiv.org/abs/2504.10430", "authors": ["Minqian Liu", "Zhiyang Xu", "Xinyi Zhang", "Heajun An", "Sarvech Qadir", "Qi Zhang", "Pamela J. Wisniewski", "Jin-Hee Cho", "Sang Won Lee", "Ruoxi Jia", "Lifu Huang"], "title": "LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "20 pages, 7 figures, 4 tables", "summary": "Recent advancements in Large Language Models (LLMs) have enabled them to\napproach human-level persuasion capabilities. However, such potential also\nraises concerns about the safety risks of LLM-driven persuasion, particularly\ntheir potential for unethical influence through manipulation, deception,\nexploitation of vulnerabilities, and many other harmful tactics. In this work,\nwe present a systematic investigation of LLM persuasion safety through two\ncritical aspects: (1) whether LLMs appropriately reject unethical persuasion\ntasks and avoid unethical strategies during execution, including cases where\nthe initial persuasion goal appears ethically neutral, and (2) how influencing\nfactors like personality traits and external pressures affect their behavior.\nTo this end, we introduce PersuSafety, the first comprehensive framework for\nthe assessment of persuasion safety which consists of three stages, i.e.,\npersuasion scene creation, persuasive conversation simulation, and persuasion\nsafety assessment. PersuSafety covers 6 diverse unethical persuasion topics and\n15 common unethical strategies. Through extensive experiments across 8 widely\nused LLMs, we observe significant safety concerns in most LLMs, including\nfailing to identify harmful persuasion tasks and leveraging various unethical\npersuasion strategies. Our study calls for more attention to improve safety\nalignment in progressive and goal-driven conversations such as persuasion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08745", "pdf": "https://arxiv.org/pdf/2504.08745", "abs": "https://arxiv.org/abs/2504.08745", "authors": ["Mert Yazan", "Suzan Verberne", "Frederik Situmeang"], "title": "Improving RAG for Personalization with Author Features and Contrastive Examples", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalization with retrieval-augmented generation (RAG) often fails to\ncapture fine-grained features of authors, making it hard to identify their\nunique traits. To enrich the RAG context, we propose providing Large Language\nModels (LLMs) with author-specific features, such as average sentiment polarity\nand frequently used words, in addition to past samples from the author's\nprofile. We introduce a new feature called Contrastive Examples: documents from\nother authors are retrieved to help LLM identify what makes an author's style\nunique in comparison to others. Our experiments show that adding a couple of\nsentences about the named entities, dependency patterns, and words a person\nuses frequently significantly improves personalized text generation. Combining\nfeatures with contrastive examples boosts the performance further, achieving a\nrelative 15% improvement over baseline RAG while outperforming the benchmarks.\nOur results show the value of fine-grained features for better personalization,\nwhile opening a new research dimension for including contrastive examples as a\ncomplement with RAG. We release our code publicly.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained", "dimension"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09925", "pdf": "https://arxiv.org/pdf/2504.09925", "abs": "https://arxiv.org/abs/2504.09925", "authors": ["Zheng Liu", "Mengjie Liu", "Jingzhou Chen", "Jingwei Xu", "Bin Cui", "Conghui He", "Wentao Zhang"], "title": "FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce FUSION, a family of multimodal large language models (MLLMs)\nwith a fully vision-language alignment and integration paradigm. Unlike\nexisting methods that primarily rely on late-stage modality interaction during\nLLM decoding, our approach achieves deep, dynamic integration throughout the\nentire processing pipeline. To this end, we propose Text-Guided Unified Vision\nEncoding, incorporating textual information in vision encoding to achieve\npixel-level integration. We further design Context-Aware Recursive Alignment\nDecoding that recursively aggregates visual features conditioned on textual\ncontext during decoding, enabling fine-grained, question-level semantic\nintegration. To guide feature mapping and mitigate modality discrepancies, we\ndevelop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a\nSynthesized Language-Driven Question-Answer (QA) dataset through a new data\nsynthesis method, prioritizing high-quality QA pairs to optimize text-guided\nfeature integration. Building on these foundations, we train FUSION at two\nscales-3B, 8B-and demonstrate that our full-modality integration approach\nsignificantly outperforms existing methods with only 630 vision tokens.\nNotably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most\nbenchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited\nto 300 vision tokens. Our ablation studies show that FUSION outperforms\nLLaVA-NeXT on over half of the benchmarks under same configuration without\ndynamic resolution, highlighting the effectiveness of our approach. We release\nour code, model weights, and dataset. https://github.com/starriver030515/FUSION", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08846", "pdf": "https://arxiv.org/pdf/2504.08846", "abs": "https://arxiv.org/abs/2504.08846", "authors": ["Mostafa Faghih Shojaei", "Rahul Gulati", "Benjamin A. Jasperson", "Shangshang Wang", "Simone Cimolato", "Dangli Cao", "Willie Neiswanger", "Krishna Garikipati"], "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09967", "pdf": "https://arxiv.org/pdf/2504.09967", "abs": "https://arxiv.org/abs/2504.09967", "authors": ["Xun Zhu", "Fanbin Mo", "Zheng Zhang", "Jiaxi Wang", "Yiming Shi", "Ming Wu", "Chuang Zhang", "Miao Li", "Ji Wu"], "title": "Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of medical generalist foundation models has revolutionized\nconventional task-specific model development paradigms, aiming to better handle\nmultiple tasks through joint training on large-scale medical datasets. However,\nrecent advances prioritize simple data scaling or architectural component\nenhancement, while neglecting to re-examine multi-task learning from a\ndata-centric perspective. Critically, simply aggregating existing data\nresources leads to decentralized image-task alignment, which fails to cultivate\ncomprehensive image understanding or align with clinical needs for\nmulti-dimensional image interpretation. In this paper, we introduce the\nimage-centric multi-annotation X-ray dataset (IMAX), the first attempt to\nenhance the multi-task learning capabilities of medical multi-modal large\nlanguage models (MLLMs) from the data construction level. To be specific, IMAX\nis featured from the following attributes: 1) High-quality data curation. A\ncomprehensive collection of more than 354K entries applicable to seven\ndifferent medical tasks. 2) Image-centric dense annotation. Each X-ray image is\nassociated with an average of 4.10 tasks and 7.46 training entries, ensuring\nmulti-task representation richness per image. Compared to the general\ndecentralized multi-annotation X-ray dataset (DMAX), IMAX consistently\ndemonstrates significant multi-task average performance gains ranging from\n3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs.\nMoreover, we investigate differences in statistical patterns exhibited by IMAX\nand DMAX training processes, exploring potential correlations between\noptimization dynamics and multi-task performance. Finally, leveraging the core\nconcept of IMAX data construction, we propose an optimized DMAX-based training\nstrategy to alleviate the dilemma of obtaining high-quality IMAX data in\npractical scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation", "multi-dimensional"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09973", "pdf": "https://arxiv.org/pdf/2504.09973", "abs": "https://arxiv.org/abs/2504.09973", "authors": ["Gang Wu", "Junjun Jiang", "Kui Jiang", "Xianming Liu", "Liqiang Nie"], "title": "Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Aitical/CPLIR", "summary": "All-in-one image restoration, addressing diverse degradation types with a\nunified model, presents significant challenges in designing task-specific\nprompts that effectively guide restoration across multiple degradation\nscenarios. While adaptive prompt learning enables end-to-end optimization, it\noften yields overlapping or redundant task representations. Conversely,\nexplicit prompts derived from pretrained classifiers enhance discriminability\nbut may discard critical visual information for reconstruction. To address\nthese limitations, we introduce Contrastive Prompt Learning (CPL), a novel\nframework that fundamentally enhances prompt-task alignment through two\ncomplementary innovations: a \\emph{Sparse Prompt Module (SPM)} that efficiently\ncaptures degradation-specific features while minimizing redundancy, and a\n\\emph{Contrastive Prompt Regularization (CPR)} that explicitly strengthens task\nboundaries by incorporating negative prompt samples across different\ndegradation types. Unlike previous approaches that focus primarily on\ndegradation classification, CPL optimizes the critical interaction between\nprompts and the restoration model itself. Extensive experiments across five\ncomprehensive benchmarks demonstrate that CPL consistently enhances\nstate-of-the-art all-in-one restoration models, achieving significant\nimprovements in both standard multi-task scenarios and challenging composite\ndegradation settings. Our framework establishes new state-of-the-art\nperformance while maintaining parameter efficiency, offering a principled\nsolution for unified image restoration.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.08949", "pdf": "https://arxiv.org/pdf/2504.08949", "abs": "https://arxiv.org/abs/2504.08949", "authors": ["Haokai Ma", "Yunshan Ma", "Ruobing Xie", "Lei Meng", "Jialie Shen", "Xingwu Sun", "Zhanhui Kang", "Tat-Seng Chua"], "title": "Large Language Model Empowered Recommendation Meets All-domain Continual Pre-Training", "categories": ["cs.IR", "cs.CL"], "comment": "In submission", "summary": "Recent research efforts have investigated how to integrate Large Language\nModels (LLMs) into recommendation, capitalizing on their semantic comprehension\nand open-world knowledge for user behavior understanding. These approaches\npredominantly employ supervised fine-tuning on single-domain user interactions\nto adapt LLMs for specific recommendation tasks. However, they typically\nencounter dual challenges: the mismatch between general language\nrepresentations and domain-specific preference patterns, as well as the limited\nadaptability to multi-domain recommendation scenarios. To bridge these gaps, we\nintroduce CPRec -- an All-domain Continual Pre-Training framework for\nRecommendation -- designed to holistically align LLMs with universal user\nbehaviors through the continual pre-training paradigm. Specifically, we first\ndesign a unified prompt template and organize users' multi-domain behaviors\ninto domain-specific behavioral sequences and all-domain mixed behavioral\nsequences that emulate real-world user decision logic. To optimize behavioral\nknowledge infusion, we devise a Warmup-Stable-Annealing learning rate schedule\ntailored for the continual pre-training paradigm in recommendation to\nprogressively enhance the LLM's capability in knowledge adaptation from\nopen-world knowledge to universal recommendation tasks. To evaluate the\neffectiveness of our CPRec, we implement it on a large-scale dataset covering\nseven domains and conduct extensive experiments on five real-world datasets\nfrom two distinct platforms. Experimental results confirm that our continual\npre-training paradigm significantly mitigates the semantic-behavioral\ndiscrepancy and achieves state-of-the-art performance in all recommendation\nscenarios. The source code will be released upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09058", "pdf": "https://arxiv.org/pdf/2504.09058", "abs": "https://arxiv.org/abs/2504.09058", "authors": ["Chengyuan Liu", "Shihang Wang", "Lizhi Qing", "Kaisong Song", "Junjie Cao", "Jun Lin", "Ji Zhang", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Towards Stepwise Domain Knowledge-Driven Reasoning Optimization and Reflection Improvement", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recently, stepwise supervision on Chain of Thoughts (CoTs) presents an\nenhancement on the logical reasoning tasks such as coding and math, with the\nhelp of Monte Carlo Tree Search (MCTS). However, its contribution to tasks\nrequiring domain-specific expertise and knowledge remains unexplored. Motivated\nby the interest, we identify several potential challenges of vanilla MCTS\nwithin this context, and propose the framework of Stepwise Domain\nKnowledge-Driven Reasoning Optimization, employing the MCTS algorithm to\ndevelop step-level supervision for problems that require essential\ncomprehension, reasoning, and specialized knowledge. Additionally, we also\nintroduce the Preference Optimization towards Reflection Paths, which\niteratively learns self-reflection on the reasoning thoughts from better\nperspectives. We have conducted extensive experiments to evaluate the advantage\nof the methodologies. Empirical results demonstrate the effectiveness on\nvarious legal-domain problems. We also report a diverse set of valuable\nfindings, hoping to encourage the enthusiasm to the research of domain-specific\nLLMs and MCTS.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09271", "pdf": "https://arxiv.org/pdf/2504.09271", "abs": "https://arxiv.org/abs/2504.09271", "authors": ["Koustuv Saha", "Yoshee Jain", "Munmun De Choudhury"], "title": "Linguistic Comparison of AI- and Human-Written Responses to Online Mental Health Queries", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SI"], "comment": null, "summary": "The ubiquity and widespread use of digital and online technologies have\ntransformed mental health support, with online mental health communities\n(OMHCs) providing safe spaces for peer support. More recently, generative AI\nand large language models (LLMs) have introduced new possibilities for\nscalable, around-the-clock mental health assistance that could potentially\naugment and supplement the capabilities of OMHCs. Although genAI shows promise\nin delivering immediate and personalized responses, their effectiveness in\nreplicating the nuanced, experience-based support of human peers remains an\nopen question. In this study, we harnessed 24,114 posts and 138,758 online\ncommunity (OC) responses from 55 OMHCs on Reddit. We prompted several\nstate-of-the-art LLMs (GPT-4-Turbo, Llama-3, and Mistral-7B) with these posts,\nand compared their (AI) responses to human-written (OC) responses based on a\nvariety of linguistic measures across psycholinguistics and lexico-semantics.\nOur findings revealed that AI responses are more verbose, readable, and\nanalytically structured, but lack linguistic diversity and personal narratives\ninherent in human-human interactions. Through a qualitative examination, we\nfound validation as well as complementary insights into the nature of AI\nresponses, such as its neutrality of stance and the absence of seeking\nback-and-forth clarifications. We discuss the ethical and practical\nimplications of integrating generative AI into OMHCs, advocating for frameworks\nthat balance AI's scalability and timeliness with the irreplaceable\nauthenticity, social interactiveness, and expertise of human connections that\nform the ethos of online support communities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10021", "pdf": "https://arxiv.org/pdf/2504.10021", "abs": "https://arxiv.org/abs/2504.10021", "authors": ["Nikolai Rhrich", "Alwin Hoffmann", "Richard Nordsieck", "Emilio Zarbali", "Alireza Javanmardi"], "title": "Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Whereas in general computer vision, transformer-based architectures have\nquickly become the gold standard, microelectronics defect detection still\nheavily relies on convolutional neural networks (CNNs). We hypothesize that\nthis is due to the fact that a) transformers have an increased need for data\nand b) labelled image generation procedures for microelectronics are costly,\nand labelled data is therefore sparse. Whereas in other domains, pre-training\non large natural image datasets can mitigate this problem, in microelectronics\ntransfer learning is hindered due to the dissimilarity of domain data and\nnatural images. Therefore, we evaluate self pre-training, where models are\npre-trained on the target dataset, rather than another dataset. We propose a\nvision transformer (ViT) pre-training framework for defect detection in\nmicroelectronics based on masked autoencoders (MAE). In MAE, a large share of\nimage patches is masked and reconstructed by the model during pre-training. We\nperform pre-training and defect detection using a dataset of less than 10.000\nscanning acoustic microscopy (SAM) images labelled using transient thermal\nanalysis (TTA). Our experimental results show that our approach leads to\nsubstantial performance gains compared to a) supervised ViT, b) ViT pre-trained\non natural image datasets, and c) state-of-the-art CNN-based defect detection\nmodels used in the literature. Additionally, interpretability analysis reveals\nthat our self pre-trained models, in comparison to ViT baselines, correctly\nfocus on defect-relevant features such as cracks in the solder material. This\ndemonstrates that our approach yields fault-specific feature representations,\nmaking our self pre-trained models viable for real-world defect detection in\nmicroelectronics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09466", "pdf": "https://arxiv.org/pdf/2504.09466", "abs": "https://arxiv.org/abs/2504.09466", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yulin Hu", "Yang Deng", "An Zhang", "Xingyu Sui", "Xinyang Han", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "AdaSteer: Your Aligned LLM is Inherently an Adaptive Jailbreak Defender", "categories": ["cs.CR", "cs.CL"], "comment": "17 pages, 6 figures, 9 tables", "summary": "Despite extensive efforts in safety alignment, large language models (LLMs)\nremain vulnerable to jailbreak attacks. Activation steering offers a\ntraining-free defense method but relies on fixed steering coefficients,\nresulting in suboptimal protection and increased false rejections of benign\ninputs. To address this, we propose AdaSteer, an adaptive activation steering\nmethod that dynamically adjusts model behavior based on input characteristics.\nWe identify two key properties: Rejection Law (R-Law), which shows that\nstronger steering is needed for jailbreak inputs opposing the rejection\ndirection, and Harmfulness Law (H-Law), which differentiates adversarial and\nbenign inputs. AdaSteer steers input representations along both the Rejection\nDirection (RD) and Harmfulness Direction (HD), with adaptive coefficients\nlearned via logistic regression, ensuring robust jailbreak defense while\npreserving benign input handling. Experiments on LLaMA-3.1, Gemma-2, and\nQwen2.5 show that AdaSteer outperforms baseline methods across multiple\njailbreak attacks with minimal impact on utility. Our results highlight the\npotential of interpretable model internals for real-time, flexible safety\nenforcement in LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09479", "pdf": "https://arxiv.org/pdf/2504.09479", "abs": "https://arxiv.org/abs/2504.09479", "authors": ["Zhiqing Cui", "Jiahao Yuan", "Hanqing Wang", "Yanshu Li", "Chenxu Du", "Zhenglong Ding"], "title": "Draw with Thought: Unleashing Multimodal Reasoning for Scientific Diagram Generation", "categories": ["cs.AI", "cs.CL"], "comment": "26 pages, 14 figures", "summary": "Scientific diagrams are vital tools for communicating structured knowledge\nacross disciplines. However, they are often published as static raster images,\nlosing symbolic semantics and limiting reuse. While Multimodal Large Language\nModels (MLLMs) offer a pathway to bridging vision and structure, existing\nmethods lack semantic control and structural interpretability, especially on\ncomplex diagrams. We propose Draw with Thought (DwT), a training-free framework\nthat guides MLLMs to reconstruct diagrams into editable mxGraph XML code\nthrough cognitively-grounded Chain-of-Thought reasoning. DwT enables\ninterpretable and controllable outputs without model fine-tuning by dividing\nthe task into two stages: Coarse-to-Fine Planning, which handles perceptual\nstructuring and semantic specification, and Structure-Aware Code Generation,\nenhanced by format-guided refinement. To support evaluation, we release\nPlot2XML, a benchmark of 247 real-world scientific diagrams with gold-standard\nXML annotations. Extensive experiments across eight MLLMs show that our\napproach yields high-fidelity, semantically aligned, and structurally valid\nreconstructions, with human evaluations confirming strong alignment in both\naccuracy and visual aesthetics, offering a scalable solution for converting\nstatic visuals into executable representations and advancing machine\nunderstanding of scientific graphics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "code generation"], "score": 4}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09710", "pdf": "https://arxiv.org/pdf/2504.09710", "abs": "https://arxiv.org/abs/2504.09710", "authors": ["Zhenting Wang", "Guofeng Cui", "Kun Wan", "Wentian Zhao"], "title": "DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in reinforcement learning (RL)-based post-training have led\nto notable improvements in large language models (LLMs), particularly in\nenhancing their reasoning capabilities to handle complex tasks. However, most\nexisting methods treat the training data as a unified whole, overlooking the\nfact that modern LLM training often involves a mixture of data from diverse\ndistributions-varying in both source and difficulty. This heterogeneity\nintroduces a key challenge: how to adaptively schedule training across\ndistributions to optimize learning efficiency. In this paper, we present a\nprincipled curriculum learning framework grounded in the notion of\ndistribution-level learnability. Our core insight is that the magnitude of\npolicy advantages reflects how much a model can still benefit from further\ntraining on a given distribution. Based on this, we propose a\ndistribution-level curriculum learning framework for RL-based LLM\npost-training, which leverages the Upper Confidence Bound (UCB) principle to\ndynamically adjust sampling probabilities for different distrubutions. This\napproach prioritizes distributions with either high average advantage\n(exploitation) or low sample count (exploration), yielding an adaptive and\ntheoretically grounded training schedule. We instantiate our curriculum\nlearning framework with GRPO as the underlying RL algorithm and demonstrate its\neffectiveness on logic reasoning datasets with multiple difficulties and\nsources. Our experiments show that our framework significantly improves\nconvergence speed and final performance, highlighting the value of\ndistribution-aware curriculum strategies in LLM post-training. Code:\nhttps://github.com/ZhentingWang/DUMP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09737", "pdf": "https://arxiv.org/pdf/2504.09737", "abs": "https://arxiv.org/abs/2504.09737", "authors": ["Nitya Thakkar", "Mert Yuksekgonul", "Jake Silberg", "Animesh Garg", "Nanyun Peng", "Fei Sha", "Rose Yu", "Carl Vondrick", "James Zou"], "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "30 pages, 7 figures", "summary": "Peer review at AI conferences is stressed by rapidly rising submission\nvolumes, leading to deteriorating review quality and increased author\ndissatisfaction. To address these issues, we developed Review Feedback Agent, a\nsystem leveraging multiple large language models (LLMs) to improve review\nclarity and actionability by providing automated feedback on vague comments,\ncontent misunderstandings, and unprofessional remarks to reviewers. Implemented\nat ICLR 2025 as a large randomized control study, our system provided optional\nfeedback to more than 20,000 randomly selected reviews. To ensure high-quality\nfeedback for reviewers at this scale, we also developed a suite of automated\nreliability tests powered by LLMs that acted as guardrails to ensure feedback\nquality, with feedback only being sent to reviewers if it passed all the tests.\nThe results show that 27% of reviewers who received feedback updated their\nreviews, and over 12,000 feedback suggestions from the agent were incorporated\nby those reviewers. This suggests that many reviewers found the AI-generated\nfeedback sufficiently helpful to merit updating their reviews. Incorporating AI\nfeedback led to significantly longer reviews (an average increase of 80 words\namong those who updated after receiving feedback) and more informative reviews,\nas evaluated by blinded researchers. Moreover, reviewers who were selected to\nreceive AI feedback were also more engaged during paper rebuttals, as seen in\nlonger author-reviewer discussions. This work demonstrates that carefully\ndesigned LLM-generated review feedback can enhance peer review quality by\nmaking reviews more specific and actionable while increasing engagement between\nreviewers and authors. The Review Feedback Agent is publicly available at\nhttps://github.com/zou-group/review_feedback_agent.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["AI feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09816", "pdf": "https://arxiv.org/pdf/2504.09816", "abs": "https://arxiv.org/abs/2504.09816", "authors": ["Quentin Fitte-Rey", "Matyas Amrouche", "Romain Deveaud"], "title": "Augmented Relevance Datasets with Fine-Tuned Small LLMs", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25", "summary": "Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10106", "pdf": "https://arxiv.org/pdf/2504.10106", "abs": "https://arxiv.org/abs/2504.10106", "authors": ["Marc Gutirrez-Prez", "Antonio Agudo"], "title": "SoccerNet-v3D: Leveraging Sports Broadcast Replays for 3D Scene Understanding", "categories": ["cs.CV", "cs.AI", "I.2; I.4; I.5"], "comment": null, "summary": "Sports video analysis is a key domain in computer vision, enabling detailed\nspatial understanding through multi-view correspondences. In this work, we\nintroduce SoccerNet-v3D and ISSIA-3D, two enhanced and scalable datasets\ndesigned for 3D scene understanding in soccer broadcast analysis. These\ndatasets extend SoccerNet-v3 and ISSIA by incorporating field-line-based camera\ncalibration and multi-view synchronization, enabling 3D object localization\nthrough triangulation. We propose a monocular 3D ball localization task built\nupon the triangulation of ground-truth 2D ball annotations, along with several\ncalibration and reprojection metrics to assess annotation quality on demand.\nAdditionally, we present a single-image 3D ball localization method as a\nbaseline, leveraging camera calibration and ball size priors to estimate the\nball's position from a monocular viewpoint. To further refine 2D annotations,\nwe introduce a bounding box optimization technique that ensures alignment with\nthe 3D scene representation. Our proposed datasets establish new benchmarks for\n3D soccer scene understanding, enhancing both spatial and temporal analysis in\nsports analytics. Finally, we provide code to facilitate access to our\nannotations and the generation pipelines for the datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10117", "pdf": "https://arxiv.org/pdf/2504.10117", "abs": "https://arxiv.org/abs/2504.10117", "authors": ["Peizheng Li", "Shuxiao Ding", "You Zhou", "Qingwen Zhang", "Onat Inak", "Larissa Triess", "Niklas Hanselmann", "Marius Cordts", "Andreas Zell"], "title": "AGO: Adaptive Grounding for Open World 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Open-world 3D semantic occupancy prediction aims to generate a voxelized 3D\nrepresentation from sensor inputs while recognizing both known and unknown\nobjects. Transferring open-vocabulary knowledge from vision-language models\n(VLMs) offers a promising direction but remains challenging. However, methods\nbased on VLM-derived 2D pseudo-labels with traditional supervision are limited\nby a predefined label space and lack general prediction capabilities. Direct\nalignment with pretrained image embeddings, on the other hand, fails to achieve\nreliable performance due to often inconsistent image and text representations\nin VLMs. To address these challenges, we propose AGO, a novel 3D occupancy\nprediction framework with adaptive grounding to handle diverse open-world\nscenarios. AGO first encodes surrounding images and class prompts into 3D and\ntext embeddings, respectively, leveraging similarity-based grounding training\nwith 3D pseudo-labels. Additionally, a modality adapter maps 3D embeddings into\na space aligned with VLM-derived image embeddings, reducing modality gaps.\nExperiments on Occ3D-nuScenes show that AGO improves unknown object prediction\nin zero-shot and few-shot transfer while achieving state-of-the-art\nclosed-world self-supervised performance, surpassing prior methods by 4.09\nmIoU.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10081", "pdf": "https://arxiv.org/pdf/2504.10081", "abs": "https://arxiv.org/abs/2504.10081", "authors": ["Yichi Zhang", "Zihao Zeng", "Dongbai Li", "Yao Huang", "Zhijie Deng", "Yinpeng Dong"], "title": "RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have been\nrapidly progressing and achieving breakthrough performance on complex reasoning\ntasks such as mathematics and coding. However, the open-source R1 models have\nraised safety concerns in wide applications, such as the tendency to comply\nwith malicious queries, which greatly impacts the utility of these powerful\nmodels in their applications. In this paper, we introduce RealSafe-R1 as\nsafety-aligned versions of DeepSeek-R1 distilled models. To train these models,\nwe construct a dataset of 15k safety-aware reasoning trajectories generated by\nDeepSeek-R1, under explicit instructions for expected refusal behavior. Both\nquantitative experiments and qualitative case studies demonstrate the models'\nimprovements, which are shown in their safety guardrails against both harmful\nqueries and jailbreak attacks. Importantly, unlike prior safety alignment\nefforts that often compromise reasoning performance, our method preserves the\nmodels' reasoning capabilities by maintaining the training data within the\noriginal distribution of generation. Model weights of RealSafe-R1 are\nopen-source at https://huggingface.co/RealSafe.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10179", "pdf": "https://arxiv.org/pdf/2504.10179", "abs": "https://arxiv.org/abs/2504.10179", "authors": ["Anwesha Mohanty", "Venkatesh Balavadhani Parthasarathy", "Arsalan Shahid"], "title": "The Future of MLLM Prompting is Adaptive: A Comprehensive Experimental Evaluation of Prompt Engineering Methods for Robust Multimodal Performance", "categories": ["cs.AI", "cs.CL", "cs.ET"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are set to transform how machines\nprocess and generate human-like responses by integrating diverse modalities\nsuch as text, images, and code. Yet, effectively harnessing their capabilities\nhinges on optimal prompt engineering. We present a comprehensive experimental\nevaluation of seven prompt engineering methods applied to 13 open-source MLLMs\nover 24 tasks spanning Reasoning and Compositionality, Multimodal Understanding\nand Alignment, Complex Code Generation and Execution, and Knowledge Retrieval\nand Integration. Our approach stratifies models by parameter count into Small\n(<4B), Medium (4B-10B), and Large (>10B) categories and compares prompting\ntechniques including Zero-Shot, One-Shot, Few-Shot, Chain-of-Thought,\nAnalogical, Generated Knowledge, and Tree-of-Thought. While Large MLLMs excel\nin structured tasks such as code generation, achieving accuracies up to 96.88%\nunder Few-Shot prompting, all models struggle with complex reasoning and\nabstract understanding, often yielding accuracies below 60% and high\nhallucination rates. Structured reasoning prompts frequently increased\nhallucination up to 75% in small models and led to longer response times (over\n20 seconds in Large MLLMs), while simpler prompting methods provided more\nconcise and efficient outputs. No single prompting method uniformly optimises\nall task types. Instead, adaptive strategies combining example-based guidance\nwith selective structured reasoning are essential to enhance robustness,\nefficiency, and factual accuracy. Our findings offer practical recommendations\nfor prompt engineering and support more reliable deployment of MLLMs across\napplications including AI-assisted coding, knowledge retrieval, and multimodal\ncontent understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "code generation"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10358", "pdf": "https://arxiv.org/pdf/2504.10358", "abs": "https://arxiv.org/abs/2504.10358", "authors": ["Rui Chen", "Lei Sun", "Jing Tang", "Geng Li", "Xiangxiang Chu"], "title": "FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Recent advances in video generation have posed great challenges in the\nassessment of AI-generated content, particularly with the emergence of\nincreasingly sophisticated models. The various inconsistencies and defects\nobserved in such videos are inherently complex, making overall scoring\nnotoriously difficult. In this paper, we emphasize the critical importance of\nintegrating fine-grained reasoning into video evaluation, and we propose\n$\\textbf{F}$ing$\\textbf{ER}$, a novel entity-level reasoning evaluation\nframework that first automatically generates $\\textbf{F}$ine-grained\n$\\textbf{E}$ntity-level questions, and then answers those questions by a\n$\\textbf{R}$easoning model with scores, which can be subsequently weighted\nsummed to an overall score for different applications. Specifically, we\nleverage LLMs to derive entity-level questions across five distinct\nperspectives, which (i) often focus on some specific entities of the content,\nthereby making answering or scoring much easier by MLLMs, and (ii) are more\ninterpretable. Then we construct a FingER dataset, consisting of approximately\n3.3k videos and corresponding 60k fine-grained QA annotations, each with\ndetailed reasons. Based on that, we further investigate various training\nprotocols to best incentivize the reasoning capability of MLLMs for correct\nanswer prediction. Extensive experiments demonstrate that a reasoning model\ntrained using Group Relative Policy Optimization (GRPO) with a cold-start\nstrategy achieves the best performance. Notably, our model surpasses existing\nmethods by a relative margin of $11.8\\%$ on GenAI-Bench and $5.5\\%$ on\nMonetBench with only 3.3k training videos, which is at most one-tenth of the\ntraining samples utilized by other methods. Our code and dataset will be\nreleased soon.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10483", "pdf": "https://arxiv.org/pdf/2504.10483", "abs": "https://arxiv.org/abs/2504.10483", "authors": ["Xingjian Leng", "Jaskirat Singh", "Yunzhong Hou", "Zhenchang Xing", "Saining Xie", "Liang Zheng"], "title": "REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In this paper we tackle a fundamental question: \"Can we train latent\ndiffusion models together with the variational auto-encoder (VAE) tokenizer in\nan end-to-end manner?\" Traditional deep-learning wisdom dictates that\nend-to-end training is often preferable when possible. However, for latent\ndiffusion transformers, it is observed that end-to-end training both VAE and\ndiffusion-model using standard diffusion-loss is ineffective, even causing a\ndegradation in final performance. We show that while diffusion loss is\nineffective, end-to-end training can be unlocked through the\nrepresentation-alignment (REPA) loss -- allowing both VAE and diffusion model\nto be jointly tuned during the training process. Despite its simplicity, the\nproposed training recipe (REPA-E) shows remarkable performance; speeding up\ndiffusion model training by over 17x and 45x over REPA and vanilla training\nrecipes, respectively. Interestingly, we observe that end-to-end tuning with\nREPA-E also improves the VAE itself; leading to improved latent space structure\nand downstream generation performance. In terms of final performance, our\napproach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and\nwithout classifier-free guidance on ImageNet 256 x 256. Code is available at\nhttps://end2end-diffusion.github.io.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.09209", "pdf": "https://arxiv.org/pdf/2504.09209", "abs": "https://arxiv.org/abs/2504.09209", "authors": ["Xiangyue Zhang", "Jianfang Li", "Jiaxu Zhang", "Jianqiang Ren", "Liefeng Bo", "Zhigang Tu"], "title": "EchoMask: Speech-Queried Attention-based Mask Modeling for Holistic Co-Speech Motion Generation", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": "12 pages, 12 figures", "summary": "Masked modeling framework has shown promise in co-speech motion generation.\nHowever, it struggles to identify semantically significant frames for effective\nmotion masking. In this work, we propose a speech-queried attention-based mask\nmodeling framework for co-speech motion generation. Our key insight is to\nleverage motion-aligned speech features to guide the masked motion modeling\nprocess, selectively masking rhythm-related and semantically expressive motion\nframes. Specifically, we first propose a motion-audio alignment module (MAM) to\nconstruct a latent motion-audio joint space. In this space, both low-level and\nhigh-level speech features are projected, enabling motion-aligned speech\nrepresentation using learnable speech queries. Then, a speech-queried attention\nmechanism (SQA) is introduced to compute frame-level attention scores through\ninteractions between motion keys and speech queries, guiding selective masking\ntoward motion frames with high attention scores. Finally, the motion-aligned\nspeech features are also injected into the generation network to facilitate\nco-speech motion generation. Qualitative and quantitative evaluations confirm\nthat our method outperforms existing state-of-the-art approaches, successfully\nproducing high-quality co-speech motion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-15.jsonl"}
{"id": "2504.10007", "pdf": "https://arxiv.org/pdf/2504.10007", "abs": "https://arxiv.org/abs/2504.10007", "authors": ["Jiani Ni", "He Zhao", "Jintong Gao", "Dandan Guo", "Hongyuan Zha"], "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration", "categories": ["cs.LG", "cs.CV"], "comment": "CVPR2025", "summary": "In recent years, deep neural networks (DNNs) have demonstrated\nstate-of-the-art performance across various domains. However, despite their\nsuccess, they often face calibration issues, particularly in safety-critical\napplications such as autonomous driving and healthcare, where unreliable\npredictions can have serious consequences. Recent research has started to\nimprove model calibration from the view of the classifier. However, the\nexploration of designing the classifier to solve the model calibration problem\nis insufficient. Let alone most of the existing methods ignore the calibration\nerrors arising from underconfidence. In this work, we propose a novel method by\nbalancing learnable and ETF classifiers to solve the overconfidence or\nunderconfidence problem for model Calibration named BalCAL. By introducing a\nconfidence-tunable module and a dynamic adjustment method, we ensure better\nalignment between model confidence and its true accuracy. Extensive\nexperimental validation shows that ours significantly improves model\ncalibration performance while maintaining high predictive accuracy,\noutperforming existing techniques. This provides a novel solution to the\ncalibration challenges commonly encountered in deep learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "accuracy"], "score": 2}}, "source_file": "2025-04-15.jsonl"}
