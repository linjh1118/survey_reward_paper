{"id": "2504.11447", "pdf": "https://arxiv.org/pdf/2504.11447", "abs": "https://arxiv.org/abs/2504.11447", "authors": ["An Zhaol", "Shengyuan Zhang", "Ling Yang", "Zejian Li", "Jiale Wu", "Haoran Xu", "AnYang Wei", "Perry Pengyun GU Lingyun Sun"], "title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion", "categories": ["cs.CV"], "comment": "Our code is public available on\n  https://github.com/happyw1nd/DistillationDPO", "summary": "The application of diffusion models in 3D LiDAR scene completion is limited\ndue to diffusion's slow sampling speed. Score distillation accelerates\ndiffusion sampling but with performance degradation, while post-training with\ndirect policy optimization (DPO) boosts performance using preference data. This\npaper proposes Distillation-DPO, a novel diffusion distillation framework for\nLiDAR scene completion with preference aligment. First, the student model\ngenerates paired completion scenes with different initial noises. Second, using\nLiDAR scene evaluation metrics as preference, we construct winning and losing\nsample pairs. Such construction is reasonable, since most LiDAR scene metrics\nare informative but non-differentiable to be optimized directly. Third,\nDistillation-DPO optimizes the student model by exploiting the difference in\nscore functions between the teacher and student models on the paired completion\nscenes. Such procedure is repeated until convergence. Extensive experiments\ndemonstrate that, compared to state-of-the-art LiDAR scene completion diffusion\nmodels, Distillation-DPO achieves higher-quality scene completion while\naccelerating the completion speed by more than 5-fold. Our method is the first\nto explore adopting preference learning in distillation to the best of our\nknowledge and provide insights into preference-aligned distillation. Our code\nis public available on https://github.com/happyw1nd/DistillationDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "policy optimization", "preference", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10637", "pdf": "https://arxiv.org/pdf/2504.10637", "abs": "https://arxiv.org/abs/2504.10637", "authors": ["Afra Amini", "Tim Vieira", "Ryan Cotterell"], "title": "Better Estimation of the KL Divergence Between Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Estimating the Kullback--Leibler (KL) divergence between language models has\nmany applications, e.g., reinforcement learning from human feedback (RLHF),\ninterpretability, and knowledge distillation. However, computing the exact KL\ndivergence between two arbitrary language models is intractable. Thus,\npractitioners often resort to the use of sampling-based estimators. While it is\neasy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased\nestimate of the KL divergence between language models, this estimator\nnotoriously suffers from high variance, and can even result in a negative\nestimate of the KL divergence, a non-negative quantity. In this paper, we\nintroduce a Rao--Blackwellized estimator that is also unbiased and provably has\nvariance less than or equal to that of the standard Monte Carlo estimator. In\nan empirical study on sentiment-controlled fine-tuning, we show that our\nestimator provides more stable KL estimates and reduces variance substantially\nin practice. Additionally, we derive an analogous Rao--Blackwellized estimator\nof the gradient of the KL divergence, which leads to more stable training and\nproduces models that more frequently appear on the Pareto frontier of reward\nvs. KL compared to the ones trained with the MC estimator of the gradient.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11343", "pdf": "https://arxiv.org/pdf/2504.11343", "abs": "https://arxiv.org/abs/2504.11343", "authors": ["Wei Xiong", "Jiarui Yao", "Yuhui Xu", "Bo Pang", "Lei Wang", "Doyen Sahoo", "Junnan Li", "Nan Jiang", "Tong Zhang", "Caiming Xiong", "Hanze Dong"], "title": "A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "12 pages, 4 figures", "summary": "Reinforcement learning (RL) has become a prevailing approach for fine-tuning\nlarge language models (LLMs) on complex reasoning tasks. Among recent methods,\nGRPO stands out for its empirical success in training models such as\nDeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In\nthis work, we revisit GRPO from a reinforce-like algorithm perspective and\nanalyze its core components. Surprisingly, we find that a simple rejection\nsampling baseline, RAFT, which trains only on positively rewarded samples,\nyields competitive performance than GRPO and PPO. Our ablation studies reveal\nthat GRPO's main advantage arises from discarding prompts with entirely\nincorrect responses, rather than from its reward normalization. Motivated by\nthis insight, we propose Reinforce-Rej, a minimal extension of policy gradient\nthat filters both entirely incorrect and entirely correct samples.\nReinforce-Rej improves KL efficiency and stability, serving as a lightweight\nyet effective alternative to more complex RL algorithms. We advocate RAFT as a\nrobust and interpretable baseline, and suggest that future advances should\nfocus on more principled designs for incorporating negative samples, rather\nthan relying on them indiscriminately. Our findings provide guidance for future\nwork in reward-based LLM post-training.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "policy gradient", "reinforcement learning"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11337", "pdf": "https://arxiv.org/pdf/2504.11337", "abs": "https://arxiv.org/abs/2504.11337", "authors": ["Zhihao Xu", "Yongqi Tong", "Xin Zhang", "Jun Zhou", "Xiting Wang"], "title": "REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Multi-objective preference alignment in language models often encounters a\nchallenging trade-off: optimizing for one human preference (e.g., helpfulness)\nfrequently compromises others (e.g., harmlessness) due to the inherent\nconflicts between competing objectives. While prior work mainly focuses on\nalgorithmic solutions, we explore a novel data-driven approach to uncover the\ntypes of data that can effectively mitigate these conflicts. Specifically, we\npropose the concept of Reward Consistency (RC), which identifies samples that\nalign with multiple preference objectives, thereby reducing conflicts during\ntraining. Through gradient-based analysis, we demonstrate that RC-compliant\nsamples inherently constrain performance degradation during multi-objective\noptimization. Building on these insights, we further develop Reward Consistency\nSampling, a framework that automatically constructs preference datasets that\neffectively mitigate conflicts during multi-objective alignment. Our generated\ndata achieves an average improvement of 13.37% in both the harmless rate and\nhelpfulness win rate when optimizing harmlessness and helpfulness, and can\nconsistently resolve conflicts in varying multi-objective scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference", "helpfulness", "harmlessness", "consistency"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11346", "pdf": "https://arxiv.org/pdf/2504.11346", "abs": "https://arxiv.org/abs/2504.11346", "authors": ["Yu Gao", "Lixue Gong", "Qiushan Guo", "Xiaoxia Hou", "Zhichao Lai", "Fanshi Li", "Liang Li", "Xiaochen Lian", "Chao Liao", "Liyang Liu", "Wei Liu", "Yichun Shi", "Shiqi Sun", "Yu Tian", "Zhi Tian", "Peng Wang", "Rui Wang", "Xuanda Wang", "Xun Wang", "Ye Wang", "Guofeng Wu", "Jie Wu", "Xin Xia", "Xuefeng Xiao", "Zhonghua Zhai", "Xinyu Zhang", "Qi Zhang", "Yuwei Zhang", "Shijia Zhao", "Jianchao Yang", "Weilin Huang"], "title": "Seedream 3.0 Technical Report", "categories": ["cs.CV"], "comment": "Seedream 3.0 Technical Report", "summary": "We present Seedream 3.0, a high-performance Chinese-English bilingual image\ngeneration foundation model. We develop several technical improvements to\naddress existing challenges in Seedream 2.0, including alignment with\ncomplicated prompts, fine-grained typography generation, suboptimal visual\naesthetics and fidelity, and limited image resolutions. Specifically, the\nadvancements of Seedream 3.0 stem from improvements across the entire pipeline,\nfrom data construction to model deployment. At the data stratum, we double the\ndataset using a defect-aware training paradigm and a dual-axis collaborative\ndata-sampling framework. Furthermore, we adopt several effective techniques\nsuch as mixed-resolution training, cross-modality RoPE, representation\nalignment loss, and resolution-aware timestep sampling in the pre-training\nphase. During the post-training stage, we utilize diversified aesthetic\ncaptions in SFT, and a VLM-based reward model with scaling, thereby achieving\noutputs that well align with human preferences. Furthermore, Seedream 3.0\npioneers a novel acceleration paradigm. By employing consistent noise\nexpectation and importance-aware timestep sampling, we achieve a 4 to 8 times\nspeedup while maintaining image quality. Seedream 3.0 demonstrates significant\nimprovements over Seedream 2.0: it enhances overall capabilities, in particular\nfor text-rendering in complicated Chinese characters which is important to\nprofessional typography generation. In addition, it provides native\nhigh-resolution output (up to 2K), allowing it to generate images with high\nvisual quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11455", "pdf": "https://arxiv.org/pdf/2504.11455", "abs": "https://arxiv.org/abs/2504.11455", "authors": ["Junke Wang", "Zhi Tian", "Xun Wang", "Xinyu Zhang", "Weilin Huang", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL", "categories": ["cs.CV"], "comment": "technical report, work in progress", "summary": "This work presents SimpleAR, a vanilla autoregressive visual generation\nframework without complex architecure modifications. Through careful\nexploration of training and inference optimization, we demonstrate that: 1)\nwith only 0.5B parameters, our model can generate 1024x1024 resolution images\nwith high fidelity, and achieve competitive results on challenging\ntext-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both\nsupervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO)\ntraining could lead to significant improvements on generation aesthectics and\nprompt alignment; and 3) when optimized with inference acceleraton techniques\nlike vLLM, the time for SimpleAR to generate an 1024x1024 image could be\nreduced to around 14 seconds. By sharing these findings and open-sourcing the\ncode, we hope to reveal the potential of autoregressive visual generation and\nencourage more participation in this research field. Code is available at\nhttps://github.com/wdrink/SimpleAR.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference optimization"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11118", "pdf": "https://arxiv.org/pdf/2504.11118", "abs": "https://arxiv.org/abs/2504.11118", "authors": ["Henrik Krauss", "Takehisa Yairi"], "title": "Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This study introduces a novel method for revealing human covert attention\npatterns using gameplay data alone, utilizing offline attention techniques from\nreinforcement learning (RL). We propose the contextualized, task-relevant (CTR)\nattention network, which generates attention maps from both human and RL agent\ngameplay in Atari environments. These maps are sparse yet retain the necessary\ninformation for the current player's decision making. We compare the\nCTR-derived attention maps with a temporally integrated overt attention (TIOA)\nmodel based on eye-tracking data, serving as a point of comparison and\ndiscussion. Visual inspection reveals distinct attention patterns: human CTR\nmaps focus on the player and rather nearby opponents, occasionally shifting\nbetween stronger focus and broader views - sometimes even attending to empty\nspace ahead. In contrast, agent maps maintain a consistent broad focus on most\nobjects, including distant ones and the player. Quantitative analysis further\ndemonstrates that human CTR maps align more closely with TIOA than agent maps\ndo. Our findings indicate that the CTR attention network can effectively reveal\nhuman covert attention patterns from gameplay alone, without the need for\nadditional data like brain activity recordings. This work contributes to\nunderstanding human-agent attention differences and enables the development of\nRL agents augmented with human covert attention.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "comparison"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10504", "pdf": "https://arxiv.org/pdf/2504.10504", "abs": "https://arxiv.org/abs/2504.10504", "authors": ["Rita Sevastjanova", "Robin Gerling", "Thilo Spinner", "Mennatallah El-Assady"], "title": "LayerFlow: Layer-wise Exploration of LLM Embeddings using Uncertainty-aware Interlinked Projections", "categories": ["cs.CL", "cs.GR"], "comment": null, "summary": "Large language models (LLMs) represent words through contextual word\nembeddings encoding different language properties like semantics and syntax.\nUnderstanding these properties is crucial, especially for researchers\ninvestigating language model capabilities, employing embeddings for tasks\nrelated to text similarity, or evaluating the reasons behind token importance\nas measured through attribution methods. Applications for embedding exploration\nfrequently involve dimensionality reduction techniques, which reduce\nhigh-dimensional vectors to two dimensions used as coordinates in a\nscatterplot. This data transformation step introduces uncertainty that can be\npropagated to the visual representation and influence users' interpretation of\nthe data. To communicate such uncertainties, we present LayerFlow - a visual\nanalytics workspace that displays embeddings in an interlinked projection\ndesign and communicates the transformation, representation, and interpretation\nuncertainty. In particular, to hint at potential data distortions and\nuncertainties, the workspace includes several visual components, such as convex\nhulls showing 2D and HD clusters, data point pairwise distances, cluster\nsummaries, and projection quality metrics. We show the usability of the\npresented workspace through replication and expert case studies that highlight\nthe need to communicate uncertainty through multiple visual components and\ndifferent data perspectives.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10647", "pdf": "https://arxiv.org/pdf/2504.10647", "abs": "https://arxiv.org/abs/2504.10647", "authors": ["Nafis Sadeq", "Xin Xu", "Zhouhang Xie", "Julian McAuley", "Byungkyu Kang", "Prarit Lamba", "Xiang Gao"], "title": "Improving In-Context Learning with Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Language models rely on semantic priors to perform in-context learning, which\nleads to poor performance on tasks involving inductive reasoning.\nInstruction-tuning methods based on imitation learning can superficially\nenhance the in-context learning performance of language models, but they often\nfail to improve the model's understanding of the underlying rules that connect\ninputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning\ndistillation technique designed to improve the inductive reasoning capabilities\nof language models. Through a careful combination of data augmentation,\nfiltering, supervised fine-tuning, and alignment, ReDis achieves significant\nperformance improvements across a diverse range of tasks, including 1D-ARC,\nList Function, ACRE, and MiniSCAN. Experiments on three language model\nbackbones show that ReDis outperforms equivalent few-shot prompting baselines\nacross all tasks and even surpasses the teacher model, GPT-4o, in some cases.\nReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%,\n2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within\na similar hypothesis search space. The code, dataset, and model checkpoints\nwill be made available at\nhttps://github.com/NafisSadeq/reasoning-distillation.git.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792", "abs": "https://arxiv.org/abs/2504.10792", "authors": ["Jessica Lin", "Amir Zeldes"], "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation", "consistency", "summarization"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10906", "pdf": "https://arxiv.org/pdf/2504.10906", "abs": "https://arxiv.org/abs/2504.10906", "authors": ["Changjiang Gao", "Hankun Lin", "Shujian Huang", "Xin Huang", "Xue Han", "Junlan Feng", "Chao Deng", "Jiajun Chen"], "title": "Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From", "categories": ["cs.CL"], "comment": null, "summary": "The ability of cross-lingual context retrieval is a fundamental aspect of\ncross-lingual alignment of large language models (LLMs), where the model\nextracts context information in one language based on requests in another\nlanguage. Despite its importance in real-life applications, this ability has\nnot been adequately investigated for state-of-the-art models. In this paper, we\nevaluate the cross-lingual context retrieval ability of over 40 LLMs across 12\nlanguages to understand the source of this ability, using cross-lingual machine\nreading comprehension (xMRC) as a representative scenario. Our results show\nthat several small, post-trained open LLMs show strong cross-lingual context\nretrieval ability, comparable to closed-source LLMs such as GPT-4o, and their\nestimated oracle performances greatly improve after post-training. Our\ninterpretability analysis shows that the cross-lingual context retrieval\nprocess can be divided into two main phases: question encoding and answer\nretrieval, which are formed in pre-training and post-training, respectively.\nThe phasing stability correlates with xMRC performance, and the xMRC bottleneck\nlies at the last model layers in the second phase, where the effect of\npost-training can be evidently observed. Our results also indicate that\nlarger-scale pretraining cannot improve the xMRC performance. Instead, larger\nLLMs need further multilingual post-training to fully unlock their\ncross-lingual context retrieval potential. Our code and is available at\nhttps://github.com/NJUNLP/Cross-Lingual-Context-Retrieval", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11001", "pdf": "https://arxiv.org/pdf/2504.11001", "abs": "https://arxiv.org/abs/2504.11001", "authors": ["Alan Dao", "Thinh Le"], "title": "ReZero: Enhancing LLM search ability by trying one-more-time", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM)\nperformance on knowledge-intensive tasks but depends heavily on initial search\nquery quality. Current methods, often using Reinforcement Learning (RL),\ntypically focus on query formulation or reasoning over results, without\nexplicitly encouraging persistence after a failed search. We introduce ReZero\n(Retry-Zero), a novel RL framework that directly rewards the act of retrying a\nsearch query following an initial unsuccessful attempt. This incentivizes the\nLLM to explore alternative queries rather than prematurely halting. ReZero\ndemonstrates significant improvement, achieving 46.88% accuracy compared to a\n25% baseline. By rewarding persistence, ReZero enhances LLM robustness in\ncomplex information-seeking scenarios where initial queries may prove\ninsufficient.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11004", "pdf": "https://arxiv.org/pdf/2504.11004", "abs": "https://arxiv.org/abs/2504.11004", "authors": ["Jinwu Hu", "Wei Zhang", "Yufeng Wang", "Yu Hu", "Bin Xiao", "Mingkui Tan", "Qing Du"], "title": "Dynamic Compressing Prompts for Efficient Inference of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Under review (submited in 2024.11)", "summary": "Large Language Models (LLMs) have shown outstanding performance across a\nvariety of tasks, partly due to advanced prompting techniques. However, these\ntechniques often require lengthy prompts, which increase computational costs\nand can hinder performance because of the limited context windows of LLMs.\nWhile prompt compression is a straightforward solution, existing methods\nconfront the challenges of retaining essential information, adapting to context\nchanges, and remaining effective across different tasks. To tackle these\nissues, we propose a task-agnostic method called Dynamic Compressing Prompts\n(LLM-DCP). Our method reduces the number of prompt tokens while aiming to\npreserve the performance as much as possible. We model prompt compression as a\nMarkov Decision Process (MDP), enabling the DCP-Agent to sequentially remove\nredundant tokens by adapting to dynamic contexts and retaining crucial content.\nWe develop a reward function for training the DCP-Agent that balances the\ncompression rate, the quality of the LLM output, and the retention of key\ninformation. This allows for prompt token reduction without needing an external\nblack-box LLM. Inspired by the progressive difficulty adjustment in curriculum\nlearning, we introduce a Hierarchical Prompt Compression (HPC) training\nstrategy that gradually increases the compression difficulty, enabling the\nDCP-Agent to learn an effective compression method that maintains information\nintegrity. Experiments demonstrate that our method outperforms state-of-the-art\ntechniques, especially at higher compression rates. The code for our approach\nwill be available at https://github.com/Fhujinwu/DCP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11331", "pdf": "https://arxiv.org/pdf/2504.11331", "abs": "https://arxiv.org/abs/2504.11331", "authors": ["Hao Liu", "Lijun He", "Jiaxi Liang", "Zhihan Ren", "Fan Li"], "title": "Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis", "categories": ["cs.CL", "cs.MM"], "comment": "submitted to ACM MM2025", "summary": "Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "aspect-based", "fine-grained"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10829", "pdf": "https://arxiv.org/pdf/2504.10829", "abs": "https://arxiv.org/abs/2504.10829", "authors": ["Hengyu Shi", "Junhao Su", "Huansheng Ning", "Xiaoming Wei", "Jialin Gao"], "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Conditional layout generation aims to automatically generate visually\nappealing and semantically coherent layouts from user-defined constraints.\nWhile recent methods based on generative models have shown promising results,\nthey typically require substantial amounts of training data or extensive\nfine-tuning, limiting their versatility and practical applicability.\nAlternatively, some training-free approaches leveraging in-context learning\nwith Large Language Models (LLMs) have emerged, but they often suffer from\nlimited reasoning capabilities and overly simplistic ranking mechanisms, which\nrestrict their ability to generate consistently high-quality layouts. To this\nend, we propose LayoutCoT, a novel approach that leverages the reasoning\ncapabilities of LLMs through a combination of Retrieval-Augmented Generation\n(RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms\nlayout representations into a standardized serialized format suitable for\nprocessing by LLMs. A Layout-aware RAG is used to facilitate effective\nretrieval and generate a coarse layout by LLMs. This preliminary layout,\ntogether with the selected exemplars, is then fed into a specially designed CoT\nreasoning module for iterative refinement, significantly enhancing both\nsemantic coherence and visual quality. We conduct extensive experiments on five\npublic datasets spanning three conditional layout generation tasks.\nExperimental results demonstrate that LayoutCoT achieves state-of-the-art\nperformance without requiring training or fine-tuning. Notably, our CoT\nreasoning module enables standard LLMs, even those without explicit deep\nreasoning abilities, to outperform specialized deep-reasoning models such as\ndeepseek-R1, highlighting the potential of our approach in unleashing the deep\nreasoning capabilities of LLMs for layout generation tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["iterative refinement"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11381", "pdf": "https://arxiv.org/pdf/2504.11381", "abs": "https://arxiv.org/abs/2504.11381", "authors": ["Juan Diego Rodriguez", "Wenxuan Ding", "Katrin Erk", "Greg Durrett"], "title": "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have become generally more capable and\naccurate across many tasks, some fundamental sources of unreliability remain in\ntheir behavior. One key limitation is their inconsistency at reporting the the\nsame information when prompts are changed. In this paper, we consider the\ndiscrepancy between a model's generated answer and their own verification of\nthat answer, the generator-validator gap. We define this gap in a more\nstringent way than prior work: we expect correlation of scores from a generator\nand a validator over the entire set of candidate answers. We show that\naccording to this measure, a large gap exists in various settings, including\nquestion answering, lexical semantics tasks, and next-word prediction. We then\npropose RankAlign, a ranking-based training method, and show that it\nsignificantly closes the gap by 31.8% on average, surpassing all baseline\nmethods. Moreover, this approach generalizes well to out-of-domain tasks and\nlexical items.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "question answering"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11426", "pdf": "https://arxiv.org/pdf/2504.11426", "abs": "https://arxiv.org/abs/2504.11426", "authors": ["Xue Zhang", "Songming Zhang", "Yunlong Liang", "Fandong Meng", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "A Dual-Space Framework for General Knowledge Distillation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 9 figures, 11 tables, under review. Code is available at:\n  https://github.com/songmzhang/DSKDv2. arXiv admin note: text overlap with\n  arXiv:2406.17328", "summary": "Knowledge distillation (KD) is a promising solution to compress large\nlanguage models (LLMs) by transferring their knowledge to smaller models.\nDuring this process, white-box KD methods usually minimize the distance between\nthe output distributions of the teacher model and the student model to transfer\nmore information. However, we reveal that the current white-box KD framework\nexhibits two limitations: a) bridging probability distributions from different\noutput spaces will limit the similarity between the teacher model and the\nstudent model; b) this framework cannot be applied to LLMs with different\nvocabularies. One of the root causes for these limitations is that the\ndistributions from the teacher and the student for KD are output by different\nprediction heads, which yield distributions in different output spaces and\ndimensions. Therefore, in this paper, we propose a dual-space knowledge\ndistillation (DSKD) framework that unifies the prediction heads of the teacher\nand the student models for KD. Specifically, we first introduce two projectors\nwith ideal initialization to project the teacher/student hidden states into the\nstudent/teacher representation spaces. After this, the hidden states from\ndifferent models can share the same head and unify the output spaces of the\ndistributions. Furthermore, we develop an exact token alignment (ETA) algorithm\nto align the same tokens in two differently-tokenized sequences. Based on the\nabove, our DSKD framework is a general KD framework that supports both\noff-policy and on-policy KD, and KD between any two LLMs regardless of their\nvocabularies. Extensive experiments on instruction-following, mathematical\nreasoning, and code generation benchmarks show that DSKD significantly\noutperforms existing methods based on the current white-box KD framework and\nsurpasses other cross-tokenizer KD methods for LLMs with different\nvocabularies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11456", "pdf": "https://arxiv.org/pdf/2504.11456", "abs": "https://arxiv.org/abs/2504.11456", "authors": ["Zhiwei He", "Tian Liang", "Jiahao Xu", "Qiuzhi Liu", "Xingyu Chen", "Yue Wang", "Linfeng Song", "Dian Yu", "Zhenwen Liang", "Wenxuan Wang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "WIP", "summary": "The capacity for complex mathematical reasoning is a key benchmark for\nartificial intelligence. While reinforcement learning (RL) applied to LLMs\nshows promise, progress is significantly hindered by the lack of large-scale\ntraining data that is sufficiently challenging, possesses verifiable answer\nformats suitable for RL, and is free from contamination with evaluation\nbenchmarks. To address these limitations, we introduce DeepMath-103K, a new,\nlarge-scale dataset comprising approximately 103K mathematical problems,\nspecifically designed to train advanced reasoning models via RL. DeepMath-103K\nis curated through a rigorous pipeline involving source analysis, stringent\ndecontamination against numerous benchmarks, and filtering for high difficulty\n(primarily Levels 5-9), significantly exceeding existing open resources in\nchallenge. Each problem includes a verifiable final answer, enabling rule-based\nRL, and three distinct R1-generated solutions suitable for diverse training\nparadigms like supervised fine-tuning or distillation. Spanning a wide range of\nmathematical topics, DeepMath-103K promotes the development of generalizable\nreasoning. We demonstrate that models trained on DeepMath-103K achieve\nsignificant improvements on challenging mathematical benchmarks, validating its\neffectiveness. We release DeepMath-103K publicly to facilitate community\nprogress in building more capable AI reasoning systems:\nhttps://github.com/zwhe99/DeepMath.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "mathematical reasoning"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10738", "pdf": "https://arxiv.org/pdf/2504.10738", "abs": "https://arxiv.org/abs/2504.10738", "authors": ["Ankit Kumar Shaw", "Kun Jiang", "Tuopu Wen", "Chandan Kumar Sah", "Yining Shi", "Mengmeng Yang", "Diange Yang", "Xiaoli Lian"], "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9; I.2.7; I.2.10; I.5.5; I.5.4; I.2.11"], "comment": "Kun Jiang, Mengmeng Yang and Diange Yang are Corresponding Author.\n  The main paper and supplementary material are both included here, total 23\n  pages (main paper is 10 pages and supplementary material is 13 pages), total\n  17 figures (6 figures in main paper and 11 figures in supplementary\n  material), this paper is Accepted to CVPR WDFM-AD Workshop 2025, The code\n  will be available at https://Ankit-Zefan.github.io/CleanMap/", "summary": "The rapid growth of intelligent connected vehicles (ICVs) and integrated\nvehicle-road-cloud systems has increased the demand for accurate, real-time HD\nmap updates. However, ensuring map reliability remains challenging due to\ninconsistencies in crowdsourced data, which suffer from motion blur, lighting\nvariations, adverse weather, and lane marking degradation. This paper\nintroduces CleanMAP, a Multimodal Large Language Model (MLLM)-based\ndistillation framework designed to filter and refine crowdsourced data for\nhigh-confidence HD map updates. CleanMAP leverages an MLLM-driven lane\nvisibility scoring model that systematically quantifies key visual parameters,\nassigning confidence scores (0-10) based on their impact on lane detection. A\nnovel dynamic piecewise confidence-scoring function adapts scores based on lane\nvisibility, ensuring strong alignment with human evaluations while effectively\nfiltering unreliable data. To further optimize map accuracy, a\nconfidence-driven local map fusion strategy ranks and selects the top-k\nhighest-scoring local maps within an optimal confidence range (best score minus\n10%), striking a balance between data quality and quantity. Experimental\nevaluations on a real-world autonomous vehicle dataset validate CleanMAP's\neffectiveness, demonstrating that fusing the top three local maps achieves the\nlowest mean map update error of 0.28m, outperforming the baseline (0.37m) and\nmeeting stringent accuracy thresholds (<= 0.32m). Further validation with\nreal-vehicle data confirms 84.88% alignment with human evaluators, reinforcing\nthe model's robustness and reliability. This work establishes CleanMAP as a\nscalable and deployable solution for crowdsourced HD map updates, ensuring more\nprecise and reliable autonomous navigation. The code will be available at\nhttps://Ankit-Zefan.github.io/CleanMap/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10886", "pdf": "https://arxiv.org/pdf/2504.10886", "abs": "https://arxiv.org/abs/2504.10886", "authors": ["Jiseon Kim", "Jea Kwon", "Luiz Felipe Vecchietti", "Alice Oh", "Meeyoung Cha"], "title": "Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Accepted to ICLR 2025 Workshop - BiAlign (Bidirectional Human-AI\n  Alignment)", "summary": "Deploying large language models (LLMs) with agency in real-world applications\nraises critical questions about how these models will behave. In particular,\nhow will their decisions align with humans when faced with moral dilemmas? This\nstudy examines the alignment between LLM-driven decisions and human judgment in\nvarious contexts of the moral machine experiment, including personas reflecting\ndifferent sociodemographics. We find that the moral decisions of LLMs vary\nsubstantially by persona, showing greater shifts in moral decisions for\ncritical tasks than humans. Our data also indicate an interesting partisan\nsorting phenomenon, where political persona predominates the direction and\ndegree of LLM decisions. We discuss the ethical implications and risks\nassociated with deploying these models in applications that involve moral\ndecisions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10995", "pdf": "https://arxiv.org/pdf/2504.10995", "abs": "https://arxiv.org/abs/2504.10995", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Long Teng", "Zijun Li", "Shichao Kan"], "title": "TMCIR: Token Merge Benefits Composed Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2310.05473 by other authors", "summary": "Composed Image Retrieval (CIR) retrieves target images using a multi-modal\nquery that combines a reference image with text describing desired\nmodifications. The primary challenge is effectively fusing this visual and\ntextual information. Current cross-modal feature fusion approaches for CIR\nexhibit an inherent bias in intention interpretation. These methods tend to\ndisproportionately emphasize either the reference image features\n(visual-dominant fusion) or the textual modification intent (text-dominant\nfusion through image-to-text conversion). Such an imbalanced representation\noften fails to accurately capture and reflect the actual search intent of the\nuser in the retrieval results. To address this challenge, we propose TMCIR, a\nnovel framework that advances composed image retrieval through two key\ninnovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP\nencoders contrastively using intent-reflecting pseudo-target images,\nsynthesized from reference images and textual descriptions via a diffusion\nmodel. This step enhances the encoder ability of text to capture nuanced\nintents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune\nall encoders contrastively by comparing adaptive token-fusion features with the\ntarget image. This mechanism dynamically balances visual and textual\nrepresentations within the contrastive learning pipeline, optimizing the\ncomposed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR\ndatasets demonstrate that TMCIR significantly outperforms state-of-the-art\nmethods, particularly in capturing nuanced user intent.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11367", "pdf": "https://arxiv.org/pdf/2504.11367", "abs": "https://arxiv.org/abs/2504.11367", "authors": ["Rui Tang", "Ziyun Yong", "Shuyu Jiang", "Xingshu Chen", "Yaofang Liu", "Yi-Cheng Zhang", "Gui-Quan Sun", "Wei Wang"], "title": "Network Alignment", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "Complex networks are frequently employed to model physical or virtual complex\nsystems. When certain entities exist across multiple systems simultaneously,\nunveiling their corresponding relationships across the networks becomes\ncrucial. This problem, known as network alignment, holds significant\nimportance. It enhances our understanding of complex system structures and\nbehaviours, facilitates the validation and extension of theoretical physics\nresearch about studying complex systems, and fosters diverse practical\napplications across various fields. However, due to variations in the\nstructure, characteristics, and properties of complex networks across different\nfields, the study of network alignment is often isolated within each domain,\nwith even the terminologies and concepts lacking uniformity. This review\ncomprehensively summarizes the latest advancements in network alignment\nresearch, focusing on analyzing network alignment characteristics and progress\nin various domains such as social network analysis, bioinformatics,\ncomputational linguistics and privacy protection. It provides a detailed\nanalysis of various methods' implementation principles, processes, and\nperformance differences, including structure consistency-based methods, network\nembedding-based methods, and graph neural network-based (GNN-based) methods.\nAdditionally, the methods for network alignment under different conditions,\nsuch as in attributed networks, heterogeneous networks, directed networks, and\ndynamic networks, are presented. Furthermore, the challenges and the open\nissues for future studies are also discussed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11019", "pdf": "https://arxiv.org/pdf/2504.11019", "abs": "https://arxiv.org/abs/2504.11019", "authors": ["Hyejin Lee", "Seokjun Hong", "Jeonghoon Song", "Haechan Cho", "Zhixiong Jin", "Byeonghun Kim", "Joobin Jin", "Jaegyun Im", "Byeongjoon Noh", "Hwasoo Yeo"], "title": "DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen", "categories": ["cs.CV", "I.2.10; I.4.8; H.2.8; J.7"], "comment": "30 pages, 15 figures", "summary": "Reliable traffic data are essential for understanding urban mobility and\ndeveloping effective traffic management strategies. This study introduces the\nDRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale\nurban traffic dataset collected systematically from synchronized drone videos\nat approximately 250 meters altitude, covering nine interconnected\nintersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle\ntrajectories that include directional information, processed through video\nsynchronization and orthomap alignment, resulting in a comprehensive dataset of\n81,699 vehicle trajectories. Through our DRIFT dataset, researchers can\nsimultaneously analyze traffic at multiple scales - from individual vehicle\nmaneuvers like lane-changes and safety metrics such as time-to-collision to\naggregate network flow dynamics across interconnected urban intersections. The\nDRIFT dataset is structured to enable immediate use without additional\npreprocessing, complemented by open-source models for object detection and\ntrajectory extraction, as well as associated analytical tools. DRIFT is\nexpected to significantly contribute to academic research and practical\napplications, such as traffic flow analysis and simulation studies. The dataset\nand related resources are publicly accessible at\nhttps://github.com/AIxMobility/The-DRIFT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "safety"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11393", "pdf": "https://arxiv.org/pdf/2504.11393", "abs": "https://arxiv.org/abs/2504.11393", "authors": ["Ian Magnusson", "Nguyen Tai", "Ben Bogin", "David Heineman", "Jena D. Hwang", "Luca Soldaini", "Akshita Bhagia", "Jiacheng Liu", "Dirk Groeneveld", "Oyvind Tafjord", "Noah A. Smith", "Pang Wei Koh", "Jesse Dodge"], "title": "DataDecide: How to Predict Best Pretraining Data with Small Experiments", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Because large language models are expensive to pretrain on different\ndatasets, using smaller-scale experiments to decide on data is crucial for\nreducing costs. Which benchmarks and methods of making decisions from observed\nperformance at small scale most accurately predict the datasets that yield the\nbest large models? To empower open exploration of this question, we release\nmodels, data, and evaluations in DataDecide -- the most extensive open suite of\nmodels over differences in data and scale. We conduct controlled pretraining\nexperiments across 25 corpora with differing sources, deduplication, and\nfiltering up to 100B tokens, model sizes up to 1B parameters, and 3 random\nseeds. We find that the ranking of models at a single, small size (e.g., 150M\nparameters) is a strong baseline for predicting best models at our larger\ntarget scale (1B) (~80% of com parisons correct). No scaling law methods among\n8 baselines exceed the compute-decision frontier of single-scale predictions,\nbut DataDecide can measure improvement in future scaling laws. We also identify\nthat using continuous likelihood metrics as proxies in small experiments makes\nbenchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable\nat the target 1B scale with just 0.01% of the compute.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11134", "pdf": "https://arxiv.org/pdf/2504.11134", "abs": "https://arxiv.org/abs/2504.11134", "authors": ["Gustav Hanning", "Gabrielle Flood", "Viktor Larsson"], "title": "Visual Re-Ranking with Non-Visual Side Information", "categories": ["cs.CV"], "comment": "Accepted at Scandinavian Conference on Image Analysis (SCIA) 2025", "summary": "The standard approach for visual place recognition is to use global image\ndescriptors to retrieve the most similar database images for a given query\nimage. The results can then be further improved with re-ranking methods that\nre-order the top scoring images. However, existing methods focus on re-ranking\nbased on the same image descriptors that were used for the initial retrieval,\nwhich we argue provides limited additional signal.\n  In this work we propose Generalized Contextual Similarity Aggregation (GCSA),\nwhich is a graph neural network-based re-ranking method that, in addition to\nthe visual descriptors, can leverage other types of available side information.\nThis can for example be other sensor data (such as signal strength of nearby\nWiFi or BlueTooth endpoints) or geometric properties such as camera poses for\ndatabase images. In many applications this information is already present or\ncan be acquired with low effort. Our architecture leverages the concept of\naffinity vectors to allow for a shared encoding of the heterogeneous\nmulti-modal input. Two large-scale datasets, covering both outdoor and indoor\nlocalization scenarios, are utilized for training and evaluation. In\nexperiments we show significant improvement not only on image retrieval\nmetrics, but also for the downstream visual localization task.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11164", "pdf": "https://arxiv.org/pdf/2504.11164", "abs": "https://arxiv.org/abs/2504.11164", "authors": ["Chenming Li", "Chengxu Liu", "Yuanting Fan", "Xiao Jin", "Xingsong Hou", "Xueming Qian"], "title": "TSAL: Few-shot Text Segmentation Based on Attribute Learning", "categories": ["cs.CV"], "comment": null, "summary": "Recently supervised learning rapidly develops in scene text segmentation.\nHowever, the lack of high-quality datasets and the high cost of pixel\nannotation greatly limit the development of them. Considering the\nwell-performed few-shot learning methods for downstream tasks, we investigate\nthe application of the few-shot learning method to scene text segmentation. We\npropose TSAL, which leverages CLIP's prior knowledge to learn text attributes\nfor segmentation. To fully utilize the semantic and texture information in the\nimage, a visual-guided branch is proposed to separately extract text and\nbackground features. To reduce data dependency and improve text detection\naccuracy, the adaptive prompt-guided branch employs effective adaptive prompt\ntemplates to capture various text attributes. To enable adaptive prompts\ncapture distinctive text features and complex background distribution, we\npropose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of\ndifferent attributes with visual features and prompt prototypes, AFA enables\nadaptive prompts to capture both general and distinctive attribute information.\nTSAL can capture the unique attributes of text and achieve precise segmentation\nusing only few images. Experiments demonstrate that our method achieves SOTA\nperformance on multiple text segmentation datasets under few-shot settings and\nshow great potential in text-related domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11218", "pdf": "https://arxiv.org/pdf/2504.11218", "abs": "https://arxiv.org/abs/2504.11218", "authors": ["Zeming wei", "Junyi Lin", "Yang Liu", "Weixing Chen", "Jingzhou Luo", "Guanbin Li", "Liang Lin"], "title": "3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians", "categories": ["cs.CV"], "comment": "The first large-scale 3D Gaussians Affordance Reasoning Benchmark", "summary": "3D affordance reasoning is essential in associating human instructions with\nthe functional regions of 3D objects, facilitating precise, task-oriented\nmanipulations in embodied AI. However, current methods, which predominantly\ndepend on sparse 3D point clouds, exhibit limited generalizability and\nrobustness due to their sensitivity to coordinate variations and the inherent\nsparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers\nhigh-fidelity, real-time rendering with minimal computational overhead by\nrepresenting scenes as dense, continuous distributions. This positions 3DGS as\na highly effective approach for capturing fine-grained affordance details and\nimproving recognition accuracy. Nevertheless, its full potential remains\nlargely untapped due to the absence of large-scale, 3DGS-specific affordance\ndatasets. To overcome these limitations, we present 3DAffordSplat, the first\nlarge-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning.\nThis dataset includes 23,677 Gaussian instances, 8,354 point cloud instances,\nand 6,631 manually annotated affordance labels, encompassing 21 object\ncategories and 18 affordance types. Building upon this dataset, we introduce\nAffordSplatNet, a novel model specifically designed for affordance reasoning\nusing 3DGS representations. AffordSplatNet features an innovative cross-modal\nstructure alignment module that exploits structural consistency priors to align\n3D point cloud and 3DGS representations, resulting in enhanced affordance\nrecognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat\ndataset significantly advances affordance learning within the 3DGS domain,\nwhile AffordSplatNet consistently outperforms existing methods across both seen\nand unseen settings, highlighting its robust generalization capabilities.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11289", "pdf": "https://arxiv.org/pdf/2504.11289", "abs": "https://arxiv.org/abs/2504.11289", "authors": ["Xiang Wang", "Shiwei Zhang", "Longxiang Tang", "Yingya Zhang", "Changxin Gao", "Yuehuan Wang", "Nong Sang"], "title": "UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer", "categories": ["cs.CV"], "comment": "The training and inference code (based on Wan2.1) is available at\n  https://github.com/ali-vilab/UniAnimate-DiT", "summary": "This report presents UniAnimate-DiT, an advanced project that leverages the\ncutting-edge and powerful capabilities of the open-source Wan2.1 model for\nconsistent human image animation. Specifically, to preserve the robust\ngenerative capabilities of the original Wan2.1 model, we implement Low-Rank\nAdaptation (LoRA) technique to fine-tune a minimal set of parameters,\nsignificantly reducing training memory overhead. A lightweight pose encoder\nconsisting of multiple stacked 3D convolutional layers is designed to encode\nmotion information of driving poses. Furthermore, we adopt a simple\nconcatenation operation to integrate the reference appearance into the model\nand incorporate the pose information of the reference image for enhanced pose\nalignment. Experimental results show that our approach achieves visually\nappearing and temporally consistent high-fidelity animations. Trained on 480p\n(832x480) videos, UniAnimate-DiT demonstrates strong generalization\ncapabilities to seamlessly upscale to 720P (1280x720) during inference. The\ntraining and inference code is publicly available at\nhttps://github.com/ali-vilab/UniAnimate-DiT.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11306", "pdf": "https://arxiv.org/pdf/2504.11306", "abs": "https://arxiv.org/abs/2504.11306", "authors": ["Trinnhallen Brisley", "Aryan Gandhi", "Joseph Magen"], "title": "Context-Aware Palmprint Recognition via a Relative Similarity Metric", "categories": ["cs.CV"], "comment": null, "summary": "We propose a new approach to matching mechanism for palmprint recognition by\nintroducing a Relative Similarity Metric (RSM) that enhances the robustness and\ndiscriminability of existing matching frameworks. While conventional systems\nrely on direct pairwise similarity measures, such as cosine or Euclidean\ndistances, these metrics fail to capture how a pairwise similarity compares\nwithin the context of the entire dataset. Our method addresses this by\nevaluating the relative consistency of similarity scores across up to all\nidentities, allowing for better suppression of false positives and negatives.\nApplied atop the CCNet architecture, our method achieves a new state-of-the-art\n0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous\nmethods and demonstrating the efficacy of incorporating relational structure\ninto the palmprint matching process.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11368", "pdf": "https://arxiv.org/pdf/2504.11368", "abs": "https://arxiv.org/abs/2504.11368", "authors": ["Jingkun Chen", "Haoran Duan", "Xiao Zhang", "Boyan Gao", "Tao Tan", "Vicente Grau", "Jungong Han"], "title": "From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation", "categories": ["cs.CV", "68T45", "I.2.10; I.4.8"], "comment": "10 pages, 5 figures", "summary": "Medical image segmentation remains challenging due to the high cost of\npixel-level annotations for training. In the context of weak supervision,\nclinician gaze data captures regions of diagnostic interest; however, its\nsparsity limits its use for segmentation. In contrast, vision-language models\n(VLMs) provide semantic context through textual descriptions but lack the\nexplanation precision required. Recognizing that neither source alone suffices,\nwe propose a teacher-student framework that integrates both gaze and language\nsupervision, leveraging their complementary strengths. Our key insight is that\ngaze data indicates where clinicians focus during diagnosis, while VLMs explain\nwhy those regions are significant. To implement this, the teacher model first\nlearns from gaze points enhanced by VLM-generated descriptions of lesion\nmorphology, establishing a foundation for guiding the student model. The\nteacher then directs the student through three strategies: (1) Multi-scale\nfeature alignment to fuse visual cues with textual semantics; (2)\nConfidence-weighted consistency constraints to focus on reliable predictions;\n(3) Adaptive masking to limit error propagation in uncertain areas. Experiments\non the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves\nDice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over\ngaze baselines without increasing the annotation burden. By preserving\ncorrelations among predictions, gaze data, and lesion descriptions, our\nframework also maintains clinical interpretability. This work illustrates how\nintegrating human visual attention with AI-generated semantic context can\neffectively overcome the limitations of individual weak supervision signals,\nthereby advancing the development of deployable, annotation-efficient medical\nAI systems. Code is available at: https://github.com/jingkunchen/FGI.git.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11415", "pdf": "https://arxiv.org/pdf/2504.11415", "abs": "https://arxiv.org/abs/2504.11415", "authors": ["Nikolette Pedersen", "Regitze Sydendal", "Andreas Wulff", "Ralf Raumanns", "Eike Petersen", "Veronika Cheplygina"], "title": "Robustness and sex differences in skin cancer detection: logistic regression vs CNNs", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages (excluding appendix), 2 figures (excluding appendix),\n  submitted to MIUA 2025 conference (response pending)", "summary": "Deep learning has been reported to achieve high performances in the detection\nof skin cancer, yet many challenges regarding the reproducibility of results\nand biases remain. This study is a replication (different data, same analysis)\nof a study on Alzheimer's disease [28] which studied robustness of logistic\nregression (LR) and convolutional neural networks (CNN) across patient sexes.\nWe explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset\nwith LR trained on handcrafted features reflecting dermatological guidelines\n(ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We\nevaluate these models in alignment with [28]: across multiple training datasets\nwith varied sex composition to determine their robustness. Our results show\nthat both the LR and the CNN were robust to the sex distributions, but the\nresults also revealed that the CNN had a significantly higher accuracy (ACC)\nand area under the receiver operating characteristics (AUROC) for male patients\nthan for female patients. We hope these findings to contribute to the growing\nfield of investigating potential bias in popular medical machine learning\nmethods. The data and relevant scripts to reproduce our results can be found in\nour Github.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11423", "pdf": "https://arxiv.org/pdf/2504.11423", "abs": "https://arxiv.org/abs/2504.11423", "authors": ["Dazhong Shen", "Guanglu Song", "Yi Zhang", "Bingqi Ma", "Lujundong Li", "Dongzhi Jiang", "Zhuofan Zong", "Yu Liu"], "title": "ADT: Tuning Diffusion Models with Adversarial Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved outstanding image generation by reversing a\nforward noising process to approximate true data distributions. During\ntraining, these models predict diffusion scores from noised versions of true\nsamples in a single forward pass, while inference requires iterative denoising\nstarting from white noise. This training-inference divergences hinder the\nalignment between inference and training data distributions, due to potential\nprediction biases and cumulative error accumulation. To address this problem,\nwe propose an intuitive but effective fine-tuning framework, called Adversarial\nDiffusion Tuning (ADT), by stimulating the inference process during\noptimization and aligning the final outputs with training data by adversarial\nsupervision. Specifically, to achieve robust adversarial training, ADT features\na siamese-network discriminator with a fixed pre-trained backbone and\nlightweight trainable parameters, incorporates an image-to-image sampling\nstrategy to smooth discriminative difficulties, and preserves the original\ndiffusion loss to prevent discriminator hacking. In addition, we carefully\nconstrain the backward-flowing path for back-propagating gradients along the\ninference path without incurring memory overload or gradient explosion.\nFinally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3),\ndemonstrate that ADT significantly improves both distribution alignment and\nimage quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11457", "pdf": "https://arxiv.org/pdf/2504.11457", "abs": "https://arxiv.org/abs/2504.11457", "authors": ["Ziqi Pang", "Xin Xu", "Yu-Xiong Wang"], "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "With the success of image generation, generative diffusion models are\nincreasingly adopted for discriminative tasks, as pixel generation provides a\nunified perception interface. However, directly repurposing the generative\ndenoising process for discriminative objectives reveals critical gaps rarely\naddressed previously. Generative models tolerate intermediate sampling errors\nif the final distribution remains plausible, but discriminative tasks require\nrigorous accuracy throughout, as evidenced in challenging multi-modal tasks\nlike referring image segmentation. Motivated by this gap, we analyze and\nenhance alignment between generative diffusion processes and perception tasks,\nfocusing on how perception quality evolves during denoising. We find: (1)\nearlier denoising steps contribute disproportionately to perception quality,\nprompting us to propose tailored learning objectives reflecting varying\ntimestep contributions; (2) later denoising steps show unexpected perception\ndegradation, highlighting sensitivity to training-denoising distribution\nshifts, addressed by our diffusion-tailored data augmentation; and (3)\ngenerative processes uniquely enable interactivity, serving as controllable\nuser interfaces adaptable to correctional prompts in multi-round interactions.\nOur insights significantly improve diffusion-based perception models without\narchitectural changes, achieving state-of-the-art performance on depth\nestimation, referring image segmentation, and generalist perception tasks. Code\navailable at https://github.com/ziqipang/ADDP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2410.10291", "pdf": "https://arxiv.org/pdf/2410.10291", "abs": "https://arxiv.org/abs/2410.10291", "authors": ["Xiangru Zhu", "Penglei Sun", "Yaoxian Song", "Yanghua Xiao", "Zhixu Li", "Chengyu Wang", "Jun Huang", "Bei Yang", "Xiaoxiao Xu"], "title": "Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by ICLR 2025", "summary": "Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.10916", "pdf": "https://arxiv.org/pdf/2504.10916", "abs": "https://arxiv.org/abs/2504.10916", "authors": ["Zhenyu Yang", "Haiming Zhu", "Rihui Zhang", "Haipeng Zhang", "Jianliang Wang", "Chunhao Wang", "Minbin Chen", "Fang-Fang Yin"], "title": "Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification", "categories": ["physics.med-ph", "cs.CV"], "comment": "27 pages, 3 figures", "summary": "Background: Deep learning has significantly advanced medical image analysis,\nwith Vision Transformers (ViTs) offering a powerful alternative to\nconvolutional models by modeling long-range dependencies through\nself-attention. However, ViTs are inherently data-intensive and lack\ndomain-specific inductive biases, limiting their applicability in medical\nimaging. In contrast, radiomics provides interpretable, handcrafted descriptors\nof tissue heterogeneity but suffers from limited scalability and integration\ninto end-to-end learning frameworks. In this work, we propose the\nRadiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features\nwith data-driven visual embeddings within a ViT backbone.\n  Purpose: To develop a hybrid RE-ViT framework that integrates radiomics and\npatch-wise ViT embeddings through early fusion, enhancing robustness and\nperformance in medical image classification.\n  Methods: Following the standard ViT pipeline, images were divided into\npatches. For each patch, handcrafted radiomic features were extracted and fused\nwith linearly projected pixel embeddings. The fused representations were\nnormalized, positionally encoded, and passed to the ViT encoder. A learnable\n[CLS] token aggregated patch-level information for classification. We evaluated\nRE-ViT on three public datasets (including BUSI, ChestXray2017, and Retinal\nOCT) using accuracy, macro AUC, sensitivity, and specificity. RE-ViT was\nbenchmarked against CNN-based (VGG-16, ResNet) and hybrid (TransMed) models.\n  Results: RE-ViT achieved state-of-the-art results: on BUSI,\nAUC=0.950+/-0.011; on ChestXray2017, AUC=0.989+/-0.004; on Retinal OCT,\nAUC=0.986+/-0.001, which outperforms other comparison models.\n  Conclusions: The RE-ViT framework effectively integrates radiomics with ViT\narchitectures, demonstrating improved performance and generalizability across\nmultimodal medical image classification tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11031", "pdf": "https://arxiv.org/pdf/2504.11031", "abs": "https://arxiv.org/abs/2504.11031", "authors": ["Timm Linder", "Kadir Yilmaz", "David B. Adrian", "Bastian Leibe"], "title": "Acquisition of high-quality images for camera calibration in robotics applications via speech prompts", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 6 figures", "summary": "Accurate intrinsic and extrinsic camera calibration can be an important\nprerequisite for robotic applications that rely on vision as input. While there\nis ongoing research on enabling camera calibration using natural images, many\nsystems in practice still rely on using designated calibration targets with\ne.g. checkerboard patterns or April tag grids. Once calibration images from\ndifferent perspectives have been acquired and feature descriptors detected,\nthose are typically used in an optimization process to minimize the geometric\nreprojection error. For this optimization to converge, input images need to be\nof sufficient quality and particularly sharpness; they should neither contain\nmotion blur nor rolling-shutter artifacts that can arise when the calibration\nboard was not static during image capture. In this work, we present a novel\ncalibration image acquisition technique controlled via voice commands recorded\nwith a clip-on microphone, that can be more robust and user-friendly than e.g.\ntriggering capture with a remote control, or filtering out blurry frames from a\nvideo sequence in postprocessing. To achieve this, we use a state-of-the-art\nspeech-to-text transcription model with accurate per-word timestamping to\ncapture trigger words with precise temporal alignment. Our experiments show\nthat the proposed method improves user experience by being fast and efficient,\nallowing us to successfully calibrate complex multi-camera setups.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
{"id": "2504.11247", "pdf": "https://arxiv.org/pdf/2504.11247", "abs": "https://arxiv.org/abs/2504.11247", "authors": ["Fikrican zgr", "Ren Zurbrgg", "Suryansh Kumar"], "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "10 pages, 9 figures, 6 tables", "summary": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art\nalgorithm for achieving sample-efficient multi-goal reinforcement learning (RL)\nin robotic manipulation tasks with binary rewards. HER facilitates learning\nfrom failed attempts by replaying trajectories with redefined goals. However,\nit relies on a heuristic-based replay method that lacks a principled framework.\nTo address this limitation, we introduce a novel replay strategy,\n\"Next-Future\", which focuses on rewarding single-step transitions. This\napproach significantly enhances sample efficiency and accuracy in learning\nmulti-goal Markov decision processes (MDPs), particularly under stringent\naccuracy requirements -- a critical aspect for performing complex and precise\nrobotic-arm tasks. We demonstrate the efficacy of our method by highlighting\nhow single-step learning enables improved value approximation within the\nmulti-goal RL framework. The performance of the proposed replay strategy is\nevaluated across eight challenging robotic manipulation tasks, using ten random\nseeds for training. Our results indicate substantial improvements in sample\nefficiency for seven out of eight tasks and higher success rates in six tasks.\nFurthermore, real-world experiments validate the practical feasibility of the\nlearned policies, demonstrating the potential of \"Next-Future\" in solving\ncomplex robotic-arm tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-16.jsonl"}
