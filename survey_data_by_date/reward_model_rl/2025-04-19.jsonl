{"id": "2504.12522", "pdf": "https://arxiv.org/pdf/2504.12522", "abs": "https://arxiv.org/abs/2504.12522", "authors": ["Alexander Shypula", "Shuo Li", "Botong Zhang", "Vishakh Padmakumar", "Kayo Yin", "Osbert Bastani"], "title": "Evaluating the Diversity and Quality of LLM Generated Content", "categories": ["cs.CL", "cs.AI"], "comment": "ICLR 2025 Third Workshop on Deep Learning for Code", "summary": "Recent work suggests that preference-tuning techniques--including\nReinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO,\nas well as alternatives like DPO--reduce diversity, creating a dilemma given\nthat such models are widely deployed in applications requiring diverse outputs.\nTo address this, we introduce a framework for measuring effective semantic\ndiversity--diversity among outputs that meet quality thresholds--which better\nreflects the practical utility of large language models (LLMs). Using\nopen-ended tasks that require no human intervention, we find counterintuitive\nresults: although preference-tuned models--especially those trained via\nRL--exhibit reduced lexical and syntactic diversity, they produce greater\neffective semantic diversity than SFT or base models, not from increasing\ndiversity among high-quality outputs, but from generating more high-quality\noutputs overall. We discover that preference tuning reduces syntactic diversity\nwhile preserving semantic diversity--revealing a distinction between diversity\nin form and diversity in content that traditional metrics often overlook. Our\nanalysis further shows that smaller models are consistently more\nparameter-efficient at generating unique content within a fixed sampling\nbudget, offering insights into the relationship between model scaling and\ndiversity. These findings have important implications for applications that\nrequire diverse yet high-quality outputs, from creative assistance to synthetic\ndata generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "PPO", "reinforcement learning", "preference", "DPO"], "score": 5}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13134", "pdf": "https://arxiv.org/pdf/2504.13134", "abs": "https://arxiv.org/abs/2504.13134", "authors": ["Anamika Lochab", "Ruqi Zhang"], "title": "Energy-Based Reward Models for Robust Language Model Alignment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "alignment", "reward hacking"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12328", "pdf": "https://arxiv.org/pdf/2504.12328", "abs": "https://arxiv.org/abs/2504.12328", "authors": ["Jialun Zhong", "Wei Shen", "Yanzeng Li", "Songyang Gao", "Hua Lu", "Yicheng Chen", "Yang Zhang", "Wei Zhou", "Jinjie Gu", "Lei Zou"], "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reward Model (RM) has demonstrated impressive potential for enhancing Large\nLanguage Models (LLM), as RM can serve as a proxy for human preferences,\nproviding signals to guide LLMs' behavior in various tasks. In this paper, we\nprovide a comprehensive overview of relevant research, exploring RMs from the\nperspectives of preference collection, reward modeling, and usage. Next, we\nintroduce the applications of RMs and discuss the benchmarks for evaluation.\nFurthermore, we conduct an in-depth analysis of the challenges existing in the\nfield and dive into the potential research directions. This paper is dedicated\nto providing beginners with a comprehensive introduction to RMs and\nfacilitating future studies. The resources are publicly available at\ngithub\\footnote{https://github.com/JLZhong23/awesome-reward-models}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12833", "pdf": "https://arxiv.org/pdf/2504.12833", "abs": "https://arxiv.org/abs/2504.12833", "authors": ["Elior Benarous", "Yilun Du", "Heng Yang"], "title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "RLAIF", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13122", "pdf": "https://arxiv.org/pdf/2504.13122", "abs": "https://arxiv.org/abs/2504.13122", "authors": ["Haojian Huang", "Haodong Chen", "Shengqiong Wu", "Meng Luo", "Jinlan Fu", "Xinya Du", "Hanwang Zhang", "Hao Fei"], "title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "categories": ["cs.CV", "cs.LG"], "comment": "Code and Data: https://github.com/HaroldChen19/VistaDPO", "summary": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13125", "pdf": "https://arxiv.org/pdf/2504.13125", "abs": "https://arxiv.org/abs/2504.13125", "authors": ["Varun Rao", "Youran Sun", "Mahendra Kumar", "Tejas Mutneja", "Agastya Mukherjee", "Haizhao Yang"], "title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12558", "pdf": "https://arxiv.org/pdf/2504.12558", "abs": "https://arxiv.org/abs/2504.12558", "authors": ["Negar Arabzadeh", "Charles L. A. Clarke"], "title": "Benchmarking LLM-based Relevance Judgment Methods", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "question answering"], "score": 3}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12663", "pdf": "https://arxiv.org/pdf/2504.12663", "abs": "https://arxiv.org/abs/2504.12663", "authors": ["Xiaotian Zhang", "Ruizhe Chen", "Yang Feng", "Zuozhu Liu"], "title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12911", "pdf": "https://arxiv.org/pdf/2504.12911", "abs": "https://arxiv.org/abs/2504.12911", "authors": ["Chengyi Ju", "Weijie Shi", "Chengzhong Liu", "Jiaming Ji", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "Benchmarking Multi-National Value Alignment for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311", "abs": "https://arxiv.org/abs/2504.12311", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12442", "pdf": "https://arxiv.org/pdf/2504.12442", "abs": "https://arxiv.org/abs/2504.12442", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "3D-PointZshotS: Geometry-Aware 3D Point Cloud Zero-Shot Semantic Segmentation Narrowing the Visual-Semantic Gap", "categories": ["cs.CV"], "comment": null, "summary": "Existing zero-shot 3D point cloud segmentation methods often struggle with\nlimited transferability from seen classes to unseen classes and from semantic\nto visual space. To alleviate this, we introduce 3D-PointZshotS, a\ngeometry-aware zero-shot segmentation framework that enhances both feature\ngeneration and alignment using latent geometric prototypes (LGPs).\nSpecifically, we integrate LGPs into a generator via a cross-attention\nmechanism, enriching semantic features with fine-grained geometric details. To\nfurther enhance stability and generalization, we introduce a self-consistency\nloss, which enforces feature robustness against point-wise perturbations.\nAdditionally, we re-represent visual and semantic features in a shared space,\nbridging the semantic-visual gap and facilitating knowledge transfer to unseen\nclasses. Experiments on three real-world datasets, namely ScanNet,\nSemanticKITTI, and S3DIS, demonstrate that our method achieves superior\nperformance over four baselines in terms of harmonic mIoU. The code is\navailable at \\href{https://github.com/LexieYang/3D-PointZshotS}{Github}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12800", "pdf": "https://arxiv.org/pdf/2504.12800", "abs": "https://arxiv.org/abs/2504.12800", "authors": ["Yifei Tong", "Runze Tian", "Xiao Han", "Dingyao Liu", "Fenggen Yu", "Yan Zhang"], "title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12515", "pdf": "https://arxiv.org/pdf/2504.12515", "abs": "https://arxiv.org/abs/2504.12515", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "Event Quality Score (EQS): Assessing the Realism of Simulated Event Camera Streams via Distances in Latent Space", "categories": ["cs.CV"], "comment": "Accepted at 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Fifth International Workshop on Event-Based\n  Vision", "summary": "Event cameras promise a paradigm shift in vision sensing with their low\nlatency, high dynamic range, and asynchronous nature of events. Unfortunately,\nthe scarcity of high-quality labeled datasets hinders their widespread adoption\nin deep learning-driven computer vision. To mitigate this, several simulators\nhave been proposed to generate synthetic event data for training models for\ndetection and estimation tasks. However, the fundamentally different sensor\ndesign of event cameras compared to traditional frame-based cameras poses a\nchallenge for accurate simulation. As a result, most simulated data fail to\nmimic data captured by real event cameras. Inspired by existing work on using\ndeep features for image comparison, we introduce event quality score (EQS), a\nquality metric that utilizes activations of the RVT architecture. Through\nsim-to-real experiments on the DSEC driving dataset, it is shown that a higher\nEQS implies improved generalization to real-world data after training on\nsimulated events. Thus, optimizing for EQS can lead to developing more\nrealistic event camera simulators, effectively reducing the simulation gap. EQS\nis available at https://github.com/eventbasedvision/EQS.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13072", "pdf": "https://arxiv.org/pdf/2504.13072", "abs": "https://arxiv.org/abs/2504.13072", "authors": ["Wenqi Dong", "Bangbang Yang", "Zesong Yang", "Yuan Li", "Tao Hu", "Hujun Bao", "Yuewen Ma", "Zhaopeng Cui"], "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Project webpage: https://zju3dv.github.io/hiscene/", "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324", "abs": "https://arxiv.org/abs/2504.12324", "authors": ["Mengying Yuan", "Wangzi Xuan", "Fei Li"], "title": "Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in both natural\nlanguage processing and information retrieval. While NLI has developed many\nsub-directions such as sentence-level NLI, document-level NLI and cross-lingual\nNLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In\nthis paper, we propose a novel paradigm for CDCL-NLI that extends traditional\nNLI capabilities to multi-document, multilingual scenarios. To support this\ntask, we construct a high-quality CDCL-NLI dataset including 1,110 instances\nand spanning 26 languages. To build a baseline for this task, we also propose\nan innovative method that integrates RST-enhanced graph fusion and\ninterpretability prediction. Our method employs RST (Rhetorical Structure\nTheory) on RGAT (Relation-aware Graph Attention Network) for cross-document\ncontext modeling, coupled with a structure-aware semantic alignment mechanism\nbased on lexical chains for cross-lingual understanding. For NLI\ninterpretability, we develop an EDU-level attribution framework that generates\nextractive explanations. Extensive experiments demonstrate our approach's\nsuperior performance, achieving significant improvements over both traditional\nNLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our\nwork sheds light on the study of NLI and will bring research interest on\ncross-document cross-lingual context understanding, semantic retrieval and\ninterpretability inference. Our dataset and code are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer\nreview}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12339", "pdf": "https://arxiv.org/pdf/2504.12339", "abs": "https://arxiv.org/abs/2504.12339", "authors": ["Yaodong Song", "Hongjie Chen", "Jie Lian", "Yuxin Zhang", "Guangmin Xia", "Zehan Li", "Genliang Zhao", "Jian Kang", "Yongxiang Li", "Jie Li"], "title": "GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS)\nsynthesis through discrete tokenization paradigms, current architectures\nexhibit fundamental tensions between three critical dimensions: 1) irreversible\nloss of acoustic characteristics caused by quantization of speech prompts; 2)\nstringent dependence on precisely aligned prompt speech-text pairs that limit\nreal-world deployment; and 3) catastrophic forgetting of the LLM's native text\ncomprehension during optimization for speech token generation. To address these\nchallenges, we propose an LLM-based text-to-speech Generation approach\nOptimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework\nintroduces two key innovations: (1) The modality-alignment branch combines a\nspeech encoder and projector to capture continuous acoustic embeddings,\nenabling bidirectional correlation between paralinguistic features (language,\ntimbre, emotion) and semantic text representations without transcript\ndependency; (2) The speech-generation branch employs modular fine-tuning on\ntop-k layers of an LLM for speech token prediction while freezing the bottom-k\nlayers to preserve foundational linguistic knowledge. Moreover, multi-token\nprediction is introduced to support real-time streaming TTS synthesis.\nExperimental results demonstrate that our GOAT-TTS achieves performance\ncomparable to state-of-the-art TTS models while validating the efficacy of\nsynthesized dialect speech data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12344", "pdf": "https://arxiv.org/pdf/2504.12344", "abs": "https://arxiv.org/abs/2504.12344", "authors": ["Nay Myat Min", "Long H. Pham", "Yige Li", "Jun Sun"], "title": "Propaganda via AI? A Study on Semantic Backdoors in Large Language Models", "categories": ["cs.CL"], "comment": "18 pages, 1 figure", "summary": "Large language models (LLMs) demonstrate remarkable performance across myriad\nlanguage tasks, yet they remain vulnerable to backdoor attacks, where\nadversaries implant hidden triggers that systematically manipulate model\noutputs. Traditional defenses focus on explicit token-level anomalies and\ntherefore overlook semantic backdoors-covert triggers embedded at the\nconceptual level (e.g., ideological stances or cultural references) that rely\non meaning-based cues rather than lexical oddities. We first show, in a\ncontrolled finetuning setting, that such semantic backdoors can be implanted\nwith only a small poisoned corpus, establishing their practical feasibility. We\nthen formalize the notion of semantic backdoors in LLMs and introduce a\nblack-box detection framework, RAVEN (short for \"Response Anomaly Vigilance for\nuncovering semantic backdoors\"), which combines semantic entropy with\ncross-model consistency analysis. The framework probes multiple models with\nstructured topic-perspective prompts, clusters the sampled responses via\nbidirectional entailment, and flags anomalously uniform outputs; cross-model\ncomparison isolates model-specific anomalies from corpus-wide biases. Empirical\nevaluations across diverse LLM families (GPT-4o, Llama, DeepSeek, Mistral)\nuncover previously undetected semantic backdoors, providing the first\nproof-of-concept evidence of these hidden vulnerabilities and underscoring the\nurgent need for concept-level auditing of deployed language models. We\nopen-source our code and data at https://github.com/NayMyatMin/RAVEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12717", "pdf": "https://arxiv.org/pdf/2504.12717", "abs": "https://arxiv.org/abs/2504.12717", "authors": ["Shin'ya Yamaguchi", "Dewei Feng", "Sekitoshi Kanai", "Kazuki Adachi", "Daiki Chijiwa"], "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025; Code: https://github.com/yshinya6/clip-refine", "summary": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12491", "pdf": "https://arxiv.org/pdf/2504.12491", "abs": "https://arxiv.org/abs/2504.12491", "authors": ["Hansi Zeng", "Kai Hui", "Honglei Zhuang", "Zhen Qin", "Zhenrui Yue", "Hamed Zamani", "Dana Alon"], "title": "Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?", "categories": ["cs.CL"], "comment": null, "summary": "While metrics available during pre-training, such as perplexity, correlate\nwell with model performance at scaling-laws studies, their predictive\ncapacities at a fixed model size remain unclear, hindering effective model\nselection and development. To address this gap, we formulate the task of\nselecting pre-training checkpoints to maximize downstream fine-tuning\nperformance as a pairwise classification problem: predicting which of two LLMs,\ndiffering in their pre-training, will perform better after supervised\nfine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants\nwith systematically varied pre-training configurations, e.g., objectives or\ndata, and evaluate them on diverse downstream tasks after SFT. We first conduct\na study and demonstrate that the conventional perplexity is a misleading\nindicator. As such, we introduce novel unsupervised and supervised proxy\nmetrics derived from pre-training that successfully reduce the relative\nperformance prediction error rate by over 50%. Despite the inherent complexity\nof this task, we demonstrate the practical utility of our proposed proxies in\nspecific scenarios, paving the way for more efficient design of pre-training\nschemes optimized for various downstream tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12553", "pdf": "https://arxiv.org/pdf/2504.12553", "abs": "https://arxiv.org/abs/2504.12553", "authors": ["Zahra Pourbahman", "Fatemeh Rajabi", "Mohammadhossein Sadeghi", "Omid Ghahroodi", "Somaye Bakhshaei", "Arash Amini", "Reza Kazemi", "Mahdieh Soleymani Baghshah"], "title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "safety"], "score": 4}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13042", "pdf": "https://arxiv.org/pdf/2504.13042", "abs": "https://arxiv.org/abs/2504.13042", "authors": ["Dachun Kai", "Yueyi Zhang", "Jin Wang", "Zeyu Xiao", "Zhiwei Xiong", "Xiaoyan Sun"], "title": "Event-Enhanced Blurry Video Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2025. Project page:\n  https://dachunkai.github.io/evtexture.github.io/", "summary": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12734", "pdf": "https://arxiv.org/pdf/2504.12734", "abs": "https://arxiv.org/abs/2504.12734", "authors": ["Yongrui Chen", "Junhao He", "Linbo Fu", "Shenyu Zhang", "Rihui Jin", "Xinbang Dai", "Jiaqi Li", "Dehai Min", "Nan Hu", "Yuxin Zhang", "Guilin Qi", "Yi Huang", "Tongtong Wu"], "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13055", "pdf": "https://arxiv.org/pdf/2504.13055", "abs": "https://arxiv.org/abs/2504.13055", "authors": ["Xiangyan Liu", "Jinjie Ni", "Zijian Wu", "Chao Du", "Longxu Dou", "Haonan Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12767", "pdf": "https://arxiv.org/pdf/2504.12767", "abs": "https://arxiv.org/abs/2504.12767", "authors": ["Fatma Elsafoury", "David Hartmann"], "title": "Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts", "categories": ["cs.CL"], "comment": null, "summary": "We know that language models (LMs) form biases and stereotypes of minorities,\nleading to unfair treatments of members of these groups, thanks to research\nmainly in the US and the broader English-speaking world. As the negative\nbehavior of these models has severe consequences for society and individuals,\nindustry and academia are actively developing methods to reduce the bias in\nLMs. However, there are many under-represented groups and languages that have\nbeen overlooked so far. This includes marginalized groups that are specific to\nindividual countries and regions in the English speaking and Western world, but\ncrucially also almost all marginalized groups in the rest of the world. The UN\nestimates, that between 600 million to 1.2 billion people worldwide are members\nof marginalized groups and in need for special protection. If we want to\ndevelop inclusive LMs that work for everyone, we have to broaden our\nunderstanding to include overlooked marginalized groups and low-resource\nlanguages and dialects.\n  In this work, we contribute to this effort with the first study investigating\noffensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,\nthe remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we\ninvestigate the impact of low-resource languages and dialects on the study of\nbias in LMs, demonstrating the limitations of current bias metrics, as we\nmeasure significantly higher bias when using the Egyptian Arabic dialect versus\nModern Standard Arabic. Our results show, LMs indeed show higher bias against\nmany marginalized groups in comparison to dominant groups. However, this is not\nthe case for Arabic LMs, where the bias is high against both marginalized and\ndominant groups in relation to religion and ethnicity.\n  Our results also show higher intersectional bias against Non-binary, LGBTQIA+\nand Black women.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13074", "pdf": "https://arxiv.org/pdf/2504.13074", "abs": "https://arxiv.org/abs/2504.13074", "authors": ["Guibin Chen", "Dixuan Lin", "Jiangping Yang", "Chunze Lin", "Juncheng Zhu", "Mingyuan Fan", "Hao Zhang", "Sheng Chen", "Zheng Chen", "Chengchen Ma", "Weiming Xiong", "Wei Wang", "Nuo Pang", "Kang Kang", "Zhiheng Xu", "Yuzhe Jin", "Yupeng Liang", "Yubing Song", "Peng Zhao", "Boyuan Xu", "Di Qiu", "Debang Li", "Zhengcong Fei", "Yang Li", "Yahui Zhou"], "title": "SkyReels-V2: Infinite-length Film Generative Model", "categories": ["cs.CV"], "comment": "31 pages,10 figures", "summary": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13099", "pdf": "https://arxiv.org/pdf/2504.13099", "abs": "https://arxiv.org/abs/2504.13099", "authors": ["Ranjan Sapkota", "Rahul Harsha Cheppally", "Ajay Sharda", "Manoj Karkee"], "title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity", "categories": ["cs.CV"], "comment": null, "summary": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12913", "pdf": "https://arxiv.org/pdf/2504.12913", "abs": "https://arxiv.org/abs/2504.12913", "authors": ["Fanyi Yang", "Jianfeng Liu", "Xin Zhang", "Haoyu Liu", "Xixin Cao", "Yuefeng Zhan", "Hao Sun", "Weiwei Deng", "Feng Sun", "Qi Zhang"], "title": "MAIN: Mutual Alignment Is Necessary for instruction tuning", "categories": ["cs.CL"], "comment": null, "summary": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13123", "pdf": "https://arxiv.org/pdf/2504.13123", "abs": "https://arxiv.org/abs/2504.13123", "authors": ["Xinsong Zhang", "Yarong Zeng", "Xinting Huang", "Hu Hu", "Runquan Xie", "Han Hu", "Zhanhui Kang"], "title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["DPO"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13129", "pdf": "https://arxiv.org/pdf/2504.13129", "abs": "https://arxiv.org/abs/2504.13129", "authors": ["Jialuo Li", "Wenhao Chai", "Xingyu Fu", "Haiyang Xu", "Saining Xie"], "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://jialuo-li.github.io/Science-T2I-Web", "summary": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12982", "pdf": "https://arxiv.org/pdf/2504.12982", "abs": "https://arxiv.org/abs/2504.12982", "authors": ["Jiatai Wang", "Zhiwei Xu", "Di Jin", "Xuewen Yang", "Tao Li"], "title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13068", "pdf": "https://arxiv.org/pdf/2504.13068", "abs": "https://arxiv.org/abs/2504.13068", "authors": ["Sudesh Ramesh Bhagat", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "agreement", "kappa", "accuracy"], "score": 5}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13181", "pdf": "https://arxiv.org/pdf/2504.13181", "abs": "https://arxiv.org/abs/2504.13181", "authors": ["Daniel Bolya", "Po-Yao Huang", "Peize Sun", "Jang Hyun Cho", "Andrea Madotto", "Chen Wei", "Tengyu Ma", "Jiale Zhi", "Jathushan Rajasegaran", "Hanoona Rasheed", "Junke Wang", "Marco Monteiro", "Hu Xu", "Shiyu Dong", "Nikhila Ravi", "Daniel Li", "Piotr Dollr", "Christoph Feichtenhofer"], "title": "Perception Encoder: The best visual embeddings are not at the output of the network", "categories": ["cs.CV"], "comment": "Initial Submission", "summary": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12408", "pdf": "https://arxiv.org/pdf/2504.12408", "abs": "https://arxiv.org/abs/2504.12408", "authors": ["Negar Arabzadeh", "Charles L. A . Clarke"], "title": "A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "reliability", "kappa"], "score": 4}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12356", "pdf": "https://arxiv.org/pdf/2504.12356", "abs": "https://arxiv.org/abs/2504.12356", "authors": ["Sidun Liu", "Wenyu Li", "Peng Qiao", "Yong Dou"], "title": "Regist3R: Incremental Registration with Stereo Foundation Model", "categories": ["eess.IV", "cs.CV"], "comment": "19 pages", "summary": "Multi-view 3D reconstruction has remained an essential yet challenging\nproblem in the field of computer vision. While DUSt3R and its successors have\nachieved breakthroughs in 3D reconstruction from unposed images, these methods\nexhibit significant limitations when scaling to multi-view scenarios, including\nhigh computational cost and cumulative error induced by global alignment. To\naddress these challenges, we propose Regist3R, a novel stereo foundation model\ntailored for efficient and scalable incremental reconstruction. Regist3R\nleverages an incremental reconstruction paradigm, enabling large-scale 3D\nreconstructions from unordered and many-view image collections. We evaluate\nRegist3R on public datasets for camera pose estimation and 3D reconstruction.\nOur experiments demonstrate that Regist3R achieves comparable performance with\noptimization-based methods while significantly improving computational\nefficiency, and outperforms existing multi-view reconstruction models.\nFurthermore, to assess its performance in real-world applications, we introduce\na challenging oblique aerial dataset which has long spatial spans and hundreds\nof views. The results highlight the effectiveness of Regist3R. We also\ndemonstrate the first attempt to reconstruct large-scale scenes encompassing\nover thousands of views through pointmap-based foundation models, showcasing\nits potential for practical applications in large-scale 3D reconstruction\ntasks, including urban modeling, aerial mapping, and beyond.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12661", "pdf": "https://arxiv.org/pdf/2504.12661", "abs": "https://arxiv.org/abs/2504.12661", "authors": ["Menglan Chen", "Xianghe Pang", "Jingjing Dong", "WenHao Wang", "Yaxin Du", "Siheng Chen"], "title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12680", "pdf": "https://arxiv.org/pdf/2504.12680", "abs": "https://arxiv.org/abs/2504.12680", "authors": ["Baining Zhao", "Ziyou Wang", "Jianjie Fang", "Chen Gao", "Fanhang Man", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li", "Wenwu Zhu"], "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning", "categories": ["cs.AI", "cs.CV"], "comment": "12 pages, 5 figures", "summary": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12867", "pdf": "https://arxiv.org/pdf/2504.12867", "abs": "https://arxiv.org/abs/2504.12867", "authors": ["Guanrou Yang", "Chen Yang", "Qian Chen", "Ziyang Ma", "Wenxi Chen", "Wen Wang", "Tianrui Wang", "Yifan Yang", "Zhikang Niu", "Wenrui Liu", "Fan Yu", "Zhihao Du", "Zhifu Gao", "ShiLiang Zhang", "Xie Chen"], "title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "consistency", "reliability", "fine-grained"], "score": 5}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12879", "pdf": "https://arxiv.org/pdf/2504.12879", "abs": "https://arxiv.org/abs/2504.12879", "authors": ["Grigory Kovalev", "Mikhail Tikhomirov", "Evgeny Kozhevnikov", "Max Kornilov", "Natalia Loukachevitch"], "title": "Building Russian Benchmark for Evaluation of Information Retrieval Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.12800", "pdf": "https://arxiv.org/pdf/2504.12800", "abs": "https://arxiv.org/abs/2504.12800", "authors": ["Yifei Tong", "Runze Tian", "Xiao Han", "Dingyao Liu", "Fenggen Yu", "Yan Zhang"], "title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13038", "pdf": "https://arxiv.org/pdf/2504.13038", "abs": "https://arxiv.org/abs/2504.13038", "authors": ["Leo Leppnen", "Lili Aunimo", "Arto Hellas", "Jukka K. Nurminen", "Linda Mannila"], "title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses", "categories": ["cs.CY", "cs.CL", "K.3.1; I.2.7"], "comment": "10 pages, 4 figures", "summary": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13059", "pdf": "https://arxiv.org/pdf/2504.13059", "abs": "https://arxiv.org/abs/2504.13059", "authors": ["Yao Mu", "Tianxing Chen", "Zanxin Chen", "Shijia Peng", "Zhiqian Lan", "Zeyu Gao", "Zhixuan Liang", "Qiaojun Yu", "Yude Zou", "Mingkun Xu", "Lunkai Lin", "Zhiqiang Xie", "Mingyu Ding", "Ping Luo"], "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "CVPR 2025 Highlight. 22 pages. Project page:\n  https://robotwin-benchmark.github.io/", "summary": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13151", "pdf": "https://arxiv.org/pdf/2504.13151", "abs": "https://arxiv.org/abs/2504.13151", "authors": ["Aaron Mueller", "Atticus Geiger", "Sarah Wiegreffe", "Dana Arad", "Ivn Arcuschin", "Adam Belfki", "Yik Siu Chan", "Jaden Fiotto-Kaufman", "Tal Haklay", "Michael Hanna", "Jing Huang", "Rohan Gupta", "Yaniv Nikankin", "Hadas Orgad", "Nikhil Prakash", "Anja Reusch", "Aruna Sankaranarayanan", "Shun Shao", "Alessandro Stolfo", "Martin Tutek", "Amir Zur", "David Bau", "Yonatan Belinkov"], "title": "MIB: A Mechanistic Interpretability Benchmark", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-04-19.jsonl"}
{"id": "2504.13072", "pdf": "https://arxiv.org/pdf/2504.13072", "abs": "https://arxiv.org/abs/2504.13072", "authors": ["Wenqi Dong", "Bangbang Yang", "Zesong Yang", "Yuan Li", "Tao Hu", "Hujun Bao", "Yuewen Ma", "Zhaopeng Cui"], "title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Project webpage: https://zju3dv.github.io/hiscene/", "summary": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-19.jsonl"}
