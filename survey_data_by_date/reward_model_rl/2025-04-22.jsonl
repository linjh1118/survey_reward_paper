{"id": "2504.14439", "pdf": "https://arxiv.org/pdf/2504.14439", "abs": "https://arxiv.org/abs/2504.14439", "authors": ["Avinandan Bose", "Zhihan Xiong", "Yuejie Chi", "Simon Shaolei Du", "Lin Xiao", "Maryam Fazel"], "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "preference", "alignment"], "score": 7}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14177", "pdf": "https://arxiv.org/pdf/2504.14177", "abs": "https://arxiv.org/abs/2504.14177", "authors": ["Li He", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback", "preference", "AI feedback", "alignment"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["agreement", "consistency", "fine-grained"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870", "abs": "https://arxiv.org/abs/2504.14870", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "title": "OTC: Optimal Tool Calls via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "proximal policy optimization", "reinforcement learning", "policy optimization", "preference"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15176", "pdf": "https://arxiv.org/pdf/2504.15176", "abs": "https://arxiv.org/abs/2504.15176", "authors": ["Miaomiao Cai", "Simiao Li", "Wei Li", "Xudong Huang", "Hanting Chen", "Jie Hu", "Yunhe Wang"], "title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have improved Real-World Image\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\nintegration, risking misalignment with human preference and may leading to\nartifacts, hallucinations and harmful content generation. To this end, we are\nthe first to introduce human preference alignment into Real-ISR, a technique\nthat has been successfully applied in Large Language Models and Text-to-Image\ntasks to effectively enhance the alignment of generated outputs with human\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\ntechnique that directly learns from the human preference dataset. Nevertheless,\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\nare difficult to reconcile with the image-level preferences of DPO, which can\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\nPreference Optimization (DSPO) to align instance-level human preferences by\nincorporating semantic guidance, which is through two strategies: (a) semantic\ninstance alignment strategy, implementing instance-level alignment to ensure\nfine-grained perceptual consistency, and (b) user description feedback\nstrategy, mitigating hallucinations through semantic textual feedback on\ninstance-level images. As a plug-and-play solution, DSPO proves highly\neffective in both one-step and multi-step SR frameworks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference", "alignment", "DPO", "direct preference optimization"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "human preference", "consistency", "fine-grained"], "score": 5}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14290", "pdf": "https://arxiv.org/pdf/2504.14290", "abs": "https://arxiv.org/abs/2504.14290", "authors": ["Shouwei Ruan", "Zhenyu Wu", "Yao Huang", "Ruochen Zhang", "Yitong Sun", "Caixin Kang", "Xingxing Wei"], "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Ensuring the safety of generated content remains a fundamental challenge for\nText-to-Image (T2I) generation. Existing studies either fail to guarantee\ncomplete safety under potentially harmful concepts or struggle to balance\nsafety with generation quality. To address these issues, we propose\nSafety-Constrained Direct Preference Optimization (SC-DPO), a novel framework\nfor safety alignment in T2I models. SC-DPO integrates safety constraints into\nthe general human preference calibration, aiming to maximize the likelihood of\ngenerating human-preferred samples while minimizing the safety cost of the\ngenerated outputs. In SC-DPO, we introduce a safety cost model to accurately\nquantify harmful levels for images, and train it effectively using the proposed\ncontrastive learning and cost anchoring objectives. To apply SC-DPO for\neffective T2I safety alignment, we constructed SCP-10K, a safety-constrained\npreference dataset containing rich harmful concepts, which blends\nsafety-constrained preference pairs under both harmful and clean instructions,\nfurther mitigating the trade-off between safety and sample quality.\nAdditionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,\npromoting the model's learning of difficult preference pair samples. Extensive\nexperiments demonstrate that SC-DPO outperforms existing methods, effectively\ndefending against various NSFW content while maintaining optimal sample quality\nand human preference alignment. Additionally, SC-DPO exhibits resilience\nagainst adversarial prompts designed to generate harmful content.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO", "direct preference optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "human preference", "safety"], "score": 4}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14096", "pdf": "https://arxiv.org/pdf/2504.14096", "abs": "https://arxiv.org/abs/2504.14096", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Video-language models (Video-LLMs) excel at understanding video content but\nstruggle with spatial relationships, temporal ordering, and cross-frame\ncontinuity. To address these limitations, we introduce VideoPASTA (Preference\nAlignment with Spatio-Temporal-Cross Frame Adversaries), a framework that\nenhances Video-LLMs through targeted preference optimization. VideoPASTA trains\nmodels to distinguish accurate video representations from carefully generated\nadversarial examples that deliberately violate spatial, temporal, or\ncross-frame relations. By applying Direct Preference Optimization to just 7,020\npreference pairs, VideoPASTA learns robust representations that capture\nfine-grained spatial relationships and long-range temporal dynamics.\nExperiments on standard video benchmarks show significant relative performance\ngains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over\nthe baseline Qwen2.5-VL model. These results demonstrate that targeted\nalignment, rather than massive pretraining or architectural modifications,\neffectively addresses core video-language challenges. Notably, VideoPASTA\nachieves these improvements without human annotation or captioning, relying on\njust 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior\nwork. This efficiency makes our approach a scalable, plug-and-play solution\nthat seamlessly integrates with existing models while preserving their\ncapabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14363", "pdf": "https://arxiv.org/pdf/2504.14363", "abs": "https://arxiv.org/abs/2504.14363", "authors": ["Shihan Dou", "Muling Wu", "Jingwen Xu", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay", "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 3 figures", "summary": "Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "policy gradient", "reinforcement learning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue", "code generation", "mathematical reasoning"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14520", "pdf": "https://arxiv.org/pdf/2504.14520", "abs": "https://arxiv.org/abs/2504.14520", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence", "summary": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback", "reinforcement learning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14287", "pdf": "https://arxiv.org/pdf/2504.14287", "abs": "https://arxiv.org/abs/2504.14287", "authors": ["Demetris Paschalides", "George Pallis", "Marios D. Dikaiakos"], "title": "Probing the Subtle Ideological Manipulation of Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14534", "pdf": "https://arxiv.org/pdf/2504.14534", "abs": "https://arxiv.org/abs/2504.14534", "authors": ["Liang Peng", "Boxi Wu", "Haoran Cheng", "Yibo Zhao", "Xiaofei He"], "title": "SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Previous text-to-image diffusion models typically employ supervised\nfine-tuning (SFT) to enhance pre-trained base models. However, this approach\nprimarily minimizes the loss of mean squared error (MSE) at the pixel level,\nneglecting the need for global optimization at the image level, which is\ncrucial for achieving high perceptual quality and structural coherence. In this\npaper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a\nnovel paradigm that optimizes both fine-grained details at the pixel level and\nglobal image quality. By integrating direct preference optimization into the\nmodel, SUDO generates preference image pairs in a self-supervised manner,\nenabling the model to prioritize global-level learning while complementing the\npixel-level MSE loss. As an effective alternative to supervised fine-tuning,\nSUDO can be seamlessly applied to any text-to-image diffusion model.\nImportantly, it eliminates the need for costly data collection and annotation\nefforts typically associated with traditional direct preference optimization\nmethods. Through extensive experiments on widely-used models, including Stable\nDiffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both\nglobal and local image quality. The codes are provided at\n\\href{https://github.com/SPengLiang/SUDO}{this link}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14642", "pdf": "https://arxiv.org/pdf/2504.14642", "abs": "https://arxiv.org/abs/2504.14642", "authors": ["Lin Li", "Wei Chen", "Jiahui Li", "Long Chen"], "title": "Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension", "categories": ["cs.CV"], "comment": "Ongoing project", "summary": "Recent advances in multi-modal large language models (MLLMs) have\nsignificantly improved object-level grounding and region captioning, but remain\nlimited in visual relation understanding (\\eg, scene graph generation),\nparticularly in modeling \\textit{N}-ary relationships that identify multiple\nsemantic roles among an action event. Such a lack of \\textit{semantic\ndependencies} modeling among multi-entities leads to unreliable outputs,\nintensifying MLLMs' hallucinations and over-reliance on language priors. To\nthis end, we propose Relation-R1, the first unified relational comprehension\nframework that explicitly integrates cognitive chain-of-thought (CoT)-guided\nSupervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)\nwithin a reinforcement learning (RL) paradigm. Specifically, we first establish\nfoundational reasoning capabilities via SFT, enforcing structured outputs with\nthinking processes. Then, GRPO is utilized to refine these outputs via\nmulti-reward optimization, prioritizing visual-semantic grounding over\nlanguage-induced biases, thereby improving generalization capability. Extensive\nexperiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1\nachieves state-of-the-art performance in both binary and \\textit{N}-ary\nrelation understanding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14848", "pdf": "https://arxiv.org/pdf/2504.14848", "abs": "https://arxiv.org/abs/2504.14848", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Ruibo Hou", "Jiaming Guo", "Zihao Zhang", "Yifan Hao", "Yunji Chen"], "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Yufeng Yuan", "Yu Yue", "Mingxuan Wang", "Xiaochen Zuo", "Jiaze Chen", "Lin Yan", "Wenyuan Xu", "Chi Zhang", "Xin Liu", "Chengyi Wang", "TianTian Fan", "Lingjun Liu", "Qiying Yu", "Xiangpeng Wei", "Zhiqi Lin", "Ruofei Zhu", "Qingping Yang", "Chengzhi Wei", "Jerry He", "Guanlin Liu", "Zheng Wu", "Xiangyu Yu", "Zhicheng Liu", "Jingjing Xu", "Jiangjie Chen", "Haojie Pan", "Shengding Hu", "Zhengyin Du", "Wenqi Wang", "Zewei Sun", "Chenwei Lou", "Bole Ma", "Zihan Wang", "Mofan Zhang", "Wang Zhang", "Gaohong Liu", "Kaihua Jiang", "Haibin Lin", "Ru Zhang", "Juncai Liu", "Li Han", "Jinxin Chi", "Wenqiang Zhang", "Jiayi Xu", "Jun Yuan", "Zhen Xiao", "Yuqiao Xian", "Jingqiao Wu", "Kai Hua", "Na Zhou", "Jianhui Duan", "Heyang Lu", "Changbao Wang", "Jinxiang Ou", "Shihang Wang", "Xiaoran Jin", "Xuesong Yao", "Chengyin Xu", "Wenchang Ma", "Zhecheng An", "Renming Pang", "Xia Xiao", "Jing Su", "Yuyu Zhang", "Tao Sun", "Kaibo Liu", "Yifan Sun", "Kai Shen", "Sijun Zhang", "Yiyuan Ma", "Xingyan Bin", "Ji Li", "Yao Luo", "Deyi Liu", "Shiyi Zhan", "Yunshui Li", "Yuan Yang", "Defa Zhu", "Ke Shen", "Chenggang Li", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14113", "pdf": "https://arxiv.org/pdf/2504.14113", "abs": "https://arxiv.org/abs/2504.14113", "authors": ["Jiyong Kwag", "Alper Yilmaz", "Charles Toth"], "title": "Lightweight Road Environment Segmentation using Vector Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Road environment segmentation plays a significant role in autonomous driving.\nNumerous works based on Fully Convolutional Networks (FCNs) and Transformer\narchitectures have been proposed to leverage local and global contextual\nlearning for efficient and accurate semantic segmentation. In both\narchitectures, the encoder often relies heavily on extracting continuous\nrepresentations from the image, which limits the ability to represent\nmeaningful discrete information. To address this limitation, we propose\nsegmentation of the autonomous driving environment using vector quantization.\nVector quantization offers three primary advantages for road environment\nsegmentation. (1) Each continuous feature from the encoder is mapped to a\ndiscrete vector from the codebook, helping the model discover distinct features\nmore easily than with complex continuous features. (2) Since a discrete feature\nacts as compressed versions of the encoder's continuous features, they also\ncompress noise or outliers, enhancing the image segmentation task. (3) Vector\nquantization encourages the latent space to form coarse clusters of continuous\nfeatures, forcing the model to group similar features, making the learned\nrepresentations more structured for the decoding process. In this work, we\ncombined vector quantization with the lightweight image segmentation model\nMobileUNETR and used it as a baseline model for comparison to demonstrate its\nefficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,\noutperforming the baseline by 2.9 % without increasing the model's initial size\nor complexity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14129", "pdf": "https://arxiv.org/pdf/2504.14129", "abs": "https://arxiv.org/abs/2504.14129", "authors": ["Yaning Zhang", "Jiahe Zhang", "Chunjie Ma", "Weili Guan", "Tian Gan", "Zan Gao"], "title": "BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution", "categories": ["cs.CV"], "comment": null, "summary": "The challenge of tracing the source attribution of forged faces has gained\nsignificant attention due to the rapid advancement of generative models.\nHowever, existing deepfake attribution (DFA) works primarily focus on the\ninteraction among various domains in vision modality, and other modalities such\nas texts and face parsing are not fully explored. Besides, they tend to fail to\nassess the generalization performance of deepfake attributors to unseen\ngenerators in a fine-grained manner. In this paper, we propose a novel bi-modal\nguided multi-perspective representation learning (BMRL) framework for zero-shot\ndeepfake attribution (ZS-DFA), which facilitates effective traceability to\nunseen generators. Specifically, we design a multi-perspective visual encoder\n(MPVE) to explore general deepfake attribution visual characteristics across\nthree views (i.e., image, noise, and edge). We devise a novel parsing encoder\nto focus on global face attribute embeddings, enabling parsing-guided DFA\nrepresentation learning via vision-parsing matching. A language encoder is\nproposed to capture fine-grained language embeddings, facilitating\nlanguage-guided general visual forgery representation learning through\nvision-language alignment. Additionally, we present a novel deepfake\nattribution contrastive center (DFACC) loss, to pull relevant generators closer\nand push irrelevant ones away, which can be introduced into DFA models to\nenhance traceability. Experimental results demonstrate that our method\noutperforms the state-of-the-art on the ZS-DFA task through various protocols\nevaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14202", "pdf": "https://arxiv.org/pdf/2504.14202", "abs": "https://arxiv.org/abs/2504.14202", "authors": ["Zichuan Liu", "Liming Jiang", "Qing Yan", "Yumin Jia", "Hao Kang", "Xin Lu"], "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14367", "pdf": "https://arxiv.org/pdf/2504.14367", "abs": "https://arxiv.org/abs/2504.14367", "authors": ["Gabriel Machado Santos", "Rita Maria da Silva Julia", "Marcelo Zanchetta do Nascimento"], "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages Accepted for publication in IEEE CEC 2025", "summary": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452", "abs": "https://arxiv.org/abs/2504.14452", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14468", "pdf": "https://arxiv.org/pdf/2504.14468", "abs": "https://arxiv.org/abs/2504.14468", "authors": ["Yijun Liu"], "title": "sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment", "categories": ["cs.CL", "cs.LG", "eess.SP", "q-bio.NC"], "comment": "Accepted for poster presentation at the CVPR 2025 Workshop on\n  Multimodal Foundation Models (MMFM3)", "summary": "Interpreting neural activity through meaningful latent representations\nremains a complex and evolving challenge at the intersection of neuroscience\nand artificial intelligence. We investigate the potential of multimodal\nfoundation models to align invasive brain recordings with natural language. We\npresent SSENSE, a contrastive learning framework that projects single-subject\nstereo-electroencephalography (sEEG) signals into the sentence embedding space\nof a frozen CLIP model, enabling sentence-level retrieval directly from brain\nactivity. SSENSE trains a neural encoder on spectral representations of sEEG\nusing InfoNCE loss, without fine-tuning the text encoder. We evaluate our\nmethod on time-aligned sEEG and spoken transcripts from a naturalistic\nmovie-watching dataset. Despite limited data, SSENSE achieves promising\nresults, demonstrating that general-purpose language representations can serve\nas effective priors for neural decoding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14280", "pdf": "https://arxiv.org/pdf/2504.14280", "abs": "https://arxiv.org/abs/2504.14280", "authors": ["Jindong Li", "Yongguang Li", "Yali Fu", "Jiahong Liu", "Yixin Liu", "Menglin Yang", "Irwin King"], "title": "CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As machine learning evolves, domain generalization (DG) and domain adaptation\n(DA) have become crucial for enhancing model robustness across diverse\nenvironments. Contrastive Language-Image Pretraining (CLIP) plays a significant\nrole in these tasks, offering powerful zero-shot capabilities that allow models\nto perform effectively in unseen domains. However, there remains a significant\ngap in the literature, as no comprehensive survey currently exists that\nsystematically explores the applications of CLIP in DG and DA, highlighting the\nnecessity for this review. This survey presents a comprehensive review of\nCLIP's applications in DG and DA. In DG, we categorize methods into optimizing\nprompt learning for task alignment and leveraging CLIP as a backbone for\neffective feature extraction, both enhancing model adaptability. For DA, we\nexamine both source-available methods utilizing labeled source data and\nsource-free approaches primarily based on target domain data, emphasizing\nknowledge transfer mechanisms and strategies for improved performance across\ndiverse contexts. Key challenges, including overfitting, domain diversity, and\ncomputational efficiency, are addressed, alongside future research\nopportunities to advance robustness and efficiency in practical applications.\nBy synthesizing existing literature and pinpointing critical gaps, this survey\nprovides valuable insights for researchers and practitioners, proposing\ndirections for effectively leveraging CLIP to enhance methodologies in domain\ngeneralization and adaptation. Ultimately, this work aims to foster innovation\nand collaboration in the quest for more resilient machine learning models that\ncan perform reliably across diverse real-world scenarios. A more up-to-date\nversion of the papers is maintained at:\nhttps://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14294", "pdf": "https://arxiv.org/pdf/2504.14294", "abs": "https://arxiv.org/abs/2504.14294", "authors": ["Pourya Shamsolmoali", "Masoumeh Zareapoor", "Huiyu Zhou", "Michael Felsberg", "Dacheng Tao", "Xuelong Li"], "title": "From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion", "categories": ["cs.CV"], "comment": "Accepted in TPAMI", "summary": "Image completion is a challenging task, particularly when ensuring that\ngenerated content seamlessly integrates with existing parts of an image. While\nrecent diffusion models have shown promise, they often struggle with\nmaintaining coherence between known and unknown (missing) regions. This issue\narises from the lack of explicit spatial and semantic alignment during the\ndiffusion process, resulting in content that does not smoothly integrate with\nthe original image. Additionally, diffusion models typically rely on global\nlearned distributions rather than localized features, leading to\ninconsistencies between the generated and existing image parts. In this work,\nwe propose ConFill, a novel framework that introduces a Context-Adaptive\nDiscrepancy (CAD) model to ensure that intermediate distributions of known and\nunknown regions are closely aligned throughout the diffusion process. By\nincorporating CAD, our model progressively reduces discrepancies between\ngenerated and original images at each diffusion step, leading to contextually\naligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism\nthat adaptively increases the sampling rate in regions with high reconstruction\ncomplexity. This approach enables precise adjustments, enhancing detail and\nintegration in restored areas. Extensive experiments demonstrate that ConFill\noutperforms current methods, setting a new benchmark in image completion.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669", "abs": "https://arxiv.org/abs/2504.14669", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14738", "pdf": "https://arxiv.org/pdf/2504.14738", "abs": "https://arxiv.org/abs/2504.14738", "authors": ["Reya Vir", "Shreya Shankar", "Harrison Chase", "Will Fu-Hinthorn", "Aditya Parameswaran"], "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "reliability", "criteria"], "score": 4}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14348", "pdf": "https://arxiv.org/pdf/2504.14348", "abs": "https://arxiv.org/abs/2504.14348", "authors": ["Le Wang", "Zonghao Ying", "Tianyuan Zhang", "Siyuan Liang", "Shengshan Hu", "Mingchuan Zhang", "Aishan Liu", "Xianglong Liu"], "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection", "categories": ["cs.CV"], "comment": "17 pages, 4 figures", "summary": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14772", "pdf": "https://arxiv.org/pdf/2504.14772", "abs": "https://arxiv.org/abs/2504.14772", "authors": ["Luyang Fang", "Xiaowei Yu", "Jiazhang Cai", "Yongkai Chen", "Shushan Wu", "Zhengliang Liu", "Zhenyuan Yang", "Haoran Lu", "Xilin Gong", "Yufang Liu", "Terry Ma", "Wei Ruan", "Ali Abbasi", "Jing Zhang", "Tao Wang", "Ehsan Latif", "Wei Liu", "Wei Zhang", "Soheil Kolouri", "Xiaoming Zhai", "Dajiang Zhu", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14804", "pdf": "https://arxiv.org/pdf/2504.14804", "abs": "https://arxiv.org/abs/2504.14804", "authors": ["Jiaxin GUO", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Zongyao Li", "Hengchao Shang", "Daimeng Wei", "Hao Yang"], "title": "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14856", "pdf": "https://arxiv.org/pdf/2504.14856", "abs": "https://arxiv.org/abs/2504.14856", "authors": ["Jiajun Shen", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation", "categories": ["cs.CL"], "comment": "19 pages, 14 figures", "summary": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "helpfulness"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14429", "pdf": "https://arxiv.org/pdf/2504.14429", "abs": "https://arxiv.org/abs/2504.14429", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "reliability", "accuracy"], "score": 5}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15013", "pdf": "https://arxiv.org/pdf/2504.15013", "abs": "https://arxiv.org/abs/2504.15013", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "An-Zi Yen"], "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs", "categories": ["cs.CL"], "comment": "Accepted by iRAISE@AAAI2025", "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15047", "pdf": "https://arxiv.org/pdf/2504.15047", "abs": "https://arxiv.org/abs/2504.15047", "authors": ["Quy-Anh Dang", "Chris Ngo", "Truong-Son Hy"], "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241", "abs": "https://arxiv.org/abs/2504.15241", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14553", "pdf": "https://arxiv.org/pdf/2504.14553", "abs": "https://arxiv.org/abs/2504.14553", "authors": ["Weijun Zhuang", "Qizhang Li", "Xin Li", "Ming Liu", "Xiaopeng Hong", "Feng Gao", "Fan Yang", "Wangmeng Zuo"], "title": "Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Detection and Moment Retrieval constitute two pivotal tasks\nin video understanding, focusing on precisely localizing temporal segments\ncorresponding to specific actions or events. Recent advancements introduced\nMoment Detection to unify these two tasks, yet existing approaches remain\nconfined to closed-set scenarios, limiting their applicability in open-world\ncontexts. To bridge this gap, we present Grounding-MD, an innovative, grounded\nvideo-language pre-training framework tailored for open-world moment detection.\nOur framework incorporates an arbitrary number of open-ended natural language\nqueries through a structured prompt mechanism, enabling flexible and scalable\nmoment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a\nText-Guided Fusion Decoder to facilitate comprehensive video-text alignment and\nenable effective cross-task collaboration. Through large-scale pre-training on\ntemporal action detection and moment retrieval datasets, Grounding-MD\ndemonstrates exceptional semantic representation learning capabilities,\neffectively handling diverse and complex query conditions. Comprehensive\nevaluations across four benchmark datasets including ActivityNet, THUMOS14,\nActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD\nestablishes new state-of-the-art performance in zero-shot and supervised\nsettings in open-world moment detection scenarios. All source code and trained\nmodels will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14600", "pdf": "https://arxiv.org/pdf/2504.14600", "abs": "https://arxiv.org/abs/2504.14600", "authors": ["Zheng Chen", "Jingkai Wang", "Kai Liu", "Jue Gong", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Jianxing Zhang", "Jinlong Wu", "Jun Wang", "Zheng Xie", "Hakjae Jeon", "Suejin Han", "Hyung-Ju Chun", "Hyunhee Park", "Zhicun Yin", "Junjie Chen", "Ming Liu", "Xiaoming Li", "Chao Zhou", "Wangmeng Zuo", "Weixia Zhang", "Dingquan Li", "Kede Ma", "Yun Zhang", "Zhuofan Zheng", "Yuyue Liu", "Shizhen Tang", "Zihao Zhang", "Yi Ning", "Hao Jiang", "Wenjie An", "Kangmeng Yu", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Junjun Jiang", "Yingfu Zhang", "Gang He", "Siqi Wang", "Kepeng Xu", "Zhenyang Liu", "Changxin Zhou", "Shanlan Shen", "Yubo Duan", "Yiang Chen", "Jin Guo", "Mengru Yang", "Jen-Wei Lee", "Chia-Ming Lee", "Chih-Chung Hsu", "Hu Peng", "Chunming He"], "title": "NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_RealWorld_Face_Restoration", "summary": "This paper provides a review of the NTIRE 2025 challenge on real-world face\nrestoration, highlighting the proposed solutions and the resulting outcomes.\nThe challenge focuses on generating natural, realistic outputs while\nmaintaining identity consistency. Its goal is to advance state-of-the-art\nsolutions for perceptual quality and realism, without imposing constraints on\ncomputational resources or training data. The track of the challenge evaluates\nperformance using a weighted image quality assessment (IQA) score and employs\nthe AdaFace model as an identity checker. The competition attracted 141\nregistrants, with 13 teams submitting valid models, and ultimately, 10 teams\nachieved a valid score in the final ranking. This collaborative effort advances\nthe performance of real-world face restoration while offering an in-depth\noverview of the latest trends in the field.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887", "abs": "https://arxiv.org/abs/2504.13887", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite the growing integration of AI chatbots as conversational agents in\npublic discourse, empirical evidence regarding their capacity to foster\nintercultural empathy remains limited. Using a randomized dialogue experiment,\nwe examined how different types of AI chatbot interaction, i.e., deliberative\nversus non-deliberative and culturally aligned versus non-aligned, affect\nintercultural empathy across cultural groups. Results show that deliberative\nconversations increased intercultural empathy among American participants but\nnot Latin American participants, who perceived AI responses as culturally\ninaccurate and failing to represent their cultural contexts and perspectives\nauthentically. Real-time interaction analyses reveal that these differences\nstem from cultural knowledge gaps inherent in Large Language Models. Despite\nexplicit prompting and instruction to represent cultural perspectives in\nparticipants' native languages, AI systems still exhibit significant\ndisparities in cultural representation. This highlights the importance of\ndesigning AI systems capable of culturally authentic engagement in deliberative\nconversations. Our study contributes to deliberation theory and AI alignment\nresearch by underscoring AI's role in intercultural dialogue and the persistent\nchallenge of representational asymmetry in democratic discourse.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14658", "pdf": "https://arxiv.org/pdf/2504.14658", "abs": "https://arxiv.org/abs/2504.14658", "authors": ["Jing Zhang", "Dan Guo", "Zhangbin Li", "Meng Wang"], "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art", "categories": ["cs.CV"], "comment": null, "summary": "This paper focuses on a key challenge in visual art understanding: given an\nart image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for the emotional arousal.\nDespite recent advances in art understanding, pixel-level emotion understanding\nstill faces a dual challenge: first, the subjectivity of emotion makes it\ndifficult for general segmentation models like SAM to adapt to emotion-oriented\nsegmentation tasks; and second, the abstract nature of art expression makes it\ndifficult for captioning models to balance pixel-level semantic understanding\nand emotion reasoning. To solve the above problems, this paper proposes the\nEmotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the\nsegmentation model SAM with emotion comprehension capability. First, to enable\nthe model to perform segmentation under the guidance of emotional intent well,\nwe introduce an emotional prompt with a learnable mask token as the conditional\ninput for segmentation decoding. Then, we design an emotion projector to\nestablish the association between emotion and visual features. Next, more\nimportantly, to address emotion-visual stimuli alignment, we develop a\nlightweight prefix projector, a module that fuses the learned emotional mask\nwith the corresponding emotion into a unified representation compatible with\nthe language model.Finally, we input the joint visual, mask, and emotional\ntokens into the language model and output the emotional explanations. It\nensures that the generated interpretations remain semantically and emotionally\ncoherent with the visual stimuli. The method innovatively realizes end-to-end\nmodeling from low-level pixel features to high-level emotion interpretation,\nproviding the first interpretable fine-grained analysis framework for artistic\nemotion computing. Extensive experiments validate the effectiveness of our\nmodel.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13958", "pdf": "https://arxiv.org/pdf/2504.13958", "abs": "https://arxiv.org/abs/2504.13958", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Qi He", "Hongru Wang", "Xiusi Chen", "Dilek Hakkani-Tür", "Gokhan Tur", "Heng Ji"], "title": "ToolRL: Reward is All Tool Learning Needs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 Pages, 12 Figures, 12 Tables", "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14708", "pdf": "https://arxiv.org/pdf/2504.14708", "abs": "https://arxiv.org/abs/2504.14708", "authors": ["Parshuram N. Aarotale", "Ajita Rattani"], "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14709", "pdf": "https://arxiv.org/pdf/2504.14709", "abs": "https://arxiv.org/abs/2504.14709", "authors": ["Hui Zhou", "Shaoshuai Shi", "Hongsheng Li"], "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147", "abs": "https://arxiv.org/abs/2504.14147", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232", "abs": "https://arxiv.org/abs/2504.14232", "authors": ["Antoun Yaacoub", "Jérôme Da-Rugna", "Zainab Assaghir"], "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14239", "pdf": "https://arxiv.org/pdf/2504.14239", "abs": "https://arxiv.org/abs/2504.14239", "authors": ["Yuhang Liu", "Pengxiang Li", "Congkai Xie", "Xavier Hu", "Xiaotian Han", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages, 3 figures, work in progress", "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["criteria"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14884", "pdf": "https://arxiv.org/pdf/2504.14884", "abs": "https://arxiv.org/abs/2504.14884", "authors": ["Jingyu Xing", "Chenwei Tang", "Tao Wang", "Rong Xiao", "Wei Ju", "Ji-Zhe Zhou", "Liangli Zhen", "Jiancheng Lv"], "title": "Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in unsupervised anomaly detection (UAD) have shifted from\nsingle-class to multi-class scenarios. In such complex contexts, the increasing\npattern diversity has brought two challenges to reconstruction-based\napproaches: (1) over-generalization: anomalies that are subtle or share\ncompositional similarities with normal patterns may be reconstructed with high\nfidelity, making them difficult to distinguish from normal instances; and (2)\ninsufficient normality reconstruction: complex normal features, such as\nintricate textures or fine-grained structures, may not be faithfully\nreconstructed due to the model's limited representational capacity, resulting\nin false positives. Existing methods typically focus on addressing the former,\nwhich unintentionally exacerbate the latter, resulting in inadequate\nrepresentation of intricate normal patterns. To concurrently address these two\nchallenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This\nnetwork includes two critical components: a Dual-Decoder Reverse Distillation\nNetwork (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the\nDRD-Net incorporates a restoration decoder designed to recover normal features\nfrom synthetic abnormal inputs and an identity decoder to reconstruct features\nthat maintain the anomalous semantics. By exploiting the discrepancy between\nfeatures produced by two decoders, our approach refines anomaly scores beyond\nthe conventional encoder-decoder comparison paradigm, effectively reducing\nfalse positives and enhancing localization accuracy. Furthermore, the CMM\nexplicitly encodes and preserves class-specific normal prototypes, actively\nsteering the network away from anomaly reconstruction. Comprehensive\nexperimental results across several benchmarks demonstrate the superior\nperformance of our MDD-Net framework over current SoTA approaches in\nmulti-class UAD tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14919", "pdf": "https://arxiv.org/pdf/2504.14919", "abs": "https://arxiv.org/abs/2504.14919", "authors": ["Donghyeong Kim", "Chaewon Park", "Suhwan Cho", "Hyeonjeong Lim", "Minseok Kang", "Jungho Lee", "Sangyoun Lee"], "title": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14975", "pdf": "https://arxiv.org/pdf/2504.14975", "abs": "https://arxiv.org/abs/2504.14975", "authors": ["Hongbin Xu", "Chaohui Yu", "Feng Xiao", "Jiazheng Xing", "Hai Ci", "Weitao Chen", "Ming Li"], "title": "Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization", "categories": ["cs.CV"], "comment": "Preprint version. The code will be open after finishing the reviewing\n  process", "summary": "Despite the remarkable progress of 3D generation, achieving controllability,\ni.e., ensuring consistency between generated 3D content and input conditions\nlike edge and depth, remains a significant challenge. Existing methods often\nstruggle to maintain accurate alignment, leading to noticeable discrepancies.\nTo address this issue, we propose \\name{}, a new framework that enhances\ncontrollable 3D generation by explicitly encouraging cyclic consistency between\nthe second-order 3D content, generated based on extracted signals from the\nfirst-order generation, and its original input controls. Specifically, we\nemploy an efficient feed-forward backbone that can generate a 3D object from an\ninput condition and a text prompt. Given an initial viewpoint and a control\nsignal, a novel view is rendered from the generated 3D content, from which the\nextracted condition is used to regenerate the 3D content. This re-generated\noutput is then rendered back to the initial viewpoint, followed by another\nround of control signal extraction, forming a cyclic process with two\nconsistency constraints. \\emph{View consistency} ensures coherence between the\ntwo generated 3D objects, measured by semantic similarity to accommodate\ngenerative diversity. \\emph{Condition consistency} aligns the final extracted\nsignal with the original input control, preserving structural or geometric\ndetails throughout the process. Extensive experiments on popular benchmarks\ndemonstrate that \\name{} significantly improves controllability, especially for\nfine-grained details, outperforming existing methods across various conditions\n(e.g., +14.17\\% PSNR for edge, +6.26\\% PSNR for sketch).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14988", "pdf": "https://arxiv.org/pdf/2504.14988", "abs": "https://arxiv.org/abs/2504.14988", "authors": ["Hong-Tao Yu", "Xiu-Shen Wei", "Yuxin Peng", "Serge Belongie"], "title": "Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable multimodal perception capabilities, garnering significant attention.\nWhile numerous evaluation studies have emerged, assessing LVLMs both\nholistically and on specialized tasks, fine-grained image tasks-fundamental to\ncomputer vision-remain largely unexplored. To fill this gap, we introduce a\ncomprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49\nmillion questions and 3.32 million images. Our evaluation systematically\nexamines LVLMs from both human-oriented and machine-oriented perspectives,\nfocusing on their semantic recognition and fine-grained feature representation\ncapabilities. Through extensive experiments on eight representative LVLMs/VLMs,\nwe uncover key findings regarding the influence of training paradigms, modality\nalignment, perturbation susceptibility, and fine-grained category reasoning on\ntask performance. This work provides critical insights into the limitations of\ncurrent LVLMs and offers guidance for future data construction and model design\nin the development of more advanced LVLMs. Our code is open-source and\navailable at https://github.com/SEU-VIPGroup/FG-BMK.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15003", "pdf": "https://arxiv.org/pdf/2504.15003", "abs": "https://arxiv.org/abs/2504.15003", "authors": ["Xin Li", "Xijun Wang", "Bingchen Li", "Kun Yuan", "Yizhen Shao", "Suhang Yao", "Ming Sun", "Chao Zhou", "Radu Timofte", "Zhibo Chen"], "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study", "categories": ["cs.CV"], "comment": "KwaiSR dataset, a new dataset for image super-resolution, used for\n  CVPR NTIRE 2025 Challenge; CVPR 2025 workshop paper", "summary": "In this work, we build the first benchmark dataset for short-form UGC Image\nSuper-resolution in the wild, termed KwaiSR, intending to advance the research\non developing image super-resolution algorithms for short-form UGC platforms.\nThis dataset is collected from the Kwai Platform, which is composed of two\nparts, i.e., synthetic and wild parts. Among them, the synthetic dataset,\nincluding 1,900 image pairs, is produced by simulating the degradation\nfollowing the distribution of real-world low-quality short-form UGC images,\naiming to provide the ground truth for training and objective comparison in the\nvalidation/testing. The wild dataset contains low-quality images collected\ndirectly from the Kwai Platform, which are filtered using the quality\nassessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset\ncontains 1800 synthetic image pairs and 1900 wild images, which are divided\ninto training, validation, and testing parts with a ratio of 8:1:1. Based on\nthe KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form\nUGC Video quality assessment and enhancement, which attracts lots of\nresearchers to develop the algorithm for it. The results of this competition\nhave revealed that our KwaiSR dataset is pretty challenging for existing Image\nSR methods, which is expected to lead to a new direction in the image\nsuper-resolution field. The dataset can be found from\nhttps://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15135", "pdf": "https://arxiv.org/pdf/2504.15135", "abs": "https://arxiv.org/abs/2504.15135", "authors": ["Juyeon Kim", "Geon Lee", "Taeuk Kim", "Kijung Shin"], "title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "SIGIR 2025 (Short)", "summary": "Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15032", "pdf": "https://arxiv.org/pdf/2504.15032", "abs": "https://arxiv.org/abs/2504.15032", "authors": ["Weijie He", "Mushui Liu", "Yunlong Yu", "Zhao Wang", "Chao Wu"], "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "consistency"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC outperform\nstate-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average\nmAP/R@1 on two training orders.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15145", "pdf": "https://arxiv.org/pdf/2504.15145", "abs": "https://arxiv.org/abs/2504.15145", "authors": ["Huzheng Yang", "Katherine Xu", "Michael D. Grossberg", "Yutong Bai", "Jianbo Shi"], "title": "\"I Know It When I See It\": Mood Spaces for Connecting and Expressing Visual Concepts", "categories": ["cs.CV"], "comment": "Project page: https://huzeyann.github.io/mspace/", "summary": "Expressing complex concepts is easy when they can be labeled or quantified,\nbut many ideas are hard to define yet instantly recognizable. We propose a Mood\nBoard, where users convey abstract concepts with examples that hint at the\nintended direction of attribute changes. We compute an underlying Mood Space\nthat 1) factors out irrelevant features and 2) finds the connections between\nimages, thus bringing relevant concepts closer. We invent a fibration\ncomputation to compress/decompress pre-trained features into/from a compact\nspace, 50-100x smaller. The main innovation is learning to mimic the pairwise\naffinity relationship of the image tokens across exemplars. To focus on the\ncoarse-to-fine hierarchical structures in the Mood Space, we compute the top\neigenvector structure from the affinity matrix and define a loss in the\neigenvector space. The resulting Mood Space is locally linear and compact,\nallowing image-level operations, such as object averaging, visual analogy, and\npose transfer, to be performed as a simple vector operation in Mood Space. Our\nlearning is efficient in computation without any fine-tuning, needs only a few\n(2-20) exemplars, and takes less than a minute to learn.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15259", "pdf": "https://arxiv.org/pdf/2504.15259", "abs": "https://arxiv.org/abs/2504.15259", "authors": ["Yunxuan Cai", "Sitao Xiang", "Zongjian Li", "Haiwei Chen", "Yajie Zhao"], "title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15281", "pdf": "https://arxiv.org/pdf/2504.15281", "abs": "https://arxiv.org/abs/2504.15281", "authors": ["Cailin Zhuang", "Yaoqi Hu", "Xuanyang Zhang", "Wei Cheng", "Jiacheng Bao", "Shengqi Liu", "Yiying Yang", "Xianfang Zeng", "Gang Yu", "Ming Li"], "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians", "categories": ["cs.CV"], "comment": "16 pages; Project page: https://styleme3d.github.io/", "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.13975", "pdf": "https://arxiv.org/pdf/2504.13975", "abs": "https://arxiv.org/abs/2504.13975", "authors": ["Mehmet Yamaç", "Muhammad Numan Yousaf", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "13 pages", "summary": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.14554", "pdf": "https://arxiv.org/pdf/2504.14554", "abs": "https://arxiv.org/abs/2504.14554", "authors": ["Chongye Guo", "Jinhu Fu", "Junfeng Fang", "Kun Wang", "Guorui Feng"], "title": "REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": "10 pages, 7 figures", "summary": "The rapid advancement of generative AI highlights the importance of\ntext-to-image (T2I) security, particularly with the threat of backdoor\npoisoning. Timely disclosure and mitigation of security vulnerabilities in T2I\nmodels are crucial for ensuring the safe deployment of generative models. We\nexplore a novel training-free backdoor poisoning paradigm through model\nediting, which is recently employed for knowledge updating in large language\nmodels. Nevertheless, we reveal the potential security risks posed by model\nediting techniques to image generation models. In this work, we establish the\nprinciples for backdoor attacks based on model editing, and propose a\nrelationship-driven precise backdoor poisoning method, REDEditing. Drawing on\nthe principles of equivalent-attribute alignment and stealthy poisoning, we\ndevelop an equivalent relationship retrieval and joint-attribute transfer\napproach that ensures consistent backdoor image generation through concept\nrebinding. A knowledge isolation constraint is proposed to preserve benign\ngeneration integrity. Our method achieves an 11\\% higher attack success rate\ncompared to state-of-the-art approaches. Remarkably, adding just one line of\ncode enhances output naturalness while improving backdoor stealthiness by 24\\%.\nThis work aims to heighten awareness regarding this security vulnerability in\neditable image generation models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
{"id": "2504.15129", "pdf": "https://arxiv.org/pdf/2504.15129", "abs": "https://arxiv.org/abs/2504.15129", "authors": ["Kangyao Huang", "Hao Wang", "Yu Luo", "Jingyu Chen", "Jintao Chen", "Xiangkui Zhang", "Xiangyang Ji", "Huaping Liu"], "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-22.jsonl"}
