{"id": "2504.16656", "pdf": "https://arxiv.org/pdf/2504.16656", "abs": "https://arxiv.org/abs/2504.16656", "authors": ["Chris", "Yichen Wei", "Yi Peng", "Xiaokun Wang", "Weijie Qiu", "Wei Shen", "Tianyidan Xie", "Jiangbo Pei", "Jianhao Zhang", "Yunzhuo Hao", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We present Skywork R1V2, a next-generation multimodal reasoning model and a\nmajor leap forward from its predecessor, Skywork R1V. At its core, R1V2\nintroduces a hybrid reinforcement learning paradigm that harmonizes\nreward-model guidance with rule-based strategies, thereby addressing the\nlong-standing challenge of balancing sophisticated reasoning capabilities with\nbroad generalization. To further enhance training efficiency, we propose the\nSelective Sample Buffer (SSB) mechanism, which effectively counters the\n``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization\n(GRPO) by prioritizing high-value samples throughout the optimization process.\nNotably, we observe that excessive reinforcement signals can induce visual\nhallucinations--a phenomenon we systematically monitor and mitigate through\ncalibrated reward thresholds throughout the training process. Empirical results\naffirm the exceptional capability of R1V2, with benchmark-leading performances\nsuch as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and\n74.0 on MMMU. These results underscore R1V2's superiority over existing\nopen-source models and demonstrate significant progress in closing the\nperformance gap with premier proprietary systems, including Gemini 2.5 and\nOpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to\npromote openness and reproducibility\nhttps://huggingface.co/Skywork/Skywork-R1V2-38B.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16628", "pdf": "https://arxiv.org/pdf/2504.16628", "abs": "https://arxiv.org/abs/2504.16628", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data", "categories": ["cs.LG", "cs.CL"], "comment": "19 pages, 6 figure, Multiobjective Alignment of LLMs", "summary": "Aligning large language models with multiple human expectations and values is\ncrucial for ensuring that they adequately serve a variety of user needs. To\nthis end, offline multiobjective alignment algorithms such as the\nRewards-in-Context algorithm have shown strong performance and efficiency.\nHowever, inappropriate preference representations and training with imbalanced\nreward scores limit the performance of such algorithms. In this work, we\nintroduce ParetoHqD that addresses the above issues by representing human\npreferences as preference directions in the objective space and regarding data\nnear the Pareto front as ''high-quality'' data. For each preference, ParetoHqD\nfollows a two-stage supervised fine-tuning process, where each stage uses an\nindividual Pareto high-quality training set that best matches its preference\ndirection. The experimental results have demonstrated the superiority of\nParetoHqD over five baselines on two multiobjective alignment tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16103", "pdf": "https://arxiv.org/pdf/2504.16103", "abs": "https://arxiv.org/abs/2504.16103", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "Shape Your Ground: Refining Road Surfaces Beyond Planar Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Road surface reconstruction from aerial images is fundamental for autonomous\ndriving, urban planning, and virtual simulation, where smoothness, compactness,\nand accuracy are critical quality factors. Existing reconstruction methods\noften produce artifacts and inconsistencies that limit usability, while\ndownstream tasks have a tendency to represent roads as planes for simplicity\nbut at the cost of accuracy. We introduce FlexRoad, the first framework to\ndirectly address road surface smoothing by fitting Non-Uniform Rational\nB-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric\nreconstructions or geodata providers. Our method at its core utilizes the\nElevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust\nanomaly correction, significantly reducing surface roughness and fitting\nerrors. To facilitate quantitative comparison between road surface\nreconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse\ncollection of road surface and terrain profiles derived from openly accessible\ngeodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D\nDataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used\nroad surface representations across various metrics while being insensitive to\nvarious input sources, terrains, and noise types. By performing ablation\nstudies, we identify the key role of each component towards high-quality\nreconstruction performance, making FlexRoad a generic method for realistic road\nsurface modeling.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16128", "pdf": "https://arxiv.org/pdf/2504.16128", "abs": "https://arxiv.org/abs/2504.16128", "authors": ["Stanley Mugisha", "Rashid Kisitu", "Florence Tushabe"], "title": "Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "12 pages and 4 figures", "summary": "Integrating deep learning applications into agricultural IoT systems faces a\nserious challenge of balancing the high accuracy of Vision Transformers (ViTs)\nwith the efficiency demands of resource-constrained edge devices. Large\ntransformer models like the Swin Transformers excel in plant disease\nclassification by capturing global-local dependencies. However, their\ncomputational complexity (34.1 GFLOPs) limits applications and renders them\nimpractical for real-time on-device inference. Lightweight models such as\nMobileNetV3 and TinyML would be suitable for on-device inference but lack the\nrequired spatial reasoning for fine-grained disease detection. To bridge this\ngap, we propose a hybrid knowledge distillation framework that synergistically\ntransfers logit and attention knowledge from a Swin Transformer teacher to a\nMobileNetV3 student model. Our method includes the introduction of adaptive\nattention alignment to resolve cross-architecture mismatch (resolution,\nchannels) and a dual-loss function optimizing both class probabilities and\nspatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled\nMobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%\nreduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU\nand 86ms/image on smartphone CPUs). Key innovations include IoT-centric\nvalidation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching\nattention maps. Comparative experiments show significant improvements over\nstandalone CNNs and prior distillation methods, with a 3.5% accuracy gain over\nMobileNetV3 baselines. Significantly, this work advances real-time,\nenergy-efficient crop monitoring in precision agriculture and demonstrates how\nwe can attain ViT-level diagnostic precision on edge devices. Code and models\nwill be made available for replication after acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "fine-grained"], "score": 3}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16379", "pdf": "https://arxiv.org/pdf/2504.16379", "abs": "https://arxiv.org/abs/2504.16379", "authors": ["Yash Akhauri", "Anthony Fei", "Chi-Chih Chang", "Ahmed F. AbouElhamayed", "Yueying Li", "Mohamed S. Abdelfattah"], "title": "SplitReason: Learning To Offload Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning in large language models (LLMs) tends to produce substantially\nlonger token generation sequences than simpler language modeling tasks. This\nextended generation length reflects the multi-step, compositional nature of\nreasoning and is often correlated with higher solution accuracy. From an\nefficiency perspective, longer token generation exacerbates the inherently\nsequential and memory-bound decoding phase of LLMs. However, not all parts of\nthis expensive reasoning process are equally difficult to generate. We leverage\nthis observation by offloading only the most challenging parts of the reasoning\nprocess to a larger, more capable model, while performing most of the\ngeneration with a smaller, more efficient model; furthermore, we teach the\nsmaller model to identify these difficult segments and independently trigger\noffloading when needed. To enable this behavior, we annotate difficult segments\nacross 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)\ndataset. We then apply supervised fine-tuning (SFT) and reinforcement learning\nfine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to\noffload the most challenging parts of its own reasoning process to a larger\nmodel. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while\noffloading 1.35% and 5% of the generated tokens respectively. We open-source\nour SplitReason model, data, code and logs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16368", "pdf": "https://arxiv.org/pdf/2504.16368", "abs": "https://arxiv.org/abs/2504.16368", "authors": ["Linhua Kong", "Dongxia Chang", "Lian Liu", "Zisen Kong", "Pengyuan Li", "Yao Zhao"], "title": "Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D object detection algorithms based on radar and camera fusion\nhave shown excellent performance, setting the stage for their application in\nautonomous driving perception tasks. Existing methods have focused on dealing\nwith feature misalignment caused by the domain gap between radar and camera.\nHowever, existing methods either neglect inter-modal features interaction\nduring alignment or fail to effectively align features at the same spatial\nlocation across modalities. To alleviate the above problems, we propose a new\nalignment model called Radar Camera Alignment (RCAlign). Specifically, we\ndesign a Dual-Route Alignment (DRA) module based on contrastive learning to\nalign and fuse the features between radar and camera. Moreover, considering the\nsparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is\nproposed to improve the densification of radar BEV features with the knowledge\ndistillation loss. Experiments show RCAlign achieves a new state-of-the-art on\nthe public nuScenes benchmark in radar camera fusion for 3D Object Detection.\nFurthermore, the RCAlign achieves a significant performance gain (4.3\\% NDS and\n8.4\\% mAP) in real-time 3D detection compared to the latest state-of-the-art\nmethod (RCBEVDet).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16455", "pdf": "https://arxiv.org/pdf/2504.16455", "abs": "https://arxiv.org/abs/2504.16455", "authors": ["Shun Zou", "Yi Zou", "Juncheng Li", "Guangwei Gao", "Guojun Qi"], "title": "Cross Paradigm Representation and Alignment Transformer for Image Deraining", "categories": ["cs.CV"], "comment": "code: https://github.com/zs1314/CPRAformer", "summary": "Transformer-based networks have achieved strong performance in low-level\nvision tasks like image deraining by utilizing spatial or channel-wise\nself-attention. However, irregular rain patterns and complex geometric overlaps\nchallenge single-paradigm architectures, necessitating a unified framework to\nintegrate complementary global-local and spatial-channel representations. To\naddress this, we propose a novel Cross Paradigm Representation and Alignment\nTransformer (CPRAformer). Its core idea is the hierarchical representation and\nalignment, leveraging the strengths of both paradigms (spatial-channel and\nglobal-local) to aid image reconstruction. It bridges the gap within and\nbetween paradigms, aligning and coordinating them to enable deep interaction\nand fusion of features. Specifically, we use two types of self-attention in the\nTransformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial\npixel refinement self-attention (SPR-SA). SPC-SA enhances global channel\ndependencies through dynamic sparsity, while SPR-SA focuses on spatial rain\ndistribution and fine-grained texture recovery. To address the feature\nmisalignment and knowledge differences between them, we introduce the Adaptive\nAlignment Frequency Module (AAFM), which aligns and interacts with features in\na two-stage progressive manner, enabling adaptive guidance and complementarity.\nThis reduces the information gap within and between paradigms. Through this\nunified cross-paradigm dynamic interaction framework, we achieve the extraction\nof the most valuable interactive fusion information from the two paradigms.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on eight benchmark datasets and further validates CPRAformer's\nrobustness in other image restoration tasks and downstream applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16487", "pdf": "https://arxiv.org/pdf/2504.16487", "abs": "https://arxiv.org/abs/2504.16487", "authors": ["Yahao Lu", "Yuehui Li", "Xingyuan Guo", "Shuai Yuan", "Yukai Shi", "Liang Lin"], "title": "Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning", "categories": ["cs.CV"], "comment": "A benchmark associated with real-world scenes for the Infrared Small\n  Target Detection (ISTD) is presented", "summary": "Infrared small target detection (ISTD) is highly sensitive to sensor type,\nobservation conditions, and the intrinsic properties of the target. These\nfactors can introduce substantial variations in the distribution of acquired\ninfrared image data, a phenomenon known as domain shift. Such distribution\ndiscrepancies significantly hinder the generalization capability of ISTD models\nacross diverse scenarios. To tackle this challenge, this paper introduces an\nISTD framework enhanced by domain adaptation. To alleviate distribution shift\nbetween datasets and achieve cross-sample alignment, we introduce Cross-view\nChannel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion\nstrategy, which integrates target information with diverse background features,\nenhancing the model' s ability to extract critical data characteristics. To\nfurther mitigate the impact of noise on ISTD, we develop a Noise-guided\nRepresentation learning strategy. This approach enables the model to learn more\nnoise-resistant feature representations, to improve its generalization\ncapability across diverse noisy domains. Finally, we develop a dedicated\ninfrared small target dataset, RealScene-ISTD. Compared to state-of-the-art\nmethods, our approach demonstrates superior performance in terms of detection\nprobability (Pd), false alarm rate (Fa), and intersection over union (IoU). The\ncode is available at: https://github.com/luy0222/RealScene-ISTD.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16499", "pdf": "https://arxiv.org/pdf/2504.16499", "abs": "https://arxiv.org/abs/2504.16499", "authors": ["Daniil Sinitsyn", "Linus HÃ¤renstam-Nielsen", "Daniel Cremers"], "title": "PRaDA: Projective Radial Distortion Averaging", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. 8 pages + references", "summary": "We tackle the problem of automatic calibration of radially distorted cameras\nin challenging conditions. Accurately determining distortion parameters\ntypically requires either 1) solving the full Structure from Motion (SfM)\nproblem involving camera poses, 3D points, and the distortion parameters, which\nis only possible if many images with sufficient overlap are provided, or 2)\nrelying heavily on learning-based methods that are comparatively less accurate.\nIn this work, we demonstrate that distortion calibration can be decoupled from\n3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding\nmany of the associated complexities. This is achieved by working in Projective\nSpace, where the geometry is unique up to a homography, which encapsulates all\ncamera parameters except for distortion. Our proposed method, Projective Radial\nDistortion Averaging, averages multiple distortion estimates in a fully\nprojective framework without creating 3d points and full bundle adjustment. By\nrelying on pairwise projective relations, our methods support any\nfeature-matching approaches without constructing point tracks across multiple\nimages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16855", "pdf": "https://arxiv.org/pdf/2504.16855", "abs": "https://arxiv.org/abs/2504.16855", "authors": ["Zijing Shi", "Meng Fang", "Ling Chen"], "title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents", "categories": ["cs.CL"], "comment": null, "summary": "Text-based games provide valuable environments for language-based autonomous\nagents. However, planning-then-learning paradigms, such as those combining\nMonte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably\ntime-consuming due to extensive iterations. Additionally, these algorithms\nperform uncertainty-driven exploration but lack language understanding and\nreasoning abilities. In this paper, we introduce the Monte Carlo planning with\nDynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages\nthe language understanding and reasoning capabilities of Large Language Models\n(LLMs) alongside the exploratory advantages of tree search algorithms.\nSpecifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,\nenabling them to learn from past experiences and dynamically adjust action\nevaluations during planning. We conduct experiments on a series of text-based\ngames from the Jericho benchmark. Our results demonstrate that the MC-DML\nalgorithm significantly enhances performance across various games at the\ninitial planning phase, outperforming strong contemporary methods that require\nmultiple iterations. This demonstrates the effectiveness of our algorithm,\npaving the way for more efficient language-grounded planning in complex\nenvironments.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16591", "pdf": "https://arxiv.org/pdf/2504.16591", "abs": "https://arxiv.org/abs/2504.16591", "authors": ["Tristan Kenneweg", "Philip Kenneweg", "Barbara Hammer"], "title": "JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning", "categories": ["cs.CV"], "comment": "Published at ESANN 2025", "summary": "Joint-Embedding Predictive Architectures (JEPA) have recently become popular\nas promising architectures for self-supervised learning. Vision transformers\nhave been trained using JEPA to produce embeddings from images and videos,\nwhich have been shown to be highly suitable for downstream tasks like\nclassification and segmentation. In this paper, we show how to adapt the JEPA\narchitecture to reinforcement learning from images. We discuss model collapse,\nshow how to prevent it, and provide exemplary data on the classical Cart Pole\ntask.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16616", "pdf": "https://arxiv.org/pdf/2504.16616", "abs": "https://arxiv.org/abs/2504.16616", "authors": ["Haosheng Chen", "Lian Luo", "Mengjingcheng Mo", "Zhanjie Wu", "Guobao Xiao", "Ji Gan", "Jiaxu Leng", "Xinbo Gao"], "title": "EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras, with microsecond temporal resolution and high dynamic range\n(HDR) characteristics, emit high-speed event stream for perception tasks.\nDespite the recent advancement in GNN-based perception methods, they are prone\nto use straightforward pairwise connectivity mechanisms in the pure Euclidean\nspace where they struggle to capture long-range dependencies and fail to\neffectively characterize the inherent hierarchical structures of non-uniformly\ndistributed event stream. To this end, in this paper we propose a novel\napproach named EHGCN, which is a pioneer to perceive event stream in both\nEuclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an\nadaptive sampling strategy to dynamically regulate sampling rates, retaining\ndiscriminative events while attenuating chaotic noise. Then we present a Markov\nVector Field (MVF)-driven motion-aware hyperedge generation method based on\nmotion state transition probabilities, thereby eliminating cross-target\nspurious associations and providing critically topological priors while\ncapturing long-range dependencies between events. Finally, we propose a\nEuclidean-Hyperbolic GCN to fuse the information locally aggregated and\nglobally hierarchically modeled in Euclidean and hyperbolic spaces,\nrespectively, to achieve hybrid event perception. Experimental results on event\nperception tasks such as object detection and recognition validate the\neffectiveness of our approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16727", "pdf": "https://arxiv.org/pdf/2504.16727", "abs": "https://arxiv.org/abs/2504.16727", "authors": ["Zhiyuan Fan", "Yumeng Wang", "Sandeep Polisetty", "Yi R.", "Fung"], "title": "V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) excel in various vision-language tasks.\nYet, their robustness to visual variations in position, scale, orientation, and\ncontext that objects in natural scenes inevitably exhibit due to changes in\nviewpoint and environment remains largely underexplored. To bridge this gap, we\nintroduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating\nVisual Variation Robustness of LVLMs, which encompasses automated evaluation\ndataset generation and principled metrics for thorough robustness assessment.\nThrough extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability\nto visual variations, in which even advanced models that excel at complex\nvision-language tasks significantly underperform on simple tasks such as object\nrecognition. Interestingly, these models exhibit a distinct visual position\nbias that contradicts theories of effective receptive fields, and demonstrate a\nhuman-like visual acuity threshold. To identify the source of these\nvulnerabilities, we present a systematic framework for component-level\nanalysis, featuring a novel visualization approach for aligned visual features.\nResults show that these vulnerabilities stem from error accumulation in the\npipeline architecture and inadequate multimodal alignment. Complementary\nexperiments with synthetic data further demonstrate that these limitations are\nfundamentally architectural deficiencies, scoring the need for architectural\ninnovations in future LVLM designs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16801", "pdf": "https://arxiv.org/pdf/2504.16801", "abs": "https://arxiv.org/abs/2504.16801", "authors": ["Xiaoxing Hu", "Kaicheng Yang", "Jun Wang", "Haoran Xu", "Ziyong Feng", "Yupei Wang"], "title": "Decoupled Global-Local Alignment for Improving Compositional Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has achieved success on\nmultiple downstream tasks by aligning image and text modalities. However, the\nnature of global contrastive learning limits CLIP's ability to comprehend\ncompositional concepts, such as relations and attributes. Although recent\nstudies employ global hard negative samples to improve compositional\nunderstanding, these methods significantly compromise the model's inherent\ngeneral capabilities by forcibly distancing textual negative samples from\nimages in the embedding space. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework that improves compositional\nunderstanding while substantially mitigating losses in general capabilities. To\noptimize the retention of the model's inherent capabilities, we incorporate a\nself-distillation mechanism within the global alignment process, aligning the\nlearnable image-text encoder with a frozen teacher model derived from an\nexponential moving average. Under the constraint of self-distillation, it\neffectively mitigates the catastrophic forgetting of pretrained knowledge\nduring fine-tuning. To improve compositional understanding, we first leverage\nthe in-context learning capability of Large Language Models (LLMs) to construct\nabout 2M high-quality negative captions across five types. Subsequently, we\npropose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)\nloss to enhance vision-language compositionally. Extensive experimental results\ndemonstrate the effectiveness of the DeGLA framework. Compared to previous\nstate-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across\nthe VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average\nperformance improvement of 13.0% on zero-shot classification tasks across\neleven datasets. Our code will be released at\nhttps://github.com/xiaoxing2001/DeGLA", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16870", "pdf": "https://arxiv.org/pdf/2504.16870", "abs": "https://arxiv.org/abs/2504.16870", "authors": ["Chenxi Duan"], "title": "High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data", "categories": ["cs.CV"], "comment": null, "summary": "Addressing gaps caused by cloud cover and the long revisit cycle of\nsatellites is vital for providing essential data to support remote sensing\napplications. This paper tackles the challenges of missing optical data\nsynthesis, particularly in complex scenarios with cloud cover. We propose\nCRSynthNet, a novel image synthesis network that incorporates innovative\ndesigned modules such as the DownUp Block and Fusion Attention to enhance\naccuracy. Experimental results validate the effectiveness of CRSynthNet,\ndemonstrating substantial improvements in restoring structural details,\npreserving spectral consist, and achieving superior visual effects that far\nexceed those produced by comparison methods. It achieves quantitative\nimprovements across multiple metrics: a peak signal-to-noise ratio (PSNR) of\n26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean\nsquare error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12\ndataset, a valuable resource specifically designed to address cloud cover\nchallenges in missing optical data synthesis study. The dataset uniquely\nincludes cloud-covered images and leverages earlier image to predict later\nimage, offering a realistic representation of real-world scenarios. This study\noffer practical method and valuable resources for optical satellite image\nsynthesis task.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16915", "pdf": "https://arxiv.org/pdf/2504.16915", "abs": "https://arxiv.org/abs/2504.16915", "authors": ["Chong Mou", "Yanze Wu", "Wenxu Wu", "Zinan Guo", "Pengze Zhang", "Yufeng Cheng", "Yiming Luo", "Fei Ding", "Shiwen Zhang", "Xinghui Li", "Mengtian Li", "Songtao Zhao", "Jian Zhang", "Qian He", "Xinglong Wu"], "title": "DreamO: A Unified Framework for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16275", "pdf": "https://arxiv.org/pdf/2504.16275", "abs": "https://arxiv.org/abs/2504.16275", "authors": ["Jannis Born", "Filip Skogh", "Kahn Rhrissorrakrai", "Filippo Utro", "Nico Wagner", "Aleksandros Sobczyk"], "title": "Quantum Doubly Stochastic Transformers", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CV"], "comment": "Under Review", "summary": "At the core of the Transformer, the Softmax normalizes the attention matrix\nto be right stochastic. Previous research has shown that this often\ndestabilizes training and that enforcing the attention matrix to be doubly\nstochastic (through Sinkhorn's algorithm) consistently improves performance\nacross different tasks, domains and Transformer flavors. However, Sinkhorn's\nalgorithm is iterative, approximative, non-parametric and thus inflexible\nw.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been\nproven that DSMs can be obtained with a parametric quantum circuit, yielding a\nnovel quantum inductive bias for DSMs with no known classical analogue.\nMotivated by this, we demonstrate the feasibility of a hybrid classical-quantum\ndoubly stochastic Transformer (QDSFormer) that replaces the Softmax in the\nself-attention layer with a variational quantum circuit. We study the\nexpressive power of the circuit and find that it yields more diverse DSMs that\nbetter preserve information than classical operators. Across multiple\nsmall-scale object recognition tasks, we find that our QDSFormer consistently\nsurpasses both a standard Vision Transformer and other doubly stochastic\nTransformers. Beyond the established Sinkformer, this comparison includes a\nnovel quantum-inspired doubly stochastic Transformer (based on QR\ndecomposition) that can be of independent interest. The QDSFormer also shows\nimproved training stability and lower performance variation suggesting that it\nmay mitigate the notoriously unstable training of ViTs on small-scale data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16667", "pdf": "https://arxiv.org/pdf/2504.16667", "abs": "https://arxiv.org/abs/2504.16667", "authors": ["Zhaohan Daniel Guo", "Bernardo Avila Pires", "Khimya Khetarpal", "Dale Schuurmans", "Bo Dai"], "title": "Representation Learning via Non-Contrastive Mutual Information", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML", "I.2.6; I.2.10"], "comment": null, "summary": "Labeling data is often very time consuming and expensive, leaving us with a\nmajority of unlabeled data. Self-supervised representation learning methods\nsuch as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very\nsuccessful at learning meaningful latent representations from unlabeled image\ndata, resulting in much more general and transferable representations for\ndownstream tasks. Broadly, self-supervised methods fall into two types: 1)\nContrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as\nBYOL. Contrastive methods are generally trying to maximize mutual information\nbetween related data points, so they need to compare every data point to every\nother data point, resulting in high variance, and thus requiring large batch\nsizes to work well. Non-contrastive methods like BYOL have much lower variance\nas they do not need to make pairwise comparisons, but are much trickier to\nimplement as they have the possibility of collapsing to a constant vector. In\nthis paper, we aim to develop a self-supervised objective that combines the\nstrength of both types. We start with a particular contrastive method called\nthe Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we\nconvert it into a more general non-contrastive form; this removes the pairwise\ncomparisons resulting in lower variance, but keeps the mutual information\nformulation of the contrastive method preventing collapse. We call our new\nobjective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by\nlearning image representations on ImageNet (similar to SimCLR and BYOL) and\nshow that it consistently improves upon the Spectral Contrastive loss baseline.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
{"id": "2504.16798", "pdf": "https://arxiv.org/pdf/2504.16798", "abs": "https://arxiv.org/abs/2504.16798", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis", "categories": ["cs.MM", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal neuroimaging provides complementary structural and functional\ninsights into both human brain organization and disease-related dynamics.\nRecent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's\ndisease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,\nfMRI) with behavioral cognitive scores tabular data biomarkers. However, the\nintrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI\ndynamics vs. 3D anatomical sMRI structure) presents critical challenges for\ndiscriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a\ngeometry-aware multimodal co-attention network with latent alignment for early\nAD diagnosis using sMRI and fMRI. At the core of our approach is a\nmulti-patch-to-multi-patch (M2M) contrastive loss function that quantifies and\nreduces representational discrepancies via geometry-weighted patch\ncorrespondence, explicitly aligning fMRI components across brain regions with\ntheir sMRI structural substrates without one-to-one constraints. Additionally,\nwe propose a latent-as-query co-attention module to autonomously discover\nfusion patterns, circumventing modality prioritization biases while minimizing\nfeature redundancy. We conduct extensive experiments to confirm the\neffectiveness of our method and highlight the correspondance between fMRI and\nsMRI as AD biomarkers.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-24.jsonl"}
