{"id": "2504.20157", "pdf": "https://arxiv.org/pdf/2504.20157", "abs": "https://arxiv.org/abs/2504.20157", "authors": ["Zae Myung Kim", "Chanwoo Park", "Vipul Raheja", "Dongyeop Kang"], "title": "Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reward-based alignment methods for large language models (LLMs) face two key\nlimitations: vulnerability to reward hacking, where models exploit flaws in the\nreward signal; and reliance on brittle, labor-intensive prompt engineering when\nLLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a\nframework that addresses these challenges by integrating a meta-reward model\nthat dynamically refines the reward model's prompt throughout training. In MPO,\nthe meta-reward model monitors the evolving training context and continuously\nadjusts the reward model's prompt to maintain high alignment, providing an\nadaptive reward signal that resists exploitation by the policy. This\nmeta-learning approach promotes a more stable policy optimization, and greatly\nreduces the need for manual reward prompt design. It yields performance on par\nwith or better than models guided by extensively hand-crafted reward prompts.\nFurthermore, we show that MPO maintains its effectiveness across diverse tasks,\nsuch as question answering and mathematical reasoning, without requiring\nspecialized reward designs. Beyond standard RLAIF, MPO's meta-learning\nformulation is readily extensible to higher-level alignment frameworks.\nOverall, this method addresses theoretical and practical challenges in\nreward-based RL alignment for LLMs, paving the way for more robust and\nadaptable alignment strategies. The code and models will be publicly shared.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "policy optimization", "RLAIF", "alignment", "reward hacking"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20605", "pdf": "https://arxiv.org/pdf/2504.20605", "abs": "https://arxiv.org/abs/2504.20605", "authors": ["Mihai Nadas", "Laura Diosan", "Andrei Piscoran", "Andreea Tomescu"], "title": "TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Moral stories are a time-tested vehicle for transmitting values, yet modern\nNLP lacks a large, structured corpus that couples coherent narratives with\nexplicit ethical lessons. We close this gap with TF1-EN-3M, the first open\ndataset of three million English-language fables generated exclusively by\ninstruction-tuned models no larger than 8B parameters. Each story follows a\nsix-slot scaffold (character -> trait -> setting -> conflict -> resolution ->\nmoral), produced through a combinatorial prompt engine that guarantees genre\nfidelity while covering a broad thematic space.\n  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores\ngrammar, creativity, moral clarity, and template adherence with (ii)\nreference-free diversity and readability metrics. Among ten open-weight\ncandidates, an 8B-parameter Llama-3 variant delivers the best quality-speed\ntrade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)\nat approximately 13.5 cents per 1,000 fables.\n  We release the dataset, generation code, evaluation scripts, and full\nmetadata under a permissive license, enabling exact reproducibility and cost\nbenchmarking. TF1-EN-3M opens avenues for research in instruction following,\nnarrative intelligence, value alignment, and child-friendly educational AI,\ndemonstrating that large-scale moral storytelling no longer requires\nproprietary giant models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20468", "pdf": "https://arxiv.org/pdf/2504.20468", "abs": "https://arxiv.org/abs/2504.20468", "authors": ["Yuanchen Wu", "Lu Zhang", "Hang Yao", "Junlong Du", "Ke Yan", "Shouhong Ding", "Yunsheng Wu", "Xiaoqiang Li"], "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive results across\nvarious cross-modal tasks. However, hallucinations, i.e., the models generating\ncounterfactual responses, remain a challenge. Though recent studies have\nattempted to alleviate object perception hallucinations, they focus on the\nmodels' response generation, and overlooking the task question itself. This\npaper discusses the vulnerability of LVLMs in solving counterfactual\npresupposition questions (CPQs), where the models are prone to accept the\npresuppositions of counterfactual objects and produce severe hallucinatory\nresponses. To this end, we introduce \"Antidote\", a unified, synthetic\ndata-driven post-training framework for mitigating both types of hallucination\nabove. It leverages synthetic data to incorporate factual priors into questions\nto achieve self-correction, and decouple the mitigation process into a\npreference optimization problem. Furthermore, we construct \"CP-Bench\", a novel\nbenchmark to evaluate LVLMs' ability to correctly handle CPQs and produce\nfactual responses. Applied to the LLaVA series, Antidote can simultaneously\nenhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR\nby 30-50%, all without relying on external supervision from stronger LVLMs or\nhuman feedback and introducing noticeable catastrophic forgetting issues.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20073", "pdf": "https://arxiv.org/pdf/2504.20073", "abs": "https://arxiv.org/abs/2504.20073", "authors": ["Zihan Wang", "Kangrui Wang", "Qineng Wang", "Pingyue Zhang", "Linjie Li", "Zhengyuan Yang", "Kefan Yu", "Minh Nhat Nguyen", "Licheng Liu", "Eli Gottlieb", "Monica Lam", "Yiping Lu", "Kyunghyun Cho", "Jiajun Wu", "Li Fei-Fei", "Lijuan Wang", "Yejin Choi", "Manling Li"], "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents presents unique\nchallenges including long-horizon decision making and interacting with\nstochastic environment feedback. While reinforcement learning (RL) has enabled\nprogress in static tasks, multi-turn agent RL training remains underexplored.\nWe propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a\ngeneral framework for trajectory-level agent RL, and introduce RAGEN, a modular\nsystem for training and evaluating LLM agents. Our study on three stylized\nenvironments reveals three core findings. First, our agent RL training shows a\nrecurring mode of Echo Trap where reward variance cliffs and gradient spikes;\nwe address this with StarPO-S, a stabilized variant with trajectory filtering,\ncritic incorporation, and decoupled clipping. Second, we find the shaping of RL\nrollouts would benefit from diverse initial states, medium interaction\ngranularity and more frequent sampling. Third, we show that without\nfine-grained, reasoning-aware reward signals, agent reasoning hardly emerge\nthrough multi-turn RL and they may show shallow strategies or hallucinated\nthoughts. Code and environments are available at\nhttps://github.com/RAGEN-AI/RAGEN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20458", "pdf": "https://arxiv.org/pdf/2504.20458", "abs": "https://arxiv.org/abs/2504.20458", "authors": ["Xiaolei Wang", "Chunxuan Xia", "Junyi Li", "Fanzhe Meng", "Lei Huang", "Jinpeng Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by SIGIR 2025", "summary": "Conversational recommendation systems (CRSs) use multi-turn interaction to\ncapture user preferences and provide personalized recommendations. A\nfundamental challenge in CRSs lies in effectively understanding user\npreferences from conversations. User preferences can be multifaceted and\ncomplex, posing significant challenges for accurate recommendations even with\naccess to abundant external knowledge. While interaction with users can clarify\ntheir true preferences, frequent user involvement can lead to a degraded user\nexperience.\n  To address this problem, we propose a generative reward model based simulated\nuser, named GRSU, for automatic interaction with CRSs. The simulated user\nprovides feedback to the items recommended by CRSs, enabling them to better\ncapture intricate user preferences through multi-turn interaction. Inspired by\ngenerative reward models, we design two types of feedback actions for the\nsimulated user: i.e., generative item scoring, which offers coarse-grained\nfeedback, and attribute-based item critique, which provides fine-grained\nfeedback. To ensure seamless integration, these feedback actions are unified\ninto an instruction-based format, allowing the development of a unified\nsimulated user via instruction tuning on synthesized data. With this simulated\nuser, automatic multi-turn interaction with CRSs can be effectively conducted.\nFurthermore, to strike a balance between effectiveness and efficiency, we draw\ninspiration from the paradigm of reward-guided search in complex reasoning\ntasks and employ beam search for the interaction process. On top of this, we\npropose an efficient candidate ranking method to improve the recommendation\nresults derived from interaction. Extensive experiments on public datasets\ndemonstrate the effectiveness, efficiency, and transferability of our approach.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["beam search"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20571", "pdf": "https://arxiv.org/pdf/2504.20571", "abs": "https://arxiv.org/abs/2504.20571", "authors": ["Yiping Wang", "Qing Yang", "Zhiyuan Zeng", "Liliang Ren", "Lucas Liu", "Baolin Peng", "Hao Cheng", "Xuehai He", "Kuan Wang", "Jianfeng Gao", "Weizhu Chen", "Shuohang Wang", "Simon Shaolei Du", "Yelong Shen"], "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages, 12 figures, link: https://github.com/ypwang61/One-Shot-RLVR", "summary": "We show that reinforcement learning with verifiable reward using one training\nexample (1-shot RLVR) is effective in incentivizing the math reasoning\ncapabilities of large language models (LLMs). Applying RLVR to the base model\nQwen2.5-Math-1.5B, we identify a single example that elevates model performance\non MATH500 from 36.0% to 73.6%, and improves the average performance across six\ncommon mathematical reasoning benchmarks from 17.6% to 35.7%. This result\nmatches the performance obtained using the 1.2k DeepScaleR subset (MATH500:\n73.6%, average: 35.9%), which includes the aforementioned example. Similar\nsubstantial improvements are observed across various models (Qwen2.5-Math-7B,\nLlama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and\nPPO), and different math examples (many of which yield approximately 30% or\ngreater improvement on MATH500 when employed as a single training example). In\naddition, we identify some interesting phenomena during 1-shot RLVR, including\ncross-domain generalization, increased frequency of self-reflection, and\nsustained test performance improvement even after the training accuracy has\nsaturated, a phenomenon we term post-saturation generalization. Moreover, we\nverify that the effectiveness of 1-shot RLVR primarily arises from the policy\ngradient loss, distinguishing it from the \"grokking\" phenomenon. We also show\nthe critical role of promoting exploration (e.g., by adding entropy loss with\nan appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe\nthat applying entropy loss alone, without any outcome reward, significantly\nenhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings\ncan inspire future work on RLVR data efficiency and encourage a re-examination\nof both recent progress and the underlying mechanisms in RLVR. Our code, model,\nand data are open source at https://github.com/ypwang61/One-Shot-RLVR", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20054", "pdf": "https://arxiv.org/pdf/2504.20054", "abs": "https://arxiv.org/abs/2504.20054", "authors": ["Jiayang Sun", "Hongbo Wang", "Jie Cao", "Huaibo Huang", "Ran He"], "title": "Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images, they often\nstruggle with accurate counting, attributes, and spatial relationships in\ncomplex multi-object scenes. To address these challenges, we propose Marmot, a\nnovel and generalizable framework that employs Multi-Agent Reasoning for\nMulti-Object Self-Correcting, enhancing image-text alignment and facilitating\nmore coherent multi-object image editing. Our framework adopts a\ndivide-and-conquer strategy that decomposes the self-correction task into three\ncritical dimensions (counting, attributes, and spatial relationships), and\nfurther divided into object-level subtasks. We construct a multi-agent editing\nsystem featuring a decision-execution-verification mechanism, effectively\nmitigating inter-object interference and enhancing editing reliability. To\nresolve the problem of subtask integration, we propose a Pixel-Domain Stitching\nSmoother that employs mask-guided two-stage latent space optimization. This\ninnovation enables parallel processing of subtask results, thereby enhancing\nruntime efficiency while eliminating multi-stage distortion accumulation.\nExtensive experiments demonstrate that Marmot significantly improves accuracy\nin object counting, attribute assignment, and spatial relationships for image\ngeneration tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20444", "pdf": "https://arxiv.org/pdf/2504.20444", "abs": "https://arxiv.org/abs/2504.20444", "authors": ["Mika Hämäläinen"], "title": "On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and\nClaude. We do this by repurposing the famous experiment Asch (1946) conducted\nusing human subjects. The experiment is simple, given two candidates with equal\ndescriptions which one is preferred if one description has positive adjectives\nfirst before negative ones and another description has negative adjectives\nfollowed by positive ones. We test this in two experiments. In one experiment,\nLLMs are given both candidates simultaneously in the same prompt, and in\nanother experiment, LLMs are given both candidates separately. We test all the\nmodels with 200 candidate pairs. We found that, in the first experiment,\nChatGPT preferred the candidate with positive adjectives listed first, while\nGemini preferred both equally often. Claude refused to make a choice. In the\nsecond experiment, ChatGPT and Claude were most likely to rank both candidates\nequally. In the case where they did not give an equal rating, both showed a\nclear preference to a candidate that had negative adjectives listed first.\nGemini was most likely to prefer a candidate with negative adjectives listed\nfirst.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20343", "pdf": "https://arxiv.org/pdf/2504.20343", "abs": "https://arxiv.org/abs/2504.20343", "authors": ["Amaan Izhar", "Nurul Japar", "Norisma Idris", "Ting Dang"], "title": "MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025, 8 pages, 8 figures, 3 tables", "summary": "Medical image reporting (MIR) aims to generate structured clinical\ndescriptions from radiological images. Existing methods struggle with\nfine-grained feature extraction, multimodal alignment, and generalization\nacross diverse imaging types, often relying on vanilla transformers and\nfocusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language\nmixture-of-experts model with gated cross-aligned fusion, designed to address\nthese limitations. Our architecture includes: (i) a multiscale vision encoder\n(MSVE) for capturing anatomical details at varying resolutions, (ii) a\nmultihead dual-branch latent attention (MDLA) module for vision-language\nalignment through latent bottleneck representations, and (iii) a modulated\nmixture-of-experts (MoE) decoder for adaptive expert specialization. We extend\nMIR to CT scans, retinal imaging, MRI scans, and gross pathology images,\nreporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.\nExtensive experiments and ablations confirm improved clinical accuracy,\ncross-modal alignment, and model interpretability. Code is available at\nhttps://github.com/AI-14/micar-vl-moe.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20409", "pdf": "https://arxiv.org/pdf/2504.20409", "abs": "https://arxiv.org/abs/2504.20409", "authors": ["Jingfeng Guo", "Jinnan Chen", "Weikai Chen", "Zhenyu Sun", "Lanjiong Li", "Baozhu Zhao", "Lingting Zhu", "Xin Wang", "Qi Liu"], "title": "GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation", "categories": ["cs.CV"], "comment": null, "summary": "This work presents GarmentX, a novel framework for generating diverse,\nhigh-fidelity, and wearable 3D garments from a single input image. Traditional\ngarment reconstruction methods directly predict 2D pattern edges and their\nconnectivity, an overly unconstrained approach that often leads to severe\nself-intersections and physically implausible garment structures. In contrast,\nGarmentX introduces a structured and editable parametric representation\ncompatible with GarmentCode, ensuring that the decoded sewing patterns always\nform valid, simulation-ready 3D garments while allowing for intuitive\nmodifications of garment shape and style. To achieve this, we employ a masked\nautoregressive model that sequentially predicts garment parameters, leveraging\nautoregressive modeling for structured generation while mitigating\ninconsistencies in direct pattern prediction. Additionally, we introduce\nGarmentX dataset, a large-scale dataset of 378,682 garment parameter-image\npairs, constructed through an automatic data generation pipeline that\nsynthesizes diverse and high-quality garment images conditioned on parametric\ngarment representations. Through integrating our method with GarmentX dataset,\nwe achieve state-of-the-art performance in geometric fidelity and input image\nalignment, significantly outperforming prior approaches. We will release\nGarmentX dataset upon publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20466", "pdf": "https://arxiv.org/pdf/2504.20466", "abs": "https://arxiv.org/abs/2504.20466", "authors": ["Woo Yi Yang", "Jiarui Wang", "Sijing Wu", "Huiyu Duan", "Yuxin Zhu", "Liu Yang", "Kang Fu", "Guangtao Zhai", "Xiongkuo Min"], "title": "LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in generative artificial intelligence have enabled the\ncreation of 3D human faces (HFs) for applications including media production,\nvirtual reality, security, healthcare, and game development, etc. However,\nassessing the quality and realism of these AI-generated 3D human faces remains\na significant challenge due to the subjective nature of human perception and\ninnate perceptual sensitivity to facial features. To this end, we conduct a\ncomprehensive study on the quality assessment of AI-generated 3D human faces.\nWe first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of\nAI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)\ncollected across two dimensions, i.e., quality and authenticity, 2,000\ndistortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,\nwe propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating\n3DHF capable of quality and authenticity score prediction, distortion-aware\nvisual question answering, and distortion-aware saliency prediction.\nExperimental results show that LMME3DHF achieves state-of-the-art performance,\nsurpassing existing methods in both accurately predicting quality scores for\nAI-generated 3D human faces and effectively identifying distortion-aware\nsalient regions and distortion types, while maintaining strong alignment with\nhuman perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be\nreleased upon the publication.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "question answering"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20679", "pdf": "https://arxiv.org/pdf/2504.20679", "abs": "https://arxiv.org/abs/2504.20679", "authors": ["Wing Yan Li", "Zeqiang Wang", "Jon Johnson", "Suparna De"], "title": "Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Automated detection of semantically equivalent questions in longitudinal\nsocial science surveys is crucial for long-term studies informing empirical\nresearch in the social, economic, and health sciences. Retrieving equivalent\nquestions faces dual challenges: inconsistent representation of theoretical\nconstructs (i.e. concept/sub-concept) across studies as well as between\nquestion and response options, and the evolution of vocabulary and structure in\nlongitudinal text. To address these challenges, our multi-disciplinary\ncollaboration of computer scientists and survey specialists presents a new\ninformation retrieval (IR) task of identifying concept (e.g. Housing, Job,\netc.) equivalence across question and response options to harmonise\nlongitudinal population studies. This paper investigates multiple unsupervised\napproaches on a survey dataset spanning 1946-2020, including probabilistic\nmodels, linear probing of language models, and pre-trained neural networks\nspecialised for IR. We show that IR-specialised neural models achieve the\nhighest overall performance with other approaches performing comparably.\nAdditionally, the re-ranking of the probabilistic model's results with neural\nmodels only introduces modest improvements of 0.07 at most in F1-score.\nQualitative post-hoc evaluation by survey specialists shows that models\ngenerally have a low sensitivity to questions with high lexical overlap,\nparticularly in cases where sub-concepts are mismatched. Altogether, our\nanalysis serves to further research on harmonising longitudinal studies in\nsocial science.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20498", "pdf": "https://arxiv.org/pdf/2504.20498", "abs": "https://arxiv.org/abs/2504.20498", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection", "categories": ["cs.CV"], "comment": "Manuscript submitted to IEEE Transactions on Multimedia", "summary": "Single-source Domain Generalization (SDG) in object detection aims to develop\na detector using only data from a source domain that can exhibit strong\ngeneralization capability when applied to unseen target domains. Existing\nmethods are built upon CNN-based detectors and primarily improve robustness by\nemploying carefully designed data augmentation strategies integrated with\nfeature alignment techniques. However, data augmentation methods have inherent\ndrawbacks; they are only effective when the augmented sample distribution\napproximates or covers the unseen scenarios, thus failing to enhance\ngeneralization across all unseen domains. Furthermore, while the recent\nDetection Transformer (DETR) has demonstrated superior generalization\ncapability in domain adaptation tasks due to its efficient global information\nextraction, its potential in SDG tasks remains unexplored. To this end, we\nintroduce a strong DETR-based detector named the Style-Adaptive Detection\nTransformer (SA-DETR) for SDG in object detection. Specifically, we present a\ndomain style adapter that projects the style representation of the unseen\ntarget domain into the training domain, enabling dynamic style adaptation.\nThen, we propose an object-aware contrastive learning module to guide the\ndetector in extracting domain-invariant features through contrastive learning.\nBy using object-aware gating masks to constrain feature aggregation in both\nspatial and semantic dimensions, this module achieves cross-domain contrast of\ninstance-level features, thereby enhancing generalization. Extensive\nexperiments demonstrate the superior performance and generalization capability\nof SA-DETR across five different weather scenarios. Code is released at\nhttps://github.com/h751410234/SA-DETR.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20629", "pdf": "https://arxiv.org/pdf/2504.20629", "abs": "https://arxiv.org/abs/2504.20629", "authors": ["Jeongsoo Choi", "Ji-Hoon Kim", "Kim Sung-Bin", "Tae-Hyun Oh", "Joon Son Chung"], "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In this paper, we address the task of multimodal-to-speech generation, which\naims to synthesize high-quality speech from multiple input modalities: text,\nvideo, and reference audio. This task has gained increasing attention due to\nits wide range of applications, such as film production, dubbing, and virtual\navatars. Despite recent progress, existing methods still suffer from\nlimitations in speech intelligibility, audio-video synchronization, speech\nnaturalness, and voice similarity to the reference speaker. To address these\nchallenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer\nthat generates accurate, synchronized, and natural-sounding speech from aligned\nmultimodal inputs. Built upon the in-context learning capability of the DiT\narchitecture, AlignDiT explores three effective strategies to align multimodal\nrepresentations. Furthermore, we introduce a novel multimodal classifier-free\nguidance mechanism that allows the model to adaptively balance information from\neach modality during speech synthesis. Extensive experiments demonstrate that\nAlignDiT significantly outperforms existing methods across multiple benchmarks\nin terms of quality, synchronization, and speaker similarity. Moreover,\nAlignDiT exhibits strong generalization capability across various multimodal\ntasks, such as video-to-speech synthesis and visual forced alignment,\nconsistently achieving state-of-the-art performance. The demo page is available\nat https://mm.kaist.ac.kr/projects/AlignDiT .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20084", "pdf": "https://arxiv.org/pdf/2504.20084", "abs": "https://arxiv.org/abs/2504.20084", "authors": ["Xiaojian Li", "Haoyuan Shi", "Rongwu Xu", "Wei Xu"], "title": "AI Awareness", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about\nincreasingly capable systems that demonstrate remarkable abilities in\nreasoning, language understanding, and problem-solving. These advancements have\nprompted a renewed examination of AI awareness, not as a philosophical question\nof consciousness, but as a measurable, functional capacity. In this review, we\nexplore the emerging landscape of AI awareness, which includes meta-cognition\n(the ability to represent and reason about its own state), self-awareness\n(recognizing its own identity, knowledge, limitations, inter alia), social\nawareness (modeling the knowledge, intentions, and behaviors of other agents),\nand situational awareness (assessing and responding to the context in which it\noperates).\n  First, we draw on insights from cognitive science, psychology, and\ncomputational theory to trace the theoretical foundations of awareness and\nexamine how the four distinct forms of AI awareness manifest in\nstate-of-the-art AI. Next, we systematically analyze current evaluation methods\nand empirical findings to better understand these manifestations. Building on\nthis, we explore how AI awareness is closely linked to AI capabilities,\ndemonstrating that more aware AI agents tend to exhibit higher levels of\nintelligent behaviors. Finally, we discuss the risks associated with AI\nawareness, including key topics in AI safety, alignment, and broader ethical\nconcerns.\n  AI awareness is a double-edged sword: it improves general capabilities, i.e.,\nreasoning, safety, while also raises concerns around misalignment and societal\nrisks, demanding careful oversight as AI capabilities grow. On the whole, our\ninterdisciplinary review provides a roadmap for future research and aims to\nclarify the role of AI awareness in the ongoing development of intelligent\nmachines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20094", "pdf": "https://arxiv.org/pdf/2504.20094", "abs": "https://arxiv.org/abs/2504.20094", "authors": ["Zheng Hui", "Xiaokai Wei", "Yexi Jiang", "Kevin Gao", "Chen Wang", "Frank Ong", "Se-eun Yoon", "Rachit Pareek", "Michelle Gong"], "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?", "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": null, "summary": "In this paper, we propose a multi-agent collaboration framework called MATCHA\nfor conversational recommendation system, leveraging large language models\n(LLMs) to enhance personalization and user engagement. Users can request\nrecommendations via free-form text and receive curated lists aligned with their\ninterests, preferences, and constraints. Our system introduces specialized\nagents for intent analysis, candidate generation, ranking, re-ranking,\nexplainability, and safeguards. These agents collaboratively improve\nrecommendations accuracy, diversity, and safety. On eight metrics, our model\nachieves superior or comparable performance to the current state-of-the-art.\nThrough comparisons with six baseline models, our approach addresses key\nchallenges in conversational recommendation systems for game recommendations,\nincluding: (1) handling complex, user-specific requests, (2) enhancing\npersonalization through multi-agent collaboration, (3) empirical evaluation and\ndeployment, and (4) ensuring safe and trustworthy interactions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "accuracy"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20879", "pdf": "https://arxiv.org/pdf/2504.20879", "abs": "https://arxiv.org/abs/2504.20879", "authors": ["Shivalika Singh", "Yiyang Nan", "Alex Wang", "Daniel D'Souza", "Sayash Kapoor", "Ahmet Üstün", "Sanmi Koyejo", "Yuntian Deng", "Shayne Longpre", "Noah Smith", "Beyza Ermis", "Marzieh Fadaee", "Sara Hooker"], "title": "The Leaderboard Illusion", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "comment": "68 pages, 18 figures, 9 tables", "summary": "Measuring progress is fundamental to the advancement of any scientific field.\nAs benchmarks play an increasingly central role, they also grow more\nsusceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard\nfor ranking the most capable AI systems. Yet, in this work we identify\nsystematic issues that have resulted in a distorted playing field. We find that\nundisclosed private testing practices benefit a handful of providers who are\nable to test multiple variants before public release and retract scores if\ndesired. We establish that the ability of these providers to choose the best\nscore leads to biased Arena scores due to selective disclosure of performance\nresults. At an extreme, we identify 27 private LLM variants tested by Meta in\nthe lead-up to the Llama-4 release. We also establish that proprietary closed\nmodels are sampled at higher rates (number of battles) and have fewer models\nremoved from the arena than open-weight and open-source alternatives. Both\nthese policies lead to large data access asymmetries over time. Providers like\nGoogle and OpenAI have received an estimated 19.2% and 20.4% of all data on the\narena, respectively. In contrast, a combined 83 open-weight models have only\nreceived an estimated 29.7% of the total data. We show that access to Chatbot\nArena data yields substantial benefits; even limited additional data can result\nin relative performance gains of up to 112% on the arena distribution, based on\nour conservative estimates. Together, these dynamics result in overfitting to\nArena-specific dynamics rather than general model quality. The Arena builds on\nthe substantial efforts of both the organizers and an open community that\nmaintains this valuable evaluation platform. We offer actionable\nrecommendations to reform the Chatbot Arena's evaluation framework and promote\nfairer, more transparent benchmarking for the field", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20865", "pdf": "https://arxiv.org/pdf/2504.20865", "abs": "https://arxiv.org/abs/2504.20865", "authors": ["Lorenzo Pellegrini", "Davide Cozzolino", "Serafino Pandolfini", "Davide Maltoni", "Matteo Ferrara", "Luisa Verdoliva", "Marco Prati", "Marco Ramilli"], "title": "AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 4 tables, code available:\n  https://github.com/MI-BioLab/AI-GenBench", "summary": "The rapid advancement of generative AI has revolutionized image creation,\nenabling high-quality synthesis from text prompts while raising critical\nchallenges for media authenticity. We present Ai-GenBench, a novel benchmark\ndesigned to address the urgent need for robust detection of AI-generated images\nin real-world scenarios. Unlike existing solutions that evaluate models on\nstatic datasets, Ai-GenBench introduces a temporal evaluation framework where\ndetection methods are incrementally trained on synthetic images, historically\nordered by their generative models, to test their ability to generalize to new\ngenerative models, such as the transition from GANs to diffusion models. Our\nbenchmark focuses on high-quality, diverse visual content and overcomes key\nlimitations of current approaches, including arbitrary dataset splits, unfair\ncomparisons, and excessive computational demands. Ai-GenBench provides a\ncomprehensive dataset, a standardized evaluation protocol, and accessible tools\nfor both researchers and non-experts (e.g., journalists, fact-checkers),\nensuring reproducibility while maintaining practical training requirements. By\nestablishing clear evaluation rules and controlled augmentation strategies,\nAi-GenBench enables meaningful comparison of detection methods and scalable\nsolutions. Code and data are publicly available to ensure reproducibility and\nto support the development of robust forensic detectors to keep pace with the\nrise of new synthetic generators.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "factuality", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20996", "pdf": "https://arxiv.org/pdf/2504.20996", "abs": "https://arxiv.org/abs/2504.20996", "authors": ["Sicheng Mo", "Thao Nguyen", "Xun Huang", "Siddharth Srinivasan Iyer", "Yijun Li", "Yuchen Liu", "Abhishek Tandon", "Eli Shechtman", "Krishna Kumar Singh", "Yong Jae Lee", "Bolei Zhou", "Yuheng Li"], "title": "X-Fusion: Introducing New Modality to Frozen Large Language Models", "categories": ["cs.CV"], "comment": "Project Page: https://sichengmo.github.io/XFusion/", "summary": "We propose X-Fusion, a framework that extends pretrained Large Language\nModels (LLMs) for multimodal tasks while preserving their language\ncapabilities. X-Fusion employs a dual-tower design with modality-specific\nweights, keeping the LLM's parameters frozen while integrating vision-specific\ninformation for both understanding and generation. Our experiments demonstrate\nthat X-Fusion consistently outperforms alternative architectures on both\nimage-to-text and text-to-image tasks. We find that incorporating\nunderstanding-focused data improves generation quality, reducing image data\nnoise enhances overall performance, and feature alignment accelerates\nconvergence for smaller models but has minimal impact on larger ones. Our\nfindings provide valuable insights into building efficient unified multimodal\nmodels.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20340", "pdf": "https://arxiv.org/pdf/2504.20340", "abs": "https://arxiv.org/abs/2504.20340", "authors": ["Khoi Trinh", "Scott Seidenberger", "Raveen Wijewickrama", "Murtuza Jadliwala", "Anindya Maiti"], "title": "A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "With AI-generated content becoming ubiquitous across the web, social media,\nand other digital platforms, it is vital to examine how such content are\ninspired and generated. The creation of AI-generated images often involves\nrefining the input prompt iteratively to achieve desired visual outcomes. This\nstudy focuses on the relatively underexplored concept of image regeneration\nusing AI, in which a human operator attempts to closely recreate a specific\ntarget image by iteratively refining their prompt. Image regeneration is\ndistinct from normal image generation, which lacks any predefined visual\nreference. A separate challenge lies in determining whether existing image\nsimilarity metrics (ISMs) can provide reliable, objective feedback in iterative\nworkflows, given that we do not fully understand if subjective human judgments\nof similarity align with these metrics. Consequently, we must first validate\ntheir alignment with human perception before assessing their potential as a\nfeedback mechanism in the iterative prompt refinement process. To address these\nresearch gaps, we present a structured user study evaluating how iterative\nprompt refinement affects the similarity of regenerated images relative to\ntheir targets, while also examining whether ISMs capture the same improvements\nperceived by human observers. Our findings suggest that incremental prompt\nadjustments substantially improve alignment, verified through both subjective\nevaluations and quantitative measures, underscoring the broader potential of\niterative workflows to enhance generative AI content creation across various\napplication domains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-04-30.jsonl"}
{"id": "2504.20930", "pdf": "https://arxiv.org/pdf/2504.20930", "abs": "https://arxiv.org/abs/2504.20930", "authors": ["Ziqing Fan", "Cheng Liang", "Chaoyi Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in reasoning-enhanced large language models (LLMs) and\nmultimodal LLMs (MLLMs) have significantly improved performance in complex\ntasks, yet medical AI models often overlook the structured reasoning processes\ninherent in clinical practice. In this work, we present ChestX-Reasoner, a\nradiology diagnosis MLLM designed to leverage process supervision mined\ndirectly from clinical reports, reflecting the step-by-step reasoning followed\nby radiologists. We construct a large dataset by extracting and refining\nreasoning chains from routine radiology reports. Our two-stage training\nframework combines supervised fine-tuning and reinforcement learning guided by\nprocess rewards to better align model reasoning with clinical standards. We\nintroduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual\nquestion answering samples with 301K clinically validated reasoning steps, and\npropose RadRScore, a metric evaluating reasoning factuality, completeness, and\neffectiveness. ChestX-Reasoner outperforms existing medical and general-domain\nMLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,\nand 18% improvements in reasoning ability compared to the best medical MLLM,\nthe best general MLLM, and its base model, respectively, as well as 3.3%, 24%,\nand 27% improvements in outcome accuracy. All resources are open-sourced to\nfacilitate further research in medical reasoning MLLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "factuality", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-04-30.jsonl"}
