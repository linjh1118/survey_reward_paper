{"id": "2505.01162", "pdf": "https://arxiv.org/pdf/2505.01162", "abs": "https://arxiv.org/abs/2505.01162", "authors": ["Chebrolu Niranjan", "Kokil Jaidka", "Gerard Christopher Yeo"], "title": "On the Limitations of Steering in Language Model Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Steering vectors are a promising approach to aligning language model behavior\nat inference time. In this paper, we propose a framework to assess the\nlimitations of steering vectors as alignment mechanisms. Using a framework of\ntransformer hook interventions and antonym-based function vectors, we evaluate\nthe role of prompt structure and context complexity in steering effectiveness.\nOur findings indicate that steering vectors are promising for specific\nalignment tasks, such as value alignment, but may not provide a robust\nfoundation for general-purpose alignment in LLMs, particularly in complex\nscenarios. We establish a methodological foundation for future investigations\ninto steering capabilities of reasoning models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.00725", "pdf": "https://arxiv.org/pdf/2505.00725", "abs": "https://arxiv.org/abs/2505.00725", "authors": ["Bithiah Yuan"], "title": "FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.1; H.3.3"], "comment": "Submitted in partial fulfillment of the requirements for the Master\n  of Science degree in Computer Science at the University of Freiburg, July 31,\n  2020", "summary": "Motivated by the emerging demand in the financial industry for the automatic\nanalysis of unstructured and structured data at scale, Question Answering (QA)\nsystems can provide lucrative and competitive advantages to companies by\nfacilitating the decision making of financial advisers. Consequently, we\npropose a novel financial QA system using the transformer-based pre-trained\nBERT language model to address the limitations of data scarcity and language\nspecificity in the financial domain. Our system focuses on financial\nnon-factoid answer selection, which retrieves a set of passage-level texts and\nselects the most relevant as the answer. To increase efficiency, we formulate\nthe answer selection task as a re-ranking problem, in which our system consists\nof an Answer Retriever using BM25, a simple information retrieval approach, to\nfirst return a list of candidate answers, and an Answer Re-ranker built with\nvariants of pre-trained BERT language models to re-rank and select the most\nrelevant answers. We investigate various learning, further pre-training, and\nfine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a\nmodel built from applying the Transfer and Adapt further fine-tuning and\npointwise learning approach, is the most effective, improving the\nstate-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on\nNDCG, and 21% on Precision@1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.00753", "pdf": "https://arxiv.org/pdf/2505.00753", "abs": "https://arxiv.org/abs/2505.00753", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Yankai Chen", "Chunyu Miao", "Hoang Nguyen", "Yue Zhou", "Weizhi Zhang", "Liancheng Fang", "Langzhou He", "Yangning Li", "Yuwei Cao", "Dongyuan Li", "Renhe Jiang", "Philip S. Yu"], "title": "A Survey on Large Language Model based Human-Agent Systems", "categories": ["cs.CL", "cs.LG"], "comment": "Paper lists and resources are available at\n  \\url{https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers}", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThis paper provides the first comprehensive and structured survey of LLM-HAS.\nIt clarifies fundamental concepts, systematically presents core components\nshaping these systems, including environment & profiling, human feedback,\ninteraction types, orchestration and communication, explores emerging\napplications, and discusses unique challenges and opportunities. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "reliability"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "agreement"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01238", "pdf": "https://arxiv.org/pdf/2505.01238", "abs": "https://arxiv.org/abs/2505.01238", "authors": ["Mahdi Dhaini", "Kafaite Zahra Hussain", "Efstratios Zaradoukas", "Gjergji Kasneci"], "title": "EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to the xAI World Conference (2025) - System Demonstration", "summary": "As Natural Language Processing (NLP) models continue to evolve and become\nintegral to high-stakes applications, ensuring their interpretability remains a\ncritical challenge. Given the growing variety of explainability methods and\ndiverse stakeholder requirements, frameworks that help stakeholders select\nappropriate explanations tailored to their specific use cases are increasingly\nimportant. To address this need, we introduce EvalxNLP, a Python framework for\nbenchmarking state-of-the-art feature attribution methods for transformer-based\nNLP models. EvalxNLP integrates eight widely recognized explainability\ntechniques from the Explainable AI (XAI) literature, enabling users to generate\nand evaluate explanations based on key properties such as faithfulness,\nplausibility, and complexity. Our framework also provides interactive,\nLLM-based textual explanations, facilitating user understanding of the\ngenerated explanations and evaluation outcomes. Human evaluation results\nindicate high user satisfaction with EvalxNLP, suggesting it is a promising\nframework for benchmarking explanation methods across diverse user groups. By\noffering a user-friendly and extensible platform, EvalxNLP aims at\ndemocratizing explainability tools and supporting the systematic comparison and\nadvancement of XAI techniques in NLP.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01040", "pdf": "https://arxiv.org/pdf/2505.01040", "abs": "https://arxiv.org/abs/2505.01040", "authors": ["Ru-yu Yan", "Da-Qing Zhang"], "title": "Edge Detection based on Channel Attention and Inter-region Independence Test", "categories": ["cs.CV"], "comment": null, "summary": "Existing edge detection methods often suffer from noise amplification and\nexcessive retention of non-salient details, limiting their applicability in\nhigh-precision industrial scenarios. To address these challenges, we propose\nCAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM)\nand Edge Detection via Independence Testing (EDIT). The CAM module adaptively\nenhances discriminative edge features through multi-channel fusion, while the\nEDIT module employs region-wise statistical independence analysis (using\nFisher's exact test and chi-square test) to suppress uncorrelated\nnoise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate\nstate-of-the-art performance. Among the nine comparison algorithms, the\nF-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of\n19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the\nlatest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations\nfurther reveal a 2.2\\% PSNR improvement under Gaussian noise compared to\nbaseline methods. Qualitative results exhibit cleaner edge maps with reduced\nartifacts, demonstrating its potential for high-precision industrial\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.00831", "pdf": "https://arxiv.org/pdf/2505.00831", "abs": "https://arxiv.org/abs/2505.00831", "authors": ["Quang P. M. Pham", "Khoi T. N. Nguyen", "Nhi H. Doan", "Cuong A. Pham", "Kentaro Inui", "Dezhen Song"], "title": "SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation", "categories": ["cs.RO", "cs.CL"], "comment": "Paper is under review", "summary": "Efficient path planning in robotics, particularly within large-scale, dynamic\nenvironments, remains a significant hurdle. While Large Language Models (LLMs)\noffer strong reasoning capabilities, their high computational cost and limited\nadaptability in dynamic scenarios hinder real-time deployment on edge devices.\nWe present SmallPlan -- a novel framework leveraging LLMs as teacher models to\ntrain lightweight Small Language Models (SLMs) for high-level path planning\ntasks. In SmallPlan, the SLMs provide optimal action sequences to navigate\nacross scene graphs that compactly represent full-scaled 3D scenes. The SLMs\nare trained in a simulation-powered, interleaved manner with LLM-guided\nsupervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not\nonly enables SLMs to successfully complete navigation tasks but also makes them\naware of important factors like travel distance and number of trials. Through\nexperiments, we demonstrate that the fine-tuned SLMs perform competitively with\nlarger models like GPT-4o on sequential path planning, without suffering from\nhallucination and overfitting. SmallPlan is resource-efficient, making it\nwell-suited for edge-device deployment and advancing practical autonomous\nrobotics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01079", "pdf": "https://arxiv.org/pdf/2505.01079", "abs": "https://arxiv.org/abs/2505.01079", "authors": ["Daneul Kim", "Jaeah Lee", "Jaesik Park"], "title": "Improving Editability in Image Generation with Layer-wise Memory", "categories": ["cs.CV", "eess.IV"], "comment": "CVPR 2025. Project page :\n  https://carpedkm.github.io/projects/improving_edit/index.html", "summary": "Most real-world image editing tasks require multiple sequential edits to\nachieve desired results. Current editing approaches, primarily designed for\nsingle-object modifications, struggle with sequential editing: especially with\nmaintaining previous edits along with adapting new objects naturally into the\nexisting content. These limitations significantly hinder complex editing\nscenarios where multiple objects need to be modified while preserving their\ncontextual relationships. We address this fundamental challenge through two key\nproposals: enabling rough mask inputs that preserve existing content while\nnaturally integrating new elements and supporting consistent editing across\nmultiple modifications. Our framework achieves this through layer-wise memory,\nwhich stores latent representations and prompt embeddings from previous edits.\nWe propose Background Consistency Guidance that leverages memorized latents to\nmaintain scene coherence and Multi-Query Disentanglement in cross-attention\nthat ensures natural adaptation to existing content. To evaluate our method, we\npresent a new benchmark dataset incorporating semantic alignment metrics and\ninteractive editing scenarios. Through comprehensive experiments, we\ndemonstrate superior performance in iterative image editing tasks with minimal\nuser effort, requiring only rough masks while maintaining high-quality results\nthroughout multiple editing steps.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01104", "pdf": "https://arxiv.org/pdf/2505.01104", "abs": "https://arxiv.org/abs/2505.01104", "authors": ["Do Huu Dat", "Nam Hyeonu", "Po-Yuan Mao", "Tae-Hyun Oh"], "title": "VSC: Visual Search Compositional Text-to-Image Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have shown impressive capabilities in\ngenerating realistic visuals from natural-language prompts, yet they often\nstruggle with accurately binding attributes to corresponding objects,\nespecially in prompts containing multiple attribute-object pairs. This\nchallenge primarily arises from the limitations of commonly used text encoders,\nsuch as CLIP, which can fail to encode complex linguistic relationships and\nmodifiers effectively. Existing approaches have attempted to mitigate these\nissues through attention map control during inference and the use of layout\ninformation or fine-tuning during training, yet they face performance drops\nwith increased prompt complexity. In this work, we introduce a novel\ncompositional generation method that leverages pairwise image embeddings to\nimprove attribute-object binding. Our approach decomposes complex prompts into\nsub-prompts, generates corresponding images, and computes visual prototypes\nthat fuse with text embeddings to enhance representation. By applying\nsegmentation-based localization training, we address cross-attention\nmisalignment, achieving improved accuracy in binding multiple attributes to\nobjects. Our approaches outperform existing compositional text-to-image\ndiffusion models on the benchmark T2I CompBench, achieving better image\nquality, evaluated by humans, and emerging robustness under scaling number of\nbinding pairs in the prompt.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01207", "pdf": "https://arxiv.org/pdf/2505.01207", "abs": "https://arxiv.org/abs/2505.01207", "authors": ["Qingyu Xian", "Weiqin Jiao", "Hao Cheng", "Berend Jan van der Zwaag", "Yanqiu Huang"], "title": "T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view camera pose estimation, which aims to estimate the\n6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from\ndifferent viewpoints, is a fundamental yet challenging problem in remote\nsensing applications. Existing methods often overlook the translation\ninformation between each pair of viewpoints, leading to suboptimal performance\nin sparse-view scenarios. To address this limitation, we introduce T-Graph, a\nlightweight, plug-and-play module to enhance camera pose estimation in\nsparse-view settings. T-graph takes paired image features as input and maps\nthem through a Multilayer Perceptron (MLP). It then constructs a fully\nconnected translation graph, where nodes represent cameras and edges encode\ntheir translation relationships. It can be seamlessly integrated into existing\nmodels as an additional branch in parallel with the original prediction,\nmaintaining efficiency and ease of use. Furthermore, we introduce two pairwise\ntranslation representations, relative-t and pair-t, formulated under different\nlocal coordinate systems. While relative-t captures intuitive spatial\nrelationships, pair-t offers a rotation-disentangled alternative. The two\nrepresentations contribute to enhanced adaptability across diverse application\nscenarios, further improving our module's robustness. Extensive experiments on\ntwo state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D\nand IMC PhotoTourism) validate both the effectiveness and generalizability of\nT-Graph. The results demonstrate consistent improvements across various\nmetrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8\nviewpoints.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01237", "pdf": "https://arxiv.org/pdf/2505.01237", "abs": "https://arxiv.org/abs/2505.01237", "authors": ["Edson Araujo", "Andrew Rouditchenko", "Yuan Gong", "Saurabhchand Bhati", "Samuel Thomas", "Brian Kingsbury", "Leonid Karlinsky", "Rogerio Feris", "James R. Glass"], "title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": "To be published at CVPR 2025, code available at\n  https://github.com/edsonroteia/cav-mae-sync", "summary": "Recent advances in audio-visual learning have shown promising results in\nlearning representations across modalities. However, most approaches rely on\nglobal audio representations that fail to capture fine-grained temporal\ncorrespondences with visual frames. Additionally, existing methods often\nstruggle with conflicting optimization objectives when trying to jointly learn\nreconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync\nas a simple yet effective extension of the original CAV-MAE framework for\nself-supervised audio-visual learning. We address three key challenges: First,\nwe tackle the granularity mismatch between modalities by treating audio as a\ntemporal sequence aligned with video frames, rather than using global\nrepresentations. Second, we resolve conflicting optimization goals by\nseparating contrastive and reconstruction objectives through dedicated global\ntokens. Third, we improve spatial localization by introducing learnable\nregister tokens that reduce semantic load on patch tokens. We evaluate the\nproposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on\nzero-shot retrieval, classification and localization tasks demonstrating\nstate-of-the-art performance and outperforming more complex architectures.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-05.jsonl"}
{"id": "2505.01263", "pdf": "https://arxiv.org/pdf/2505.01263", "abs": "https://arxiv.org/abs/2505.01263", "authors": ["Gaoxiang Cong", "Liang Li", "Jiadong Pan", "Zhedong Zhang", "Amin Beheshti", "Anton van den Hengel", "Yuankai Qi", "Qingming Huang"], "title": "FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Movie Dubbing aims to convert scripts into speeches that align with the given\nmovie clip in both temporal and emotional aspects while preserving the vocal\ntimbre of a given brief reference audio. Existing methods focus primarily on\nreducing the word error rate while ignoring the importance of lip-sync and\nacoustic quality. To address these issues, we propose a large language model\n(LLM) based flow matching architecture for dubbing, named FlowDubber, which\nachieves high-quality audio-visual sync and pronunciation by incorporating a\nlarge speech language model and dual contrastive aligning while achieving\nbetter acoustic quality via the proposed voice-enhanced flow matching than\nprevious works. First, we introduce Qwen2.5 as the backbone of LLM to learn the\nin-context sequence from movie scripts and reference audio. Then, the proposed\nsemantic-aware learning focuses on capturing LLM semantic knowledge at the\nphoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment\nwith lip movement, reducing ambiguities where similar phonemes might be\nconfused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves\nacoustic quality in two aspects, which introduces an LLM-based acoustics flow\nmatching guidance to strengthen clarity and uses affine style prior to enhance\nidentity when recovering noise into mel-spectrograms via gradient vector field\nprediction. Extensive experiments demonstrate that our method outperforms\nseveral state-of-the-art methods on two primary benchmarks. The demos are\navailable at\n{\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-05.jsonl"}
