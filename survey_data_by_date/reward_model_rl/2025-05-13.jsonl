{"id": "2505.07271", "pdf": "https://arxiv.org/pdf/2505.07271", "abs": "https://arxiv.org/abs/2505.07271", "authors": ["Jiwoo Hong", "Noah Lee", "Eunki Kim", "Guijin Son", "Woojin Chung", "Aman Gupta", "Shao Tang", "James Thorne"], "title": "On the Robustness of Reward Models for Language Model Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "The Bradley-Terry (BT) model is widely practiced in reward modeling for\nreinforcement learning with human feedback (RLHF). Despite its effectiveness,\nreward models (RMs) trained with BT model loss are prone to over-optimization,\nlosing generalizability to unseen input distributions. In this paper, we study\nthe cause of over-optimization in RM training and its downstream effects on the\nRLHF procedure, accentuating the importance of distributional robustness of RMs\nin unseen data. First, we show that the excessive dispersion of hidden state\nnorms is the main source of over-optimization. Then, we propose batch-wise\nsum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch,\nconstraining the rewards with extreme magnitudes. We assess the impact of BSR\nin improving robustness in RMs through four scenarios of over-optimization,\nwhere BSR consistently manifests better robustness. Subsequently, we compare\nthe plain BT model and BSR on RLHF training and empirically show that robust\nRMs better align the policy to the gold preference model. Finally, we apply BSR\nto high-quality data and models, which surpasses state-of-the-art RMs in the 8B\nscale by adding more than 5% in complex preference prediction tasks. By\nconducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length\nby 40% while adding a 7% increase in win rate, further highlighting that\nrobustness in RMs induces robustness in RLHF training. We release the code,\ndata, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "RLHF", "human feedback", "reinforcement learning", "preference", "alignment"], "score": 6}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07263", "pdf": "https://arxiv.org/pdf/2505.07263", "abs": "https://arxiv.org/abs/2505.07263", "authors": ["Xiaokun Wang", "Chris", "Jiangbo Pei", "Wei Shen", "Yi Peng", "Yunzhuo Hao", "Weijie Qiu", "Ai Jian", "Tianyidan Xie", "Xuchen Song", "Yang Liu", "Yahui Zhou"], "title": "Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We propose Skywork-VL Reward, a multimodal reward model that provides reward\nsignals for both multimodal understanding and reasoning tasks. Our technical\napproach comprises two key components: First, we construct a large-scale\nmultimodal preference dataset that covers a wide range of tasks and scenarios,\nwith responses collected from both standard vision-language models (VLMs) and\nadvanced VLM reasoners. Second, we design a reward model architecture based on\nQwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage\nfine-tuning using pairwise ranking loss on pairwise preference data.\nExperimental evaluations show that Skywork-VL Reward achieves state-of-the-art\nresults on multimodal VL-RewardBench and exhibits competitive performance on\nthe text-only RewardBench benchmark. Furthermore, preference data constructed\nbased on our Skywork-VL Reward proves highly effective for training Mixed\nPreference Optimization (MPO), leading to significant improvements in\nmultimodal reasoning capabilities. Our results underscore Skywork-VL Reward as\na significant advancement toward general-purpose, reliable reward models for\nmultimodal alignment. Our model has been publicly released to promote\ntransparency and reproducibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "ranking", "pairwise", "alignment"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "preference dataset"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07818", "pdf": "https://arxiv.org/pdf/2505.07818", "abs": "https://arxiv.org/abs/2505.07818", "authors": ["Zeyue Xue", "Jie Wu", "Yu Gao", "Fangyuan Kong", "Lingting Zhu", "Mengzhao Chen", "Zhiheng Liu", "Wei Liu", "Qiushan Guo", "Weilin Huang", "Ping Luo"], "title": "DanceGRPO: Unleashing GRPO on Visual Generation", "categories": ["cs.CV"], "comment": "Project Page: https://dancegrpo.github.io/", "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback", "reinforcement learning", "policy optimization", "alignment"], "score": 5}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06496", "pdf": "https://arxiv.org/pdf/2505.06496", "abs": "https://arxiv.org/abs/2505.06496", "authors": ["Erik Nijkamp", "Bo Pang", "Egor Pakhomov", "Akash Gokul", "Jin Qu", "Silvio Savarese", "Yingbo Zhou", "Caiming Xiong"], "title": "xGen-small Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce xGen-small, a family of 4B and 9B Transformer decoder models\noptimized for long-context applications. Our vertically integrated pipeline\nunites domain-balanced, frequency-aware data curation; multi-stage pre-training\nwith quality annealing and length extension to 128k tokens; and targeted\npost-training via supervised fine-tuning, preference learning, and online\nreinforcement learning. xGen-small delivers strong performance across various\ntasks, especially in math and coding domains, while excelling at long context\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "reinforcement learning", "preference"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06418", "pdf": "https://arxiv.org/pdf/2505.06418", "abs": "https://arxiv.org/abs/2505.06418", "authors": ["Ming Liu", "Liwen Wang", "Wensheng Zhang"], "title": "Is your multimodal large language model a good science tutor?", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate impressive performance\non scientific reasoning tasks (e.g., ScienceQA). However, most existing\nbenchmarks focus narrowly on the accuracy of the final answer while ignoring\nother metrics. In particular, when applying MLLMs to educational contexts, the\ngoal is not only correctness but also the ability to teach. In this paper, we\npropose a framework that evaluates MLLMs as science tutors using a\ncomprehensive educational rubric and a simulated student model that judges the\nteaching performance of the tutors. Given a list of candidate MLLM science\ntutors, we use rubric-based student judgments to produce a range of tutor\nperformance scores, identifying both strong and weak tutors. Using the training\nsection of the ScienceQA dataset, we then construct a data set of pairwise\ncomparisons between the outputs of strong and weak tutors. This enables us to\napply multiple preference optimization methods to fine-tune an underperforming\ntutor model (Qwen2-VL-2B) into more effective ones. Our results also show that\nstrong problem-solving skills do not guarantee high-quality tutoring and that\nperformance optimization-guided refinements can yield more educationally\naligned tutor models. This approach opens avenues for building MLLMs that serve\nnot only as problem solvers, but as genuinely helpful educational assistants.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "rubric"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06698", "pdf": "https://arxiv.org/pdf/2505.06698", "abs": "https://arxiv.org/abs/2505.06698", "authors": ["Zongqi Wang", "Tianle Gu", "Chen Gong", "Xin Tian", "Siqi Bao", "Yujiu Yang"], "title": "From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena\nare seeing growing adoption for the evaluation of Large Language Models (LLMs).\nExisting research has primarily focused on approximating human-based model\nrankings using limited data and LLM-as-a-Judge. However, the fundamental\npremise of these studies, which attempts to replicate human rankings, is\nflawed. Specifically, these benchmarks typically offer only overall scores,\nlimiting their utility to leaderboard rankings, rather than providing feedback\nthat can guide model optimization and support model profiling. Therefore, we\nadvocate for an evaluation paradigm shift from approximating human-based model\nrankings to providing feedback with analytical value. To this end, we introduce\nFeedbacker, an evaluation framework that provides comprehensive and\nfine-grained results, thereby enabling thorough identification of a model's\nspecific strengths and weaknesses. Such feedback not only supports the targeted\noptimization of the model but also enhances the understanding of its behavior.\nFeedbacker comprises three key components: an extensible tree-based query\ntaxonomy builder, an automated query synthesis scheme, and a suite of\nvisualization and analysis tools. Furthermore, we propose a novel\nLLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise\nevaluation. This method derives evaluation criteria by pre-comparing the\ndifferences between several auxiliary responses, achieving the accuracy of\npairwise evaluation while maintaining the time complexity of pointwise\nevaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs,\nwe demonstrate the usage of Feedbacker and highlight its effectiveness and\npotential. Our homepage project is available at\nhttps://liudan193.github.io/Feedbacker.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy", "fine-grained", "criteria"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07558", "pdf": "https://arxiv.org/pdf/2505.07558", "abs": "https://arxiv.org/abs/2505.07558", "authors": ["Rei Higuchi", "Taiji Suzuki"], "title": "Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences is crucial for\nsafe deployment, yet existing methods assume specific preference models like\nBradley-Terry model. This assumption leads to statistical inconsistency, where\nmore data doesn't guarantee convergence to true human preferences. To address\nthis critical gap, we introduce a novel alignment method Direct Density Ratio\nOptimization (DDRO). DDRO directly estimates the density ratio between\npreferred and unpreferred output distributions, circumventing the need for\nexplicit human preference modeling. We theoretically prove that DDRO is\nstatistically consistent, ensuring convergence to the true preferred\ndistribution as the data size grows, regardless of the underlying preference\nstructure. Experiments demonstrate that DDRO achieves superior performance\ncompared to existing methods on many major benchmarks. DDRO unlocks the\npotential for truly data-driven alignment, paving the way for more reliable and\nhuman-aligned LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["human preference"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07500", "pdf": "https://arxiv.org/pdf/2505.07500", "abs": "https://arxiv.org/abs/2505.07500", "authors": ["Bahram Mohammadi", "Ehsan Abbasnejad", "Yuankai Qi", "Qi Wu", "Anton Van Den Hengel", "Javen Qinfeng Shi"], "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The remote embodied referring expression (REVERIE) task requires an agent to\nnavigate through complex indoor environments and localize a remote object\nspecified by high-level instructions, such as \"bring me a spoon\", without\npre-exploration. Hence, an efficient navigation plan is essential for the final\nsuccess. This paper proposes a novel parameter-efficient action planner using\nlarge language models (PEAP-LLM) to generate a single-step instruction at each\nlocation. The proposed model consists of two modules, LLM goal planner (LGP)\nand LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan\nfrom REVERIE instructions, including the target object and room. Then, LAP\ngenerates a single-step instruction with the goal-oriented plan, high-level\ninstruction, and current visual observation as input. PEAP-LLM enables the\nembodied agent to interact with LAP as the path planner on the fly. A simple\ndirect application of LLMs hardly achieves good performance. Also, existing\nhard-prompt-based methods are error-prone in complicated scenarios and need\nhuman intervention. To address these issues and prevent the LLM from generating\nhallucinations and biased information, we propose a novel two-stage method for\nfine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct\npreference optimization (DPO). SFT improves the quality of generated\ninstructions, while DPO utilizes environmental feedback. Experimental results\nshow the superiority of our proposed model on REVERIE compared to the previous\nstate-of-the-art.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07538", "pdf": "https://arxiv.org/pdf/2505.07538", "abs": "https://arxiv.org/abs/2505.07538", "authors": ["Bohan Wang", "Zhongqi Yue", "Fengda Zhang", "Shuo Chen", "Li'an Bi", "Junzhe Zhang", "Xue Song", "Kennard Yanting Chan", "Jiachun Pan", "Weijia Wu", "Mingze Zhou", "Wang Lin", "Kaihang Pan", "Saining Zhang", "Liyu Jia", "Wentao Hu", "Wei Zhao", "Hanwang Zhang"], "title": "Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "We completely discard the conventional spatial prior in image representation\nand introduce a novel discrete visual tokenizer: Self-consistency Tokenizer\n(Selftok). At its design core, we compose an autoregressive (AR) prior --\nmirroring the causal structure of language -- into visual tokens by using the\nreverse diffusion process of image generation. The AR property makes Selftok\nfundamentally distinct from traditional spatial tokens in the following two key\nways: - Selftok offers an elegant and minimalist approach to unify diffusion\nand AR for vision-language models (VLMs): By representing images with Selftok\ntokens, we can train a VLM using a purely discrete autoregressive architecture\n-- like that in LLMs -- without requiring additional modules or training\nobjectives. - We theoretically show that the AR prior satisfies the Bellman\nequation, whereas the spatial prior does not. Therefore, Selftok supports\nreinforcement learning (RL) for visual generation with effectiveness comparable\nto that achieved in LLMs. Besides the AR property, Selftok is also a SoTA\ntokenizer that achieves a favorable trade-off between high-quality\nreconstruction and compression rate. We use Selftok to build a pure AR VLM for\nboth visual comprehension and generation tasks. Impressively, without using any\ntext-image training pairs, a simple policy gradient RL working in the visual\ntokens can significantly boost the visual generation benchmark, surpassing all\nthe existing models by a large margin. Therefore, we believe that Selftok\neffectively addresses the long-standing challenge that visual tokens cannot\nsupport effective RL. When combined with the well-established strengths of RL\nin LLMs, this brings us one step closer to realizing a truly multimodal LLM.\nProject Page: https://selftok-team.github.io/report/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06538", "pdf": "https://arxiv.org/pdf/2505.06538", "abs": "https://arxiv.org/abs/2505.06538", "authors": ["Xinyue Lou", "You Li", "Jinan Xu", "Xiangyu Shi", "Chi Chen", "Kaiyu Huang"], "title": "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "The rapid development of multimodal large reasoning models (MLRMs) has\ndemonstrated broad application potential, yet their safety and reliability\nremain critical concerns that require systematic exploration. To address this\ngap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs\nacross 5 benchmarks and unveil prevalent safety degradation phenomena in most\nadvanced models. Moreover, our analysis reveals distinct safety patterns across\ndifferent benchmarks: significant safety degradation is observed across\njailbreak robustness benchmarks, whereas safety-awareness benchmarks\ndemonstrate less pronounced degradation. In particular, a long thought process\nin some scenarios even enhances safety performance. Therefore, it is a\npotential approach to addressing safety issues in MLRMs by leveraging the\nintrinsic reasoning capabilities of the model to detect unsafe intent. To\noperationalize this insight, we construct a multimodal tuning dataset that\nincorporates a safety-oriented thought process. Experimental results from\nfine-tuning existing MLRMs with this dataset effectively enhances the safety on\nboth jailbreak robustness and safety-awareness benchmarks. This study provides\na new perspective for developing safe MLRMs. Our dataset is available at\nhttps://github.com/xinyuelou/Think-in-Safety.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety", "reliability"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06548", "pdf": "https://arxiv.org/pdf/2505.06548", "abs": "https://arxiv.org/abs/2505.06548", "authors": ["Aniruddha Roy", "Pretam Ray", "Abhilash Nandy", "Somak Aditya", "Pawan Goyal"], "title": "REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback", "categories": ["cs.CL"], "comment": "11 pages", "summary": "Instruction-based Large Language Models (LLMs) have proven effective in\nnumerous few-shot or zero-shot Natural Language Processing (NLP) tasks.\nHowever, creating human-annotated instruction data is time-consuming,\nexpensive, and often limited in quantity and task diversity. Previous research\nendeavors have attempted to address this challenge by proposing frameworks\ncapable of generating instructions in a semi-automated and task-agnostic manner\ndirectly from the model itself. Many of these efforts have relied on large\nAPI-only parameter-based models such as GPT-3.5 (175B), which are expensive,\nand subject to limits on a number of queries. This paper explores the\nperformance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,\nand Mistral 7B, using a semi-automated framework, thereby reducing human\nintervention, effort, and cost required to generate an instruction dataset for\nfine-tuning LLMs. Furthermore, we demonstrate that incorporating a\nReinforcement Learning (RL) based training algorithm into this LLMs-based\nframework leads to further enhancements. Our evaluation of the dataset reveals\nthat these RL-based frameworks achieve a substantial improvements in 63-66% of\nthe tasks compared to previous approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06552", "pdf": "https://arxiv.org/pdf/2505.06552", "abs": "https://arxiv.org/abs/2505.06552", "authors": ["Doyoung Kim", "Youngjun Lee", "Joeun Kim", "Jihwan Bang", "Hwanjun Song", "Susik Yoon", "Jae-Gil Lee"], "title": "References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Conversational query reformulation (CQR) has become indispensable for\nimproving retrieval in dialogue-based applications. However, existing\napproaches typically rely on reference passages for optimization, which are\nimpractical to acquire in real-world scenarios. To address this limitation, we\nintroduce a novel reference-free preference optimization framework DualReform\nthat generates pseudo reference passages from commonly-encountered\nconversational datasets containing only queries and responses. DualReform\nattains this goal through two key innovations: (1) response-based inference,\nwhere responses serve as proxies to infer pseudo reference passages, and (2)\nresponse refinement via the dual-role of CQR, where a CQR model refines\nresponses based on the shared objectives between response refinement and CQR.\nDespite not relying on reference passages, DualReform achieves 96.9--99.1% of\nthe retrieval accuracy attainable only with reference passages and surpasses\nthe state-of-the-art method by up to 31.6%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dialogue"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06512", "pdf": "https://arxiv.org/pdf/2505.06512", "abs": "https://arxiv.org/abs/2505.06512", "authors": ["Hang Wang", "Zhi-Qi Cheng", "Chenhao Lin", "Chao Shen", "Lei Zhang"], "title": "HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Text-to-image synthesis has progressed to the point where models can generate\nvisually compelling images from natural language prompts. Yet, existing methods\noften fail to reconcile high-level semantic fidelity with explicit spatial\ncontrol, particularly in scenes involving multiple objects, nuanced relations,\nor complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal\nAlignment (HCMA) framework for grounded text-to-image generation. HCMA\nintegrates two alignment modules into each diffusion sampling step: a global\nmodule that continuously aligns latent representations with textual\ndescriptions to ensure scene-level coherence, and a local module that employs\nbounding-box layouts to anchor objects at specified locations, enabling\nfine-grained spatial control. Extensive experiments on the MS-COCO 2014\nvalidation set show that HCMA surpasses state-of-the-art baselines, achieving a\n0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP\nScore. These results demonstrate HCMA's effectiveness in faithfully capturing\nintricate textual semantics while adhering to user-defined spatial constraints,\noffering a robust solution for semantically grounded image generation.Our code\nis available at https://github.com/hwang-cs-ime/HCMA", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06516", "pdf": "https://arxiv.org/pdf/2505.06516", "abs": "https://arxiv.org/abs/2505.06516", "authors": ["Yilin Dong", "Tianyun Zhu", "Xinde Li", "Jean Dezert", "Rigui Zhou", "Changming Zhu", "Lei Cao", "Shuzhi Sam Ge"], "title": "Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "16 pages, 28 figures", "summary": "Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to\nderive a quantum mass function (QMF) as a fuzzy metric type from information\nobtained from various data sources. In addition, QDST uses quantum parallel\ncomputing to speed up computation. Nevertheless, the effective management of\nconflicts between multiple QMFs in QDST is a challenging question. This work\naims to address this problem by proposing a Quantum Conflict Indicator (QCI)\nthat measures the conflict between two QMFs in decision-making. Then, the\nproperties of the QCI are carefully investigated. The obtained results validate\nits compliance with desirable conflict measurement properties such as\nnon-negativity, symmetry, boundedness, extreme consistency and insensitivity to\nrefinement. We then apply the proposed QCI in conflict fusion methods and\ncompare its performance with several commonly used fusion approaches. This\ncomparison demonstrates the superiority of the QCI-based conflict fusion\nmethod. Moreover, the Class Description Domain Space (C-DDS) and its optimized\nversion, C-DDS+ by utilizing the QCI-based fusion method, are proposed to\naddress the Out-of-Distribution (OOD) detection task. The experimental results\nshow that the proposed approach gives better OOD performance with respect to\nseveral state-of-the-art baseline OOD detection methods. Specifically, it\nachieves an average increase in Area Under the Receiver Operating\nCharacteristic Curve (AUC) of 1.2% and a corresponding average decrease in\nFalse Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the\noptimal baseline method.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06527", "pdf": "https://arxiv.org/pdf/2505.06527", "abs": "https://arxiv.org/abs/2505.06527", "authors": ["Jing Hu", "Kaiwei Yu", "Hongjiang Xian", "Shu Hu", "Xin Wang"], "title": "Improving Generalization of Medical Image Registration Foundation Model", "categories": ["cs.CV", "cs.AI"], "comment": "IJCNN", "summary": "Deformable registration is a fundamental task in medical image processing,\naiming to achieve precise alignment by establishing nonlinear correspondences\nbetween images. Traditional methods offer good adaptability and\ninterpretability but are limited by computational efficiency. Although deep\nlearning approaches have significantly improved registration speed and\naccuracy, they often lack flexibility and generalizability across different\ndatasets and tasks. In recent years, foundation models have emerged as a\npromising direction, leveraging large and diverse datasets to learn universal\nfeatures and transformation patterns for image registration, thus demonstrating\nstrong cross-task transferability. However, these models still face challenges\nin generalization and robustness when encountering novel anatomical structures,\nvarying imaging conditions, or unseen modalities. To address these limitations,\nthis paper incorporates Sharpness-Aware Minimization (SAM) into foundation\nmodels to enhance their generalization and robustness in medical image\nregistration. By optimizing the flatness of the loss landscape, SAM improves\nmodel stability across diverse data distributions and strengthens its ability\nto handle complex clinical scenarios. Experimental results show that foundation\nmodels integrated with SAM achieve significant improvements in cross-dataset\nregistration performance, offering new insights for the advancement of medical\nimage registration technology. Our code is available at\nhttps://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06536", "pdf": "https://arxiv.org/pdf/2505.06536", "abs": "https://arxiv.org/abs/2505.06536", "authors": ["Feng Liu", "Ziwang Fu", "Yunlong Wang", "Qijian Zheng"], "title": "TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2111.02172", "summary": "The fusion technique is the key to the multimodal emotion recognition task.\nRecently, cross-modal attention-based fusion methods have demonstrated high\nperformance and strong robustness. However, cross-modal attention suffers from\nredundant features and does not capture complementary features well. We find\nthat it is not necessary to use the entire information of one modality to\nreinforce the other during cross-modal interaction, and the features that can\nreinforce a modality may contain only a part of it. To this end, we design an\ninnovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN).\nSpecifically, for the redundant features, we make one modality perform\nintra-modal feature selection through a self-attention mechanism, so that the\nselected features can adaptively and efficiently interact with another\nmodality. To better capture the complementary information between the\nmodalities, we obtain the fused weight vector by splicing and use the weight\nvector to achieve feature reinforcement of the modalities. We apply TCAFN to\nthe RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal\nrepresentations to validate the effectiveness of the proposed fusion method.\nThe experimental results show that TACFN brings a significant performance\nimprovement compared to other methods and reaches the state-of-the-art. All\ncode and models could be accessed from https://github.com/shuzihuaiyu/TACFN.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06566", "pdf": "https://arxiv.org/pdf/2505.06566", "abs": "https://arxiv.org/abs/2505.06566", "authors": ["Zequn Xie", "Haoming Ji", "Lingwei Meng"], "title": "Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image person search aims to identify an individual based on a text\ndescription. To reduce data collection costs, large-scale text-image datasets\nare created from co-occurrence pairs found online. However, this can introduce\nnoise, particularly mismatched pairs, which degrade retrieval performance.\nExisting methods often focus on negative samples, amplifying this noise. To\naddress these issues, we propose the Dynamic Uncertainty and Relational\nAlignment (DURA) framework, which includes the Key Feature Selector (KFS) and a\nnew loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and\nmodels noise uncertainty, improving retrieval reliability. The bidirectional\nevidence from cross-modal similarity is modeled as a Dirichlet distribution,\nenhancing adaptability to noisy data. DSH adjusts the difficulty of negative\nsamples to improve robustness in noisy environments. Our experiments on three\ndatasets show that the method offers strong noise resistance and improves\nretrieval performance in both low- and high-noise scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06708", "pdf": "https://arxiv.org/pdf/2505.06708", "abs": "https://arxiv.org/abs/2505.06708", "authors": ["Zihan Qiu", "Zekun Wang", "Bo Zheng", "Zeyu Huang", "Kaiyue Wen", "Songlin Yang", "Rui Men", "Le Yu", "Fei Huang", "Suozhi Huang", "Dayiheng Liu", "Jingren Zhou", "Junyang Lin"], "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free", "categories": ["cs.CL"], "comment": null, "summary": "Gating mechanisms have been widely utilized, from early models like LSTMs and\nHighway Networks to recent state space models, linear attention, and also\nsoftmax attention. Yet, existing literature rarely examines the specific\neffects of gating. In this work, we conduct comprehensive experiments to\nsystematically investigate gating-augmented softmax attention variants.\nSpecifically, we perform a comprehensive comparison over 30 variants of 15B\nMixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion\ntoken dataset. Our central finding is that a simple modification-applying a\nhead-specific sigmoid gate after the Scaled Dot-Product Attention\n(SDPA)-consistently improves performance. This modification also enhances\ntraining stability, tolerates larger learning rates, and improves scaling\nproperties. By comparing various gating positions and computational variants,\nwe attribute this effectiveness to two key factors: (1) introducing\nnon-linearity upon the low-rank mapping in the softmax attention, and (2)\napplying query-dependent sparse gating scores to modulate the SDPA output.\nNotably, we find this sparse gating mechanism mitigates 'attention sink' and\nenhances long-context extrapolation performance, and we also release related\n$\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and\n$\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate\nfuture research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07233", "pdf": "https://arxiv.org/pdf/2505.07233", "abs": "https://arxiv.org/abs/2505.07233", "authors": ["Jiashuo Sun", "Xianrui Zhong", "Sizhe Zhou", "Jiawei Han"], "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 6 figures, 15 tables", "summary": "Retrieval-augmented generation (RAG) systems combine large language models\n(LLMs) with external knowledge retrieval, making them highly effective for\nknowledge-intensive tasks. A crucial but often under-explored component of\nthese systems is the reranker, which refines retrieved documents to enhance\ngeneration quality and explainability. The challenge of selecting the optimal\nnumber of documents (k) remains unsolved: too few may omit critical\ninformation, while too many introduce noise and inefficiencies. Although recent\nstudies have explored LLM-based rerankers, they primarily leverage internal\nmodel knowledge and overlook the rich supervisory signals that LLMs can\nprovide, such as using response quality as feedback for optimizing reranking\ndecisions. In this paper, we propose DynamicRAG, a novel RAG framework where\nthe reranker dynamically adjusts both the order and number of retrieved\ndocuments based on the query. We model the reranker as an agent optimized\nthrough reinforcement learning (RL), using rewards derived from LLM output\nquality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates\nsuperior performance, achieving state-of-the-art results. The model, data and\ncode are available at https://github.com/GasolSun36/DynamicRAG", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06796", "pdf": "https://arxiv.org/pdf/2505.06796", "abs": "https://arxiv.org/abs/2505.06796", "authors": ["Ye Zhu", "Yunan Wang", "Zitong Yu"], "title": "Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Multimodal news contains a wealth of information and is easily affected by\ndeepfake modeling attacks. To combat the latest image and text generation\nmethods, we present a new Multimodal Fake News Detection dataset (MFND)\ncontaining 11 manipulated types, designed to detect and localize highly\nauthentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning\n(SDML) model for fake news, which fully uses unimodal and mutual modal features\nto mine the intrinsic semantics of news. Under shallow inference, we propose\nthe momentum distillation-based light punishment contrastive learning for\nfine-grained uniform spatial image and text semantic alignment, and an adaptive\ncross-modal fusion module to enhance mutual modal features. Under deep\ninference, we design a two-branch framework to augment the image and text\nunimodal features, respectively merging with mutual modalities features, for\nfour predictions via dedicated detection and localization projections.\nExperiments on both mainstream and our proposed datasets demonstrate the\nsuperiority of the model. Codes and dataset are released at\nhttps://github.com/yunan-wang33/sdml.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07313", "pdf": "https://arxiv.org/pdf/2505.07313", "abs": "https://arxiv.org/abs/2505.07313", "authors": ["Baixuan Xu", "Chunyang Li", "Weiqi Wang", "Wei Fan", "Tianshi Zheng", "Haochen Shi", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "Designing effective collaboration structure for multi-agent LLM systems to\nenhance collective reasoning is crucial yet remains under-explored. In this\npaper, we systematically investigate how collaborative reasoning performance is\naffected by three key design dimensions: (1) Expertise-Domain Alignment, (2)\nCollaboration Paradigm (structured workflow vs. diversity-driven integration),\nand (3) System Scale. Our findings reveal that expertise alignment benefits are\nhighly domain-contingent, proving most effective for contextual reasoning\ntasks. Furthermore, collaboration focused on integrating diverse knowledge\nconsistently outperforms rigid task decomposition. Finally, we empirically\nexplore the impact of scaling the multi-agent system with expertise\nspecialization and study the computational trade off, highlighting the need for\nmore efficient communication protocol design. This work provides concrete\nguidelines for configuring specialized multi-agent system and identifies\ncritical architectural trade-offs and bottlenecks for scalable multi-agent\nreasoning. The code will be made available upon acceptance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06886", "pdf": "https://arxiv.org/pdf/2505.06886", "abs": "https://arxiv.org/abs/2505.06886", "authors": ["Ahmed Qazi", "Hamd Jalil", "Asim Iqbal"], "title": "Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "comment": "12 pages, 8 figures, 1 table", "summary": "The mouse is one of the most studied animal models in the field of systems\nneuroscience. Understanding the generalized patterns and decoding the neural\nrepresentations that are evoked by the diverse range of natural scene stimuli\nin the mouse visual cortex is one of the key quests in computational vision. In\nrecent years, significant parallels have been drawn between the primate visual\ncortex and hierarchical deep neural networks. However, their generalized\nefficacy in understanding mouse vision has been limited. In this study, we\ninvestigate the functional alignment between the mouse visual cortex and deep\nlearning models for object classification tasks. We first introduce a\ngeneralized representational learning strategy that uncovers a striking\nresemblance between the functional mapping of the mouse visual cortex and\nhigh-performing deep learning models on both top-down (population-level) and\nbottom-up (single cell-level) scenarios. Next, this representational similarity\nacross the two systems is further enhanced by the addition of Neural Response\nNormalization (NeuRN) layer, inspired by the activation profile of excitatory\nand inhibitory neurons in the visual cortex. To test the performance effect of\nNeuRN on real-world tasks, we integrate it into deep learning models and\nobserve significant improvements in their robustness against data shifts in\ndomain generalization tasks. Our work proposes a novel framework for comparing\nthe functional architecture of the mouse visual cortex with deep learning\nmodels. Our findings carry broad implications for the development of advanced\nAI models that draw inspiration from the mouse visual cortex, suggesting that\nthese models serve as valuable tools for studying the neural representations of\nthe mouse visual cortex and, as a result, enhancing their performance on\nreal-world tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "question answering"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06903", "pdf": "https://arxiv.org/pdf/2505.06903", "abs": "https://arxiv.org/abs/2505.06903", "authors": ["Yuanzhuo Wang", "Junwen Duan", "Xinyu Li", "Jianxin Wang"], "title": "CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal medical image analysis is essential for clinical decision-making,\nyet existing methods either align images and text at a coarse level - causing\npotential semantic mismatches - or depend solely on visual information, lacking\nmedical semantic integration. We present CheXLearner, the first end-to-end\nframework that unifies anatomical region detection, Riemannian manifold-based\nstructure alignment, and fine-grained regional semantic guidance. Our proposed\nMed-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to\nrobustly align anatomical structures and capture pathologically meaningful\ndiscrepancies across temporal chest X-rays. By introducing regional progression\ndescriptions as supervision, CheXLearner achieves enhanced cross-modal\nrepresentation learning and supports dynamic low-level feature optimization.\nExperiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and\n80.32% (+11.05%) F1-score on anatomical region progression detection -\nsubstantially outperforming state-of-the-art baselines, especially in\nstructurally complex regions. Additionally, our model attains a 91.52% average\nAUC score in downstream disease classification, validating its superior feature\nrepresentation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07591", "pdf": "https://arxiv.org/pdf/2505.07591", "abs": "https://arxiv.org/abs/2505.07591", "authors": ["Junjie Ye", "Caishuang Huang", "Zhuohan Chen", "Wenjie Fu", "Chenyuan Yang", "Leyi Yang", "Yilong Wu", "Peng Wang", "Meng Zhou", "Xiaolong Yang", "Tao Gui", "Qi Zhang", "Zhongchao Shi", "Jianping Fan", "Xuanjing Huang"], "title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction following evaluates large language models (LLMs) on their ability\nto generate outputs that adhere to user-defined constraints. However, existing\nbenchmarks often rely on templated constraint prompts, which lack the diversity\nof real-world usage and limit fine-grained performance assessment. To fill this\ngap, we propose a multi-dimensional constraint framework encompassing three\nconstraint patterns, four constraint categories, and four difficulty levels.\nBuilding on this framework, we develop an automated instruction generation\npipeline that performs constraint expansion, conflict detection, and\ninstruction rewriting, yielding 1,200 code-verifiable instruction-following\ntest samples. We evaluate 19 LLMs across seven model families and uncover\nsubstantial variation in performance across constraint forms. For instance,\naverage performance drops from 77.67% at Level I to 32.96% at Level IV.\nFurthermore, we demonstrate the utility of our approach by using it to generate\ndata for reinforcement learning, achieving substantial gains in instruction\nfollowing without degrading general performance. In-depth analysis indicates\nthat these gains stem primarily from modifications in the model's attention\nmodules parameters, which enhance constraint recognition and adherence. Code\nand data are available in https://github.com/Junjie-Ye/MulDimIF.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07596", "pdf": "https://arxiv.org/pdf/2505.07596", "abs": "https://arxiv.org/abs/2505.07596", "authors": ["Ziyang Huang", "Xiaowei Yuan", "Yiming Ju", "Jun Zhao", "Kang Liu"], "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06920", "pdf": "https://arxiv.org/pdf/2505.06920", "abs": "https://arxiv.org/abs/2505.06920", "authors": ["Timing Li", "Bing Cao", "Pengfei Zhu", "Bin Xiao", "Qinghua Hu"], "title": "Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Acquiring accurately aligned multi-modal image pairs is fundamental for\nachieving high-quality multi-modal image fusion. To address the lack of ground\ntruth in current multi-modal image registration and fusion methods, we propose\na novel self-supervised \\textbf{B}i-directional\n\\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically,\nB-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator\n(IPDG) to achieve self-supervised global-local registration. Visible-infrared\nimage pairs with spatially misaligned differences are aligned to obtain global\ndifferences through the registration module. The same image pairs are processed\nby PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain\nlocal differences. IPDG converts the obtained local differences into\npseudo-global differences, which are used to perform global-local difference\nconsistency with the global differences. Furthermore, aiming at eliminating the\neffect of modal gaps on the registration module, we design a neighborhood\ndynamic alignment loss to achieve cross-modal image edge alignment. Extensive\nexperiments on misaligned multi-modal images demonstrate the effectiveness of\nthe proposed method in multi-modal image alignment and fusion against the\ncompeting methods. Our code will be publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07608", "pdf": "https://arxiv.org/pdf/2505.07608", "abs": "https://arxiv.org/abs/2505.07608", "authors": ["Xiaomi LLM-Core Team", ":", "Bingquan Xia", "Bowen Shen", "Cici", "Dawei Zhu", "Di Zhang", "Gang Wang", "Hailin Zhang", "Huaqiu Liu", "Jiebao Xiao", "Jinhao Dong", "Liang Zhao", "Peidian Li", "Peng Wang", "Shihua Yu", "Shimao Chen", "Weikun Wang", "Wenhan Ma", "Xiangwei Deng", "Yi Huang", "Yifan Song", "Zihan Jiang", "Bowen Ye", "Can Cai", "Chenhong He", "Dong Zhang", "Duo Zhang", "Guoan Wang", "Hao Tian", "Haochen Zhao", "Heng Qu", "Hongshen Xu", "Jun Shi", "Kainan Bao", "QingKai Fang", "Kang Zhou", "Kangyang Zhou", "Lei Li", "Menghang Zhu", "Nuo Chen", "Qiantong Wang", "Shaohui Liu", "Shicheng Li", "Shuhao Gu", "Shuhuai Ren", "Shuo Liu", "Sirui Deng", "Weiji Zhuang", "Weiwei Lv", "Wenyu Yang", "Xin Zhang", "Xing Yong", "Xing Zhang", "Xingchen Song", "Xinzhe Xu", "Xu Wang", "Yihan Yan", "Yu Tu", "Yuanyuan Tian", "Yudong Wang", "Yue Yu", "Zhenru Lin", "Zhichao Song", "Zihao Yue"], "title": "MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present MiMo-7B, a large language model born for reasoning tasks, with\noptimization across both pre-training and post-training stages. During\npre-training, we enhance the data preprocessing pipeline and employ a\nthree-stage data mixing strategy to strengthen the base model's reasoning\npotential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional\nMulti-Token Prediction objective for enhanced performance and accelerated\ninference speed. During post-training, we curate a dataset of 130K verifiable\nmathematics and programming problems for reinforcement learning, integrating a\ntest-difficulty-driven code-reward scheme to alleviate sparse-reward issues and\nemploying strategic data resampling to stabilize training. Extensive\nevaluations show that MiMo-7B-Base possesses exceptional reasoning potential,\noutperforming even much larger 32B models. The final RL-tuned model,\nMiMo-7B-RL, achieves superior performance on mathematics, code and general\nreasoning tasks, surpassing the performance of OpenAI o1-mini. The model\ncheckpoints are available at https://github.com/xiaomimimo/MiMo.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07610", "pdf": "https://arxiv.org/pdf/2505.07610", "abs": "https://arxiv.org/abs/2505.07610", "authors": ["Kenza Amara", "Rita Sevastjanova", "Mennatallah El-Assady"], "title": "Concept-Level Explainability for Auditing & Steering LLM Responses", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 7 figures, Submission to Neurips 2025", "summary": "As large language models (LLMs) become widely deployed, concerns about their\nsafety and alignment grow. An approach to steer LLM behavior, such as\nmitigating biases or defending against jailbreaks, is to identify which parts\nof a prompt influence specific aspects of the model's output. Token-level\nattribution methods offer a promising solution, but still struggle in text\ngeneration, explaining the presence of each token in the output separately,\nrather than the underlying semantics of the entire LLM response. We introduce\nConceptX, a model-agnostic, concept-level explainability method that identifies\nthe concepts, i.e., semantically rich tokens in the prompt, and assigns them\nimportance based on the outputs' semantic similarity. Unlike current\ntoken-level methods, ConceptX also offers to preserve context integrity through\nin-place token replacements and supports flexible explanation goals, e.g.,\ngender bias. ConceptX enables both auditing, by uncovering sources of bias, and\nsteering, by modifying prompts to shift the sentiment or reduce the harmfulness\nof LLM responses, without requiring retraining. Across three LLMs, ConceptX\noutperforms token-level methods like TokenSHAP in both faithfulness and human\nalignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for\nrandom edits and lower attack success rates from 0.463 to 0.242, outperforming\nattribution and paraphrasing baselines. While prompt engineering and\nself-explaining methods sometimes yield safer responses, ConceptX offers a\ntransparent and faithful alternative for improving LLM safety and alignment,\ndemonstrating the practical value of attribution-based explainability in\nguiding LLM behavior.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07050", "pdf": "https://arxiv.org/pdf/2505.07050", "abs": "https://arxiv.org/abs/2505.07050", "authors": ["Binbin Wei", "Yuhang Zhang", "Shishun Tian", "Muxin Liao", "Wei Li", "Wenbin Zou"], "title": "Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) aims to align source and target domain\ndistributions to close the domain gap, but still struggles with obtaining the\ntarget data. Fortunately, Domain Generalization (DG) excels without the need\nfor any target data. Recent works expose that depth maps contribute to improved\ngeneralized performance in the UDA tasks, but they ignore the noise and holes\nin depth maps due to device and environmental factors, failing to sufficiently\nand effectively learn domain-invariant representation. Although\nhigh-sensitivity region suppression has shown promising results in learning\ndomain-invariant features, existing methods cannot be directly applicable to\ndepth maps due to their unique characteristics. Hence, we propose a novel\nframework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal\nstylization flow (DSSS), focusing on learning domain-invariant features from\ndepth maps for the DG semantic segmentation. Specifically, we propose the RGB-D\ninter-modal stylization flow to generate stylized depth maps for sensitivity\ndetection, cleverly utilizing RGB information as the stylization source. Then,\na class-wise soft spatial sensitivity suppression is designed to identify and\nemphasize non-sensitive depth features that contain more domain-invariant\ninformation. Furthermore, an RGB-D soft alignment loss is proposed to ensure\nthat the stylized depth maps only align part of the RGB features while still\nretaining the unique depth information. To our best knowledge, our DSSS\nframework is the first work to integrate RGB and Depth information in the\nmulti-class DG semantic segmentation task. Extensive experiments over multiple\nbackbone networks show that our framework achieves remarkable performance\nimprovement.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07057", "pdf": "https://arxiv.org/pdf/2505.07057", "abs": "https://arxiv.org/abs/2505.07057", "authors": ["Junhao Xia", "Chaoyang Zhang", "Yecheng Zhang", "Chengyang Zhou", "Zhichang Wang", "Bochun Liu", "Dongshuo Yin"], "title": "DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video generation based on diffusion models presents a challenging multimodal\ntask, with video editing emerging as a pivotal direction in this field. Recent\nvideo editing approaches primarily fall into two categories: training-required\nand training-free methods. While training-based methods incur high\ncomputational costs, training-free alternatives often yield suboptimal\nperformance. To address these limitations, we propose DAPE, a high-quality yet\ncost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for\nvideo editing. In the first stage, we design an efficient norm-tuning method to\nenhance temporal consistency in generated videos. The second stage introduces a\nvision-friendly adapter to improve visual quality. Additionally, we identify\ncritical shortcomings in existing benchmarks, including limited category\ndiversity, imbalanced object distribution, and inconsistent frame counts. To\nmitigate these issues, we curate a large dataset benchmark comprising 232\nvideos with rich annotations and 6 editing prompts, enabling objective and\ncomprehensive evaluation of advanced methods. Extensive experiments on existing\ndatasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate\nthat DAPE significantly improves temporal coherence and text-video alignment\nwhile outperforming previous state-of-the-art approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06843", "pdf": "https://arxiv.org/pdf/2505.06843", "abs": "https://arxiv.org/abs/2505.06843", "authors": ["Zihan Guan", "Mengxuan Hu", "Ronghang Zhu", "Sheng Li", "Anil Vullikanti"], "title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety", "categories": ["cs.LG", "cs.CL"], "comment": "26 pages, 13 figures", "summary": "Recent studies have uncovered a troubling vulnerability in the fine-tuning\nstage of large language models (LLMs): even fine-tuning on entirely benign\ndatasets can lead to a significant increase in the harmfulness of LLM outputs.\nBuilding on this finding, our red teaming study takes this threat one step\nfurther by developing a more effective attack. Specifically, we analyze and\nidentify samples within benign datasets that contribute most to safety\ndegradation, then fine-tune LLMs exclusively on these samples. We approach this\nproblem from an outlier detection perspective and propose Self-Inf-N, to detect\nand extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs\non 100 outlier samples selected by Self-Inf-N in the benign datasets severely\ncompromises LLM safety alignment. Extensive experiments across seven mainstream\nLLMs demonstrate that our attack exhibits high transferability across different\narchitectures and remains effective in practical scenarios. Alarmingly, our\nresults indicate that most existing mitigation strategies fail to defend\nagainst this attack, underscoring the urgent need for more robust alignment\nsafeguards. Codes are available at\nhttps://github.com/GuanZihan/Benign-Samples-Matter.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06898", "pdf": "https://arxiv.org/pdf/2505.06898", "abs": "https://arxiv.org/abs/2505.06898", "authors": ["Honglong Yang", "Shanshan Song", "Yi Qin", "Lehan Wang", "Haonan Wang", "Xinpeng Ding", "Qixiang Zhang", "Bodong Du", "Xiaomeng Li"], "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generalist Medical AI (GMAI) systems have demonstrated expert-level\nperformance in biomedical perception tasks, yet their clinical utility remains\nlimited by inadequate multi-modal explainability and suboptimal prognostic\ncapabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI\nassistant that integrates textual and visual interpretability to support\ntransparent and trustworthy medical decision-making. XMedGPT not only produces\naccurate diagnostic and descriptive outputs, but also grounds referenced\nanatomical sites within medical images, bridging critical gaps in\ninterpretability and enhancing clinician usability. To support real-world\ndeployment, we introduce a reliability indexing mechanism that quantifies\nuncertainty through consistency-based assessment via interactive\nquestion-answering. We validate XMedGPT across four pillars: multi-modal\ninterpretability, uncertainty quantification, and prognostic modeling, and\nrigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical\nregions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between\nvisual rationales and clinical outcomes. For uncertainty estimation, it attains\nan AUC of 0.862 on visual question answering and 0.764 on radiology report\ngeneration. In survival and recurrence prediction for lung and glioma cancers,\nit surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%.\nRigorous benchmarking across 347 datasets covers 40 imaging modalities and\nexternal validation spans 4 anatomical systems confirming exceptional\ngeneralizability, with performance gains surpassing existing GMAI by 20.7% for\nin-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together,\nXMedGPT represents a significant leap forward in clinician-centric AI\nintegration, offering trustworthy and scalable support for diverse healthcare\napplications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "question answering"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07172", "pdf": "https://arxiv.org/pdf/2505.07172", "abs": "https://arxiv.org/abs/2505.07172", "authors": ["Zexian Yang", "Dian Li", "Dayan Wu", "Gang Liu", "Weiping Wang"], "title": "Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in multimodal reasoning tasks, existing\nLarge Vision-Language Models (LVLMs) are prone to producing visually ungrounded\nresponses when interpreting associated images. In contrast, when humans embark\non learning new knowledge, they often rely on a set of fundamental pre-study\nprinciples: reviewing outlines to grasp core concepts, summarizing key points\nto guide their focus and enhance understanding. However, such preparatory\nactions are notably absent in the current instruction tuning processes. This\npaper presents Re-Critic, an easily scalable rationale-augmented framework\ndesigned to incorporate fundamental rules and chain-of-thought (CoT) as a\nbridge to enhance reasoning abilities. Specifically, Re-Critic develops a\nvisual rationale synthesizer that scalably augments raw instructions with\nrationale explanation. To probe more contextually grounded responses, Re-Critic\nemploys an in-context self-critic mechanism to select response pairs for\npreference tuning. Experiments demonstrate that models fine-tuned with our\nrationale-augmented dataset yield gains that extend beyond\nhallucination-specific tasks to broader multimodal reasoning tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07198", "pdf": "https://arxiv.org/pdf/2505.07198", "abs": "https://arxiv.org/abs/2505.07198", "authors": ["Xufei Wang", "Gengxuan Tian", "Junqiao Zhao", "Siyue Tao", "Qiwen Gu", "Qiankun Yu", "Tiantian Feng"], "title": "Ranking-aware Continual Learning for LiDAR Place Recognition", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Place recognition plays a significant role in SLAM, robot navigation, and\nautonomous driving applications. Benefiting from deep learning, the performance\nof LiDAR place recognition (LPR) has been greatly improved. However, many\nexisting learning-based LPR methods suffer from catastrophic forgetting, which\nseverely harms the performance of LPR on previously trained places after\ntraining on a new environment. In this paper, we introduce a continual learning\nframework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate\nforgetting. Inspired by the ranking process of place recognition retrieval, we\npresent a ranking-aware knowledge distillation loss that encourages the network\nto preserve the high-level place recognition knowledge. We also introduce a\nknowledge fusion module to integrate the knowledge of old and new models for\nLiDAR place recognition. Our extensive experiments demonstrate that KDF can be\napplied to different networks to overcome catastrophic forgetting, surpassing\nthe state-of-the-art methods in terms of mean Recall@1 and forgetting score.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07209", "pdf": "https://arxiv.org/pdf/2505.07209", "abs": "https://arxiv.org/abs/2505.07209", "authors": ["Yan Xie", "Zequn Zeng", "Hao Zhang", "Yucheng Ding", "Yi Wang", "Zhengjue Wang", "Bo Chen", "Hongwei Liu"], "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Concept Bottleneck Models (CBMs) try to make the decision-making process\ntransparent by exploring an intermediate concept space between the input image\nand the output prediction. Existing CBMs just learn coarse-grained relations\nbetween the whole image and the concepts, less considering local image\ninformation, leading to two main drawbacks: i) they often produce spurious\nvisual-concept relations, hence decreasing model reliability; and ii) though\nCBMs could explain the importance of every concept to the final prediction, it\nis still challenging to tell which visual region produces the prediction. To\nsolve these problems, this paper proposes a Disentangled Optimal Transport CBM\n(DOT-CBM) framework to explore fine-grained visual-concept relations between\nlocal image patches and concepts. Specifically, we model the concept prediction\nprocess as a transportation problem between the patches and concepts, thereby\nachieving explicit fine-grained feature alignment. We also incorporate\northogonal projection losses within the modality to enhance local feature\ndisentanglement. To further address the shortcut issues caused by statistical\nbiases in the data, we utilize the visual saliency map and concept label\nstatistics as transportation priors. Thus, DOT-CBM can visualize inversion\nheatmaps, provide more reliable concept predictions, and produce more accurate\nclass predictions. Comprehensive experiments demonstrate that our proposed\nDOT-CBM achieves SOTA performance on several tasks, including image\nclassification, local part detection and out-of-distribution generalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07167", "pdf": "https://arxiv.org/pdf/2505.07167", "abs": "https://arxiv.org/abs/2505.07167", "authors": ["Haoran Gu", "Handing Wang", "Yi Mei", "Mengjie Zhang", "Yaochu Jin"], "title": "One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have been extensively used across diverse\ndomains, including virtual assistants, automated code generation, and\nscientific research. However, they remain vulnerable to jailbreak attacks,\nwhich manipulate the models into generating harmful responses despite safety\nalignment. Recent studies have shown that current safety-aligned LLMs often\nundergo the shallow safety alignment, where the first few tokens largely\ndetermine whether the response will be harmful. Through comprehensive\nobservations, we find that safety-aligned LLMs and various defense strategies\ngenerate highly similar initial tokens in their refusal responses, which we\ndefine as safety trigger tokens. Building on this insight, we propose\n\\texttt{D-STT}, a simple yet effective defense algorithm that identifies and\nexplicitly decodes safety trigger tokens of the given safety-aligned LLM to\ntrigger the model's learned safety patterns. In this process, the safety\ntrigger is constrained to a single token, which effectively preserves model\nusability by introducing minimum intervention in the decoding process.\nExtensive experiments across diverse jailbreak attacks and benign prompts\ndemonstrate that \\ours significantly reduces output harmfulness while\npreserving model usability and incurring negligible response time overhead,\noutperforming ten baseline methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "code generation"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07300", "pdf": "https://arxiv.org/pdf/2505.07300", "abs": "https://arxiv.org/abs/2505.07300", "authors": ["Sofia Casarin", "Sergio Escalera", "Oswald Lanz"], "title": "L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers", "categories": ["cs.CV"], "comment": "accepted at CVPR 2025", "summary": "Training-free Neural Architecture Search (NAS) efficiently identifies\nhigh-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot\nand one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the\nneed for model training, and (ii) interpretable, with proxy designs often\ntheoretically grounded. Despite rapid developments in the field, current SOTA\nZC proxies are typically constrained to well-established convolutional search\nspaces. With the rise of Large Language Models shaping the future of deep\nlearning, this work extends ZC proxy applicability to Vision Transformers\n(ViTs). We present a new benchmark using the Autoformer search space evaluated\non 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients\ninformation (L-SWAG), a novel, generalizable metric that characterizes both\nconvolutional and transformer architectures across 14 tasks. Additionally,\nprevious works highlighted how different proxies contain complementary\ninformation, motivating the need for a ML model to identify useful\ncombinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low\nInformation gain and Bias Re-Alignment), a method that strategically combines\nproxies to best represent a specific benchmark. Integrated into the NAS search,\nLIBRA-NAS outperforms evolution and gradient-based NAS techniques by\nidentifying an architecture with a 17.0% test error on ImageNet1k in just 0.1\nGPU days.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07768", "pdf": "https://arxiv.org/pdf/2505.07768", "abs": "https://arxiv.org/abs/2505.07768", "authors": ["Yifeng Di", "Tianyi Zhang"], "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Accepted to ICSE 2025", "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["code generation"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07373", "pdf": "https://arxiv.org/pdf/2505.07373", "abs": "https://arxiv.org/abs/2505.07373", "authors": ["Lintao Xiang", "Hongpei Zheng", "Bailin Deng", "Hujun Yin"], "title": "Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Neural implicit surface reconstruction using volume rendering techniques has\nrecently achieved significant advancements in creating high-fidelity surfaces\nfrom multiple 2D images. However, current methods primarily target scenes with\nconsistent illumination and struggle to accurately reconstruct 3D geometry in\nuncontrolled environments with transient occlusions or varying appearances.\nWhile some neural radiance field (NeRF)-based variants can better manage\nphotometric variations and transient objects in complex scenes, they are\ndesigned for novel view synthesis rather than precise surface reconstruction\ndue to limited surface constraints. To overcome this limitation, we introduce a\nnovel approach that applies multiple geometric constraints to the implicit\nsurface optimization process, enabling more accurate reconstructions from\nunconstrained image collections. First, we utilize sparse 3D points from\nstructure-from-motion (SfM) to refine the signed distance function estimation\nfor the reconstructed surface, with a displacement compensation to accommodate\nnoise in the sparse points. Additionally, we employ robust normal priors\nderived from a normal predictor, enhanced by edge prior filtering and\nmulti-view consistency constraints, to improve alignment with the actual\nsurface geometry. Extensive testing on the Heritage-Recon benchmark and other\ndatasets has shown that the proposed method can accurately reconstruct surfaces\nfrom in-the-wild images, yielding geometries with superior accuracy and\ngranularity compared to existing techniques. Our approach enables high-quality\n3D reconstruction of various landmarks, making it applicable to diverse\nscenarios such as digital preservation of cultural heritage sites.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07530", "pdf": "https://arxiv.org/pdf/2505.07530", "abs": "https://arxiv.org/abs/2505.07530", "authors": ["Raul Ismayilov", "Luuk Spreeuwers", "Dzemila Sero"], "title": "FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic face datasets are increasingly used to overcome the limitations of\nreal-world biometric data, including privacy concerns, demographic imbalance,\nand high collection costs. However, many existing methods lack fine-grained\ncontrol over identity attributes and fail to produce paired,\nidentity-consistent images under structured capture conditions. We introduce\nFLUXSynID, a framework for generating high-resolution synthetic face datasets\nwith user-defined identity attribute distributions and paired document-style\nand trusted live capture images. The dataset generated using the FLUXSynID\nframework shows improved alignment with real-world identity distributions and\ngreater inter-set diversity compared to prior work. The FLUXSynID framework for\ngenerating custom datasets, along with a dataset of 14,889 synthetic\nidentities, is publicly released to support biometric research, including face\nrecognition and morphing attack detection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07622", "pdf": "https://arxiv.org/pdf/2505.07622", "abs": "https://arxiv.org/abs/2505.07622", "authors": ["Zhuo Song", "Ye Zhang", "Kunhong Li", "Longguang Wang", "Yulan Guo"], "title": "A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization is a promising solution for large-scale\nlocalization problems, requiring the sequential execution of retrieval and\nmetric localization tasks to achieve fine-grained predictions. However,\nexisting methods typically focus on designing standalone models for these two\ntasks, resulting in inefficient collaboration and increased training overhead.\nIn this paper, we propose UnifyGeo, a novel unified hierarchical\ngeo-localization framework that integrates retrieval and metric localization\ntasks into a single network. Specifically, we first employ a unified learning\nstrategy with shared parameters to jointly learn multi-granularity\nrepresentation, facilitating mutual reinforcement between these two tasks.\nSubsequently, we design a re-ranking mechanism guided by a dedicated loss\nfunction, which enhances geo-localization performance by improving both\nretrieval accuracy and metric localization references. Extensive experiments\ndemonstrate that UnifyGeo significantly outperforms the state-of-the-arts in\nboth task-isolated and task-associated settings. Remarkably, on the challenging\nVIGOR benchmark, which supports fine-grained localization evaluation, the\n1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from\n0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively.\nCode will be made publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07689", "pdf": "https://arxiv.org/pdf/2505.07689", "abs": "https://arxiv.org/abs/2505.07689", "authors": ["Quang Vinh Nguyen", "Minh Duc Nguyen", "Thanh Hoang Son Vo", "Hyung-Jeong Yang", "Soo-Hyung Kim"], "title": "Anatomical Attention Alignment representation for Radiology Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06595", "pdf": "https://arxiv.org/pdf/2505.06595", "abs": "https://arxiv.org/abs/2505.06595", "authors": ["Hai-Vy Nguyen", "Fabrice Gamboa", "Sixin Zhang", "Reda Chhaibi", "Serge Gratton", "Thierry Giaccone"], "title": "Feature Representation Transferring to Lightweight Models via Perception Coherence", "categories": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "math.PR"], "comment": null, "summary": "In this paper, we propose a method for transferring feature representation to\nlightweight student models from larger teacher models. We mathematically define\na new notion called \\textit{perception coherence}. Based on this notion, we\npropose a loss function, which takes into account the dissimilarities between\ndata points in feature space through their ranking. At a high level, by\nminimizing this loss function, the student model learns to mimic how the\nteacher model \\textit{perceives} inputs. More precisely, our method is\nmotivated by the fact that the representational capacity of the student model\nis weaker than the teacher model. Hence, we aim to develop a new method\nallowing for a better relaxation. This means that, the student model does not\nneed to preserve the absolute geometry of the teacher one, while preserving\nglobal coherence through dissimilarity ranking. Our theoretical insights\nprovide a probabilistic perspective on the process of feature representation\ntransfer. Our experiments results show that our method outperforms or achieves\non-par performance compared to strong baseline methods for representation\ntransferring.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.06963", "pdf": "https://arxiv.org/pdf/2505.06963", "abs": "https://arxiv.org/abs/2505.06963", "authors": ["Tarik Houichime", "Younes EL Amrani"], "title": "Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces an innovative approach for the autonomous landing of\nUnmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera,\ntherefore obviating the requirement for depth estimation cameras. Drawing on\nthe inherent human estimating process, the proposed method reframes the landing\ntask as an optimization problem. The UAV employs variations in the visual\ncharacteristics of a specially designed lenticular circle on the landing pad,\nwhere the perceived color and form provide critical information for estimating\nboth altitude and depth. Reinforcement learning algorithms are utilized to\napproximate the functions governing these estimations, enabling the UAV to\nascertain ideal landing settings via training. This method's efficacy is\nassessed by simulations and experiments, showcasing its potential for robust\nand accurate autonomous landing without dependence on complex sensor setups.\nThis research contributes to the advancement of cost-effective and efficient\nUAV landing solutions, paving the way for wider applicability across various\nfields.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07548", "pdf": "https://arxiv.org/pdf/2505.07548", "abs": "https://arxiv.org/abs/2505.07548", "authors": ["Lingkun Luo", "Shiqiang Hu", "Liming Chen"], "title": "Noise Optimized Conditional Diffusion for Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 figures This work has been accepted by the International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet\nthe scarcity of High-Confidence Pseudo-Labeled Target Domain Samples\n(\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical\nalignment, causing DA failures. To address this challenge, we propose\n\\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for\n\\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly\nintegrates the generative capabilities of conditional diffusion models with the\ndecision-making requirements of DA to achieve task-coupled optimization for\nefficient adaptation. For robust cross-domain consistency, we modify the DA\nclassifier to align with the conditional diffusion classifier within a unified\noptimization framework, enabling forward training on noise-varying cross-domain\nsamples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0},\n\\mathbf{I}) \\) initialization in diffusion models often generates\nclass-confused hcpl-tds, compromising discriminative DA. To resolve this, we\nintroduce a class-aware noise optimization strategy that refines sampling\nregions for reverse class-specific hcpl-tds generation, effectively enhancing\ncross-domain alignment. Extensive experiments across 5 benchmark datasets and\n29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over\n31 state-of-the-art methods, validating its robustness and effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency"], "score": 2}}, "source_file": "2025-05-13.jsonl"}
{"id": "2505.07600", "pdf": "https://arxiv.org/pdf/2505.07600", "abs": "https://arxiv.org/abs/2505.07600", "authors": ["Oriol Barbany", "Adri Colom", "Carme Torras"], "title": "Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA 2025 Workshop \"Reflections on Representations and\n  Manipulating Deformable Objects\". Project page\n  https://barbany.github.io/bifold/", "summary": "Manipulating clothes is challenging due to their complex dynamics, high\ndeformability, and frequent self-occlusions. Garments exhibit a nearly infinite\nnumber of configurations, making explicit state representations difficult to\ndefine. In this paper, we analyze BiFold, a model that predicts\nlanguage-conditioned pick-and-place actions from visual observations, while\nimplicitly encoding garment state through end-to-end learning. To address\nscenarios such as crumpled garments or recovery from failed manipulations,\nBiFold leverages temporal context to improve state estimation. We examine the\ninternal representations of the model and present evidence that its fine-tuning\nand temporal context enable effective alignment between text and image regions,\nas well as temporal consistency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-13.jsonl"}
