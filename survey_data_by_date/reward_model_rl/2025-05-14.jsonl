{"id": "2505.08106", "pdf": "https://arxiv.org/pdf/2505.08106", "abs": "https://arxiv.org/abs/2505.08106", "authors": ["Jiashen", "Du", "Jesse Yao", "Allen Liu", "Zhekai Zhang"], "title": "Are LLMs complicated ethical dilemma analyzers?", "categories": ["cs.CL", "cs.AI"], "comment": "CS194-280 Advanced LLM Agents project. Project page:\n  https://github.com/ALT-JS/ethicaLLM", "summary": "One open question in the study of Large Language Models (LLMs) is whether\nthey can emulate human ethical reasoning and act as believable proxies for\nhuman judgment. To investigate this, we introduce a benchmark dataset\ncomprising 196 real-world ethical dilemmas and expert opinions, each segmented\ninto five structured components: Introduction, Key Factors, Historical\nTheoretical Perspectives, Resolution Strategies, and Key Takeaways. We also\ncollect non-expert human responses for comparison, limited to the Key Factors\nsection due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,\nClaude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric\nframework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine\nsimilarity, and Universal Sentence Encoder similarity. Metric weights are\ncomputed through an inversion-based ranking alignment and pairwise AHP\nanalysis, enabling fine-grained comparison of model outputs to expert\nresponses. Our results show that LLMs generally outperform non-expert humans in\nlexical and structural alignment, with GPT-4o-mini performing most consistently\nacross all sections. However, all models struggle with historical grounding and\nproposing nuanced resolution strategies, which require contextual abstraction.\nHuman responses, while less structured, occasionally achieve comparable\nsemantic similarity, suggesting intuitive moral reasoning. These findings\nhighlight both the strengths and current limitations of LLMs in ethical\ndecision-making.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "ranking", "pairwise", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08561", "pdf": "https://arxiv.org/pdf/2505.08561", "abs": "https://arxiv.org/abs/2505.08561", "authors": ["Ayush K. Rai", "Kyle Min", "Tarun Krishna", "Feiyan Hu", "Alan F. Smeaton", "Noel E. O'Connor"], "title": "Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection", "categories": ["cs.CV"], "comment": null, "summary": "Masked video modeling~(MVM) has emerged as a highly effective pre-training\nstrategy for visual foundation models, whereby the model reconstructs masked\nspatiotemporal tokens using information from visible tokens. However, a key\nchallenge in such approaches lies in selecting an appropriate masking strategy.\nPrevious studies have explored predefined masking techniques, including random\nand tube-based masking, as well as approaches that leverage key motion priors,\noptical flow and semantic cues from externally pre-trained models. In this\nwork, we introduce a novel and generalizable Trajectory-Aware Adaptive Token\nSampler (TATS), which models the motion dynamics of tokens and can be\nseamlessly integrated into the masked autoencoder (MAE) framework to select\nmotion-centric tokens in videos. Additionally, we propose a unified training\nstrategy that enables joint optimization of both MAE and TATS from scratch\nusing Proximal Policy Optimization (PPO). We show that our model allows for\naggressive masking without compromising performance on the downstream task of\naction recognition while also ensuring that the pre-training remains memory\nefficient. Extensive experiments of the proposed approach across four\nbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,\ndemonstrate the effectiveness, transferability, generalization, and efficiency\nof our work compared to other state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 4}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07903", "pdf": "https://arxiv.org/pdf/2505.07903", "abs": "https://arxiv.org/abs/2505.07903", "authors": ["Zeyang Sha", "Shiwen Cui", "Weiqiang Wang"], "title": "SEM: Reinforcement Learning for Search-Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models(LLMs) have demonstrated their\ncapabilities not only in reasoning but also in invoking external tools,\nparticularly search engines. However, teaching models to discern when to invoke\nsearch and when to rely on their internal knowledge remains a significant\nchallenge. Existing reinforcement learning approaches often lead to redundant\nsearch behaviors, resulting in inefficiencies and over-cost. In this paper, we\npropose SEM, a novel post-training reinforcement learning framework that\nexplicitly trains LLMs to optimize search usage. By constructing a balanced\ndataset combining MuSiQue and MMLU, we create scenarios where the model must\nlearn to distinguish between questions it can answer directly and those\nrequiring external retrieval. We design a structured reasoning template and\nemploy Group Relative Policy Optimization(GRPO) to post-train the model's\nsearch behaviors. Our reward function encourages accurate answering without\nunnecessary search while promoting effective retrieval when needed.\nExperimental results demonstrate that our method significantly reduces\nredundant search operations while maintaining or improving answer accuracy\nacross multiple challenging benchmarks. This framework advances the model's\nreasoning efficiency and extends its capability to judiciously leverage\nexternal knowledge.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07886", "pdf": "https://arxiv.org/pdf/2505.07886", "abs": "https://arxiv.org/abs/2505.07886", "authors": ["Chun-Pai Yang", "Kan Zheng", "Shou-De Lin"], "title": "PLHF: Prompt Optimization with Few-Shot Human Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic prompt optimization frameworks are developed to obtain suitable\nprompts for large language models (LLMs) with respect to desired output quality\nmetrics. Although existing approaches can handle conventional tasks such as\nfixed-solution question answering, defining the metric becomes complicated when\nthe output quality cannot be easily assessed by comparisons with standard\ngolden samples. Consequently, optimizing the prompts effectively and\nefficiently without a clear metric becomes a critical challenge. To address the\nissue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman\n\"F\"eedback), a few-shot prompt optimization framework inspired by the\nwell-known RLHF technique. Different from naive strategies, PLHF employs a\nspecific evaluator module acting as the metric to estimate the output quality.\nPLHF requires only a single round of human feedback to complete the entire\nprompt optimization process. Empirical results on both public and industrial\ndatasets show that PLHF outperforms prior output grading strategies for LLM\nprompt optimizations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "human feedback"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07968", "pdf": "https://arxiv.org/pdf/2505.07968", "abs": "https://arxiv.org/abs/2505.07968", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Lucas A. Salas", "Jiang Gui"], "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential in the field of health\ncare, yet they face great challenges in adapting to rapidly evolving medical\nknowledge. This can lead to outdated or contradictory treatment suggestions.\nThis study investigated how LLMs respond to evolving clinical guidelines,\nfocusing on concept drift and internal inconsistencies. We developed the\nDriftMedQA benchmark to simulate guideline evolution and assessed the temporal\nreliability of various LLMs. Our evaluation of seven state-of-the-art models\nacross 4,290 scenarios demonstrated difficulties in rejecting outdated\nrecommendations and frequently endorsing conflicting guidance. Additionally, we\nexplored two mitigation strategies: Retrieval-Augmented Generation and\npreference fine-tuning via Direct Preference Optimization. While each method\nimproved model performance, their combination led to the most consistent and\nreliable results. These findings underscore the need to improve LLM robustness\nto temporal shifts to ensure more dependable applications in clinical practice.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "direct preference optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reliability"], "score": 3}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08468", "pdf": "https://arxiv.org/pdf/2505.08468", "abs": "https://arxiv.org/abs/2505.08468", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Ahmed Masry", "Mizanur Rahman", "Amran Bhuiyan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025 Industry Track", "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "consistency", "accuracy", "question answering", "rubric", "criteria"], "score": 7}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08498", "pdf": "https://arxiv.org/pdf/2505.08498", "abs": "https://arxiv.org/abs/2505.08498", "authors": ["Takumi Shibata", "Yuichi Miyamura"], "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08734", "pdf": "https://arxiv.org/pdf/2505.08734", "abs": "https://arxiv.org/abs/2505.08734", "authors": ["Ben Yao", "Qiuchi Li", "Yazhou Zhang", "Siyu Yang", "Bohan Zhang", "Prayag Tiwari", "Jing Qin"], "title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 10 figures, 16 tables", "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "dialogue", "dimension"], "score": 5}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08468", "pdf": "https://arxiv.org/pdf/2505.08468", "abs": "https://arxiv.org/abs/2505.08468", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Ahmed Masry", "Mizanur Rahman", "Amran Bhuiyan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025 Industry Track", "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement", "consistency", "accuracy", "question answering", "rubric", "criteria"], "score": 7}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07887", "pdf": "https://arxiv.org/pdf/2505.07887", "abs": "https://arxiv.org/abs/2505.07887", "authors": ["Songyin Wu", "Zhaoyang Lv", "Yufeng Zhu", "Duncan Frost", "Zhengqin Li", "Ling-Qi Yan", "Carl Ren", "Richard Newcombe", "Zhao Dong"], "title": "Monocular Online Reconstruction with Enhanced Detail Preservation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose an online 3D Gaussian-based dense mapping framework for\nphotorealistic details reconstruction from a monocular image stream. Our\napproach addresses two key challenges in monocular online reconstruction:\ndistributing Gaussians without relying on depth maps and ensuring both local\nand global consistency in the reconstructed maps. To achieve this, we introduce\ntwo key modules: the Hierarchical Gaussian Management Module for effective\nGaussian distribution and the Global Consistency Optimization Module for\nmaintaining alignment and coherence at all scales. In addition, we present the\nMulti-level Occupancy Hash Voxels (MOHV), a structure that regularizes\nGaussians for capturing details across multiple levels of granularity. MOHV\nensures accurate reconstruction of both fine and coarse geometries and\ntextures, preserving intricate details while maintaining overall structural\nintegrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our\nframework achieves superior reconstruction quality with high computational\nefficiency. Moreover, it integrates seamlessly with various tracking systems,\nensuring generality and scalability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07984", "pdf": "https://arxiv.org/pdf/2505.07984", "abs": "https://arxiv.org/abs/2505.07984", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to JSTARS on April 2, 2025. Code and dataset will be\n  available upon acceptance", "summary": "Remarkable capabilities in understanding and generating text-image content\nhave been demonstrated by recent advancements in multimodal large language\nmodels (MLLMs). However, their effectiveness in specialized\ndomains-particularly those requiring resource-efficient and domain-specific\nadaptations-has remained limited. In this work, a lightweight multimodal\nlanguage model termed MilChat is introduced, specifically adapted to analyze\nremote sensing imagery in secluded areas, including challenging missile launch\nsites. A new dataset, MilData, was compiled by verifying hundreds of aerial\nimages through expert review, and subtle military installations were\nhighlighted via detailed captions. Supervised fine-tuning on a 2B-parameter\nopen-source MLLM with chain-of-thought (CoT) reasoning annotations was\nperformed, enabling more accurate and interpretable explanations. Additionally,\nGroup Relative Policy Optimization (GRPO) was leveraged to enhance the model's\nability to detect critical domain-specific cues-such as defensive layouts and\nkey military structures-while minimizing false positives on civilian scenes.\nThrough empirical evaluations, it has been shown that MilChat significantly\noutperforms both larger, general-purpose multimodal models and existing remote\nsensing-adapted approaches on open-ended captioning and classification metrics.\nOver 80% recall and 98% precision were achieved on the newly proposed MilData\nbenchmark, underscoring the potency of targeted fine-tuning and reinforcement\nlearning in specialized real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08101", "pdf": "https://arxiv.org/pdf/2505.08101", "abs": "https://arxiv.org/abs/2505.08101", "authors": ["Luu Tung Hai", "Thinh D. Le", "Zhicheng Ding", "Qing Tian", "Truong-Son Hy"], "title": "Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud processing has gained significant attention due to its critical\nrole in applications such as autonomous driving and 3D object recognition.\nHowever, deploying high-performance models like Point Transformer V3 in\nresource-constrained environments remains challenging due to their high\ncomputational and memory demands. This work introduces a novel distillation\nframework that leverages topology-aware representations and gradient-guided\nknowledge distillation to effectively transfer knowledge from a high-capacity\nteacher to a lightweight student model. Our approach captures the underlying\ngeometric structures of point clouds while selectively guiding the student\nmodel's learning process through gradient-based feature alignment. Experimental\nresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the\nproposed method achieves competitive performance, with an approximately 16x\nreduction in model size and a nearly 1.9x decrease in inference time compared\nto its teacher model. Notably, on NuScenes, our method achieves\nstate-of-the-art performance among knowledge distillation techniques trained\nsolely on LiDAR data, surpassing prior knowledge distillation baselines in\nsegmentation performance. Our implementation is available publicly at:\n  https://github.com/HySonLab/PointDistill", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07858", "pdf": "https://arxiv.org/pdf/2505.07858", "abs": "https://arxiv.org/abs/2505.07858", "authors": ["Siyuan Yan", "Mo Zhu", "Guo-qing Jiang", "Jianfei Wang", "Jiaxing Chen", "Wentai Zhang", "Xiang Liao", "Xiao Cui", "Chen Zhang", "Zhuoran Song", "Ran Zhu"], "title": "Scaling Laws for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "The escalating demand for efficient decoding in large language models (LLMs)\nis particularly critical for reasoning-intensive architectures like OpenAI-o3\nand DeepSeek-R1, which depend on extended chain-of-thought reasoning. This\nstudy investigates speculative decoding techniques through dense LLM\narchitectures to establish foundational insights for accelerating reasoning\ntasks. While speculative decoding methods leveraging parallel\ndraft-verification cycles have emerged as promising acceleration techniques,\nthe scaling laws governing decoding efficiency remain under-explored compared\nto conventional backbone LLMs developed through Pretraining->SFT->RLHF training\nparadigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2\nand 1.3) governing draft model acceptance rate (or decoding speed) across three\ndimensions: pretraining token volume, draft model capacity, and decoding batch\nsize. Building on these laws, we achieve Scylla, which coordinates\nmulti-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical\nvalidation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and\n0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on\nsummarization and QA tasks (Figure 2). Industrial inference engine deployments\ndemonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),\nvalidating the transformative potential of systematic scaling for efficient LLM\ninference. Code will be released later.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization", "multi-dimensional"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08273", "pdf": "https://arxiv.org/pdf/2505.08273", "abs": "https://arxiv.org/abs/2505.08273", "authors": ["Nibir Chandra Mandal", "Oishee Bintey Hoque", "Abhijin Adiga", "Samarth Swarup", "Mandy Wilson", "Lu Feng", "Yangfeng Ji", "Miaomiao Zhang", "Geoffrey Fox", "Madhav Marathe"], "title": "IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping", "categories": ["cs.CV"], "comment": null, "summary": "We introduce IrrMap, the first large-scale dataset (1.1 million patches) for\nirrigation method mapping across regions. IrrMap consists of multi-resolution\nsatellite imagery from LandSat and Sentinel, along with key auxiliary data such\nas crop type, land use, and vegetation indices. The dataset spans 1,687,899\nfarms and 14,117,330 acres across multiple western U.S. states from 2013 to\n2023, providing a rich and diverse foundation for irrigation analysis and\nensuring geospatial alignment and quality control. The dataset is ML-ready,\nwith standardized 224x224 GeoTIFF patches, the multiple input modalities,\ncarefully chosen train-test-split data, and accompanying dataloaders for\nseamless deep learning model training andbenchmarking in irrigation mapping.\nThe dataset is also accompanied by a complete pipeline for dataset generation,\nenabling researchers to extend IrrMap to new regions for irrigation data\ncollection or adapt it with minimal effort for other similar applications in\nagricultural and geospatial analysis. We also analyze the irrigation method\ndistribution across crop groups, spatial irrigation patterns (using Shannon\ndiversity indices), and irrigated area variations for both LandSat and\nSentinel, providing insights into regional and resolution-based differences. To\npromote further exploration, we openly release IrrMap, along with the derived\ndatasets, benchmark models, and pipeline code, through a GitHub repository:\nhttps://github.com/Nibir088/IrrMap and Data repository:\nhttps://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and\nimplementation details.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08281", "pdf": "https://arxiv.org/pdf/2505.08281", "abs": "https://arxiv.org/abs/2505.08281", "authors": ["Anle Ke", "Xu Zhang", "Tong Chen", "Ming Lu", "Chao Zhou", "Jiawen Gu", "Zhan Ma"], "title": "Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Existing multimodal large model-based image compression frameworks often rely\non a fragmented integration of semantic retrieval, latent compression, and\ngenerative models, resulting in suboptimal performance in both reconstruction\nfidelity and coding efficiency. To address these challenges, we propose a\nresidual-guided ultra lowrate image compression named ResULIC, which\nincorporates residual signals into both semantic retrieval and the\ndiffusion-based generation process. Specifically, we introduce Semantic\nResidual Coding (SRC) to capture the semantic disparity between the original\nimage and its compressed latent representation. A perceptual fidelity optimizer\nis further applied for superior reconstruction quality. Additionally, we\npresent the Compression-aware Diffusion Model (CDM), which establishes an\noptimal alignment between bitrates and diffusion time steps, improving\ncompression-reconstruction synergy. Extensive experiments demonstrate the\neffectiveness of ResULIC, achieving superior objective and subjective\nperformance compared to state-of-the-art diffusion-based methods with - 80.7%,\n-66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at\nhttps: //njuvision.github.io/ResULIC/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08054", "pdf": "https://arxiv.org/pdf/2505.08054", "abs": "https://arxiv.org/abs/2505.08054", "authors": ["Zhehao Zhang", "Weijie Xu", "Fanyou Wu", "Chandan K. Reddy"], "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Safety alignment approaches in large language models (LLMs) often lead to the\nover-refusal of benign queries, significantly diminishing their utility in\nsensitive scenarios. To address this challenge, we introduce FalseReject, a\ncomprehensive resource containing 16k seemingly toxic queries accompanied by\nstructured responses across 44 safety-related categories. We propose a\ngraph-informed adversarial multi-agent interaction framework to generate\ndiverse and complex prompts, while structuring responses with explicit\nreasoning to aid models in accurately distinguishing safe from unsafe contexts.\nFalseReject includes training datasets tailored for both standard\ninstruction-tuned models and reasoning-oriented models, as well as a\nhuman-annotated benchmark test set. Our extensive benchmarking on 29\nstate-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.\nEmpirical results demonstrate that supervised finetuning with FalseReject\nsubstantially reduces unnecessary refusals without compromising overall safety\nor general language capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08351", "pdf": "https://arxiv.org/pdf/2505.08351", "abs": "https://arxiv.org/abs/2505.08351", "authors": ["Mina Almasi", "Ross Deans Kristensen-McLachlan"], "title": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring", "categories": ["cs.CL"], "comment": null, "summary": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08525", "pdf": "https://arxiv.org/pdf/2505.08525", "abs": "https://arxiv.org/abs/2505.08525", "authors": ["Yiqi Chen", "Ganghai Huang", "Sheng Zhang", "Jianglin Dai"], "title": "Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular topological structures (e.g., fissures and\nvasculature) is critical in various fields to guarantee dependable downstream\nquantitative analysis and modeling. However, in dense prediction tasks such as\nsemantic segmentation and super-resolution, conventional upsampling operators\ncannot accommodate the slenderness of tubular structures and the curvature of\nmorphology. This paper introduces a dynamic snake upsampling operators and a\nboundary-skeleton weighted loss tailored for topological tubular structures.\nSpecifically, we design a snake upsampling operators based on an adaptive\nsampling domain, which dynamically adjusts the sampling stride according to the\nfeature map and selects a set of subpixel sampling points along the serpentine\npath, enabling more accurate subpixel-level feature recovery for tubular\nstructures. Meanwhile, we propose a skeleton-to-boundary increasing weighted\nloss that trades off main body and boundary weight allocation based on mask\nclass ratio and distance field, preserving main body overlap while enhancing\nfocus on target topological continuity and boundary alignment precision.\nExperiments across various domain datasets and backbone networks show that this\nplug-and-play dynamic snake upsampling operator and boundary-skeleton weighted\nloss boost both pixel-wise segmentation accuracy and topological consistency of\nresults.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08463", "pdf": "https://arxiv.org/pdf/2505.08463", "abs": "https://arxiv.org/abs/2505.08463", "authors": ["Fujun Zhang", "XiangDong Su"], "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08605", "pdf": "https://arxiv.org/pdf/2505.08605", "abs": "https://arxiv.org/abs/2505.08605", "authors": ["Zhe Li", "Hadrien Reynaud", "Bernhard Kainz"], "title": "Leveraging Multi-Modal Information to Enhance Dataset Distillation", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Dataset distillation aims to create a compact and highly representative\nsynthetic dataset that preserves the knowledge of a larger real dataset. While\nexisting methods primarily focus on optimizing visual representations,\nincorporating additional modalities and refining object-level information can\nsignificantly improve the quality of distilled datasets. In this work, we\nintroduce two key enhancements to dataset distillation: caption-guided\nsupervision and object-centric masking. To integrate textual information, we\npropose two strategies for leveraging caption features: the feature\nconcatenation, where caption embeddings are fused with visual features at the\nclassification stage, and caption matching, which introduces a caption-based\nalignment loss during training to ensure semantic coherence between real and\nsynthetic data. Additionally, we apply segmentation masks to isolate target\nobjects and remove background distractions, introducing two loss functions\ndesigned for object-centric learning: masked feature alignment loss and masked\ngradient matching loss. Comprehensive evaluations demonstrate that integrating\ncaption-based guidance and object-centric masking enhances dataset\ndistillation, leading to synthetic datasets that achieve superior performance\non downstream tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08617", "pdf": "https://arxiv.org/pdf/2505.08617", "abs": "https://arxiv.org/abs/2505.08617", "authors": ["Zhaochen Su", "Linjie Li", "Mingyang Song", "Yunzhuo Hao", "Zhengyuan Yang", "Jun Zhang", "Guanjie Chen", "Jiawei Gu", "Juntao Li", "Xiaoye Qu", "Yu Cheng"], "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\".", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07908", "pdf": "https://arxiv.org/pdf/2505.07908", "abs": "https://arxiv.org/abs/2505.07908", "authors": ["Karahan Sarıtaş", "Çağatay Yıldız"], "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07851", "pdf": "https://arxiv.org/pdf/2505.07851", "abs": "https://arxiv.org/abs/2505.07851", "authors": ["Jaeyoung Huh", "Ankur Kapoor", "Young-Ho Kim"], "title": "Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Intra-cardiac Echocardiography (ICE) plays a crucial role in\nElectrophysiology (EP) and Structural Heart Disease (SHD) interventions by\nproviding high-resolution, real-time imaging of cardiac structures. However,\nexisting navigation methods rely on electromagnetic (EM) tracking, which is\nsusceptible to interference and position drift, or require manual adjustments\nbased on operator expertise. To overcome these limitations, we propose a novel\nanatomy-aware pose estimation system that determines the ICE catheter position\nand orientation solely from ICE images, eliminating the need for external\ntracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep\nlearning model, which captures spatial relationships between ICE images and\nanatomical structures. The model is trained on a clinically acquired dataset of\n851 subjects, including ICE images paired with position and orientation labels\nnormalized to the left atrium (LA) mesh. ICE images are patchified into 16x16\nembeddings and processed through a transformer network, where a [CLS] token\nindependently predicts position and orientation via separate linear layers. The\nmodel is optimized using a Mean Squared Error (MSE) loss function, balancing\npositional and orientational accuracy. Experimental results demonstrate an\naverage positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98\ndeg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.\nQualitative assessments further validate alignment between predicted and target\nviews within 3D cardiac meshes. This AI-driven system enhances procedural\nefficiency, reduces operator workload, and enables real-time ICE catheter\nlocalization for tracking-free procedures. The proposed method can function\nindependently or complement existing mapping systems like CARTO, offering a\ntransformative approach to ICE-guided interventions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07887", "pdf": "https://arxiv.org/pdf/2505.07887", "abs": "https://arxiv.org/abs/2505.07887", "authors": ["Songyin Wu", "Zhaoyang Lv", "Yufeng Zhu", "Duncan Frost", "Zhengqin Li", "Ling-Qi Yan", "Carl Ren", "Richard Newcombe", "Zhao Dong"], "title": "Monocular Online Reconstruction with Enhanced Detail Preservation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose an online 3D Gaussian-based dense mapping framework for\nphotorealistic details reconstruction from a monocular image stream. Our\napproach addresses two key challenges in monocular online reconstruction:\ndistributing Gaussians without relying on depth maps and ensuring both local\nand global consistency in the reconstructed maps. To achieve this, we introduce\ntwo key modules: the Hierarchical Gaussian Management Module for effective\nGaussian distribution and the Global Consistency Optimization Module for\nmaintaining alignment and coherence at all scales. In addition, we present the\nMulti-level Occupancy Hash Voxels (MOHV), a structure that regularizes\nGaussians for capturing details across multiple levels of granularity. MOHV\nensures accurate reconstruction of both fine and coarse geometries and\ntextures, preserving intricate details while maintaining overall structural\nintegrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our\nframework achieves superior reconstruction quality with high computational\nefficiency. Moreover, it integrates seamlessly with various tracking systems,\nensuring generality and scalability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08148", "pdf": "https://arxiv.org/pdf/2505.08148", "abs": "https://arxiv.org/abs/2505.08148", "authors": ["Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Hassan Jameel Asghar", "Benjamin Zi Hao Zhao", "Dali Kaafar"], "title": "A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Millions of users leverage generative pretrained transformer (GPT)-based\nlanguage models developed by leading model providers for a wide range of tasks.\nTo support enhanced user interaction and customization, many platforms-such as\nOpenAI-now enable developers to create and publish tailored model instances,\nknown as custom GPTs, via dedicated repositories or application stores. These\ncustom GPTs empower users to browse and interact with specialized applications\ndesigned to meet specific needs. However, as custom GPTs see growing adoption,\nconcerns regarding their security vulnerabilities have intensified. Existing\nresearch on these vulnerabilities remains largely theoretical, often lacking\nempirical, large-scale, and statistically rigorous assessments of associated\nrisks.\n  In this study, we analyze 14,904 custom GPTs to assess their susceptibility\nto seven exploitable threats, such as roleplay-based attacks, system prompt\nleakage, phishing content generation, and malicious code synthesis, across\nvarious categories and popularity tiers within the OpenAI marketplace. We\nintroduce a multi-metric ranking system to examine the relationship between a\ncustom GPT's popularity and its associated security risks.\n  Our findings reveal that over 95% of custom GPTs lack adequate security\nprotections. The most prevalent vulnerabilities include roleplay-based\nvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing\n(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit\ninherent security weaknesses, which are often inherited or amplified in custom\nGPTs. These results highlight the urgent need for enhanced security measures\nand stricter content moderation to ensure the safe deployment of GPT-based\napplications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.07908", "pdf": "https://arxiv.org/pdf/2505.07908", "abs": "https://arxiv.org/abs/2505.07908", "authors": ["Karahan Sarıtaş", "Çağatay Yıldız"], "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08445", "pdf": "https://arxiv.org/pdf/2505.08445", "abs": "https://arxiv.org/abs/2505.08445", "authors": ["Adel Ammar", "Anis Koubaa", "Omer Nacar", "Wadii Boulila"], "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622", "abs": "https://arxiv.org/abs/2505.08622", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08727", "pdf": "https://arxiv.org/pdf/2505.08727", "abs": "https://arxiv.org/abs/2505.08727", "authors": ["Fangyuan Yu"], "title": "Memorization-Compression Cycles Improve Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": "12 pages, 6 figures", "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622", "abs": "https://arxiv.org/abs/2505.08622", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-14.jsonl"}
