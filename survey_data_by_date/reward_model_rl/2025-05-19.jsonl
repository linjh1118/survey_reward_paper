{"id": "2505.10775", "pdf": "https://arxiv.org/pdf/2505.10775", "abs": "https://arxiv.org/abs/2505.10775", "authors": ["Kian Ahrabian", "Pegah Jandaghi", "Negar Mokhberian", "Sai Praneeth Karimireddy", "Jay Pujara"], "title": "A Systematic Analysis of Base Model Choice for Reward Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 13 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward\nmodeling have become a crucial part of training powerful large language models\n(LLMs). One commonly overlooked factor in training high-quality reward models\n(RMs) is the effect of the base model, which is becoming more challenging to\nchoose given the rapidly growing pool of LLMs. In this work, we present a\nsystematic analysis of the effect of base model selection on reward modeling\nperformance. Our results show that the performance can be improved by up to 14%\ncompared to the most common (i.e., default) choice. Moreover, we showcase the\nstrong statistical relation between some existing benchmarks and downstream\nperformances. We also demonstrate that the results from a small set of\nbenchmarks could be combined to boost the model selection ($+$18% on average in\nthe top 5-10). Lastly, we illustrate the impact of different post-training\nsteps on the final performance and explore using estimated data distributions\nto reduce performance prediction error.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 5}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11080", "pdf": "https://arxiv.org/pdf/2505.11080", "abs": "https://arxiv.org/abs/2505.11080", "authors": ["Yapei Chang", "Yekyung Kim", "Michael Krumdick", "Amir Zadeh", "Chuan Li", "Chris Tanner", "Mohit Iyyer"], "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 11 figures, 15 tables", "summary": "Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward function", "policy optimization", "preference", "alignment"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597", "abs": "https://arxiv.org/abs/2505.10597", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "MingXu Cai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) are essential for aligning large language models (LLMs)\nwith human values. However, noisy preferences in human feedback often lead to\nreward misgeneralization, where RMs overfit to spurious patterns and provide\nmisleading signals during policy optimization. We systematically analyze the\ntraining dynamics of preference pairs and identify that noisy examples are\nharder to fit and introduce instability. Empirical evidence shows that LLMs\noptimized using reward models trained on full noisy datasets perform worse than\nthose trained on filtered, high-quality preferences. To address this, we\npropose Collaborative Reward Modeling (CRM), an online framework that enhances\nrobustness by combining peer review and curriculum learning. Two reward models\nare trained in parallel and assess each other's data selections to filter out\npotential noise. Curriculum learning structures the preference data from easy\nto hard, ensuring synchronized training and stable feedback. Extensive\nexperiments demonstrate that CRM improves generalization, with up to 9.94\npoints of accuracy gain on RewardBench under 40 percent label noise. CRM is\nalso compatible with implicit-reward alignment methods, offering a practical\nand versatile strategy for robust alignment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "human feedback", "policy optimization", "preference", "alignment"], "score": 5}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11070", "pdf": "https://arxiv.org/pdf/2505.11070", "abs": "https://arxiv.org/abs/2505.11070", "authors": ["Renjie Chen", "Wenfeng Lin", "Yichen Zhang", "Jiangchuan Wei", "Boyuan Liu", "Chao Feng", "Jiao Ran", "Mingyu Guo"], "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aligning text-to-image (T2I) diffusion models with Direct Preference\nOptimization (DPO) has shown notable improvements in generation quality.\nHowever, applying DPO to T2I faces two challenges: the sensitivity of DPO to\npreference pairs and the labor-intensive process of collecting and annotating\nhigh-quality data. In this work, we demonstrate that preference pairs with\nmarginal differences can degrade DPO performance. Since DPO relies exclusively\non relative ranking while disregarding the absolute difference of pairs, it may\nmisclassify losing samples as wins, or vice versa. We empirically show that\nextending the DPO from pairwise to groupwise and incorporating reward\nstandardization for reweighting leads to performance gains without explicit\ndata selection. Furthermore, we propose Group Preference Optimization (GPO), an\neffective self-improvement method that enhances performance by leveraging the\nmodel's own capabilities without requiring external data. Extensive experiments\ndemonstrate that GPO is effective across various diffusion models and tasks.\nSpecifically, combining with widely used computer vision models, such as YOLO\nand OCR, the GPO improves the accurate counting and text rendering capabilities\nof the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a\nplug-and-play method, no extra overhead is introduced during inference.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "ranking", "pairwise", "DPO"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10717", "pdf": "https://arxiv.org/pdf/2505.10717", "abs": "https://arxiv.org/abs/2505.10717", "authors": ["Jean-Philippe Corbeil", "Amin Dada", "Jean-Michel Attendu", "Asma Ben Abacha", "Alessandro Sordoni", "Lucas Caccia", "Fran√ßois Beaulieu", "Thomas Lin", "Jens Kleesiek", "Paul Vozila"], "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11475", "pdf": "https://arxiv.org/pdf/2505.11475", "abs": "https://arxiv.org/abs/2505.11475", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11480", "pdf": "https://arxiv.org/pdf/2505.11480", "abs": "https://arxiv.org/abs/2505.11480", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "PPO", "reinforcement learning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11404", "pdf": "https://arxiv.org/pdf/2505.11404", "abs": "https://arxiv.org/abs/2505.11404", "authors": ["Wenchuan Zhang", "Penghao Zhang", "Jingru Guo", "Tao Cheng", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy", "question answering"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11166", "pdf": "https://arxiv.org/pdf/2505.11166", "abs": "https://arxiv.org/abs/2505.11166", "authors": ["Huashan Sun", "Shengyi Liao", "Yansen Han", "Yu Bai", "Yang Gao", "Cheng Fu", "Weizhou Shen", "Fanqi Wan", "Ming Yan", "Ji Zhang", "Fei Huang"], "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11225", "pdf": "https://arxiv.org/pdf/2505.11225", "abs": "https://arxiv.org/abs/2505.11225", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly\nimprove the reasoning abilities and performance of large language models\n(LLMs), it often results in verbose outputs and increases inference cost. Prior\napproaches for efficient test-time scaling, typically using universal budget\nconstraints or query-level length optimization, do not leverage historical\ninformation from previous encounters with the same problem during training. We\nhypothesize that this limits their ability to progressively make solutions more\nconcise over time. To address this, we present History-Aware Policy\nOptimization (HAPO), which keeps track of a history state (e.g., the minimum\nlength over previously generated correct responses) for each problem. HAPO\nemploys a novel length reward function based on this history state to\nincentivize the discovery of correct solutions that are more concise than those\npreviously found. Crucially, this reward structure avoids overly penalizing\nshorter incorrect responses with the goal of facilitating exploration towards\nmore efficient solutions. By combining this length reward with a correctness\nreward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to\ntrain DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and\nQwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span\nvarious difficulty levels. Experiment results demonstrate that HAPO effectively\ninduces LLMs' concise reasoning abilities, producing length reductions of\n33-59% with accuracy drops of only 2-5%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277", "abs": "https://arxiv.org/abs/2505.11277", "authors": ["Yaorui Shi", "Shihan Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11034", "pdf": "https://arxiv.org/pdf/2505.11034", "abs": "https://arxiv.org/abs/2505.11034", "authors": ["Fabian Gr√∂ger", "Simone Lionetti", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Ludovic Amruthalingam", "Elisabeth Victoria Goessinger", "Hanna Lindemann", "Marie Bargiela", "Marie Hofbauer", "Omar Badri", "Philipp Tschandl", "Arash Koochek", "Matthew Groh", "Alexander A. Navarini", "Marc Pouly"], "title": "CleanPatrick: A Benchmark for Image Data Cleaning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust machine learning depends on clean data, yet current image data\ncleaning benchmarks rely on synthetic noise or narrow human studies, limiting\ncomparison and real-world relevance. We introduce CleanPatrick, the first\nlarge-scale benchmark for data cleaning in the image domain, built upon the\npublicly available Fitzpatrick17k dermatology dataset. We collect 496,377\nbinary annotations from 933 medical crowd workers, identify off-topic samples\n(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation\nmodel inspired by item-response theory followed by expert review to derive\nhigh-quality ground truth. CleanPatrick formalizes issue detection as a ranking\ntask and adopts typical ranking metrics mirroring real audit workflows.\nBenchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident\nLearning, NoiseRank, and SelfClean, we find that, on CleanPatrick,\nself-supervised representations excel at near-duplicate detection, classical\nmethods achieve competitive off-topic detection under constrained review\nbudgets, and label-error detection remains an open challenge for fine-grained\nmedical classification. By releasing both the dataset and the evaluation\nframework, CleanPatrick enables a systematic comparison of image-cleaning\nstrategies and paves the way for more reliable data-centric artificial\nintelligence.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178", "abs": "https://arxiv.org/abs/2505.11178", "authors": ["Yixin Wan", "Kai-Wei Chang"], "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11245", "pdf": "https://arxiv.org/pdf/2505.11245", "abs": "https://arxiv.org/abs/2505.11245", "authors": ["Fu-Yun Wang", "Yunhao Shui", "Jingtan Piao", "Keqiang Sun", "Hongsheng Li"], "title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Diffusion models have made substantial advances in image generation, yet\nmodels trained on large, unfiltered datasets often yield outputs misaligned\nwith human preferences. Numerous methods have been proposed to fine-tune\npre-trained diffusion models, achieving notable improvements in aligning\ngenerated outputs with human preferences. However, we argue that existing\npreference alignment methods neglect the critical role of handling\nunconditional/negative-conditional outputs, leading to a diminished capacity to\navoid generating undesirable outcomes. This oversight limits the efficacy of\nclassifier-free guidance~(CFG), which relies on the contrast between\nconditional generation and unconditional/negative-conditional generation to\noptimize output quality. In response, we propose a straightforward but\nversatile effective approach that involves training a model specifically\nattuned to negative preferences. This method does not require new training\nstrategies or datasets but rather involves minor modifications to existing\ntechniques. Our approach integrates seamlessly with models such as SD1.5, SDXL,\nvideo diffusion models and models that have undergone preference optimization,\nconsistently enhancing their alignment with human preferences.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178", "abs": "https://arxiv.org/abs/2505.11178", "authors": ["Yixin Wan", "Kai-Wei Chang"], "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "accuracy", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832", "abs": "https://arxiv.org/abs/2505.10832", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Project Page: https://github.com/TU2021/AutoThink", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10769", "pdf": "https://arxiv.org/pdf/2505.10769", "abs": "https://arxiv.org/abs/2505.10769", "authors": ["Manyu Li", "Ruian He", "Zixian Zhang", "Weimin Tan", "Bo Yan"], "title": "Unifying Segment Anything in Microscopy with Multimodal Large Language Model", "categories": ["cs.CV", "68T99"], "comment": "18 pages, 9 figures", "summary": "Accurate segmentation of regions of interest in biomedical images holds\nsubstantial value in image analysis. Although several foundation models for\nbiomedical segmentation have currently achieved excellent performance on\ncertain datasets, they typically demonstrate sub-optimal performance on unseen\ndomain data. We owe the deficiency to lack of vision-language knowledge before\nsegmentation. Multimodal Large Language Models (MLLMs) bring outstanding\nunderstanding and reasoning capabilities to multimodal tasks, which inspires us\nto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling\nvision models to demonstrate superior generalization capabilities on\ncross-domain datasets. In this paper, we propose using MLLMs to guide SAM in\nlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,\nnamed uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment\n(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find\nthat after SAM receives global VLK prompts, its performance improves\nsignificantly, but there are deficiencies in boundary contour perception.\nTherefore, we further propose Semantic Boundary Regularization (SBR) to prompt\nSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%\nin SA across 9 in-domain microscopy datasets, achieving state-of-the-art\nperformance. Our method also demonstrates improvements of 6.79% in Dice and\n10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization\ncapabilities. Code is available at https://github.com/ieellee/uLLSAM.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10810", "pdf": "https://arxiv.org/pdf/2505.10810", "abs": "https://arxiv.org/abs/2505.10810", "authors": ["Gabriel Maldonado", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 2 tables. Presented at the CVPR 2025 Human\n  Motion Generation (HuMoGen) Workshop. Introduces MoCLIP, a CLIP-based\n  fine-tuning strategy for motion generation, with results on HumanML3D dataset\n  and ablation studies", "summary": "Human motion generation is essential for fields such as animation, robotics,\nand virtual reality, requiring models that effectively capture motion dynamics\nfrom text descriptions. Existing approaches often rely on Contrastive\nLanguage-Image Pretraining (CLIP)-based text encoders, but their training on\ntext-image pairs constrains their ability to understand temporal and kinematic\nstructures inherent in motion and motion generation. This work introduces\nMoCLIP, a fine-tuned CLIP model with an additional motion encoding head,\ntrained on motion sequences using contrastive learning and tethering loss. By\nexplicitly incorporating motion-aware representations, MoCLIP enhances motion\nfidelity while remaining compatible with existing CLIP-based pipelines and\nseamlessly integrating into various CLIP-based methods. Experiments demonstrate\nthat MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining\ncompetitive FID, leading to improved text-to-motion alignment results. These\nresults highlight MoCLIP's versatility and effectiveness, establishing it as a\nrobust framework for enhancing motion generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10823", "pdf": "https://arxiv.org/pdf/2505.10823", "abs": "https://arxiv.org/abs/2505.10823", "authors": ["Xue Li", "Jameson Merkow", "Noel C. F. Codella", "Alberto Santamaria-Pang", "Naiteek Sangani", "Alexander Ersoy", "Christopher Burt", "John W. Garrett", "Richard J. Bruce", "Joshua D. Warner", "Tyler Bradshaw", "Ivan Tarapov", "Matthew P. Lungren", "Alan B. McMillan"], "title": "From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Foundation models, pretrained on extensive datasets, have significantly\nadvanced machine learning by providing robust and transferable embeddings\napplicable to various domains, including medical imaging diagnostics. This\nstudy evaluates the utility of embeddings derived from both general-purpose and\nmedical domain-specific foundation models for training lightweight adapter\nmodels in multi-class radiography classification, focusing specifically on tube\nplacement assessment. A dataset comprising 8842 radiographs classified into\nseven distinct categories was employed to extract embeddings using six\nfoundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,\nRad-DINO, and CXR-Foundation. Adapter models were subsequently trained using\nclassical machine learning algorithms. Among these combinations,\nMedImageInsight embeddings paired with an support vector machine adapter\nyielded the highest mean area under the curve (mAUC) at 93.8%, followed closely\nby Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and\nDenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,\nrespectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.\nNotably, most adapter models demonstrated computational efficiency, achieving\ntraining within one minute and inference within seconds on CPU, underscoring\ntheir practicality for clinical applications. Furthermore, fairness analyses on\nadapters trained on MedImageInsight-derived embeddings indicated minimal\ndisparities, with gender differences in performance within 2% and standard\ndeviations across age groups not exceeding 3%. These findings confirm that\nfoundation model embeddings-especially those from MedImageInsight-facilitate\naccurate, computationally efficient, and equitable diagnostic classification\nusing lightweight adapters for radiographic image analysis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10917", "pdf": "https://arxiv.org/pdf/2505.10917", "abs": "https://arxiv.org/abs/2505.10917", "authors": ["Mingxiao Li", "Na Su", "Fang Qu", "Zhizhou Zhong", "Ziyang Chen", "Zhaopeng Tu", "Xiaolong Li"], "title": "VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization", "categories": ["cs.CV"], "comment": null, "summary": "Current multimodal large language models (MLLMs) face a critical challenge in\nmodality alignment, often exhibiting a bias towards textual information at the\nexpense of other modalities like vision. This paper conducts a systematic\ninformation-theoretic analysis of the widely used cross-entropy loss in MLLMs,\nuncovering its implicit alignment objective. Our theoretical investigation\nreveals that this implicit objective has inherent limitations, leading to a\ndegradation of cross-modal alignment as text sequence length increases, thereby\nhindering effective multimodal information fusion. To overcome these drawbacks,\nwe propose Vision-Text Alignment (VISTA), a novel approach guided by our\ntheoretical insights. VISTA introduces an explicit alignment objective designed\nto maximize cross-modal mutual information, preventing the degradation of\nvisual alignment. Notably, VISTA enhances the visual understanding capabilities\nof existing MLLMs without requiring any additional trainable modules or extra\ntraining data, making it both efficient and practical. Our method significantly\noutperforms baseline models across more than a dozen benchmark datasets,\nincluding VQAv2, MMStar, and MME, paving the way for new directions in MLLM\nmodal alignment research.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10921", "pdf": "https://arxiv.org/pdf/2505.10921", "abs": "https://arxiv.org/abs/2505.10921", "authors": ["Junyi Yuan", "Jian Zhang", "Fangyu Wu", "Dongming Lu", "Huanda Lu", "Qiufeng Wang"], "title": "Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution", "categories": ["cs.CV"], "comment": null, "summary": "China has a long and rich history, encompassing a vast cultural heritage that\nincludes diverse multimodal information, such as silk patterns, Dunhuang\nmurals, and their associated historical narratives. Cross-modal retrieval plays\na pivotal role in understanding and interpreting Chinese cultural heritage by\nbridging visual and textual modalities to enable accurate text-to-image and\nimage-to-text retrieval. However, despite the growing interest in multimodal\nresearch, there is a lack of specialized datasets dedicated to Chinese cultural\nheritage, limiting the development and evaluation of cross-modal learning\nmodels in this domain. To address this gap, we propose a multimodal dataset\nnamed CulTi, which contains 5,726 image-text pairs extracted from two series of\nprofessional documents, respectively related to ancient Chinese silk and\nDunhuang murals. Compared to existing general-domain multimodal datasets, CulTi\npresents a challenge for cross-modal retrieval: the difficulty of local\nalignment between intricate decorative motifs and specialized textual\ndescriptions. To address this challenge, we propose LACLIP, a training-free\nlocal alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances\nthe alignment of global textual descriptions with local visual regions by\ncomputing weighted similarity scores during inference. Experimental results on\nCulTi demonstrate that LACLIP significantly outperforms existing models in\ncross-modal retrieval, particularly in handling fine-grained semantic\nassociations within Chinese cultural heritage.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336", "abs": "https://arxiv.org/abs/2505.11336", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "title": "XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision", "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368", "abs": "https://arxiv.org/abs/2505.11368", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11413", "pdf": "https://arxiv.org/pdf/2505.11413", "abs": "https://arxiv.org/abs/2505.11413", "authors": ["Sijia Chen", "Xiaomin Li", "Mengxue Zhang", "Eric Hanchen Jiang", "Qingcheng Zeng", "Chen-Hsiang Yu"], "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462", "abs": "https://arxiv.org/abs/2505.11462", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11141", "pdf": "https://arxiv.org/pdf/2505.11141", "abs": "https://arxiv.org/abs/2505.11141", "authors": ["Yansheng Qiu", "Li Xiao", "Zhaopan Xu", "Pengfei Zhou", "Zheng Wang", "Kaipeng Zhang"], "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of achieving Artificial General Intelligence (AGI) is to imitate\nhumans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have\ndemonstrated that large language models (LLMs) with human-like reasoning\ncapabilities exhibit exceptional performance and are being gradually integrated\ninto multimodal large language models (MLLMs). However, whether these models\npossess capabilities comparable to humans in handling reasoning tasks remains\nunclear at present. In this paper, we propose Human-Aligned Bench, a benchmark\nfor fine-grained alignment of multimodal reasoning with human performance.\nSpecifically, we collected 9,794 multimodal questions that solely rely on\ncontextual reasoning, including bilingual (Chinese and English) multimodal\nquestions and pure text-based questions, encompassing four question types:\nvisual reasoning, definition judgment, analogical reasoning, and logical\njudgment. More importantly, each question is accompanied by human success rates\nand options that humans are prone to choosing incorrectly. Extensive\nexperiments on the Human-Aligned Bench reveal notable differences between the\nperformance of current MLLMs in multimodal reasoning and human performance. The\nfindings on our benchmark provide insights into the development of the\nnext-generation models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11182", "pdf": "https://arxiv.org/pdf/2505.11182", "abs": "https://arxiv.org/abs/2505.11182", "authors": ["Yuzhuo Dai", "Jiaqi Jin", "Zhibin Dong", "Siwei Wang", "Xinwang Liu", "En Zhu", "Xihong Yang", "Xinbiao Gan", "Yu Feng"], "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning", "categories": ["cs.CV", "cs.AI"], "comment": "The paper has been accepted by the 42nd CVPR 2025. The main text has\n  9 pages, including 8 figures and 4 tables. The appendix has 8 pages, with 10\n  figures and 6 tables. The reference list has 3 pages", "summary": "In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "fine-grained"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11192", "pdf": "https://arxiv.org/pdf/2505.11192", "abs": "https://arxiv.org/abs/2505.11192", "authors": ["Myunsoo Kim", "Seong-Woong Shim", "Byung-Jun Lee"], "title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314", "abs": "https://arxiv.org/abs/2505.11314", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11326", "pdf": "https://arxiv.org/pdf/2505.11326", "abs": "https://arxiv.org/abs/2505.11326", "authors": ["Keunwoo Peter Yu", "Joyce Chai"], "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "question answering"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11154", "pdf": "https://arxiv.org/pdf/2505.11154", "abs": "https://arxiv.org/abs/2505.11154", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Yu Liu", "Wenbo Jiang", "Wenshu Fan", "Qingchuan Zhao", "Guowen Xu"], "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11200", "pdf": "https://arxiv.org/pdf/2505.11200", "abs": "https://arxiv.org/abs/2505.11200", "authors": ["Xihuai Wang", "Ziyi Zhao", "Siyu Ren", "Shao Zhang", "Song Li", "Xiaoyu Li", "Ziwen Wang", "Lin Qiu", "Guanglu Wan", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang"], "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "comment": "Under Review", "summary": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "multi-dimensional"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11274", "pdf": "https://arxiv.org/pdf/2505.11274", "abs": "https://arxiv.org/abs/2505.11274", "authors": ["Zheng Li", "Qingxiu Dong", "Jingyuan Ma", "Di Zhang", "Zhifang Sui"], "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314", "abs": "https://arxiv.org/abs/2505.11314", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "fine-grained"], "score": 4}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365", "abs": "https://arxiv.org/abs/2505.11365", "authors": ["Pierre Le Jeune", "Beno√Æt Mal√©sieux", "Weixuan Xiao", "Matteo Dora"], "title": "Phare: A Safety Probe for Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "reliability"], "score": 3}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11454", "pdf": "https://arxiv.org/pdf/2505.11454", "abs": "https://arxiv.org/abs/2505.11454", "authors": ["Shaina Raza", "Aravind Narayanan", "Vahid Reza Khazaie", "Ashmal Vayani", "Mukund S. Chettiar", "Amandeep Singh", "Mubarak Shah", "Deval Pandya"], "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "testbed", "annotation", "accuracy", "question answering", "criteria"], "score": 8}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11468", "pdf": "https://arxiv.org/pdf/2505.11468", "abs": "https://arxiv.org/abs/2505.11468", "authors": ["Dingbang Huang", "Wenbo Li", "Yifei Zhao", "Xinyu Pan", "Yanhong Zeng", "Bo Dai"], "title": "PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/dingbang777/PSDiffusion/", "summary": "Diffusion models have made remarkable advancements in generating high-quality\nimages from textual descriptions. Recent works like LayerDiffuse have extended\nthe previous single-layer, unified image generation paradigm to transparent\nimage layer generation. However, existing multi-layer generation methods fail\nto handle the interactions among multiple layers such as rational global\nlayout, physics-plausible contacts and visual effects like shadows and\nreflections while maintaining high alpha quality. To solve this problem, we\npropose PSDiffusion, a unified diffusion framework for simultaneous multi-layer\ntext-to-image generation. Our model can automatically generate multi-layer\nimages with one RGB background and multiple RGBA foregrounds through a single\nfeed-forward process. Unlike existing methods that combine multiple tools for\npost-decomposition or generate layers sequentially and separately, our method\nintroduces a global-layer interactive mechanism that generates layered-images\nconcurrently and collaboratively, ensuring not only high quality and\ncompleteness for each layer, but also spatial and visual interactions among\nlayers for global coherence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vuliƒá"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10672", "pdf": "https://arxiv.org/pdf/2505.10672", "abs": "https://arxiv.org/abs/2505.10672", "authors": ["Hania Ghouse", "Muzammil Behzad"], "title": "MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Efficient and accurate multi-organ segmentation from abdominal CT volumes is\na fundamental challenge in medical image analysis. Existing 3D segmentation\napproaches are computationally and memory intensive, often processing entire\nvolumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods\nsuffer from class imbalance and lack cross-view contextual awareness. To\naddress these limitations, we propose a novel, anatomically-aware slice\nselector pipeline that reduces input volume prior to segmentation. Our unified\nframework introduces a vision-language model (VLM) for cross-view organ\npresence detection using fused tri-slice (2.5D) representations from axial,\nsagittal, and coronal planes. Our proposed model acts as an \"expert\" in\nanatomical localization, reasoning over multi-view representations to\nselectively retain slices with high structural relevance. This enables\nspatially consistent filtering across orientations while preserving contextual\ncues. More importantly, since standard segmentation metrics such as Dice or IoU\nfail to measure the spatial precision of such slice selection, we introduce a\nnovel metric, Slice Localization Concordance (SLC), which jointly captures\nanatomical coverage and spatial alignment with organ-centric reference slices.\nUnlike segmentation-specific metrics, SLC provides a model-agnostic evaluation\nof localization fidelity. Our model offers substantial improvement gains\nagainst several baselines across all organs, demonstrating both accurate and\nreliable organ-focused slice filtering. These results show that our method\nenables efficient and spatially consistent organ filtering, thereby\nsignificantly reducing downstream segmentation cost while maintaining high\nanatomical fidelity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10729", "pdf": "https://arxiv.org/pdf/2505.10729", "abs": "https://arxiv.org/abs/2505.10729", "authors": ["NingFeng Que", "Xiaofei Wang", "Jingjing Chen", "Yixuan Jiang", "Chao Li"], "title": "Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "Early accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) is a promising technique that characterizes the\nspatial gene profiling patterns within the tissue context. Comprehensive ST\nanalysis depends on consecutive slices for 3D spatial insights, whereas the\nmissing intermediate tissue sections and high costs limit the practical\nfeasibility of generating multi-slice ST. In this paper, we propose C2-STi, the\nfirst attempt for interpolating missing ST slices at arbitrary intermediate\npositions between adjacent ST slices. Despite intuitive, effective ST\ninterpolation presents significant challenges, including 1) limited continuity\nacross heterogeneous tissue sections, 2) complex intrinsic correlation across\ngenes, and 3) intricate cellular structures and biological semantics within\neach tissue section. To mitigate these challenges, in C2-STi, we design 1) a\ndistance-aware local structural modulation module to adaptively capture\ncross-slice deformations and enhance positional correlations between ST slices,\n2) a pyramid gene co-expression correlation module to capture multi-scale\nbiological associations among genes, and 3) a cross-modal alignment module that\nintegrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter\nand align the essential cellular features across ST and H\\&E images. Extensive\nexperiments on the public dataset demonstrate our superiority over\nstate-of-the-art approaches on both single-slice and multi-slice ST\ninterpolation. Codes are available at\nhttps://github.com/XiaofeiWang2018/C2-STi.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "correlation"], "score": 2}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10873", "pdf": "https://arxiv.org/pdf/2505.10873", "abs": "https://arxiv.org/abs/2505.10873", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Hashing for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Image Analysis and Processing\n  (ICIAP 2023)", "summary": "We focus on the problem of identifying samples in a set that do not conform\nto structured patterns represented by low-dimensional manifolds. An effective\nway to solve this problem is to embed data in a high dimensional space, called\nPreference Space, where anomalies can be identified as the most isolated\npoints. In this work, we employ Locality Sensitive Hashing to avoid explicit\ncomputation of distances in high dimensions and thus improve Anomaly Detection\nefficiency. Specifically, we present an isolation-based anomaly detection\ntechnique designed to work in the Preference Space which achieves\nstate-of-the-art performance at a lower computational cost. Code is publicly\navailable at\nhttps://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10874", "pdf": "https://arxiv.org/pdf/2505.10874", "abs": "https://arxiv.org/abs/2505.10874", "authors": ["Luca Magri", "Filippo Leveni", "Giacomo Boracchi"], "title": "MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at Computer Vision and Pattern Recognition (CVPR 2021)", "summary": "We address the problem of recovering multiple structures of different classes\nin a dataset contaminated by noise and outliers. In particular, we consider\ngeometric structures defined by a mixture of underlying parametric models (e.g.\nplanes and cylinders, homographies and fundamental matrices), and we tackle the\nrobust fitting problem by preference analysis and clustering. We present a new\nalgorithm, termed MultiLink, that simultaneously deals with multiple classes of\nmodels. MultiLink combines on-the-fly model fitting and model selection in a\nnovel linkage scheme that determines whether two clusters are to be merged. The\nresulting method features many practical advantages with respect to methods\nbased on preference analysis, being faster, less sensitive to the inlier\nthreshold, and able to compensate limitations deriving from hypotheses\nsampling. Experiments on several public datasets demonstrate that Multi-Link\nfavourably compares with state of the art alternatives, both in multi-class and\nsingle-class problems. Code is publicly made available for download.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10876", "pdf": "https://arxiv.org/pdf/2505.10876", "abs": "https://arxiv.org/abs/2505.10876", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Preference Isolation Forest for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Submitted to Pattern Recognition", "summary": "We address the problem of detecting anomalies as samples that do not conform\nto structured patterns represented by low-dimensional manifolds. To this end,\nwe conceive a general anomaly detection framework called Preference Isolation\nForest (PIF), that combines the benefits of adaptive isolation-based methods\nwith the flexibility of preference embedding. The key intuition is to embed the\ndata into a high-dimensional preference space by fitting low-dimensional\nmanifolds, and to identify anomalies as isolated points. We propose three\nisolation approaches to identify anomalies: $i$) Voronoi-iForest, the most\ngeneral solution, $ii$) RuzHash-iForest, that avoids explicit computation of\ndistances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a\nlocality prior to improve efficiency and effectiveness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.10923", "pdf": "https://arxiv.org/pdf/2505.10923", "abs": "https://arxiv.org/abs/2505.10923", "authors": ["Simeon Adebola", "Shuangyu Xie", "Chung Min Kim", "Justin Kerr", "Bart M. van Marrewijk", "Mieke van Vlaardingen", "Tim van Daalen", "Robert van Loo", "Jose Luis Susa Rincon", "Eugen Solowjow", "Rick van de Zedde", "Ken Goldberg"], "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate temporal reconstructions of plant growth are essential for plant\nphenotyping and breeding, yet remain challenging due to complex geometries,\nocclusions, and non-rigid deformations of plants. We present a novel framework\nfor building temporal digital twins of plants by combining 3D Gaussian\nSplatting with a robust sample alignment pipeline. Our method begins by\nreconstructing Gaussian Splats from multi-view camera data, then leverages a\ntwo-stage registration approach: coarse alignment through feature-based\nmatching and Fast Global Registration, followed by fine alignment with\nIterative Closest Point. This pipeline yields a consistent 4D model of plant\ndevelopment in discrete time steps. We evaluate the approach on data from the\nNetherlands Plant Eco-phenotyping Center, demonstrating detailed temporal\nreconstructions of Sequoia and Quinoa species. Videos and Images can be seen at\nhttps://berkeleyautomation.github.io/GrowSplat/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11032", "pdf": "https://arxiv.org/pdf/2505.11032", "abs": "https://arxiv.org/abs/2505.11032", "authors": ["Yuran Wang", "Ruihai Wu", "Yue Chen", "Jiarui Wang", "Jiaqi Liang", "Ziyu Zhu", "Haoran Geng", "Jitendra Malik", "Pieter Abbeel", "Hao Dong"], "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Garment manipulation is a critical challenge due to the diversity in garment\ncategories, geometries, and deformations. Despite this, humans can effortlessly\nhandle garments, thanks to the dexterity of our hands. However, existing\nresearch in the field has struggled to replicate this level of dexterity,\nprimarily hindered by the lack of realistic simulations of dexterous garment\nmanipulation. Therefore, we propose DexGarmentLab, the first environment\nspecifically designed for dexterous (especially bimanual) garment manipulation,\nwhich features large-scale high-quality 3D assets for 15 task scenarios, and\nrefines simulation techniques tailored for garment modeling to reduce the\nsim-to-real gap. Previous data collection typically relies on teleoperation or\ntraining expert reinforcement learning (RL) policies, which are labor-intensive\nand inefficient. In this paper, we leverage garment structural correspondence\nto automatically generate a dataset with diverse trajectories using only a\nsingle expert demonstration, significantly reducing manual intervention.\nHowever, even extensive demonstrations cannot cover the infinite states of\ngarments, which necessitates the exploration of new algorithms. To improve\ngeneralization across diverse garment shapes and deformations, we propose a\nHierarchical gArment-manipuLation pOlicy (HALO). It first identifies\ntransferable affordance points to accurately locate the manipulation area, then\ngenerates generalizable trajectories to complete the task. Through extensive\nexperiments and detailed analysis of our method and baseline, we demonstrate\nthat HALO consistently outperforms existing methods, successfully generalizing\nto previously unseen instances even with significant variations in shape and\ndeformation where others fail. Our project page is available at:\nhttps://wayrise.github.io/DexGarmentLab/.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vuliƒá"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-19.jsonl"}
