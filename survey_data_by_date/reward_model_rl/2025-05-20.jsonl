{"id": "2505.11777", "pdf": "https://arxiv.org/pdf/2505.11777", "abs": "https://arxiv.org/abs/2505.11777", "authors": ["Fu-Yun Wang", "Keqiang Sun", "Yao Teng", "Xihui Liu", "Jiaming Song", "Hongsheng Li"], "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable success in various visual\ngeneration tasks, including image, video, and 3D content generation. Preference\noptimization (PO) is a prominent and growing area of research that aims to\nalign these models with human preferences. While existing PO methods primarily\nconcentrate on producing favorable outputs, they often overlook the\nsignificance of classifier-free guidance (CFG) in mitigating undesirable\nresults. Diffusion-NPO addresses this gap by introducing negative preference\noptimization (NPO), training models to generate outputs opposite to human\npreferences and thereby steering them away from unfavorable outcomes. However,\nprior NPO approaches, including Diffusion-NPO, rely on costly and fragile\nprocedures for obtaining explicit preference annotations (e.g., manual pairwise\nlabeling or reward model training), limiting their practicality in domains\nwhere such data are scarce or difficult to acquire. In this work, we introduce\nSelf-NPO, a Negative Preference Optimization approach that learns exclusively\nfrom the model itself, thereby eliminating the need for manual data labeling or\nreward model training. Moreover, our method is highly efficient and does not\nrequire exhaustive data sampling. We demonstrate that Self-NPO integrates\nseamlessly into widely used diffusion models, including SD1.5, SDXL, and\nCogVideoX, as well as models already optimized for human preferences,\nconsistently enhancing both their generation quality and alignment with human\npreferences.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference", "pairwise", "alignment"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584", "abs": "https://arxiv.org/abs/2505.12584", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "preference dataset", "safety"], "score": 5}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "preference", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference learning", "preference", "DPO"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "DPO", "direct preference optimization"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "AI feedback"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "pairwise", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "preference"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison", "pairwise"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "reinforcement learning"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "reward model evaluation", "correlation"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11852", "pdf": "https://arxiv.org/pdf/2505.11852", "abs": "https://arxiv.org/abs/2505.11852", "authors": ["Jingkun Yue", "Siqi Zhang", "Zinan Jia", "Huihuan Xu", "Zongbo Han", "Xiaohong Liu", "Guangyu Wang"], "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12098", "pdf": "https://arxiv.org/pdf/2505.12098", "abs": "https://arxiv.org/abs/2505.12098", "authors": ["Jiarui Wang", "Huiyu Duan", "Ziheng Jia", "Yu Zhao", "Woo Yi Yang", "Zicheng Zhang", "Zijian Chen", "Juntong Wang", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12099", "pdf": "https://arxiv.org/pdf/2505.12099", "abs": "https://arxiv.org/abs/2505.12099", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to BMVC 2025. Code, models, and the captions for datasets\n  will be released", "summary": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "consistency", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723", "abs": "https://arxiv.org/abs/2505.12723", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization", "preference"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "pairwise"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "correlation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "pairwise"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Akyürek", "Ekin Akyürek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "self-correction"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13281", "pdf": "https://arxiv.org/pdf/2505.13281", "abs": "https://arxiv.org/abs/2505.13281", "authors": ["Zekun Wang", "Sashank Varma"], "title": "Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, CosSci 2025", "summary": "With the rapid improvement of machine learning (ML) models, cognitive\nscientists are increasingly asking about their alignment with how humans think.\nHere, we ask this question for computer vision models and human sensitivity to\ngeometric and topological (GT) concepts. Under the core knowledge account,\nthese concepts are innate and supported by dedicated neural circuitry. In this\nwork, we investigate an alternative explanation, that GT concepts are learned\n``for free'' through everyday interaction with the environment. We do so using\ncomputer visions models, which are trained on large image datasets. We build on\nprior studies to investigate the overall performance and human alignment of\nthree classes of models -- convolutional neural networks (CNNs),\ntransformer-based models, and vision-language models -- on an odd-one-out task\ntesting 43 GT concepts spanning seven classes. Transformer-based models achieve\nthe highest overall accuracy, surpassing that of young children. They also show\nstrong alignment with children's performance, finding the same classes of\nconcepts easy vs. difficult. By contrast, vision-language models underperform\ntheir vision-only counterparts and deviate further from human profiles,\nindicating that na\\\"ive multimodality might compromise abstract geometric\nsensitivity. These findings support the use of computer vision models to\nevaluate the sufficiency of the learning account for explaining human\nsensitivity to GT concepts, while also suggesting that integrating linguistic\nand visual representations might have unpredicted deleterious consequences.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "rubric"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "human preference", "correlation", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "test-time compute"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11676", "pdf": "https://arxiv.org/pdf/2505.11676", "abs": "https://arxiv.org/abs/2505.11676", "authors": ["Ziyu Zhao", "Xiaoguang Li", "Linjia Shi", "Nasrin Imanpour", "Song Wang"], "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Open-vocabulary semantic segmentation aims to segment images into distinct\nsemantic regions for both seen and unseen categories at the pixel level.\nCurrent methods utilize text embeddings from pre-trained vision-language models\nlike CLIP but struggle with the inherent domain gap between image and text\nembeddings, even after extensive alignment during training. Additionally,\nrelying solely on deep text-aligned features limits shallow-level feature\nguidance, which is crucial for detecting small objects and fine details,\nultimately reducing segmentation accuracy. To address these limitations, we\npropose a dual prompting framework, DPSeg, for this task. Our approach combines\ndual-prompt cost volume generation, a cost volume-guided decoder, and a\nsemantic-guided prompt refinement strategy that leverages our dual prompting\nscheme to mitigate alignment issues in visual prompt generation. By\nincorporating visual embeddings from a visual prompt encoder, our approach\nreduces the domain gap between text and image embeddings while providing\nmulti-level guidance through shallow features. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches on multiple public datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12619", "pdf": "https://arxiv.org/pdf/2505.12619", "abs": "https://arxiv.org/abs/2505.12619", "authors": ["Jiashun Wang", "Yifeng Jiang", "Haotian Zhang", "Chen Tessler", "Davis Rempe", "Jessica Hodgins", "Xue Bin Peng"], "title": "HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos", "categories": ["cs.GR"], "comment": "14 pages, 10 figures", "summary": "Recent data-driven methods leveraging deep reinforcement learning have been\nan effective paradigm for developing controllers that enable physically\nsimulated characters to produce natural human-like behaviors. However, these\ndata-driven methods often struggle to adapt to novel environments and compose\ndiverse skills coherently to perform more complex tasks. To address these\nchallenges, we propose a hybrid imitation learning (HIL) framework that\ncombines motion tracking, for precise skill replication, with adversarial\nimitation learning, to enhance adaptability and skill composition. This hybrid\nlearning framework is implemented through parallel multi-task environments and\na unified observation space, featuring an agent-centric scene representation to\nfacilitate effective learning from the hybrid parallel environments. Our\nframework trains a unified controller on parkour data sourced from Internet\nvideos, enabling a simulated character to traverse through new environments\nusing diverse and life-like parkour skills. Evaluations across challenging\nparkour environments demonstrate that our method improves motion quality,\nincreases skill diversity, and achieves competitive task completion compared to\nprevious learning-based methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11895", "pdf": "https://arxiv.org/pdf/2505.11895", "abs": "https://arxiv.org/abs/2505.11895", "authors": ["Chih-Ting Liao", "Bin Ren", "Guofeng Mei", "Xu Zheng"], "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922", "abs": "https://arxiv.org/abs/2505.11922", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-correction"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11945", "pdf": "https://arxiv.org/pdf/2505.11945", "abs": "https://arxiv.org/abs/2505.11945", "authors": ["Bonan li", "Zicheng Zhang", "Songhua Liu", "Weihao Yu", "Xinchao Wang"], "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958", "abs": "https://arxiv.org/abs/2505.11958", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965", "abs": "https://arxiv.org/abs/2505.11965", "authors": ["Xu Liu", "Guanyi Chen"], "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11997", "pdf": "https://arxiv.org/pdf/2505.11997", "abs": "https://arxiv.org/abs/2505.11997", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin", "Tonghua Su", "Yue Gao", "Yang Song", "Lei Fan"], "title": "Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance", "categories": ["cs.CV"], "comment": "Code: https://github.com/MCPathology/MRePath", "summary": "Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference-time"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12021", "pdf": "https://arxiv.org/pdf/2505.12021", "abs": "https://arxiv.org/abs/2505.12021", "authors": ["Kazuhiko Kawamoto", "Atsuhiro Endo", "Hiroshi Kera"], "title": "Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Task arithmetic enables efficient model editing by representing task-specific\nchanges as vectors in parameter space. Task arithmetic typically assumes that\nthe source and target models are initialized from the same pre-trained\nparameters. This assumption limits its applicability in cross-model transfer\nsettings, where models are independently pre-trained on different datasets. To\naddress this challenge, we propose a method based on few-shot orthogonal\nalignment, which aligns task vectors to the parameter space of a differently\npre-trained target model. These transformations preserve key properties of task\nvectors, such as norm and rank, and are learned using only a small number of\nlabeled examples. We evaluate the method using two Vision Transformers\npre-trained on YFCC100M and LAION400M, and test on eight classification\ndatasets. Experimental results show that our method improves transfer accuracy\nover direct task vector application and achieves performance comparable to\nfew-shot fine-tuning, while maintaining the modularity and reusability of task\nvectors. Our code is available at\nhttps://github.com/kawakera-lab/CrossModelTransfer.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12081", "pdf": "https://arxiv.org/pdf/2505.12081", "abs": "https://arxiv.org/abs/2505.12081", "authors": ["Yuqi Liu", "Tianyuan Qu", "Zhisheng Zhong", "Bohao Peng", "Shu Liu", "Bei Yu", "Jiaya Jia"], "title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency", "reliability", "kappa"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215", "abs": "https://arxiv.org/abs/2505.12215", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12200", "pdf": "https://arxiv.org/pdf/2505.12200", "abs": "https://arxiv.org/abs/2505.12200", "authors": ["Bohan Jia", "Wenxuan Huang", "Yuntian Tang", "Junbo Qiao", "Jincheng Liao", "Shaosheng Cao", "Fei Zhao", "Zhaopeng Feng", "Zhouhong Gu", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Lin Chen", "Fei Zhao", "Zihan Wang", "Yuan Xie", "Shaohui Lin"], "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259", "abs": "https://arxiv.org/abs/2505.12259", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "multi-dimensional"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12251", "pdf": "https://arxiv.org/pdf/2505.12251", "abs": "https://arxiv.org/abs/2505.12251", "authors": ["Haozhe Xiang", "Han Zhang", "Yu Cheng", "Xiongwen Quan", "Wanwan Huang"], "title": "SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal medical image fusion plays a crucial role in medical diagnosis by\nintegrating complementary information from different modalities to enhance\nimage readability and clinical applicability. However, existing methods mainly\nfollow computer vision standards for feature extraction and fusion strategy\nformulation, overlooking the rich semantic information inherent in medical\nimages. To address this limitation, we propose a novel semantic-guided medical\nimage fusion approach that, for the first time, incorporates medical prior\nknowledge into the fusion process. Specifically, we construct a publicly\navailable multimodal medical image-text dataset, upon which text descriptions\ngenerated by BiomedGPT are encoded and semantically aligned with image features\nin a high-dimensional space via a semantic interaction alignment module. During\nthis process, a cross attention based linear transformation automatically maps\nthe relationship between textual and visual features to facilitate\ncomprehensive learning. The aligned features are then embedded into a\ntext-injection module for further feature-level fusion. Unlike traditional\nmethods, we further generate diagnostic reports from the fused images to assess\nthe preservation of medical information. Additionally, we design a medical\nsemantic loss function to enhance the retention of textual cues from the source\nimages. Experimental results on test datasets demonstrate that the proposed\nmethod achieves superior performance in both qualitative and quantitative\nevaluations while preserving more critical medical information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12274", "pdf": "https://arxiv.org/pdf/2505.12274", "abs": "https://arxiv.org/abs/2505.12274", "authors": ["Yixiao Chen", "Zhiyuan Ma", "Guoli Jia", "Che Jiang", "Jianjun Li", "Bowen Zhou"], "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12340", "pdf": "https://arxiv.org/pdf/2505.12340", "abs": "https://arxiv.org/abs/2505.12340", "authors": ["Jirong Zha", "Yuxuan Fan", "Kai Li", "Han Li", "Chen Gao", "Xinlei Chen", "Yong Li"], "title": "DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking", "categories": ["cs.CV"], "comment": "10 pages", "summary": "State estimation is challenging for 3D object tracking with high\nmaneuverability, as the target's state transition function changes rapidly,\nirregularly, and is unknown to the estimator. Existing work based on\ninteracting multiple model (IMM) achieves more accurate estimation than\nsingle-filter approaches through model combination, aligning appropriate models\nfor different motion modes of the target object over time. However, two\nlimitations of conventional IMM remain unsolved. First, the solution space of\nthe model combination is constrained as the target's diverse kinematic\nproperties in different directions are ignored. Second, the model combination\nweights calculated by the observation likelihood are not accurate enough due to\nthe measurement uncertainty. In this paper, we propose a novel framework, DIMM,\nto effectively combine estimates from different motion models in each\ndirection, thus increasing the 3D object tracking accuracy. First, DIMM extends\nthe model combination solution space of conventional IMM from a hyperplane to a\nhypercube by designing a 3D-decoupled multi-hierarchy filter bank, which\ndescribes the target's motion with various-order linear models. Second, DIMM\ngenerates more reliable combination weight matrices through a differentiable\nadaptive fusion network for importance allocation rather than solely relying on\nthe observation likelihood; it contains an attention-based twin delayed deep\ndeterministic policy gradient (TD3) method with a hierarchical reward.\nExperiments demonstrate that DIMM significantly improves the tracking accuracy\nof existing state estimation methods by 31.61%~99.23%.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12391", "pdf": "https://arxiv.org/pdf/2505.12391", "abs": "https://arxiv.org/abs/2505.12391", "authors": ["Zhengyang Lu", "Qian Xia", "Weifan Wang", "Feng Wang"], "title": "CLIP-aware Domain-Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452", "abs": "https://arxiv.org/abs/2505.12452", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "fine-grained"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["monte carlo tree search", "MCTS"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12434", "pdf": "https://arxiv.org/pdf/2505.12434", "abs": "https://arxiv.org/abs/2505.12434", "authors": ["Qi Wang", "Yanrui Yu", "Ye Yuan", "Rui Mao", "Tianfei Zhou"], "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "categories": ["cs.CV"], "comment": "Code: https://github.com/QiWang98/VideoRFT", "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strength the RL phase, we introduce a novel semantic-consistency reward\nthat explicitly promotes the alignment between textual reasoning with visual\nevidence. This reward encourages the model to produce coherent, context-aware\nreasoning outputs grounded in visual input. Extensive experiments show that\nVIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "rubric"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12499", "pdf": "https://arxiv.org/pdf/2505.12499", "abs": "https://arxiv.org/abs/2505.12499", "authors": ["Jian Xiao", "Zijie Song", "Jialong Hu", "Hao Cheng", "Zhenzhen Hu", "Jia Li", "Richang Hong"], "title": "Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akın Yılmaz", "A. Murat Tekalp", "R. Gökberk Cinbiş"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12606", "pdf": "https://arxiv.org/pdf/2505.12606", "abs": "https://arxiv.org/abs/2505.12606", "authors": ["Shiyu Xuan", "Zechao Li", "Jinhui Tang"], "title": "Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object tracking integrates auxiliary modalities such as depth,\nthermal infrared, event flow, and language to provide additional information\nbeyond RGB images, showing great potential in improving tracking stabilization\nin complex scenarios. Existing methods typically start from an RGB-based\ntracker and learn to understand auxiliary modalities only from training data.\nConstrained by the limited multi-modal training data, the performance of these\nmethods is unsatisfactory. To alleviate this limitation, this work proposes a\nunified multi-modal tracker Diff-MM by exploiting the multi-modal understanding\ncapability of the pre-trained text-to-image generation model. Diff-MM leverages\nthe UNet of pre-trained Stable Diffusion as a tracking feature extractor\nthrough the proposed parallel feature extraction pipeline, which enables\npairwise image inputs for object tracking. We further introduce a multi-modal\nsub-module tuning method that learns to gain complementary information between\ndifferent modalities. By harnessing the extensive prior knowledge in the\ngeneration model, we achieve a unified tracker with uniform parameters for\nRGB-N/D/T/E tracking. Experimental results demonstrate the promising\nperformance of our method compared with recently proposed trackers, e.g., its\nAUC outperforms OneTracker by 8.3% on TNL2K.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12620", "pdf": "https://arxiv.org/pdf/2505.12620", "abs": "https://arxiv.org/abs/2505.12620", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan YU", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale", "o1"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12635", "pdf": "https://arxiv.org/pdf/2505.12635", "abs": "https://arxiv.org/abs/2505.12635", "authors": ["Mingqi Shao", "Feng Xiong", "Zhaoxu Sun", "Mu Xu"], "title": "MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control", "categories": ["cs.CV"], "comment": "Project page: https://amap-cvlab.github.io/MV-Painter", "summary": "Recently, significant advances have been made in 3D object generation.\nBuilding upon the generated geometry, current pipelines typically employ image\ndiffusion models to generate multi-view RGB images, followed by UV texture\nreconstruction through texture baking. While 3D geometry generation has\nimproved significantly, supported by multiple open-source frameworks, 3D\ntexture generation remains underexplored. In this work, we systematically\ninvestigate 3D texture generation through the lens of three core dimensions:\nreference-texture alignment, geometry-texture consistency, and local texture\nquality. To tackle these issues, we propose MVPainter, which employs data\nfiltering and augmentation strategies to enhance texture fidelity and detail,\nand introduces ControlNet-based geometric conditioning to improve\ntexture-geometry alignment. Furthermore, we extract physically-based rendering\n(PBR) attributes from the generated views to produce PBR meshes suitable for\nreal-world rendering applications. MVPainter achieves state-of-the-art results\nacross all three dimensions, as demonstrated by human-aligned evaluations. To\nfacilitate further research and reproducibility, we also release our full\npipeline as an open-source system, including data construction, model\narchitecture, and evaluation tools.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "consistency"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717", "abs": "https://arxiv.org/abs/2505.12717", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718", "abs": "https://arxiv.org/abs/2505.12718", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12656", "pdf": "https://arxiv.org/pdf/2505.12656", "abs": "https://arxiv.org/abs/2505.12656", "authors": ["Yongchang Gao", "Meiling Jin", "Zhaofei Yu", "Tiejun Huang", "Guozhang Chen"], "title": "SPKLIP: Aligning Spike Video Streams with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Spike cameras offer unique sensing capabilities but their sparse,\nasynchronous output challenges semantic understanding, especially for Spike\nVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due to\nmodality mismatch. We introduce SPKLIP, the first architecture specifically for\nSpike-VLA. SPKLIP employs a hierarchical spike feature extractor that\nadaptively models multi-scale temporal dynamics in event streams, and uses\nspike-text contrastive learning to directly align spike video with language,\nenabling effective few-shot learning. A full-spiking visual encoder variant,\nintegrating SNN components into our pipeline, demonstrates enhanced energy\nefficiency. Experiments show state-of-the-art performance on benchmark spike\ndatasets and strong few-shot generalization on a newly contributed real-world\ndataset. SPKLIP's energy efficiency highlights its potential for neuromorphic\ndeployment, advancing event-based multimodal research. The source code and\ndataset are available at [link removed for anonymity].", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12674", "pdf": "https://arxiv.org/pdf/2505.12674", "abs": "https://arxiv.org/abs/2505.12674", "authors": ["Mingyuan Zhou", "Yi Gu", "Zhendong Wang"], "title": "Few-Step Diffusion via Score identity Distillation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "consistency", "reliability", "fine-grained"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12693", "pdf": "https://arxiv.org/pdf/2505.12693", "abs": "https://arxiv.org/abs/2505.12693", "authors": ["Luyao Lei", "Shuo Xu", "Yifan Bai", "Xing Wei"], "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-modal 3D occupancy prediction is limited by\nineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion\nstrategies and surface detail loss caused by sparse, noisy annotations. The\nmismatch stems from the heterogeneous scale and distribution of point cloud and\nimage features, leading to biased matching under fixed neighborhood fusion. To\naddress this, we propose a target-scale adaptive, bidirectional symmetric\nretrieval mechanism. It expands the neighborhood for large targets to enhance\ncontext awareness and shrinks it for small ones to improve efficiency and\nsuppress noise, enabling accurate cross-modal feature alignment. This mechanism\nexplicitly establishes spatial correspondences and improves fusion accuracy.\nFor surface detail loss, sparse labels provide limited supervision, resulting\nin poor predictions for small objects. We introduce an improved volume\nrendering pipeline based on 3D Gaussian Splatting, which takes fused features\nas input to render images, applies photometric consistency supervision, and\njointly optimizes 2D-3D consistency. This enhances surface detail\nreconstruction while suppressing noise propagation. In summary, we propose\nTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy\nprediction, enhanced by volume rendering supervision. Experiments on the\nnuScenes and SemanticKITTI benchmarks validate its effectiveness.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12820", "pdf": "https://arxiv.org/pdf/2505.12820", "abs": "https://arxiv.org/abs/2505.12820", "authors": ["Hulin Li"], "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection", "categories": ["cs.CV"], "comment": "ECCV 2024", "summary": "Multi-head detectors typically employ a features-fused-pyramid-neck for\nmulti-scale detection and are widely adopted in the industry. However, this\napproach faces feature misalignment when representations from different\nhierarchical levels of the feature pyramid are forcibly fused point-to-point.\nTo address this issue, we designed an independent hierarchy pyramid (IHP)\narchitecture to evaluate the effectiveness of the features-unfused-pyramid-neck\nfor multi-head detectors. Subsequently, we introduced soft nearest neighbor\ninterpolation (SNI) with a weight downscaling factor to mitigate the impact of\nfeature fusion at different hierarchies while preserving key textures.\nFurthermore, we present a features adaptive selection method for down sampling\nin extended spatial windows (ESD) to retain spatial features and enhance\nlightweight convolutional techniques (GSConvE). These advancements culminate in\nour secondary features alignment solution (SA) for real-time detection,\nachieving state-of-the-art results on Pascal VOC and MS COCO. Code will be\nreleased at https://github.com/AlanLi1997/rethinking-fpn. This paper has been\naccepted by ECCV2024 and published on Springer Nature.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970", "abs": "https://arxiv.org/abs/2505.12970", "authors": ["Robin Jegan", "Andreas Henrich"], "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12849", "pdf": "https://arxiv.org/pdf/2505.12849", "abs": "https://arxiv.org/abs/2505.12849", "authors": ["Ben Liu", "Zhen Qin"], "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 5 tables", "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034", "abs": "https://arxiv.org/abs/2505.13034", "authors": ["Márton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12911", "pdf": "https://arxiv.org/pdf/2505.12911", "abs": "https://arxiv.org/abs/2505.12911", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Giuseppe Averta"], "title": "HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos", "categories": ["cs.CV"], "comment": "Project page https://github.com/sapeirone/hiero", "summary": "Human activities are particularly complex and variable, and this makes\nchallenging for deep learning models to reason about them. However, we note\nthat such variability does have an underlying structure, composed of a\nhierarchy of patterns of related actions. We argue that such structure can\nemerge naturally from unscripted videos of human activities, and can be\nleveraged to better reason about their content. We present HiERO, a\nweakly-supervised method to enrich video segments features with the\ncorresponding hierarchical activity threads. By aligning video clips with their\nnarrated descriptions, HiERO infers contextual, semantic and temporal reasoning\nwith an hierarchical architecture. We prove the potential of our enriched\nfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with\nminimal additional training, and in zero-shot for procedure learning tasks\n(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art\nperformance in all the benchmarks, and for procedure learning tasks it\noutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)\nin zero shot. Our results prove the relevance of using knowledge of the\nhierarchy of human activities for multiple reasoning tasks in egocentric\nvision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "consistency"], "score": 4}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13088", "pdf": "https://arxiv.org/pdf/2505.13088", "abs": "https://arxiv.org/abs/2505.13088", "authors": ["Zhaoyi Wang", "Shengyu Huang", "Jemil Avers Butt", "Yuanzhou Cai", "Matej Varga", "Andreas Wieser"], "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.\n  19 pages, 14 figures", "summary": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13091", "pdf": "https://arxiv.org/pdf/2505.13091", "abs": "https://arxiv.org/abs/2505.13091", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204", "abs": "https://arxiv.org/abs/2505.13204", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "summarization", "question answering"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13233", "pdf": "https://arxiv.org/pdf/2505.13233", "abs": "https://arxiv.org/abs/2505.13233", "authors": ["Lincan Cai", "Jingxuan Kang", "Shuang Li", "Wenxuan Ma", "Binhui Xie", "Zhida Qin", "Jian Liang"], "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time adaptation"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282", "abs": "https://arxiv.org/abs/2505.13282", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13261", "pdf": "https://arxiv.org/pdf/2505.13261", "abs": "https://arxiv.org/abs/2505.13261", "authors": ["Mingrui Chen", "Haogeng Liu", "Hao Liang", "Huaibo Huang", "Wentao Zhang", "Ran He"], "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we investigate how explicitly modeling problem's difficulty\nprior information shapes the effectiveness of reinforcement learning based\nfine-tuning for multimodal reasoning. Our exploration mainly comprises of\nfollowing three perspective: First, through offline data curation, we analyze\nthe U-shaped difficulty distribution of two given datasets using the base model\nby multi-round sampling, and then filter out prompts that are either too simple\nor extremely difficult to provide meaningful gradients and perform subsequent\ntwo-stage training. Second, we implement an online advantage differentiation,\ncomputing group-wise empirical accuracy as a difficulty proxy to adaptively\nreweight advantages estimation, providing stronger learning signals for more\nchallenging problems. Finally, we introduce difficulty hints as explicit\nprompts for more complex samples in the second training stage, encouraging the\nmodel to calibrate its reasoning depth and perform reflective validation\nchecks. Our comprehensive approach demonstrates significant performances across\nvarious multi-modal mathematical reasoning benchmarks with only 2K+0.6K\ntwo-stage training data.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13279", "pdf": "https://arxiv.org/pdf/2505.13279", "abs": "https://arxiv.org/abs/2505.13279", "authors": ["Zhiqiang Yan", "Jianhao Jiao", "Zhengxue Wang", "Gim Hee Lee"], "title": "Event-Driven Dynamic Scene Depth Completion", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13300", "pdf": "https://arxiv.org/pdf/2505.13300", "abs": "https://arxiv.org/abs/2505.13300", "authors": ["Zekai Li", "Xinhao Zhong", "Samir Khaki", "Zhiyuan Liang", "Yuhao Zhou", "Mingjia Shi", "Ziqiao Wang", "Xuanlei Zhao", "Wangbo Zhao", "Ziheng Qin", "Mengxuan Wu", "Pengfei Zhou", "Haonan Wang", "David Junhao Zhang", "Jia-Wei Liu", "Shaobo Wang", "Dai Liu", "Linfeng Zhang", "Guang Li", "Kun Wang", "Zheng Zhu", "Zhiheng Ma", "Joey Tianyi Zhou", "Jiancheng Lv", "Yaochu Jin", "Peihao Wang", "Kaipeng Zhang", "Lingjuan Lyu", "Yiran Huang", "Zeynep Akata", "Zhiwei Deng", "Xindi Wu", "George Cazenavette", "Yuzhang Shang", "Justin Cui", "Jindong Gu", "Qian Zheng", "Hao Ye", "Shuo Wang", "Xiaobo Wang", "Yan Yan", "Angela Yao", "Mike Zheng Shou", "Tianlong Chen", "Hakan Bilen", "Baharan Mirzasoleiman", "Manolis Kellis", "Konstantinos N. Plataniotis", "Zhangyang Wang", "Bo Zhao", "Yang You", "Kai Wang"], "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "categories": ["cs.CV"], "comment": "20 pages, 4 figures", "summary": "In recent years, dataset distillation has provided a reliable solution for\ndata compression, where models trained on the resulting smaller synthetic\ndatasets achieve performance comparable to those trained on the original\ndatasets. To further improve the performance of synthetic datasets, various\ntraining pipelines and optimization objectives have been proposed, greatly\nadvancing the field of dataset distillation. Recent decoupled dataset\ndistillation methods introduce soft labels and stronger data augmentation\nduring the post-evaluation phase and scale dataset distillation up to larger\ndatasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy\nstill a reliable metric to fairly evaluate dataset distillation methods? Our\nempirical findings suggest that the performance improvements of these methods\noften stem from additional techniques rather than the inherent quality of the\nimages themselves, with even randomly sampled images achieving superior\nresults. Such misaligned evaluation settings severely hinder the development of\nDD. Therefore, we propose DD-Ranking, a unified evaluation framework, along\nwith new general evaluation metrics to uncover the true performance\nimprovements achieved by different methods. By refocusing on the actual\ninformation enhancement of distilled datasets, DD-Ranking provides a more\ncomprehensive and fair evaluation standard for future research advancements.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13306", "pdf": "https://arxiv.org/pdf/2505.13306", "abs": "https://arxiv.org/abs/2505.13306", "authors": ["Chengsong Sun", "Weiping Li", "Xiang Li", "Yuankun Liu", "Lianlei Shan"], "title": "GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference-time", "scaling", "reasoning model"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "annotation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13426", "pdf": "https://arxiv.org/pdf/2505.13426", "abs": "https://arxiv.org/abs/2505.13426", "authors": ["Liang Chen", "Hongcheng Gao", "Tianyu Liu", "Zhiqi Huang", "Flood Sung", "Xinyu Zhou", "Yuxin Wu", "Baobao Chang"], "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning", "categories": ["cs.CV"], "comment": "21 pages, 14 figures, code released at\n  https://github.com/chenllliang/G1", "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability", "accuracy"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12278", "pdf": "https://arxiv.org/pdf/2505.12278", "abs": "https://arxiv.org/abs/2505.12278", "authors": ["Zhengyi Luo", "Chen Tessler", "Toru Lin", "Ye Yuan", "Tairan He", "Wenli Xiao", "Yunrong Guo", "Gal Chechik", "Kris Kitani", "Linxi Fan", "Yuke Zhu"], "title": "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://zhengyiluo.github.io/PDC", "summary": "Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12322", "pdf": "https://arxiv.org/pdf/2505.12322", "abs": "https://arxiv.org/abs/2505.12322", "authors": ["Ali Gholamzadeh", "Noor Sajid"], "title": "Model alignment using inter-modal bridges", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["multi-dimensional"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "correlation"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scaling", "scaling law"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy gradient"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13414", "pdf": "https://arxiv.org/pdf/2505.13414", "abs": "https://arxiv.org/abs/2505.13414", "authors": ["Yaqian Chen", "Hanxue Gu", "Haoyu Dong", "Qihang Li", "Yuwen Chen", "Nicholas Konz", "Lin Li", "Maciej A. Mazurowski"], "title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-20.jsonl"}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["multi-step reasoning", "monte carlo tree search", "MCTS"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning", "fine-grained"], "score": 3}}, "source_file": "2025-05-20.jsonl"}
