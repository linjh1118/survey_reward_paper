{"id": "2505.13487", "pdf": "https://arxiv.org/pdf/2505.13487", "abs": "https://arxiv.org/abs/2505.13487", "authors": ["Ashwin Kumar", "Yuzi He", "Aram H. Markosyan", "Bobbie Chern", "Imanol Arrieta-Ibarra"], "title": "Detecting Prefix Bias in LLM-based Reward Models", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Human Feedback (RLHF) has emerged as a key\nparadigm for task-specific fine-tuning of language models using human\npreference data. While numerous publicly available preference datasets provide\npairwise comparisons of responses, the potential for biases in the resulting\nreward models remains underexplored. In this work, we introduce novel methods\nto detect and evaluate prefix bias -- a systematic shift in model preferences\ntriggered by minor variations in query prefixes -- in LLM-based reward models\ntrained on such datasets. We leverage these metrics to reveal significant\nbiases in preference models across racial and gender dimensions. Our\ncomprehensive evaluation spans diverse open-source preference datasets and\nreward model architectures, demonstrating susceptibility to this kind of bias\nregardless of the underlying model architecture. Furthermore, we propose a data\naugmentation strategy to mitigate these biases, showing its effectiveness in\nreducing the impact of prefix bias. Our findings highlight the critical need\nfor bias-aware dataset design and evaluation in developing fair and reliable\nreward models, contributing to the broader discourse on fairness in AI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "RLHF", "human feedback", "reinforcement learning", "preference", "pairwise"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14185", "pdf": "https://arxiv.org/pdf/2505.14185", "abs": "https://arxiv.org/abs/2505.14185", "authors": ["Kaustubh Ponkshe", "Shaan Shah", "Raghav Singhal", "Praneeth Vepakomma"], "title": "Safety Subspaces are Not Distinct: A Fine-Tuning Case Study", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Kaustubh Ponkshe, Shaan Shah, and Raghav Singhal contributed equally\n  to this work", "summary": "Large Language Models (LLMs) rely on safety alignment to produce socially\nacceptable responses. This is typically achieved through instruction tuning and\nreinforcement learning from human feedback. However, this alignment is known to\nbe brittle: further fine-tuning, even on benign or lightly contaminated data,\ncan degrade safety and reintroduce harmful behaviors. A growing body of work\nsuggests that alignment may correspond to identifiable geometric directions in\nweight space, forming subspaces that could, in principle, be isolated or\npreserved to defend against misalignment. In this work, we conduct a\ncomprehensive empirical study of this geometric perspective. We examine whether\nsafety-relevant behavior is concentrated in specific subspaces, whether it can\nbe separated from general-purpose learning, and whether harmfulness arises from\ndistinguishable patterns in internal representations. Across both parameter and\nactivation space, our findings are consistent: subspaces that amplify safe\nbehaviors also amplify unsafe ones, and prompts with different safety\nimplications activate overlapping representations. We find no evidence of a\nsubspace that selectively governs safety. These results challenge the\nassumption that alignment is geometrically localized. Rather than residing in\ndistinct directions, safety appears to emerge from entangled, high-impact\ncomponents of the model's broader learning dynamics. This suggests that\nsubspace-based defenses may face fundamental limitations and underscores the\nneed for alternative strategies to preserve alignment under continued training.\nWe corroborate these findings through multiple experiments on five open-source\nLLMs. Our code is publicly available at:\nhttps://github.com/CERT-Lab/safety-subspaces.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning from human feedback", "human feedback", "reinforcement learning", "alignment"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14264", "pdf": "https://arxiv.org/pdf/2505.14264", "abs": "https://arxiv.org/abs/2505.14264", "authors": ["Jian Xiong", "Jingbo Zhou", "Jingyong Ye", "Dejing Dou"], "title": "AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum", "categories": ["cs.LG", "cs.CL"], "comment": "14 pages, 7 figures", "summary": "Reinforcement learning (RL) has emerged as an effective approach for\nenhancing the reasoning capabilities of large language models (LLMs),\nespecially in scenarios where supervised fine-tuning (SFT) falls short due to\nlimited chain-of-thought (CoT) data. Among RL-based post-training methods,\ngroup relative advantage estimation, as exemplified by Group Relative Policy\nOptimization (GRPO), has attracted considerable attention for eliminating the\ndependency on the value model, thereby simplifying training compared to\ntraditional approaches like Proximal Policy Optimization (PPO). However, we\nobserve that exsiting group relative advantage estimation method still suffers\nfrom training inefficiencies, particularly when the estimated advantage\napproaches zero. To address this limitation, we propose Advantage-Augmented\nPolicy Optimization (AAPO), a novel RL algorithm that optimizes the\ncross-entropy (CE) loss using advantages enhanced through a momentum-based\nestimation scheme. This approach effectively mitigates the inefficiencies\nassociated with group relative advantage estimation. Experimental results on\nmultiple mathematical reasoning benchmarks demonstrate the superior performance\nof AAPO.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 4}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973", "abs": "https://arxiv.org/abs/2505.13973", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14682", "pdf": "https://arxiv.org/pdf/2505.14682", "abs": "https://arxiv.org/abs/2505.14682", "authors": ["Rui Tian", "Mingfei Gao", "Mingze Xu", "Jiaming Hu", "Jiasen Lu", "Zuxuan Wu", "Yinfei Yang", "Afshin Dehghan"], "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": "Technical report", "summary": "We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test time", "scaling"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "direct preference optimization"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14279", "pdf": "https://arxiv.org/pdf/2505.14279", "abs": "https://arxiv.org/abs/2505.14279", "authors": ["Jennifer D'Souza", "Hamed Babaei Giglou", "Quentin MÃ¼nch"], "title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, Accepted as a Long Paper at the 63rd Annual\n  Meeting of the Association for Computational Linguistics (ACL 2025)", "summary": "Large Language Models (LLMs) drive scientific question-answering on modern\nsearch engines, yet their evaluation robustness remains underexplored. We\nintroduce YESciEval, an open-source framework that combines fine-grained\nrubric-based assessment with reinforcement learning to mitigate optimism bias\nin LLM evaluators. We release multidisciplinary scienceQ&A datasets, including\nadversarial variants, with evaluation scores from multiple LLMs. Independent of\nproprietary models and human feedback, our approach enables scalable, cost-free\nevaluation. By advancing reliable LLM-as-a-judge models, this work supports AI\nalignment and fosters robust, transparent evaluation essential for scientific\ninquiry and artificial general intelligence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "reinforcement learning", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "question answering", "fine-grained", "rubric"], "score": 4}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973", "abs": "https://arxiv.org/abs/2505.13973", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization", "alignment"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13878", "pdf": "https://arxiv.org/pdf/2505.13878", "abs": "https://arxiv.org/abs/2505.13878", "authors": ["Yanggan Gu", "Zhaoyi Yan", "Yuanyi Wang", "Yiming Zhang", "Qi Zhou", "Fei Wu", "Hongxia Yang"], "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages", "summary": "Model fusion combines multiple Large Language Models (LLMs) with different\nstrengths into a more powerful, integrated model through lightweight training\nmethods. Existing works on model fusion focus primarily on supervised\nfine-tuning (SFT), leaving preference alignment (PA) --a critical phase for\nenhancing LLM performance--largely unexplored. The current few fusion methods\non PA phase, like WRPO, simplify the process by utilizing only response outputs\nfrom source models while discarding their probability information. To address\nthis limitation, we propose InfiFPO, a preference optimization method for\nimplicit model fusion. InfiFPO replaces the reference model in Direct\nPreference Optimization (DPO) with a fused source model that synthesizes\nmulti-source probabilities at the sequence level, circumventing complex\nvocabulary alignment challenges in previous works and meanwhile maintaining the\nprobability information. By introducing probability clipping and max-margin\nfusion strategies, InfiFPO enables the pivot model to align with human\npreferences while effectively distilling knowledge from source models.\nComprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO\nconsistently outperforms existing model fusion and preference optimization\nmethods. When using Phi-4 as the pivot model, InfiFPO improve its average\nperformance from 79.95 to 83.33 on 11 benchmarks, significantly improving its\ncapabilities in mathematics, coding, and reasoning tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment", "DPO"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13500", "pdf": "https://arxiv.org/pdf/2505.13500", "abs": "https://arxiv.org/abs/2505.13500", "authors": ["Prithviraj Singh Shahani", "Matthias Scheutz"], "title": "Noise Injection Systemically Degrades Large Language Model Safety Guardrails", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages,3 figures", "summary": "Safety guardrails in large language models (LLMs) are a critical component in\npreventing harmful outputs. Yet, their resilience under perturbation remains\npoorly understood. In this paper, we investigate the robustness of safety\nfine-tuning in LLMs by systematically injecting Gaussian noise into model\nactivations. We show across multiple open-weight models that (1) Gaussian noise\nraises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety\nfine-tuning affords no extra protection, and (3) that chain-of-thought\nreasoning remains largely intact. The findings reveal critical vulnerabilities\nin current safety alignment techniques and highlight the potential of\nreasoning-based and reinforcement learning approaches as promising direction\nfor developing more robust AI safety systems. These results have important\nimplications for real-world deployment of LLMs in safety-critical applications\nas these results imply that widely-deployed safety tuning methods can fail even\nwithout adversarial prompts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14197", "pdf": "https://arxiv.org/pdf/2505.14197", "abs": "https://arxiv.org/abs/2505.14197", "authors": ["Xinshen Zhang", "Zhen Ye", "Xu Zheng"], "title": "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method", "categories": ["cs.CV"], "comment": null, "summary": "Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide\nunparalleled spatial awareness for immersive applications like augmented\nreality and embodied AI. However, the capability of existing multi-modal large\nlanguage models (MLLMs) to comprehend and reason about such panoramic scenes\nremains underexplored. This paper addresses this gap by introducing OmniVQA,\nthe first dataset and conducting the first benchmark for omnidirectional visual\nquestion answering. Our evaluation of state-of-the-art MLLMs reveals\nsignificant limitations in handling omnidirectional visual question answering,\nhighlighting persistent challenges in object localization, feature extraction,\nand hallucination suppression within panoramic contexts. These results\nunderscore the disconnect between current MLLM capabilities and the demands of\nomnidirectional visual understanding, which calls for dedicated architectural\nor training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA\ndataset and benchmark, we further introduce a rule-based reinforcement learning\nmethod, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group\nrelative policy optimization (GRPO) by proposing three novel reward functions:\n(1) reasoning process similarity reward, (2) answer semantic accuracy reward,\nand (3) structured format compliance reward. Extensive experiments on our\nOmniVQA demonstrate the superiority of our proposed method in omnidirectional\nspace (+6% improvement).", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy", "question answering"], "score": 5}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13965", "pdf": "https://arxiv.org/pdf/2505.13965", "abs": "https://arxiv.org/abs/2505.13965", "authors": ["Jiamin Su", "Yibo Yan", "Zhuoran Gao", "Han Zhang", "Xiang Liu", "Xuming Hu"], "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.11916", "summary": "Automated Essay Scoring (AES) is crucial for modern education, particularly\nwith the increasing prevalence of multimodal assessments. However, traditional\nAES methods struggle with evaluation generalizability and multimodal\nperception, while even recent Multimodal Large Language Model (MLLM)-based\napproaches can produce hallucinated justifications and scores misaligned with\nhuman judgment. To address the limitations, we introduce CAFES, the first\ncollaborative multi-agent framework specifically designed for AES. It\norchestrates three specialized agents: an Initial Scorer for rapid,\ntrait-specific evaluations; a Feedback Pool Manager to aggregate detailed,\nevidence-grounded strengths; and a Reflective Scorer that iteratively refines\nscores based on this feedback to enhance human alignment. Extensive\nexperiments, using state-of-the-art MLLMs, achieve an average relative\nimprovement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,\nespecially for grammatical and lexical diversity. Our proposed CAFES framework\npaves the way for an intelligent multimodal AES system. The code will be\navailable upon acceptance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "human alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "kappa"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14460", "pdf": "https://arxiv.org/pdf/2505.14460", "abs": "https://arxiv.org/abs/2505.14460", "authors": ["Tianhe Wu", "Jian Zou", "Jie Liang", "Lei Zhang", "Kede Ma"], "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank", "categories": ["cs.CV"], "comment": null, "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14268", "pdf": "https://arxiv.org/pdf/2505.14268", "abs": "https://arxiv.org/abs/2505.14268", "authors": ["Hui Huang", "Yancheng He", "Hongli Zhou", "Rui Zhang", "Wei Liu", "Weixun Wang", "Wenbo Su", "Bo Zheng", "Jiaheng Liu"], "title": "Think-J: Learning to Think for Generative LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 14 figures", "summary": "LLM-as-a-Judge refers to the automatic modeling of preferences for responses\ngenerated by Large Language Models (LLMs), which is of significant importance\nfor both LLM evaluation and reward modeling. Although generative LLMs have made\nsubstantial progress in various tasks, their performance as LLM-Judge still\nfalls short of expectations. In this work, we propose Think-J, which improves\ngenerative LLM-as-a-Judge by learning how to think. We first utilized a small\namount of curated data to develop the model with initial judgment thinking\ncapabilities. Subsequently, we optimize the judgment thinking traces based on\nreinforcement learning (RL). We propose two methods for judgment thinking\noptimization, based on offline and online RL, respectively. The offline RL\nrequires training a critic model to construct positive and negative examples\nfor learning. The online method defines rule-based reward as feedback for\noptimization. Experimental results showed that our approach can significantly\nenhance the evaluation capability of generative LLM-Judge, surpassing both\ngenerative and classifier-based LLM-Judge without requiring extra human\nannotations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14442", "pdf": "https://arxiv.org/pdf/2505.14442", "abs": "https://arxiv.org/abs/2505.14442", "authors": ["Mete Ismayilzada", "Antonio Laverghetta Jr.", "Simone A. Luchini", "Reet Patel", "Antoine Bosselut", "Lonneke van der Plas", "Roger Beaty"], "title": "Creative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "While Large Language Models (LLMs) have demonstrated impressive performance\nacross natural language generation tasks, their ability to generate truly\ncreative content-characterized by novelty, diversity, surprise, and\nquality-remains limited. Existing methods for enhancing LLM creativity often\nfocus narrowly on diversity or specific tasks, failing to address creativity's\nmultifaceted nature in a generalizable way. In this work, we propose Creative\nPreference Optimization (CrPO), a novel alignment method that injects signals\nfrom multiple creativity dimensions into the preference optimization objective\nin a modular fashion. We train and evaluate creativity-augmented versions of\nseveral models using CrPO and MuCE, a new large-scale human preference dataset\nspanning over 200,000 human-generated responses and ratings from more than 30\npsychological creativity assessments. Our models outperform strong baselines,\nincluding GPT-4o, on both automated and human evaluations, producing more\nnovel, diverse, and surprising generations while maintaining high output\nquality. Additional evaluations on NoveltyBench further confirm the\ngeneralizability of our approach. Together, our results demonstrate that\ndirectly optimizing for creativity within preference frameworks is a promising\ndirection for advancing the creative capabilities of LLMs without compromising\noutput quality.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "preference dataset", "human preference"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14631", "pdf": "https://arxiv.org/pdf/2505.14631", "abs": "https://arxiv.org/abs/2505.14631", "authors": ["Lingjie Jiang", "Xun Wu", "Shaohan Huang", "Qingxiu Dong", "Zewen Chi", "Li Dong", "Xingxing Zhang", "Tengchao Lv", "Lei Cui", "Furu Wei"], "title": "Think Only When You Need with Large Hybrid-Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) have shown substantially improved\nreasoning capabilities over traditional Large Language Models (LLMs) by\nincorporating extended thinking processes prior to producing final responses.\nHowever, excessively lengthy thinking introduces substantial overhead in terms\nof token consumption and latency, which is particularly unnecessary for simple\nqueries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the\nfirst kind of model capable of adaptively determining whether to perform\nthinking based on the contextual information of user queries. To achieve this,\nwe propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as\na cold start, followed by online reinforcement learning with the proposed\nHybrid Group Policy Optimization (HGPO) to implicitly learn to select the\nappropriate thinking mode. Furthermore, we introduce a metric called Hybrid\nAccuracy to quantitatively assess the model's capability for hybrid thinking.\nExtensive experimental results show that LHRMs can adaptively perform hybrid\nthinking on queries of varying difficulty and type. It outperforms existing\nLRMs and LLMs in reasoning and general capabilities while significantly\nimproving efficiency. Together, our work advocates for a reconsideration of the\nappropriate use of extended thinking processes and provides a solid starting\npoint for building hybrid thinking systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14674", "pdf": "https://arxiv.org/pdf/2505.14674", "abs": "https://arxiv.org/abs/2505.14674", "authors": ["Jiaxin Guo", "Zewen Chi", "Li Dong", "Qingxiu Dong", "Xun Wu", "Shaohan Huang", "Furu Wei"], "title": "Reward Reasoning Model", "categories": ["cs.CL"], "comment": null, "summary": "Reward models play a critical role in guiding large language models toward\noutputs that align with human expectations. However, an open challenge remains\nin effectively utilizing test-time compute to enhance reward model performance.\nIn this work, we introduce Reward Reasoning Models (RRMs), which are\nspecifically designed to execute a deliberate reasoning process before\ngenerating final rewards. Through chain-of-thought reasoning, RRMs leverage\nadditional test-time compute for complex queries where appropriate rewards are\nnot immediately apparent. To develop RRMs, we implement a reinforcement\nlearning framework that fosters self-evolved reward reasoning capabilities\nwithout requiring explicit reasoning traces as training data. Experimental\nresults demonstrate that RRMs achieve superior performance on reward modeling\nbenchmarks across diverse domains. Notably, we show that RRMs can adaptively\nexploit test-time compute to further improve reward accuracy. The pretrained\nreward reasoning models are available at\nhttps://huggingface.co/Reward-Reasoning.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "test-time compute", "reasoning model"], "score": 3}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reward modeling"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14680", "pdf": "https://arxiv.org/pdf/2505.14680", "abs": "https://arxiv.org/abs/2505.14680", "authors": ["Sunhao Dai", "Wenjie Wang", "Liang Pang", "Jun Xu", "See-Kiong Ng", "Ji-Rong Wen", "Tat-Seng Chua"], "title": "NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "comment": "SIGIR 2025 Perspective Paper", "summary": "Generative AI search is reshaping information retrieval by offering\nend-to-end answers to complex queries, reducing users' reliance on manually\nbrowsing and summarizing multiple web pages. However, while this paradigm\nenhances convenience, it disrupts the feedback-driven improvement loop that has\nhistorically powered the evolution of traditional Web search. Web search can\ncontinuously improve their ranking models by collecting large-scale,\nfine-grained user feedback (e.g., clicks, dwell time) at the document level. In\ncontrast, generative AI search operates through a much longer search pipeline,\nspanning query decomposition, document retrieval, and answer generation, yet\ntypically receives only coarse-grained feedback on the final answer. This\nintroduces a feedback loop disconnect, where user feedback for the final output\ncannot be effectively mapped back to specific system components, making it\ndifficult to improve each intermediate stage and sustain the feedback loop. In\nthis paper, we envision NExT-Search, a next-generation paradigm designed to\nreintroduce fine-grained, process-level feedback into generative AI search.\nNExT-Search integrates two complementary modes: User Debug Mode, which allows\nengaged users to intervene at key stages; and Shadow User Mode, where a\npersonalized user agent simulates user preferences and provides AI-assisted\nfeedback for less interactive users. Furthermore, we envision how these\nfeedback signals can be leveraged through online adaptation, which refines\ncurrent search outputs in real-time, and offline update, which aggregates\ninteraction logs to periodically fine-tune query decomposition, retrieval, and\ngeneration models. By restoring human control over key stages of the generative\nAI search pipeline, we believe NExT-Search offers a promising direction for\nbuilding feedback-rich AI search systems that can evolve continuously alongside\nhuman feedback.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["human feedback", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13731", "pdf": "https://arxiv.org/pdf/2505.13731", "abs": "https://arxiv.org/abs/2505.13731", "authors": ["Pengyue Jia", "Seongheon Park", "Song Gao", "Xiangyu Zhao", "Yixuan Li"], "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization", "categories": ["cs.CV"], "comment": null, "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13746", "pdf": "https://arxiv.org/pdf/2505.13746", "abs": "https://arxiv.org/abs/2505.13746", "authors": ["Satoshi Kondo"], "title": "ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Surgical phase recognition from video is a technology that automatically\nclassifies the progress of a surgical procedure and has a wide range of\npotential applications, including real-time surgical support, optimization of\nmedical resources, training and skill assessment, and safety improvement.\nRecent advances in surgical phase recognition technology have focused primarily\non Transform-based methods, although methods that extract spatial features from\nindividual frames using a CNN and video features from the resulting time series\nof spatial features using time series modeling have shown high performance.\nHowever, there remains a paucity of research on training methods for CNNs\nemployed for feature extraction or representation learning in surgical phase\nrecognition. In this study, we propose a method for representation learning in\nsurgical workflow analysis using a vision-language model (ReSW-VL). Our\nproposed method involves fine-tuning the image encoder of a CLIP (Convolutional\nLanguage Image Model) vision-language model using prompt learning for surgical\nphase recognition. The experimental results on three surgical phase recognition\ndatasets demonstrate the effectiveness of the proposed method in comparison to\nconventional methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13488", "pdf": "https://arxiv.org/pdf/2505.13488", "abs": "https://arxiv.org/abs/2505.13488", "authors": ["Federico Germani", "Giovanni Spitale"], "title": "Source framing triggers systematic evaluation bias in Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used not only to generate text\nbut also to evaluate it, raising urgent questions about whether their judgments\nare consistent, unbiased, and robust to framing effects. In this study, we\nsystematically examine inter- and intra-model agreement across four\nstate-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and\nMistral) tasked with evaluating 4,800 narrative statements on 24 different\ntopics of social, political, and public health relevance, for a total of\n192,000 assessments. We manipulate the disclosed source of each statement to\nassess how attribution to either another LLM or a human author of specified\nnationality affects evaluation outcomes. We find that, in the blind condition,\ndifferent LLMs display a remarkably high degree of inter- and intra-model\nagreement across topics. However, this alignment breaks down when source\nframing is introduced. Here we show that attributing statements to Chinese\nindividuals systematically lowers agreement scores across all models, and in\nparticular for Deepseek Reasoner. Our findings reveal that framing effects can\ndeeply affect text evaluation, with significant implications for the integrity,\nneutrality, and fairness of LLM-mediated information systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "agreement"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13856", "pdf": "https://arxiv.org/pdf/2505.13856", "abs": "https://arxiv.org/abs/2505.13856", "authors": ["Ruqin Zhou", "San Jiang", "Wanshou Jiang", "Yongsheng Zhang", "Chenguang Dai"], "title": "SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction", "categories": ["cs.CV"], "comment": "13 pages, 9 figures", "summary": "Vectorized HD map is essential for autonomous driving. Remarkable work has\nbeen achieved in recent years, but there are still major issues: (1) in the\ngeneration of the BEV features, single modality-based methods are of limited\nperception capability, while direct concatenation-based multi-modal methods\nfail to capture synergies and disparities between different modalities,\nresulting in limited ranges with feature holes; (2) in the classification and\nlocalization of map elements, only point information is used without the\nconsideration of element infor-mation and neglects the interaction between\npoint information and element information, leading to erroneous shapes and\nelement entanglement with low accuracy. To address above issues, we introduce\nSuperMapNet for long-range and high-accuracy vectorized HD map construction. It\nuses both camera images and LiDAR point clouds as input, and first tightly\ncouple semantic information from camera images and geometric information from\nLiDAR point clouds by a cross-attention based synergy enhancement module and a\nflow-based disparity alignment module for long-range BEV feature generation.\nAnd then, local features from point queries and global features from element\nqueries are tightly coupled by three-level interactions for high-accuracy\nclassification and localization, where Point2Point interaction learns local\ngeometric information between points of the same element and of each point,\nElement2Element interaction learns relation constraints between different\nelements and semantic information of each elements, and Point2Element\ninteraction learns complement element information for its constituent points.\nExperiments on the nuScenes and Argoverse2 datasets demonstrate superior\nperformances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP under\nhard/easy settings, respectively. The code is made publicly available1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13491", "pdf": "https://arxiv.org/pdf/2505.13491", "abs": "https://arxiv.org/abs/2505.13491", "authors": ["Aakash Gupta", "Nataraj Das"], "title": "ProdRev: A DNN framework for empowering customers using generative pre-trained transformers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2022 International Conference on Decision Aid Sciences and\n  Applications (DASA)", "summary": "Following the pandemic, customers, preference for using e-commerce has\naccelerated. Since much information is available in multiple reviews (sometimes\nrunning in thousands) for a single product, it can create decision paralysis\nfor the buyer. This scenario disempowers the consumer, who cannot be expected\nto go over so many reviews since its time consuming and can confuse them.\nVarious commercial tools are available, that use a scoring mechanism to arrive\nat an adjusted score. It can alert the user to potential review manipulations.\nThis paper proposes a framework that fine-tunes a generative pre-trained\ntransformer to understand these reviews better. Furthermore, using\n\"common-sense\" to make better decisions. These models have more than 13 billion\nparameters. To fine-tune the model for our requirement, we use the curie engine\nfrom generative pre-trained transformer (GPT3). By using generative models, we\nare introducing abstractive summarization. Instead of using a simple extractive\nmethod of summarizing the reviews. This brings out the true relationship\nbetween the reviews and not simply copy-paste. This introduces an element of\n\"common sense\" for the user and helps them to quickly make the right decisions.\nThe user is provided the pros and cons of the processed reviews. Thus the\nuser/customer can take their own decisions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13508", "pdf": "https://arxiv.org/pdf/2505.13508", "abs": "https://arxiv.org/abs/2505.13508", "authors": ["Zijia Liu", "Peixuan Han", "Haofei Yu", "Haoru Li", "Jiaxuan You"], "title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities but lack\nrobust temporal intelligence, struggling to integrate reasoning about the past\nwith predictions and plausible generations of the future. Meanwhile, existing\nmethods typically target isolated temporal skills, such as question answering\nabout past events or basic forecasting, and exhibit poor generalization,\nparticularly when dealing with events beyond their knowledge cutoff or\nrequiring creative foresight. To address these limitations, we introduce\n\\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)\nLLM with comprehensive temporal abilities: understanding, prediction, and\ncreative generation. Our approach features a novel three-stage development\npath; the first two constitute a \\textit{reinforcement learning (RL)\ncurriculum} driven by a meticulously designed dynamic rule-based reward system.\nThis framework progressively builds (1) foundational temporal understanding and\nlogical event-time mappings from historical data, (2) future event prediction\nskills for events beyond its knowledge cutoff, and finally (3) enables\nremarkable generalization to creative future scenario generation without any\nfine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms\nmodels over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,\non highly challenging future event prediction and creative scenario generation\nbenchmarks. This work provides strong evidence that thoughtfully engineered,\nprogressive RL fine-tuning allows smaller, efficient models to achieve superior\ntemporal performance, offering a practical and scalable path towards truly\ntime-aware AI. To foster further research, we also release \\textit{Time-Bench},\na large-scale multi-task temporal reasoning dataset derived from 10 years of\nnews data, and our series of \\textit{Time-R1} checkpoints.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "question answering"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13527", "pdf": "https://arxiv.org/pdf/2505.13527", "abs": "https://arxiv.org/abs/2505.13527", "authors": ["Jingyu Peng", "Maolin Wang", "Nan Wang", "Xiangyu Zhao", "Jiatong Li", "Kai Zhang", "Qi Liu"], "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "safety"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14014", "pdf": "https://arxiv.org/pdf/2505.14014", "abs": "https://arxiv.org/abs/2505.14014", "authors": ["Zelin Zhang", "Tao Zhang", "KediLI", "Xu Zheng"], "title": "EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent efforts have explored multimodal semantic segmentation using various\nbackbone architectures. However, while most methods aim to improve accuracy,\ntheir computational efficiency remains underexplored. To address this, we\npropose EGFormer, an efficient multimodal semantic segmentation framework that\nflexibly integrates an arbitrary number of modalities while significantly\nreducing model parameters and inference time without sacrificing performance.\nOur framework introduces two novel modules. First, the Any-modal Scoring Module\n(ASM) assigns importance scores to each modality independently, enabling\ndynamic ranking based on their feature maps. Second, the Modal Dropping Module\n(MDM) filters out less informative modalities at each stage, selectively\npreserving and aggregating only the most valuable features. This design allows\nthe model to leverage useful information from all available modalities while\ndiscarding redundancy, thus ensuring high segmentation quality. In addition to\nefficiency, we evaluate EGFormer on a synthetic-to-real transfer task to\ndemonstrate its generalizability. Extensive experiments show that EGFormer\nachieves competitive performance with up to 88 percent reduction in parameters\nand 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it\nfurther achieves state-of-the-art transfer performance compared to existing\nmethods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13628", "pdf": "https://arxiv.org/pdf/2505.13628", "abs": "https://arxiv.org/abs/2505.13628", "authors": ["Nathaniel Krasner", "Nicholas Lanuzo", "Antonios Anastasopoulos"], "title": "Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Multilingual alignment of sentence representations has mostly required\nbitexts to bridge the gap between languages. We investigate whether visual\ninformation can bridge this gap instead. Image caption datasets are very easy\nto create without requiring multilingual expertise, so this offers a more\nefficient alternative for low-resource languages. We find that multilingual\nimage-caption alignment can implicitly align the text representations between\nlanguages, languages unseen by the encoder in pretraining can be incorporated\ninto this alignment post-hoc, and these aligned representations are usable for\ncross-lingual Natural Language Understanding (NLU) and bitext retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13772", "pdf": "https://arxiv.org/pdf/2505.13772", "abs": "https://arxiv.org/abs/2505.13772", "authors": ["Dimitris Roussis", "Leon Voukoutis", "Georgios Paraskevopoulos", "Sokratis Sofianopoulos", "Prokopis Prokopidis", "Vassilis Papavasileiou", "Athanasios Katsamanis", "Stelios Piperidis", "Vassilis Katsouros"], "title": "Krikri: Advancing Open Large Language Models for Greek", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored\nfor the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been\nextensively trained on high-quality Greek data to ensure superior adaptation to\nlinguistic nuances. With 8 billion parameters, it offers advanced capabilities\nwhile maintaining efficient computational performance. Llama-Krikri-8B supports\nboth Modern Greek and English, and is also equipped to handle polytonic text\nand Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage\npost-training pipeline, utilizing both human and synthetic instruction and\npreference data, by applying techniques such as MAGPIE. In addition, for\nevaluation, we propose three novel public benchmarks for Greek. Our evaluation\non existing as well as the proposed benchmarks shows notable improvements over\ncomparable Greek and multilingual LLMs in both natural language understanding\nand generation as well as code generation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "code generation"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13844", "pdf": "https://arxiv.org/pdf/2505.13844", "abs": "https://arxiv.org/abs/2505.13844", "authors": ["Congchi Yin", "Yongpeng Zhang", "Xuyun Wen", "Piji Li"], "title": "Improve Language Model and Brain Alignment via Associative Memory", "categories": ["cs.CL"], "comment": "Accepted by Findings of ACL 2025", "summary": "Associative memory engages in the integration of relevant information for\ncomprehension in the human cognition system. In this work, we seek to improve\nalignment between language models and human brain while processing speech\ninformation by integrating associative memory. After verifying the alignment\nbetween language model and brain by mapping language model activations to brain\nactivity, the original text stimuli expanded with simulated associative memory\nare regarded as input to computational language models. We find the alignment\nbetween language model and brain is improved in brain regions closely related\nto associative memory processing. We also demonstrate large language models\nafter specific supervised fine-tuning better align with brain response, by\nbuilding the \\textit{Association} dataset containing 1000 samples of stories,\nwith instructions encouraging associative memory as input and associated\ncontent as output.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14204", "pdf": "https://arxiv.org/pdf/2505.14204", "abs": "https://arxiv.org/abs/2505.14204", "authors": ["Yang Hu", "Runchen Wang", "Stephen Chong Zhao", "Xuhui Zhan", "Do Hun Kim", "Mark Wallace", "David A. Tovar"], "title": "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment", "categories": ["cs.CV", "q-bio.NC"], "comment": "10 pages, 5 figures, 2 tables", "summary": "We introduce Perceptual-Initialization (PI), a paradigm shift in visual\nrepresentation learning that incorporates human perceptual structure during the\ninitialization phase rather than as a downstream fine-tuning step. By\nintegrating human-derived triplet embeddings from the NIGHTS dataset to\ninitialize a CLIP vision encoder, followed by self-supervised learning on\nYFCC15M, our approach demonstrates significant zero-shot performance\nimprovements, without any task-specific fine-tuning, across 29 zero shot\nclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains\nemerge after approximately 15 epochs of pretraining. Benefits are observed\nacross datasets of various scales, with improvements manifesting at different\nstages of the pretraining process depending on dataset characteristics. Our\napproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and\nretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,\nwithout requiring any adaptation to target domains. These findings challenge\nthe conventional wisdom of using human-perceptual data primarily for\nfine-tuning and demonstrate that embedding human perceptual structure during\nearly representation learning yields more capable and vision-language aligned\nsystems that generalize immediately to unseen tasks. Our work shows that\n\"beginning with you\", starting with human perception, provides a stronger\nfoundation for general-purpose vision-language intelligence.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "accuracy"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14231", "pdf": "https://arxiv.org/pdf/2505.14231", "abs": "https://arxiv.org/abs/2505.14231", "authors": ["Sule Bai", "Mingxing Li", "Yong Liu", "Jing Tang", "Haoji Zhang", "Lei Sun", "Xiangxiang Chu", "Yansong Tang"], "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13995", "pdf": "https://arxiv.org/pdf/2505.13995", "abs": "https://arxiv.org/abs/2505.13995", "authors": ["Myra Cheng", "Sunny Yu", "Cinoo Lee", "Pranav Khadpe", "Lujain Ibrahim", "Dan Jurafsky"], "title": "Social Sycophancy: A Broader Understanding of LLM Sycophancy", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "A serious risk to the safety and utility of LLMs is sycophancy, i.e.,\nexcessive agreement with and flattery of the user. Yet existing work focuses on\nonly one aspect of sycophancy: agreement with users' explicitly stated beliefs\nthat can be compared to a ground truth. This overlooks forms of sycophancy that\narise in ambiguous contexts such as advice and support-seeking, where there is\nno clear ground truth, yet sycophancy can reinforce harmful implicit\nassumptions, beliefs, or actions. To address this gap, we introduce a richer\ntheory of social sycophancy in LLMs, characterizing sycophancy as the excessive\npreservation of a user's face (the positive self-image a person seeks to\nmaintain in an interaction). We present ELEPHANT, a framework for evaluating\nsocial sycophancy across five face-preserving behaviors (emotional validation,\nmoral endorsement, indirect language, indirect action, and accepting framing)\non two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole\n(AITA). Across eight models, we show that LLMs consistently exhibit high rates\nof social sycophancy: on OEQ, they preserve face 47% more than humans, and on\nAITA, they affirm behavior deemed inappropriate by crowdsourced human judgments\nin 42% of cases. We further show that social sycophancy is rewarded in\npreference datasets and is not easily mitigated. Our work provides theoretical\ngrounding and empirical tools (datasets and code) for understanding and\naddressing this under-recognized but consequential issue.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety", "agreement"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14341", "pdf": "https://arxiv.org/pdf/2505.14341", "abs": "https://arxiv.org/abs/2505.14341", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14346", "pdf": "https://arxiv.org/pdf/2505.14346", "abs": "https://arxiv.org/abs/2505.14346", "authors": ["Mingfang Zhang", "Ryo Yonetani", "Yifei Huang", "Liangyang Ouyang", "Ruicong Liu", "Yoichi Sato"], "title": "Egocentric Action-aware Inertial Localization in Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel inertial localization framework named Egocentric\nAction-aware Inertial Localization (EAIL), which leverages egocentric action\ncues from head-mounted IMU signals to localize the target individual within a\n3D point cloud. Human inertial localization is challenging due to IMU sensor\nnoise that causes trajectory drift over time. The diversity of human actions\nfurther complicates IMU signal processing by introducing various motion\npatterns. Nevertheless, we observe that some actions observed through the\nhead-mounted IMU correlate with spatial environmental structures (e.g., bending\ndown to look inside an oven, washing dishes next to a sink), thereby serving as\nspatial anchors to compensate for the localization drift. The proposed EAIL\nframework learns such correlations via hierarchical multi-modal alignment. By\nassuming that the 3D point cloud of the environment is available, it\ncontrastively learns modality encoders that align short-term egocentric action\ncues in IMU signals with local environmental features in the point cloud. These\nencoders are then used in reasoning the IMU data and the point cloud over time\nand space to perform inertial localization. Interestingly, these encoders can\nfurther be utilized to recognize the corresponding sequence of actions as a\nby-product. Extensive experiments demonstrate the effectiveness of the proposed\nframework over state-of-the-art inertial localization and inertial action\nrecognition baselines.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14359", "pdf": "https://arxiv.org/pdf/2505.14359", "abs": "https://arxiv.org/abs/2505.14359", "authors": ["Ruoxin Chen", "Junwei Xi", "Zhiyuan Yan", "Ke-Yue Zhang", "Shuang Wu", "Jingyi Xie", "Xu Chen", "Lei Xu", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable", "categories": ["cs.CV"], "comment": "12 Pages, 9 figures", "summary": "Existing detectors are often trained on biased datasets, leading to the\npossibility of overfitting on non-causal image attributes that are spuriously\ncorrelated with real/synthetic labels. While these biased features enhance\nperformance on the training data, they result in substantial performance\ndegradation when applied to unbiased datasets. One common solution is to\nperform dataset alignment through generative reconstruction, matching the\nsemantic content between real and synthetic images. However, we revisit this\napproach and show that pixel-level alignment alone is insufficient. The\nreconstructed images still suffer from frequency-level misalignment, which can\nperpetuate spurious correlations. To illustrate, we observe that reconstruction\nmodels tend to restore the high-frequency details lost in real images (possibly\ndue to JPEG compression), inadvertently creating a frequency-level\nmisalignment, where synthetic images appear to have richer high-frequency\ncontent than real ones. This misalignment leads to models associating\nhigh-frequency features with synthetic labels, further reinforcing biased cues.\nTo resolve this, we propose Dual Data Alignment (DDA), which aligns both the\npixel and frequency domains. Moreover, we introduce two new test sets:\nDDA-COCO, containing DDA-aligned synthetic images for testing detector\nperformance on the most aligned dataset, and EvalGEN, featuring the latest\ngenerative models for assessing detectors under new generative architectures\nsuch as visual auto-regressive generators. Finally, our extensive evaluations\ndemonstrate that a detector trained exclusively on DDA-aligned MSCOCO could\nimprove across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on\nin-the-wild benchmarks, highlighting the improved generalizability of unbiased\ndetectors.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14361", "pdf": "https://arxiv.org/pdf/2505.14361", "abs": "https://arxiv.org/abs/2505.14361", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "categories": ["cs.CV"], "comment": "Accepted by IEEE Geoscience and Remote Sensing Magazine", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14362", "pdf": "https://arxiv.org/pdf/2505.14362", "abs": "https://arxiv.org/abs/2505.14362", "authors": ["Ziwei Zheng", "Michael Yang", "Jack Hong", "Chenxiao Zhao", "Guohai Xu", "Le Yang", "Chao Shen", "Xing Yu"], "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["mathematical reasoning", "fine-grained"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14405", "pdf": "https://arxiv.org/pdf/2505.14405", "abs": "https://arxiv.org/abs/2505.14405", "authors": ["Jiafeng Liang", "Shixin Jiang", "Xuan Dong", "Ning Wang", "Zheng Chu", "Hui Su", "Jinlan Fu", "Ming Liu", "See-Kiong Ng", "Bing Qin"], "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "reliability"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14158", "pdf": "https://arxiv.org/pdf/2505.14158", "abs": "https://arxiv.org/abs/2505.14158", "authors": ["Sanjay Govindan", "Maurice Pagnucco", "Yang Song"], "title": "Temporal Alignment of Time Sensitive Facts with Activation Engineering", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are trained on diverse and often conflicting\nknowledge spanning multiple domains and time periods. Some of this knowledge is\nonly valid within specific temporal contexts, such as answering the question,\n\"Who is the President of the United States in 2022?\" Ensuring LLMs generate\ntime appropriate responses is crucial for maintaining relevance and accuracy.\nIn this work we explore activation engineering as a method for temporally\naligning LLMs to improve factual recall without any training or dataset\ncreation. In this research we explore an activation engineering technique to\nground three versions of LLaMA 2 to specific points in time and examine the\neffects of varying injection layers and prompting strategies. Our experiments\ndemonstrate up to a 44% and 16% improvement in relative and explicit prompting\nrespectively, achieving comparable performance to the fine-tuning method\nproposed by Zhao et al. (2024) . Notably, our approach achieves similar results\nto the fine-tuning baseline while being significantly more computationally\nefficient and requiring no pre-aligned datasets.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14476", "pdf": "https://arxiv.org/pdf/2505.14476", "abs": "https://arxiv.org/abs/2505.14476", "authors": ["Farshad Sangari Abiz", "Reshad Hosseini", "Babak N. Araabi"], "title": "Enhancing Interpretability of Sparse Latent Representations with Class Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models for learning\nlatent representations. Standard VAEs generate dispersed and unstructured\nlatent spaces by utilizing all dimensions, which limits their interpretability,\nespecially in high-dimensional spaces. To address this challenge, Variational\nSparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting\nin sparse latent representations for each input. These sparse representations,\ncharacterized by a limited number of active dimensions, are inherently more\ninterpretable. Despite this advantage, VSC falls short in providing structured\ninterpretations across samples within the same class. Intuitively, samples from\nthe same class are expected to share similar attributes while allowing for\nvariations in those attributes. This expectation should manifest as consistent\npatterns of active dimensions in their latent representations, but VSC does not\nenforce such consistency.\n  In this paper, we propose a novel approach to enhance the latent space\ninterpretability by ensuring that the active dimensions in the latent space are\nconsistent across samples within the same class. To achieve this, we introduce\na new loss function that encourages samples from the same class to share\nsimilar active dimensions. This alignment creates a more structured and\ninterpretable latent space, where each shared dimension corresponds to a\nhigh-level concept, or \"factor.\" Unlike existing disentanglement-based methods\nthat primarily focus on global factors shared across all classes, our method\ncaptures both global and class-specific factors, thereby enhancing the utility\nand interpretability of latent representations.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "dimension"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14178", "pdf": "https://arxiv.org/pdf/2505.14178", "abs": "https://arxiv.org/abs/2505.14178", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Yiwei Xu", "Chenyu You"], "title": "Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is the first - and often underappreciated - layer of computation\nin language models. While Chain-of-Thought (CoT) prompting enables transformer\nmodels to approximate recurrent computation by externalizing intermediate\nsteps, we show that the success of such reasoning is fundamentally bounded by\nthe structure of tokenized inputs. This work presents a theoretical and\nempirical investigation into how tokenization schemes, particularly\nsubword-based methods like byte-pair encoding (BPE), impede symbolic\ncomputation by merging or obscuring atomic reasoning units. We introduce the\nnotion of Token Awareness to formalize how poor token granularity disrupts\nlogical alignment and prevents models from generalizing symbolic procedures.\nThrough systematic evaluation on arithmetic and symbolic tasks, we demonstrate\nthat token structure dramatically affect reasoning performance, causing failure\neven with CoT, while atomically-aligned formats unlock strong generalization,\nallowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,\no1) in structured reasoning. Our findings reveal that symbolic reasoning\nability in LLMs is not purely architectural, but deeply conditioned on\ntoken-level representations.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14226", "pdf": "https://arxiv.org/pdf/2505.14226", "abs": "https://arxiv.org/abs/2505.14226", "authors": ["Darpan Aswal", "Siddharth D Jaiswal"], "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become increasingly powerful, with\nmultilingual and multimodal capabilities improving by the day. These models are\nbeing evaluated through audits, alignment studies and red-teaming efforts to\nexpose model vulnerabilities towards generating harmful, biased and unfair\ncontent. Existing red-teaming efforts have previously focused on the English\nlanguage, using fixed template-based attacks; thus, models continue to be\nsusceptible to multilingual jailbreaking strategies, especially in the\nmultimodal context. In this study, we introduce a novel strategy that leverages\ncode-mixing and phonetic perturbations to jailbreak LLMs for both text and\nimage generation tasks. We also introduce two new jailbreak strategies that\nshow higher effectiveness than baseline strategies. Our work presents a method\nto effectively bypass safety filters in LLMs while maintaining interpretability\nby applying phonetic misspellings to sensitive words in code-mixed prompts. Our\nnovel prompts achieve a 99% Attack Success Rate for text generation and 78% for\nimage generation, with Attack Relevance Rate of 100% for text generation and\n95% for image generation when using the phonetically perturbed code-mixed\nprompts. Our interpretability experiments reveal that phonetic perturbations\nimpact word tokenization, leading to jailbreak success. Our study motivates\nincreasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14677", "pdf": "https://arxiv.org/pdf/2505.14677", "abs": "https://arxiv.org/abs/2505.14677", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13507", "pdf": "https://arxiv.org/pdf/2505.13507", "abs": "https://arxiv.org/abs/2505.13507", "authors": ["Haoyang Chen"], "title": "Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning\nknown-class distributions across domains while identifying\ntarget-domain-specific unknown categories. Current approaches often fail to\nleverage semantic relationships between modalities and struggle with error\naccumulation in unknown sample detection. We propose to harness Contrastive\nLanguage-Image Pretraining (CLIP) to address these limitations through two key\ninnovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts\nconditioned on domain discrepancy metrics dynamically adapt CLIP's text\nencoder, enabling semantic consistency between source and target domains\nwithout explicit unknown-class supervision. 2) Gradient-aware open-set\nseparation: A gradient analysis module quantifies domain shift by comparing the\nL2-norm of gradients from the learned prompts, where known/unknown samples\nexhibit statistically distinct gradient behaviors. Evaluations on Office-Home\nshow that our method consistently outperforms CLIP baseline and standard\nbaseline. Ablation studies confirm the gradient norm's critical role.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13740", "pdf": "https://arxiv.org/pdf/2505.13740", "abs": "https://arxiv.org/abs/2505.13740", "authors": ["Chenning Yu", "Sicun Gao"], "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14436", "pdf": "https://arxiv.org/pdf/2505.14436", "abs": "https://arxiv.org/abs/2505.14436", "authors": ["Yuqiao Tan", "Shizhu He", "Kang Liu", "Jun Zhao"], "title": "Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL'25 Main. Code link:\n  https://github.com/Trae1ounG/Neural_Incompatibility", "summary": "Large Language Models (LLMs) offer a transparent brain with accessible\nparameters that encode extensive knowledge, which can be analyzed, located and\ntransferred. Consequently, a key research challenge is to transcend traditional\nknowledge transfer paradigms rooted in symbolic language and achieve genuine\nParametric Knowledge Transfer (PKT). Significantly, exploring effective methods\nfor transferring knowledge across LLMs of different scales through parameters\npresents an intriguing and valuable research direction. In this paper, we first\ndemonstrate $\\textbf{Alignment}$ in parametric space is the fundamental\nprerequisite to achieve successful cross-scale PKT. We redefine the previously\nexplored knowledge transfer as Post-Align PKT (PostPKT), which utilizes\nextracted parameters for LoRA initialization and requires subsequent fine-tune\nfor alignment. Hence, to reduce cost for further fine-tuning, we introduce a\nnovel Pre-Align PKT (PrePKT) paradigm and propose a solution called\n$\\textbf{LaTen}$\n($\\textbf{L}$oc$\\textbf{a}$te-$\\textbf{T}$h$\\textbf{e}$n-Alig$\\textbf{n}$) that\naligns the parametric spaces of LLMs across scales only using several training\nsteps without following training. Comprehensive experiments on four benchmarks\ndemonstrate that both PostPKT and PrePKT face challenges in achieving\nconsistently stable transfer. Through in-depth analysis, we identify\n$\\textbf{Neural Incompatibility}$ as the ethological and parametric structural\ndifferences between LLMs of varying scales, presenting fundamental challenges\nto achieving effective PKT. These findings provide fresh insights into the\nparametric architectures of LLMs and highlight promising directions for future\nresearch on efficient PKT. Our code is available at\nhttps://github.com/Trae1ounG/Neural_Incompatibility.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14455", "pdf": "https://arxiv.org/pdf/2505.14455", "abs": "https://arxiv.org/abs/2505.14455", "authors": ["Chihan Huang", "Hao Tang"], "title": "CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although autoregressive models have dominated language modeling in recent\nyears, there has been a growing interest in exploring alternative paradigms to\nthe conventional next-token prediction framework. Diffusion-based language\nmodels have emerged as a compelling alternative due to their powerful parallel\ngeneration capabilities and inherent editability. However, these models are\noften constrained by fixed-length generation. A promising direction is to\ncombine the strengths of both paradigms, segmenting sequences into blocks,\nmodeling autoregressive dependencies across blocks while leveraging discrete\ndiffusion to estimate the conditional distribution within each block given the\npreceding context. Nevertheless, their practical application is often hindered\nby two key limitations: rigid fixed-length outputs and a lack of flexible\ncontrol mechanisms. In this work, we address the critical limitations of fixed\ngranularity and weak controllability in current large diffusion language\nmodels. We propose CtrlDiff, a dynamic and controllable semi-autoregressive\nframework that adaptively determines the size of each generation block based on\nlocal semantics using reinforcement learning. Furthermore, we introduce a\nclassifier-guided control mechanism tailored to discrete diffusion, which\nsignificantly reduces computational overhead while facilitating efficient\npost-hoc conditioning without retraining. Extensive experiments demonstrate\nthat CtrlDiff sets a new standard among hybrid diffusion models, narrows the\nperformance gap to state-of-the-art autoregressive approaches, and enables\neffective conditional text generation across diverse tasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14180", "pdf": "https://arxiv.org/pdf/2505.14180", "abs": "https://arxiv.org/abs/2505.14180", "authors": ["Songhao Wu", "Quan Tu", "Mingjie Zhong", "Hong Liu", "Jia Xu", "Jinjie Gu", "Rui Yan"], "title": "Bridge the Gap between Past and Future: Siamese Model Optimization for Context-Aware Document Ranking", "categories": ["cs.IR", "cs.CV", "H.3.3"], "comment": null, "summary": "In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14552", "pdf": "https://arxiv.org/pdf/2505.14552", "abs": "https://arxiv.org/abs/2505.14552", "authors": ["Jiajun Shi", "Jian Yang", "Jiaheng Liu", "Xingyuan Bu", "Jiangjie Chen", "Junting Zhou", "Kaijing Ma", "Zhoufutu Wen", "Bingli Wang", "Yancheng He", "Liang Song", "Hualei Zhu", "Shilong Li", "Xingjian Wang", "Wei Zhang", "Ruibin Yuan", "Yifan Yao", "Wenjun Yang", "Yunli Wang", "Siyuan Fang", "Siyu Yuan", "Qianyu He", "Xiangru Tang", "Yingshui Tan", "Wangchunshu Zhou", "Zhaoxiang Zhang", "Zhoujun Li", "Wenhao Huang", "Ge Zhang"], "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Recent advancements in large language models (LLMs) underscore the need for\nmore comprehensive evaluation methods to accurately assess their reasoning\ncapabilities. Existing benchmarks are often domain-specific and thus cannot\nfully capture an LLM's general reasoning potential. To address this limitation,\nwe introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic\nevaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over\nfifty games in either textual or visual formats and supports interactive,\nmulti-turn assessments with reinforcement learning scenarios. Using KORGym, we\nconduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent\nreasoning patterns within model families and demonstrating the superior\nperformance of closed-source models. Further analysis examines the effects of\nmodality, reasoning strategies, reinforcement learning techniques, and response\nlength on model performance. We expect KORGym to become a valuable resource for\nadvancing LLM reasoning research and developing evaluation methodologies suited\nto complex, interactive environments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14582", "pdf": "https://arxiv.org/pdf/2505.14582", "abs": "https://arxiv.org/abs/2505.14582", "authors": ["Shangziqi Zhao", "Jiahao Yuan", "Guisong Yang", "Usman Naseem"], "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning", "categories": ["cs.CL"], "comment": "17 pages,4 figures", "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies -- targeting entire chains, core reasoning, and verification -- we\nfind that pruning verification steps yields consistent accuracy gains while\nreducing inference cost, outperforming token-level baselines and uncompressed\nfine-tuning. In contrast, pruning reasoning or all-chain steps degrades\nperformance, revealing that small models benefit not from shorter CoTs, but\nfrom semantically leaner ones. Our findings highlight pruning as a structural\noptimization strategy for aligning CoT reasoning with SLM capacity.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["self-verification"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14585", "pdf": "https://arxiv.org/pdf/2505.14585", "abs": "https://arxiv.org/abs/2505.14585", "authors": ["Wenbin Hu", "Haoran Li", "Huihao Jing", "Qi Hu", "Ziqian Zeng", "Sirui Han", "Heli Xu", "Tianshu Chu", "Peizhao Hu", "Yangqiu Song"], "title": "Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) exhibit remarkable capabilities, they also\nintroduce significant safety and privacy risks. Current mitigation strategies\noften fail to preserve contextual reasoning capabilities in risky scenarios.\nInstead, they rely heavily on sensitive pattern matching to protect LLMs, which\nlimits the scope. Furthermore, they overlook established safety and privacy\nstandards, leading to systemic risks for legal compliance. To address these\ngaps, we formulate safety and privacy issues into contextualized compliance\nproblems following the Contextual Integrity (CI) theory. Under the CI\nframework, we align our model with three critical regulatory standards: GDPR,\nEU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with\na rule-based reward to incentivize contextual reasoning capabilities while\nenhancing compliance with safety and privacy norms. Through extensive\nexperiments, we demonstrate that our method not only significantly enhances\nlegal compliance (achieving a +17.64% accuracy improvement in safety/privacy\nbenchmarks) but also further improves general reasoning capability. For\nOpenThinker-7B, a strong reasoning model that significantly outperforms its\nbase model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its\ngeneral reasoning capabilities, with +2.05% and +8.98% accuracy improvement on\nthe MMLU and LegalBench benchmark, respectively.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["reasoning model"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "safety", "accuracy"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14607", "pdf": "https://arxiv.org/pdf/2505.14607", "abs": "https://arxiv.org/abs/2505.14607", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Joy Mahapatra", "Utpal Garain"], "title": "sudoLLM : On Multi-role Alignment of Language Models", "categories": ["cs.CL", "cs.CR", "I.2.7"], "comment": "Under review. Code and data to be released later", "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14617", "pdf": "https://arxiv.org/pdf/2505.14617", "abs": "https://arxiv.org/abs/2505.14617", "authors": ["Sahar Abdelnabi", "Ahmed Salem"], "title": "Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Reasoning-focused large language models (LLMs) sometimes alter their behavior\nwhen they detect that they are being evaluated, an effect analogous to the\nHawthorne phenomenon, which can lead them to optimize for test-passing\nperformance or to comply more readily with harmful prompts if real-world\nconsequences appear absent. We present the first quantitative study of how such\n\"test awareness\" impacts model behavior, particularly its safety alignment. We\nintroduce a white-box probing framework that (i) linearly identifies\nawareness-related activations and (ii) steers models toward or away from test\nawareness while monitoring downstream performance. We apply our method to\ndifferent state-of-the-art open-source reasoning LLMs across both realistic and\nhypothetical tasks. Our results demonstrate that test awareness significantly\nimpact safety alignment, and is different for different models. By providing\nfine-grained control over this latent effect, our work aims to increase trust\nin how we perform safety evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety", "fine-grained"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14633", "pdf": "https://arxiv.org/pdf/2505.14633", "abs": "https://arxiv.org/abs/2505.14633", "authors": ["Yu Ying Chiu", "Zhilin Wang", "Sharan Maiya", "Yejin Choi", "Kyle Fish", "Sydney Levine", "Evan Hubinger"], "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "34 pages, 11 figures, see associated data at\n  https://huggingface.co/datasets/kellycyy/AIRiskDilemmas and code at\n  https://github.com/kellycyy/LitmusValues", "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14652", "pdf": "https://arxiv.org/pdf/2505.14652", "abs": "https://arxiv.org/abs/2505.14652", "authors": ["Xueguang Ma", "Qian Liu", "Dongfu Jiang", "Ge Zhang", "Zejun Ma", "Wenhu Chen"], "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "mathematical reasoning"], "score": 3}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14684", "pdf": "https://arxiv.org/pdf/2505.14684", "abs": "https://arxiv.org/abs/2505.14684", "authors": ["Haolei Xu", "Yuchen Yan", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Shengpei Jiang", "Kaitao Song", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress on\nmathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing\nmathematical CoT datasets often suffer from Thought Leaps due to experts\nomitting intermediate steps, which negatively impacts model learning and\ngeneralization. We propose the CoT Thought Leap Bridge Task, which aims to\nautomatically detect leaps and generate missing intermediate reasoning steps to\nrestore the completeness and coherence of CoT. To facilitate this, we\nconstructed a specialized training dataset called ScaleQM+, based on the\nstructured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought\nleaps. Through comprehensive experiments on mathematical reasoning benchmarks,\nwe demonstrate that models fine-tuned on bridged datasets consistently\noutperform those trained on original datasets, with improvements of up to\n+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)\nand provides better starting points for reinforcement learning (+3.1%),\nfunctioning as a plug-and-play module compatible with existing optimization\ntechniques. Furthermore, CoT-Bridge demonstrate improved generalization to\nout-of-domain logical reasoning tasks, confirming that enhancing reasoning\ncompleteness yields broadly applicable benefits.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13482", "pdf": "https://arxiv.org/pdf/2505.13482", "abs": "https://arxiv.org/abs/2505.13482", "authors": ["Anand Selvadurai", "Jasheen Shaik", "Girish Chandrasekar", "ShriRadhaKrishnan Balamurugan", "Eswara Reddy"], "title": "MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "9 pages, 1 figure. This manuscript is a substantial revision of a\n  previously submitted paper. We have explicitly clarified novelty,\n  strengthened scholarly depth, and expanded experimental validation", "summary": "Embedding models have become essential for retrieval-augmented generation\n(RAG) tasks, semantic clustering, and text re-ranking. But despite their\ngrowing use, many of these come with notable limitations. For example, Jina\nfails to capture the semantic content of medical documents, while models such\nas MiniLM often perform poorly on long-form documents. Domain-adapted models,\nwhile specialized, often underperform in general-purpose tasks, reducing their\noverall applicability. General-domain tokenizers often misinterpret medical\nvocabulary. The limitations of current embedding models, whether in\ntokenization accuracy, domain comprehension, or handling long sequences,\nhighlight the need for more versatile solutions. In this work, we present\nMedEIR, a novel embedding model and tokenizer jointly optimized for both\nmedical and general NLP tasks, incorporating ALiBi-based long-context\nprocessing to support sequences of up to 8,192 tokens. MedEIR was pre-trained\non only 6 billion tokens, significantly fewer than Jina's, followed by\nfine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina\nV2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),\nNFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID\n(79.56). These results highlight the potential of MedEIR as a highly effective\nembedding model, demonstrating strong performance across both general-purpose\nand domain-specific tasks and outperforming existing models on multiple\nbenchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13515", "pdf": "https://arxiv.org/pdf/2505.13515", "abs": "https://arxiv.org/abs/2505.13515", "authors": ["Yanan Li", "Fanxu Meng", "Muhan Zhang", "Shiai Zhu", "Shangguang Wang", "Mengwei Xu"], "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained\non earlier versions quickly become obsolete. The conventional practice of\nretraining LoRA weights from scratch on the latest model is costly,\ntime-consuming, and environmentally detrimental, particularly as the diversity\nof LLMs and downstream tasks expands. This motivates a critical question: \"How\ncan we efficiently leverage existing LoRA weights to adapt to newer model\nversions?\" To address this, we propose LoRASuite, a modular approach tailored\nspecifically to various types of LLM updates. First, we compute a transfer\nmatrix utilizing known parameters from both old and new LLMs. Next, we allocate\ncorresponding layers and attention heads based on centered kernel alignment and\ncosine similarity metrics, respectively. A subsequent small-scale, skillful\nfine-tuning step ensures numerical stability. Experimental evaluations\ndemonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA\nmethods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even\nexceeds the performance of full-scale LoRA retraining, with average\nimprovements of +1.4 and +6.6 points on math tasks, respectively. Additionally,\nLoRASuite significantly reduces memory consumption by 5.5 GB and computational\ntime by 78.23%.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13531", "pdf": "https://arxiv.org/pdf/2505.13531", "abs": "https://arxiv.org/abs/2505.13531", "authors": ["Shitong Duan", "Xiaoyuan Yi", "Peng Zhang", "Dongkuan Xu", "Jing Yao", "Tun Lu", "Ning Gu", "Xing Xie"], "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Assessing Large Language Models (LLMs)' underlying value differences enables\ncomprehensive comparison of their misalignment, cultural adaptability, and\nbiases. Nevertheless, current value measurement datasets face the\ninformativeness challenge: with often outdated, contaminated, or generic test\nquestions, they can only capture the shared value orientations among different\nLLMs, leading to saturated and thus uninformative results. To address this\nproblem, we introduce AdAEM, a novel, self-extensible assessment framework for\nrevealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM\ncan automatically and adaptively generate and extend its test questions. This\nis achieved by probing the internal value boundaries of a diverse set of LLMs\ndeveloped across cultures and time periods in an in-context optimization\nmanner. The optimization process theoretically maximizes an\ninformation-theoretic objective to extract the latest or culturally\ncontroversial topics, providing more distinguishable and informative insights\nabout models' value differences. In this way, AdAEM is able to co-evolve with\nthe development of LLMs, consistently tracking their value dynamics. Using\nAdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct\nan extensive analysis to manifest our method's validity and effectiveness, and\nbenchmark the values of 16 LLMs, laying the groundwork for better value\nresearch.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13718", "pdf": "https://arxiv.org/pdf/2505.13718", "abs": "https://arxiv.org/abs/2505.13718", "authors": ["Safal Shrestha", "Minwu Kim", "Aadim Nepal", "Anubhav Shrestha", "Keith Ross"], "title": "Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Designing effective reasoning-capable LLMs typically requires training using\nReinforcement Learning with Verifiable Rewards (RLVR) or distillation with\ncarefully curated Long Chain of Thoughts (CoT), both of which depend heavily on\nextensive training data. This creates a major challenge when the amount of\nquality training data is scarce. We propose a sample-efficient, two-stage\ntraining strategy to develop reasoning LLMs under limited supervision. In the\nfirst stage, we \"warm up\" the model by distilling Long CoTs from a toy domain,\nnamely, Knights \\& Knaves (K\\&K) logic puzzles to acquire general reasoning\nskills. In the second stage, we apply RLVR to the warmed-up model using a\nlimited set of target-domain examples. Our experiments demonstrate that this\ntwo-phase approach offers several benefits: $(i)$ the warmup phase alone\nfacilitates generalized reasoning, leading to performance improvements across a\nrange of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both\nthe base model and the warmed-up model are RLVR trained on the same small\ndataset ($\\leq100$ examples), the warmed-up model consistently outperforms the\nbase model; $(iii)$ Warming up before RLVR training allows a model to maintain\ncross-domain generalizability even after training on a specific domain; $(iv)$\nIntroducing warmup in the pipeline improves not only accuracy but also overall\nsample efficiency during RLVR training. The results in this paper highlight the\npromise of warmup for building robust reasoning LLMs in data-scarce\nenvironments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13757", "pdf": "https://arxiv.org/pdf/2505.13757", "abs": "https://arxiv.org/abs/2505.13757", "authors": ["Runchu Tian", "Xueqiang Xu", "Bowen Jin", "SeongKu Kang", "Jiawei Han"], "title": "LLM-Based Compact Reranking with Document Features for Scientific Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "17 pages, 4 figures", "summary": "Scientific retrieval is essential for advancing academic discovery. Within\nthis process, document reranking plays a critical role by refining first-stage\nretrieval results. However, large language model (LLM) listwise reranking faces\nunique challenges in the scientific domain. First-stage retrieval is often\nsuboptimal in the scientific domain, so relevant documents are ranked lower.\nMoreover, conventional listwise reranking uses the full text of candidate\ndocuments in the context window, limiting the number of candidates that can be\nconsidered. As a result, many relevant documents are excluded before reranking,\nwhich constrains overall retrieval performance. To address these challenges, we\nexplore compact document representations based on semantic features such as\ncategories, sections, and keywords, and propose a training-free, model-agnostic\nreranking framework for scientific retrieval called CoRank. The framework\ninvolves three stages: (i) offline extraction of document-level features, (ii)\ncoarse reranking using these compact representations, and (iii) fine-grained\nreranking on full texts of the top candidates from stage (ii). This hybrid\ndesign provides a high-level abstraction of document semantics, expands\ncandidate coverage, and retains critical details required for precise ranking.\nExperiments on LitSearch and CSFCube show that CoRank significantly improves\nreranking performance across different LLM backbones, increasing nDCG@10 from\n32.0 to 39.7. Overall, these results highlight the value of information\nextraction for reranking in scientific retrieval.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13820", "pdf": "https://arxiv.org/pdf/2505.13820", "abs": "https://arxiv.org/abs/2505.13820", "authors": ["Jun Liu", "Zhenglun Kong", "Peiyan Dong", "Changdi Yang", "Tianqi Li", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "title": "Structured Agent Distillation for Large Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13847", "pdf": "https://arxiv.org/pdf/2505.13847", "abs": "https://arxiv.org/abs/2505.13847", "authors": ["Tianle Yang", "Chengzhe Sun", "Siwei Lyu", "Phil Rose"], "title": "Forensic deepfake audio detection using segmental speech features", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "This study explores the potential of using acoustic features of segmental\nspeech sounds to detect deepfake audio. These features are highly interpretable\nbecause of their close relationship with human articulatory processes and are\nexpected to be more difficult for deepfake models to replicate. The results\ndemonstrate that certain segmental features commonly used in forensic voice\ncomparison are effective in identifying deep-fakes, whereas some global\nfeatures provide little value. These findings underscore the need to approach\naudio deepfake detection differently for forensic voice comparison and offer a\nnew perspective on leveraging segmental features for this purpose.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.13862", "pdf": "https://arxiv.org/pdf/2505.13862", "abs": "https://arxiv.org/abs/2505.13862", "authors": ["Guobin Shen", "Dongcheng Zhao", "Linghao Feng", "Xiang He", "Jihang Wang", "Sicheng Shen", "Haibo Tong", "Yiting Dong", "Jindong Li", "Xiang Zheng", "Yi Zeng"], "title": "PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable capabilities but remain\nvulnerable to adversarial prompts known as jailbreaks, which can bypass safety\nalignment and elicit harmful outputs. Despite growing efforts in LLM safety\nresearch, existing evaluations are often fragmented, focused on isolated attack\nor defense techniques, and lack systematic, reproducible analysis. In this\nwork, we introduce PandaGuard, a unified and modular framework that models LLM\njailbreak safety as a multi-agent system comprising attackers, defenders, and\njudges. Our framework implements 19 attack methods and 12 defense mechanisms,\nalong with multiple judgment strategies, all within a flexible plugin\narchitecture supporting diverse LLM interfaces, multiple interaction modes, and\nconfiguration-driven experimentation that enhances reproducibility and\npractical deployment. Built on this framework, we develop PandaBench, a\ncomprehensive benchmark that evaluates the interactions between these\nattack/defense methods across 49 LLMs and various judgment approaches,\nrequiring over 3 billion tokens to execute. Our extensive evaluation reveals\nkey insights into model vulnerabilities, defense cost-performance trade-offs,\nand judge consistency. We find that no single defense is optimal across all\ndimensions and that judge disagreement introduces nontrivial variance in safety\nassessments. We release the code, configurations, and evaluation results to\nsupport transparent and reproducible research in LLM safety.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "safety", "consistency"], "score": 4}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14146", "pdf": "https://arxiv.org/pdf/2505.14146", "abs": "https://arxiv.org/abs/2505.14146", "authors": ["Pengcheng Jiang", "Xueqiang Xu", "Jiacheng Lin", "Jinfeng Xiao", "Zifeng Wang", "Jimeng Sun", "Jiawei Han"], "title": "s3: You Don't Need That Much Data to Train a Search Agent via RL", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems empower large language models\n(LLMs) to access external knowledge during inference. Recent advances have\nenabled LLMs to act as search agents via reinforcement learning (RL), improving\ninformation acquisition through multi-turn interactions with retrieval engines.\nHowever, existing approaches either optimize retrieval using search-only\nmetrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM\nto jointly reason and retrieve-entangling retrieval with generation and\nlimiting the real search utility and compatibility with frozen or proprietary\nmodels. In this work, we propose s3, a lightweight, model-agnostic framework\nthat decouples the searcher from the generator and trains the searcher using a\nGain Beyond RAG reward: the improvement in generation accuracy over naive RAG.\ns3 requires only 2.4k training samples to outperform baselines trained on over\n70x more data, consistently delivering stronger downstream performance across\nsix general QA and five medical QA benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14216", "pdf": "https://arxiv.org/pdf/2505.14216", "abs": "https://arxiv.org/abs/2505.14216", "authors": ["Minwu Kim", "Anubhav Shrestha", "Safal Shrestha", "Aadim Nepal", "Keith Ross"], "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "23 pages", "summary": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy but fails to improve capability, while\ndistillation can improve both. In this paper, we investigate the mechanisms\nbehind these phenomena. First, we demonstrate that RLVR does not improve\ncapability because it focuses on improving the accuracy of the less-difficult\nquestions to the detriment of the accuracy of the most difficult questions,\nthereby leading to no improvement in capability. Second, we find that RLVR does\nnot merely increase the success probability for the less difficult questions,\nbut in our small model settings produces quality responses that were absent in\nits output distribution before training. In addition, we show these responses\nare neither noticeably longer nor feature more reflection-related keywords,\nunderscoring the need for more reliable indicators of response quality. Third,\nwe show that while distillation reliably improves accuracy by learning strong\nreasoning patterns, it only improves capability when new knowledge is\nintroduced. Moreover, when distilling only with reasoning patterns and no new\nknowledge, the accuracy of the less-difficult questions improves to the\ndetriment of the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in language models.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14356", "pdf": "https://arxiv.org/pdf/2505.14356", "abs": "https://arxiv.org/abs/2505.14356", "authors": ["Sho Inoue", "Shai Wang", "Haizhou Li"], "title": "PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is accepted to Interspeech 2025; Added an extra page for\n  supplementary figures; Project page:\n  https://github.com/shinshoji01/Personality-Prediction-for-Conversation-Agents", "summary": "Despite significant progress in neural spoken dialog systems,\npersonality-aware conversation agents -- capable of adapting behavior based on\npersonalities -- remain underexplored due to the absence of personality\nannotations in speech datasets. We propose a pipeline that preprocesses raw\naudio recordings to create a dialogue dataset annotated with timestamps,\nresponse types, and emotion/sentiment labels. We employ an automatic speech\nrecognition (ASR) system to extract transcripts and timestamps, then generate\nconversation-level annotations. Leveraging these annotations, we design a\nsystem that employs large language models to predict conversational\npersonality. Human evaluators were engaged to identify conversational\ncharacteristics and assign personality labels. Our analysis demonstrates that\nthe proposed system achieves stronger alignment with human judgments compared\nto existing approaches.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "dialogue"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14410", "pdf": "https://arxiv.org/pdf/2505.14410", "abs": "https://arxiv.org/abs/2505.14410", "authors": ["Jinzuomu Zhong", "Suyuan Liu", "Dan Wells", "Korin Richmond"], "title": "Pairwise Evaluation of Accent Similarity in Speech Synthesis", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025", "summary": "Despite growing interest in generating high-fidelity accents, evaluating\naccent similarity in speech synthesis has been underexplored. We aim to enhance\nboth subjective and objective evaluation methods for accent similarity.\nSubjectively, we refine the XAB listening test by adding components that\nachieve higher statistical significance with fewer listeners and lower costs.\nOur method involves providing listeners with transcriptions, having them\nhighlight perceived accent differences, and implementing meticulous screening\nfor reliability. Objectively, we utilise pronunciation-related metrics, based\non distances between vowel formants and phonetic posteriorgrams, to evaluate\naccent generation. Comparative experiments reveal that these metrics, alongside\naccent similarity, speaker similarity, and Mel Cepstral Distortion, can be\nused. Moreover, our findings underscore significant limitations of common\nmetrics like Word Error Rate in assessing underrepresented accents.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "reliability"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14412", "pdf": "https://arxiv.org/pdf/2505.14412", "abs": "https://arxiv.org/abs/2505.14412", "authors": ["PaweÅ Batorski", "Adrian Kosmala", "Paul Swoboda"], "title": "PRL: Prompts from Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective prompt engineering remains a central challenge in fully harnessing\nthe capabilities of LLMs. While well-designed prompts can dramatically enhance\nperformance, crafting them typically demands expert intuition and a nuanced\nunderstanding of the task. Moreover, the most impactful prompts often hinge on\nsubtle semantic cues, ones that may elude human perception but are crucial for\nguiding LLM behavior. In this paper, we introduce PRL (Prompts from\nReinforcement Learning), a novel RL-based approach for automatic prompt\ngeneration. Unlike previous methods, PRL can produce novel few-shot examples\nthat were not seen during training. Our approach achieves state-of-the-art\nperformance across a range of benchmarks, including text classification,\nsimplification, and summarization. On the classification task, it surpasses\nprior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it\nimproves the average ROUGE scores on the summarization task by 4.32 over APE\nand by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over\nAPE and by 6.01 over EvoPrompt. Our code is available at\nhttps://github.com/Batorskq/prl .", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["summarization"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14438", "pdf": "https://arxiv.org/pdf/2505.14438", "abs": "https://arxiv.org/abs/2505.14438", "authors": ["Yuanbo Fang", "Haoze Sun", "Jun Liu", "Tao Zhang", "Zenan Zhou", "Weipeng Chen", "Xiaofen Xing", "Xiangmin Xu"], "title": "S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "End-to-end speech large language models ((LLMs)) extend the capabilities of\ntext-based models to directly process and generate audio tokens. However, this\noften leads to a decline in reasoning and generation performance compared to\ntext input, a phenomenon referred to as intelligence degradation. To\nsystematically evaluate this gap, we propose S2SBench, a benchmark designed to\nquantify performance degradation in Speech LLMs. It includes diagnostic\ndatasets targeting sentence continuation and commonsense reasoning under audio\ninput. We further introduce a pairwise evaluation protocol based on perplexity\ndifferences between plausible and implausible samples to measure degradation\nrelative to text input. We apply S2SBench to analyze the training process of\nBaichuan-Audio, which further demonstrates the benchmark's effectiveness. All\ndatasets and evaluation code are available at\nhttps://github.com/undobug/S2SBench.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14625", "pdf": "https://arxiv.org/pdf/2505.14625", "abs": "https://arxiv.org/abs/2505.14625", "authors": ["Zhangchen Xu", "Yuetai Li", "Fengqing Jiang", "Bhaskar Ramasubramanian", "Luyao Niu", "Bill Yuchen Lin", "Radha Poovendran"], "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "reliability"], "score": 2}}, "source_file": "2025-05-21.jsonl"}
{"id": "2505.14667", "pdf": "https://arxiv.org/pdf/2505.14667", "abs": "https://arxiv.org/abs/2505.14667", "authors": ["Wonje Jeung", "Sangyeon Yoon", "Minsuk Kahng", "Albert No"], "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages", "summary": "Large Reasoning Models (LRMs) have become powerful tools for complex problem\nsolving, but their structured reasoning pathways can lead to unsafe outputs\nwhen exposed to harmful prompts. Existing safety alignment methods reduce\nharmful outputs but can degrade reasoning depth, leading to significant\ntrade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated\njailbreak attacks. To address this, we introduce SAFEPATH, a lightweight\nalignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at\nthe start of their reasoning, in response to harmful prompts, while leaving the\nrest of the reasoning process unsupervised. Empirical results across multiple\nbenchmarks indicate that SAFEPATH effectively reduces harmful outputs while\nmaintaining reasoning performance. Specifically, SAFEPATH reduces harmful\nresponses by up to 90.0% and blocks 83.3% of jailbreak attempts in the\nDeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than\nDirect Refusal and 314.1x less than SafeChain. We further introduce a zero-shot\nvariant that requires no fine-tuning. In addition, we provide a comprehensive\nanalysis of how existing methods in LLMs generalize, or fail, when applied to\nreasoning-centric models, revealing critical gaps and new directions for safer\nAI.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-21.jsonl"}
