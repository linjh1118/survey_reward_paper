{"id": "2505.15074", "pdf": "https://arxiv.org/pdf/2505.15074", "abs": "https://arxiv.org/abs/2505.15074", "authors": ["Yuhang Zhou", "Jing Zhu", "Shengyi Qian", "Zhuokai Zhao", "Xiyao Wang", "Xiaoyu Liu", "Ming Li", "Paiheng Xu", "Wei Ai", "Furong Huang"], "title": "DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly aligned with human preferences\nthrough Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,\nGroup Relative Policy Optimization (GRPO) has gained attention for its\nsimplicity and strong performance, notably eliminating the need for a learned\nvalue function. However, GRPO implicitly assumes a balanced domain distribution\nand uniform semantic alignment across groups - assumptions that rarely hold in\nreal-world datasets. When applied to multi-domain, imbalanced data, GRPO\ndisproportionately optimizes for dominant domains, neglecting underrepresented\nones and resulting in poor generalization and fairness. We propose\nDomain-Informed Self-Consistency Policy Optimization (DISCO), a principled\nextension to GRPO that addresses inter-group imbalance with two key\ninnovations. Domain-aware reward scaling counteracts frequency bias by\nreweighting optimization based on domain prevalence. Difficulty-aware reward\nscaling leverages prompt-level self-consistency to identify and prioritize\nuncertain prompts that offer greater learning value. Together, these strategies\npromote more equitable and effective policy learning across domains. Extensive\nexperiments across multiple LLMs and skewed training distributions show that\nDISCO improves generalization, outperforms existing GRPO variants by 5% on\nQwen3 models, and sets new state-of-the-art results on multi-domain alignment\nbenchmarks.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["RLHF", "reinforcement learning from human feedback", "human feedback", "reinforcement learning", "policy optimization", "alignment"], "score": 6}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15107", "pdf": "https://arxiv.org/pdf/2505.15107", "abs": "https://arxiv.org/abs/2505.15107", "authors": ["Ziliang Wang", "Xuhui Zheng", "Kang An", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "title": "StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "20 pages, 6 figures", "summary": "Efficient multi-hop reasoning requires Large Language Models (LLMs) based\nagents to acquire high-value external knowledge iteratively. Previous work has\nexplored reinforcement learning (RL) to train LLMs to perform search-based\ndocument retrieval, achieving notable improvements in QA performance, but\nunderperform on complex, multi-hop QA resulting from the sparse rewards from\nglobal signal only. To address this gap in existing research, we introduce\nStepSearch, a framework for search LLMs that trained with step-wise proximal\npolicy optimization method. It consists of richer and more detailed\nintermediate search rewards and token-level process supervision based on\ninformation gain and redundancy penalties to better guide each search step. We\nconstructed a fine-grained question-answering dataset containing\nsub-question-level search trajectories based on open source datasets through a\nset of data pipeline method. On standard multi-hop QA benchmarks, it\nsignificantly outperforms global-reward baselines, achieving 11.2% and 4.2%\nabsolute improvements for 3B and 7B models over various search with RL\nbaselines using only 19k training data, demonstrating the effectiveness of\nfine-grained, stepwise supervision in optimizing deep search LLMs. Our\nimplementation is publicly available at\nhttps://github.com/zxh20001117/StepSearch.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["proximal policy optimization", "reinforcement learning", "policy optimization"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15810", "pdf": "https://arxiv.org/pdf/2505.15810", "abs": "https://arxiv.org/abs/2505.15810", "authors": ["Yuqi Zhou", "Sunhao Dai", "Shuai Wang", "Kaiwen Zhou", "Qinqlin Jia", "Junxu"], "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward function", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15034", "pdf": "https://arxiv.org/pdf/2505.15034", "abs": "https://arxiv.org/abs/2505.15034", "authors": ["Kaiwen Zha", "Zhengqi Gao", "Maohao Shen", "Zhang-Wei Hong", "Duane S. Boning", "Dina Katabi"], "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Tech report. The first two authors contributed equally", "summary": "Reinforcement learning (RL) has recently emerged as a compelling approach for\nenhancing the reasoning capabilities of large language models (LLMs), where an\nLLM generator serves as a policy guided by a verifier (reward model). However,\ncurrent RL post-training methods for LLMs typically use verifiers that are\nfixed (rule-based or frozen pretrained) or trained discriminatively via\nsupervised fine-tuning (SFT). Such designs are susceptible to reward hacking\nand generalize poorly beyond their training distributions. To overcome these\nlimitations, we propose Tango, a novel framework that uses RL to concurrently\ntrain both an LLM generator and a verifier in an interleaved manner. A central\ninnovation of Tango is its generative, process-level LLM verifier, which is\ntrained via RL and co-evolves with the generator. Importantly, the verifier is\ntrained solely based on outcome-level verification correctness rewards without\nrequiring explicit process-level annotations. This generative RL-trained\nverifier exhibits improved robustness and superior generalization compared to\ndeterministic or SFT-trained verifiers, fostering effective mutual\nreinforcement with the generator. Extensive experiments demonstrate that both\ncomponents of Tango achieve state-of-the-art results among 7B/8B-scale models:\nthe generator attains best-in-class performance across five competition-level\nmath benchmarks and four challenging out-of-domain reasoning tasks, while the\nverifier leads on the ProcessBench dataset. Remarkably, both components exhibit\nparticularly substantial improvements on the most difficult mathematical\nreasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "reinforcement learning", "reward hacking"], "score": 3}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14972", "pdf": "https://arxiv.org/pdf/2505.14972", "abs": "https://arxiv.org/abs/2505.14972", "authors": ["Haoyi Qiu", "Kung-Hsiang Huang", "Ruichen Zheng", "Jiao Sun", "Nanyun Peng"], "title": "Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies", "categories": ["cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) are increasingly deployed in globally\ndistributed applications, such as tourism assistants, yet their ability to\nproduce culturally appropriate responses remains underexplored. Existing\nmultimodal safety benchmarks primarily focus on physical safety and overlook\nviolations rooted in cultural norms, which can result in symbolic harm. To\naddress this gap, we introduce CROSS, a benchmark designed to assess the\ncultural safety reasoning capabilities of LVLMs. CROSS includes 1,284\nmultilingual visually grounded queries from 16 countries, three everyday\ndomains, and 14 languages, where cultural norm violations emerge only when\nimages are interpreted in context. We propose CROSS-Eval, an intercultural\ntheory-based framework that measures four key dimensions: cultural awareness,\nnorm education, compliance, and helpfulness. Using this framework, we evaluate\n21 leading LVLMs, including mixture-of-experts models and reasoning models.\nResults reveal significant cultural safety gaps: the best-performing model\nachieves only 61.79% in awareness and 37.73% in compliance. While some\nopen-source models reach GPT-4o-level performance, they still fall notably\nshort of proprietary models. Our results further show that increasing reasoning\ncapacity improves cultural alignment but does not fully resolve the issue. To\nimprove model performance, we develop two enhancement strategies: supervised\nfine-tuning with culturally grounded, open-ended data and preference tuning\nwith contrastive response pairs that highlight safe versus unsafe behaviors.\nThese methods substantially improve GPT-4o's cultural awareness (+60.14%) and\ncompliance (+55.2%), while preserving general multimodal capabilities with\nminimal performance reduction on general multimodal understanding benchmarks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "helpfulness", "safety"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15173", "pdf": "https://arxiv.org/pdf/2505.15173", "abs": "https://arxiv.org/abs/2505.15173", "authors": ["Zhipei Xu", "Xuanyu Zhang", "Xing Zhou", "Jian Zhang"], "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnologies, particularly in video generation, has led to unprecedented\ncreative capabilities but also increased threats to information integrity,\nidentity security, and public trust. Existing detection methods, while\neffective in general scenarios, lack robust solutions for human-centric videos,\nwhich pose greater risks due to their realism and potential for legal and\nethical misuse. Moreover, current detection approaches often suffer from poor\ngeneralization, limited scalability, and reliance on labor-intensive supervised\nfine-tuning. To address these challenges, we propose AvatarShield, the first\ninterpretable MLLM-based framework for detecting human-centric fake videos,\nenhanced via Group Relative Policy Optimization (GRPO). Through our carefully\ndesigned accuracy detection reward and temporal compensation reward, it\neffectively avoids the use of high-cost text annotation data, enabling precise\ntemporal modeling and forgery detection. Meanwhile, we design a dual-encoder\narchitecture, combining high-level semantic reasoning and low-level artifact\namplification to guide MLLMs in effective forgery detection. We further collect\nFakeHumanVid, a large-scale human-centric video benchmark that includes\nsynthesis methods guided by pose, audio, and text inputs, enabling rigorous\nevaluation of detection methods in real-world scenes. Extensive experiments\nshow that AvatarShield significantly outperforms existing approaches in both\nin-domain and cross-domain detection, setting a new standard for human-centric\nvideo forensics.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15055", "pdf": "https://arxiv.org/pdf/2505.15055", "abs": "https://arxiv.org/abs/2505.15055", "authors": ["Hongli Zhou", "Hui Huang", "Ziqing Zhao", "Lvyuan Han", "Huicheng Wang", "Kehai Chen", "Muyun Yang", "Wei Bao", "Jian Dong", "Bing Xu", "Conghui Zhu", "Hailong Cao", "Tiejun Zhao"], "title": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory", "categories": ["cs.CL"], "comment": null, "summary": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining main-stream prominent LLM benchmarks using results\nfrom diverse models. We first propose a new framework for accurate and reliable\nestimations of item characteristics and model abilities. Specifically, we\npropose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced\nItem Response Theory framework that incorporates a rich set of item parameters\nwithin an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive\nanalysis which reveals significant and varied shortcomings in the measurement\nquality of current benchmarks. Furthermore, we demonstrate that leveraging\nPSN-IRT is able to construct smaller benchmarks while maintaining stronger\nalignment with human preference.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "human preference"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15214", "pdf": "https://arxiv.org/pdf/2505.15214", "abs": "https://arxiv.org/abs/2505.15214", "authors": ["Sangyeon Yoon", "Wonje Jeung", "Albert No"], "title": "R-TOFU: Unlearning in Large Reasoning Models", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Large Reasoning Models (LRMs) embed private or copyrighted information not\nonly in their final answers but also throughout multi-step chain-of-thought\n(CoT) traces, making reliable unlearning far more demanding than in standard\nLLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to\nthis setting. R-TOFU augments existing unlearning tasks with realistic CoT\nannotations and provides step-wise metrics that expose residual knowledge\ninvisible to answer-level checks. Using R-TOFU, we carry out a comprehensive\ncomparison of gradient-based and preference-optimization baselines and show\nthat conventional answer-only objectives leave substantial forget traces in\nreasoning. We further propose Reasoned IDK, a preference-optimization variant\nthat preserves coherent yet inconclusive reasoning, achieving a stronger\nbalance between forgetting efficacy and model utility than earlier refusal\nstyles. Finally, we identify a failure mode: decoding variants such as\nZeroThink and LessThink can still reveal forgotten content despite seemingly\nsuccessful unlearning, emphasizing the need to evaluate models under diverse\ndecoding settings. Together, the benchmark, analysis, and new baseline\nestablish a systematic foundation for studying and improving unlearning in LRMs\nwhile preserving their reasoning capabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "comparison"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15249", "pdf": "https://arxiv.org/pdf/2505.15249", "abs": "https://arxiv.org/abs/2505.15249", "authors": ["Yerin Hwang", "Dongryeol Lee", "Kyungmin Min", "Taegwan Kang", "Yong-il Kim", "Kyomin Jung"], "title": "Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": "(21pgs, 12 Tables, 9 Figures)", "summary": "Recently, large vision-language models (LVLMs) have emerged as the preferred\ntools for judging text-image alignment, yet their robustness along the visual\nmodality remains underexplored. This work is the first study to address a key\nresearch question: Can adversarial visual manipulations systematically fool\nLVLM judges into assigning unfairly inflated scores? We define potential image\ninduced biases within the context of T2I evaluation and examine how these\nbiases affect the evaluations of LVLM judges. Moreover, we introduce a novel,\nfine-grained, multi-domain meta-evaluation benchmark named FRAME, which is\ndeliberately constructed to exhibit diverse score distributions. By introducing\nthe defined biases into the benchmark, we reveal that all tested LVLM judges\nexhibit vulnerability across all domains, consistently inflating scores for\nmanipulated images. Further analysis reveals that combining multiple biases\namplifies their effects, and pairwise evaluations are similarly susceptible.\nMoreover, we observe that visual biases persist under prompt-based mitigation\nstrategies, highlighting the vulnerability of current LVLM evaluation systems\nand underscoring the urgent need for more robust LVLM judges.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15277", "pdf": "https://arxiv.org/pdf/2505.15277", "abs": "https://arxiv.org/abs/2505.15277", "authors": ["Hyungjoo Chae", "Sunghwan Kim", "Junhee Cho", "Seungone Kim", "Seungjun Moon", "Gyeom Hwangbo", "Dongha Lim", "Minjin Kim", "Yeonjun Hwang", "Minju Gwak", "Dongwook Choi", "Minseok Kang", "Gwanhoon Im", "ByeongUng Cho", "Hyojun Kim", "Jun Hee Han", "Taeyoon Kwon", "Minju Kim", "Beong-woo Kwak", "Dongjin Kang", "Jinyoung Yeo"], "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Web navigation is a unique domain that can automate many repetitive real-life\ntasks and is challenging as it requires long-horizon sequential decision making\nbeyond typical multimodal large language model (MLLM) tasks. Yet, specialized\nreward models for web navigation that can be utilized during both training and\ntest-time have been absent until now. Despite the importance of speed and\ncost-effectiveness, prior works have utilized MLLMs as reward models, which\nposes significant constraints for real-world deployment. To address this, in\nthis work, we propose the first process reward model (PRM) called Web-Shepherd\nwhich could assess web navigation trajectories in a step-level. To achieve\nthis, we first construct the WebPRM Collection, a large-scale dataset with 40K\nstep-level preference pairs and annotated checklists spanning diverse domains\nand difficulty levels. Next, we also introduce the WebRewardBench, the first\nmeta-evaluation benchmark for evaluating PRMs. In our experiments, we observe\nthat our Web-Shepherd achieves about 30 points better accuracy compared to\nusing GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by\nusing GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve\n10.9 points better performance, in 10 less cost compared to using GPT-4o-mini\nas the verifier. Our model, dataset, and code are publicly available at LINK.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "scale"], "score": 2}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "preference"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15779", "pdf": "https://arxiv.org/pdf/2505.15779", "abs": "https://arxiv.org/abs/2505.15779", "authors": ["Chuanhao Li", "Jianwen Sun", "Yukang Feng", "Mingliang Zhai", "Yifan Chang", "Kaipeng Zhang"], "title": "IA-T2I: Internet-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 7 figures, a framework that integrates reference images\n  from the Internet into T2I/TI2I models", "summary": "Current text-to-image (T2I) generation models achieve promising results, but\nthey fail on the scenarios where the knowledge implied in the text prompt is\nuncertain. For example, a T2I model released in February would struggle to\ngenerate a suitable poster for a movie premiering in April, because the\ncharacter designs and styles are uncertain to the model. To solve this problem,\nwe propose an Internet-Augmented text-to-image generation (IA-T2I) framework to\ncompel T2I models clear about such uncertain knowledge by providing them with\nreference images. Specifically, an active retrieval module is designed to\ndetermine whether a reference image is needed based on the given text prompt; a\nhierarchical image selection module is introduced to find the most suitable\nimage returned by an image search engine to enhance the T2I model; a\nself-reflection mechanism is presented to continuously evaluate and refine the\ngenerated image to ensure faithful alignment with the text prompt. To evaluate\nthe proposed framework's performance, we collect a dataset named Img-Ref-T2I,\nwhere text prompts include three types of uncertain knowledge: (1) known but\nrare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt\nto guide GPT-4o in making preference evaluation, which has been shown to have\nan evaluation accuracy similar to that of human preference evaluation.\nExperimental results demonstrate the effectiveness of our framework,\noutperforming GPT-4o by about 30% in human evaluation.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset", "human preference", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15809", "pdf": "https://arxiv.org/pdf/2505.15809", "abs": "https://arxiv.org/abs/2505.15809", "authors": ["Ling Yang", "Ye Tian", "Bowen Li", "Xinchen Zhang", "Ke Shen", "Yunhai Tong", "Mengdi Wang"], "title": "MMaDA: Multimodal Large Diffusion Language Models", "categories": ["cs.CV"], "comment": "Project: https://github.com/Gen-Verse/MMaDA", "summary": "We introduce MMaDA, a novel class of multimodal diffusion foundation models\ndesigned to achieve superior performance across diverse domains such as textual\nreasoning, multimodal understanding, and text-to-image generation. The approach\nis distinguished by three key innovations: (i) MMaDA adopts a unified diffusion\narchitecture with a shared probabilistic formulation and a modality-agnostic\ndesign, eliminating the need for modality-specific components. This\narchitecture ensures seamless integration and processing across different data\ntypes. (ii) We implement a mixed long chain-of-thought (CoT) fine-tuning\nstrategy that curates a unified CoT format across modalities. By aligning\nreasoning processes between textual and visual domains, this strategy\nfacilitates cold-start training for the final reinforcement learning (RL)\nstage, thereby enhancing the model's ability to handle complex tasks from the\noutset. (iii) We propose UniGRPO, a unified policy-gradient-based RL algorithm\nspecifically tailored for diffusion foundation models. Utilizing diversified\nreward modeling, UniGRPO unifies post-training across both reasoning and\ngeneration tasks, ensuring consistent performance improvements. Experimental\nresults demonstrate that MMaDA-8B exhibits strong generalization capabilities\nas a unified multimodal foundation model. It surpasses powerful models like\nLLaMA-3-7B and Qwen2-7B in textual reasoning, outperforms Show-o and SEED-X in\nmultimodal understanding, and excels over SDXL and Janus in text-to-image\ngeneration. These achievements highlight MMaDA's effectiveness in bridging the\ngap between pretraining and post-training within unified diffusion\narchitectures, providing a comprehensive framework for future research and\ndevelopment. We open-source our code and trained models at:\nhttps://github.com/Gen-Verse/MMaDA", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward modeling", "reinforcement learning"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15607", "pdf": "https://arxiv.org/pdf/2505.15607", "abs": "https://arxiv.org/abs/2505.15607", "authors": ["David Dinucu-Jianu", "Jakub Macina", "Nico Daheim", "Ido Hakimi", "Iryna Gurevych", "Mrinmaya Sachan"], "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "David Dinucu-Jianu and Jakub Macina contributed equally. Code\n  available: https://github.com/eth-lre/PedagogicalRL", "summary": "Large language models (LLMs) can transform education, but their optimization\nfor direct question-answering often undermines effective pedagogy which\nrequires strategically withholding answers. To mitigate this, we propose an\nonline reinforcement learning (RL)-based alignment framework that can quickly\nadapt LLMs into effective tutors using simulated student-tutor interactions by\nemphasizing pedagogical quality and guided problem-solving over simply giving\naway answers. We use our method to train a 7B parameter tutor model without\nhuman annotations which reaches similar performance to larger proprietary\nmodels like LearnLM. We introduce a controllable reward weighting to balance\npedagogical support and student solving accuracy, allowing us to trace the\nPareto frontier between these two objectives. Our models better preserve\nreasoning capabilities than single-turn SFT baselines and can optionally\nenhance interpretability through thinking tags that expose the model's\ninstructional planning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15692", "pdf": "https://arxiv.org/pdf/2505.15692", "abs": "https://arxiv.org/abs/2505.15692", "authors": ["Jinyang Wu", "Chonghua Liao", "Mingkuan Feng", "Shuai Zhang", "Zhengqi Wen", "Pengpeng Shao", "Huazhe Xu", "Jianhua Tao"], "title": "Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) has emerged as an effective method for training\nreasoning models. However, existing RL approaches typically bias the model's\noutput distribution toward reward-maximizing paths without introducing external\nknowledge. This limits their exploration capacity and results in a narrower\nreasoning capability boundary compared to base models. To address this\nlimitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel\nframework that augments RL by incorporating external high-level guidance\n(\"thought patterns\"). By adaptively integrating structured thoughts during\ntraining, TAPO effectively balances model-internal exploration and external\nguidance exploitation. Extensive experiments show that our approach\nsignificantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva\nMath. Notably, these high-level thought patterns, abstracted from only 500\nprior samples, generalize effectively across various tasks and models. This\nhighlights TAPO's potential for broader applications across multiple tasks and\ndomains. Our further analysis reveals that introducing external guidance\nproduces powerful reasoning models with superior explainability of inference\nbehavior and enhanced output readability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15710", "pdf": "https://arxiv.org/pdf/2505.15710", "abs": "https://arxiv.org/abs/2505.15710", "authors": ["Tianqi Du", "Zeming Wei", "Quan Chen", "Chenheng Zhang", "Yisen Wang"], "title": "Advancing LLM Safe Alignment with Safety Representation Ranking", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has demonstrated\nmilestone success in a variety of tasks, yet their potential for generating\nharmful content has raised significant safety concerns. Existing safety\nevaluation approaches typically operate directly on textual responses,\noverlooking the rich information embedded in the model's internal\nrepresentations. In this paper, we propose Safety Representation Ranking (SRR),\na listwise ranking framework that selects safe responses using hidden states\nfrom the LLM itself. SRR encodes both instructions and candidate completions\nusing intermediate transformer representations and ranks candidates via a\nlightweight similarity-based scorer. Our approach directly leverages internal\nmodel states and supervision at the list level to capture subtle safety\nsignals. Experiments across multiple benchmarks show that SRR significantly\nimproves robustness to adversarial prompts. Our code will be available upon\npublication.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15249", "pdf": "https://arxiv.org/pdf/2505.15249", "abs": "https://arxiv.org/abs/2505.15249", "authors": ["Yerin Hwang", "Dongryeol Lee", "Kyungmin Min", "Taegwan Kang", "Yong-il Kim", "Kyomin Jung"], "title": "Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": "(21pgs, 12 Tables, 9 Figures)", "summary": "Recently, large vision-language models (LVLMs) have emerged as the preferred\ntools for judging text-image alignment, yet their robustness along the visual\nmodality remains underexplored. This work is the first study to address a key\nresearch question: Can adversarial visual manipulations systematically fool\nLVLM judges into assigning unfairly inflated scores? We define potential image\ninduced biases within the context of T2I evaluation and examine how these\nbiases affect the evaluations of LVLM judges. Moreover, we introduce a novel,\nfine-grained, multi-domain meta-evaluation benchmark named FRAME, which is\ndeliberately constructed to exhibit diverse score distributions. By introducing\nthe defined biases into the benchmark, we reveal that all tested LVLM judges\nexhibit vulnerability across all domains, consistently inflating scores for\nmanipulated images. Further analysis reveals that combining multiple biases\namplifies their effects, and pairwise evaluations are similarly susceptible.\nMoreover, we observe that visual biases persist under prompt-based mitigation\nstrategies, highlighting the vulnerability of current LVLM evaluation systems\nand underscoring the urgent need for more robust LVLM judges.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "fine-grained"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15776", "pdf": "https://arxiv.org/pdf/2505.15776", "abs": "https://arxiv.org/abs/2505.15776", "authors": ["Changtai Zhu", "Siyin Wang", "Ruijun Feng", "Kai Song", "Xipeng Qiu"], "title": "ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Conversational search systems require effective handling of context-dependent\nqueries that often contain ambiguity, omission, and coreference. Conversational\nQuery Reformulation (CQR) addresses this challenge by transforming these\nqueries into self-contained forms suitable for off-the-shelf retrievers.\nHowever, existing CQR approaches suffer from two critical constraints: high\ndependency on costly external supervision from human annotations or large\nlanguage models, and insufficient alignment between the rewriting model and\ndownstream retrievers. We present ConvSearch-R1, the first self-driven\nframework that completely eliminates dependency on external rewrite supervision\nby leveraging reinforcement learning to optimize reformulation directly through\nretrieval signals. Our novel two-stage approach combines Self-Driven Policy\nWarm-Up to address the cold-start problem through retrieval-guided\nself-distillation, followed by Retrieval-Guided Reinforcement Learning with a\nspecially designed rank-incentive reward shaping mechanism that addresses the\nsparsity issue in conventional retrieval metrics. Extensive experiments on\nTopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly\noutperforms previous state-of-the-art methods, achieving over 10% improvement\non the challenging TopiOCQA dataset while using smaller 3B parameter models\nwithout any external supervision.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14999", "pdf": "https://arxiv.org/pdf/2505.14999", "abs": "https://arxiv.org/abs/2505.14999", "authors": ["Eric Hanchen Jiang", "Haozheng Luo", "Shengyuan Pang", "Xiaomin Li", "Zhenting Qi", "Hengli Li", "Cheng-Fu Yang", "Zongyu Lin", "Xinfeng Li", "Hao Xu", "Kai-Wei Chang", "Ying Nian Wu"], "title": "Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Mathematical reasoning presents a significant challenge for Large Language\nModels (LLMs), often requiring robust multi step logical consistency. While\nChain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee\ncorrectness, and improving reliability via extensive sampling is\ncomputationally costly. This paper introduces the Energy Outcome Reward Model\n(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy\nBased Models (EBMs) to simplify the training of reward models by learning to\nassign a scalar energy score to CoT solutions using only outcome labels,\nthereby avoiding detailed annotations. It achieves this by interpreting\ndiscriminator output logits as negative energies, effectively ranking\ncandidates where lower energy is assigned to solutions leading to correct final\noutcomes implicitly favoring coherent reasoning. On mathematical benchmarks\n(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with\nLlama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively\nleverages a given pool of candidate solutions to match or exceed the\nperformance of brute force sampling, thereby enhancing LLM reasoning outcome\nreliability through its streamlined post hoc verification process.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["chain of thought"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reward model", "ranking"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency", "reliability", "accuracy", "mathematical reasoning"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15201", "pdf": "https://arxiv.org/pdf/2505.15201", "abs": "https://arxiv.org/abs/2505.15201", "authors": ["Christian Walder", "Deep Karkhanis"], "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts\nfor each problem and reward them independently. This optimizes for pass@1\nperformance and prioritizes the strength of isolated samples at the expense of\nthe diversity and collective utility of sets of samples. This under-utilizes\nthe sampling capacity, limiting exploration and eventual improvement on harder\nexamples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a\ntransformation on the final rewards which leads to direct optimization of\npass@k performance, thus optimizing for sets of samples that maximize reward\nwhen considered jointly. Our contribution is to derive novel low variance\nunbiased estimators for pass@k and its gradient, in both the binary and\ncontinuous reward settings. We show optimization with our estimators reduces to\nstandard RL with rewards that have been jointly transformed by a stable and\nefficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable\nrobust optimization of pass@k for any arbitrary k <= n. Moreover, instead of\ntrading off pass@1 performance for pass@k gains, our method allows annealing k\nduring training, optimizing both metrics and often achieving strong pass@1\nnumbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the\nvariance reducing properties of our formulations. We also include real-world\nexamples using the open-source LLM, GEMMA-2. We find that our transformation\neffectively optimizes for the target k. Furthermore, higher k values enable\nsolving more and harder problems, while annealing k boosts both the pass@1 and\npass@k . Crucially, for challenging task sets where conventional pass@1\noptimization stalls, our pass@k approach unblocks learning, likely due to\nbetter exploration by prioritizing joint utility over the utility of individual\nsamples.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning", "policy optimization"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15311", "pdf": "https://arxiv.org/pdf/2505.15311", "abs": "https://arxiv.org/abs/2505.15311", "authors": ["Yurun Yuan", "Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Policy-based methods currently dominate reinforcement learning (RL) pipelines\nfor large language model (LLM) reasoning, leaving value-based approaches\nlargely unexplored. We revisit the classical paradigm of Bellman Residual\nMinimization and introduce Trajectory Bellman Residual Minimization (TBRM), an\nalgorithm that naturally adapts this idea to LLMs, yielding a simple yet\neffective off-policy algorithm that optimizes a single trajectory-level Bellman\nobjective using the model's own logits as $Q$-values. TBRM removes the need for\ncritics, importance-sampling ratios, or clipping, and operates with only one\nrollout per prompt. We prove convergence to the near-optimal KL-regularized\npolicy from arbitrary off-policy data via an improved\nchange-of-trajectory-measure analysis. Experiments on standard\nmathematical-reasoning benchmarks show that TBRM consistently outperforms\npolicy-based baselines, like PPO and GRPO, with comparable or lower\ncomputational and memory overhead. Our results indicate that value-based RL\nmight be a principled and efficient alternative for enhancing reasoning\ncapabilities in LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["PPO", "reinforcement learning"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15365", "pdf": "https://arxiv.org/pdf/2505.15365", "abs": "https://arxiv.org/abs/2505.15365", "authors": ["Stefan Pasch"], "title": "AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\nsettings, their ability to refuse ethically sensitive prompts-such as those\ninvolving hate speech or illegal activities-has become central to content\nmoderation and responsible AI practices. While refusal responses can be viewed\nas evidence of ethical alignment and safety-conscious behavior, recent research\nsuggests that users may perceive them negatively. At the same time, automated\nassessments of model outputs are playing a growing role in both evaluation and\ntraining. In particular, LLM-as-a-Judge frameworks-in which one model is used\nto evaluate the output of another-are now widely adopted to guide benchmarking\nand fine-tuning. This paper examines whether such model-based evaluators assess\nrefusal responses differently than human users. Drawing on data from Chatbot\nArena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how\ndifferent types of refusals are rated. We distinguish ethical refusals, which\nexplicitly cite safety or normative concerns (e.g., \"I can't help with that\nbecause it may be harmful\"), and technical refusals, which reflect system\nlimitations (e.g., \"I can't answer because I lack real-time data\"). We find\nthat LLM-as-a-Judge systems evaluate ethical refusals significantly more\nfavorably than human users, a divergence not observed for technical refusals.\nWe refer to this divergence as a moderation bias-a systematic tendency for\nmodel-based evaluators to reward refusal behaviors more than human users do.\nThis raises broader questions about transparency, value alignment, and the\nnormative assumptions embedded in automated evaluation systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment", "value alignment"], "score": 2}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "safety"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14705", "pdf": "https://arxiv.org/pdf/2505.14705", "abs": "https://arxiv.org/abs/2505.14705", "authors": ["Xin Zhang", "Ziruo Zhang", "Jiawei Du", "Zuozhu Liu", "Joey Tianyi Zhou"], "title": "Beyond Modality Collapse: Representations Blending for Multimodal Dataset Distillation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Dataset Distillation (MDD) seeks to condense large-scale\nimage-text datasets into compact surrogates while retaining their effectiveness\nfor cross-modal learning. Despite recent progress, existing MDD approaches\noften suffer from \\textit{\\textbf{Modality Collapse}}, characterized by\nover-concentrated intra-modal representations and enlarged distributional gap\nacross modalities. In this paper, at the first time, we identify this issue as\nstemming from a fundamental conflict between the over-compression behavior\ninherent in dataset distillation and the cross-modal supervision imposed by\ncontrastive objectives. To alleviate modality collapse, we introduce\n\\textbf{RepBlend}, a novel MDD framework that weakens overdominant cross-modal\nsupervision via representation blending, thereby significantly enhancing\nintra-modal diversity. Additionally, we observe that current MDD methods impose\nasymmetric supervision across modalities, resulting in biased optimization. To\naddress this, we propose symmetric projection trajectory matching, which\nsynchronizes the optimization dynamics using modality-specific projection\nheads, thereby promoting balanced supervision and enhancing cross-modal\nalignment. Experiments on Flickr-30K and MS-COCO show that RepBlend\nconsistently outperforms prior state-of-the-art MDD methods, achieving\nsignificant gains in retrieval performance (e.g., +9.4 IR@10, +6.3 TR@10 under\nthe 100-pair setting) and offering up to 6.7$\\times$ distillation speedup.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14810", "pdf": "https://arxiv.org/pdf/2505.14810", "abs": "https://arxiv.org/abs/2505.14810", "authors": ["Tingchen Fu", "Jiawei Gu", "Yafu Li", "Xiaoye Qu", "Yu Cheng"], "title": "Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction-following is essential for aligning large language models (LLMs)\nwith user intent. While recent reasoning-oriented models exhibit impressive\nperformance on complex mathematical problems, their ability to adhere to\nnatural language instructions remains underexplored. In this work, we introduce\nMathIF, a dedicated benchmark for evaluating instruction-following in\nmathematical reasoning tasks. Our empirical analysis reveals a consistent\ntension between scaling up reasoning capacity and maintaining controllability,\nas models that reason more effectively often struggle to comply with user\ndirectives. We find that models tuned on distilled long chains-of-thought or\ntrained with reasoning-oriented reinforcement learning often degrade in\ninstruction adherence, especially when generation length increases.\nFurthermore, we show that even simple interventions can partially recover\nobedience, though at the cost of reasoning performance. These findings\nhighlight a fundamental tension in current LLM training paradigms and motivate\nthe need for more instruction-aware reasoning models. We release the code and\ndata at https://github.com/TingchenFu/MathIF.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scaling"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "mathematical reasoning"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14818", "pdf": "https://arxiv.org/pdf/2505.14818", "abs": "https://arxiv.org/abs/2505.14818", "authors": ["Leon Lin", "Jun Zheng", "Haidong Wang"], "title": "WebNovelBench: Placing LLM Novelists on the Web Novel Distribution", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Robustly evaluating the long-form storytelling capabilities of Large Language\nModels (LLMs) remains a significant challenge, as existing benchmarks often\nlack the necessary scale, diversity, or objective measures. To address this, we\nintroduce WebNovelBench, a novel benchmark specifically designed for evaluating\nlong-form novel generation. WebNovelBench leverages a large-scale dataset of\nover 4,000 Chinese web novels, framing evaluation as a synopsis-to-story\ngeneration task. We propose a multi-faceted framework encompassing eight\nnarrative quality dimensions, assessed automatically via an LLM-as-Judge\napproach. Scores are aggregated using Principal Component Analysis and mapped\nto a percentile rank against human-authored works. Our experiments demonstrate\nthat WebNovelBench effectively differentiates between human-written\nmasterpieces, popular web novels, and LLM-generated content. We provide a\ncomprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling\nabilities and offering insights for future development. This benchmark provides\na scalable, replicable, and data-driven methodology for assessing and advancing\nLLM-driven narrative generation.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "dataset"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14728", "pdf": "https://arxiv.org/pdf/2505.14728", "abs": "https://arxiv.org/abs/2505.14728", "authors": ["Xiao Lin", "Zhining Liu", "Ze Yang", "Gaotang Li", "Ruizhong Qiu", "Shuke Wang", "Hui Liu", "Haotian Li", "Sumit Keswani", "Vishwa Pardeshi", "Huijun Zhao", "Wei Fan", "Hanghang Tong"], "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM"], "comment": "21 pages, 11 figures, 7 tables", "summary": "Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "fine-grained"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14886", "pdf": "https://arxiv.org/pdf/2505.14886", "abs": "https://arxiv.org/abs/2505.14886", "authors": ["Danqing Wang", "Zhuorui Ye", "Xinran Zhao", "Fei Fang", "Lei Li"], "title": "Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters", "categories": ["cs.CL"], "comment": "9 main pages", "summary": "Winning competitive debates requires sophisticated reasoning and argument\nskills. There are unique challenges in the competitive debate: (1) The time\nconstraints force debaters to make strategic choices about which points to\npursue rather than covering all possible arguments; (2) The persuasiveness of\nthe debate relies on the back-and-forth interaction between arguments, which a\nsingle final game status cannot evaluate. To address these challenges, we\npropose TreeDebater, a novel debate framework that excels in competitive\ndebate. We introduce two tree structures: the Rehearsal Tree and Debate Flow\nTree. The Rehearsal Tree anticipates the attack and defenses to evaluate the\nstrength of the claim, while the Debate Flow Tree tracks the debate status to\nidentify the active actions. TreeDebater allocates its time budget among\ncandidate actions and uses the speech time controller and feedback from the\nsimulated audience to revise its statement. The human evaluation on both the\nstage-level and the debate-level comparison shows that our TreeDebater\noutperforms the state-of-the-art multi-agent debate system. Further\ninvestigation shows that TreeDebater shows better strategies in limiting time\nto important debate actions, aligning with the strategies of human debate\nexperts.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15158", "pdf": "https://arxiv.org/pdf/2505.15158", "abs": "https://arxiv.org/abs/2505.15158", "authors": ["Yunsheng Ma", "Burhaneddin Yaman", "Xin Ye", "Mahmut Yurt", "Jingru Luo", "Abhirup Mallik", "Ziran Wang", "Liu Ren"], "title": "ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages", "summary": "Recent advances have explored integrating large language models (LLMs) into\nend-to-end autonomous driving systems to enhance generalization and\ninterpretability. However, most existing approaches are limited to either\ndriving performance or vision-language reasoning, making it difficult to\nachieve both simultaneously. In this paper, we propose ALN-P3, a unified\nco-distillation framework that introduces cross-modal alignment between \"fast\"\nvision-based autonomous driving systems and \"slow\" language-driven reasoning\nmodules. ALN-P3 incorporates three novel alignment mechanisms: Perception\nAlignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),\nwhich explicitly align visual tokens with corresponding linguistic outputs\nacross the full perception, prediction, and planning stack. All alignment\nmodules are applied only during training and incur no additional costs during\ninference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,\nTOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both\ndriving decisions and language reasoning, achieving state-of-the-art results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15172", "pdf": "https://arxiv.org/pdf/2505.15172", "abs": "https://arxiv.org/abs/2505.15172", "authors": ["Xinran Wang", "Muxi Diao", "Yuanzhi Liu", "Chunyu Wang", "Kongming Liang", "Zhanyu Ma", "Jun Guo"], "title": "Harnessing Caption Detailness for Data-Efficient Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Training text-to-image (T2I) models with detailed captions can significantly\nimprove their generation quality. Existing methods often rely on simplistic\nmetrics like caption length to represent the detailness of the caption in the\nT2I training set. In this paper, we propose a new metric to estimate caption\ndetailness based on two aspects: image coverage rate (ICR), which evaluates\nwhether the caption covers all regions/objects in the image, and average object\ndetailness (AOD), which quantifies the detailness of each object's description.\nThrough experiments on the COCO dataset using ShareGPT4V captions, we\ndemonstrate that T2I models trained on high-ICR and -AOD captions achieve\nsuperior performance on DPG and other benchmarks. Notably, our metric enables\nmore effective data selection-training on only 20% of full data surpasses both\nfull-dataset training and length-based selection method, improving alignment\nand reconstruction ability. These findings highlight the critical role of\ndetail-aware metrics over length-based heuristics in caption selection for T2I\ntasks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15191", "pdf": "https://arxiv.org/pdf/2505.15191", "abs": "https://arxiv.org/abs/2505.15191", "authors": ["Hana Satou", "Alan Mitkiy", "F Monkey"], "title": "Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation", "categories": ["cs.CV"], "comment": null, "summary": "Transfer learning under domain shift remains a fundamental challenge due to\nthe divergence between source and target data manifolds. In this paper, we\npropose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework\nthat decomposes adversarial perturbations into on-manifold and off-manifold\ncomponents to simultaneously capture semantic variation and model brittleness.\nWe theoretically demonstrate that enforcing on-manifold consistency reduces\nhypothesis complexity and improves generalization, while off-manifold\nregularization smooths decision boundaries in low-density regions. Moreover, we\nintroduce a geometry-aware alignment loss that minimizes geodesic discrepancy\nbetween source and target manifolds. Experiments on DomainNet, VisDA, and\nOffice-Home show that MAADA consistently outperforms existing adversarial and\nadaptation methods in both unsupervised and few-shot settings, demonstrating\nsuperior structural robustness and cross-domain generalization.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15031", "pdf": "https://arxiv.org/pdf/2505.15031", "abs": "https://arxiv.org/abs/2505.15031", "authors": ["Wenqing Wu", "Haixu Xi", "Chengzhi Zhang"], "title": "Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Peer review is vital in academia for evaluating research quality. Top AI\nconferences use reviewer confidence scores to ensure review reliability, but\nexisting studies lack fine-grained analysis of text-score consistency,\npotentially missing key details. This work assesses consistency at word,\nsentence, and aspect levels using deep learning and NLP conference review data.\nWe employ deep learning to detect hedge sentences and aspects, then analyze\nreport length, hedge word/sentence frequency, aspect mentions, and sentiment to\nevaluate text-score alignment. Correlation, significance, and regression tests\nexamine confidence scores' impact on paper outcomes. Results show high\ntext-score consistency across all levels, with regression revealing higher\nconfidence scores correlate with paper rejection, validating expert assessments\nand peer review fairness.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["correlation", "consistency", "reliability", "fine-grained"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15194", "pdf": "https://arxiv.org/pdf/2505.15194", "abs": "https://arxiv.org/abs/2505.15194", "authors": ["Hana Satou", "F Monkey"], "title": "GAMA: Geometry-Aware Manifold Alignment via Structured Adversarial Perturbations for Robust Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptation remains a challenge when there is significant manifold\ndiscrepancy between source and target domains. Although recent methods leverage\nmanifold-aware adversarial perturbations to perform data augmentation, they\noften neglect precise manifold alignment and systematic exploration of\nstructured perturbations. To address this, we propose GAMA (Geometry-Aware\nManifold Alignment), a structured framework that achieves explicit manifold\nalignment via adversarial perturbation guided by geometric information. GAMA\nsystematically employs tangent space exploration and manifold-constrained\nadversarial optimization, simultaneously enhancing semantic consistency,\nrobustness to off-manifold deviations, and cross-domain alignment. Theoretical\nanalysis shows that GAMA tightens the generalization bound via structured\nregularization and explicit alignment. Empirical results on DomainNet, VisDA,\nand Office-Home demonstrate that GAMA consistently outperforms existing\nadversarial and adaptation methods in both unsupervised and few-shot settings,\nexhibiting superior robustness, generalization, and manifold alignment\ncapability.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15050", "pdf": "https://arxiv.org/pdf/2505.15050", "abs": "https://arxiv.org/abs/2505.15050", "authors": ["Gaurav Kumar", "Debajyoti Mazumder", "Ayush Garg", "Jasabanta Patro"], "title": "Improving the fact-checking performance of language models by relying on their entailment ability", "categories": ["cs.CL"], "comment": "44 pages", "summary": "Automated fact-checking is a crucial task in this digital age. To verify a\nclaim, current approaches majorly follow one of two strategies i.e. (i) relying\non embedded knowledge of language models, and (ii) fine-tuning them with\nevidence pieces. While the former can make systems to hallucinate, the later\nhave not been very successful till date. The primary reason behind this is that\nfact verification is a complex process. Language models have to parse through\nmultiple pieces of evidence before making a prediction. Further, the evidence\npieces often contradict each other. This makes the reasoning process even more\ncomplex. We proposed a simple yet effective approach where we relied on\nentailment and the generative ability of language models to produce\n''supporting'' and ''refuting'' justifications (for the truthfulness of a\nclaim). We trained language models based on these justifications and achieved\nsuperior results. Apart from that, we did a systematic comparison of different\nprompting and fine-tuning strategies, as it is currently lacking in the\nliterature. Some of our observations are: (i) training language models with raw\nevidence sentences registered an improvement up to 8.20% in macro-F1, over the\nbest performing baseline for the RAW-FC dataset, (ii) similarly, training\nlanguage models with prompted claim-evidence understanding (TBE-2) registered\nan improvement (with a margin up to 16.39%) over the baselines for the same\ndataset, (iii) training language models with entailed justifications (TBE-3)\noutperformed the baselines by a huge margin (up to 28.57% and 44.26% for\nLIAR-RAW and RAW-FC, respectively). We have shared our code repository to\nreproduce the results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "truthfulness"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15232", "pdf": "https://arxiv.org/pdf/2505.15232", "abs": "https://arxiv.org/abs/2505.15232", "authors": ["Ting Huang", "Zeyu Zhang", "Ruicheng Zhang", "Yang Zhao"], "title": "DC-Scene: Data-Centric Learning for 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "3D scene understanding plays a fundamental role in vision applications such\nas robotics, autonomous driving, and augmented reality. However, advancing\nlearning-based 3D scene understanding remains challenging due to two key\nlimitations: (1) the large scale and complexity of 3D scenes lead to higher\ncomputational costs and slower training compared to 2D counterparts; and (2)\nhigh-quality annotated 3D datasets are significantly scarcer than those\navailable for 2D vision. These challenges underscore the need for more\nefficient learning paradigms. In this work, we propose DC-Scene, a data-centric\nframework tailored for 3D scene understanding, which emphasizes enhancing data\nquality and training efficiency. Specifically, we introduce a CLIP-driven\ndual-indicator quality (DIQ) filter, combining vision-language alignment scores\nwith caption-loss perplexity, along with a curriculum scheduler that\nprogressively expands the training pool from the top 25% to 75% of\nscene-caption pairs. This strategy filters out noisy samples and significantly\nreduces dependence on large-scale labeled 3D data. Extensive experiments on\nScanRefer and Nr3D demonstrate that DC-Scene achieves state-of-the-art\nperformance (86.1 CIDEr with the top-75% subset vs. 85.4 with the full dataset)\nwhile reducing training cost by approximately two-thirds, confirming that a\ncompact set of high-quality samples can outperform exhaustive training. Code\nwill be available at https://github.com/AIGeeksGroup/DC-Scene.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15062", "pdf": "https://arxiv.org/pdf/2505.15062", "abs": "https://arxiv.org/abs/2505.15062", "authors": ["Jiashu He", "Jinxuan Fan", "Bowen Jiang", "Ignacio Houine", "Dan Roth", "Alejandro Ribeiro"], "title": "Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When addressing complex questions that require new information, people often\nassociate the question with existing knowledge to derive a sensible answer. For\ninstance, when evaluating whether melatonin aids insomnia, one might associate\n\"hormones helping mental disorders\" with \"melatonin being a hormone and\ninsomnia a mental disorder\" to complete the reasoning. Large Language Models\n(LLMs) also require such associative thinking, particularly in resolving\nscientific inquiries when retrieved knowledge is insufficient and does not\ndirectly answer the question. Graph Inspired Veracity Extrapolation (GIVE)\naddresses this by using a knowledge graph (KG) to extrapolate structured\nknowledge. However, it involves the construction and pruning of many\nhypothetical triplets, which limits efficiency and generalizability. We propose\nSelf-GIVE, a retrieve-RL framework that enhances LLMs with automatic\nassociative thinking through reinforcement learning. Self-GIVE extracts\nstructured information and entity sets to assist the model in linking to the\nqueried concepts. We address GIVE's key limitations: (1) extensive LLM calls\nand token overhead for knowledge extrapolation, (2) difficulty in deploying on\nsmaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate\nknowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE\nwith a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B\nmodels by up to $\\textbf{28.5%$\\rightarrow$71.4%}$ and\n$\\textbf{78.6$\\rightarrow$90.5%}$ in samples $\\textbf{unseen}$ in challenging\nbiomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or\noutperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\\%.\nSelf-GIVE enhances the scalable integration of structured retrieval and\nreasoning with associative thinking.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15233", "pdf": "https://arxiv.org/pdf/2505.15233", "abs": "https://arxiv.org/abs/2505.15233", "authors": ["Yuxuan Du", "Zhendong Wang", "Yuhao Luo", "Caiyong Piao", "Zhiyuan Yan", "Hao Li", "Li Yuan"], "title": "CAD: A General Multimodal Framework for Video Deepfake Detection via Cross-Modal Alignment and Distillation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid emergence of multimodal deepfakes (visual and auditory content are\nmanipulated in concert) undermines the reliability of existing detectors that\nrely solely on modality-specific artifacts or cross-modal inconsistencies. In\nthis work, we first demonstrate that modality-specific forensic traces (e.g.,\nface-swap artifacts or spectral distortions) and modality-shared semantic\nmisalignments (e.g., lip-speech asynchrony) offer complementary evidence, and\nthat neglecting either aspect limits detection performance. Existing approaches\neither naively fuse modality-specific features without reconciling their\nconflicting characteristics or focus predominantly on semantic misalignment at\nthe expense of modality-specific fine-grained artifact cues. To address these\nshortcomings, we propose a general multimodal framework for video deepfake\ndetection via Cross-Modal Alignment and Distillation (CAD). CAD comprises two\ncore components: 1) Cross-modal alignment that identifies inconsistencies in\nhigh-level semantic synchronization (e.g., lip-speech mismatches); 2)\nCross-modal distillation that mitigates feature conflicts during fusion while\npreserving modality-specific forensic traces (e.g., spectral distortions in\nsynthetic audio). Extensive experiments on both multimodal and unimodal (e.g.,\nimage-only/video-only)deepfake benchmarks demonstrate that CAD significantly\noutperforms previous methods, validating the necessity of harmonious\nintegration of multimodal complementary information.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15241", "pdf": "https://arxiv.org/pdf/2505.15241", "abs": "https://arxiv.org/abs/2505.15241", "authors": ["Kim Yun", "Hana Satou", "F Monkey"], "title": "GAMA++: Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Despite progress in geometry-aware domain adaptation, current methods such as\nGAMA still suffer from two unresolved issues: (1) insufficient disentanglement\nof task-relevant and task-irrelevant manifold dimensions, and (2) rigid\nperturbation schemes that ignore per-class alignment asymmetries. To address\nthis, we propose GAMA++, a novel framework that introduces (i) latent space\ndisentanglement to isolate label-consistent manifold directions from nuisance\nfactors, and (ii) an adaptive contrastive perturbation strategy that tailors\nboth on- and off-manifold exploration to class-specific manifold curvature and\nalignment discrepancy. We further propose a cross-domain contrastive\nconsistency loss that encourages local semantic clusters to align while\npreserving intra-domain diversity. Our method achieves state-of-the-art results\non DomainNet, Office-Home, and VisDA benchmarks under both standard and\nfew-shot settings, with notable improvements in class-level alignment fidelity\nand boundary robustness. GAMA++ sets a new standard for semantic geometry\nalignment in transfer learning.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15087", "pdf": "https://arxiv.org/pdf/2505.15087", "abs": "https://arxiv.org/abs/2505.15087", "authors": ["Zhiyu Shen", "Jiyuan Liu", "Yunhe Pang", "Yanghui Rao"], "title": "HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora", "categories": ["cs.CL"], "comment": "27 pages. Code will be available at\n  [https://github.com/Zh1yuShen/HopWeaver]", "summary": "Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's\ncapability to integrate information from diverse sources. However, creating\nextensive and high-quality MHQA datasets is challenging: (i) manual annotation\nis expensive, and (ii) current synthesis methods often produce simplistic\nquestions or require extensive manual guidance. This paper introduces\nHopWeaver, the first automatic framework synthesizing authentic multi-hop\nquestions from unstructured text corpora without human intervention. HopWeaver\nsynthesizes two types of multi-hop questions (bridge and comparison) using an\ninnovative approach that identifies complementary documents across corpora. Its\ncoherent pipeline constructs authentic reasoning paths that integrate\ninformation across multiple documents, ensuring synthesized questions\nnecessitate authentic multi-hop reasoning. We further present a comprehensive\nsystem for evaluating synthesized multi-hop questions. Empirical evaluations\ndemonstrate that the synthesized questions achieve comparable or superior\nquality to human-annotated datasets at a lower cost. Our approach is valuable\nfor developing MHQA datasets in specialized domains with scarce annotated\nresources. The code for HopWeaver is publicly available.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["annotation", "question answering"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15287", "pdf": "https://arxiv.org/pdf/2505.15287", "abs": "https://arxiv.org/abs/2505.15287", "authors": ["Yuchen Li", "Chaoran Feng", "Zhenyu Tang", "Kaiyuan Deng", "Wangbo Yu", "Yonghong Tian", "Li Yuan"], "title": "GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation", "categories": ["cs.CV"], "comment": "21 pages, 7 figures. More details at\n  http://intothemild.github.io/GS2E.github.io", "summary": "We introduce GS2E (Gaussian Splatting to Event), a large-scale synthetic\nevent dataset for high-fidelity event vision tasks, captured from real-world\nsparse multi-view RGB images. Existing event datasets are often synthesized\nfrom dense RGB videos, which typically lack viewpoint diversity and geometric\nconsistency, or depend on expensive, difficult-to-scale hardware setups. GS2E\novercomes these limitations by first reconstructing photorealistic static\nscenes using 3D Gaussian Splatting, and subsequently employing a novel,\nphysically-informed event simulation pipeline. This pipeline generally\nintegrates adaptive trajectory interpolation with physically-consistent event\ncontrast threshold modeling. Such an approach yields temporally dense and\ngeometrically consistent event streams under diverse motion and lighting\nconditions, while ensuring strong alignment with underlying scene structures.\nExperimental results on event-based 3D reconstruction demonstrate GS2E's\nsuperior generalization capabilities and its practical value as a benchmark for\nadvancing event vision research.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "consistency"], "score": 3}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15108", "pdf": "https://arxiv.org/pdf/2505.15108", "abs": "https://arxiv.org/abs/2505.15108", "authors": ["Ian Steenstra", "Timothy W. Bickmore"], "title": "A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual\nAgents acting as psychotherapists presents significant opportunities for\nexpanding mental healthcare access. However, their deployment has also been\nlinked to serious adverse outcomes, including user harm and suicide,\nfacilitated by a lack of standardized evaluation methodologies capable of\ncapturing the nuanced risks of therapeutic interaction. Current evaluation\ntechniques lack the sensitivity to detect subtle changes in patient cognition\nand behavior during therapy sessions that may lead to subsequent\ndecompensation. We introduce a novel risk taxonomy specifically designed for\nthe systematic evaluation of conversational AI psychotherapists. Developed\nthrough an iterative process including review of the psychotherapy risk\nliterature, qualitative interviews with clinical and legal experts, and\nalignment with established clinical criteria (e.g., DSM-5) and existing\nassessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured\napproach to identifying and assessing user/patient harms. We provide a\nhigh-level overview of this taxonomy, detailing its grounding, and discuss\npotential use cases. We discuss two use cases in detail: monitoring cognitive\nmodel-based risk factors during a counseling conversation to detect unsafe\ndeviations, in both human-AI counseling sessions and in automated benchmarking\nof AI psychotherapists with simulated patients. The proposed taxonomy offers a\nfoundational step towards establishing safer and more responsible innovation in\nthe domain of AI-driven mental health support.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "criteria"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15117", "pdf": "https://arxiv.org/pdf/2505.15117", "abs": "https://arxiv.org/abs/2505.15117", "authors": ["Bowen Jin", "Jinsung Yoon", "Priyanka Kargupta", "Sercan O. Arik", "Jiawei Han"], "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "22 pages", "summary": "Reinforcement learning (RL) has demonstrated strong potential in training\nlarge language models (LLMs) capable of complex reasoning for real-world\nproblem solving. More recently, RL has been leveraged to create sophisticated\nLLM-based search agents that adeptly combine reasoning with search engine use.\nWhile the use of RL for training search agents is promising, the optimal design\nof such agents remains not fully understood. In particular, key factors -- such\nas (1) reward formulation, (2) the choice and characteristics of the underlying\nLLM, and (3) the role of the search engine in the RL process -- require further\ninvestigation. In this work, we conduct comprehensive empirical studies to\nsystematically investigate these and offer actionable insights. We highlight\nseveral key findings: format rewards are effective in improving final\nperformance, whereas intermediate retrieval rewards have limited impact; the\nscale and initialization of the LLM (general-purpose vs. reasoning-specialized)\nsignificantly influence RL outcomes; and the choice of search engine plays a\ncritical role in shaping RL training dynamics and the robustness of the trained\nagent during inference. These establish important guidelines for successfully\nbuilding and deploying LLM-based search agents in real-world applications. Code\nis available at https://github.com/PeterGriffinJin/Search-R1.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15182", "pdf": "https://arxiv.org/pdf/2505.15182", "abs": "https://arxiv.org/abs/2505.15182", "authors": ["Jeonghye Kim", "Sojeong Rhee", "Minbeom Kim", "Dohyung Kim", "Sangmook Lee", "Youngchul Sung", "Kyomin Jung"], "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in LLM agents have largely built on reasoning backbones like\nReAct, which interleave thought and action in complex environments. However,\nReAct often produces ungrounded or incoherent reasoning steps, leading to\nmisalignment between the agent's actual state and goal. Our analysis finds that\nthis stems from ReAct's inability to maintain consistent internal beliefs and\ngoal alignment, causing compounding errors and hallucinations. To address this,\nwe introduce ReflAct, a novel backbone that shifts reasoning from merely\nplanning next actions to continuously reflecting on the agent's state relative\nto its goal. By explicitly grounding decisions in states and enforcing ongoing\ngoal alignment, ReflAct dramatically improves strategic reliability. This\ndesign delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%\non average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even\noutperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),\nshowing that strengthening the core reasoning backbone is key to reliable agent\nperformance.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["reliability"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15398", "pdf": "https://arxiv.org/pdf/2505.15398", "abs": "https://arxiv.org/abs/2505.15398", "authors": ["Huilin Zhu", "Senyao Li", "Jingling Yuan", "Zhengwei Yang", "Yu Guo", "Wenxuan Liu", "Xian Zhong", "Shengfeng He"], "title": "Expanding Zero-Shot Object Counting with Rich Prompts", "categories": ["cs.CV"], "comment": null, "summary": "Expanding pre-trained zero-shot counting models to handle unseen categories\nrequires more than simply adding new prompts, as this approach does not achieve\nthe necessary alignment between text and visual features for accurate counting.\nWe introduce RichCount, the first framework to address these limitations,\nemploying a two-stage training strategy that enhances text encoding and\nstrengthens the model's association with objects in images. RichCount improves\nzero-shot counting for unseen categories through two key objectives: (1)\nenriching text features with a feed-forward network and adapter trained on\ntext-image similarity, thereby creating robust, aligned representations; and\n(2) applying this refined encoder to counting tasks, enabling effective\ngeneralization across diverse prompts and complex images. In this manner,\nRichCount goes beyond simple prompt expansion to establish meaningful feature\nalignment that supports accurate counting across novel categories. Extensive\nexperiments on three benchmark datasets demonstrate the effectiveness of\nRichCount, achieving state-of-the-art performance in zero-shot counting and\nsignificantly enhancing generalization to unseen categories in open-world\nscenarios.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15257", "pdf": "https://arxiv.org/pdf/2505.15257", "abs": "https://arxiv.org/abs/2505.15257", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Yulin Hu", "Xingyu Sui", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners", "categories": ["cs.CL"], "comment": "26 pages, 13 figures", "summary": "Multilingual reasoning remains a significant challenge for large language\nmodels (LLMs), with performance disproportionately favoring high-resource\nlanguages. Drawing inspiration from cognitive neuroscience, which suggests that\nhuman reasoning functions largely independently of language processing, we\nhypothesize that LLMs similarly encode reasoning and language as separable\ncomponents that can be disentangled to enhance multilingual reasoning. To\nevaluate this, we perform a causal intervention by ablating language-specific\nrepresentations at inference time. Experiments on 10 open-source LLMs spanning\n11 typologically diverse languages show that this language-specific ablation\nconsistently boosts multilingual reasoning performance. Layer-wise analyses\nfurther confirm that language and reasoning representations can be effectively\ndecoupled throughout the model, yielding improved multilingual reasoning\ncapabilities, while preserving top-layer language features remains essential\nfor maintaining linguistic fidelity. Compared to post-training such as\nsupervised fine-tuning or reinforcement learning, our training-free ablation\nachieves comparable or superior results with minimal computational overhead.\nThese findings shed light on the internal mechanisms underlying multilingual\nreasoning in LLMs and suggest a lightweight and interpretable strategy for\nimproving cross-lingual generalization.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["inference time"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15436", "pdf": "https://arxiv.org/pdf/2505.15436", "abs": "https://arxiv.org/abs/2505.15436", "authors": ["Xintong Zhang", "Zhi Gao", "Bofei Zhang", "Pengxiang Li", "Xiaowen Zhang", "Yang Liu", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15438", "pdf": "https://arxiv.org/pdf/2505.15438", "abs": "https://arxiv.org/abs/2505.15438", "authors": ["Jianyuan Guo", "Peike Li", "Trevor Cohn"], "title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation", "categories": ["cs.CV"], "comment": "Technical report, 21 pages", "summary": "Sign Language Translation (SLT) aims to map sign language videos to spoken\nlanguage text. A common approach relies on gloss annotations as an intermediate\nrepresentation, decomposing SLT into two sub-tasks: video-to-gloss recognition\nand gloss-to-text translation. While effective, this paradigm depends on\nexpert-annotated gloss labels, which are costly and rarely available in\nexisting datasets, limiting its scalability. To address this challenge, we\npropose a gloss-free pseudo gloss generation framework that eliminates the need\nfor human-annotated glosses while preserving the structured intermediate\nrepresentation. Specifically, we prompt a Large Language Model (LLM) with a few\nexample text-gloss pairs using in-context learning to produce draft sign\nglosses from spoken language text. To enhance the correspondence between\nLLM-generated pseudo glosses and the sign sequences in video, we correct the\nordering in the pseudo glosses for better alignment via a weakly supervised\nlearning process. This reordering facilitates the incorporation of auxiliary\nalignment objectives, and allows for the use of efficient supervision via a\nConnectionist Temporal Classification (CTC) loss. We train our SLT mode, which\nconsists of a vision encoder and a translator, through a three-stage pipeline,\nwhich progressively narrows the modality gap between sign language and spoken\nlanguage. Despite its simplicity, our approach outperforms previous\nstate-of-the-art gloss-free frameworks on two SLT benchmarks and achieves\ncompetitive results compared to gloss-based methods.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15447", "pdf": "https://arxiv.org/pdf/2505.15447", "abs": "https://arxiv.org/abs/2505.15447", "authors": ["Ziqiang Xu", "Qi Dai", "Tian Xie", "Yifan Yang", "Kai Qiu", "DongDong Chen", "Zuxuan Wu", "Chong Luo"], "title": "ViaRL: Adaptive Temporal Grounding via Visual Iterated Amplification Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video understanding is inherently intention-driven-humans naturally focus on\nrelevant frames based on their goals. Recent advancements in multimodal large\nlanguage models (MLLMs) have enabled flexible query-driven reasoning; however,\nvideo-based frameworks like Video Chain-of-Thought lack direct training signals\nto effectively identify relevant frames. Current approaches often rely on\nheuristic methods or pseudo-label supervised annotations, which are both costly\nand limited in scalability across diverse scenarios. To overcome these\nchallenges, we introduce ViaRL, the first framework to leverage rule-based\nreinforcement learning (RL) for optimizing frame selection in intention-driven\nvideo understanding. An iterated amplification strategy is adopted to perform\nalternating cyclic training in the video CoT system, where each component\nundergoes iterative cycles of refinement to improve its capabilities. ViaRL\nutilizes the answer accuracy of a downstream model as a reward signal to train\na frame selector through trial-and-error, eliminating the need for expensive\nannotations while closely aligning with human-like learning processes.\nComprehensive experiments across multiple benchmarks, including VideoMME,\nLVBench, and MLVU, demonstrate that ViaRL consistently delivers superior\ntemporal grounding performance and robust generalization across diverse video\nunderstanding tasks, highlighting its effectiveness and scalability. Notably,\nViaRL achieves a nearly 15\\% improvement on Needle QA, a subset of MLVU, which\nis required to search a specific needle within a long video and regarded as one\nof the most suitable benchmarks for evaluating temporal grounding.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15333", "pdf": "https://arxiv.org/pdf/2505.15333", "abs": "https://arxiv.org/abs/2505.15333", "authors": ["Yuhao Zhang", "Xiangnan Ma", "Kaiqi Kou", "Peizhuo Liu", "Weiqiao Shan", "Benyou Wang", "Tong Xiao", "Yuxin Huang", "Zhengtao Yu", "Jingbo Zhu"], "title": "Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to ACL 2025 Findings", "summary": "The success of building textless speech-to-speech translation (S2ST) models\nhas attracted much attention. However, S2ST still faces two main challenges: 1)\nextracting linguistic features for various speech signals, called cross-modal\n(CM), and 2) learning alignment of difference languages in long sequences,\ncalled cross-lingual (CL). We propose the unit language to overcome the two\nmodeling challenges. The unit language can be considered a text-like\nrepresentation format, constructed using $n$-gram language modeling. We\nimplement multi-task learning to utilize the unit language in guiding the\nspeech modeling process. Our initial results reveal a conflict when applying\nsource and target unit languages simultaneously. We propose task prompt\nmodeling to mitigate this conflict. We conduct experiments on four languages of\nthe Voxpupil dataset. Our method demonstrates significant improvements over a\nstrong baseline and achieves performance comparable to models trained with\ntext.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15347", "pdf": "https://arxiv.org/pdf/2505.15347", "abs": "https://arxiv.org/abs/2505.15347", "authors": ["Xiang Liu", "Hong Chen", "Xuming Hu", "Xiaowen Chu"], "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "dialogue"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15355", "pdf": "https://arxiv.org/pdf/2505.15355", "abs": "https://arxiv.org/abs/2505.15355", "authors": ["Xabier de Zuazo", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "Decoding Phone Pairs from MEG Signals Across Speech Modalities", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD", "eess.AS", "I.2.6; I.5.1"], "comment": "21 pages, 4 figures, 1 graphical abstract, submitted to Computer\n  Speech and Language (special issue on Iberian Languages)", "summary": "Understanding the neural mechanisms underlying speech production is essential\nfor both advancing cognitive neuroscience theory and developing practical\ncommunication technologies. In this study, we investigated\nmagnetoencephalography signals to decode phones from brain activity during\nspeech production and perception (passive listening and voice playback) tasks.\nUsing a dataset comprising 17 participants, we performed pairwise phone\nclassification, extending our analysis to 15 phonetic pairs. Multiple machine\nlearning approaches, including regularized linear models and neural network\narchitectures, were compared to determine their effectiveness in decoding\nphonetic information. Our results demonstrate significantly higher decoding\naccuracy during speech production (76.6%) compared to passive listening and\nplayback modalities (~51%), emphasizing the richer neural information available\nduring overt speech. Among the models, the Elastic Net classifier consistently\noutperformed more complex neural networks, highlighting the effectiveness of\ntraditional regularization techniques when applied to limited and\nhigh-dimensional MEG datasets. Besides, analysis of specific brain frequency\nbands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)\nand Theta (4-7 Hz), contributed the most substantially to decoding accuracy,\nsuggesting that these bands encode critical speech production-related neural\nprocesses. Despite using advanced denoising methods, it remains unclear whether\ndecoding solely reflects neural activity or if residual muscular or movement\nartifacts also contributed, indicating the need for further methodological\nrefinement. Overall, our findings underline the critical importance of\nexamining overt speech production paradigms, which, despite their complexity,\noffer opportunities to improve brain-computer interfaces to help individuals\nwith severe speech impairments.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset", "accuracy"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15372", "pdf": "https://arxiv.org/pdf/2505.15372", "abs": "https://arxiv.org/abs/2505.15372", "authors": ["Peng Wang", "Ruihan Tao", "Qiguang Chen", "Mengkang Hu", "Libo Qin"], "title": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Recently, large language model (LLM)-based agents have achieved significant\nsuccess in interactive environments, attracting significant academic and\nindustrial attention. Despite these advancements, current research\npredominantly focuses on English scenarios. In reality, there are over 7,000\nlanguages worldwide, all of which demand access to comparable agentic services.\nNevertheless, the development of language agents remains inadequate for meeting\nthe diverse requirements of multilingual agentic applications. To fill this\ngap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an\ninteractive web environment, which evaluates the planning and interaction\nperformance of language agents across multiple languages, thereby contributing\nto the advancement of global agent intelligence. Additionally, we assess the\nperformance of various LLMs and cross-lingual alignment methods, examining\ntheir effectiveness in enhancing agents. Our findings reveal that even advanced\nmodels like GPT-4o, when combined with cross-lingual techniques, fail to\nachieve satisfactory results. We hope that X-WebAgentBench can serve as a\nvaluable benchmark for multilingual agent scenario in real-world applications.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15576", "pdf": "https://arxiv.org/pdf/2505.15576", "abs": "https://arxiv.org/abs/2505.15576", "authors": ["Xin Huang", "Ruibin Li", "Tong Jia", "Wei Zheng", "Ya Wang"], "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at the International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially\ncompositional reasoning (CR) tasks, which require distinguishing fine-grained\nsemantic differences between visual and textual embeddings. However, existing\nmethods primarily fine-tune the model by generating text-based hard negative\nsamples, neglecting the importance of image-based negative samples, which\nresults in insufficient training of the visual encoder and ultimately impacts\nthe overall performance of the model. Moreover, negative samples are typically\ntreated uniformly, without considering their difficulty levels, and the\nalignment of positive samples is insufficient, which leads to challenges in\naligning difficult sample pairs. To address these issues, we propose Adaptive\nHard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard\nnegatives into the visual domain to generate semantically disturbed image-based\nnegatives for training the model, thereby enhancing its overall performance.\nAHNPL also introduces a contrastive learning approach using a multimodal hard\nnegative loss to improve the model's discrimination of hard negatives within\neach modality and a dynamic margin loss that adjusts the contrastive margin\naccording to sample difficulty to enhance the distinction of challenging sample\npairs. Experiments on three public datasets demonstrate that our method\neffectively boosts VLMs' performance on complex CR tasks. The source code is\navailable at https://github.com/nynu-BDAI/AHNPL.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15428", "pdf": "https://arxiv.org/pdf/2505.15428", "abs": "https://arxiv.org/abs/2505.15428", "authors": ["Momose Oyama", "Ryo Kishino", "Hiroaki Yamagiwa", "Hidetoshi Shimodaira"], "title": "Likelihood Variance as Text Importance for Resampling Texts to Map Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We address the computational cost of constructing a model map, which embeds\ndiverse language models into a common space for comparison via KL divergence.\nThe map relies on log-likelihoods over a large text set, making the cost\nproportional to the number of texts. To reduce this cost, we propose a\nresampling method that selects important texts with weights proportional to the\nvariance of log-likelihoods across models for each text. Our method\nsignificantly reduces the number of required texts while preserving the\naccuracy of KL divergence estimates. Experiments show that it achieves\ncomparable performance to uniform sampling with about half as many texts, and\nalso facilitates efficient incorporation of new models into an existing map.\nThese results enable scalable and efficient construction of language model\nmaps.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15431", "pdf": "https://arxiv.org/pdf/2505.15431", "abs": "https://arxiv.org/abs/2505.15431", "authors": ["Ao Liu", "Botong Zhou", "Can Xu", "Chayse Zhou", "ChenChen Zhang", "Chengcheng Xu", "Chenhao Wang", "Decheng Wu", "Dengpeng Wu", "Dian Jiao", "Dong Du", "Dong Wang", "Feng Zhang", "Fengzong Lian", "Guanghui Xu", "Guanwei Zhang", "Hai Wang", "Haipeng Luo", "Han Hu", "Huilin Xu", "Jiajia Wu", "Jianchen Zhu", "Jianfeng Yan", "Jiaqi Zhu", "Jihong Zhang", "Jinbao Xue", "Jun Xia", "Junqiang Zheng", "Kai Liu", "Kai Zhang", "Kai Zheng", "Kejiao Li", "Keyao Wang", "Lan Jiang", "Lixin Liu", "Lulu Wu", "Mengyuan Huang", "Peijie Yu", "Peiqi Wang", "Qian Wang", "Qianbiao Xiang", "Qibin Liu", "Qingfeng Sun", "Richard Guo", "Ruobing Xie", "Saiyong Yang", "Shaohua Chen", "Shihui Hu", "Shuai Li", "Shuaipeng Li", "Shuang Chen", "Suncong Zheng", "Tao Yang", "Tian Zhang", "Tinghao Yu", "Weidong Han", "Weijie Liu", "Weijin Zhou", "Weikang Wang", "Wesleye Chen", "Xiao Feng", "Xiaoqin Ren", "Xingwu Sun", "Xiong Kuang", "Xuemeng Huang", "Xun Cao", "Yanfeng Chen", "Yang Du", "Yang Zhen", "Yangyu Tao", "Yaping Deng", "Yi Shen", "Yigeng Hong", "Yiqi Chen", "Yiqing Huang", "Yuchi Deng", "Yue Mao", "Yulong Wang", "Yuyuan Zeng", "Zenan Xu", "Zhanhui Kang", "Zhe Zhao", "ZhenXiang Yan", "Zheng Fang", "Zhichao Hu", "Zhongzhi Chen", "Zhuoyu Li", "Zongwei Li", "Alex Yan", "Ande Liang", "Baitong Liu", "Beiping Pan", "Bin Xing", "Binghong Wu", "Bingxin Qu", "Bolin Ni", "Boyu Wu", "Chen Li", "Cheng Jiang", "Cheng Zhang", "Chengjun Liu", "Chengxu Yang", "Chiyu Wang", "Chong Zha", "Daisy Yi", "Di Wang", "Fanyang Lu", "Fei Chen", "Feifei Liu", "Feng Zheng", "Guanghua Yu", "Guiyang Li", "Guohua Wang", "Haisheng Lin", "Han Liu", "Han Wang", "Hao Fei", "Hao Lu", "Haoqing Jiang", "Haoran Sun", "Haotian Zhu", "Huangjin Dai", "Huankui Chen", "Huawen Feng", "Huihui Cai", "Huxin Peng", "Jackson Lv", "Jiacheng Shi", "Jiahao Bu", "Jianbo Li", "Jianglu Hu", "Jiangtao Guan", "Jianing Xu", "Jianwei Cai", "Jiarong Zhang", "Jiawei Song", "Jie Jiang", "Jie Liu", "Jieneng Yang", "Jihong Zhang", "Jin lv", "Jing Zhao", "Jinjian Li", "Jinxing Liu", "Jun Zhao", "Juntao Guo", "Kai Wang", "Kan Wu", "Lei Fu", "Lei He", "Lei Wang", "Li Liu", "Liang Dong", "Liya Zhan", "Long Cheng", "Long Xu", "Mao Zheng", "Meng Liu", "Mengkang Hu", "Nanli Chen", "Peirui Chen", "Peng He", "Pengju Pan", "Pengzhi Wei", "Qi Yang", "Qi Yi", "Roberts Wang", "Rongpeng Chen", "Rui Sun", "Rui Yang", "Ruibin Chen", "Ruixu Zhou", "Shaofeng Zhang", "Sheng Zhang", "Shihao Xu", "Shuaishuai Chang", "Shulin Liu", "SiQi Wang", "Songjia Feng", "Songling Yuan", "Tao Zhang", "Tianjiao Lang", "Tongkai Li", "Wei Deng", "Wei Li", "Weichao Wang", "Weigang Zhang", "Weixuan Sun", "Wen Ouyang", "Wenxiang Jiao", "Wenzhi Sun", "Wenzhuo Jia", "Xiang Zhang", "Xiangyu He", "Xianshun Ren", "XiaoYing Zhu", "Xiaolong Guo", "Xiaoxue Li", "Xiaoyu Ma", "Xican Lu", "Xinhua Feng", "Xinting Huang", "Xinyu Guan", "Xirui Li", "Xu Zhang", "Xudong Gao", "Xun Luo", "Xuxiang Qi", "Yangkun Chen", "Yangyu Tao", "Yanling Xiao", "Yantao Mai", "Yanze Chen", "Yao Ding", "Yeting Yang", "YiFan Song", "Yifan Yang", "Yijiao Zhu", "Yinhe Wu", "Yixian Liu", "Yong Yang", "Yuanjun Cai", "Yuanlin Tu", "Yue Zhang", "Yufei Huang", "Yuhang Zhou", "Yuhao Jiang", "Yuhong Liu", "Yuhui Hu", "Yujin Lin", "Yun Yang", "Yunhao Wang", "Yusong Zhang", "Zekun Wu", "Zelong Zhang", "Zhan Yu", "Zhaoliang Yang", "Zhe Zhao", "Zheng Li", "Zhenyu Huang", "Zhiguang Liu", "Zhijiang Xu", "Zhiqing Kui", "Zhiyin Zeng", "Zhiyuan Xiong", "Zhuo Han", "Zifan Wu", "Zigang Geng", "Zilong Zhao", "Ziyan Tang", "Ziyuan Zhu", "Zonglei Zhu", "Zhijiang Xu"], "title": "Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15456", "pdf": "https://arxiv.org/pdf/2505.15456", "abs": "https://arxiv.org/abs/2505.15456", "authors": ["Weixiang Zhao", "Xingyu Sui", "Yulin Hu", "Jiahe Guo", "Haixiao Liu", "Biye Li", "Yanyan Zhao", "Bing Qin", "Ting Liu"], "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment", "categories": ["cs.CL"], "comment": "30 pages, 18 figures, 10 tables", "summary": "Personalized alignment is essential for enabling large language models (LLMs)\nto engage effectively in user-centric dialogue. While recent prompt-based and\noffline optimization methods offer preliminary solutions, they fall short in\ncold-start scenarios and long-term personalization due to their inherently\nstatic and shallow designs. In this work, we introduce the Reinforcement\nLearning for Personalized Alignment (RLPA) framework, in which an LLM interacts\nwith a simulated user model to iteratively infer and refine user profiles\nthrough dialogue. The training process is guided by a dual-level reward\nstructure: the Profile Reward encourages accurate construction of user\nrepresentations, while the Response Reward incentivizes generation of responses\nconsistent with the inferred profile. We instantiate RLPA by fine-tuning\nQwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art\nperformance in personalized dialogue. Empirical evaluations demonstrate that\nQwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,\nand even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.\nFurther analysis highlights Qwen-RLPA's robustness in reconciling conflicting\nuser preferences, sustaining long-term personalization and delivering more\nefficient inference compared to recent reasoning-focused LLMs. These results\nemphasize the potential of dynamic profile inference as a more effective\nparadigm for building personalized dialogue systems.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dialogue"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15687", "pdf": "https://arxiv.org/pdf/2505.15687", "abs": "https://arxiv.org/abs/2505.15687", "authors": ["Zhe Xu", "Cheng Jin", "Yihui Wang", "Ziyi Liu", "Hao Chen"], "title": "Discovering Pathology Rationale and Token Allocation for Efficient Multimodal Pathology Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal pathological image understanding has garnered widespread interest\ndue to its potential to improve diagnostic accuracy and enable personalized\ntreatment through integrated visual and textual data. However, existing methods\nexhibit limited reasoning capabilities, which hamper their ability to handle\ncomplex diagnostic scenarios. Additionally, the enormous size of pathological\nimages leads to severe computational burdens, further restricting their\npractical deployment. To address these limitations, we introduce a novel\nbilateral reinforcement learning framework comprising two synergistic branches.\nOne reinforcement branch enhances the reasoning capability by enabling the\nmodel to learn task-specific decision processes, i.e., pathology rationales,\ndirectly from labels without explicit reasoning supervision. While the other\nbranch dynamically allocates a tailored number of tokens to different images\nbased on both their visual content and task context, thereby optimizing\ncomputational efficiency. We apply our method to various pathological tasks\nsuch as visual question answering, cancer subtyping, and lesion detection.\nExtensive experiments show an average +41.7 absolute performance improvement\nwith 70.3% lower inference costs over the base models, achieving both reasoning\naccuracy and computational efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "question answering"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15475", "pdf": "https://arxiv.org/pdf/2505.15475", "abs": "https://arxiv.org/abs/2505.15475", "authors": ["Zhanyue Qin", "Yue Ding", "Deyuan Liu", "Qingbin Liu", "Junxian Cai", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Cuiyun Gao", "Dianbo Sui"], "title": "LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Nowadays, Large Language Models (LLMs) have attracted widespread attention\ndue to their powerful performance. However, due to the unavoidable exposure to\nsocially biased data during training, LLMs tend to exhibit social biases,\nparticularly gender bias. To better explore and quantifying the degree of\ngender bias in LLMs, we propose a pair of datasets named GenBiasEval and\nGenHintEval, respectively. The GenBiasEval is responsible for evaluating the\ndegree of gender bias in LLMs, accompanied by an evaluation metric named\nAFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is\nused to assess whether LLMs can provide responses consistent with prompts that\ncontain gender hints, along with the accompanying evaluation metric UB-Score\n(UnBias Score). Besides, in order to mitigate gender bias in LLMs more\neffectively, we present the LFTF (Locating First and Then Fine-Tuning)\nalgorithm.The algorithm first ranks specific LLM blocks by their relevance to\ngender bias in descending order using a metric called BMI (Block Mitigating\nImportance Score). Based on this ranking, the block most strongly associated\nwith gender bias is then fine-tuned using a carefully designed loss function.\nNumerous experiments have shown that our proposed LFTF algorithm can\nsignificantly mitigate gender bias in LLMs while maintaining their general\ncapabilities.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["ranking"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15765", "pdf": "https://arxiv.org/pdf/2505.15765", "abs": "https://arxiv.org/abs/2505.15765", "authors": ["Kaizhi Zheng", "Ruijian Zhang", "Jing Gu", "Jie Yang", "Xin Eric Wang"], "title": "Constructing a 3D Town from a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Acquiring detailed 3D scenes typically demands costly equipment, multi-view\ndata, or labor-intensive modeling. Therefore, a lightweight alternative,\ngenerating complex 3D scenes from a single top-down image, plays an essential\nrole in real-world applications. While recent 3D generative models have\nachieved remarkable results at the object level, their extension to full-scene\ngeneration often leads to inconsistent geometry, layout hallucinations, and\nlow-quality meshes. In this work, we introduce 3DTown, a training-free\nframework designed to synthesize realistic and coherent 3D scenes from a single\ntop-down view. Our method is grounded in two principles: region-based\ngeneration to improve image-to-3D alignment and resolution, and spatial-aware\n3D inpainting to ensure global scene coherence and high-quality geometry\ngeneration. Specifically, we decompose the input image into overlapping regions\nand generate each using a pretrained 3D object generator, followed by a masked\nrectified flow inpainting process that fills in missing geometry while\nmaintaining structural continuity. This modular design allows us to overcome\nresolution bottlenecks and preserve spatial structure without requiring 3D\nsupervision or fine-tuning. Extensive experiments across diverse scenes show\nthat 3DTown outperforms state-of-the-art baselines, including Trellis,\nHunyuan3D-2, and TripoSG, in terms of geometry quality, spatial coherence, and\ntexture fidelity. Our results demonstrate that high-quality 3D town generation\nis achievable from a single image using a principled, training-free approach.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15791", "pdf": "https://arxiv.org/pdf/2505.15791", "abs": "https://arxiv.org/abs/2505.15791", "authors": ["Fengyuan Dai", "Zifeng Zhuang", "Yufei Huang", "Siteng Huang", "Bangyan Liao", "Donglin Wang", "Fajie Yuan"], "title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Diffusion models have emerged as powerful generative tools across various\ndomains, yet tailoring pre-trained models to exhibit specific desirable\nproperties remains challenging. While reinforcement learning (RL) offers a\npromising solution,current methods struggle to simultaneously achieve stable,\nefficient fine-tuning and support non-differentiable rewards. Furthermore,\ntheir reliance on sparse rewards provides inadequate supervision during\nintermediate steps, often resulting in suboptimal generation quality. To\naddress these limitations, dense and differentiable signals are required\nthroughout the diffusion process. Hence, we propose VAlue-based Reinforced\nDiffusion (VARD): a novel approach that first learns a value function\npredicting expection of rewards from intermediate states, and subsequently uses\nthis value function with KL regularization to provide dense supervision\nthroughout the generation process. Our method maintains proximity to the\npretrained model while enabling effective and stable training via\nbackpropagation. Experimental results demonstrate that our approach facilitates\nbetter trajectory guidance, improves training efficiency and extends the\napplicability of RL to diffusion models optimized for complex,\nnon-differentiable reward functions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15804", "pdf": "https://arxiv.org/pdf/2505.15804", "abs": "https://arxiv.org/abs/2505.15804", "authors": ["Zongzhao Li", "Zongyang Ma", "Mingze Li", "Songyou Li", "Yu Rong", "Tingyang Xu", "Ziqi Zhang", "Deli Zhao", "Wenbing Huang"], "title": "STAR-R1: Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["fine-grained"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15812", "pdf": "https://arxiv.org/pdf/2505.15812", "abs": "https://arxiv.org/abs/2505.15812", "authors": ["Satoshi Kosugi"], "title": "Leveraging the Powerful Attention of a Pre-trained Diffusion Model for Exemplar-based Image Colorization", "categories": ["cs.CV"], "comment": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "summary": "Exemplar-based image colorization aims to colorize a grayscale image using a\nreference color image, ensuring that reference colors are applied to\ncorresponding input regions based on their semantic similarity. To achieve\naccurate semantic matching between regions, we leverage the self-attention\nmodule of a pre-trained diffusion model, which is trained on a large dataset\nand exhibits powerful attention capabilities. To harness this power, we propose\na novel, fine-tuning-free approach based on a pre-trained diffusion model,\nmaking two key contributions. First, we introduce dual attention-guided color\ntransfer. We utilize the self-attention module to compute an attention map\nbetween the input and reference images, effectively capturing semantic\ncorrespondences. The color features from the reference image is then\ntransferred to the semantically matching regions of the input image, guided by\nthis attention map, and finally, the grayscale features are replaced with the\ncorresponding color features. Notably, we utilize dual attention to calculate\nattention maps separately for the grayscale and color images, achieving more\nprecise semantic alignment. Second, we propose classifier-free colorization\nguidance, which enhances the transferred colors by combining color-transferred\nand non-color-transferred outputs. This process improves the quality of\ncolorization. Our experimental results demonstrate that our method outperforms\nexisting techniques in terms of image quality and fidelity to the reference.\nSpecifically, we use 335 input-reference pairs from previous research,\nachieving an FID of 95.27 (image quality) and an SI-FID of 5.51 (fidelity to\nthe reference). Our source code is available at\nhttps://github.com/satoshi-kosugi/powerful-attention.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dataset"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15612", "pdf": "https://arxiv.org/pdf/2505.15612", "abs": "https://arxiv.org/abs/2505.15612", "authors": ["Wei Liu", "Ruochen Zhou", "Yiyun Deng", "Yuzhen Huang", "Junteng Liu", "Yuntian Deng", "Yizhe Zhang", "Junxian He"], "title": "Learn to Reason Efficiently with Adaptive Length-based Reward Shaping", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) have shown remarkable capabilities in solving\ncomplex problems through reinforcement learning (RL), particularly by\ngenerating long reasoning traces. However, these extended outputs often exhibit\nsubstantial redundancy, which limits the efficiency of LRMs. In this paper, we\ninvestigate RL-based approaches to promote reasoning efficiency. Specifically,\nwe first present a unified framework that formulates various efficient\nreasoning methods through the lens of length-based reward shaping. Building on\nthis perspective, we propose a novel Length-bAsed StEp Reward shaping method\n(LASER), which employs a step function as the reward, controlled by a target\nlength. LASER surpasses previous methods, achieving a superior Pareto-optimal\nbalance between performance and efficiency. Next, we further extend LASER based\non two key intuitions: (1) The reasoning behavior of the model evolves during\ntraining, necessitating reward specifications that are also adaptive and\ndynamic; (2) Rather than uniformly encouraging shorter or longer chains of\nthought (CoT), we posit that length-based reward shaping should be\ndifficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.\nThis approach is expected to facilitate a combination of fast and slow\nthinking, leading to a better overall tradeoff. The resulting method is termed\nLASER-D (Dynamic and Difficulty-aware). Experiments on\nDeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and\nDeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both\nreasoning performance and response length efficiency. For instance, LASER-D and\nits variant achieve a +6.1 improvement on AIME2024 while reducing token usage\nby 63%. Further analysis reveals our RL-based compression produces more concise\nreasoning patterns with less redundant \"self-reflections\". Resources are at\nhttps://github.com/hkust-nlp/Laser.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14722", "pdf": "https://arxiv.org/pdf/2505.14722", "abs": "https://arxiv.org/abs/2505.14722", "authors": ["Pierre-Marc Jodoin", "Manon Edde", "Gabriel Girard", "Félix Dumais", "Guillaume Theaud", "Matthieu Dumont", "Jean-Christophe Houde", "Yoan David", "Maxime Descoteaux"], "title": "ComBAT Harmonization for diffusion MRI: Challenges and Best Practices", "categories": ["stat.AP", "cs.CV", "cs.LG", "physics.med-ph"], "comment": null, "summary": "Over the years, ComBAT has become the standard method for harmonizing\nMRI-derived measurements, with its ability to compensate for site-related\nadditive and multiplicative biases while preserving biological variability.\nHowever, ComBAT relies on a set of assumptions that, when violated, can result\nin flawed harmonization. In this paper, we thoroughly review ComBAT's\nmathematical foundation, outlining these assumptions, and exploring their\nimplications for the demographic composition necessary for optimal results.\n  Through a series of experiments involving a slightly modified version of\nComBAT called Pairwise-ComBAT tailored for normative modeling applications, we\nassess the impact of various population characteristics, including population\nsize, age distribution, the absence of certain covariates, and the magnitude of\nadditive and multiplicative factors. Based on these experiments, we present\nfive essential recommendations that should be carefully considered to enhance\nconsistency and supporting reproducibility, two essential factors for open\nscience, collaborative research, and real-life clinical deployment.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["pairwise"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["consistency"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15646", "pdf": "https://arxiv.org/pdf/2505.15646", "abs": "https://arxiv.org/abs/2505.15646", "authors": ["Ke Hu", "Krishna Puvvada", "Elena Rastorgueva", "Zhehuai Chen", "He Huang", "Shuoyang Ding", "Kunal Dhawan", "Hainan Xu", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Word Level Timestamp Generation for Automatic Speech Recognition and Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce a data-driven approach for enabling word-level timestamp\nprediction in the Canary model. Accurate timestamp information is crucial for a\nvariety of downstream tasks such as speech content retrieval and timed\nsubtitles. While traditional hybrid systems and end-to-end (E2E) models may\nemploy external modules for timestamp prediction, our approach eliminates the\nneed for separate alignment mechanisms. By leveraging the NeMo Forced Aligner\n(NFA) as a teacher model, we generate word-level timestamps and train the\nCanary model to predict timestamps directly. We introduce a new <|timestamp|>\ntoken, enabling the Canary model to predict start and end timestamps for each\nword. Our method demonstrates precision and recall rates between 80% and 90%,\nwith timestamp prediction errors ranging from 20 to 120 ms across four\nlanguages, with minimal WER degradation. Additionally, we extend our system to\nautomatic speech translation (AST) tasks, achieving timestamp prediction errors\naround 200 milliseconds.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14866", "pdf": "https://arxiv.org/pdf/2505.14866", "abs": "https://arxiv.org/abs/2505.14866", "authors": ["Nisarga Nilavadi", "Andrey Rudenko", "Timm Linder"], "title": "UPTor: Unified 3D Human Pose Dynamics and Trajectory Prediction for Human-Robot Interaction", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://nisarganc.github.io/UPTor-page/", "summary": "We introduce a unified approach to forecast the dynamics of human keypoints\nalong with the motion trajectory based on a short sequence of input poses.\nWhile many studies address either full-body pose prediction or motion\ntrajectory prediction, only a few attempt to merge them. We propose a motion\ntransformation technique to simultaneously predict full-body pose and\ntrajectory key-points in a global coordinate frame. We utilize an off-the-shelf\n3D human pose estimation module, a graph attention network to encode the\nskeleton structure, and a compact, non-autoregressive transformer suitable for\nreal-time motion prediction for human-robot interaction and human-aware\nnavigation. We introduce a human navigation dataset ``DARKO'' with specific\nfocus on navigational activities that are relevant for human-aware mobile robot\nnavigation. We perform extensive evaluation on Human3.6M, CMU-Mocap, and our\nDARKO dataset. In comparison to prior work, we show that our approach is\ncompact, real-time, and accurate in predicting human navigation motion across\nall datasets. Result animations, our dataset, and code will be available at\nhttps://nisarganc.github.io/UPTor-page/", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["comparison"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation", "dataset"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15682", "pdf": "https://arxiv.org/pdf/2505.15682", "abs": "https://arxiv.org/abs/2505.15682", "authors": ["Cosimo Iaia", "Bhavin Choksi", "Emily Wiebers", "Gemma Roig", "Christian J. Fiebach"], "title": "The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect", "categories": ["cs.CL", "I.2.7; J.4"], "comment": "13 pages, 4 Figures, 1 Table", "summary": "The nouns of our language refer to either concrete entities (like a table) or\nabstract concepts (like justice or love), and cognitive psychology has\nestablished that concreteness influences how words are processed. Accordingly,\nunderstanding how concreteness is represented in our mind and brain is a\ncentral question in psychology, neuroscience, and computational linguistics.\nWhile the advent of powerful language models has allowed for quantitative\ninquiries into the nature of semantic representations, it remains largely\nunderexplored how they represent concreteness. Here, we used behavioral\njudgments to estimate semantic distances implicitly used by humans, for a set\nof carefully selected abstract and concrete nouns. Using Representational\nSimilarity Analysis, we find that the implicit representational space of\nparticipants and the semantic representations of language models are\nsignificantly aligned. We also find that both representational spaces are\nimplicitly aligned to an explicit representation of concreteness, which was\nobtained from our participants using an additional concreteness rating task.\nImportantly, using ablation experiments, we demonstrate that the human-to-model\nalignment is substantially driven by concreteness, but not by other important\nword characteristics established in psycholinguistics. These results indicate\nthat humans and language models converge on the concreteness dimension, but not\non other dimensions.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["dimension"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15234", "pdf": "https://arxiv.org/pdf/2505.15234", "abs": "https://arxiv.org/abs/2505.15234", "authors": ["Saqib Qamar", "Mohd Fazil", "Parvez Ahmad", "Ghulam Muhammad"], "title": "SAMA-UNet: Enhancing Medical Image Segmentation with Self-Adaptive Mamba-Like Attention and Causal-Resonance Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays an important role in various clinical\napplications, but existing models often struggle with the computational\ninefficiencies and challenges posed by complex medical data. State Space\nSequence Models (SSMs) have demonstrated promise in modeling long-range\ndependencies with linear computational complexity, yet their application in\nmedical image segmentation remains hindered by incompatibilities with image\ntokens and autoregressive assumptions. Moreover, it is difficult to achieve a\nbalance in capturing both local fine-grained information and global semantic\ndependencies. To address these challenges, we introduce SAMA-UNet, a novel\narchitecture for medical image segmentation. A key innovation is the\nSelf-Adaptive Mamba-like Aggregated Attention (SAMA) block, which integrates\ncontextual self-attention with dynamic weight modulation to prioritise the most\nrelevant features based on local and global contexts. This approach reduces\ncomputational complexity and improves the representation of complex image\nfeatures across multiple scales. We also suggest the Causal-Resonance\nMulti-Scale Module (CR-MSM), which enhances the flow of information between the\nencoder and decoder by using causal resonance learning. This mechanism allows\nthe model to automatically adjust feature resolution and causal dependencies\nacross scales, leading to better semantic alignment between the low-level and\nhigh-level features in U-shaped architectures. Experiments on MRI, CT, and\nendoscopy images show that SAMA-UNet performs better in segmentation accuracy\nthan current methods using CNN, Transformer, and Mamba. The implementation is\npublicly available at GitHub.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15792", "pdf": "https://arxiv.org/pdf/2505.15792", "abs": "https://arxiv.org/abs/2505.15792", "authors": ["Danna Zheng", "Mirella Lapata", "Jeff Z. Pan"], "title": "Long-Form Information Alignment Evaluation Beyond Atomic Facts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Information alignment evaluators are vital for various NLG evaluation tasks\nand trustworthy LLM deployment, reducing hallucinations and enhancing user\ntrust. Current fine-grained methods, like FactScore, verify facts individually\nbut neglect inter-fact dependencies, enabling subtle vulnerabilities. In this\nwork, we introduce MontageLie, a challenging benchmark that constructs\ndeceptive narratives by \"montaging\" truthful statements without introducing\nexplicit hallucinations. We demonstrate that both coarse-grained LLM-based\nevaluators and current fine-grained frameworks are susceptible to this attack,\nwith AUC-ROC scores falling below 65%. To enable more robust fine-grained\nevaluation, we propose DoveScore, a novel framework that jointly verifies\nfactual accuracy and event-order consistency. By modeling inter-fact\nrelationships, DoveScore outperforms existing fine-grained methods by over 8%,\nproviding a more robust solution for long-form text alignment evaluation. Our\ncode and datasets are available at https://github.com/dannalily/DoveScore.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy", "fine-grained"], "score": 5}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15795", "pdf": "https://arxiv.org/pdf/2505.15795", "abs": "https://arxiv.org/abs/2505.15795", "authors": ["Lisa Alazraki", "Tan Yi-Chern", "Jon Ander Campos", "Maximilian Mozes", "Marek Rei", "Max Bartolo"], "title": "Reverse Engineering Human Preferences with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are routinely evaluated by\nother LLMs trained to predict human preferences. This framework--known as\nLLM-as-a-judge--is highly scalable and relatively low cost. However, it is also\nvulnerable to malicious exploitation, as LLM responses can be tuned to overfit\nthe preferences of the judge. Previous work shows that the answers generated by\na candidate-LLM can be edited post hoc to maximise the score assigned to them\nby a judge-LLM. In this study, we adopt a different approach and use the signal\nprovided by judge-LLMs as a reward to adversarially tune models that generate\ntext preambles designed to boost downstream performance. We find that frozen\nLLMs pipelined with these models attain higher LLM-evaluation scores than\nexisting frameworks. Crucially, unlike other frameworks which intervene\ndirectly on the model's response, our method is virtually undetectable. We also\ndemonstrate that the effectiveness of the tuned preamble generator transfers\nwhen the candidate-LLM and the judge-LLM are replaced with models that are not\nused during training. These findings raise important questions about the design\nof more reliable LLM-as-a-judge evaluation settings. They also demonstrate that\nhuman preferences can be reverse engineered effectively, by pipelining LLMs to\noptimise upstream preambles via reinforcement learning--an approach that could\nfind future applications in diverse tasks and domains beyond adversarial\nattacks.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["evaluation"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15805", "pdf": "https://arxiv.org/pdf/2505.15805", "abs": "https://arxiv.org/abs/2505.15805", "authors": ["Hwan Chang", "Yumin Kim", "Yonghyun Jun", "Hwanhee Lee"], "title": "Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains such as enterprise and government, ensuring that they adhere to\nuser-defined security policies within context is critical-especially with\nrespect to information non-disclosure. While prior LLM studies have focused on\ngeneral safety and socially sensitive data, large-scale benchmarks for\ncontextual security preservation against attacks remain lacking. To address\nthis, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating\nLLM adherence to contextual non-disclosure policies in question answering.\nDerived from realistic contexts, our dataset includes explicit policies and\nqueries designed as direct and challenging indirect attacks seeking prohibited\ninformation. We evaluate 10 LLMs on our benchmark and reveal a significant\nvulnerability: many models violate user-defined policies and leak sensitive\ninformation. This failure is particularly severe against indirect attacks,\nhighlighting a critical gap in current LLM safety alignment for sensitive\napplications. Our analysis reveals that while models can often identify the\ncorrect answer to a query, they struggle to incorporate policy constraints\nduring generation. In contrast, they exhibit a partial ability to revise\noutputs when explicitly prompted. Our findings underscore the urgent need for\nmore robust methods to guarantee contextual security.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["scale"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "dataset", "safety", "question answering"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.14728", "pdf": "https://arxiv.org/pdf/2505.14728", "abs": "https://arxiv.org/abs/2505.14728", "authors": ["Xiao Lin", "Zhining Liu", "Ze Yang", "Gaotang Li", "Ruizhong Qiu", "Shuke Wang", "Hui Liu", "Haotian Li", "Sumit Keswani", "Vishwa Pardeshi", "Huijun Zhao", "Wei Fan", "Hanghang Tong"], "title": "MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.MM"], "comment": "21 pages, 11 figures, 7 tables", "summary": "Warning: This paper contains examples of harmful language and images. Reader\ndiscretion is advised. Recently, vision-language models have demonstrated\nincreasing influence in morally sensitive domains such as autonomous driving\nand medical analysis, owing to their powerful multimodal reasoning\ncapabilities. As these models are deployed in high-stakes real-world\napplications, it is of paramount importance to ensure that their outputs align\nwith human moral values and remain within moral boundaries. However, existing\nwork on moral alignment either focuses solely on textual modalities or relies\nheavily on AI-generated images, leading to distributional biases and reduced\nrealism. To overcome these limitations, we introduce MORALISE, a comprehensive\nbenchmark for evaluating the moral alignment of vision-language models (VLMs)\nusing diverse, expert-verified real-world data. We begin by proposing a\ncomprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,\nspanning the personal, interpersonal, and societal moral domains encountered in\neveryday life. Built on this framework, we manually curate 2,481 high-quality\nimage-text pairs, each annotated with two fine-grained labels: (1) topic\nannotation, identifying the violated moral topic(s), and (2) modality\nannotation, indicating whether the violation arises from the image or the text.\nFor evaluation, we encompass two tasks, \\textit{moral judgment} and\n\\textit{moral norm attribution}, to assess models' awareness of moral\nviolations and their reasoning ability on morally salient content. Extensive\nexperiments on 19 popular open- and closed-source VLMs show that MORALISE poses\na significant challenge, revealing persistent moral limitations in current\nstate-of-the-art models. The full benchmark is publicly available at\nhttps://huggingface.co/datasets/Ze1025/MORALISE.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "annotation", "fine-grained"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15158", "pdf": "https://arxiv.org/pdf/2505.15158", "abs": "https://arxiv.org/abs/2505.15158", "authors": ["Yunsheng Ma", "Burhaneddin Yaman", "Xin Ye", "Mahmut Yurt", "Jingru Luo", "Abhirup Mallik", "Ziran Wang", "Liu Ren"], "title": "ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages", "summary": "Recent advances have explored integrating large language models (LLMs) into\nend-to-end autonomous driving systems to enhance generalization and\ninterpretability. However, most existing approaches are limited to either\ndriving performance or vision-language reasoning, making it difficult to\nachieve both simultaneously. In this paper, we propose ALN-P3, a unified\nco-distillation framework that introduces cross-modal alignment between \"fast\"\nvision-based autonomous driving systems and \"slow\" language-driven reasoning\nmodules. ALN-P3 incorporates three novel alignment mechanisms: Perception\nAlignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),\nwhich explicitly align visual tokens with corresponding linguistic outputs\nacross the full perception, prediction, and planning stack. All alignment\nmodules are applied only during training and incur no additional costs during\ninference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,\nTOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both\ndriving decisions and language reasoning, achieving state-of-the-art results.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15259", "pdf": "https://arxiv.org/pdf/2505.15259", "abs": "https://arxiv.org/abs/2505.15259", "authors": ["Hyunseok Lee", "Jeonghoon Kim", "Beomjun Kim", "Jihoon Tack", "Chansong Jo", "Jaehong Lee", "Cheonbok Park", "Sookyo In", "Jinwoo Shin", "Kang Min Yoo"], "title": "ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled\nautonomous agents to interact with computers via Graphical User Interfaces\n(GUIs), where accurately localizing the coordinates of interface elements\n(e.g., buttons) is often required for fine-grained actions. However, this\nremains significantly challenging, leading prior works to rely on large-scale\nweb datasets to improve the grounding accuracy. In this work, we propose\nReasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a\nnovel and effective framework for web grounding that enables MLLMs to learn\ndata efficiently through self-generated reasoning and spatial-aware criticism.\nMore specifically, ReGUIDE learns to (i) self-generate a language reasoning\nprocess for the localization via online reinforcement learning, and (ii)\ncriticize the prediction using spatial priors that enforce equivariance under\ninput transformations. At inference time, ReGUIDE further boosts performance\nthrough a test-time scaling strategy, which combines spatial search with\ncoordinate aggregation. Our experiments demonstrate that ReGUIDE significantly\nadvances web grounding performance across multiple benchmarks, outperforming\nbaselines with substantially fewer training data points (e.g., only 0.2%\nsamples compared to the best open-sourced baselines).", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["test-time", "inference time", "scaling", "scale"], "score": 4}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy", "fine-grained"], "score": 2}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15276", "pdf": "https://arxiv.org/pdf/2505.15276", "abs": "https://arxiv.org/abs/2505.15276", "authors": ["Rongzhi Zhu", "Yi Liu", "Zequn Sun", "Yiwei Wang", "Wei Hu"], "title": "When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have significantly advanced performance on\ncomplex tasks, yet their tendency to overthink introduces inefficiencies. This\nstudy investigates the internal mechanisms of reinforcement learning\n(RL)-trained LRMs when prompted to save thinking, revealing three distinct\nthinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking\n(IT). Through comprehensive analysis of confidence in thinking termination,\nattention from thinking to generation, and attentional focus on input sections,\nwe uncover key factors influencing the reasoning behaviors. We further find\nthat NT reduces output length at the cost of accuracy, while ET and IT maintain\naccuracy with reduced response length. Our findings expose fundamental\ninconsistencies in RL-optimized LRMs, necessitating adaptive improvements for\nreliable efficiency.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["reinforcement learning"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["accuracy"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15298", "pdf": "https://arxiv.org/pdf/2505.15298", "abs": "https://arxiv.org/abs/2505.15298", "authors": ["Kangan Qian", "Sicong Jiang", "Yang Zhong", "Ziang Luo", "Zilin Huang", "Tianze Zhu", "Kun Jiang", "Mengmeng Yang", "Zheng Fu", "Jinyu Miao", "Yining Shi", "He Zhe Lim", "Li Liu", "Tianbao Zhou", "Hongyi Wang", "Huang Yu", "Yifei Hu", "Guang Li", "Guang Chen", "Hao Ye", "Lijun Sun", "Diange Yang"], "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "18 pages, 8 figures", "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.", "survey_categories": {"test_time_scaling": {"chapter_name": "Test-Time Scaling", "matched_keywords": ["o1"], "score": 1}, "reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["policy optimization"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["benchmark", "evaluation", "consistency", "accuracy"], "score": 4}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15433", "pdf": "https://arxiv.org/pdf/2505.15433", "abs": "https://arxiv.org/abs/2505.15433", "authors": ["Beni Egressy", "Jan Stühmer"], "title": "Set-LLM: A Permutation-Invariant LLM", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While large language models (LLMs) demonstrate impressive capabilities across\nnumerous applications, their robustness remains a critical concern. This paper\nis motivated by a specific vulnerability: the order sensitivity of LLMs. This\nvulnerability manifests itself as the order bias observed when LLMs decide\nbetween possible options (for example, a preference for the first option) and\nthe tendency of LLMs to provide different answers when options are reordered.\nThe use cases for this scenario extend beyond the classical case of\nmultiple-choice question answering to the use of LLMs as automated evaluators\nin AI pipelines, comparing output generated by different models. We introduce\nSet-LLM, a novel architectural adaptation for pretrained LLMs that enables the\nprocessing of mixed set-text inputs with permutation invariance guarantees. The\nadaptations involve a new attention mask and new positional encodings\nspecifically designed for sets. We provide a theoretical proof of invariance\nand demonstrate through experiments that Set-LLM can be trained effectively,\nachieving comparable or improved performance and maintaining the runtime of the\noriginal model, while eliminating order sensitivity.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["preference"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["question answering"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
{"id": "2505.15738", "pdf": "https://arxiv.org/pdf/2505.15738", "abs": "https://arxiv.org/abs/2505.15738", "authors": ["Xiaoxue Yang", "Bozhidar Stevanoski", "Matthieu Meeus", "Yves-Alexandre de Montjoye"], "title": "Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.", "survey_categories": {"reward_model_rl": {"chapter_name": "Reward Model for RL", "matched_keywords": ["alignment"], "score": 1}, "reward_model_benchmark": {"chapter_name": "Reward Model Benchmark", "matched_keywords": ["safety"], "score": 1}}, "source_file": "2025-05-22.jsonl"}
